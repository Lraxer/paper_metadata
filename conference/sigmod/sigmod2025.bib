@article{DBLP:journals/pacmmod/AbouziedHA25,
	author = {Azza Abouzied and
                  Joseph M. Hellerstein and
                  Divyakant Agrawal},
	title = {{PACMMOD} V3, {N1} (SIGMOD), February 2025: Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {1:1--1:2},
	year = {2025},
	url = {https://doi.org/10.1145/3709651},
	doi = {10.1145/3709651},
	timestamp = {Tue, 08 Apr 2025 20:24:30 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AbouziedHA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Proceedings of the ACM on Management of Data (PACMMOD) is concerned with the principles, algorithms, techniques, systems, and applications of database management systems, data management technology, and science and engineering of data. It includes articles reporting cutting-edge data management, data engineering, and data science research. We are pleased to present the 1st issue of Volume 3 of PACMMOD. This issue contains papers that were submitted to the SIGMOD research track in July 2024. Papers accepted in this issue are part of the pool of presentation candidates in the research track of the ACM SIGMOD Conference on Management of Data 2025, to be held in Berlin, Germany.}
}


@article{DBLP:journals/pacmmod/GiannakourisT25,
	author = {Victor Giannakouris and
                  Immanuel Trummer},
	title = {{\(\lambda\)}-Tune: Harnessing Large Language Models for Automated
                  Database System Tuning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {2:1--2:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709652},
	doi = {10.1145/3709652},
	timestamp = {Tue, 08 Apr 2025 20:24:30 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GiannakourisT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce λ-Tune, a framework that leverages Large Language Models (LLMs) for automated database system tuning. The design of λ-Tune is motivated by the capabilities of the latest generation of LLMs. Different from prior work, leveraging LLMs to extract tuning hints for single parameters, λ-Tune generates entire configuration scripts, based on a large input document, describing the tuning context. λ-Tune generates alternative configurations, using a principled approach to identify the best configuration, out of a small set of candidates. In doing so, it minimizes reconfiguration overheads and ensures that evaluation costs are bounded as a function of the optimal run time. By treating prompt generation as a cost-based optimization problem, λ-Tune conveys the most relevant context to the LLM while bounding the number of input tokens and, therefore, monetary fees for LLM invocations. We compare λ-Tune to various baselines, using multiple benchmarks and PostgreSQL and MySQL as target systems for tuning, showing that λ-Tune is significantly more robust than prior approaches.}
}


@article{DBLP:journals/pacmmod/LaignerZLGZ25,
	author = {Rodrigo Laigner and
                  Zhexiang Zhang and
                  Yijian Liu and
                  Leonardo Freitas Gomes and
                  Yongluan Zhou},
	title = {Online Marketplace: {A} Benchmark for Data Management in Microservices},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {3:1--3:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709653},
	doi = {10.1145/3709653},
	timestamp = {Tue, 08 Apr 2025 20:24:30 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LaignerZLGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Microservice architectures have become a popular approach for designing scalable distributed applications. Despite their extensive use in industrial settings for over a decade, there is limited understanding of the data management challenges that arise in these applications. Consequently, it has been difficult to advance data system technologies that effectively support microservice applications. To fill this gap, we present  Online Marketplace,  a microservice benchmark that highlights core data management challenges that existing benchmarks fail to address. These challenges include transaction processing, query processing, event processing, constraint enforcement, and data replication. We have defined criteria for various data management issues to enable proper comparison across data systems and platforms. Through case studies with state-of-the-art data platforms, we discuss the issues encountered while implementing and meeting  Online Marketplace's  criteria. By capturing the overhead of meeting the key data management requirements that are overlooked by existing benchmarks, we gain actionable insights into the experimental platforms. This highlights the significance of  Online Marketplace  in advancing future data systems to meet the needs of microservice practitioners.}
}


@article{DBLP:journals/pacmmod/ZhangZC25,
	author = {Chenghan Zhang and
                  Yuanyuan Zhu and
                  Lijun Chang},
	title = {A Local Search Approach to Efficient (\emph{k,p})-Core Maintenance},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {4:1--4:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709654},
	doi = {10.1145/3709654},
	timestamp = {Tue, 08 Apr 2025 20:24:30 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The (( k,p ))-core model was recently proposed to capture engagement dynamics by considering both intra-community interactions (i.e., the  k -core structure) and inter-community interactions (i.e., the  p -fraction property). It is a refinement of the classic  k -core, by introducing an extra parameter  p  to customize the engagement within a community at a finer granularity. In this paper, we study the problem of maintaining all (k,p)-cores (essentially, maintaining the p-numbers for all vertices) for dynamic graphs. The existing Global approach conducts a global peeling, almost from scratch, for all vertices whose old p-numbers are within a computed range [p - ,p + ], and thus is inefficient. We propose a new Local approach which conducts local searches starting from the two end-points of the newly inserted or deleted edge, and then iteratively expands the search frontier by including their neighbors. Our algorithm is designed based on several fundamental properties that we prove in this paper to characterize the necessary condition for a vertex's p-number to change. Compared to Global, our Local approach implicitly obtains the optimal affected p-number range [p - * ,p + * ] ⊆ [p - ,p + ], and further skips many vertices whose p-numbers are within this range. Experimental results show that Local is on average two orders of magnitude faster than Global.}
}


@article{DBLP:journals/pacmmod/ScharfDGR25,
	author = {Coral Scharf and
                  Carmel Domshlak and
                  Avigdor Gal and
                  Haggai Roitman},
	title = {A Rank-Based Approach to Recommender System's Top-K Queries with
                  Uncertain Scores},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {5:1--5:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709655},
	doi = {10.1145/3709655},
	timestamp = {Tue, 08 Apr 2025 20:24:30 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ScharfDGR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Top- K  queries provide a ranked answer using a score that can either be given explicitly or computed from tuple values. Recommender systems use scores, based on user feedback on items with which they interact, to answer top- K  queries. Such scores pose the challenge of correctly ranking elements using scores that are more often than not, uncertain. In this work, we address top- K  queries based on uncertain scores. We propose to explicitly model the inherent uncertainty in the provided data and to consider a distribution of scores instead of a single score. Rooted in works of database probabilistic ranking, we offer the use of probabilistic ranking as a tool of choice for generating recommendation in the presence of uncertainty. We argue that the ranking approach should be chosen in a manner that maximizes user satisfaction, extending state-of-the-art on quality aspect of top- K  answers over uncertain data, their relationship to top- K  semantics, and improve ranking with uncertain scores in recommender systems. Towards this end, we introduce RankDist, an algorithm for efficiently computing probability of item position in a ranked recommendation. We show that rank-based (rather than score-based) methods that are computed using RankDist, which were not applied in recommender systems before, offer a guaranteed optimality by expectation and empirical superiority when tested on common benchmarks.}
}


@article{DBLP:journals/pacmmod/ZhangYWLZL25,
	author = {Wenqian Zhang and
                  Zhengyi Yang and
                  Dong Wen and
                  Wentao Li and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {Accelerating Core Decomposition in Billion-Scale Hypergraphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {6:1--6:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709656},
	doi = {10.1145/3709656},
	timestamp = {Tue, 08 Apr 2025 20:24:30 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangYWLZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hypergraphs provide a versatile framework for modeling complex relationships beyond pairwise interactions, finding applications in various domains.  k -core decomposition is a fundamental task in hypergraph analysis that decomposes hypergraphs into cohesive substructures. Existing studies capture the cohesion in hypergraphs based on the vertex neighborhood size. However, such decomposition poses unique challenges, including the efficiency of core value updates, redundant computation, and high memory consumption. We observe that the state-of-the-art algorithms do not fully address the above challenges and are unable to scale to large hypergraphs. In this paper, we propose an efficient approach for hypergraph  k -core decomposition. Novel concepts and strategies are developed to compute the core value of each vertex and reduce redundant computation of vertices. Experimental results on real-world and synthetic hypergraphs demonstrate that our approach significantly outperforms the state-of-the-art algorithm by 7 times on average while reducing the average memory usage by 36 times. Moreover, while existing algorithms fail on tens of millions hyperedges, our approach efficiently handles billion-scale hypergraphs in only a single thread.}
}


@article{DBLP:journals/pacmmod/HofmannVMZDCR25,
	author = {Dennis M. Hofmann and
                  Peter M. VanNostrand and
                  Lei Ma and
                  Huayi Zhang and
                  Joshua DeOliveira and
                  Lei Cao and
                  Elke A. Rundensteiner},
	title = {Agree to Disagree: Robust Anomaly Detection with Noisy Labels},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {7:1--7:24},
	year = {2025},
	url = {https://doi.org/10.1145/3709657},
	doi = {10.1145/3709657},
	timestamp = {Wed, 19 Mar 2025 09:50:52 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HofmannVMZDCR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the scarcity of reliable anomaly labels, recent anomaly detection methods leveraging noisy auto-generated labels either select clean samples or refurbish noisy labels. However, both approaches struggle due to the unique properties of anomalies.  Sample selection  often fails to separate sufficiently many clean anomaly samples from noisy ones, while  label refurbishment  erroneously refurbishes  marginal  clean samples. To overcome these limitations, we design Unity, the  first  learning from noisy labels (LNL) approach for anomaly detection that elegantly leverages the merits of both sample selection and label refurbishment to iteratively prepare a diverse clean sample set for network training. Unity uses a pair of deep anomaly networks to collaboratively select samples with clean labels based on prediction agreement, followed by a disagreement resolution mechanism to capture marginal samples with clean labels. Thereafter, Unity utilizes unique properties of anomalies to design an anomaly-centric contrastive learning strategy that accurately refurbishes the remaining noisy labels. The resulting set, composed of  selected and refurbished  clean samples, will be used to train the anomaly networks in the next training round. Our experimental study on 10 real-world benchmark datasets demonstrates that Unity consistently outperforms state-of-the-art LNL techniques by up to 0.31 in F-1 Score (0.52 \\rightarrow 0.83).}
}


@article{DBLP:journals/pacmmod/PurichWB25,
	author = {Joanna Purich and
                  Anthony Wise and
                  Leilani Battle},
	title = {An Adaptive Benchmark for Modeling User Exploration of Large Datasets},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {8:1--8:24},
	year = {2025},
	url = {https://doi.org/10.1145/3709658},
	doi = {10.1145/3709658},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PurichWB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present a new DBMS performance benchmark that can  simulate  user exploration with any specified dashboard design made of standard visualization and interaction components. The distinguishing feature of our SImulation-BAsed (or SIMBA) benchmark is its ability to  model user analysis goals  as a set of SQL queries to be generated through a valid sequence of user interactions, as well as  measure the completion of analysis goals  by testing for equivalence between the user's previous queries and their goal queries. In this way, the SIMBA benchmark can simulate how an analyst opportunistically searches for interesting insights at the beginning of an exploration session and eventually hones in on specific goals towards the end. To demonstrate the versatility of the SIMBA benchmark, we use it to test the performance of four DBMSs with six different dashboard specifications and compare our results with IDEBench. Our results show how goal-driven simulation can reveal gaps in DBMS performance missed by existing benchmarking methods and across a range of data exploration scenarios.}
}


@article{DBLP:journals/pacmmod/BergmannHHL25,
	author = {Rico Bergmann and
                  Claudio Hartmann and
                  Dirk Habich and
                  Wolfgang Lehner},
	title = {An Elephant Under the Microscope: Analyzing the Interaction of Optimizer
                  Components in PostgreSQL},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {9:1--9:28},
	year = {2025},
	url = {https://doi.org/10.1145/3709659},
	doi = {10.1145/3709659},
	timestamp = {Tue, 01 Apr 2025 19:03:19 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BergmannHHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite an ever-growing corpus of novel query optimization strategies, the interaction of the core components of query optimizers is still not well understood. This situation can be problematic for two main reasons: On the one hand, this may cause surprising results when two components influence each other in an unexpected way. On the other hand, this can lead to wasted effort in regard to both engineering and research, e.g., when an improvement for one component is dwarfed or entirely canceled out by problems of another component. Therefore, we argue that making improvements to a single optimization component requires a thorough understanding of how these changes might affect the other components. To achieve this understanding, we present results of a comprehensive experimental analysis of the interplay in the traditional optimizer architecture using the widely-used PostgreSQL system as prime representative. Our evaluation and analysis revisit the core building blocks of such an optimizer, i.e. per-column statistics, cardinality estimation, cost model, and plan generation. In particular, we analyze how these building blocks influence each other and how they react when faced with faulty input, such as imprecise cardinality estimates. Based on our results, we draw novel conclusions and make recommendations on how these should be taken into account.}
}


@article{DBLP:journals/pacmmod/ChenBH25,
	author = {Qing Chen and
                  Michael H. B{\"{o}}hlen and
                  Sven Helmer},
	title = {An Experimental Comparison of Tree-data Structures for Connectivity
                  Queries on Fully-dynamic Undirected Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {10:1--10:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709660},
	doi = {10.1145/3709660},
	timestamp = {Tue, 01 Apr 2025 19:03:19 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenBH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {During the past decades significant efforts have been made to propose data structures for answering connectivity queries on fully dynamic graphs, i.e., graphs with frequent insertions and deletions of edges. However, a comprehensive understanding of how these data structures perform in practice is missing, since not all of them have been implemented, let alone evaluated experimentally. We provide reference implementations for the proposed data structures and experimentally evaluate them on a wide range of graphs. Our findings show that the current solutions are not ready to be deployed in systems as is, as every data structure has critical weaknesses when used in practice. Key limitations that must be overcome are the space and time overhead incurred by balanced data structures, the degeneration of the runtime of space-efficient data structures in worst case scenarios, and the maintenance costs for balanced data structures. We detail our findings in the experimental evaluation and provide recommendations for implementing robust solutions for answering connectivity queries on dynamic graphs.}
}


@article{DBLP:journals/pacmmod/YuHBZW25,
	author = {Runjie Yu and
                  Weizhou Huang and
                  Shuhan Bai and
                  Jian Zhou and
                  Fei Wu},
	title = {AquaPipe: {A} Quality-Aware Pipeline for Knowledge Retrieval and Large
                  Language Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {11:1--11:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709661},
	doi = {10.1145/3709661},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuHBZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The knowledge retrieval methods such as Approximate Nearest Neighbor Search (ANNS) significantly enhance the generation quality of Large Language Models (LLMs) by introducing external knowledge, and this method is called Retrieval-augmented generation (RAG). However, due to the rapid growth of data size, ANNS tends to store large-scale data on disk, which greatly increases the response time of RAG systems. This paper presents AquaPipe, which pipelines the execution of disk-based ANNS and the LLM prefill phase in an RAG system, effectively overlapping the latency of knowledge retrieval and model inference to enhance the overall performance, while guaranteeing data quality. First, ANNS's recall-aware prefetching strategy enables the early return of partial text with acceptable accuracy so the  prefill  phase can launch before getting the full results. Then, we adaptively choose the remove-after-prefill or re-prefill strategies based on the LLM cost model to effectively correct disturbed pipelines caused by wrong early returns. Finally, the pipelined prefill dynamically changes the granularity of chunk size to balance the overlap efficiency and GPU efficiency, adjusting to ANNS tasks that converge at different speeds. Our experiments have demonstrated the effectiveness of AquaPipe. It successfully masks the latency of disk-based ANNS by 56% to 99%, resulting in a 1.3× to 2.6× reduction of the response time of the RAG, while the extra recall loss caused by prefetching is limited to approximately 1%.}
}


@article{DBLP:journals/pacmmod/MoLWL25,
	author = {Dingheng Mo and
                  Junfeng Liu and
                  Fan Wang and
                  Siqiang Luo},
	title = {Aster: Enhancing LSM-structures for Scalable Graph Database},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {12:1--12:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709662},
	doi = {10.1145/3709662},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MoLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is a proliferation of applications requiring the management of large-scale, evolving graphs under workloads with intensive graph updates and lookups. Driven by this challenge, we introduce  Poly-LSM , a high-performance key-value storage engine for graphs with the following novel techniques: (1)  Poly-LSM  is embedded with a new design of graph-oriented LSM-tree structure that features a hybrid storage model for concisely and effectively storing graph data. (2)  Poly-LSM  utilizes an adaptive mechanism to handle edge insertions and deletions on graphs with optimized I/O efficiency. (3)  Poly-LSM  exploits the skewness of graph data to encode the key-value entries. Building upon this foundation, we further implement  Aster , a robust and versatile graph database that supports Gremlin query language facilitating various graph applications. In our experiments, we compared  Aster  against several mainstream real-world graph databases. The results demonstrate that  Aster  outperforms all baseline graph databases, especially on large-scale graphs. Notably, on the billion-scale Twitter graph dataset,  Aster  achieves up to 17x throughput improvement compared to the best-performing baseline graph system.}
}


@article{DBLP:journals/pacmmod/ChenFWTDWLTLZD25,
	author = {Sibei Chen and
                  Ju Fan and
                  Bin Wu and
                  Nan Tang and
                  Chao Deng and
                  Pengyi Wang and
                  Ye Li and
                  Jian Tan and
                  Feifei Li and
                  Jingren Zhou and
                  Xiaoyong Du},
	title = {Automatic Database Configuration Debugging using Retrieval-Augmented
                  Language Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {13:1--13:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709663},
	doi = {10.1145/3709663},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenFWTDWLTLZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database management system (DBMS) configuration debugging, e.g., diagnosing poorly configured DBMS knobs and generating troubleshooting recommendations, is crucial in optimizing DBMS performance. However, the configuration debugging process is tedious and, sometimes challenging, even for seasoned database administrators (DBAs) with sufficient experience in DBMS configurations and good understandings of the DBMS internals (e.g., MySQL or Oracle). To address this difficulty, we propose Andromeda, a framework that utilizes large language models (LLMs) to enable automatic DBMS configuration debugging. Andromeda serves as a natural surrogate of DBAs to answer a wide range of natural language (NL) questions on DBMS configuration issues, and to generate diagnostic suggestions to fix these issues. Nevertheless, directly prompting LLMs with these professional questions may result in overly generic and often unsatisfying answers. To this end, we propose a retrieval-augmented generation (RAG) strategy that effectively provides matched domain-specific contexts for the question from multiple sources. They come from related historical questions, troubleshooting manuals and DBMS telemetries, which significantly improve the performance of configuration debugging. To support the RAG strategy, we develop a document retrieval mechanism addressing heterogeneous documents and design an effective method for telemetry analysis. Extensive experiments on real-world DBMS configuration debugging datasets show that Andromeda significantly outperforms existing solutions.}
}


@article{DBLP:journals/pacmmod/MullerBL25,
	author = {Marcus M{\"{u}}ller and
                  Lawrence Benson and
                  Viktor Leis},
	title = {B-Trees Are Back: Engineering Fast and Pageable Node Layouts},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {14:1--14:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709664},
	doi = {10.1145/3709664},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MullerBL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large main memory capacity and even larger data sets have motivated hybrid storage systems, which serve most transactions from memory, but can seamlessly transition to flash storage. In such systems, the data structure of choice is usually a B-Tree with pageable nodes. Most academic B-Tree work considers only fixed size records, making them unsuitable for most practical applications. Given the prevalence of B-Trees, surprisingly few available implementations and benchmarks of optimized B-Trees cover variable-sized records. In this paper, we describe an efficient B-Tree implementation supporting variable-sized records containing six known node layout optimizations. We evaluate each optimization to guide future implementations, and propose an optimized adaptive layout that can even compete with pure in-memory structures for many workloads. Our results show that well-engineered B-Trees can efficiently handle both in-memory and out-of-memory workloads.}
}


@article{DBLP:journals/pacmmod/LuZZ25,
	author = {Yujie Lu and
                  Zhijie Zhang and
                  Weiguo Zheng},
	title = {B{\unicode{9416}}\emph{X}: Subgraph Matching with Batch Backtracking
                  Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {15:1--15:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709665},
	doi = {10.1145/3709665},
	timestamp = {Tue, 17 Jun 2025 17:18:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph matching is a fundamental problem in graph analysis. Recently, many algorithms have been developed, often using classic backtracking search. This traditional <u>b</u>acktracking <u>s</u>earch matches <u>one</u> vertex at a time, denoted as BⓈ1, which can lead to redundant computations due to overlapping search spaces. To address this problem, we propose a novel  batch-<u>b</u>acktracking <u>s</u>earch  framework that enables matching a set of data vertices <u> X </u>, denoted as BⓈ X , in each backtracking step. BⓈ X  models the search space as a "search box", allowing for flexible search space exploration and significantly minimizing the overlap between search spaces. It effectively selects batches to cluster data vertices with similar search spaces. For each search box, we introduce a refinement method to filter out unpromising candidate mappings. Furthermore, we propose a homomorphism termination to break the backtracking process as early as possible and an efficient embedding enumeration method to list all embeddings within the search box simultaneously. Extensive experiments on real-world graphs demonstrate that BⓈ X  significantly outperforms existing state-of-the-art algorithms, achieving a speedup of one to two orders of magnitude on most graphs under the EPS metric.}
}


@article{DBLP:journals/pacmmod/YeZLYL25,
	author = {Jianxiong Ye and
                  Zhaonian Zou and
                  Dandan Liu and
                  Bin Yang and
                  Xudong Liu},
	title = {BCviz: {A} Linear-Space Index for Mining and Visualizing Cohesive
                  Bipartite Subgraphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {16:1--16:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709666},
	doi = {10.1145/3709666},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YeZLYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding the maximum biclique in a bipartite graph is a fundamental graph analysis problem. Existing methods for maximum biclique search are not very efficient because they cannot effectively reduce the size of a bipartite graph composed of large bicliques that are loosely linked together because the graph reduction strategies adopted by these methods only consider local densities of vertices. This paper proposes a novel approach to maximum biclique search. The unique feature of this approach is building a linear-space data-driven index called BCviz that helps accurately identify subgraphs containing all bicliques with sizes no less than a certain threshold. The core technique of BCviz is determining a total order of vertices that can reveal both the local density and the connectivity of the vertices. Notably, our work is the first one to take connectivity into account in graph reduction. Interestingly, the total order of vertices entails BCviz an illustrative visualization of the distribution of cohesive subgraphs in the input graph. To deeply understand BCviz, we carry out a theoretical study on its properties and reveal how it enables more effective graph reduction. Based on BCviz, we propose an exact maximum biclique search algorithm that searches for results on much smaller subgraphs than any existing method does. In addition, we improve the efficiency of index construction by two techniques. One is approximating an edge's local density with an upper bound that can be derived in linear time. The other is a lightweight vertex ordering method called one-spot ordering which reduces unnecessary cohesion computations. Extensive experiments indicate that the proposed maximum biclique search methods based on BCviz and its variants outperform the state-of-the-art search-based methods by 2--3 orders of magnitude. Compared with the state-of-the-art index for maximum biclique search, the improved BCviz index can reduce the index size by 1--2 orders of magnitude and the index construction time by up to 2 orders of magnitude.}
}


@article{DBLP:journals/pacmmod/LeeMPL25,
	author = {Bohyun Lee and
                  Seongjae Moon and
                  Jonghyeok Park and
                  Sang{-}Won Lee},
	title = {Boosting {OLTP} Performance with Per-Page Logging on {NVDIMM}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {17:1--17:28},
	year = {2025},
	url = {https://doi.org/10.1145/3709667},
	doi = {10.1145/3709667},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LeeMPL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When running OLTP workloads on flash SSDs, relational DBMSs still face the write durability overhead, severely limiting their performance. To address this challenge, we propose NV-PPL, a novel database architecture that leverages NVDIMM as a  durable log cache. NV-PPL  captures per-page redo logs and retains them on NVDIMM to absorb writes from DRAM to SSD. Our  NV-PPL  prototype, deployed on an actual NVDIMM device, demonstrates superior transaction throughput, surpassing the  same-priced  Vanilla MySQL by at least 6.9× and NV-SQL, a page-grained NVDIMM caching scheme, by up to 1.5×. Beyond write reduction, the page-wise logs in NVDIMM enable novel approaches such as redo-less recovery and redo-based multi-versioning. Compared to Vanilla MySQL, redo-less recovery reduces recovery time by one-third, while redo-based multi-versioning enhances the latency of long-lived transactions in HTAP workloads by 3× to 18×.}
}


@article{DBLP:journals/pacmmod/XuJCXH25,
	author = {Lyu Xu and
                  Jiaxin Jiang and
                  Byron Choi and
                  Jianliang Xu and
                  Bingsheng He},
	title = {Bursting Flow Query on Large Temporal Flow Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {18:1--18:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709737},
	doi = {10.1145/3709737},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuJCXH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, queries that find bursting patterns in temporal graph data have received increasing research attention. In particular, finding the flow in temporal networks whose flow values are bursting in a time interval has numerous applications, such as detecting the money laundering by the maximum average transfer flow in a transaction graph, and the congestion by the maximum average traffic flow in a road network. Despite its usefulness, there is limited research on querying such a flow pattern. In this paper, we study a novel query of finding  a flow pattern of burstiness  in a temporal flow network. In a nutshell, this query aims to find the  bursting flow f  from a source node to a sink node such that the ratio of  f 's flow value to the time interval length of  f  is maximized. To solve this query, we propose the first solution called BFQ that enumerates all the necessary time intervals and then computes the maximum flow value for each interval. Based on BFQ, we propose an efficient solution called BFQ*, which consists of optimization techniques that incrementally compute the maximum flows without computing the common parts of flows from scratch. The experimental results demonstrate the efficiency of our solutions. A case study on a real world transaction network demonstrates the application of this bursting flow query on detecting abnormal transactions.}
}


@article{DBLP:journals/pacmmod/XiangDGWXZ25,
	author = {Yongan Xiang and
                  Zezhong Ding and
                  Rui Guo and
                  Shangyou Wang and
                  Xike Xie and
                  S. Kevin Zhou},
	title = {Capsule: An Out-of-Core Training Mechanism for Colossal GNNs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {19:1--19:30},
	year = {2025},
	url = {https://doi.org/10.1145/3709669},
	doi = {10.1145/3709669},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XiangDGWXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cutting-edge platforms of graph neural networks (GNNs), such as DGL and PyG, harness the parallel processing power of GPUs to extract structural information from graph data, achieving state-of-the-art (SOTA) performance in fields such as recommendation systems, knowledge graphs, and bioinformatics. Despite the computational advantages provided by GPUs, these GNN platforms struggle with scalability challenges due to the colossal graphical structures processed and the limited memory capacities of GPUs. In response, this work introduces Capsule, a new out-of-core mechanism for large-scale GNN training. Unlike existing out-of-core GNN systems, which use main or secondary memory as operative memory and use CPU kernels during non-backpropagation computation, Capsule uses GPU memory and GPU kernels. By substantially leveraging the parallelization capabilities of GPUs, Capsule significantly enhances GNN training efficiency. In addition, Capsule can be smoothly integrated to mainstream open-source GNN frameworks, DGL and PyG, in a play-and-plug manner. Through a prototype implementation and comprehensive experiments on real datasets, we demonstrate that Capsule can achieve up to a 12.02× improvement in runtime efficiency, while using only 22.24% of the main memory, compared to SOTA out-of-core GNN systems.}
}


@article{DBLP:journals/pacmmod/KwonSJ25,
	author = {Suyong Kwon and
                  Kyuseok Shim and
                  Woohwan Jung},
	title = {Cardinality Estimation of {LIKE} Predicate Queries using Deep Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {20:1--20:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709670},
	doi = {10.1145/3709670},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KwonSJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality estimation of LIKE predicate queries has an important role in the query optimization of database systems. Traditional approaches generally use a summary of text data with some statistical assumptions. Recently, the deep learning model for cardinality estimation of LIKE predicate queries has been investigated. To provide more accurate cardinality estimates and reduce the maximum estimation errors, we propose a deep learning model that utilizes the extended  N -gram table and the conditional regression header. We next investigate how to efficiently generate training data. Our LEADER (LikE predicate trAining Data gEneRation) algorithms utilize the shareable results across the relational queries corresponding to the LIKE predicates. By analyzing the queries corresponding to LIKE predicates, we develop an efficient join method and utilize the join order for fast query execution and maximal sharing of  shareable results . Extensive experiments with real-life datasets confirm the efficiency of the proposed training data generation algorithms and the effectiveness of the proposed model.}
}


@article{DBLP:journals/pacmmod/LaiZJLZZWD25,
	author = {Yuanhao Lai and
                  Pengfei Zheng and
                  Chenpeng Ji and
                  Yan Li and
                  Songhan Zhang and
                  Rutao Zhang and
                  Zhengang Wang and
                  Yunfei Du},
	title = {Centrum: Model-based Database Auto-tuning with Minimal Distributional
                  Assumptions},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {21:1--21:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709671},
	doi = {10.1145/3709671},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LaiZJLZZWD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gaussian Process (GP)-based Bayesian optimization (BO), i.e., GP-BO, emerges as a prevailing model-based framework for DBMS (Database Management System) auto-tuning. However, recent work shows GP-BO-based DBMS auto-tuners are significantly outperformed by auto-tuners based on SMAC, which features random forest surrogate models; such results motivate us to rethink and investigate the limitations of GP-BO in auto-tuner design. We find that the fundamental assumptions of GP-BO are widely violated when modeling and optimizing DBMS performance, while tree-ensemble-BOs (e.g., SMAC) can avoid the assumption pitfalls and deliver improved tuning efficiency and effectiveness. Moreover, we argue that existing tree-ensemble-BOs restrict further advancement in DBMS auto-tuning. First, existing tree-ensemble-BOs can only achieve distribution-free point estimates, but still impose unrealistic distributional assumptions on uncertainty (interval) estimates, which can compromise surrogate modeling and distort the acquisition function. Second, recent advances in (ensemble) gradient boosting, which can further enhance surrogate modeling against vanilla GP and random forest counterparts, have rarely been applied in optimizing DBMS auto-tuners. To address these issues, we propose a novel model-based DBMS auto-tuner,  Centrum . Centrum achieves and improves distribution-free point and interval estimation in surrogate modeling with a two-phase learning procedure of stochastic gradient boosting ensembles (SGBE). Moreover, Centrum adopts a generalized SGBE-estimated locally-adaptive conformal prediction to facilitate a distribution-free interval (uncertainty) estimation and acquisition function. To our knowledge, Centrum is the first auto-tuner that realizes distribution-freeness to stress and enhance BO's practicality in DBMS auto-tuning, and the first to seamlessly fuse gradient boosting ensembles and conformal inference in BO. Extensive physical and simulation experiments on two DBMSs and three workloads show that Centrum outperforms 21 state-of-the-art (SOTA) DBMS auto-tuners based on BO with GP, random forest, gradient boosting, OOB (Out-Of-Bag) conformal ensemble and other surrogates, as well as that based on reinforcement learning and genetic algorithms.}
}


@article{DBLP:journals/pacmmod/WangPKXWG25,
	author = {Yuxiang Wang and
                  Zhangyang Peng and
                  Xiangyu Ke and
                  Xiaoliang Xu and
                  Tianxing Wu and
                  Yuan Gao},
	title = {Cohesiveness-aware Hierarchical Compressed Index for Community Search
                  on Attributed Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {22:1--22:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709672},
	doi = {10.1145/3709672},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangPKXWG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community search on attributed graphs (CSAG) is a fundamental topic in graph data mining. Given an attributed graph  G  and a query node  q , CSAG seeks a structural- and attribute-cohesive subgraph from  G  that contains  q . Exact methods based on graph traversal are time-consuming, especially for large graphs. Approximate methods improve efficiency by pruning the search space with heuristics but still take hundreds of milliseconds to tens of seconds to respond, hindering their use in time-sensitive applications. Moreover, pruning strategies are typically tailored to specific algorithms and their cohesiveness metrics, making them difficult to generalize. To address this, we study a general approach to accelerate various CSAG methods. We first present a proximity graph-based, cohesiveness-aware hierarchical index that accommodates different cohesiveness metrics. Then, we present two optimizations to enhance the index's navigability and reliability. Finally, we design a compressed storage structure for space-efficient indexing. Experiments on real-world datasets show that integrating our index with existing mainstream CSAG methods results in an average 30.7× speedup while maintaining a comparable or even better attribute cohesiveness.}
}


@article{DBLP:journals/pacmmod/ChengYWHZ25,
	author = {Qihao Cheng and
                  Da Yan and
                  Tianhao Wu and
                  Zhongyi Huang and
                  Qin Zhang},
	title = {Computing Approximate Graph Edit Distance via Optimal Transport},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {23:1--23:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709673},
	doi = {10.1145/3709673},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChengYWHZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a graph pair (G 1 , G 2 ), graph edit distance (GED) is defined as the minimum number of edit operations converting G 1  to G 2 . GED is a fundamental operation widely used in many applications, but its exact computation is NP-hard, so the approximation of GED has gained a lot of attention. Data-driven learning-based methods have been found to provide superior results compared to classical approximate algorithms, but they directly fit the coupling relationship between a pair of vertices from their vertex features. We argue that while pairwise vertex features can capture the coupling cost (discrepancy) of a pair of vertices, the vertex coupling matrix should be derived from the vertex-pair cost matrix through a more well-established method that is aware of the global context of the graph pair, such as optimal transport. In this paper, we propose an ensemble approach that integrates a supervised learning-based method and an unsupervised method, both based on optimal transport. Our learning method, GEDIOT, is based on inverse optimal transport that leverages a learnable Sinkhorn algorithm to generate the coupling matrix. Our unsupervised method, GEDGW, models GED computation as a linear combination of optimal transport and its variant, Gromov-Wasserstein discrepancy, for node and edge operations, respectively, which can be solved efficiently without needing the ground truth. Our ensemble method, GEDHOT, combines GEDIOT and GEDGW to further boost the performance. Extensive experiments demonstrate that our methods significantly outperform the existing methods in terms of the performance of GED computation, edit path generation, and model generalizability.}
}


@article{DBLP:journals/pacmmod/ZhangR25,
	author = {Chi Zhang and
                  Manuel Rigger},
	title = {Constant Optimization Driven Database System Testing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {24:1--24:24},
	year = {2025},
	url = {https://doi.org/10.1145/3709674},
	doi = {10.1145/3709674},
	timestamp = {Tue, 01 Apr 2025 19:03:20 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Logic bugs are bugs that can cause database management systems (DBMSs) to silently produce incorrect results for given queries. Such bugs are severe, because they can easily be overlooked by both developers and users, and can cause applications that rely on the DBMSs to malfunction. In this work, we propose  Constant-Optimization-Driven Database Testing  (CODDTest) as a novel approach for detecting logic bugs in DBMSs. This method draws inspiration from two well-known optimizations in compilers: constant folding and constant propagation. Our key insight is that for a certain database state and query containing a predicate, we can apply constant folding on the predicate by replacing an expression in the predicate with a constant, anticipating that the results of this predicate remain unchanged; any discrepancy indicates a bug in the DBMS. We evaluated CODDTest on five mature and extensively-tested DBMSs--SQLite, MySQL, CockroachDB, DuckDB, and TiDB--and found 45 unique, previously unknown bugs in them. Out of these, 24 are unique logic bugs. Our manual analysis of the state-of-the-art approaches indicates that 11 logic bugs are detectable only by CODDTest. We believe that CODDTest is easy to implement, and can be widely adopted in practice.}
}


@article{DBLP:journals/pacmmod/FariaP25,
	author = {Nuno Faria and
                  Jos{\'{e}} Pereira},
	title = {{CRDV:} Conflict-free Replicated Data Views},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {25:1--25:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709675},
	doi = {10.1145/3709675},
	timestamp = {Tue, 01 Apr 2025 19:03:19 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FariaP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There are now multiple proposals for Conflict-free Replicated Data Types (CRDTs) in SQL databases aimed at distributed systems. Some, such as ElectricSQL, provide only relational tables as convergent replicated maps, but this omits semantics that would be useful for merging updates. Others, such as Pg\\_crdt, provide access to a rich library of encapsulated column types. However, this puts merge and query processing outside the scope of the query optimizer and restricts the ability of an administrator to influence access paths with materialization and indexes. Our proposal, CRDV, overcomes this challenge by using two layers implemented as SQL views: The first provides a replicated relational table from an update history, while the second implements varied and rich types on top of the replicated table. This allows the definition of merge semantics, or even entire new data types, in SQL itself, and enables global optimization of user queries together with merge operations. Therefore, it naturally extends the scope of query optimization and local transactions to operations on replicated data, can be used to reproduce the functionality of common CRDTs with simple SQL idioms, and results in better performance than alternatives.}
}


@article{DBLP:journals/pacmmod/QiaoZ25,
	author = {Yiming Qiao and
                  Huanchen Zhang},
	title = {Data Chunk Compaction in Vectorized Execution},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {26:1--26:25},
	year = {2025},
	url = {https://doi.org/10.1145/3709676},
	doi = {10.1145/3709676},
	timestamp = {Tue, 01 Apr 2025 19:03:20 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/QiaoZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern analytical database management systems often adopt vectorized query execution engines that process columnar data in batches (i.e., data chunks) to minimize the interpretation overhead and improve CPU parallelism. However, certain database operators, especially hash joins, can drastically reduce the number of valid entries in a data chunk, resulting in numerous small chunks in an execution pipeline. These small chunks cannot fully enjoy the benefits of vectorized query execution, causing significant performance degradation. The key research question is when and how to compact these small data chunks during query execution. In this paper, we first model the chunk compaction problem and analyze the trade-offs between different compaction strategies. We then propose a learning-based algorithm that can adjust the compaction threshold dynamically at run time. To answer the ''how'' question, we propose a compaction method for the hash join operator, called logical compaction, that minimizes data movements when compacting data chunks. We implemented the proposed techniques in the state-of-the-art DuckDB and observed up to 63% speedup when evaluated using the Join Order Benchmark, TPC-H, and TPC-DS.}
}


@article{DBLP:journals/pacmmod/SinghCGLNRV25,
	author = {Mukul Singh and
                  Jos{\'{e}} Cambronero and
                  Sumit Gulwani and
                  Vu Le and
                  Carina Negreanu and
                  Arjun Radhakrishna and
                  Gust Verbruggen},
	title = {DataVinci: Learning Syntactic and Semantic String Repairs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {27:1--27:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709677},
	doi = {10.1145/3709677},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SinghCGLNRV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {String data is common in real-world datasets: 67.6% of values in a sample of 1.8 million real Excel spreadsheets from the web were represented as text. Automatically cleaning such string data can have a significant impact on users. Previous approaches are limited to error detection, require that the user provides annotations, examples, or constraints to fix the errors, and focus independently on syntactic errors or semantic errors in strings, but ignore that strings often contain both syntactic and semantic substrings. We introduce  DataVinci,  a fully unsupervised string data error detection and repair system.  DataVinci  learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such majority patterns as data errors.  DataVinci  can automatically derive edits to the data error based on the majority patterns and using row tuples associated with majority values as examples. To handle strings with both syntactic and semantic substrings,  DataVinci  uses an LLM to abstract (and re-concretize) portions of strings that are semantic. Because not all data columns can result in majority patterns, when available,  DataVinci  can leverage execution information from an existing data program (which uses the target data as input) to identify and correct data repairs that would not otherwise be identified.  DataVinci  outperforms eleven baseline systems on both data error detection and repair as demonstrated on four existing and new benchmarks.}
}


@article{DBLP:journals/pacmmod/SimaYWZZL25,
	author = {Qing Sima and
                  Jianke Yu and
                  Xiaoyang Wang and
                  Wenjie Zhang and
                  Ying Zhang and
                  Xuemin Lin},
	title = {Deep Overlapping Community Search via Subspace Embedding},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {28:1--28:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709678},
	doi = {10.1145/3709678},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SimaYWZZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Overlapping Community Search (OCS) identifies nodes that interact with multiple communities based on a specified query. Existing community search approaches fall into two categories: algorithm-based models and Machine Learning-based (ML) models. Despite the long-standing focus on this topic within the database domain, current solutions face two major limitations: 1) Both approaches fail to address personalized user requirements in OCS, consistently returning the same set of nodes for a given query regardless of user differences. 2) Existing ML-based CS models suffer from severe training efficiency issues. In this paper, we formally redefine the problem of OCS. By analyzing the gaps in both types of approaches, we then propose a general solution for OCS named  <u>S</u> parse  <u>S</u> ubspace  <u>F</u> ilter (SSF), which can extend any ML-based CS model to enable personalized search in overlapping structures. To overcome the efficiency issue in the current models, we introduce  <u>S</u> implified  <u>M</u> ulti-hop Attention  <u>N</u> etworks (SMN), a lightweight yet effective community search model with larger receptive fields. To the best of our knowledge, this is the first ML-based study of overlapping community search. Extensive experiments validate the superior performance of SMN within the SSF pipeline, achieving a 13.73% improvement in F1-Score and up to 3 orders of magnitude acceleration in model efficiency compared to state-of-the-art approaches.}
}


@article{DBLP:journals/pacmmod/YinGBCL25,
	author = {Ziqi Yin and
                  Jianyang Gao and
                  Pasquale Balsebre and
                  Gao Cong and
                  Cheng Long},
	title = {{DEG:} Efficient Hybrid Vector Search Using the Dynamic Edge Navigation
                  Graph},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {29:1--29:28},
	year = {2025},
	url = {https://doi.org/10.1145/3709679},
	doi = {10.1145/3709679},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YinGBCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bimodal data, such as image-text pairs, has become increasingly prevalent in the digital era. The Hybrid Vector Query (HVQ) is an effective approach for querying such data and has recently garnered considerable attention from researchers. It calculates similarity scores for objects represented by two vectors using a weighted sum of each individual vector's similarity, with a query-specific parameter α to determine the weight. Existing methods for HVQ typically construct Approximate Nearest Neighbors Search (ANNS) indexes with a fixed α value. This leads to significant performance degradation when the query's α dynamically changes based on the different scenarios and needs. In this study, we introduce the Dynamic Edge Navigation Graph ( DEG ), a graph-based ANNS index that maintains efficiency and accuracy with changing α values. It includes three novel components: (1) a greedy Pareto frontier search algorithm to compute a candidate neighbor set for each node, which comprises the node's approximate nearest neighbors for all possible α values; (2) a dynamic edge pruning strategy to determine the final edges from the candidate set and assign each edge an active range. This active range enables the dynamic use of the Relative Neighborhood Graph's pruning strategy based on the query's α values, skipping redundant edges at query time and achieving a better accuracy-efficiency trade-off; and (3) an edge seed method that accelerates the querying process. Extensive experiments on real-world datasets show that DEG demonstrates superior performance compared to existing methods under varying α values.}
}


@article{DBLP:journals/pacmmod/ZhangLZQQW25,
	author = {Yalong Zhang and
                  Rong{-}Hua Li and
                  Qi Zhang and
                  Hongchao Qin and
                  Lu Qin and
                  Guoren Wang},
	title = {Density Decomposition of Bipartite Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {30:1--30:25},
	year = {2025},
	url = {https://doi.org/10.1145/3709680},
	doi = {10.1145/3709680},
	timestamp = {Thu, 17 Apr 2025 20:21:24 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangLZQQW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mining dense subgraphs in a bipartite graph is a fundamental task in bipartite graph analysis, with numerous applications in community detection, fraud detection, and e-commerce recommendation. Existing dense subgraph models, such as biclique,  k -biplex,  k -bitruss, and (α,β)-core, often face challenges due to their high computational complexity or limitations in effectively capturing the density of the graph. To overcome these issues, in this paper, we propose a new dense subgraph model for bipartite graphs, namely (α,β)-dense subgraph, designed to capture the density structure inherent in bipartite graphs. We show that all (α,β)-dense subgraphs are nested within each other, forming a hierarchical density decomposition of the bipartite graph. To efficiently compute the (α,β)-dense subgraph, we develop a novel network flow algorithm with a carefully-designed core pruning technique. The time complexity of our algorithm is O(|E|+|E(R)| 1.5 ), where |E| denotes the number of edges and |E(R)| is the number of edges of the pruned graph, often significantly smaller than |E|. Armed with this algorithm, we also propose a novel and efficient divide-and-conquer algorithm to compute the entire density decomposition of the bipartite graph within O(p ⋅ log d max  ⋅ |E| 1.5  ) time, where  p  is typically a small constant in real-world bipartite graphs and d max  is the maximum degree. Extensive experiments and case studies on 11 real-world datasets demonstrate the effectiveness of our (α,β)-dense subgraph model and the high efficiency and scalability of our proposed algorithms.}
}


@article{DBLP:journals/pacmmod/OmarMM25,
	author = {Reham Omar and
                  Omij Mangukiya and
                  Essam Mansour},
	title = {Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective
                  Retrieval-Augmented LLMs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {31:1--31:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709681},
	doi = {10.1145/3709681},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/OmarMM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dialogue benchmarks are crucial in training and evaluating chatbots engaging in domain-specific conversations. Knowledge graphs (KGs) represent semantically rich and well-organized data spanning various domains, such as DBLP, DBpedia, and YAGO. Traditionally, dialogue benchmarks have been manually created from documents, neglecting the potential of KGs in automating this process. Some question-answering benchmarks are automatically generated using extensive preprocessing from KGs, but they do not support dialogue generation. This paper introduces Chatty-Gen, a novel multi-stage retrieval-augmented generation platform for automatically generating high-quality dialogue benchmarks tailored to a specific domain using a KG. Chatty-Gen decomposes the generation process into manageable stages and uses assertion rules for automatic validation between stages. Our approach enables control over intermediate results to prevent time-consuming restarts due to hallucinations. It also reduces reliance on costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront processing of the entire KG using efficient query-based retrieval to find representative subgraphs based on the dialogue context. Our experiments with several real and large KGs demonstrate that C hatty -G en  significantly outperforms state-of-the-art systems and ensures consistent model and system performance across multiple LLMs of diverse capabilities, such as GPT-4o, Gemini 1.5, Llama 3, and Mistral.}
}


@article{DBLP:journals/pacmmod/SattlerKLSSW25,
	author = {Rebecca Sattler and
                  Sarah Kleest{-}Mei{\ss}ner and
                  Steven Lange and
                  Markus L. Schmid and
                  Nicole Schweikardt and
                  Matthias Weidlich},
	title = {{DISCES:} Systematic Discovery of Event Stream Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {32:1--32:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709682},
	doi = {10.1145/3709682},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SattlerKLSSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The continuous evaluation of queries over an event stream provides the foundation for reactive applications in various domains. Yet, knowledge of queries that detect distinguished event patterns that are potential causes of the situation of interest is often not directly available. However, given a database of finite, historic (sub-)streams that have been gathered whenever a situation of interest was observed, one may aim at automatic discovery of the respective queries. Existing algorithms for event query discovery incorporate ad-hoc design choices, though, and it is unclear how their suitability for a database shall be assessed. In this paper, we address this gap with DISCES, an algorithmic framework for event query discovery. DISCES outlines a design space for discovery algorithms, thereby making the design choices explicit. We instantiate the framework to derive four specific algorithms, which all yield correct and complete results, but differ in their runtime sensitivity. We therefore also provide guidance on how to select one of the algorithms for a given database based on a few of its essential properties. Our experiments using simulated and real-world data illustrate that our algorithms are indeed tailored to databases showing certain properties and solve the query discovery problem several orders of magnitude faster than existing approaches.}
}


@article{DBLP:journals/pacmmod/ZhongCWE25,
	author = {Wenshao Zhong and
                  Chen Chen and
                  Xingbo Wu and
                  Jakob Eriksson},
	title = {Disco: {A} Compact Index for LSM-trees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {33:1--33:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709683},
	doi = {10.1145/3709683},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhongCWE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many key-value stores and database systems use log-structured merge-trees (LSM-trees) as their storage engines because of their excellent write performance. However, the read performance of LSM-trees is suboptimal due to the overlapping sorted runs. Most existing efforts rely on filters to reduce unnecessary I/Os, but filters fundamentally do not help locate items and often become the bottleneck of the system. We identify that the lack of efficient index is the root cause of subpar read performance in LSM-trees. In this paper, we propose Disco: a compact index for LSM-trees. Disco indexes all the keys in an LSM-tree, so a query does not have to search every run of the LSM-tree. It records compact key representations to minimize the number of key comparisons so as to minimize cache misses and I/Os for both point and range queries. Disco guarantees that both point queries and seeks issue at most one I/O to the underlying runs, achieving an I/O efficiency close to a B + -tree. Disco improves upon REMIX's pioneering multi-run index design with additional compact key representations to help improve read performance. The representations are compact so the cost of persisting Disco to disk is small. Moreover, while a traditional LSM-tree has to choose a more aggressive compaction policy that slows down write performance to have better read performance, a Disco-indexed LSM-tree can employ a write-efficient policy and still have good read performance. Experimental results show that Disco can save I/Os and improve point and range query performance by up to 220% over RocksDB while maintaining efficient writes.}
}


@article{DBLP:journals/pacmmod/LiuWYJCWTL25,
	author = {Renjie Liu and
                  Yichuan Wang and
                  Xiao Yan and
                  Haitian Jiang and
                  Zhenkun Cai and
                  Minjie Wang and
                  Bo Tang and
                  Jinyang Li},
	title = {DiskGNN: Bridging {I/O} Efficiency and Model Accuracy for Out-of-Core
                  {GNN} Training},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {34:1--34:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709738},
	doi = {10.1145/3709738},
	timestamp = {Wed, 14 May 2025 17:23:41 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuWYJCWTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) are models specialized for graph data and widely used in applications. To train GNNs on large graphs that exceed CPU memory, several systems have been designed to store data on disk and conduct out-of-core processing. However, these systems suffer from either  read amplification  when conducting random reads for node features that are smaller than a disk page, or  degraded model accuracy  by treating the graph as disconnected partitions. To close this gap, we build  DiskGNN  for high I/O efficiency and fast training without model accuracy degradation. The key technique is  offline sampling , which decouples  graph sampling  from  model computation . In particular, by conducting graph sampling  beforehand  for multiple mini-batches, DiskGNN acquires the node features that will be accessed during model computation and conducts pre-processing to pack the node features of each mini-batch contiguously on disk to avoid read amplification for computation. Given the feature access information acquired by offline sampling, DiskGNN also adopts designs including  four-level feature store  to fully utilize the memory hierarchy of GPU and CPU to cache hot node features and reduce disk access,  batched packing  to accelerate feature packing during pre-processing, and  pipelined training  to overlap disk access with other operations. We compare DiskGNN with state-of-the-art out-of-core GNN training systems. The results show that DiskGNN has more than 8x speedup over existing systems while matching their best model accuracy. DiskGNN is open-source at https://github.com/Liu-rj/DiskGNN.}
}


@article{DBLP:journals/pacmmod/FarhanKW25,
	author = {Muhammad Farhan and
                  Henning Koehler and
                  Qing Wang},
	title = {Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road
                  Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {35:1--35:25},
	year = {2025},
	url = {https://doi.org/10.1145/3709685},
	doi = {10.1145/3709685},
	timestamp = {Tue, 01 Apr 2025 19:03:19 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FarhanKW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computing the shortest-path distance between any two given vertices in road networks is an important problem. A tremendous amount of research has been conducted to address this problem, most of which are limited to static road networks. Since road networks undergo various real-time traffic conditions, there is a pressing need to address this problem for dynamic road networks. Existing state-of-the-art methods incrementally maintain an indexing structure to reflect dynamic changes on road networks. However, these methods suffer from either slow query response time or poor maintenance performance, particularly when road networks are large. In this work, we propose an efficient solution  Dual-Hierarchy Labelling (DHL)  for distance querying on dynamic road networks from a novel perspective, which incorporates two hierarchies with different but complementary data structures to support efficient query and update processing. Specifically, our proposed solution is comprised of three main components:  query hierarchy, update hierarchy,  and  hierarchical labelling,  where  query hierarchy  enables efficient query answering by exploring only a small subset of vertices in the labels of two query vertices and  update hierarchy  supports efficient maintenance of distance labelling under edge weight increase or decrease. We further develop dynamic algorithms to reflect dynamic changes by efficiently maintaining the update hierarchy and hierarchical labelling. We also propose a parallel variant of our dynamic algorithms by exploiting labelling structure which aligns well with parallel processing. We evaluate our methods on 10 large road networks and it shows that our methods significantly outperform the state-of-the-art methods, i.e., achieving considerably faster construction and update time, while being consistently 2-4 times faster in terms of query processing and consuming only 10%-20% labelling space.}
}


@article{DBLP:journals/pacmmod/LiaoLLW25,
	author = {Meihao Liao and
                  Cheng Li and
                  Rong{-}Hua Li and
                  Guoren Wang},
	title = {Efficient Index Maintenance for Effective Resistance Computation on
                  Evolving Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {36:1--36:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709686},
	doi = {10.1145/3709686},
	timestamp = {Wed, 16 Apr 2025 09:56:23 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiaoLLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study a problem of index maintenance on evolving graphs for effective resistance computation. Unlike an existing matrices-based index, we show that the index can be efficiently maintained by directly preserving samples of random walks and loop-erased walks. This approach not only enables efficient storage and rapid query response but also supports effective maintenance. We propose a novel approach to convert edge updates into  landmark  node updates. Building upon this, we present two new update algorithms for random walk and loop-erased walk samples respectively. Both algorithms update samples without requiring complete resampling, ensuring accuracy and high efficiency. A particularly challenging and innovative technique involves updating loop-erased walks. Here we develop a novel and powerful cycle decomposition technique for loop-erased walks, enabling us to update samples at the cycle level rather than the node level, significantly enhancing efficiency. Furthermore, we show that both of our methods achieve an Õ (1) time complexity per edge update in real-world graphs under a mild assumption. We conduct extensive experiments using 10 large real-world datasets to evaluate the performance of our approaches. The results show that our best algorithm can be up to two orders of magnitude faster than the baseline methods.}
}


@article{DBLP:journals/pacmmod/LiuHYLL25,
	author = {Yang Liu and
                  Hejiao Huang and
                  Kaiqiang Yu and
                  Shengxin Liu and
                  Cheng Long},
	title = {Efficient Maximum \emph{s}-Bundle Search via Local Vertex Connectivity},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {37:1--37:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709687},
	doi = {10.1145/3709687},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuHYLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The  s -bundle, as a cohesive subgraph model which relaxes the clique, remains connected whenever fewer than  n-s  vertices are removed, where  n  is the number of vertices inside. Finding the largest  s -bundle is a fundamental problem and has diverse applications in various fields such as social network analysis, graph visualization, and bioinformatics. Existing studies for solving the problem follow the same branch-and-bound framework and improve the efficiency by developing pruning techniques. As a result, all share the same worst-case time complexity of  O* (2 n ), where  O*  suppresses the polynomial factors. In this paper, we propose a new branch-and-bound algorithm, called SymBD, which achieves improved theoretical guarantees and practical performance. It adopts the existing Symmetric-BK branching strategy whose performance highly depends on the ordering of vertices. We explore various vertex orderings for improving the performance. In particular, we propose two novel vertex orderings based on the local vertex connectivity. With the proposed vertex orderings, SymBD improves the worst-case time complexity to  O* (λ n s ) where λ s  is strictly less than 2. To further boost the practical efficiency, we introduce a heuristic algorithm for computing a large initial solution and a divide-and-conquer strategy. Extensive experiments on 664 graphs demonstrate that our algorithm is up to five orders of magnitude faster than existing solutions.}
}


@article{DBLP:journals/pacmmod/XiaFL25,
	author = {Yuyang Xia and
                  Yixiang Fang and
                  Wensheng Luo},
	title = {Efficiently Counting Triangles in Large Temporal Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {38:1--38:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709688},
	doi = {10.1145/3709688},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XiaFL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many real-world applications (e.g., email networks, social networks, and phone call networks), the relationships between entities can be modeled as a temporal graph, in which each edge is associated with a timestamp representing the interaction time. As a fundamental task in temporal graph analysis, triangle counting has received much attention, and several triangle models have been developed, including δ-temporal triangle, sliding-window triangle, and (δ 1,3 , δ 1,2 , δ 2,3 )-temporal triangle. In particular, the δ-temporal triangle, requiring the gap of timestamps of any two edges within it to be bounded by a threshold δ, has been demonstrated effective in many real applications, such as cohesiveness analysis, transitivity, clustering coefficient, and graph classification. In this paper, we study fast algorithms for counting δ-temporal triangles in a given query time window. We first propose an online algorithm, which enumerates all edges in the graph and for each edge, calculates how many δ-temporal triangles end with the edge. We further develop an efficient index-based solution, which maps δ-temporal triangles into points of the 2-dimensional space and further compactly organizes these points using hierarchical structures. Besides, we study the problem of binary δ-temporal triangle counting by considering the existence of δ-temporal triangle among three vertices. Experiments on large temporal graphs show that our online algorithm is up to 70× faster than the state-of-the-art algorithm, and our index-based algorithm is up to 10 8 × faster than the online algorithm.}
}


@article{DBLP:journals/pacmmod/WuKA25,
	author = {Bowen Wu and
                  Dimitrios Koutsoukos and
                  Gustavo Alonso},
	title = {Efficiently Processing Joins and Grouped Aggregations on GPUs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {39:1--39:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709689},
	doi = {10.1145/3709689},
	timestamp = {Wed, 07 May 2025 17:16:10 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuKA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is a growing interest in leveraging GPUs for tasks beyond ML, especially in database systems. Despite the existing extensive work on GPU-based database operators, several questions are still open. For instance, the performance of almost all operators suffers from random accesses, which can account for up to 75% of the runtime. In addition, the group-by operator which is widely used in combination with joins, has not been fully explored for GPU acceleration. Furthermore, existing work often uses limited and unrepresentative workloads for evaluation and does not explore the query optimization aspect, i.e., how to choose the most efficient implementation based on the workload. In this paper, we revisit the state-of-the-art GPU-based join and group-by implementations. We identify their inefficiencies and propose several optimizations. We introduce GFTR, a novel technique to reduce random accesses, leading to speedups of up to 2.3x. We further optimize existing hash-based and sort-based group-by implementations, achieving significant speedups (19.4x and 1.7x, respectively). We also present a new partition-based group-by algorithm ideal for high group cardinalities. We analyze the optimizations with cost models, allowing us to predict the speedup. Finally, we conduct a performance evaluation to analyze each implementation. We conclude by providing practical heuristics to guide query optimizers in selecting the most efficient implementation for a given workload.}
}


@article{DBLP:journals/pacmmod/SkavantzosL25,
	author = {Philipp Skavantzos and
                  Sebastian Link},
	title = {Entity/Relationship Graphs: Principled Design, Modeling, and Data
                  Integrity Management of Graph Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {40:1--40:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709690},
	doi = {10.1145/3709690},
	timestamp = {Tue, 01 Apr 2025 19:03:20 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SkavantzosL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Chen's Entity/Relationship (E/R) framework is a lingua franca for well-designed databases. We define E/R graphs as property graphs that are instances of E/R diagrams. As the latter are a subclass of PG-Schema, E/R modeling constitutes a methodology for designing graph databases that guarantee data integrity, the absence of data redundancy and update anomalies. In addition, E/R graphs provide the first graph semantics for E/R diagrams. Further to the unification of conceptual and graph data modeling, referential integrity for E/R graphs can be managed by directed edges, called E/R links, between nodes. As a consequence, redundancy and sources of potential inconsistency can be eliminated, minimizing update maintenance. This is achieved by E/R keys that use properties and E/R links to enforce entity integrity, in contrast to property keys that rely exclusively on properties to enforce entity integrity. We use the TPC-H benchmark as running example and for extensive experiments that quantify the effort for i) managing entity integrity using property keys or E/R keys, ii) managing referential integrity using property redundancy or E/R links, iii) query evaluation. In summary, E/R diagrams form a principled core of PG-Schema for well-designed property graphs, while E/R keys constitute an efficient core of PG-Key for data integrity management.}
}


@article{DBLP:journals/pacmmod/HuberKRZFG25,
	author = {Aaron Huber and
                  Oliver Kennedy and
                  Atri Rudra and
                  Zhuoyue Zhao and
                  Su Feng and
                  Boris Glavic},
	title = {FastPDB: Towards Bag-Probabilistic Queries at Interactive Speeds},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {41:1--41:25},
	year = {2025},
	url = {https://doi.org/10.1145/3709691},
	doi = {10.1145/3709691},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuberKRZFG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Probabilistic databases (PDBs) provide users with a principled way to query data that is incomplete or imprecise. In this work, we study computing expected multiplicities of query results over probabilistic databases under bag semantics which has PTIME data complexity. However, does this imply that bag probabilistic databases are practical? We strive to answer this question from both a theoretical as well as a systems perspective. We employ concepts from fine-grained complexity to demonstrate that exact bag probabilistic query processing is fundamentally less efficient than deterministic bag query evaluation, but that fast approximations are possible by sampling monomials from a circuit representation of a result tuple's lineage. A remaining issue, however, is that constructing such circuits, while in PTIME, can nonetheless have significant overhead. To avoid this cost, we utilize approximate query processing techniques to directly sample monomials without materializing lineage upfront. Our implementation in  FastPDB  provides accurate anytime approximation of probabilistic query answers and scales to datasets orders of magnitude larger than competing methods.}
}


@article{DBLP:journals/pacmmod/ZhangYH25,
	author = {Yuemin Zhang and
                  Qingqing Ye and
                  Haibo Hu},
	title = {Federated Heavy Hitter Analytics with Local Differential Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {42:1--42:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709739},
	doi = {10.1145/3709739},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangYH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated heavy hitter analytics enables service providers to better understand the preferences of cross-party users by analyzing the most frequent items. As with federated learning, it faces challenges of privacy concerns, statistical heterogeneity, and expensive communication. Local differential privacy (LDP), as the de facto standard for privacy-preserving data collection, solves the privacy challenge by letting each user perturb her data locally and report the sanitized version. However, in federated settings, applying LDP complicates the other two challenges, due to the deteriorated utility by the injected LDP noise or increasing communication/computation costs by perturbation mechanism. To tackle these problems, we propose a novel target-aligning prefix tree mechanism satisfying ε-LDP, for federated heavy hitter analytics. In particular, we propose an adaptive extension strategy to address the inconsistencies between covering necessary prefixes and estimating heavy hitters within a party to enhance the utility. We also present a consensus-based pruning strategy that utilizes noisy prior knowledge from other parties to further align the inconsistency between finding heavy hitters in each party and providing reasonable frequency information to identify the global ones. To the best of our knowledge, our study is the first solution to the federated heavy hitter analytics in a cross-party setting while satisfying the stringent ε-LDP. Comprehensive experiments on both real-world and synthetic datasets confirm the effectiveness of our proposed mechanism.}
}


@article{DBLP:journals/pacmmod/AziziEP25,
	author = {Ilias Azizi and
                  Karima Echihabi and
                  Themis Palpanas},
	title = {Graph-Based Vector Search: An Experimental Evaluation of the State-of-the-Art},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {43:1--43:31},
	year = {2025},
	url = {https://doi.org/10.1145/3709693},
	doi = {10.1145/3709693},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AziziEP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vector data is prevalent across business and scientific applications, and its popularity is growing with the proliferation of learned embeddings. Vector data collections often reach billions of vectors with thousands of dimensions, thus, increasing the complexity of their analysis. Vector search is the backbone of many critical analytical tasks, and graph-based methods have become the best choice for analytical tasks that do not require guarantees on the quality of the answers. We briefly survey in-memory graph-based vector search, outline the chronology of the different methods and classify them according to five main design paradigms: seed selection, incremental insertion, neighborhood propagation, neighborhood diversification, and divide-and-conquer. We conduct an exhaustive experimental evaluation of twelve state-of-the-art methods on seven real data collections, with sizes up to 1 billion vectors. We share key insights about the strengths and limitations of these methods; e.g., the best approaches are typically based on incremental insertion and neighborhood diversification, and the choice of the base graph can hurt scalability. Finally, we discuss open research directions, such as the importance of devising more sophisticated data-adaptive seed selection and diversification strategies.}
}


@article{DBLP:journals/pacmmod/PandeyB25,
	author = {Shweta Pandey and
                  Arkaprava Basu},
	title = {H-Rocks: {CPU-GPU} accelerated Heterogeneous RocksDB on Persistent
                  Memory},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {44:1--44:28},
	year = {2025},
	url = {https://doi.org/10.1145/3709694},
	doi = {10.1145/3709694},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PandeyB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Persistent key-value stores (pKVS) such as RocksDB are critical to many internet-scale services. Recent works leveraged persistent memory (PM) to improve pKVS throughput. However, they are typically limited to CPUs. We develop H-Rocks to judiciously leverage both the CPU and the Graphics Processing Unit (GPU) for accelerating a wide range of RocksDB operations. H-Rocks selectively accelerates performance-critical parts of RocksDB on the GPU. It uses operation sub-batching and key-value versioning to leverage GPU's parallelism while maintaining compatibility with RocksDB. It harnesses GPU's high-bandwidth memory while limiting data movement between the CPU and GPU. In YCSB workloads, H-Rocks outperforms CPU-based pKVSs like Viper, Plush, and pmem-RocksDB by 3-18×.}
}


@article{DBLP:journals/pacmmod/WuCCJLS25,
	author = {Yifan Wu and
                  Ke Chen and
                  Gang Chen and
                  Dawei Jiang and
                  Huan Li and
                  Lidan Shou},
	title = {HyperMR: Efficient Hypergraph-enhanced Matrix Storage on Compute-in-Memory
                  Architecture},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {45:1--45:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709695},
	doi = {10.1145/3709695},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuCCJLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Matrix-vector multiplication (MVM) operations, essential for modern hardware architectures, suffer from heavy I/O overheads and costly serial multiply-add operations. The emerging Compute-in-Memory (CIM) architecture alleviates these issues by enabling in situ MVM operations with O(1) time complexity, eliminating the need to move matrices. However, current storage schemes are still inefficient on CIM due to limited optimization objectives and inflexible support for various access patterns and matrix structures. To address this, we propose HyperMR, a hypergraph-enhanced matrix storage scheme for CIM architectures. First, we identify two performance optimization objectives that are tailored to CIM and prove their NP-hardness. We then introduce a hypergraph modeling approach with a novel access-aware hypergraph generation algorithm to handle diverse matrix structures and access patterns. Moreover, we present a two-phase hypergraph partitioning method to efficiently tackle the NP-hard optimization objectives. Experimental results show that HyperMR outperforms multiple state-of-the-art storage schemes, offering valid optimization for all evaluated matrices, compared to the best-performing baseline which optimizes only 75%. HyperMR also achieves the best average optimization performance for matrix storage layouts, significantly improving efficiency in varied workload scenarios, with a 29.65% improvement on synthetic queries and up to 34.9% on scientific image filtering.}
}


@article{DBLP:journals/pacmmod/SuLS25,
	author = {Yunxiang Su and
                  Kenny Ye Liang and
                  Shaoxu Song},
	title = {In-Database Time Series Clustering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {46:1--46:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709696},
	doi = {10.1145/3709696},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SuLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series data are often clustered repeatedly across various time ranges to mine frequent subsequence patterns from different periods, which could further support downstream applications. Existing state-of-the-art (SOTA) time series clustering method, such as K-Shape, can proficiently cluster time series data referring to their shapes. However, in-database time series clustering problem has been neglected, especially in IoT scenarios with large-volume data and high efficiency demands. Most time series databases employ LSM-Tree based storage to support intensive writings, yet causing underlying data points out-of-order in timestamps. Therefore, to apply existing out-of-database methods, all data points must be fully loaded into memory and chronologically sorted. Additionally, out-of-database methods must cluster from scratch each time, making them inefficient when handling queries across different time ranges. In this work, we propose an in-database adaptation of SOTA time series clustering method K-Shape. Moreover, to solve the problem that K-Shape cannot efficiently handle long time series, we propose Medoid-Shape, as well as its in-database adaptation for further acceleration. Extensive experiments are conducted to demonstrate the higher efficiency of our proposals, with comparable effectiveness. Remarkably, all proposals have already been implemented in an open-source commodity time series database, Apache IoTDB.}
}


@article{DBLP:journals/pacmmod/SunXNC25,
	author = {Weijie Sun and
                  Zihuan Xu and
                  Wangze Ni and
                  Lei Chen},
	title = {InTime: Towards Performance Predictability In Byzantine Fault Tolerant
                  Proof-of-Stake Consensus},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {47:1--47:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709740},
	doi = {10.1145/3709740},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SunXNC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Performance predictability, ensuring low latency variability, is crucial for the reliability and efficiency of blockchain consensus. Byzantine Fault Tolerant Proof-of-Stake (BFT-PoS) consensus aims to achieve stable transaction processing latency by scheduling block generation at consistent intervals. However, BFT-PoS's incentive mechanisms grant all transaction tips to the block proposer, which can be exploited by delaying proposals to gain extra Maximal Extractable Value (MEV) rewards, thus undermining performance predictability. Existing solutions impose penalties for delays but lack a standard for measuring the extra rewards from delays or fail in malicious environments. This paper introduces InTime, a novel approach to safeguard performance predictability in BFT-PoS by economically motivating timely block proposals. We first introduce the  untimely MEV ratio , a reliable metric to measure the extra rewards gained from proposal delays, facilitating our countermeasures against deliberate delays. Furthermore, we propose the  arrival rate incentive  (ARI), aligning rewards with transaction arrival timing among nodes to reduce potential MEV manipulation. To make ARI robust against malicious behaviors, we establish a  committee time witness  (CTW) workflow to accurately gather and verify transaction arrival times. Extensive experiments demonstrate that InTime can effectively reduce latency variability by up to 95.9%.}
}


@article{DBLP:journals/pacmmod/WangZCZC25,
	author = {Chengyu Wang and
                  Tongqing Zhou and
                  Lin Chen and
                  Shan Zhao and
                  Zhiping Cai},
	title = {{ISSD:} Indicator Selection for Time Series State Detection},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {48:1--48:25},
	year = {2025},
	url = {https://doi.org/10.1145/3709698},
	doi = {10.1145/3709698},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangZCZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series data from monitoring applications captures the behaviours of objects, which can often be split into distinguishable segments that reflect the underlying state changes. Despite the recent advances in time series state detection, the indicator selection for state detection is rarely studied, most of state detection work assumes the input indicators have been properly or manually selected. However, this assumption is disconnected from practice, on one hand, manual selection is not scalable, there can be up to thousands of indicators for the runtime monitoring of certain objects, e.g., supercomputer systems. On the other hand, performing state detection on a large amount of raw indicators is both inefficient and redundant. We argue that indicator selection should be made an upstream task for selecting a subset of indicators to facilitate state analysis. To this end, we propose ISSD ( I ndicator  S election for  S tate  D etection), an indicator selection method for time series state detection. At its core, ISSD attempts to find an indicator subset that has as much high-quality states, which is measured by the channel set completeness and quality we invent based on segment-level sampling statistics. Such an indicator selection process is transformed into a multi-objective optimization problem and an approximation algorithm is designed to solve the NP-hard searching for specific end point in the Pareto front. Experiments on 5 datasets and 4 downstream methods show that ISSD has significant selection superiority compared with 6 baselines. We also elaborate on two observations of selection resilience and channel sensitivity of existing state detection methods and appeal to further research on them.}
}


@article{DBLP:journals/pacmmod/RuiHSWWC25,
	author = {Lei Rui and
                  Xiangdong Huang and
                  Shaoxu Song and
                  Chen Wang and
                  Jianmin Wang and
                  Zhao Cao},
	title = {Largest Triangle Sampling for Visualizing Time Series in Database},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {49:1--49:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709699},
	doi = {10.1145/3709699},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/RuiHSWWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In time series visualization, sampling is used to reduce the number of points while retaining the visual features of the raw time series. Area-based Largest Triangle Sampling (LTS) excels at preserving perceptually critical points. However, the heuristic solution to LTS by sequentially sampling points with the locally largest triangle area (a.k.a. Largest-Triangle-Three-Buckets, LTTB) suffers from suboptimal solution and query inefficiency. We address the shortcomings by contributing a novel Iterative Largest Triangle Sampling (ILTS) algorithm with convex hull acceleration. It refines the sampling results iteratively, capturing a broader perspective by integrating more points in each iteration. Remarkably, we prove that the largest triangle can always be found in the precomputed convex hulls, making the iterative sampling still efficient. Experiments demonstrate increased visual quality over state-of-the-art baselines and significant speedups over the brute force approach.}
}


@article{DBLP:journals/pacmmod/ZhangLRDLHUGTLGCZ25,
	author = {Longtao Zhang and
                  Ruoyu Li and
                  Congrong Ren and
                  Sheng Di and
                  Jinyang Liu and
                  Jiajun Huang and
                  Robert Underwood and
                  Pascal Grosset and
                  Dingwen Tao and
                  Xin Liang and
                  Hanqi Guo and
                  Franck Cappello and
                  Kai Zhao},
	title = {{LCP:} Enhancing Scientific Data Management with Lossy Compression
                  for Particles},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {50:1--50:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709700},
	doi = {10.1145/3709700},
	timestamp = {Wed, 16 Jul 2025 21:19:19 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangLRDLHUGTLGCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many scientific applications opt for particles instead of meshes as their basic primitives to model complex systems composed of billions of discrete entities. Such applications span a diverse array of scientific domains, including molecular dynamics, cosmology, computational fluid dynamics, and geology. The scale of the particles in those scientific applications increases substantially thanks to the ever-increasing computational power in high-performance computing (HPC) platforms. However, the actual gains from such increases are often undercut by obstacles in data management systems related to data storage, transfer, and processing. Lossy compression has been widely recognized as a promising solution to enhance scientific data management systems regarding such challenges, although most existing compression solutions are tailored for Cartesian grids and thus have sub-optimal results on discrete particle data. In this paper, we introduce LCP, an innovative lossy compressor designed for particle datasets, offering superior compression quality and higher speed than existing compression solutions. Specifically, our contribution is threefold. (1) We propose LCP-S, an error-bound aware block-wise spatial compressor to efficiently reduce particle data size while satisfying the pre-defined error criteria. This approach is universally applicable to particle data across various domains, eliminating the need for reliance on specific application domain characteristics. (2) We develop LCP, a hybrid compression solution for multi-frame particle data, featuring dynamic method selection and parameter optimization. It aims to maximize compression effectiveness while preserving data quality as much as possible by utilizing both spatial and temporal domains. (3) We evaluate our solution alongside eight state-of-the-art alternatives on eight real-world particle datasets from seven distinct domains. The results demonstrate that our solution achieves up to 104% improvement in compression ratios and up to 593% increase in speed compared to the second-best option, under the same error criteria.}
}


@article{DBLP:journals/pacmmod/WangIP25,
	author = {Qitong Wang and
                  Ioana Ileana and
                  Themis Palpanas},
	title = {LeaFi: Data Series Indexes on Steroids with Learned Filters},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {51:1--51:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709701},
	doi = {10.1145/3709701},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangIP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ever-growing collections of data series create a pressing need for efficient similarity search, which serves as the backbone for various analytics pipelines. Recent studies have shown that tree-based series indexes excel in many scenarios. However, we observe a significant waste of effort during search, due to suboptimal pruning. To address this issue, we introduce LeaFi, a novel framework that uses machine learning models to boost pruning effectiveness of tree-based data series indexes. These models act as learned filters, which predict tight node-wise distance lower bounds that are used to make pruning decisions, thus, improving pruning effectiveness. We describe the LeaFi-enhanced index building algorithm, which selects leaf nodes and generates training data to insert and train machine learning models, as well as the LeaFi-enhanced search algorithm, which calibrates learned filters at query time to support the user-defined quality target of each query. Our experimental evaluation, using two different tree-based series indexes and five diverse datasets, demonstrates the advantages of the proposed approach. LeaFi-enhanced data-series indexes improve pruning ratio by up to 20x and search time by up to 32x, while maintaining a target recall of 99%.}
}


@article{DBLP:journals/pacmmod/LiYCKZ25,
	author = {Jiangneng Li and
                  Haitao Yuan and
                  Gao Cong and
                  Han Mao Kiah and
                  Shuhao Zhang},
	title = {{MAST:} Towards Efficient Analytical Query Processing on Point Cloud
                  Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {52:1--52:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709702},
	doi = {10.1145/3709702},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiYCKZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of 3D scanning technology, particularly within autonomous driving, has led to an exponential increase in the volume of Point Cloud (PC) data. Given the rich semantic information contained in PC data, deep learning models are commonly employed for tasks such as object queries. However, current query systems that support PC data types do not process queries on semantic information. Consequently, there is a notable gap in research regarding the efficiency of invoking deep models for each PC data query, especially when dealing with large-scale models and datasets. To address this issue, this work aims to design an efficient approximate approach for supporting PC analysis queries, including PC retrieval and aggregate queries. In particular, we propose a novel framework that delivers approximate query results efficiently by sampling core PC frames within a constrained budget, thereby minimizing the reliance on deep learning models. This framework is underpinned by rigorous theoretical analysis, providing error-bound guarantees for the approximate results if the sampling policy is preferred. To achieve this, we incorporate a multi-agent reinforcement learning-based approach to optimize the sampling procedure, along with an innovative reward design leveraging spatio-temporal PC analysis. Furthermore, we exploit the spatio-temporal characteristics inherent in PC data to construct an index that accelerates the query process. Extensive experimental evaluations demonstrate that our proposed method, MAST, not only achieves accurate approximate query results but also maintains low query latency, ensuring high efficiency.}
}


@article{DBLP:journals/pacmmod/ZhaoZFNLYPJLXTC25,
	author = {Pinxue Zhao and
                  Hailin Zhang and
                  Fangcheng Fu and
                  Xiaonan Nie and
                  Qibin Liu and
                  Fang Yang and
                  Yuanbo Peng and
                  Dian Jiao and
                  Shuaipeng Li and
                  Jinbao Xue and
                  Yangyu Tao and
                  Bin Cui},
	title = {{MEMO:} Fine-grained Tensor Management For Ultra-long Context {LLM}
                  Training},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {53:1--53:28},
	year = {2025},
	url = {https://doi.org/10.1145/3709703},
	doi = {10.1145/3709703},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhaoZFNLYPJLXTC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, Large Language Models (LLMs) have been trained using extended context lengths to foster more creative applications. However, long context training poses great challenges considering the constraint of GPU memory. It not only leads to substantial activation memory consumption during training, but also incurs considerable memory fragmentation. To facilitate long context training, existing frameworks have adopted strategies such as recomputation and various forms of parallelisms. Nevertheless, these techniques rely on redundant computation or extensive communication, resulting in low Model FLOPS Utilization (MFU). In this paper, we propose MEMO, a novel LLM training framework designed for fine-grained activation memory management. Given the quadratic scaling of computation and linear scaling of memory with sequence lengths when using FlashAttention, we offload memory-consuming activations to CPU memory after each layer's forward pass and fetch them during the backward pass. To maximize the swapping of activations without hindering computation, and to avoid exhausting limited CPU memory, we implement a token-wise activation recomputation and swapping mechanism. Furthermore, we tackle the memory fragmentation issue by employing a bi-level Mixed Integer Programming (MIP) approach, optimizing memory reuse across transformer layers. Empirical results demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU compared to Megatron-LM and DeepSpeed, respectively. This improvement is attributed to MEMO's ability to minimize memory fragmentation, reduce recomputation and intensive communication, and circumvent the delays associated with the memory reorganization process due to fragmentation. By leveraging fine-grained activation memory management, MEMO facilitates efficient training of 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU of 52.30%.}
}


@article{DBLP:journals/pacmmod/XuWQLZLL25,
	author = {Lantian Xu and
                  Dong Wen and
                  Lu Qin and
                  Ronghua Li and
                  Ying Zhang and
                  Yang Lu and
                  Xuemin Lin},
	title = {Minimum Spanning Tree Maintenance in Dynamic Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {54:1--54:24},
	year = {2025},
	url = {https://doi.org/10.1145/3709704},
	doi = {10.1145/3709704},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuWQLZLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Minimum Spanning Tree (MST) is a fundamental structure in graph analytics and can be applied in various applications. The problem of maintaining MSTs in dynamic graphs is significant, as many real-world graphs are frequently updated. Existing studies on MST maintenance primarily focus on theoretical analysis and lack practical efficiency. In this paper, we propose a novel algorithm to maintain MST in dynamic graphs, which achieves high practical efficiency. In addition to the tree structure, our main idea is to maintain a replacement edge for each tree edge. In this way, the tree structure can be immediately updated when a tree edge is deleted. We propose algorithms to maintain the replacement edge for each tree edge by sharing the computation cost in the updating process. Our performance studies on large datasets demonstrate considerable improvements over state-of-the-art solutions.}
}


@article{DBLP:journals/pacmmod/BotherRGHMTK25,
	author = {Maximilian B{\"{o}}ther and
                  Ties Robroek and
                  Viktor Gsteiger and
                  Robin Holzinger and
                  Xianzhe Ma and
                  Pinar T{\"{o}}z{\"{u}}n and
                  Ana Klimovic},
	title = {Modyn: Data-Centric Machine Learning Pipeline Orchestration},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {55:1--55:30},
	year = {2025},
	url = {https://doi.org/10.1145/3709705},
	doi = {10.1145/3709705},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BotherRGHMTK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world machine learning (ML) pipelines, datasets are continuously growing. Models must incorporate this new training data to improve generalization and adapt to potential distribution shifts. The cost of model retraining is proportional to how frequently the model is retrained and how much data it is trained on, which makes the naive approach of retraining from scratch each time impractical. We present Modyn, a data-centric end-to-end machine learning platform. Modyn's ML pipeline abstraction enables users to declaratively describe policies for continuously training a model on a growing dataset. Modyn pipelines allow users to apply data selection policies (to reduce the number of data points) and triggering policies (to reduce the number of trainings). Modyn executes and orchestrates these continuous ML training pipelines. The system is open-source and comes with an ecosystem of benchmark datasets, models, and tooling. We formally discuss how to measure the performance of ML pipelines by introducing the concept of composite models, enabling fair comparison of pipelines with different data selection and triggering policies. We empirically analyze how various data selection and triggering policies impact model accuracy, and also show that Modyn enables high throughput training with sample-level data selection.}
}


@article{DBLP:journals/pacmmod/LimPLK25,
	author = {Bo{-}Young Lim and
                  Jeongha Park and
                  Kisung Lee and
                  Hyuk{-}Yoon Kwon},
	title = {Multi-Level Graph Representation Learning Through Predictive Community-based
                  Partitioning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {56:1--56:27},
	year = {2025},
	url = {https://doi.org/10.1145/3711115},
	doi = {10.1145/3711115},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LimPLK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph representation learning (GRL) aims to map a graph into a low-dimensional vector space while preserving graph topology and node properties. This study proposes a novel GRL model, Multi-Level GRL (simply, ML-GRL), that recursively partitions input graphs by selecting the most appropriate community detection algorithm at each graph or partitioned subgraph. To preserve the relationship between subgraphs, ML-GRL incorporates global graphs that effectively maintain the overall topology. ML-GRL employs a prediction model, which is pre-trained using graph-based features and covers a wide range of graph distributions, to estimate GRL accuracy of each community detection algorithm without partitioning graphs or subgraphs and evaluating them. ML-GRL improves learning accuracy by selecting the most effective community detection algorithm while enhancing learning efficiency from parallel processing of partitioned subgraphs. Through extensive experiments with two different tasks, we demonstrate ML-GRL's superiority over the six representative GRL models in terms of both learning accuracy and efficiency. Specifically, ML-GRL not only improves the accuracy of existing GRL models by 3.68 ~ 47.59% for link prediction and 1.75 ~ 40.90% for node classification but also significantly reduces their running time by 9.63 ~ 62.71% and 7.14 ~ 82.14%, respectively. Our source code is available at https://github.com/pnpy6elp/Multi_Level_GRL.}
}


@article{DBLP:journals/pacmmod/CuiLJTY25,
	author = {Pengjie Cui and
                  Haotian Liu and
                  Dong Jiang and
                  Bo Tang and
                  Ye Yuan},
	title = {Nezha: An Efficient Distributed Graph Processing System on Heterogeneous
                  Hardware},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {57:1--57:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709707},
	doi = {10.1145/3709707},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/CuiLJTY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing scale of graph data across various applications demands efficient distributed graph processing systems. Despite the widespread use of the  Scatter-Gather  model for large-scale graph processing across distributed machines, the performance still can be significantly improved as the computation ability of each machine is not fully utilized and the communication costs during graph processing are expensive in the distributed environment. In this work, we propose a novel and efficient distributed graph processing system Nezha on heterogeneous hardware, where each machine is equipped with both CPU and GPU processors and all these machines in the distributed cluster are interconnected via Remote Direct Memory Access (RDMA).To reduce the communication costs, we devise an effective communication mode with a graph-friendly communication protocol in the graph-based RDMA communication adapter of Nezha. To improve the computation efficiency, we propose a multi-device cooperative execution mechanism in Nezha, which fully utilizes the CPU and GPU processors of each machine in the distributed cluster. We also alleviate the workload imbalance issue at inter-machine and intra-machine levels via the proposed workload balancer in Nezha. We conduct extensive experiments by running 4 widely-used graph algorithms on 5 graph datasets to demonstrate the superiority of Nezha over existing systems.}
}


@article{DBLP:journals/pacmmod/YeWLZM25,
	author = {Zikai Ye and
                  Xiangyu Wang and
                  Zesen Liu and
                  Dan Zhu and
                  Jianfeng Ma},
	title = {OBIR-tree: An Efficient Oblivious Index for Spatial Keyword Queries
                  on Secure Enclaves},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {58:1--58:24},
	year = {2025},
	url = {https://doi.org/10.1145/3709708},
	doi = {10.1145/3709708},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YeWLZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the widely collected spatial-textual data has given rise to numerous applications centered on spatial keyword queries. However, securely providing spatial keyword query services in an outsourcing environment has been challenging. Existing schemes struggle to enable top- k  spatial keyword queries on encrypted data while hiding search, access, and volume patterns, which raises concerns about availability and security. To address the above issue, this paper proposes OBIR-tree, a novel index structure for oblivious (provably hides search, access, and volume patterns) top- k  spatial keyword queries on encrypted data. As a tight spatial-textual index tailored from the IR-tree and PathORAM, OBIR-tree can support sublinear search without revealing any useful information. Furthermore, we present extension designs to optimize the query latency of the OBIR-tree: (1) combine the OBIR-tree with hardware secure enclaves (  e.g.,  Intel SGX) to minimize client-server interactions; (2) build a Real/Dummy block Tree (RDT) to reduce the computational cost of oblivious operations within enclaves. Extensive experimental evaluations on real-world datasets demonstrate that the search efficiency of OBIR-tree outperforms state-of-the-art baselines by 25x ~ 723× and is practical for real-world applications.}
}


@article{DBLP:journals/pacmmod/FengQPC25,
	author = {Zijin Feng and
                  Miao Qiao and
                  Chengzhi Piao and
                  Hong Cheng},
	title = {On Graph Representation for Attributed Hypergraph Clustering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {59:1--59:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709741},
	doi = {10.1145/3709741},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FengQPC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Attributed Hypergraph Clustering (AHC) aims at partitioning a hypergraph into clusters such that nodes in the same cluster are close to each other with both high connectedness and homogeneous attributes. Existing AHC methods are all based on matrix factorization which may incur a substantial computation cost; more importantly, they inherently require a prior knowledge of the number of clusters as an input which, if inaccurately estimated, shall lead to a significant deterioration in the clustering quality. In this paper, we propose <u>A</u>ttributed <u>H</u>ypergraph <u>R</u>epresentation for <u>C</u>lustering (AHRC), a cluster-number-free hypergraph clustering consisting of an effective integration of the hypergraph topology and node attributes for hypergraph representation, a multi-hop modularity function for optimization, and a hypergraph sparsification for scalable computation. AHRC achieves cutting-edge clustering quality and efficiency: compared to the state-of-the-art (SOTA) AHC method on 10 real hypergraphs, AHRC obtains an average of 20% higher F-measure, 24% higher ARI, 26% higher Jaccard Similarity, 10% higher Purity, and runs 5.5× faster. As a byproduct, the intermediate result of graph representation dramatically boosts the clustering quality of SOTA contrastive-learning-based hypergraph clustering methods, showing the generality of our graph representation.}
}


@article{DBLP:journals/pacmmod/ChenLYYZSZ25,
	author = {Xu Chen and
                  Shuncheng Liu and
                  Tong Yuan and
                  Tao Ye and
                  Kai Zeng and
                  Han Su and
                  Kai Zheng},
	title = {Optimizing Block Skipping for High-Dimensional Data with Learned Adaptive
                  Curve},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {60:1--60:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709710},
	doi = {10.1145/3709710},
	timestamp = {Wed, 19 Mar 2025 09:50:53 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenLYYZSZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the realm of big data and cloud analytics, efficiently managing and retrieving high-dimensional data presents a critical challenge. Traditional indexes often struggle with the storage overhead inherent in large datasets. There is a growing interest in the adoption of Small Materialize Aggregation (SMA) among cloud database vendors due to its ability to maintain lightweight block-level metadata, facilitating efficient block skipping. However, SMA performance relies heavily on data layout. This is especially critical in scenarios with wide tables containing hundreds of dimensions, where the curse of dimensionality exacerbates the issue. In this paper, we propose  AdaCurve , a novel approach aimed at enhancing block skipping in high-dimensional datasets through adaptive optimization of data layout. Unlike conventional static and non-adaptive space-filling curves (SFCs),  AdaCurve  leverages machine learning to develop an adaptive curve---a dynamically adjusting optimal projection function tailored to high-dimensional workloads and data characteristics. We introduce an attention-based network to handle high-dimensional data and a learnable objective for training adaptive curves in an end-to-end manner. Extensive experiments conducted on the Spark with real-world datasets demonstrate the effectiveness of  AdaCurve . We have shown that  AdaCurve  effectively scales to datasets with dimensions of up to 1,000 columns, achieving a 2.8× improvement in block skipping compared to SFCs.}
}


@article{DBLP:journals/pacmmod/Li25,
	author = {Weihe Li},
	title = {Pandora: An Efficient and Rapid Solution for Persistence-Based Tasks
                  in High-Speed Data Streams},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {61:1--61:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709711},
	doi = {10.1145/3709711},
	timestamp = {Tue, 01 Apr 2025 19:03:20 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Li25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In data streams, persistence characterizes items that appear repeatedly across multiple non-overlapping time windows. Addressing persistence-based tasks, such as detecting highly persistent items and estimating persistence, is crucial for applications like recommendation systems and anomaly detection in high-velocity data streams. However, these tasks are challenging due to stringent requirements for rapid processing and limited memory resources. Existing methods often struggle with accuracy, especially given highly skewed data distributions and tight fastest memory budgets, where hash collisions are severe. In this paper, we introduce Pandora, a novel approximate data structure designed to tackle these challenges efficiently. Our approach incorporates the insight that items absent for extended periods are likely non-persistent, increasing their probability of eviction to accommodate potential persistent items more effectively. We validate this insight empirically and integrate it into our update strategy, providing better protection for persistent items. We formally analyze Pandora's error bounds to validate its theoretical soundness. Through extensive trace-driven tests, we demonstrate that Pandora achieves superior accuracy and processing speed compared to state-of-the-art methods across various persistence-based tasks. Additionally, we further accelerate Pandora's update speed using Single Instruction Multiple Data (SIMD) instructions, enhancing its efficiency in high-speed data stream environments. The code for our method is open-sourced.}
}


@article{DBLP:journals/pacmmod/MenSGS25,
	author = {Ziyang Men and
                  Zheqi Shen and
                  Yan Gu and
                  Yihan Sun},
	title = {Parallel kd-tree with Batch Updates},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {62:1--62:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709712},
	doi = {10.1145/3709712},
	timestamp = {Tue, 01 Apr 2025 19:03:20 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MenSGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The kd-tree is one of the most widely used data structures to manage multi-dimensional data. Due to the ever-growing data volume, it is imperative to consider parallelism in kd-trees. However, we observed challenges in existing parallel kd-tree implementations, for both constructions and updates. The goal of this paper is to develop efficient in-memory kd-trees by supporting high parallelism and cache-efficiency. We propose the Pkd-tree (Parallel kd-tree), a parallel kd-tree that is efficient both in theory and in practice. The Pkd-tree supports parallel tree construction, batch update (insertion and deletion), and various queries including  k -nearest neighbor search, range query, and range count. We proved that our algorithms have strong theoretical bounds in work (sequential time complexity), span (parallelism), and cache complexity. Our key techniques include 1) an efficient construction algorithm that optimizes work, span, and cache complexity simultaneously, and 2) reconstruction-based update algorithms that guarantee the tree to be weight-balanced. With the new algorithmic insights and careful engineering effort, we achieved a highly optimized implementation of the Pkd-tree. We tested Pkd-tree with various synthetic and real-world datasets, including both uniform and highly skewed data. We compare the Pkd-tree with state-of-the-art parallel kd-tree implementations. In all tests, with better or competitive query performance, Pkd-tree is much faster in construction and updates consistently than all baselines. We released our code.}
}


@article{DBLP:journals/pacmmod/GuFN25,
	author = {Binbin Gu and
                  Juncheng Fang and
                  Faisal Nawab},
	title = {PoneglyphDB: Efficient Non-interactive Zero-Knowledge Proofs for Arbitrary
                  SQL-Query Verification},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {63:1--63:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709713},
	doi = {10.1145/3709713},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GuFN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In database applications involving sensitive data, the dual imperatives of data confidentiality and provable (verifiable) query processing are important. This paper introduces PoneglyphDB, a database system that leverages non-interactive zero-knowledge proofs (ZKP) to support both confidentiality and provability. Unlike traditional databases, PoneglyphDB enhances confidentiality by ensuring that raw data remains exclusively with the host, while also enabling verifying the correctness of query responses by providing proofs to clients. The main innovation in this paper is proposing efficient ZKP designs (called circuits) for basic operations in SQL query processing. These basic operation circuits are then combined to form ZKP circuits for larger, more complex queries. PoneglyphDB's circuits are carefully designed to be efficient by utilizing advances in cryptography such as PLONKish-based circuits, recursive proof composition techniques, and designing with low-order polynomial constraints. We demonstrate the performance of PoneglyphDB with the standard TPC-H benchmark. Our experimental results show that PoneglyphDB can efficiently achieve both confidentiality and provability, outperforming existing state-of-the-art ZKP methods.}
}


@article{DBLP:journals/pacmmod/ZhouLHYS25,
	author = {Xinjing Zhou and
                  Viktor Leis and
                  Jinming Hu and
                  Xiangyao Yu and
                  Michael Stonebraker},
	title = {Practical {DB-OS} Co-Design with Privileged Kernel Bypass},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {64:1--64:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709714},
	doi = {10.1145/3709714},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhouLHYS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper revisits the longstanding challenge of coordinating database systems with general-purpose OS interfaces, such as POSIX, which often lack tailored support for DB requirements. Existing approaches to this DB-OS co-design struggle with limited design space, security risks, and compatibility issues. To overcome these hurdles, we propose a new co-design approach leveraging virtualization to elevate the privilege level of DB processes. Our method enables database systems to fully exploit hardware capabilities via virtualization, while minimizing the need for extensive modifications to the host OS kernel, thereby maintaining compatibility. We demonstrate the effectiveness of our approach through two novel virtual memory mechanisms tailored for database workloads: (1) an efficient snapshotting mechanism that captures memory snapshots at millisecond intervals for in-memory databases and HTAP workloads, and (2) a streamlined in-kernel buffer pool design. We introduce  Libdbos , a lightweight guest kernel implementing these mechanisms. Our evaluations highlight significant improvements in latency and efficiency compared to existing snapshotting and buffer pool designs, underscoring the potential of the approach.}
}


@article{DBLP:journals/pacmmod/MaciejewskiNPV25,
	author = {Jakub Maciejewski and
                  Konstantinos Nikoletos and
                  George Papadakis and
                  Yannis Velegrakis},
	title = {Progressive Entity Matching: {A} Design Space Exploration},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {65:1--65:25},
	year = {2025},
	url = {https://doi.org/10.1145/3709715},
	doi = {10.1145/3709715},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MaciejewskiNPV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity Resolution (ER) is typically implemented as a batch task that processes all available data before identifying duplicate records. However, applications with time or computational constraints, e.g., those running in the cloud, require a progressive approach that produces results in a pay-as-you-go fashion. Numerous algorithms have been proposed for Progressive ER in the literature. In this work, we propose a novel framework for Progressive Entity Matching that organizes relevant techniques into four consecutive steps: (i) filtering, which reduces the search space to the most likely candidate matches, (ii) weighting, which associates every pair of candidate matches with a similarity score, (iii) scheduling, which prioritizes the execution of the candidate matches so that the real duplicates precede the non-matching pairs, and (iv) matching, which applies a complex, matching function to the pairs in the order defined by the previous step. We associate each step with existing and novel techniques, illustrating that our framework overall generates a superset of the main existing works in the field. We select the most representative combinations resulting from our framework and fine-tune them over 10 established datasets for Record Linkage and 8 for Deduplication, with our results indicating that our taxonomy yields a wide range of high performing progressive techniques both in terms of effectiveness and time efficiency.}
}


@article{DBLP:journals/pacmmod/SiddiquiKCYL25,
	author = {Tarique Siddiqui and
                  Arnd Christian K{\"{o}}nig and
                  Jiashen Cao and
                  Cong Yan and
                  Shuvendu K. Lahiri},
	title = {{QURE:} AI-Assisted and Automatically Verified {UDF} Inlining},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {66:1--66:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709716},
	doi = {10.1145/3709716},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SiddiquiKCYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User-defined functions (UDFs) extend the capabilities of SQL by improving code reusability and encapsulating complex logic, but can hinder the performance due to optimization and execution inefficiencies. Prior approaches attempt to address this by rewriting UDFs into native SQL, which is then inlined into the SQL queries that invoke them. However, these approaches are either limited to simple pattern matching or require the synthesis of complex verification conditions from procedural code, a process that is brittle and difficult to automate. This limits coverage and makes the translation approaches less extensible to previously unseen procedural constructs. In this work, we present QURE, a framework that (1) leverages large language models (LLMs) to translate UDFs to native SQL, and (2) introduces a novel formal verification method to establish equivalence between the UDF and its translation. QURE uses the semantics of SQL operators to automate the derivation of verification conditions, in turn resulting in broad coverage and high extensibility. We model a large set of imperative constructs, particularly those common in Python and Pandas UDFs, in an intermediate verification language, allowing for the verification of their SQL translation. In our empirical evaluation of Python and Pandas UDFs, equivalence is successfully verified for 88% of UDF-SQL pairs (the rest lack semantically-equivalent SQLs) and LLMs correctly translate 84% of the UDFs. Executing the translated UDFs achieves median performance improvements of 23x on single-node clusters and 12x on 12-node clusters compared to the original UDFs, while also significantly reducing out-of-memory errors.}
}


@article{DBLP:journals/pacmmod/ChenS25,
	author = {Ziling Chen and
                  Shaoxu Song},
	title = {Randomized Sketches for Quantile in LSM-tree based Store},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {67:1--67:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709717},
	doi = {10.1145/3709717},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantiles are costly to compute exactly but can be efficiently estimated by quantile sketches. Extensive works on summarizing streaming data, such as KLL sketch, focus on minimizing the cost in memory to provide certain error guarantees. For the problem of quantile estimation of values in LSM-tree based stores, streaming methods have an expensive I/O cost linear to data size N. Since disk components (chunks and SSTables) in the LSM-tree are immutable once flushed, quantile sketches can be pre-computed as a type of statistics to reduce I/O cost and accelerate queries. Unfortunately, to provide deterministic additive εN error guarantees on queried data, all pre-computed deterministic sketches of queried chunks each with size N_c should provide εN_c error guarantee, resulting in no improvement in the linear I/O cost. In this study, we propose pre-computing randomized sketches which provide randomized additive error guarantees. Our major technical contributions include (1) randomized sketches for data chunks constructed in flush events, which are proved to be optimal and achieve an I/O cost proportional to √(N), (2) hierarchical randomized sketches for SSTables constructed in compaction events, that further improve the asymptotic I/O cost, (3) the KLL sketch summarizing proposed pre-computed sketches is proved to be more accurate than that summarizing streaming data, and proved to achieve sublinear I/O cost while achieving the same memory complexity as in the streaming settings. Extensive experiments on synthetic and real datasets demonstrate the superiority of the proposed techniques. The approach is deployed in an LSM-tree based time-series database Apache IoTDB.}
}


@article{DBLP:journals/pacmmod/LimYKFJ25,
	author = {Kyungmin Lim and
                  Minseok Yoon and
                  Kihwan Kim and
                  Alan David Fekete and
                  Hyungsoo Jung},
	title = {Rapid Data Ingestion through {DB-OS} Co-design},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {68:1--68:28},
	year = {2025},
	url = {https://doi.org/10.1145/3709718},
	doi = {10.1145/3709718},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LimYKFJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential data access for the rapid ingestion of large fact tables from storage is a pivotal yet resource-intensive operation in data warehouse systems, consuming substantial CPU cycles across various components of DBMSs and operating systems. Although bypassing these layers can eliminate access latency, concurrent access to the same table often results in redundant data fetching due to cache-bypassing data transfers. Thus, a new design for data access control is necessary to enhance rapid data ingestion in databases. To address this concern, we propose a novel DB-OS co-design that efficiently supports sequential data access at full device speed. Our approach, zicIO, liberates DBMSs from data access control by preparing required data just before DBMSs access it, while alleviating all known I/O latencies. The core of zicIO lies in its DB-OS co-design, which aims to (1) automate data access control and (2) relieve redundant data fetching through seamless collaboration between the DB and the OS. We implemented zicIO and integrated it with four databases to demonstrate its general applicability. The evaluation showed performance enhancements of up to 9.95x under TPC-H loads.}
}


@article{DBLP:journals/pacmmod/ChenCKY25,
	author = {Kaiwen Chen and
                  Yueting Chen and
                  Nick Koudas and
                  Xiaohui Yu},
	title = {Reliable Text-to-SQL with Adaptive Abstention},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {69:1--69:30},
	year = {2025},
	url = {https://doi.org/10.1145/3709719},
	doi = {10.1145/3709719},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenCKY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have revolutionized natural language interfaces for databases, particularly in text-to-SQL conversion. However, current approaches often generate unreliable outputs when faced with ambiguity or insufficient context. We present Reliable Text-to-SQL (RTS), a novel framework that enhances query generation reliability by incorporating abstention and human-in-the-loop mechanisms. RTS focuses on the critical schema linking phase, which aims to identify the key database elements needed for generating SQL queries. It autonomously detects potential errors during the answer generation process and responds by either abstaining or engaging in user interaction. A vital component of RTS is the Branching Point Prediction (BPP) which utilizes statistical conformal techniques on the hidden layers of the LLM model for schema linking, providing probabilistic guarantees on schema linking accuracy. We validate our approach through comprehensive experiments on the BIRD benchmark, demonstrating significant improvements in robustness and reliability. Our findings highlight the potential of combining transparent-box LLMs with human-in-the-loop processes to create more robust natural language interfaces for databases. For the BIRD benchmark, our approach achieves near-perfect schema linking accuracy, autonomously involving a human when needed. Combined with query generation, we demonstrate that near-perfect schema linking and a small query generation model can almost match SOTA accuracy achieved with a model orders of magnitude larger than the one we use.}
}


@article{DBLP:journals/pacmmod/SuHSZGJCZHG25,
	author = {Jixian Su and
                  Chiyu Hao and
                  Shixuan Sun and
                  Hao Zhang and
                  Sen Gao and
                  Jiaxin Jiang and
                  Yao Chen and
                  Chenyi Zhang and
                  Bingsheng He and
                  Minyi Guo},
	title = {Revisiting the Design of In-Memory Dynamic Graph Storage},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {70:1--70:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709720},
	doi = {10.1145/3709720},
	timestamp = {Mon, 07 Jul 2025 18:07:06 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SuHSZGJCZHG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The effectiveness of in-memory dynamic graph storage (DGS) for supporting concurrent graph read and write queries is crucial for real-time graph analytics and updates. Various methods have been proposed, for example, LLAMA, Aspen, LiveGraph, Teseo, and Sortledton. These approaches differ significantly in their support for read and write operations, space overhead, and concurrency control. However, there has been no systematic study to explore the trade-offs among these dimensions. In this paper, we evaluate the effectiveness of individual techniques and identify the performance factors affecting these storage methods by proposing a common abstraction for DGS design and implementing a generic test framework based on this abstraction. Our findings highlight several key insights: 1) Existing DGS methods exhibit substantial space overhead. For example, Aspen consumes 3.3-10.8x more memory than CSR, while the optimal fine-grained methods consume 4.1-8.9x more memory than CSR, indicating a significant memory overhead. 2) Existing methods often overlook memory access impact of modern architectures, leading to performance degradation compared to continuous storage methods. 3) Fine-grained concurrency control methods, in particular, suffer from severe efficiency and space issues due to maintaining versions and performing checks for each neighbor. These methods also experience significant contention on high-degree vertices. Our systematic study reveals these performance bottlenecks and outlines future directions to improve DGS for real-time graph analytics.}
}


@article{DBLP:journals/pacmmod/ZhengYCW25,
	author = {Zhihan Zheng and
                  Haitao Yuan and
                  Minxiao Chen and
                  Shangguang Wang},
	title = {{RLER-TTE:} An Efficient and Effective Framework for En Route Travel
                  Time Estimation with Reinforcement Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {71:1--71:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709721},
	doi = {10.1145/3709721},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhengYCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {En Route Travel Time Estimation (ER-TTE) aims to learn driving patterns from traveled routes to achieve rapid and accurate real-time predictions. However, existing methods ignore the complexity and dynamism of real-world traffic systems, resulting in significant gaps in efficiency and accuracy in real-time scenarios. Addressing this issue is a critical yet challenging task. This paper proposes a novel framework that redefines the implementation path of ER-TTE to achieve highly efficient and effective predictions. Firstly, we introduce a novel pipeline consisting of a Decision Maker and a Predictor to rectify the inefficient prediction strategies of current methods. The Decision Maker performs efficient real-time decisions to determine whether the high-complexity prediction model in the Predictor needs to be invoked, and the Predictor recalculates the travel time or infers from historical prediction results based on these decisions. Next, to tackle the dynamic and uncertain real-time scenarios, we model the online decision-making problem as a Markov decision process and design an intelligent agent based on reinforcement learning for autonomous decision-making. Moreover, to fully exploit the spatio-temporal correlation between online data and offline data, we meticulously design feature representation and encoding techniques based on the attention mechanism. Finally, to improve the flawed training and evaluation strategies of existing methods, we propose an end-to-end training and evaluation approach, incorporating curriculum learning strategies to manage spatio-temporal data for more advanced training algorithms. Extensive evaluations on three real-world datasets confirm that our method significantly outperforms state-of-the-art solutions in both accuracy and efficiency.}
}


@article{DBLP:journals/pacmmod/SharmaGGL25,
	author = {Chandan Sharma and
                  Pierre Genev{\`{e}}s and
                  Nils Gesbert and
                  Nabil Laya{\"{\i}}da},
	title = {Schema-Based Query Optimisation for Graph Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {72:1--72:29},
	year = {2025},
	url = {https://doi.org/10.1145/3709722},
	doi = {10.1145/3709722},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SharmaGGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recursive graph queries are increasingly popular for extracting information from interconnected data found in various domains such as social networks, life sciences, and business analytics. Graph data often come with schema information that describe how nodes and edges are organized. We propose a type inference mechanism that enriches recursive graph queries with relevant structural information contained in a graph schema. We show that this schema information can be useful in order to improve the performance when evaluating recursive graph queries. Furthermore, we prove that the proposed method is sound and complete, ensuring that the semantics of the query is preserved during the schema-enrichment process.}
}


@article{DBLP:journals/pacmmod/HanCZFHS25,
	author = {Zongda Han and
                  Xiang Cheng and
                  Wenhong Zhao and
                  Jiaxin Fu and
                  Zhaofeng He and
                  Sen Su},
	title = {SecureXGB: {A} Secure and Efficient Multi-party Protocol for Vertical
                  Federated XGBoost},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {73:1--73:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709723},
	doi = {10.1145/3709723},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HanCZFHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extreme Gradient Boosting (XGBoost) demonstrates excellent performance in practice and is widely used in both industry and academic research. This extensive application has led to a growing interest in employing multi-party data to develop more robust XGBoost models. In response to increasing concerns about privacy leakage, secure vertical federated XGBoost is proposed. It employs secure multi-party computation techniques, such as secret sharing (SS), to allow multiple parties holding vertically partitioned data, i.e., disjoint features on the same samples, to collaborate in constructing an XGBoost model. However, the running efficiency is the primary obstacle to the practical application of existing protocols, especially in multi-party settings. The reason is that these protocols not only require the execution of data-oblivious computations to protect intermediate results, leading to high computational complexity, but also involve a large number of SS-based non-linear operations with high overheads, e.g., division operations in gain score calculation and comparison operations in best split selection. To this end, we present a secure and efficient multi-party protocol for vertical federated XGBoost, called SecureXGB, which can perform the collaborative training of an XGBoost model in an SS-friendly manner. In SecureXGB, we first propose a parallelizable multi-party permutation method, which can secretly and efficiently permute all samples before model training to reduce the reliance on data-oblivious computations. Then, we design a linear gain score that can be evaluated without involving division operations and has equivalent utility to the original gain score. Finally, we develop a synchronous best split selection method to secretly identify the best split with the maximum gain score using a minimal number of comparison operations. Experimental results demonstrate that SecureXGB can achieve better training efficiency than state-of-the-art protocols without the loss of model accuracy.}
}


@article{DBLP:journals/pacmmod/XuCZLC25,
	author = {Kaiqiang Xu and
                  Di Chai and
                  Junxue Zhang and
                  Fan Lai and
                  Kai Chen},
	title = {Sequoia: An Accessible and Extensible Framework for Privacy-Preserving
                  Machine Learning over Distributed Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {74:1--74:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709742},
	doi = {10.1145/3709742},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuCZLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy-preserving machine learning (PPML) algorithms use secure computation protocols to allow multiple data parties to collaboratively train machine learning (ML) models while maintaining their data confidentiality. However, current PPML frameworks couple secure protocols with ML models in PPML algorithm implementations, making it challenging for non-experts to develop and optimize PPML applications, limiting their accessibility and performance. We propose Sequoia, a novel PPML framework that decouples ML models and secure protocols to optimize the development and execution of PPML applications across data parties. Sequoia offers JAX-compatible APIs for users to program their ML models, while using a compiler-executor architecture to automatically apply PPML algorithms and system optimizations for model execution over distributed data. The compiler in Sequoia incorporates cross-party PPML processes into user-defined ML models by transparently adding computation, encryption, and communication steps with extensible policies, and the executor efficiently schedules code execution across multiple data parties, considering data dependencies and device heterogeneity. Compared to existing PPML frameworks, Sequoia requires 64%-92% fewer lines of code for users to implement the same PPML algorithms, and achieves 88% speedup of training throughput in horizontal PPML.}
}


@article{DBLP:journals/pacmmod/PangPXLL25,
	author = {Junyuan Pang and
                  Jian Pei and
                  Haocheng Xia and
                  Xiang Li and
                  Jinfei Liu},
	title = {Shapley Value Estimation based on Differential Matrix},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {75:1--75:28},
	year = {2025},
	url = {https://doi.org/10.1145/3709725},
	doi = {10.1145/3709725},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PangPXLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Shapley value has been extensively used in many fields as the unique metric to fairly evaluate player contributions in cooperative settings. Since the exact computation of Shapley values is \\#P-hard in the task-agnostic setting, many studies have been developed to utilize the Monte Carlo method for Shapley value estimation. The existing methods estimate the Shapley values directly. In this paper, we explore a novel idea-inferring the Shapley values by estimating the differences between them. Technically, we estimate a differential matrix consisting of pairwise Shapley value differences to reduce the variance of the estimated Shapley values. We develop a least-squares optimization solution to derive the Shapley values from the differential matrix, minimizing the estimator variances. Additionally, we devise a Monte Carlo method for efficient estimation of the differential matrix and introduce two stratified Monte Carlo methods for further variance reduction. Our experimental results on real and synthetic data sets demonstrate the effectiveness and efficiency of the differential-matrix-based sampling approaches.}
}


@article{DBLP:journals/pacmmod/BenEfraimDS25,
	author = {Hadar Ben{-}Efraim and
                  Susan B. Davidson and
                  Amit Somech},
	title = {{SHARQ:} Explainability Framework for Association Rules on Relational
                  Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {76:1--76:25},
	year = {2025},
	url = {https://doi.org/10.1145/3709726},
	doi = {10.1145/3709726},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BenEfraimDS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Association rules are an important technique for gaining insights over large relational datasets consisting of tuples of elements (i.e. attribute-value pairs). However, it is difficult to explain the relative importance of data elements with respect to the rules in which they appear. This paper develops a measure of an element's contribution to a set of association rules based on Shapley values, denoted SHARQ (ShApley Rules Quantification). As is the case with many Shapely-based computations, the cost of a naive calculation of the score is exponential in the number of elements. To that end, we present an efficient framework for computing the exact SHARQ value of a single element whose running time is practically linear in the number of rules. Going one step further, we develop an efficient multi-element SHARQ algorithm which amortizes the cost of the single element SHARQ calculation over a set of elements. Based on the definition of SHARQ for elements we describe two additional use-cases for association rules explainability: rule importance and attribute importance. Extensive experiments over a novel benchmark dataset containing 67 instances of mined rule sets show the effectiveness of our approach.}
}


@article{DBLP:journals/pacmmod/LuomaK25,
	author = {Kyle Luoma and
                  Arun Kumar},
	title = {{SNAILS:} Schema Naming Assessments for Improved LLM-Based {SQL} Inference},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {77:1--77:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709727},
	doi = {10.1145/3709727},
	timestamp = {Tue, 01 Apr 2025 19:03:20 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuomaK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Language Models (LLMs) have revolutionized Natural Language to SQL (NL-to-SQL), dominating most NL-to-SQL benchmarks. But LLMs still face limitations due to hallucinations, semantic ambiguity, and lexical mismatches between an NL query and the database schema. Naturally, a lot of work in the ML+DB intersection aims to mitigate such LLM limitations. In this work, we shine the light on a complementary data-centric question:  How should DB schemas evolve in this era of LLMs to boost NL-to-SQL?  The intuition is that more NL-friendly schema identifiers can help LLMs work better with DBs. We dive deeper into this seemingly obvious, but hitherto underexplored and important, connection between schema identifier ''naturalness'' and the behavior of LLM-based NL-to-SQL by creating a new integrated benchmark suite we call SNAILS. SNAILS has 4 novel artifacts: (1) A collection of real-world DB schemas not present in prior NL-to-SQL benchmarks; (2) A set of labeled NL-SQL query pairs on our collection not seen before by public LLMs; (3) A notion of naturalness level for schema identifiers and a novel labeled dataset of modified identifiers; and (4) AI artifacts to automatically modify identifier naturalness. Using SNAILS, we perform a comprehensive empirical evaluation of the impact of schema naturalness on LLM-based NL-to-SQL accuracy, and present a method for improving LLM-based NL-to-SQL with natural views. Our results reveal statistically significant correlations across multiple public LLMs from OpenAI, Meta, and Google on multiple databases using both zero-shot prompting as well as more complex NL-to-SQL workflows: DIN SQL, and CodeS. We present several fine-grained insights and discuss pathways for DB practitioners to better exploit LLMs for NL-to-SQL.}
}


@article{DBLP:journals/pacmmod/LiLCGRQW25,
	author = {Xiaochen Li and
                  Tianyu Li and
                  Yitian Cheng and
                  Chen Gong and
                  Kui Ren and
                  Zhan Qin and
                  Tianhao Wang},
	title = {{SPAS:} Continuous Release of Data Streams under w-Event Differential
                  Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {78a:1--78a:27},
	year = {2025},
	url = {https://doi.org/10.1145/3714420},
	doi = {10.1145/3714420},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiLCGRQW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continuous release of data streams is frequently used in numerous applications. However, when data is sensitive, this poses privacy risks. To mitigate this risk, efforts have been devoted to devising techniques that satisfy a formal privacy notion called w-event differential privacy. Nevertheless, a recent benchmark reveals that none of the existing works offer a universally effective solution across all types of data streams, making it challenging to select an appropriate scheme for unknown data streams in practical scenarios. We identify that all existing methods are  heuristic-based  and make  data-independent  decisions. In this paper, we change this landscape by introducing SPAS which is built on  data-dependent  strategies. Specifically, SPAS continuously predicts an optimal publishing strategy within each sliding window that minimizes the error of the released results based on the characteristics of the data stream. Additionally, we develop a weighted sparse vector technique to control data sampling and manage privacy budget consumption following that optimal publishing strategy. Comprehensive experimental evaluations demonstrate the efficacy of SPAS in adapting to diverse one-dimensional and multi-dimensional data streams for both data release and range query tasks. Our code is open-sourced.}
}


@article{DBLP:journals/pacmmod/WeiLLPP25,
	author = {Jiuqi Wei and
                  Xiaodong Lee and
                  Zhenyu Liao and
                  Themis Palpanas and
                  Botao Peng},
	title = {Subspace Collision: An Efficient and Accurate Framework for High-dimensional
                  Approximate Nearest Neighbor Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {79:1--79:29},
	year = {2025},
	url = {https://doi.org/10.1145/3709729},
	doi = {10.1145/3709729},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WeiLLPP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate Nearest Neighbor (ANN) search in high-dimensional Euclidean spaces is a fundamental problem with a wide range of applications. However, there is currently no ANN method that performs well in both indexing and query answering performance, while providing rigorous theoretical guarantees for the quality of the answers. In this paper, we first design SC-score, a metric that we show follows the Pareto principle and can act as a proxy for the Euclidean distance between data points. Inspired by this, we propose a novel ANN search framework called Subspace Collision (SC), which can provide theoretical guarantees on the quality of its results. We further propose SuCo, which achieves efficient and accurate ANN search by designing a clustering-based lightweight index and query strategies for our proposed subspace collision framework. Extensive experiments on real-world datasets demonstrate that both the indexing and query answering performance of SuCo outperform state-of-the-art ANN methods that can provide theoretical guarantees, performing 1-2 orders of magnitude faster query answering with only up to one-tenth of the index memory footprint. Moreover, SuCo achieves top performance (best for hard datasets) even when compared to methods that do not provide theoretical guarantees.}
}


@article{DBLP:journals/pacmmod/GouGXL25,
	author = {Yutong Gou and
                  Jianyang Gao and
                  Yuexuan Xu and
                  Cheng Long},
	title = {SymphonyQG: Towards Symphonious Integration of Quantization and Graph
                  for Approximate Nearest Neighbor Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {80:1--80:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709730},
	doi = {10.1145/3709730},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GouGXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate nearest neighbor (ANN) search in high-dimensional Euclidean space has a broad range of applications. Among existing ANN algorithms, graph-based methods have shown superior performance in terms of the time-accuracy trade-off. However, they face performance bottlenecks due to the random memory accesses caused by the searching process on the graph indices and the costs of computing exact distances to guide the searching process. To relieve the bottlenecks, a recent method named NGT-QG makes an attempt by integrating quantization and graph. It (1) replicates and stores the quantization codes of a vertex's neighbors compactly so that they can be accessed sequentially, and (2) uses a SIMD-based implementation named FastScan to efficiently estimate distances based on the quantization codes in batch for guiding the searching process. While NGT-QG achieves promising improvements over the vanilla graph-based methods, it has not fully unleashed the potential of integrating quantization and graph. For instance, it entails a re-ranking step to compute exact distances at the end, which introduces extra random memory accesses; its graph structure is not jointly designed considering the in-batch nature of FastScan, which causes wastes of computation in searching. In this work, following NGT-QG, we present a new method named SymphonyQG, which achieves more symphonious integration of quantization and graph (e.g., it avoids the explicit re-ranking step and refines the graph structure to be more aligned with FastScan). Based on extensive experiments on real-world datasets, SymphonyQG establishes the new state-of-the-art in terms of the time-accuracy trade-off: at 95% recall, SymphonyQG achieves 1.5x-4.5x QPS compared with the most competitive baselines and achieves 3.5x-17x QPS compared with the classical library HNSWlib across all tested datasets. At the same time, its indexing is at least 8x faster than NGT-QG.}
}


@article{DBLP:journals/pacmmod/ZhangZZGWLXZJQZY25,
	author = {Yongliang Zhang and
                  Yuanyuan Zhu and
                  Hao Zhang and
                  Congli Gao and
                  Yuyang Wang and
                  Guojing Li and
                  Tianyang Xu and
                  Ming Zhong and
                  Jiawei Jiang and
                  Tieyun Qian and
                  Chenyi Zhang and
                  Jeffrey Xu Yu},
	title = {TGraph: {A} Tensor-centric Graph Processing Framework},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {81:1--81:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709731},
	doi = {10.1145/3709731},
	timestamp = {Mon, 07 Jul 2025 18:07:09 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangZZGWLXZJQZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph is ubiquitous in various real-world applications, and many graph processing systems have been developed. Recently, hardware accelerators have been exploited to speed up graph systems. However, such hardware-specific systems are hard to migrate across different hardware backends. In this paper, we propose the first tensor-based graph processing framework, Tgraph, which can be smoothly deployed and run on any powerful hardware accelerators (uniformly called XPU) that support Tensor Computation Runtimes (TCRs). TCRs, which are deep learning frameworks along with their runtimes and compilers, provide tensor-based interfaces to users to easily utilize specialized hardware accelerators without delving into the complex low-level programming details. However, building an efficient tensor-based graph processing framework is non-trivial. Thus, we make the following efforts: (1) propose a tensor-centric computation model for users to implement graph algorithms with easy-to-use programming interfaces; (2) provide a set of graph operators implemented by tensor to shield the computation model from the detailed tensor operators so that Tgraph can be easily migrated and deployed across different TCRs; (3) design a tensor-based graph compression and computation strategy and an out-of-XPU-memory computation strategy to handle large graphs. We conduct extensive experiments on multiple graph algorithms (BFS, WCC, SSSP, etc.), which validate that Tgraph not only outperforms seven state-of-the-art graph systems, but also can be smoothly deployed and run on multiple DL frameworks (PyTorch and TensorFlow) and hardware backends (Nvidia GPU, AMD GPU, and Apple MPS).}
}


@article{DBLP:journals/pacmmod/XuYZPCSZD25,
	author = {Qian Xu and
                  Juan Yang and
                  Feng Zhang and
                  Junda Pan and
                  Kang Chen and
                  Youren Shen and
                  Amelie Chi Zhou and
                  Xiaoyong Du},
	title = {Tribase: {A} Vector Data Query Engine for Reliable and Lossless Pruning
                  Compression using Triangle Inequalities},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {82:1--82:28},
	year = {2025},
	url = {https://doi.org/10.1145/3709743},
	doi = {10.1145/3709743},
	timestamp = {Thu, 29 May 2025 13:23:46 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuYZPCSZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate Nearest Neighbor Search (ANNS) is a critical problem in vector databases. Cluster-based index is utilized to narrow the search scope of ANNS, thereby accelerating the search process. Due to its scalability, it is widely employed in real-world vector search systems. However, existing cluster-based indexes often suffer from coarse granularity, requiring query vectors to compute distances with vectors of varying quality, thus increasing query complexity. Existing work aim to represent vectors with minimal cost, such as using product quantization (PQ) or linear transformations, to speed up ANNS. However, these approaches do not address the coarse granularity inherent in cluster-based index. In this paper, we present an efficient vector data query engine to enhance the granularity of cluster-based index by carefully subdividing clusters using diverse distance metrics. Building on this refined index, we introduce techniques that leverage triangle inequalities to develop highly optimized and distinct search strategies for clusters and vectors of varying qualities, thereby reducing the overhead of ANNS. Extensive experiments demonstrate that our method significantly outperforms existing in-memory cluster-based indexing algorithms, achieving up to an impressive 10× speedup and a pruning ratio exceeding 99.4%.}
}


@article{DBLP:journals/pacmmod/ChenPLWNH25,
	author = {Yahong Chen and
                  Xiaoyi Pang and
                  Xiaoguang Li and
                  Hanyi Wang and
                  Ben Niu and
                  Shengnan Hu},
	title = {{U-DPAP:} Utility-aware Efficient Range Counting on Privacy-preserving
                  Spatial Data Federation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {83a:1--83a:25},
	year = {2025},
	url = {https://doi.org/10.1145/3714333},
	doi = {10.1145/3714333},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenPLWNH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Range counting is a fundamental operation in spatial data applications. There is a growing demand to facilitate this operation over a data federation, where spatial data are separately held by multiple data providers (a.k.a., data silos). Most existing data federation schemes employ Secure Multiparty Computation (SMC) to protect privacy, but this approach is computationally expensive and leads to high latency. Consequently, private data federations are often impractical for typical database workloads.This challenge highlights the need for a private data federation scheme capable of providing fast and accurate query responses while maintaining strong privacy. To address this issue, we propose U-DPAP, a utility-aware efficient privacy-preserving method. It is the first scheme to exclusively use differential privacy for privacy protection in spatial data federation, without employing SMC. Moreover, it combines approximate query processing to further enhance efficiency. Our experimental results indicate that a straightforward combination of the two techniques results in unacceptable impacts on data utility. Thus, we design two novel algorithms: one to make differential privacy practical by optimizing the privacy-utility trade-off, and another to address the efficiency-utility trade-off in approximate query processing. The grouping-based perturbation algorithm reduces noise by grouping similar data and applying noise to the groups. The representative data silos selection algorithm minimizes approximate error by selecting representative silos using the similarity between data silos. We rigorously prove the privacy guarantees of U-DPAP. Moreover, experimental results demonstrate that U-DPAP enhances data utility by an order of magnitude while maintaining high communication efficiency.}
}


@article{DBLP:journals/pacmmod/KoXOLH25,
	author = {Ronny Ko and
                  Chuan Xiao and
                  Makoto Onizuka and
                  Zhiqiang Lin and
                  Yihe Huang},
	title = {Ultraverse: An Efficient What-if Analysis Framework for Software Applications
                  Interacting with Database Systems},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {84:1--84:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709734},
	doi = {10.1145/3709734},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KoXOLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing what-if analysis systems are predominantly tailored to operate on either only the application layer or only the database layer of software. This isolated approach limits their effectiveness in scenarios where intensive interaction between applications and database systems occurs. To address this gap, we introduce Ultraverse, a what-if analysis framework that seamlessly integrates both application and database layers. Ultraverse employs dynamic symbolic execution to effectively translate application code into compact SQL procedure representations, thereby synchronizing application semantics at both SQL and application levels during what-if replays. A novel aspect of Ultraverse is its use of advanced query dependency analysis, which serves two key purposes: (1) it eliminates the need to replay irrelevant transactions that do not influence the outcome, and (2) it facilitates parallel replay of mutually independent transactions, significantly enhancing the analysis efficiency. Ultraverse is applicable to existing unmodified database systems and legacy application codes. Our extensive evaluations of the framework have demonstrated remarkable improvements in what-if analysis speed, achieving performance gains ranging from 7.7x to 291x across diverse benchmarks.}
}


@article{DBLP:journals/pacmmod/PacheraBM25,
	author = {Amedeo Pachera and
                  Angela Bonifati and
                  Andrea Mauri},
	title = {User-Centric Property Graph Repairs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {85:1--85:27},
	year = {2025},
	url = {https://doi.org/10.1145/3709735},
	doi = {10.1145/3709735},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PacheraBM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Property graphs serve as unifying abstractions for encoding, inspecting, and updating interconnected data with greater expressive power. They are increasingly popular across various application domains involving real users. However, graph data often contains inconsistencies that need proper transformations to address underlying constraint violations and often require specific domain knowledge. In this paper, we propose an interactive and user-centric approach to repair property graphs under denial constraints. Our approach includes a novel theoretical framework comprising a query-based inconsistency detection mechanism, a dependency graph for tracking violations, and an assignment algorithm facilitating multi-user property graph repairs by leveraging independent sets. We evaluate our approach through several experiments on real-world and synthetic datasets, considering different levels of user expertise and comparing against various baselines. Even with multiple non-oracle users, our approach outperforms existing interactive and non-interactive baselines by 30% on average in terms of repair quality. Additionally, we conduct a user study to assess real user performance in property graph repairs.}
}


@article{DBLP:journals/pacmmod/LiCLDGZC25,
	author = {Meng Li and
                  Huayi Chai and
                  Siqiang Luo and
                  Haipeng Dai and
                  Rong Gu and
                  Jiaqi Zheng and
                  Guihai Chen},
	title = {{VEGA:} An Active-tuning Learned Index with Group-Wise Learning Granularity},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {1},
	pages = {86:1--86:26},
	year = {2025},
	url = {https://doi.org/10.1145/3709736},
	doi = {10.1145/3709736},
	timestamp = {Tue, 08 Apr 2025 20:24:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiCLDGZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learned indexes, which model key-value data structures by machine learning models, have been extensively studied. However, the fastest immutable learned indexes (e.g., RMI) do not provide the same tight lookup bounds as classical indexes such as B-trees. There are learned indexes that provide tight bounds (e.g., PGM) but those fall short in query performance. This gives rise to an interesting open question: whether there exists a learned index that simultaneously achieves state-of-the-art empirical performance and matching complexity? In this paper, we give a positive answer to this standing problem.We propose two new online model-building policies: (1) simplifying distribution by the adoption of a proper granularity (i.e., grouping multiple keys together for model-building) and (2) actively tuning distribution through key repositioning. Additionally, we introduce a general framework that combines these two policies for performance optimization under a given memory budget. We put everything together to design VEGA, a learned index that simultaneously achieves competitive theoretical and empirical performance compared to state-of-the-art learned indexes. We conducted extensive evaluations, demonstrating VEGA achieves both better lookup and building performance.}
}


@article{DBLP:journals/pacmmod/CarmeliGKK25,
	author = {Nofar Carmeli and
                  Floris Geerts and
                  Ahmet Kara and
                  Benny Kimelfeld},
	title = {PACMMOD, V3, {N2} (PODS), May 2025 Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {87:1--87:4},
	year = {2025},
	url = {https://doi.org/10.1145/3725224},
	doi = {10.1145/3725224},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/CarmeliGKK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We are excited to announce the third issue dedicated to the Principles of Database Systems (PODS) research track of the  Proceedings of the ACM on Management of Data  (PACMMOD) journal. In its current form, the journal hosts both a SIGMOD and a PODS research track. The PODS research track aims to provide a solid scientific foundation for methods, techniques, and solutions to the data management challenges that continually arise in our data-driven society. More specifically, articles in the PODS track of PACMMOD present principled contributions to modeling, application, system building, and both theoretical and experimental validation in the context of data management. Such articles may be based, among others, on establishing theoretical results, developing new concepts and frameworks that merit further exploration, providing experimental work that sheds light on the scientific foundations of the discipline, or conducting a rigorous analysis of widely used and/or recently developed industry artifacts.}
}


@article{DBLP:journals/pacmmod/MengelV25,
	author = {Stefan Mengel and
                  Harry Vinall{-}Smeeth},
	title = {A Lower Bound on Unambiguous Context Free Grammars via Communication
                  Complexity},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {88:1--88:19},
	year = {2025},
	url = {https://doi.org/10.1145/3725225},
	doi = {10.1145/3725225},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MengelV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by recent connections to factorised databases, we analyse the efficiency of representations by context free grammars (CFGs). Concretely, we prove a recent conjecture by Kimelfeld, Martens, and Niewerth (ICDT 2025), that for finite languages representations by general CFGs can be doubly-exponentially smaller than those by unambiguous CFGs. To do so, we show the first exponential lower bounds for representation by unambiguous CFGs of a finite language that can efficiently be represented by ambiguous CFGs. Our proof first reduces the problem to proving a lower bound in a non-standard model of communication complexity. Then, we argue similarly in spirit to a recent discrepancy argument to show the required communication complexity lower bound. Our result also implies that a finite language may admit an exponentially smaller representation as a nondeterministic finite automaton than as an unambiguous CFG.}
}


@article{DBLP:journals/pacmmod/GerlachKSSZ25,
	author = {Luisa Gerlach and
                  Tobias K{\"{o}}ppl and
                  Stefanie Scherzinger and
                  Nicole Schweikardt and
                  Ren{\'{e}} Zander},
	title = {A Quantum-Leap into Schema Matching: Beyond 1-to-1 Matchings},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {89:1--89:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725226},
	doi = {10.1145/3725226},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GerlachKSSZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Schema matching refers to the task of identifying corresponding attributes of different database relation schemas to enable the efficient integration of the associated datasets. We model the task of finding suitable 1:N/N:1 global matchings in relational schemas as an optimization problem. We show that this optimization problem is NP-hard. We then translate the optimization problem into the problem of minimizing a particular rational-valued function on binary variables. The latter enables us to utilize modern quantum algorithms for solving the global matching problem, a crucial stage in schema matching. We also report on preliminary experimental results that serve as a proof-of-concept for our approach.}
}


@article{DBLP:journals/pacmmod/EsmailpourGRS25,
	author = {Aryan Esmailpour and
                  Sainyam Galhotra and
                  Rahul Raychaudhury and
                  Stavros Sintos},
	title = {A Theoretical Framework for Distribution-Aware Dataset Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {90:1--90:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725227},
	doi = {10.1145/3725227},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/EsmailpourGRS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Effective data discovery is a cornerstone of modern data-driven decision-making. Yet, identifying datasets with specific distributional characteristics, such as percentiles or preferences, remains challenging. While recent proposals have enabled users to search based on percentile predicates, much of the research in data discovery relies on heuristic methods, which often result in biased outcomes. This paper presents the first theoretically backed framework that unifies data discovery under centralized and decentralized settings. More specifically, let  P ={P 1 ,..., P N } be a repository of  N  datasets, such that each P i  ⊂ ℝ d , where  d  is a constant. We study the percentile-aware indexing (Ptile) problem and the preference-aware indexing (Pref) problem under the centralized and the federated setting. In the centralized setting, we assume direct access to the datasets in  P . In the federated setting we are given a synopsis  S P i  which is a compressed representation of P i  that captures the structure of P i , for every i ∈ [N]. For the Ptile problem, the goal is to construct a data structure such that given a predicate (query rectangle R and an interval θ) report all indexes  J  such that j ∈ J if and only if |P j  ∩ R|/|P j | ∈ [N]. For the Ptile problem, the goal is to construct a data structure such that given a predicate (query vector v →  and an interval θ) report all indexes  J  such that j ∈ J if and only if ω k (P j ,v → )∈ θ, where ω k (p j ,v → ) is the score (inner-product) of the  k -th largest projection of P j  on v → . We first show lower bounds for the Ptile and Pref problems in the centralized setting, showing that we cannot hope for near-linear data structures with polylogarithmic query time. Then we focus on approximate data structures for both problems in both settings. We show Ø(N) space data structures with Ø(N) preprocessing time, that can answer Ptile and Pref queries in Ø(1+OUT) time, where OUT is the output size. The data structures return a set of indexes  J  such that: i) for every P i  that satisfies the predicate, i ∈ J and ii) if j ∈ J then P j  satisfies the predicate up to an additive error of ε+2δ, where ε is an arbitrarily small constant and δ is the error of the synopses.}
}


@article{DBLP:journals/pacmmod/AssadiS25,
	author = {Sepehr Assadi and
                  Vihan Shah},
	title = {An Improved Fully Dynamic Algorithm for Counting 4-Cycles in General
                  Graphs Using Fast Matrix Multiplication},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {91:1--91:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725228},
	doi = {10.1145/3725228},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AssadiS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study subgraph counting over fully dynamic graphs, which undergo edge insertions and deletions. Counting subgraphs is a fundamental problem in graph theory with numerous applications across various fields, including database theory, social network analysis, and computational biology. In database theory, we can use dynamic subgraph counting algorithms on layered graphs to maintain the sizes of joins of databases that undergo updates. Specifically, the problem of finding the number of elements in a cyclic join of size k is equivalent to counting the number of k-cycles in k-layered graphs. For example, let R, S, and T be relations that have schemas (A, B), (B, C), and (C, A) respectively. Then the size of the join of R with S with T is given by the number of triangles in the corresponding layered graph where there is a layer for each attribute, the vertices are the attribute values and the edges represent the tuples of attribute values in the relations. Maintaining the number of triangles in fully dynamic graphs is very well studied and has an upper bound of O(√m) for the update time [KNN+20]. There is also a conditional lower bound of Ω(m 1/2-γ ) for any constant γ>0, for the update time [HKNS15] under the Online Matrix-Vector (OMv) conjecture implying that O(√m) is the ''right answer' for the update time of counting triangles. More recently, [HHH22] studied the problem of maintaining the number of 4-cycles in fully dynamic graphs and designed an algorithm with O(m 2/3 ) update time which is a natural generalization of the approach for counting triangles. They also studied the problem of counting 4-cliques showing that the folklore upper bound of O(m) for the update time is tight under the static combinatorial 4-clique conjecture by giving a lower bound of Ω(m 1-γ ) for any γ>0. Thus, it seems natural that O(m 2/3  ) might be the correct answer for the complexity of the update time for counting 4-cycles. In this work, we present an improved algorithm for maintaining the number of 4-cycles in fully dynamic graphs. Our algorithm achieves a worst-case update time of O(m 2/3-ε ) for some constant ε>0. We also show that the problem of counting 4-cycles is equivalent in layered graphs and general graphs. Our approach crucially uses fast matrix multiplication and leverages recent developments therein to get an improved runtime. Using the current best value of the matrix multiplication exponent ω=2.371339 we get ε=0.009811 and if we assume the best possible exponent i.e. ω=2 then we get ε=1/24. There is also a lower bound of Ω(m 1/2-γ ) for any constant γ>0, for the update time [HKNS15,HHH22], so there is still a big gap between the best-known upper and lower bounds. The key message of our paper is demonstrating that O(m 2/3  ) is  not  the correct answer for the complexity of the update time.}
}


@article{DBLP:journals/pacmmod/FanKZ25,
	author = {Austen Z. Fan and
                  Paraschos Koutris and
                  Hangdong Zhao},
	title = {Circuit Bounds for Conjunctive Queries with Self-joins},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {92:1--92:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725229},
	doi = {10.1145/3725229},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanKZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study circuit size bounds for Conjunctive Queries (CQs) under different semiring semantics. Recent work established tight bounds for self-join-free CQs over the tropical semiring, among other results [16]. Here, we extend these results in two main directions. First, we prove a lower bound for any self-join-free CQ over the Boolean semiring by extending Razborov and Alon-Boppana's classic lower bound result for the  k -clique problem [1, 38]. Second, we characterize the circuit complexity of CQs with self-joins by relating them to appropriate self-join free CQs. Interestingly, such correspondence crucially depends on the underlying semiring. To achieve this result, we present a novel technique of investigating the circuit complexity of a CQ with self-joins through the lens of endomorphisms.}
}


@article{DBLP:journals/pacmmod/FanKR25,
	author = {Austen Z. Fan and
                  Paraschos Koutris and
                  Sudeepa Roy},
	title = {Circuits and Formulas for Datalog over Semirings},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {93:1--93:22},
	year = {2025},
	url = {https://doi.org/10.1145/3725230},
	doi = {10.1145/3725230},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanKR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study circuits and formulas for provenance polynomials of Datalog programs. We ask the following question: given an absorptive semiring and a fact of a Datalog program, what is the optimal depth and size of a circuit/formula that computes its provenance polynomial? We focus on absorptive semirings as these guarantee the existence of a polynomial-size circuit. Our main result is a dichotomy for several classes of Datalog programs on whether they admit a formula of polynomial size or not. We achieve this result by showing that for these Datalog programs the optimal circuit depth is either Θ(log  m ) or Θ(log 2 m ), where  m  is the input size. We also show that for Datalog programs with the polynomial fringe property, we can always construct low-depth circuits of size O(log 2 m ). Finally, we give characterizations of when Datalog programs are bounded over more general semirings.}
}


@article{DBLP:journals/pacmmod/GarciaR25,
	author = {Juli{\'{a}}n Garc{\'{\i}}a and
                  Cristian Riveros},
	title = {Complex Event Recognition under Time Constraints: Towards a Formal
                  Framework for Efficient Query Evaluation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {94:1--94:17},
	year = {2025},
	url = {https://doi.org/10.1145/3725231},
	doi = {10.1145/3725231},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GarciaR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Complex Event Recognition (CER) establishes a relevant solution for processing streams of events, giving users timely information. CER systems detect patterns in real-time, producing complex events and generating responses to them. In these tasks, time is a first-class citizen. Indeed, the time-sequence model distinguishes CER from other solutions, like data stream management systems. Surprisingly, until now, time constraints are usually included in CER query languages and models in a restricted way, and we still lack an understanding of the expressiveness and of efficient algorithms concerning this crucial feature: time. This work studies CER under time constraints regarding its query language, computational models, and streaming evaluation algorithms. We start by introducing an extension of Complex Event Logic (CEL), called timed CEL, with simple time operators. We show that timed CEL aids in modeling CER query languages in practice, serving as a proxy to study the expressive power of such languages under time constraints. For this purpose, we introduce an automata model for studying timed CEL, called timed Complex Event Automata (timed CEA). This model extends the existing CEA model with clocks, combining CEA and timed automata in a single model. We show that timed CEL and timed CEA are equally expressive, giving the first characterization of CER query languages under time constraints. Then, we move towards understanding the efficient evaluation of timed CEA over streams concerning its determinization and efficient algorithms. We present a class of timed CEA that are closed under determinization; furthermore, we show that this class contains swg-queries, an expressive class of CER queries recently introduced by Kleest-Meißner et al. Finally, we present a streaming evaluation algorithm with constant update time and output-linear delay for evaluating deterministic monotonic timed CEA with a single clock, which have only less equal or greater equal comparisons.}
}


@article{DBLP:journals/pacmmod/BernardiniBGS25,
	author = {Giulia Bernardini and
                  Philip Bille and
                  Inge Li G{\o}rtz and
                  Teresa Anna Steiner},
	title = {Differentially Private Substring and Document Counting},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {95:1--95:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725232},
	doi = {10.1145/3725232},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BernardiniBGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Differential privacy is the gold standard for privacy in data analysis. In many data analysis applications, the data is a database of  documents.  For databases consisting of many documents, one of the most fundamental problems is that of pattern matching and computing (i) how often a pattern appears as a substring in the database ( substring counting  ) and (ii) how many documents in the collection contain the pattern as a substring ( document counting  ). In this paper, we initiate the theoretical study of substring and document counting under differential privacy. We give an ε-differentially private data structure solving this problem for all patterns simultaneously with a maximum additive error of O(𝓁 • polylog(n𝓁|Σ|)), where 𝓁 is the maximum length of a document in the database,  n  is the number of documents, and |Σ| is the size of the alphabet. We show that this is optimal up to a O(polylog(𝓃𝓁)) factor. Further, we show that for (ε,δ)-differential privacy, the bound for document counting can be improved to O(√𝓁 • polylog(𝓃𝓁|Σ|)). Additionally, our data structures are efficient. In particular, our data structures use O(𝓃𝓁 2 ) space, O(𝓃 2 𝓁 4 ) preprocessing time, and O(|P|) query time where  P  is the query pattern. Along the way, we develop a new technique for differentially privately computing a general class of counting functions on trees of independent interest. Our data structures immediately lead to improved algorithms for related problems, such as privately mining frequent substrings and  q -grams. For  q -grams, we further improve the preprocessing time of the data structure.}
}


@article{DBLP:journals/pacmmod/ImMNP25,
	author = {Sungjin Im and
                  Benjamin Moseley and
                  Hung Q. Ngo and
                  Kirk Pruhs},
	title = {Efficient Algorithms for Cardinality Estimation and Conjunctive Query
                  Evaluation With Simple Degree Constraints},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {96:1--96:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725233},
	doi = {10.1145/3725233},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ImMNP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality estimation and conjunctive query evaluation are two of the most fundamental problems in database query processing. Recent work proposed, studied, and implemented a robust and practical information-theoretic cardinality estimation framework. In this framework, the estimator is the cardinality upper bound of a conjunctive query subject to ''degree-constraints'', which model a rich set of input data statistics. For general degree constraints, computing this bound is computationally hard. Researchers have naturally sought efficiently computable relaxed upper bounds that are as tight as possible. The polymatroid bound is the tightest among those relaxed upper bounds. While it is an open question whether the polymatroid bound can be computed in polynomial-time in general, it is known to be computable in polynomial-time for some classes of degree constraints. Our focus is on a common class of degree constraints called simple degree constraints. Researchers had not previously determined how to compute the polymatroid bound in polynomial time for this class of constraints. Our first main result is a polynomial time algorithm to compute the polymatroid bound given simple degree constraints. Our second main result is a polynomial-time algorithm to compute a ''proof sequence'' establishing this bound. This proof sequence can then be incorporated in the PANDA-framework to give a faster algorithm to evaluate a conjunctive query. In addition, we show computational limitations to extending our results to broader classes of degree constraints. Finally, our technique leads naturally to a new relaxed upper bound called the  flow bound,  which is computationally tractable.}
}


@article{DBLP:journals/pacmmod/BarceloKRSV25,
	author = {Pablo Barcel{\'{o}} and
                  Alexander Kozachinskiy and
                  Miguel Romero and
                  Bernardo Subercaseaux and
                  Jos{\'{e}} Verschae},
	title = {Explaining \emph{k}-Nearest Neighbors: Abductive and Counterfactual
                  Explanations},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {97:1--97:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725234},
	doi = {10.1145/3725234},
	timestamp = {Fri, 18 Jul 2025 07:44:47 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BarceloKRSV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite the wide use of  k -Nearest Neighbors as classification models, their explainability properties remain poorly understood from a theoretical perspective. While nearest neighbors classifiers offer interpretability from a ''data perspective'', in which the classification of an input vector x is explained by identifying the vectors v 1 , ..., v k  in the training set that determine the classification of x, we argue that such explanations can be impractical in high-dimensional applications, where each vector has hundreds or thousands of features and it is not clear what their relative importance is. Hence, we focus on understanding nearest neighbor classifications through a ''feature perspective'', in which the goal is to identify how the values of the features in x affect its classification. Concretely, we study abductive explanations such as ''minimum sufficient reasons'', which correspond to sets of features in x that are enough to guarantee its classification, and counterfactual explanations based on the minimum distance feature changes one would have to perform in x to change its classification. We present a detailed landscape of positive and negative complexity results for counterfactual and abductive explanations, distinguishing between discrete and continuous feature spaces, and considering the impact of the choice of distance function involved. Finally, we show that despite some negative complexity results, Integer Quadratic Programming and SAT solving allow for computing explanations in practice.}
}


@article{DBLP:journals/pacmmod/KhamisHS25,
	author = {Mahmoud Abo Khamis and
                  Xiao Hu and
                  Dan Suciu},
	title = {Fast Matrix Multiplication meets the Submodular Width},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {98:1--98:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725235},
	doi = {10.1145/3725235},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KhamisHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One fundamental question in database theory is the following: Given a Boolean conjunctive query  Q , what is the best complexity for computing the answer to Q in terms of the input database size  N ? When restricted to the class of combinatorial algorithms, it is known that the best known complexity for any query  Q  is captured by the  submodular width  of  Q . However, beyond combinatorial algorithms, certain queries are known to admit faster algorithms that often involve a clever combination of fast matrix multiplication and data partitioning. Nevertheless, there is no systematic way to derive and analyze the complexity of such algorithms for arbitrary queries  Q . In this work, we introduce a general framework that captures the best complexity for answering any Boolean conjunctive query  Q  using matrix multiplication. Our framework unifies both combinatorial and non-combinatorial techniques under the umbrella of information theory. It generalizes the notion of submodular width to a new stronger notion called the ω- submodular width  that naturally incorporates the power of fast matrix multiplication. We describe a matching algorithm that computes the answer to any query  Q  in time corresponding to the ω- submodular  width of  Q . We show that our framework recovers the best known complexities for Boolean queries that have been studied in the literature, to the best of our knowledge, and also discovers new algorithms for some classes of queries that improve upon the best known complexities.}
}


@article{DBLP:journals/pacmmod/RaskhodnikovaS25,
	author = {Sofya Raskhodnikova and
                  Teresa Anna Steiner},
	title = {Fully Dynamic Algorithms for Graph Databases with Edge Differential
                  Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {99:1--99:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725236},
	doi = {10.1145/3725236},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/RaskhodnikovaS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study differentially private algorithms for analyzing graph databases in the challenging setting of continual release with fully dynamic updates, where edges are inserted and deleted over time, and the algorithm is required to update the solution at every time step. Previous work has presented differentially private algorithms for many graph problems that can handle insertions only or deletions only (called partially dynamic algorithms) and obtained some hardness results for the fully dynamic setting. The only algorithms in the latter setting were for the edge count, given by Fichtenberger, Henzinger, and Ost (ESA '21), and for releasing the values of all graph cuts, given by Fichtenberger, Henzinger, and Upadhyay (ICML '23). We provide the first differentially private and fully dynamic graph algorithms for several other fundamental graph statistics (including the triangle count, the number of connected components, the size of the maximum matching, and the degree histogram), analyze their error, and show strong lower bounds on the error for all algorithms in this setting. Previously, only lower bounds for purely differentially private algorithms were known; our lower bounds give an exponential improvement in terms of the dependence on the number of time steps, while applying to algorithms satisfying pure as well as approximate differential privacy. We study two variants of edge differential privacy for fully dynamic graph algorithms: event-level and item-level. Under the former notion, two graph database update sequences are considered neighboring if, roughly speaking, they differ in at most one update; under the latter notion, they can differ only in updates pertaining to one edge. Differential privacy requires that for any two neighboring inputs, the output distributions of the algorithm are close. We give upper and lower bounds on the error of both---event-level and item-level---fully dynamic algorithms for several fundamental graph problems. No fully dynamic algorithms that are private at the item-level (the more stringent of the two notions) were known before. In the case of item-level privacy, for several problems, our algorithms match our lower bounds.}
}


@article{DBLP:journals/pacmmod/FigueiraMR25,
	author = {Diego Figueira and
                  R{\'{e}}mi Morvan and
                  Miguel Romero},
	title = {Minimizing Conjunctive Regular Path Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {100:1--100:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725237},
	doi = {10.1145/3725237},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FigueiraMR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the minimization problem for Conjunctive Regular Path Queries (CRPQs) and unions of CRPQs (UCRPQs). This is the problem of checking, given a query and a number  k , whether the query is equivalent to one of size at most  k . For CRPQs we consider the size to be the number of atoms, and for UCRPQs the maximum number of atoms in a CRPQ therein, motivated by the fact that the number of atoms has a leading influence on the cost of query evaluation. We show that the minimization problem is decidable, both for CRPQs and UCRPQs. We provide a 2ExpSpace upper-bound for CRPQ minimization, based on a brute-force enumeration algorithm, and an ExpSpace lower-bound. For UCRPQs, we show that the problem is ExpSpace-complete, having thus the same complexity as the classical containment problem. The upper bound is obtained by defining and computing a notion of maximal under-approximation. Moreover, we show that for UCRPQs using the so-called simple regular expressions consisting of concatenations of expressions of the form a +  or a 1  + ⋅⋅⋅ + a k , the minimization problem becomes "PiP2"-complete, again matching the complexity of containment.}
}


@article{DBLP:journals/pacmmod/LarroqueOT25,
	author = {Lucas Larroque and
                  Piotr Ostropolski{-}Nalewaja and
                  Micha{\"{e}}l Thomazo},
	title = {No Cliques Allowed: The Next Step Towards {BDD/FC} Conjecture},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {101:1--101:20},
	year = {2025},
	url = {https://doi.org/10.1145/3725238},
	doi = {10.1145/3725238},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LarroqueOT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses one of the fundamental open questions in the realm of existential rules: the conjecture on the finite controllability of bounded derivation depth rule sets (bdd⇒fc). We take a step toward a positive resolution of this conjecture by demonstrating that universal models generated by BDD rule sets cannot contain arbitrarily large tournaments (arbitrarily directed cliques) without entailing a loop query, ∃  E (x,x). This simple yet elegant result narrows the space of potential counterexamples to the (bdd⇒fc) conjecture.}
}


@article{DBLP:journals/pacmmod/KapralovMS25,
	author = {Michael Kapralov and
                  Mikhail Makarov and
                  Christian Sohler},
	title = {On the Adversarial Robustness of Locality-Sensitive Hashing in Hamming
                  Space},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {102:1--102:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725239},
	doi = {10.1145/3725239},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KapralovMS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Locality-sensitive hashing (Indyk-Motwani'98) is a classical data structure for approximate nearest neighbor search. It allows, after a close to linear time preprocessing of the input dataset, to find an approximately nearest neighbor of any fixed query in sublinear time in the dataset size. The resulting data structure is randomized and succeeds with high probability for every fixed query independent of the randomness of the data structure. In many modern applications of nearest neighbor search the queries are, however, chosen  adaptively.  In this paper, we study the robustness of locality-sensitive hashing in Hamming space to adaptive queries. We present a simple adversary that can, under mild assumptions on the initial point set, provably find a query to the approximate near neighbor search data structure that the data structure fails on. Crucially, our adaptive algorithm finds the hard query exponentially faster than random sampling.}
}


@article{DBLP:journals/pacmmod/PaghRWZ25,
	author = {Rasmus Pagh and
                  Lukas Retschmeier and
                  Hao Wu and
                  Hanwen Zhang},
	title = {Optimal Bounds for Private Minimum Spanning Trees via Input Perturbation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {103:1--103:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725240},
	doi = {10.1145/3725240},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PaghRWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of privately releasing an approximate minimum spanning tree (MST). Given a graph G = ( V ,  E ,  W ) where  V  is a set of  n  vertices,  E  is a set of  m  undirected edges, and  W  ∈ ℝ |E|  is an edge-weight vector, our goal is to publish an approximate MST under  edge-weight  differential privacy, as introduced by Sealfon in PODS 2016, where  V  and  E  are considered public and the weight vector is private. Our neighboring relation is 𝓁 ∞ -distance on weights: for a sensitivity parameter Δ ∞ , graphs G = ( V ,  E ,  W  ) and G' = ( V ,  E ,  W ') are neighboring if || W  - W '|| ∞  ≤ Δ ∞ ). Existing private MST algorithms face a trade-off, sacrificing either computational efficiency or accuracy. We show that it is possible to get the best of both worlds: With a suitable random perturbation of the input that does  not  suffice to make the weight vector private, the result of  any  non-private MST algorithm will be private and achieves a state-of-the-art error guarantee. Furthermore, by establishing a connection to  Private Top-k Selection  [Steinke and Ullman, FOCS '17], we give the first privacy-utility trade-off lower bound for MST under approximate differential privacy, demonstrating that the error magnitude, ~O(n 3/2 ), is optimal up to logarithmic factors. That is, our approach matches the time complexity of any non-private MST algorithm and at the same time achieves optimal error. We complement our theoretical treatment with experiments that confirm the practicality of our approach.}
}


@article{DBLP:journals/pacmmod/Hu25,
	author = {Xiao Hu},
	title = {Output-Optimal Algorithms for Join-Aggregate Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {104:1--104:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725241},
	doi = {10.1145/3725241},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Hu25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the most celebrated results of computing join-aggregate queries defined over commutative semi-rings is the classic Yannakakis algorithm proposed in 1981. It is known that the runtime of the Yannakakis algorithm is O(N + OUT) for any free-connex query, where  N  is the input size of the database and ØUT is the output size of the query result. This is already output-optimal. However, only an upper bound O(N • OUT) on the runtime is known for the large remaining class of acyclic but non-free-connex queries. Alternatively, one can convert a non-free-connex query into a free-connex one using tree decomposition techniques and then run the Yannakakis algorithm. This approach takes O(N #fn-subw  + OUT) time, where #fn-subw is the  free-connex sub-modular width  of the query. But, none of these results is known to be output-optimal. In this paper, we show a matching lower and upper bound Θ(N • OUT 1 - 1/(fn-fhtw)  + OUT) for computing general acyclic join-aggregate queries by  semiring algorithms,  where fn-fhtw is the  free-connex fractional hypertree width  of the query. For example, fn-fhtw = 1 for free-connex queries, fn-fhtw = 2 for line queries (a.k.a. chain matrix multiplication), and fn-fhtw = k for star queries (a.k.a. star matrix multiplication) with  k  relations. Although free-connex fractional hypertree width is a natural and well-established measure of how far a join-aggregate query is from being free-connex, we demonstrate that it precisely captures the output-optimal complexity of these queries. To our knowledge, this has been the first polynomial improvement over the Yannakakis algorithm in the last 40 years and completely resolves the open question of computing acyclic join-aggregate queries in an output-optimal way. As a by-product, our output-optimal algorithm for acyclic queries also yields new output-sensitive algorithms for cyclic queries via tree decomposition techniques.}
}


@article{DBLP:journals/pacmmod/KhamisKOS25,
	author = {Mahmoud Abo Khamis and
                  Ahmet Kara and
                  Dan Olteanu and
                  Dan Suciu},
	title = {Output-Sensitive Evaluation of Regular Path Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {105:1--105:20},
	year = {2025},
	url = {https://doi.org/10.1145/3725242},
	doi = {10.1145/3725242},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KhamisKOS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the classical evaluation problem for regular path queries: Given an edge-labeled graph and a regular path query, compute the set of pairs of vertices that are connected by paths that match the query. The Product Graph (PG) is the established evaluation approach for regular path queries. PG first constructs the product automaton of the data graph and the query and then uses breadth-first search to find the accepting states reachable from each initial state in the product automaton. Its data complexity is O(|V|⋅|E|), where V and E are the sets of vertices and respectively edges in the data graph. This complexity cannot be improved by combinatorial algorithms. In this paper, we introduce OSPG, an output-sensitive refinement of PG, whose data complexity is O(|E| 3/2  + min(OUT⋅√|E|, |V|⋅|E|)), where OUT is the number of distinct vertex pairs in the query output. OSPG's complexity is at most that of PG and can be asymptotically smaller for small output and sparse input. The improvement of OSPG over PG is due to the unnecessary time wasted by PG in the breadth-first search phase, in case a few output pairs are eventually discovered. For queries without Kleene star, the complexity of OSPG can be further improved to O(|E| + |E|⋅√OUT).}
}


@article{DBLP:journals/pacmmod/WoodruffXZ25,
	author = {David P. Woodruff and
                  Shenghao Xie and
                  Samson Zhou},
	title = {Perfect Sampling in Turnstile Streams Beyond Small Moments},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {106:1--106:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725243},
	doi = {10.1145/3725243},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WoodruffXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a vector  x  ∈ ℝ n  induced by a turnstile stream  S , a non-negative function G: ℝ → ℝ, a perfect  G -sampler outputs an index  i  with probability G(x i )/Σ j∈[n]  + 1/poly(n). Jayaram and Woodruff (FOCS 2018) introduced a perfect L p -sampler, where G(z)=|z| p , for  p ∈(0,2]. In this paper, we solve this problem for p>2 by a sampling-and-rejection method. Our algorithm runs in n 1-2/p  • polylog (n) bits of space, which is tight up to polylogarithmic factors in  n . Our algorithm also provides a (1+ε)-approximation to the sampled item x i  with high probability using an additional ε -2  n 1-2/p  • polylog (n) bits of space. Interestingly, we show our techniques can be generalized to perfect polynomial samplers on turnstile streams, which is a class of functions that is not scale-invariant, in contrast to the existing perfect L p  samplers. We also achieve perfect samplers for the logarithmic function G(z)=log(1+|z|) and the cap function G(z)=min(T,|z| p ). Finally, we give an application of our results to the problem of norm/moment estimation for a subset  Q  of coordinates of a vector, revealed only after the data stream is processed, e.g., when the set  Q  represents a range query, or the set n\\ Q  represents a collection of entities who wish for their information to be expunged from the dataset.}
}


@article{DBLP:journals/pacmmod/HollandCTX25,
	author = {Rayne Holland and
                  Seyit Camtepe and
                  Chandra Thapa and
                  Minhui Xue},
	title = {Private Synthetic Data Generation in Bounded Memory},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {107:1--107:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725244},
	doi = {10.1145/3725244},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HollandCTX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Protecting sensitive information on data streams is a pivotal challenge for modern systems. Current approaches to providing privacy in data streams can be broadly categorized into two strategies. The first strategy involves transforming the stream into a private sequence of values, enabling the subsequent use of non-private methods of analysis. While effective, this approach incurs high memory costs, often proportional to the size of the database. Alternatively, a compact data structure can be used to provide a private summary of the stream. However, these data structures are limited to predefined queries, restricting their flexibility. To overcome these limitations, we propose a lightweight synthetic data generator, PrivHP, that provides differential privacy guarantees. PrivHP is based on a novel method for the private hierarchical decomposition of the input domain in bounded memory. As the decomposition approximates the cumulative distribution function of the input, it serves as a lightweight structure for synthetic data generation. PrivHP is the first method to provide a principled trade-off between accuracy and space for private hierarchical decompositions. It achieves this by balancing hierarchy depth, noise addition, and selective pruning of low-frequency subdomains while preserving high-frequency ones, all identified in a privacy-preserving manner. To ensure memory efficiency, we employ private sketches to estimate subdomain frequencies without accessing the entire dataset. Central to our approach is the introduction of a pruning parameter  k , which enables an almost smooth interpolation between space usage and utility, and a measure of skew tail k , which is a vector of subdomain frequencies containing all but the largest  k  coordinates. PrivHP processes a dataset  X  using  M  =  O (k log 2  | X |)) space and, on input domain Ω = [0,1] d , while maintaining ε-differential privacy, produces a synthetic data generator that is at distance  O ( M (1-1/d) /ε n  + ||tail k ( X )|| 1 /M 1/d n ) from the empirical distribution in the expected Wasserstein metric. Compared to the state-of-the-art, PMM, which achieves accuracy  O  ((ε n) -1/d ) with memory  O (ε n), our method introduces an additional approximation error term of  O (||tail k ( X )|| 1 /(M 1/d n)), but operates in significantly reduced space. Additionally, we provide interpretable utility bounds that account for all error sources, including those introduced by the fixed hierarchy depth, privacy noise, hierarchy pruning, and frequency approximations.}
}


@article{DBLP:journals/pacmmod/AmarilliGMM25,
	author = {Antoine Amarilli and
                  Wolfgang Gatterbauer and
                  Neha Makhija and
                  Mika{\"{e}}l Monet},
	title = {Resilience for Regular Path Queries: Towards a Complexity Classification},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {108:1--108:18},
	year = {2025},
	url = {https://doi.org/10.1145/3725245},
	doi = {10.1145/3725245},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AmarilliGMM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The  resilience problem  for a query and an input set or bag database is to compute the minimum number of facts to remove from the database to make the query false. In this paper, we study how to compute the resilience of Regular Path Queries (RPQs) over graph databases. Our goal is to characterize the regular languages  L  for which it is tractable to compute the resilience of the existentially-quantified RPQ built from  L . We show that computing the resilience in this sense is tractable (even in combined complexity) for all RPQs defined from so-called  local languages . By contrast, we show hardness in data complexity for RPQs defined from the following language classes (after reducing the languages to eliminate redundant words): all finite languages featuring a word containing a repeated letter, and all languages featuring a specific kind of counterexample to being local (which we call  four-legged  languages). The latter include in particular all languages that are not  star-free . Our results also imply hardness for all non-local languages with a so-called  neutral letter . We last highlight some remaining obstacles towards a full dichotomy.}
}


@article{DBLP:journals/pacmmod/CarralGLT25,
	author = {David Carral and
                  Lukas Gerlach and
                  Lucas Larroque and
                  Micha{\"{e}}l Thomazo},
	title = {Restricted Chase Termination: You Want More than Fairness},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {109:1--109:17},
	year = {2025},
	url = {https://doi.org/10.1145/3725246},
	doi = {10.1145/3725246},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/CarralGLT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The chase is a fundamental algorithm with ubiquitous uses in database theory. Given a database and a set of existential rules (aka tuple-generating dependencies), it iteratively extends the database to ensure that the rules are satisfied in a most general way. This process may not terminate, and a major problem is to decide whether it does. This problem has been studied for a large number of chase variants, which differ by the conditions under which a rule is applied to extend the database. Surprisingly, the complexity of the universal termination of the restricted (aka standard) chase is not fully understood. We close this gap by placing universal restricted chase termination in the analytical hierarchy. This higher hardness is due to the fairness condition, and we propose an alternative condition to reduce the hardness of universal termination.}
}


@article{DBLP:journals/pacmmod/KolaitisPVW25,
	author = {Phokion G. Kolaitis and
                  Nina Pardal and
                  Jonni Virtema and
                  Jef Wijsen},
	title = {Rewriting Consistent Answers On Annotated Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {110:1--110:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725247},
	doi = {10.1145/3725247},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KolaitisPVW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We embark on a study of the consistent answers of queries over databases annotated with values from a naturally ordered positive semiring. In this setting, the consistent answers of a query are defined as the minimum of the semiring values that the query takes over all repairs of an inconsistent database. The main focus is on self-join free conjunctive queries and key constraints, which is the most extensively studied case of consistent query answering over standard databases. We introduce a variant of first-order logic with a limited form of negation, define suitable semiring semantics, and then establish the main result of the paper: the consistent query answers of a self-join free conjunctive query under key constraints are rewritable in this logic if and only if the attack graph of the query contains no cycles. This result generalizes an analogous result of Koutris and Wijsen for ordinary databases, but also yields new results for a multitude of semirings, including the bag semiring, the tropical semiring, and the fuzzy semiring. Further, for the bag semiring, we show that computing the consistent answers of any self-join free conjunctive query whose attack graph has a strong cycle is not only NP-hard but also it is NP-hard to even approximate the consistent answers with a constant relative approximation guarantee.}
}


@article{DBLP:journals/pacmmod/Zhang25,
	author = {Qin Zhang},
	title = {Robust Statistical Analysis on Streaming Data with Near-Duplicates
                  in General Metric Spaces},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {111:1--111:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725248},
	doi = {10.1145/3725248},
	timestamp = {Thu, 17 Jul 2025 11:27:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Zhang25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers statistical analysis on noisy datasets where near-duplicate elements need to be treated as identical ones. We focus on two basic problems, distinct elements and ℓ 0 -sampling, in the data stream model where the sequence of elements can only be scanned once using a limited space, under which a comprehensive data deduplication step before statistical analysis is not feasible. Previous streaming algorithms for these problems could only handle noisy datasets in  O (1)-dimensional Euclidean spaces. In this paper, we propose sublinear-space streaming algorithms that work for noisy datasets in any metric space. We also give a lower bound result showing that solving the distinct elements problem on noisy datasets in general metric spaces is inherently more difficult than solving it on noiseless datasets and on noisy datasets in  O (1)-dimensional Euclidean spaces.}
}


@article{DBLP:journals/pacmmod/BienvenuFL25,
	author = {Meghyn Bienvenu and
                  Diego Figueira and
                  Pierre Lafourcade},
	title = {Shapley Revisited: Tractable Responsibility Measures for Query Answers},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {112:1--112:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725249},
	doi = {10.1145/3725249},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BienvenuFL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Shapley value, originating from cooperative game theory, has been employed to define responsibility measures that quantify the contributions of database facts to obtaining a given query answer. For non-numeric queries, this is done by considering a cooperative game whose players are the facts and whose wealth function assigns 1 or 0 to each subset of the database, depending on whether the query answer holds in the given subset. While conceptually simple, this approach suffers from a notable drawback: the problem of computing such Shapley values is #P-hard in data complexity, even for simple conjunctive queries. This motivates us to revisit the question of what constitutes a reasonable responsibility measure and to introduce a new family of responsibility measures -- weighted sums of minimal supports (WSMS) -- which satisfy intuitive properties. Interestingly, while the definition of WSMSs is simple and bears no obvious resemblance to the Shapley value formula, we prove that every WSMS measure can be equivalently seen as the Shapley value of a suitably defined cooperative game. Moreover, WSMS measures enjoy tractable data complexity for a large class of queries, including all unions of conjunctive queries. We further explore the combined complexity of WSMS computation and establish (in)tractability results for various subclasses of conjunctive queries.}
}


@article{DBLP:journals/pacmmod/EsmailpourGHS25,
	author = {Aryan Esmailpour and
                  Boris Glavic and
                  Xiao Hu and
                  Stavros Sintos},
	title = {Smallest Synthetic Witnesses for Conjunctive Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {113:1--113:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725250},
	doi = {10.1145/3725250},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/EsmailpourGHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a self-join-free conjunctive query  Q  and a set of tuples  S , a  synthetic witness D  is a database instance such that the result of  Q  on  D  is  S . In this work, we are interested in two problems. First, the existence problem ESW decides whether any synthetic witness  D  exists. Second, given that a synthetic witness exists, the minimization problem SSW computes a synthetic witness of minimal size. The SSW problem is related to the  smallest witness problem  recently studied by Hu and Sintos [22]; however, the objective and the results are inherently different. More specifically, we show that SSW is poly-time solvable for a wider range of queries. Interestingly, in some cases, SSW is related to optimization problems in other domains, such as the  role mining  problem in data mining and the  edge concentration  problem in graph drawing. Solutions to ESW and SSW are of practical interest, e.g., for  test database generation  for applications accessing a database and for  data compression  by encoding a dataset  S  as a pair of a query  Q  and database  D . We prove that ESW is in P, presenting a simple algorithm that, given any  S , decides whether a synthetic witness exists in polynomial time in the size of  S . Next, we focus on the SSW problem. We show an algorithm that computes a minimal synthetic witness in polynomial time with respect to the size of  S  for any query  Q  that has the  head-domination  property. If  Q  does not have such a property, then SSW is generally hard. More specifically, we show that for the class of  path queries  (of any constant length), SSW cannot be solved in polynomial time unless P = NP. We then extend this hardness result to the class of  Berge-acyclic  queries that do not have the head-domination property, obtaining a full dichotomy of SSW for Berge-acyclic queries. Finally, we investigate the hardness of SSW beyond Berge-acyclic queries by showing that SSW cannot be solved in polynomial time for some cyclic queries unless P = NP.}
}


@article{DBLP:journals/pacmmod/LanzingerOPSG25,
	author = {Matthias Lanzinger and
                  Cem Okulmus and
                  Reinhard Pichler and
                  Alexander Selzer and
                  Georg Gottlob},
	title = {Soft and Constrained Hypertree Width},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {114:1--114:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725251},
	doi = {10.1145/3725251},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LanzingerOPSG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hypertree decompositions provide a way to evaluate Conjunctive Queries (CQs) in polynomial time, where the exponent of this polynomial is determined by the width of the decomposition. In theory, the goal of efficient CQ evaluation therefore has to be a minimisation of the width. However, in practical settings, it turns out that there are also other properties of a decomposition that influence the performance of query evaluation. It is therefore of interest to restrict the computation of decompositions by constraints and to guide this computation by preferences. To this end, we propose a novel framework based on candidate tree decompositions, which allows us to introduce soft hypertree width (shw). This width measure is a relaxation of hypertree width (hw); it is never greater than hw and, in some cases, shw may actually be lower than hw. Most importantly, shw preserves the tractability of deciding if a given CQ is below some fixed bound, while offering more algorithmic flexibility. In particular, it provides a natural way to incorporate preferences and constraints into the computation of decompositions. A prototype implementation and preliminary experiments confirm that this novel framework can indeed have a practical impact on query evaluation.}
}


@article{DBLP:journals/pacmmod/BuzzegaCKKP25,
	author = {Giovanni Buzzega and
                  Alessio Conte and
                  Yasuaki Kobayashi and
                  Kazuhiro Kurita and
                  Giulia Punzi},
	title = {The Complexity of Maximal Common Subsequence Enumeration},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {115:1--115:19},
	year = {2025},
	url = {https://doi.org/10.1145/3725252},
	doi = {10.1145/3725252},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BuzzegaCKKP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Frequent pattern mining is widely used to find "important" or "interesting" patterns in data. While it is not easy to mathematically define such patterns, maximal frequent patterns are promising candidates, as frequency is a natural indicator of relevance and maximality helps to summarize the output. As such, their mining has been studied on various data types, including itemsets, graphs, and strings. The complexity of mining maximal frequent itemsets and subtrees has been thoroughly investigated (e.g., [Boros et al., 2003], [Uno et al., 2004]) in the literature. On the other hand, while the idea of mining frequent subsequences in sequential data was already introduced in the seminal paper [Agrawal et al., 1995], the complexity of the problem is still open. In this paper, we investigate the complexity of the maximal common subsequence enumeration problem, which is both an important special case of maximal frequent subsequence mining and a generalization of the classic longest common subsequence (LCS) problem. We show the hardness of enumerating maximal common subsequences between multiple strings, ruling out the possibility of an  output-polynomial time  enumeration algorithm under ¶ ≠ NP, that is, an algorithm that runs in time poly(| I | +  N ), where | I | and  N  are the size of the input and number of output solutions, respectively. To circumvent this intractability, we also investigate the parameterized complexity of the problem, and show several results when the alphabet size, the number of strings, and the length of a string are taken into account as parameters.}
}


@article{DBLP:journals/pacmmod/MeelC25,
	author = {Kuldeep S. Meel and
                  Alexis de Colnet},
	title = {Towards Practical {FPRAS} for {\#}NFA: Exploiting the Power of Dependence},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {116:1--116:23},
	year = {2025},
	url = {https://doi.org/10.1145/3725253},
	doi = {10.1145/3725253},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MeelC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {#NFA refers to the problem of counting the words of length  n  accepted by a non-deterministic finite automaton. #NFA is #P-hard, and although fully-polynomial-time randomized approximation schemes (FPRAS) exist, they are all impractical. The first FPRAS for #NFA had a running time of Õ(n 17  m 17  ε -14  łog(δ -1  )), where  m  is the number of states in the automaton, δ ∈ (0,1] is the confidence parameter, and ε > 0 is the tolerance parameter (typically smaller than 1). The current best FPRAS achieved a significant improvement in the time complexity relative to the first FPRAS and obtained FPRAS with time complexity Õ((n 10  m 2  + n 6 m 3 )ε -4  łog 2 (δ -1  )). The complexity of the improved FPRAS is still too intimidating to attempt any practical implementation. In this paper, we pursue the quest for practical FPRAS for #NFA by presenting a new algorithm with a time complexity of O(n 2 m 3 łog(nm)ε -2  łog(δ -1  )). Observe that evaluating whether a word of length  n  is accepted by an NFA has a time complexity of O(nm 2 ). Therefore, our proposed FPRAS achieves sub-quadratic complexity with respect to membership checks.}
}


@article{DBLP:journals/pacmmod/HuW25,
	author = {Xiao Hu and
                  Qichen Wang},
	title = {Towards Update-Dependent Analysis of Query Maintenance},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {2},
	pages = {117:1--117:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725254},
	doi = {10.1145/3725254},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies the hardness of maintaining self-join-free conjunctive queries over a dynamic database, where tuples can be inserted or deleted. The worst-case complexity of this problem under arbitrary updates has been well understood. It is known that most practical queries require Ω(√|D|) maintenance time for each update to ensure O(1)-delay enumeration, barring a very restricted class of queries (known as "q-hierarchical" queries). Nonetheless, most real-world update sequences are not arbitrary, far away from the worst-case scenario; instead, they are so "nice" that queries can greatly benefit from their inherent structure in query maintenance. In this paper, we aim to understand the hardness of query maintenance under different update sequences, in particular, the insertion-only (or deletion-only), first-in-first-out (FIFO), arbitrarily worse sequences, as well as their "mixed" sequences. We first provide a comprehensive characterization of queries that can be maintained in O(1) time for O(1)-delay enumeration over FIFO sequences. Then, we address mixed sequences, which may exhibit insertion-only or FIFO patterns on subqueries but lack a specific pattern in totality, and introduce a structural dichotomy for determining whether the input query can be maintained in O(1) time for O(1)-delay enumeration over mixed sequences.}
}


@article{DBLP:journals/pacmmod/AbouziedHA25a,
	author = {Azza Abouzied and
                  Joseph M. Hellerstein and
                  Divyakant Agrawal},
	title = {{PACMMOD} V3, {N3} (SIGMOD), June 2025: Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {118:1--118:2},
	year = {2025},
	url = {https://doi.org/10.1145/3725255},
	doi = {10.1145/3725255},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AbouziedHA25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Proceedings of the ACM on Management of Data (PACMMOD) is concerned with the principles, algorithms, techniques, systems, and applications of database management systems, data management technology, and science and engineering of data. It includes articles reporting cutting-edge data management, data engineering, and data science research. We are pleased to present the third issue of Volume 3 of PACMMOD. This issue contains papers that were submitted to the SIGMOD research track in October 2024. Papers accepted in this issue form the final pool of presentation candidates in the research track of the ACM SIGMOD Conference on Management of Data 2025, to be held in Berlin, Germany.}
}


@article{DBLP:journals/pacmmod/BarbosaGPSCJF25,
	author = {Juliana Silva Barbosa and
                  Ulhas Gondhali and
                  Gohar Petrossian and
                  Kinshuk Sharma and
                  Sunandan Chakraborty and
                  Jennifer Jacquet and
                  Juliana Freire},
	title = {A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking
                  in Online Marketplaces},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {119:1--119:23},
	year = {2025},
	url = {https://doi.org/10.1145/3725256},
	doi = {10.1145/3725256},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BarbosaGPSCJF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wildlife trafficking remains a critical global issue, significantly impacting biodiversity, ecological stability, and public health. Despite efforts to combat this illicit trade, the rise of e-commerce platforms has made it easier to sell wildlife products, putting new pressure on wild populations of endangered and threatened species. The use of these platforms also opens a new opportunity: as criminals sell wildlife products online, they leave digital traces of their activity that can provide insights into trafficking activities as well as how they can be disrupted. The challenge lies in finding these traces. Online marketplaces publish ads for a plethora of products, and identifying ads for wildlife-related products is like finding a needle in a haystack. Learning classifiers can automate ad identification, but creating them requires costly, time-consuming data labeling that hinders support for diverse ads and research questions. This paper addresses a critical challenge in the data science pipeline for wildlife trafficking analytics: generating quality labeled data for classifiers that select relevant data. While large language models (LLMs) can directly label advertisements, doing so at scale is prohibitively expensive. We propose a cost-effective strategy that leverages LLMs to generate pseudo labels for a small sample of the data and uses these labels to create specialized classification models. Our novel method automatically gathers diverse and representative samples to be labeled while minimizing the labeling costs. Our experimental evaluation shows that our classifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We present real use cases that demonstrate the effectiveness of our approach in enabling analyses of different aspects of wildlife trafficking.}
}


@article{DBLP:journals/pacmmod/WangLYHY25,
	author = {Taiyi Wang and
                  Liang Liang and
                  Guang Yang and
                  Thomas Heinis and
                  Eiko Yoneki},
	title = {A New Paradigm in Tuning Learned Indexes: {A} Reinforcement Learning
                  Enhanced Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {120:1--120:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725257},
	doi = {10.1145/3725257},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangLYHY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learned Index Structures (LIS) have significantly advanced data management by leveraging machine learning models to optimize data indexing. However, designing these structures often involves critical trade-offs, making it challenging for both designers and end-users to find an optimal balance tailored to specific workloads and scenarios. While some indexes offer adjustable parameters that demand intensive manual tuning, others rely on fixed configurations based on heuristic auto-tuners or expert knowledge, which may not consistently deliver optimal performance. This paper introduces LIT une , a novel framework for end-to-end automatic tuning of Learned Index Structures. LIT une  employs an adaptive training pipeline equipped with a tailor-made Deep Reinforcement Learning (DRL) approach to ensure stable and efficient tuning. To accommodate long-term dynamics arising from online tuning, we further enhance LIT une  with an on-the-fly updating mechanism termed the O2 system. These innovations allow LIT une  to effectively capture state transitions in online tuning scenarios and dynamically adjust to changing data distributions and workloads, marking a significant improvement over other tuning methods. Our experimental results demonstrate that LIT une  achieves up to a 98% reduction in runtime and a 17-fold increase in throughput compared to default parameter settings given a selected Learned Index instance. These findings highlight LIT une 's effectiveness and its potential to facilitate broader adoption of LIS in real-world applications.}
}


@article{DBLP:journals/pacmmod/dHondtLYPP25,
	author = {Jens E. d'Hondt and
                  Haojun Li and
                  Fan Yang and
                  Odysseas Papapetrou and
                  John Paparrizos},
	title = {A Structured Study of Multivariate Time-Series Distance Measures},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {121:1--121:29},
	year = {2025},
	url = {https://doi.org/10.1145/3725258},
	doi = {10.1145/3725258},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/dHondtLYPP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distance measures are fundamental to time series analysis and have been extensively studied for decades. Until now, research efforts mainly focused on univariate time series, leaving multivariate cases largely under-explored. Furthermore, the existing experimental studies on multivariate distances have critical limitations: (a) focusing only on lock-step and elastic measures while ignoring categories such as sliding and kernel measures; (b) considering only one normalization technique; and (c) placing limited focus on statistical analysis of findings. Motivated by these shortcomings, we present the most complete evaluation of multivariate distance measures to date. Our study examines 30 standalone measures across 8 categories, 2 channel-dependency models, and considers 13 normalizations. We perform a comprehensive evaluation across 30 datasets and 3 downstream tasks, accompanied by rigorous statistical analysis. To ensure fairness, we conduct a thorough investigation of parameters for methods in both a supervised and an unsupervised manner. Our work verifies and extends earlier findings, showing that insights from univariate distance measures also apply to the multivariate case: (a) alternative normalization methods outperform Z-score, and for the first time, we demonstrate statistical differences in certain categories for the multivariate case; (b) multiple lock-step measures are better suited than Euclidean distance, when it comes to multivariate time series; and (c) newer elastic measures outperform the widely adopted Dynamic Time Warping distance, especially with proper parameter tuning in the supervised setting. Moreover, our results reveal that (a) sliding measures offer the best trade-off between accuracy and runtime; (b) current normalization techniques fail to significantly enhance accuracy on multivariate time series and, surprisingly, do not outperform the no normalization case, indicating a lack of appropriate solutions for normalizing multivariate time series; and (c) independent consideration of time series channels is beneficial only for elastic measures. In summary, we offer guidelines to aid in designing and selecting preprocessing strategies and multivariate distance measures for our community.}
}


@article{DBLP:journals/pacmmod/YangY25,
	author = {Yifei Yang and
                  Xiangyao Yu},
	title = {Accelerate Distributed Joins with Predicate Transfer},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {122:1--122:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725259},
	doi = {10.1145/3725259},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Join is one of the most critical operators in query processing. One effective way to optimize multi-join performance is to pre-filter rows that do not contribute to the query output. Techniques that reflect this principle include predicate pushdown, semi-join, Yannakakis algorithm, Bloom join, predicate transfer, etc. Among these, predicate transfer is the state-of-the-art pre-filtering technique that removes most non-contributing rows through a series of Bloom filters thereby delivering significant performance improvement. However, the existing predicate transfer technique has several limitations. First, the current algorithm works only on a single-threaded system while real analytics databases for large workloads are typically distributed across multiple nodes. Second, some predicate transfer steps may not filter out any rows in the destination table thus introduces performance overhead with no speedup. This issue is exacerbated in a distributed environment, where unnecessary predicate transfers lead to extra network latency and traffic. In this paper, we aim to address both limitations. First, we explore the design space of distributed predicate transfer and propose cost-based adaptive execution to maximize the performance for each individual transfer step. Second, we develop a pruning algorithm to effectively remove unnecessary transfers that do not have positive contribution to performance. We implement both techniques and evaluate on a distributed analytics query engine. Results on standard OLAP benchmarks including TPC-H and DSB with a scale factor up to 400 show that distributed predicate transfer can improve the query performance by over 3×, and reduce the amount of data exchange by over 2.7×.}
}


@article{DBLP:journals/pacmmod/WangWKGZZ25,
	author = {Mengzhao Wang and
                  Haotian Wu and
                  Xiangyu Ke and
                  Yunjun Gao and
                  Yifan Zhu and
                  Wenchao Zhou},
	title = {Accelerating Graph Indexing for {ANNS} on Modern CPUs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {123:1--123:29},
	year = {2025},
	url = {https://doi.org/10.1145/3725260},
	doi = {10.1145/3725260},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangWKGZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS) is a key component in database and artificial intelligence infrastructures. Graph-based ANNS methods, particularly HNSW, have emerged as leading solutions, offering an impressive trade-off between search efficiency and accuracy. Many vector databases utilize graph indexes as their core algorithms, benefiting from various optimizations to enhance search performance. However, the high indexing time associated with graph algorithms poses a significant challenge, especially given the increasing volume of data, query processing complexity, and dynamic index maintenance demand. This has rendered indexing time a critical performance metric for users. In this paper, we comprehensively analyze the underlying causes of the low graph indexing efficiency on modern CPUs, identifying that distance computation dominates indexing time, primarily due to high memory access latency and suboptimal arithmetic operation efficiency. We demonstrate that distance comparisons during index construction can be effectively performed using compact vector codes at an appropriate compression error. Drawing from insights gained through integrating existing compact coding methods in the graph indexing process, we propose a novel compact coding strategy, named  Flash,  designed explicitly for graph indexing and optimized for modern CPU architectures. By minimizing random memory accesses and maximizing the utilization of SIMD (Single Instruction, Multiple Data) instructions,  Flash  significantly enhances cache hit rates and arithmetic operations. Extensive experiments conducted on eight real-world datasets, ranging from ten million to one billion vectors, exhibit that  Flash  achieves a speedup of 10.4× to 22.9× in index construction efficiency, while maintaining or improving search performance.}
}


@article{DBLP:journals/pacmmod/ZengFLM25,
	author = {Yuanyuan Zeng and
                  Yixiang Fang and
                  Wensheng Luo and
                  Chenhao Ma},
	title = {Accelerating Skyline Path Enumeration with a Core Attribute Index
                  on Multi-attribute Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {124:1--124:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725261},
	doi = {10.1145/3725261},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZengFLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a building block of many graph-based areas, the  s-t  path enumeration problem aims to find all paths between  s  and  t  by satisfying a given constraint, e.g., hop numbers. In many real-world scenarios, graphs are multi-attribute, where vertices and edges are associated with numerical attributes, such as expense or distance in road networks. However, existing methods have not fully leveraged all attributes in  s-t  path analysis. Hence, in this paper, we study the problem of skyline path enumeration, which aims to identify paths that balance multiple attributes, ensuring that no skyline result is dominated by another, thus meeting diverse user needs. To efficiently tackle this problem, we design a task-oriented core attribute index, called CAI, to rule out all redundant vertices and edges not located in any skyline path. Additionally, we introduce a hop-dependency label propagation strategy to construct the CAI index in parallel, improving the indexing process. Based on this index, we further design a CAI-based querying strategy that reduces fruitless explorations between candidate vertices not in the same skyline path, significantly optimizing query processing time. Experimental evaluations on fifteen real-world graphs show that CAI outperforms existing methods by up to four orders of magnitude in speed while demonstrating enhanced scalability and well-bound memory costs.}
}


@article{DBLP:journals/pacmmod/LuYWXZC25,
	author = {Kuan Lu and
                  Zhihui Yang and
                  Sai Wu and
                  Ruichen Xia and
                  Dongxiang Zhang and
                  Gang Chen},
	title = {Adda: Towards Efficient in-Database Feature Generation via LLM-based
                  Agents},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {125:1--125:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725262},
	doi = {10.1145/3725262},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuYWXZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrating machine learning (ML) analytics into existing database management systems (DBMSs) not only eliminates the need for costly data transfers to external ML platforms but also ensures compliance with regulatory standards. While some DBMSs have integrated functionalities for training and applying ML models for analytics, these tasks still present challenges, particularly due to limited support for automatic feature engineering (AutoFE), which is crucial for optimizing ML model performance. In this paper, we introduce Adda, an agent-driven in-database feature generation tool designed to automatically create high-quality features for ML analytics directly within the database. Adda interprets ML analytics tasks described in natural language and generates code for feature construction by leveraging the power of large language models (LLMs) integrated with specialized agents. This code is then translated into SQL statements using a predefined set of operators and compiled just-in-time (JIT) into user-defined functions (UDFs). The result is a seamless, fully in-database solution for feature generation, specifically tailored for ML analytics tasks. Extensive experiments across 14 public datasets, with five ML tasks per dataset, show that Adda improves the AUC by up to 33.2% and reduces end-to-end latency by up to 100x compared to Madlib.}
}


@article{DBLP:journals/pacmmod/YeGZZ25,
	author = {Xinyi Ye and
                  Xiangyang Gou and
                  Lei Zou and
                  Wenjie Zhang},
	title = {{AJOSC:} Adaptive Join Order Selection for Continuous Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {126:1--126:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725263},
	doi = {10.1145/3725263},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YeGZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-way join, which refers to the join operation among multiple tables, is widely used in database systems. With the development of the Internet and social networks, a new variant of the multi-way join query has emerged, requiring continuous monitoring of the query results as the database is updated. This variant is called continuous multi-way join. The join order of continuous multi-way join significantly impacts the operation's cost. However, existing methods for continuous multi-way join order selection are heuristic, which may fail to select the most efficient orders. On the other hand, the high-cost order computation will become a system bottleneck if we directly transfer join order selection algorithms for static multi-way join to the dynamic setting. In this paper, we propose a new  A  daptive  J  oin  O  rder  S  election algorithm for the  C  ontinuous multi-way join queries named AJOSC. It uses dynamic programming to find the optimal join order with a new cost model specifically designed for continuous multi-way join. We further propose a lower-bound-based incremental re-optimization algorithm to restrict the search space and recompute the join order with low cost when data distribution changes. Experimental results show that AJOSC is up to two orders of magnitude faster than the state-of-the-art methods.}
}


@article{DBLP:journals/pacmmod/StrassenburgGR25,
	author = {Nils Strassenburg and
                  Boris Glavic and
                  Tilmann Rabl},
	title = {Alsatian: Optimizing Model Search for Deep Transfer Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {127:1--127:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725264},
	doi = {10.1145/3725264},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/StrassenburgGR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transfer learning is an effective technique for tuning a deep learning model when training data or computational resources are limited. Instead of training a new model from scratch, the parameters of an existing base model are adjusted for the new task. The accuracy of such a fine-tuned model depends on the suitability of the base model chosen. Model search automates the selection of such a base model by evaluating the suitability of candidate models for a specific task. This entails inference with each candidate model on task-specific data. With thousands of models available through model stores, the computational cost of model search is a major bottleneck for efficient transfer learning. In this work, we present  Alsatian , a novel model search system. Based on the observation that many candidate models overlap to a significant extent and following a careful bottleneck analysis, we propose optimization techniques that are applicable to many model search frameworks. These optimizations include: (i) splitting models into individual blocks that can be shared across models, (ii) caching of intermediate inference results and model blocks, and (iii) selecting a beneficial search order for models to maximize sharing of cached results. In our evaluation on state-of-the-art deep learning models from computer vision and natural language processing, we show that  Alsatian  outperforms baselines by up to 14x.}
}


@article{DBLP:journals/pacmmod/QiuY25,
	author = {Yuan Qiu and
                  Ke Yi},
	title = {Approximate {DBSCAN} under Differential Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {128:1--128:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725265},
	doi = {10.1145/3725265},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/QiuY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper revisits the DBSCAN problem under differential privacy (DP). Existing DP-DBSCAN algorithms aim at publishing the cluster labels of the input points. However, we show that both empirically and theoretically, this approach cannot offer any utility in the published results. We therefore propose an alternative definition of DP-DBSCAN based on the notion of spans. We argue that publishing the spans actually better serves the purposes of visualization and classification of DBSCAN. Then we present a linear-time DP-DBSCAN algorithm achieving the sandwich quality guarantee in any constant dimensions, as well as matching lower bounds on the approximation ratio. A key building block in our algorithm is a linear-time algorithm for constructing a histogram under pure-DP, which is of independent interest. Finally, we conducted experiments on both synthetic and real-world datasets to verify the practical performance of our DP-DBSCAN algorithm.}
}


@article{DBLP:journals/pacmmod/ChangN25,
	author = {Jiwon Chang and
                  Fatemeh Nargesian},
	title = {Approximating Opaque Top-k Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {129:1--129:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725266},
	doi = {10.1145/3725266},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChangN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Combining query answering and data science workloads has become prevalent. An important class of such workloads is top-k queries with a scoring function implemented as an opaque UDF - a black box whose internal structure and scores on the search domain are unavailable. Some typical examples include costly calls to fuzzy classification and regression models. The models may also be changed in an ad-hoc manner. Since the algorithm does not know the scoring function's behavior on the input data, opaque top-k queries become expensive to evaluate exactly or speed up by indexing. Hence, we propose an approximation algorithm for opaque top-k query answering. Our proposed solution is a task-independent hierarchical index and a novel bandit algorithm. The index clusters elements by some cheap vector representation then builds a tree of the clusters. Our bandit is a diminishing returns submodular epsilon-greedy bandit algorithm that maximizes the sum of the solution set's scores. Our bandit models the distribution of scores in each arm using a histogram, then targets arms with fat tails. We prove that our bandit algorithm approaches a constant factor of the optimal algorithm. We evaluate our standalone library on large synthetic, image, and tabular datasets over a variety of scoring functions. Our method accelerates the time required to achieve nearly optimal scores by up to an order of magnitude compared to exhaustive scan while consistently outperforming baseline sampling algorithms.}
}


@article{DBLP:journals/pacmmod/GaoZSC25,
	author = {Shihong Gao and
                  Xin Zhang and
                  Yanyan Shen and
                  Lei Chen},
	title = {Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable
                  {LLM} Inference Serving},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {130:1--130:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725394},
	doi = {10.1145/3725394},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GaoZSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language model (LLM) inference serving systems are essential to various LLM-based applications. As demand for LLM services continues to grow, scaling these systems to handle high request rates while meeting latency Service-Level Objectives (SLOs), referred to as effective throughput, becomes critical. However, existing systems often struggle to improve effective throughput, primarily due to a significant decline in Time To First Token (TTFT) SLO attainment. We identify two major causes of this bottleneck: (1) memory-intensive KV cache that limits batch size expansion under GPU memory constraints, and (2) rigid batch composition enforced by the default First-Come-First-Serve scheduling policy. In this paper, we introduce Apt-Serve, a scalable framework designed to enhance effective throughput in LLM inference serving. Apt-Serve features a new hybrid cache scheme that combines KV cache with a memory-efficient hidden cache for reusable input hidden state vectors, allowing large batch sizes and improving request concurrency. Based on the hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism that dynamically optimizes batch composition. We formally define the adaptive scheduling optimization problem and propose an efficient algorithm with theoretical guarantees. Extensive evaluations on three real-world datasets and LLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up to 8.8x improvement in effective throughput compared to the state-of-the-art inference serving systems.}
}


@article{DBLP:journals/pacmmod/NguyenCDA25,
	author = {Cuong D. T. Nguyen and
                  Kevin Chen and
                  Christopher DeCarolis and
                  Daniel J. Abadi},
	title = {Are Database System Researchers Making Correct Assumptions about Transaction
                  Workloads?},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {131:1--131:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725268},
	doi = {10.1145/3725268},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/NguyenCDA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many recent papers have contributed novel concurrency control and transaction processing algorithms that start by making an assumption about the transaction workload submitted by an application, and yield high performance (sometimes by an order of magnitude) under these assumptions. Two of the most common assumptions are (1) Is the read and write set of a transaction known (or easily derivable) directly by analyzing the application code in advance of transaction execution, or is the access set of a transaction dependent on the current state of the database? (2) Does the application send the entire transaction in a single request to the database system or is the transaction sent via several requests ''interactively'', with application code run in between these requests. The database community has made tremendous progress in improving throughput and latency of transaction processing when read/write sets are known in advance, and for non-interactive transactions. However, the impact of this progress is directly dependent on the accuracy of these assumptions both for current and future applications. In this paper, we conduct an extensive study of 111 open-source applications, analyzing over 30,000 transactions to evaluate the accuracy of these assumptions both as they exist in the current codebase, and how extensive are the changes required to the code for these assumptions to hold moving forward. Our study reveals that the second of these assumptions is stronger than the first. More specifically, for 90% of applications, at least 58% of transactions per application have read/write sets that can be inferred in advance. Furthermore, although only 39% of applications contain zero interactive transactions, nonetheless, the majority of the remaining 61% of applications can be converted to being completely non-interactive with minimal changes.These insights underscore the potential for further optimization and research in designing OLTP systems that balance transaction expressivity and performance.}
}


@article{DBLP:journals/pacmmod/LiLLMLT25,
	author = {Runzhong Li and
                  Qilong Li and
                  Haotian Liu and
                  Rui Mao and
                  Qing Li and
                  Bo Tang},
	title = {Athena: An Effective Learning-based Framework for Query Optimizer
                  Performance Improvement},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {132:1--132:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725395},
	doi = {10.1145/3725395},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiLLMLT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent studies have made it possible to integrate learning techniques into database systems for practical utilization. In particular, the state-of-the-art studies hook the conventional query optimizer to explore multiple execution plan candidates, then choose the optimal one with a learned model. This framework simplifies the integration of learning techniques into the database system. However, these methods still have room for improvement due to their limited plan exploration space and ineffective learning from execution plans. In this work, we propose Athena, an effective learning-based framework of query optimizer enhancer. It consists of three key components: (i) an order-centric plan explorer, (ii) a Tree-Mamba plan comparator and (iii) a time-weighted loss function. We implement Athena on top of the open-source database PostgreSQL and demonstrate its superiority via extensive experiments. Specifically, We achieve 1.75x, 1.95x, 5.69x, and 2.74x speedups over the vanilla PostgreSQL on the JOB, STATS-CEB, TPC-DS, and DSB benchmarks, respectively. Athena is 1.74x, 1.87x, 1.66x, and 2.28x faster than the state-of-the-art competitor Lero on these benchmarks. Additionally, Athena is open-sourced and it can be easily adapted to other relational database systems as all these proposed techniques in Athena are generic.}
}


@article{DBLP:journals/pacmmod/ChenHWCGZZC25,
	author = {Qixu Chen and
                  Yeye He and
                  Raymond Chi{-}Wing Wong and
                  Weiwei Cui and
                  Song Ge and
                  Haidong Zhang and
                  Dongmei Zhang and
                  Surajit Chaudhuri},
	title = {Auto-Test: Learning Semantic-Domain Constraints for Unsupervised Error
                  Detection in Tables},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {133:1--133:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725396},
	doi = {10.1145/3725396},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenHWCGZZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data cleaning is a long-standing challenge in data management. While powerful logic and statistical algorithms have been developed to detect and repair data errors in tables, existing algorithms predominantly rely on domain-experts to first manually specify data-quality constraints specific to a given table, before data cleaning algorithms can be applied. In this work, we observe that there is an important class of data-quality constraints that we call  Semantic-Domain Constraints,  which can be reliably inferred and automatically applied to  any tables,  without requiring domain-experts to manually specify on a per-table basis. We develop a principled framework to systematically learn such constraints from table corpora using large-scale statistical tests, which can further be distilled into a core set of constraints using our optimization framework, with provable quality guarantees. Extensive evaluations show that this new class of constraints can be used to both (1) directly detect errors on real tables in the wild, and (2) augment existing expert-driven data-cleaning techniques as a new class of complementary constraints. Our code and data are available at https://github.com/qixuchen/AutoTest for future research.}
}


@article{DBLP:journals/pacmmod/YangWXWDPCL25,
	author = {Yicun Yang and
                  Zhaoguo Wang and
                  Yu Xia and
                  Zhuoran Wei and
                  Haoran Ding and
                  Ruzica Piskac and
                  Haibo Chen and
                  Jinyang Li},
	title = {Automated Validating and Fixing of Text-to-SQL Translation with Execution
                  Consistency},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {134:1--134:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725271},
	doi = {10.1145/3725271},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangWXWDPCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {State-of-the-art Text-to-SQL models rely on fine-tuning or few-shot prompting to help LLMs learn from training datasets containing mappings from natural language (NL) queries to SQL statements. Consequently, the quality of the dataset can greatly affect the accuracy of these Text-to-SQL models. Unlike other NL tasks, Text-to-SQL datasets are prone to errors despite extensive manual efforts due to the subtle semantics of SQL. Our study has found a non-negligible (>30%) portion of incorrect NL to SQL mapping cases exists in popular datasets Spider and BIRD. This paper aims to improve the quality of Text-to-SQL training datasets and thereby increase the accuracy of the resulting models. To do so, we propose a necessary correctness condition called execution consistency. For a given database instance, an NL to SQL mapping satisfies execution consistency if the execution result of an NL query matches that of the corresponding SQL. We develop SQLDriller to detect incorrect NL to SQL mappings based on execution consistency in a best-effort manner by crafting database instances that likely result in violations of execution consistency. It generates multiple candidate SQL predictions that differ in their syntax structures. Using a SQL equivalence checker, SQLDriller obtains counterexample database instances that can distinguish non-equivalent candidate SQLs. It then checks the execution consistency of an NL to SQL mapping under this set of counterexamples. The evaluation shows SQLDriller effectively detects and fixes incorrect mappings in the Text-to-SQL dataset, and it improves the model accuracy by up to 13.6%.}
}


@article{DBLP:journals/pacmmod/ButrovichALZPP25,
	author = {Matthew Butrovich and
                  Samuel Arch and
                  Wan Shen Lim and
                  William Zhang and
                  Jignesh M. Patel and
                  Andrew Pavlo},
	title = {{BPF-DB:} {A} Kernel-Embedded Transactional Database Management System
                  For eBPF Applications},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {135:1--135:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725272},
	doi = {10.1145/3725272},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ButrovichALZPP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Developers rely on the eBPF framework to augment operating system (OS) behavior for the betterment of database management system (DBMS) without having to modify kernel code. But eBPF's verifier limits program complexity and data management functionality. As a result eBPF's storage options are limited to kernel-resident, non-durable data structures that lack transactional guarantees. Inspired by embedded DBMSs for user-space applications, this paper present BPF-DB, an OS-embedded DBMS that offers transactional data management for eBPF applications. We explore the storage management and concurrency control challenges associated with DBMS design in eBPF's restrictive execution environment. We demonstrate BPF-DB's capabilities with two applications based on real-world systems. The first is a Redis-compatible in-memory DBMS that uses BPF-DB as its transactional storage engine. This system matches the performance of state-of-the-art implementations while offering stronger transactional guarantees. The second application implements a stored procedure-based DBMS that provides serializable multi-statement transactions. We compare this application against VoltDB, with BPF-DB achieving 43% higher throughput. BPF-DB's robust and high-performance transactional semantics enable emerging kernel-space applications.}
}


@article{DBLP:journals/pacmmod/AgarwalSMMGSKYS25,
	author = {Shubham Agarwal and
                  Sai Sundaresan and
                  Subrata Mitra and
                  Debabrata Mahapatra and
                  Archit Gupta and
                  Rounak Sharma and
                  Nirmal Joshua Kapu and
                  Tong Yu and
                  Shiv Kumar Saini},
	title = {Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented
                  Generation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {136:1--136:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725273},
	doi = {10.1145/3725273},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgarwalSMMGSKYS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Retrieval-Augmented Generation (RAG) is often used with Large Language Models (LLMs) to infuse domain knowledge or user-specific information. In RAG, given a user query, a retriever extracts chunks of relevant text from a knowledge base. These chunks are sent to an LLM as part of the input prompt. Typically, any given chunk is repeatedly retrieved across user questions. However, currently, for every question, attention layers in LLMs fully compute the Keys and Values (KVs) repeatedly for the input chunks, as state-of-the-art methods cannot reuse KV-caches when chunks appear at arbitrary locations or with arbitrary contexts. Naive reuse leads to output quality degradation. This leads to potentially redundant computations on expensive GPUs and increases latency. In this work, we propose  Cache-Craft , a system for managing and reusing precomputed KVs corresponding to the text chunks (which we call  chunk-caches ) in RAG-based systems. We present how to identify  chunk-caches  that are reusable, how to efficiently perform a small fraction of recomputation to  fix  the cache and maintain output quality, and how to efficiently store and evict  chunk-caches  in the hardware for maximizing reuse while masking any overheads. With real production workloads as well as synthetic datasets, we show that  Cache-Craft  reduces redundant computation by  51%  over SOTA prefix-caching and  75%  over full recomputation. Additionally, with continuous batching on a real production workload, we get a  1.6×  speed up in throughput for both the LLama-3-8B and 70B models and a  2.1×  and  2×  reduction in end-to-end response latency respectively, compared to prefix-caching, while maintaining generation quality.}
}


@article{DBLP:journals/pacmmod/YinZYWLLLFZYC25,
	author = {Peiqi Yin and
                  Qihui Zhou and
                  Xiao Yan and
                  Chao Wang and
                  Eric Lo and
                  Changji Li and
                  Lan Lu and
                  Hua Fan and
                  Wenchao Zhou and
                  Ming{-}Chang Yang and
                  James Cheng},
	title = {{CARINA:} An Efficient CXL-Oriented Embedding Serving System for Recommendation
                  Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {137:1--137:29},
	year = {2025},
	url = {https://doi.org/10.1145/3725274},
	doi = {10.1145/3725274},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YinZYWLLLFZYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Embedding-based recommendation models (ERMs) require large memory to host huge embedding tables and involve massive data traffic to read the embeddings. As a new interconnect, CXL suits ERMs since it can scale up single-machine memory with performant remote memory devices. However, directly running DRAM-based ERM serving systems on CXL yields poor performance because the bandwidth of CXL is notably lower than DRAM and can be easily saturated, making CXL memory the bottleneck. The non-uniform memory access (NUMA) architecture in modern CXL servers further decreased the system performance. In this paper, we design Carina for ERM serving on heterogeneous memory with CXL by considering such bandwidth asymmetry. In particular, Carina balances the memory access from different memory devices by storing hot embeddings with high access frequencies on DRAM and specifying the placement of embedding tables on the NUMA nodes. Moreover, Carina adopts bandwidth-aware task execution, which decomposes each batch of ERM requests into fine-grained tasks and schedules the tasks to control the real-time utilization of CXL bandwidth to avoid instantaneous saturation. We evaluate Carina under real CXL devices and find that it outperforms a CXL-oblivious baseline by an average of 5.38x and 4.04x in system throughput and request latency, respectively.}
}


@article{DBLP:journals/pacmmod/YuTCCHW25,
	author = {Feng Yu and
                  Hongshi Tan and
                  Xinyu Chen and
                  Yao Chen and
                  Bingsheng He and
                  Weng{-}Fai Wong},
	title = {Clementi: Efficient Load Balancing and Communication Overlap for Multi-FPGA
                  Graph Processing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {138:1--138:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725275},
	doi = {10.1145/3725275},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuTCCHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient graph processing is critical in various modern applications, such as social network analysis, recommendation systems, and large-scale data mining. Traditional single-FPGA systems struggle to handle the increasing size and complexity of real-world graphs due to limitations in memory and computational resources. Existing multi-FPGA solutions face significant challenges, including high communication overhead caused by irregular data transfer patterns and workload imbalances stemming from skewed graph distributions. These inefficiencies hinder scalability and performance, highlighting a critical research gap. To address these issues, we introduce Clementi, an efficient multi-FPGA graph processing framework that features customized fine-grained pipelines for computation and cross-FPGA communication. Clementi uniquely integrates an accurate performance model for execution time prediction, enabling a novel scheduling method that balances workload distribution and minimizes communication overhead by overlapping communication and computation stages. Experimental results demonstrate that Clementi achieves speedups of up to 8.75× compared to state-of-the-art multi-FPGA designs, indicating significant improvements in processing efficiency as the number of FPGAs increases. This near-linear scalability underscores the framework' s potential to enhance graph processing capabilities in practical applications. Clementi is open-sourced at https://github.com/Xtra-Computing/Clementi.}
}


@article{DBLP:journals/pacmmod/JiangYCHNLSL25,
	author = {Jiaxin Jiang and
                  Siyuan Yao and
                  Yuhang Chen and
                  Bingsheng He and
                  Yudong Niu and
                  Yuchen Li and
                  Shixuan Sun and
                  Yongchao Liu},
	title = {Community Detection in Heterogeneous Information Networks Without
                  Materialization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {139:1--139:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725276},
	doi = {10.1145/3725276},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/JiangYCHNLSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community detection in heterogeneous information networks (HINs) poses significant challenges due to the diversity of entity types and the complexity of their interrelations. While traditional algorithms may perform adequately in some scenarios, many struggle with the high memory usage and computational demands of large-scale HINs. To address these challenges, we introduce a novel framework, SCAR, which efficiently uncovers community structures in HINs without requiring network materialization. SCAR leverages insights from meta-paths to interpret multi-relational data through compact vertex-based sketches, significantly reducing computational overhead and materialization overhead. We propose a sketch-based technique for estimating changes in modularity, improving both the precision and speed in community detection. Our extensive evaluations on diverse real-world datasets provide detailed comparative metrics, demonstrating that SCAR outperforms several state-of-the-art methods, including Gdy, Louvain, Leiden, Infomap, Walktrap, and Networkit, in execution time and memory consumption while maintaining competitive accuracy. Overall, SCAR offers a robust and scalable solution for revealing community structures in large HINs, with applications across various domains, including social networks, academic collaboration networks, and e-commerce platforms.}
}


@article{DBLP:journals/pacmmod/MohapatraGHK25,
	author = {Shubhankar Mohapatra and
                  Amir Gilad and
                  Xi He and
                  Benny Kimelfeld},
	title = {Computing Inconsistency Measures Under Differential Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {140:1--140:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725397},
	doi = {10.1145/3725397},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MohapatraGHK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Assessing data quality is crucial to knowing whether and how to use the data for different purposes. Specifically, given a collection of integrity constraints, various ways have been proposed to quantify the inconsistency of a database. Inconsistency measures are particularly important when we wish to assess the quality of private data without revealing sensitive information. We study the estimation of inconsistency measures for a database protected under Differential Privacy (DP). Such estimation is nontrivial since some measures intrinsically query sensitive information, and the computation of others involves functions on underlying sensitive data. Among five inconsistency measures that have been proposed in recent work, we identify that two are intractable in the DP setting. The major challenge for the other three is high sensitivity: adding or removing one tuple from the dataset may significantly affect the outcome. To mitigate that, we model the dataset using a conflict graph and investigate private graph statistics to estimate these measures. The proposed machinery includes adapting graph-projection techniques with parameter selection optimizations on the conflict graph and a DP variant of approximate vertex cover size. We experimentally show that we can effectively compute DP estimates of the three measures on five real-world datasets with denial constraints, where the density of the conflict graphs highly varies.}
}


@article{DBLP:journals/pacmmod/ZhouGZL25,
	author = {Wei Zhou and
                  Yuyang Gao and
                  Xuanhe Zhou and
                  Guoliang Li},
	title = {Cracking {SQL} Barriers: An LLM-based Dialect Translation System},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {141:1--141:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725278},
	doi = {10.1145/3725278},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhouGZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automatic dialect translation reduces the complexity of database migration, which is crucial for applications interacting with multiple database systems. However, rule-based translation tools (e.g., SQLGlot, jOOQ, SQLines) are labor-intensive to develop and often (1) fail to translate certain operations, (2) produce incorrect translations due to rule deficiencies, and (3) generate translations compatible with some database versions but not the others. In this paper, we investigate the problem of automating dialect translation with large language models (LLMs). There are three main challenges. First, queries often involve lengthy content (e.g., excessive column values) and multiple syntax elements that require translation, increasing the risk of LLM hallucination. Second, database dialects have diverse syntax trees and specifications, making it difficult for cross-dialect syntax matching. Third, dialect translation often involves complex many-to-one relationships between source and target operations, making it impractical to translate each operation in isolation. To address these challenges, we propose an automatic dialect translation system CrackSQL. First, we propose  Functionality-based Query Processing  that segments the query by functionality syntax trees and simplifies the query via (i) customized function normalization and (ii) translation-irrelevant query abstraction. Second, we design a  Cross-Dialect Syntax Embedding Model  to generate embeddings by the syntax trees and specifications (of certain version), enabling accurate query syntax matching. Third, we propose a  Local-to-Global Dialect Translation  strategy, which restricts LLM-based translation and validation on operations that cause local failures, iteratively extending these operations until translation succeeds. Experiments show CrackSQL significantly outperforms existing methods (e.g., by up to 77.42%). The code is available at https://github.com/weAIDB/CrackSQL.}
}


@article{DBLP:journals/pacmmod/MarchesinS25,
	author = {Stefano Marchesin and
                  Gianmaria Silvello},
	title = {Credible Intervals for Knowledge Graph Accuracy Estimation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {142:1--142:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725279},
	doi = {10.1145/3725279},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MarchesinS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graphs (KGs) are widely used in data-driven applications and downstream tasks, such as virtual assistants, recommendation systems, and semantic search. The accuracy of KGs directly impacts the reliability of the inferred knowledge and outcomes. Therefore, assessing the accuracy of a KG is essential for ensuring the quality of facts used in these tasks. However, the large size of real-world KGs makes manual triple-by-triple annotation impractical, thereby requiring sampling strategies to provide accuracy estimates with statistical guarantees. The current state-of-the-art approaches rely on Confidence Intervals (CIs), derived from frequentist statistics. While efficient, CIs have notable limitations and can lead to interpretation fallacies. In this paper, we propose to overcome the limitations of CIs by using  Credible Intervals  (CrIs), which are grounded in Bayesian statistics. These intervals are more suitable for reliable post-data inference, particularly in KG accuracy evaluation. We prove that CrIs offer greater reliability and stronger guarantees than frequentist approaches in this context. Additionally, we introduce aHPD, an adaptive algorithm that is more efficient for real-world KGs and statistically robust, addressing the interpretive challenges of CIs.}
}


@article{DBLP:journals/pacmmod/ParkOK25,
	author = {Sungwoo Park and
                  Seyeon Oh and
                  Min{-}Soo Kim},
	title = {cuMatch: {A} GPU-based Memory-Efficient Worst-case Optimal Join Processing
                  Method for Subgraph Queries with Complex Patterns},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {143:1--143:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725398},
	doi = {10.1145/3725398},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ParkOK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph queries are widely used but face significant challenges due to complex patterns such as negative and optional edges. While worst-case optimal joins have proven effective for subgraph queries with regular patterns, no method has been proposed that can process queries involving complex patterns in a single multi-way join. Existing CPU-based and GPU-based methods experience intermediate data explosion when processing complex patterns following regular patterns. In addition, GPU-based methods struggle with issues of wasted GPU memory and redundant computation. In this paper, we propose cuMatch, a GPU-based unified worst-case optimal join processing method for subgraph queries. It avoids intermediate data explosions by processing even complex pattern queries in a single multi-way join. It is also memory-efficient and fast, due to a new partitioning format, scheduling method, and task fusion technique. Extensive experiments demonstrate that cuMatch outperforms state-of-the-art methods by orders of magnitude, without out-of-memory errors.}
}


@article{DBLP:journals/pacmmod/GheerbrantLR25,
	author = {Am{\'{e}}lie Gheerbrant and
                  Leonid Libkin and
                  Alexandra Rogova},
	title = {Dangers of List Processing in Querying Property Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {144:1--144:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725281},
	doi = {10.1145/3725281},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GheerbrantLR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The workhorse of property graph query languages such as Cypher and GQL is pattern matching. The result of pattern matching is a collection of paths and mappings of variables to graph elements. To increase expressiveness of post-processing of pattern matching results, languages such as Cypher introduce the capability of creating lists of nodes and edges from matched paths, and provide users with standard list processing tools such as reduce. We show that on the one hand, this makes it possible to capture useful classes of queries that pattern matching alone cannot do. On the other hand, we show that this opens backdoor to very high and unexpected expressiveness. In particular one can very easily express several classical NP-hard problems by simple queries that use reduce. This level of expressiveness appears to be beyond what query optimizers can handle, and indeed this is confirmed by an experimental evaluation, showing that such queries time out already on very small graphs. We conclude our analysis with a suggestion on the use of list processing in queries that while retaining its usefulness, avoids the above pitfalls and prevents highly intractable queries.}
}


@article{DBLP:journals/pacmmod/FanHRX25,
	author = {Wenfei Fan and
                  Xiaoyu Han and
                  Weilong Ren and
                  Zihuan Xu},
	title = {Data Enhancement for Binary Classification of Relational Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {145:1--145:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725282},
	doi = {10.1145/3725282},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanHRX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies enhancement of training data  D  to improve the robustness of machine learning (ML) classifiers  M  against adversarial attacks on relational data. Data enhancing aims to (a) defuse poisoned imperceptible features embedded in  D , and (b) defend against attacks at prediction time that are unseen in  D . We show that while there exists an inherent tradeoff between the accuracy and robustness of  M  in case (b), data enhancing can improve both the accuracy and robustness at the same time in case (a). We formulate two data enhancing problems accordingly, and show that both problems are intractable.Despite the hardness, we propose a framework that integrates model training and data enhancing. Moreover, we develop algorithms for (a) detecting and debugging corrupted imperceptible features in training data, and (b) selecting and adding adversarial examples to training data to defend against unseen attacks at prediction time. Using real-life datasets, we empirically verify that the method is at least 20.4% more robust and 2.02X faster than SOTA methods for classifiers  M , without degrading the accuracy of  M .}
}


@article{DBLP:journals/pacmmod/ZhaoSYYKZ25,
	author = {Junyi Zhao and
                  Kai Su and
                  Yifei Yang and
                  Xiangyao Yu and
                  Paraschos Koutris and
                  Huanchen Zhang},
	title = {Debunking the Myth of Join Ordering: Toward Robust {SQL} Analytics},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {146:1--146:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725283},
	doi = {10.1145/3725283},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhaoSYYKZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Join order optimization is critical in achieving good query performance. Despite decades of research and practice, modern query optimizers could still generate inferior join plans that are orders of magnitude slower than optimal. Existing research on robust query processing often lacks theoretical guarantees on join-order robustness while sacrificing query performance. In this paper, we rediscover the recent Predicate Transfer technique from a robustness point of view. We introduce two new algorithms, LargestRoot and SafeSubjoin, and then propose Robust Predicate Transfer (RPT) that is provably robust against arbitrary join orders of an acyclic query. We integrated Robust Predicate Transfer with DuckDB, a state-of-the-art analytical database, and evaluated against all the queries in TPC-H, JOB, TPC-DS, and DSB benchmarks. Our experimental results show that RPT improves join-order robustness by orders of magnitude compared to the baseline. With RPT, the largest ratio between the maximum and minimum execution time out of random join orders for a single acyclic query is only 1.6x (the ratio is close to 1 for most evaluated queries). Meanwhile, applying RPT also improves the end-to-end query performance by ≈1.5x (per-query geometric mean). We hope that this work sheds light on solving the practical join ordering problem.}
}


@article{DBLP:journals/pacmmod/DingLZYYWWW25,
	author = {Chen Ding and
                  Kai Lu and
                  Quanyi Zhang and
                  Zekun Ye and
                  Ting Yao and
                  Daohui Wang and
                  Huatao Wu and
                  Jiguang Wan},
	title = {DFlush: DPU-Offloaded Flush for Disaggregated LSM-based Key-Value
                  Stores},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {147:1--147:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725284},
	doi = {10.1145/3725284},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DingLZYYWWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rapid increase of storage and network bandwidth incurs higher CPU consumption in modern data systems. This phenomenon is particularly evident for log-structured merged key-value stores (LSM-KVS), which rely on resource-intensive background operations to flush and compact disk data. While extensive research has been conducted to reduce the CPU overhead of background compaction, less attention has been paid to background flushing, which can also consume a significant amount of valuable CPU cycles and disrupt CPU caches, ultimately impacting overall performance. In this paper, we propose DFlush, a novel solution that uses DPUs to offload background flush operations to reduce its CPU cost. DPUs are an appealing choice for this goal due to their cost-effectiveness, ease of programming, and widespread deployment. However, their complex hardware architecture requires careful design of both the data and control planes. To fully harness the DPU's capabilities, DFlush decomposes a flush job into fine-grained steps, mapped them to DPU hardware units, and accelerates them through pipeline, data, and channel parallelism, ensuring data-plane efficiency. It also introduces an adaptive control plane that dynamically schedules flush jobs from different LSM-KVS instances based on their priority, reducing write stall and tail latency. Our experiments on a real DPU platform with an industrial-grade LSM-KVS show that DFlush delivers higher throughput, significantly lower tail latency, and saves up to dozens of CPU cores per LSM-KVS server while reducing energy consumption.}
}


@article{DBLP:journals/pacmmod/JiangYZHSZLW25,
	author = {Mengxu Jiang and
                  Zhi Yang and
                  Fangyuan Zhang and
                  Guanhao Hou and
                  Jieming Shi and
                  Wenchao Zhou and
                  Feifei Li and
                  Sibo Wang},
	title = {{DIGRA:} {A} Dynamic Graph Indexing for Approximate Nearest Neighbor
                  Search with Range Filter},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {148:1--148:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725399},
	doi = {10.1145/3725399},
	timestamp = {Fri, 18 Jul 2025 07:37:51 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/JiangYZHSZLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advancements in AI have enabled models to map real-world entities, such as product images, into high-dimensional vectors, making approximate nearest neighbor search (ANNS) crucial for various applications. Often, these vectors are associated with additional attributes like price, prompting the need for range-filtered ANNS where users seek similar items within specific attribute ranges. Naive solutions like pre-filtering and post-filtering are straightforward but inefficient. Specialized indexes, such as SeRF, SuperPostFiltering, and iRangeGraph, have been developed to address these queries effectively. However, these solutions do not support dynamic updates, limiting their practicality in real-world scenarios where datasets frequently change. To address these challenges, we propose  DIGRA,  a novel dynamic graph index for range-filtered ANNS.  DIGRA  supports efficient dynamic updates while maintaining a balance among query efficiency, update efficiency, indexing cost, and result quality. Our approach introduces a dynamic multi-way tree structure combined with carefully integrated ANNS indices to handle range filtered ANNS efficiently. We employ a lazy weight-based update mechanism to significantly reduce update costs and adopt optimized choice of ANNS index to lower construction and update overhead. Experimental results demonstrate that DIGRA achieves superior trade-offs, making it suitable for large-scale dynamic datasets in real-world applications.}
}


@article{DBLP:journals/pacmmod/FarhanKW25a,
	author = {Muhammad Farhan and
                  Henning Koehler and
                  Qing Wang},
	title = {Divide-and-Conquer: Scalable Shortest Path Counting on Large Road
                  Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {149:1--149:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725400},
	doi = {10.1145/3725400},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FarhanKW25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The shortest path counting problem is crucial for various applications in road networks, such as network robustness analysis, traffic flow distribution, and navigation optimization. Unlike traditional shortest path problems, it requires enumerating all possible shortest paths, making it computationally challenging, especially in dense urban networks with numerous equal-length paths. Existing methods, such as 2-hop labeling schemes, precompute shortest-path distances and counts for efficient queries but struggle to scale in large networks. In this work, we propose a novel divide-and-conquer approach based on recursive vertex bipartitioning to address this limitation. At its core, we establish a count reconstruction theorem that efficiently combines shortest subpath counts from smaller subgraphs to accurately reconstruct shortest path counts for the entire graph. This approach significantly reduces computational overhead and storage requirements. We also introduce a 2-hop count labeling scheme that integrates effectively with this divide-and-conquer framework. Experimental results show that our approach significantly outperforms state-of-the-art solutions, doubling query processing speed, reducing label construction time to one-fourth, and requiring only around 20% of labeling space.}
}


@article{DBLP:journals/pacmmod/JiangYLWHC25,
	author = {Jiaxin Jiang and
                  Siyuan Yao and
                  Yuchen Li and
                  Qiange Wang and
                  Bingsheng He and
                  Min Chen},
	title = {Dupin: {A} Parallel Framework for Densest Subgraph Discovery in Fraud
                  Detection on Massive Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {150:1--150:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725287},
	doi = {10.1145/3725287},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/JiangYLWHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting fraudulent activities in financial and e-commerce transaction networks is crucial. One effective method for this is Densest Subgraph Discovery (DSD). However, deploying DSD methods in production systems faces substantial scalability challenges due to the predominantly sequential nature of existing methods, which impedes their ability to handle large-scale transaction networks and results in significant detection delays. To address these challenges, we introduce Dupin, a novel parallel processing framework designed for efficient DSD processing in billion-scale graphs. Dupin is powered by a processing engine that exploits the unique properties of the peeling process, with theoretical guarantees on detection quality and efficiency. Dupin provides user-friendly APIs for flexible customization of DSD objectives and ensures robust adaptability to diverse fraud detection scenarios. Empirical evaluations indicate that Dupin consistently outperforms several existing DSD methods, achieving performance improvements of up to two orders of magnitude compared to traditional approaches. On billion-scale graphs, Dupin demonstrates the potential to enhance the prevention of fraudulent transactions by approximately 49.5 basis points and reduces density error from 30.3% to below 5.0%, as supported by our experimental results.}
}


@article{DBLP:journals/pacmmod/XieWXYL25,
	author = {Dongdong Xie and
                  Pinghui Wang and
                  Quanqing Xu and
                  Chuanhui Yang and
                  Rundong Li},
	title = {Efficient and Accurate Differentially Private Cardinality Continual
                  Releases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {151:1--151:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725288},
	doi = {10.1145/3725288},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XieWXYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurately estimating the number of unique elements that appear in data streams in real time is a fundamental problem with applications including network traffic monitoring and real-time social media analytics. Traditional sketch-based algorithms such as FM Sketch and HyperLogLog offer memory-friendly solutions for cardinality estimation but fall short in scenarios where the stream elements are privacy-sensitive and require differential privacy. Although recent approaches have incorporated differential privacy into the above cardinality estimators, they are limited to single-query settings, restricting their applicability. Previous methods for private cardinality continual release settings-i.e., releasing the cardinality after each new element in the stream-demand large memory resources and are thus difficult to apply in practice. In this paper, we present a novel cardinality estimation framework, FC, which ensures differential privacy under continual releases while simultaneously achieving low memory usage, high accuracy, and efficient computation. Our approach innovatively leverages an efficient cardinality estimator and privacy-preserving mechanisms to overcome the limitations of existing methods. Comprehensive experiments demonstrate that our method reduces memory usage by up to 504 times compared to the best previous method while maintaining nearly the same accuracy. Additionally, under identical memory constraints, our method improves the estimation accuracy by orders of magnitude.}
}


@article{DBLP:journals/pacmmod/ZhangJHSFZLW25,
	author = {Fangyuan Zhang and
                  Mengxu Jiang and
                  Guanhao Hou and
                  Jieming Shi and
                  Hua Fan and
                  Wenchao Zhou and
                  Feifei Li and
                  Sibo Wang},
	title = {Efficient Dynamic Indexing for Range Filtered Approximate Nearest
                  Neighbor Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {152:1--152:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725401},
	doi = {10.1145/3725401},
	timestamp = {Fri, 18 Jul 2025 07:37:56 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangJHSFZLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a set O of objects consisting of  n  high-dimensional vectors, the problem of  approximate nearest neighbor (ANN)  search for a query vector  q  is crucial in many applications where objects are represented as feature vectors in high-dimensional spaces. Each object in O often has attributes like popularity or price, which influence the search. Practically, searching for the nearest neighbor to  q  might include a range filter specifying the desired attribute values, e.g., within a specific price range. Existing solutions for range filtered ANN search often face trade-offs among excessive storage, poor query performance, and limited support for updates. To address this challenge, we propose RangePQ, a novel indexing scheme that supports efficient range filtered ANN searches and updates, requiring only linear space. Our scheme integrates seamlessly with existing PQ-based index---a widely recognized, scalable index type for ANN searches---to enhance range-filtered ANN queries and update capabilities. Our indexing method, supporting arbitrary range filters, has a space complexity of (O(n log K)), where K is a parameter of the PQ-based index and log K scales with O(log n). To reduce the space cost, we further present a hybrid two-layer structure to reduce space usage to O(n), preserving query efficiency without additional update costs. Experimental results demonstrate that our indexing scheme significantly improves query performance while maintaining competitive update performance and space efficiency.}
}


@article{DBLP:journals/pacmmod/WangW25,
	author = {Libin Wang and
                  Raymond Chi{-}Wing Wong},
	title = {Efficient Indexing for Flexible Label-Constrained Shortest Path Queries
                  in Road Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {153:1--153:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725402},
	doi = {10.1145/3725402},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The point-to-point shortest path query is widely used in many spatial applications, e.g., navigation systems. However, the returned shortest path minimizing only one objective fails to satisfy users' various routing requirements in practice. For example, the user may specify the order of using several transportation modes in the planned route. The  Label-Constrained Shortest Path (LCSP)  query under regular languages is powerful enough to express diversified routing demands in a labeled road network where each edge is associated with a  label  to denote its road type. The complex routing demand can be formulated by a  regular language,  and the edge labels along each path should be a word under the given regular language. Previous LCSP solutions were either inefficient in query processing or inflexible in their use of the languages since they made some assumptions about the given language. In this paper, we propose an efficient index-based solution called  Border-based State Move (BSM),  which can answer LCSP queries quickly with flexible use of the language constraint. Specifically, our BSM builds indexes to skip the exploration between a vertex and its  border  vertices during query processing. Our experiments conducted on real road networks demonstrated the superiority of our proposed BSM. It can reduce the query time over state-of-the-art solutions by two orders of magnitude.}
}


@article{DBLP:journals/pacmmod/NixD25,
	author = {Joris Nix and
                  Jens Dittrich},
	title = {Extending {SQL} to Return a Subdatabase},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {154:1--154:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725291},
	doi = {10.1145/3725291},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/NixD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Every SQL statement is limited to return a single, possibly denormalized table. This approximately 50-year-old design decision has far-reaching consequences. The most apparent problem is the redundancy introduced through denormalization, which can result in long transfer times of query results and high memory usage for materializing intermediate results. Additionally, regardless of their goals, users are forced to fit query computations into one single result, mixing the data retrieval and transformation aspect of SQL. Moreover, both problems violate the principles and core ideas of normal forms. In this paper, we argue for eliminating the single-table limitation of SQL. We extend SQL's SELECT clause by the keyword 'RESULTDB' to support returning a result subdatabase. Our extension has clear semantics, i.e., by annotating any existing SQL statement with the RESULTDB keyword, the DBMS returns the tables participating in the query, each restricted to the relevant tuples that occur in the traditional single-table query result. Thus, we do not denormalize the query result in any way. Our approach has significant, far-reaching consequences, impacting the querying of hierarchical data, materialized views, and distributed databases, while maintaining backward compatibility. In addition, our proposal paves the way for a long list of exciting future research opportunities. We propose multiple algorithms to integrate our feature into both closed-source and open-source database systems. For closed-source systems, we provide several SQL-based rewrite methods. In addition, we present an efficient algorithm for cyclic and acyclic join graphs that we integrated into an open-source database system. We conduct a comprehensive experimental study. Our results show that returning multiple individual result sets can significantly decrease the result set size. Furthermore, our rewrite methods and algorithm introduce minimal overhead and can even outperform single-table execution in certain cases.}
}


@article{DBLP:journals/pacmmod/ZhangJHZW25,
	author = {Hanbing Zhang and
                  Yinan Jing and
                  Zhenying He and
                  Kai Zhang and
                  X. Sean Wang},
	title = {{FAAQP:} Fast and Accurate Approximate Query Processing based on Bitmap-augmented
                  Sum-Product Network},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {155:1--155:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725292},
	doi = {10.1145/3725292},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangJHZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For interactive data exploration, approximate query processing (AQP) is a useful approach that provides a timely response by trading query accuracy. To reduce query latency, existing AQP methods use samples or models rather than the underlying data to answer queries. However, it is difficult to achieve satisfactory results in terms of query accuracy and query latency simultaneously with these methods. For the sample-based methods, this is because the more accurate the query results are, the more samples are needed and the more time cost is required for processing. The model-based methods have lower query latency, but they cannot return the approximate results with high accuracy because the existing models cannot capture the complex data distribution accurately. In this paper, we propose a fast and accurate AQP method  FAAQP . In FAAQP, we propose a novel unsupervised model  bitmap-augmented sum-product network  (BSPN) that combines the advantages of the sum-product network with bitmaps to capture the characteristics of data distribution more accurately. Then, we propose a budget-aware BSPN construction method that builds BSPN models with the maximum query accuracy for the given storage budget. Furthermore, to reduce the query latency of FAAQP, we propose a bitmap merging strategy that makes a trade-off between query accuracy and query latency. Experimental results on real-world and synthetic datasets show that FAAQP outperforms the state-of-the-art AQP methods and achieves 1.3×-9.0× improvements in query accuracy with a low query latency.}
}


@article{DBLP:journals/pacmmod/LiLYGR25,
	author = {Benton Li and
                  Nativ Levy and
                  Brit Youngmann and
                  Sainyam Galhotra and
                  Sudeepa Roy},
	title = {Fair and Actionable Causal Prescription Ruleset},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {156:1--156:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725293},
	doi = {10.1145/3725293},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiLYGR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Prescriptions, or actionable recommendations, are commonly generated across various fields to influence key outcomes such as improving public health, enhancing economic policies, or increasing business efficiency. While traditional association-based methods may identify correlations, they often fail to reveal the underlying causal factors needed for informed decision-making. On the other hand, in decision making for tasks with significant societal or economic impact, it is crucial to provide recommendations that are interpretable and justifiable, and equitable in terms of the outcome for both the protected and non-protected groups. Motivated by these two goals, this paper introduces a fairness-aware framework leveraging causal reasoning for generating a set of interpretable and actionable prescription rules (ruleset) toward betterment of an outcome while preventing exacerbating inequalities for protected groups. By considering group and individual fairness metrics from the literature, we ensure that both protected and non-protected groups benefit from these recommendations, providing a balanced and equitable approach to decision-making. We employ efficient optimizations to explore the vast and complex search space considering both fairness and coverage of the prescription ruleset. Empirical evaluation and case study on real-world datasets demonstrates the utility of our framework for different use cases.}
}


@article{DBLP:journals/pacmmod/GavriilidisBBM25,
	author = {Haralampos Gavriilidis and
                  Kaustubh Beedkar and
                  Matthias Boehm and
                  Volker Markl},
	title = {Fast and Scalable Data Transfer Across Data Systems},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {157:1--157:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725294},
	doi = {10.1145/3725294},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GavriilidisBBM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fast and scalable data transfer is crucial in today's decentralized data ecosystems and data-driven applications. Example use cases include transferring data from operational systems to consolidated data warehouse environments, or from relational database systems to data lakes for exploratory data analysis or ML model training. Traditional data transfer approaches rely on efficient point-to-point connectors or general middleware with generic intermediate data representations. Physical environments (e.g., on-premise, cloud, or consumer nodes) also have become increasingly heterogeneous. Existing work still struggles to achieve both, fast and scalable data transfer as well as generality in terms of heterogeneous systems and environments. Hence, in this paper, we introduce a holistic data transfer framework. Our XDBC framework splits the data transfer pipeline into logical components and provides a wide variety of physical implementations for these components. This design allows a seamless integration of different systems as well as the automatic optimizations of data transfer configurations according to workload and environment characteristics. Our evaluation shows that XDBC outperforms state-of-the-art generic data transfer tools by up to 5x, while being on par with specialized approaches.}
}


@article{DBLP:journals/pacmmod/XieYL25,
	author = {Jiadong Xie and
                  Jeffrey Xu Yu and
                  Yingfan Liu},
	title = {Fast Approximate Similarity Join in Vector Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {158:1--158:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725403},
	doi = {10.1145/3725403},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XieYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advancements in deep learning, particularly in embedding models, have enabled the effective representation of various data types such as text, images, and audio as vectors, thereby facilitating semantic analysis. A large number of massive vector datasets are maintained in vector databases. Approximate similarity join is a core operation in vector database systems that joins two datasets, and outputs all pairs of vectors from the two datasets, if the distance between such a pair of two vectors is no more than a specified value. Existing approaches for similarity join are selection-based such that they treat each data point in a dataset as an individual query point to search data points by an approximate range query in another dataset. Such methods do not fully capitalize on the inherent properties of the join operation itself. In this paper, we propose a new join algorithm, SimJoin. Our join algorithm aims at boosting join processing efficiency by leveraging relationships between partial join results (e.g., join windows). In brief, our join algorithm accelerates the join processing to process a join window by utilizing the join windows from the processed data points. Then, we discuss optimizing join window order to minimize join costs. In addition, we discuss how to support  k -similarity join, and how to maintain proximity graph index based on k-similarity join. Extensive experiments on real-world and synthetic datasets demonstrate the significant performance superiority of our proposed algorithms over existing state-of-the-art methods.}
}


@article{DBLP:journals/pacmmod/SurianarayananMSL25,
	author = {Vaishali Surianarayanan and
                  Anikait Mundhra and
                  Ajaykrishnan E. S and
                  Daniel Lokshtanov},
	title = {Fast Hypertree Decompositions via Linear Programming: Fractional and
                  Generalized},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {159:1--159:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725296},
	doi = {10.1145/3725296},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SurianarayananMSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient query evaluation in databases and solving constraint satisfaction problems (CSPs) are crucial for improving performance in many real-world applications, from large-scale data management to decision-making systems. These problems naturally admit hypergraph representations, and are efficiently solvable using hypertree decomposition techniques, when the decomposition width is small. However, these techniques require finding small-width decompositions efficiently. This remains a significant challenge despite research from both the database and theory communities. In this work we present  Ralph (Randomized Approximation using Linear Programming for Hypertree-Decompositions),  a fast algorithm to compute low width fractional and generalized hypertree decompositions for input hypergraphs, as well as lower bounds for these widths. We build on the recent breakthrough by Korchemna et al. [FOCS 2024], which introduced the first polynomial time approximation algorithm for fractional (generalized) hypertree width. Our approach combines this theoretical advancement with practical heuristic improvements utilizing (mixed-integer) linear programs. Along the way, we present new algorithms with strong theoretical guarantees. Through empirical evaluation on the nearly 3700 instances of HyperBench, a well established benchmark suite for hypertree decompositions, we find near optimal decompositions for all previously solved instances and low width decompositions for all 500 previously unsolved instances, effectively pushing state-of-the-art.}
}


@article{DBLP:journals/pacmmod/YuWLLC25,
	author = {Kaiqiang Yu and
                  Kaixin Wang and
                  Cheng Long and
                  Laks V. S. Lakshmanan and
                  Reynold Cheng},
	title = {Fast Maximum Common Subgraph Search: {A} Redundancy-Reduced Backtracking
                  Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {160:1--160:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725404},
	doi = {10.1145/3725404},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuWLLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given two input graphs, finding the largest subgraph that occurs in both, i.e., finding the maximum common subgraph, is a fundamental operator for evaluating the similarity between two graphs in graph data analysis. Existing works for solving the problem are of either theoretical or practical interest, but not both. Specifically, the algorithms with a theoretical guarantee on the running time are known to be not practically efficient; algorithms following the recently proposed backtracking framework called McSplit, run fast in practice but do not have any theoretical guarantees. In this paper, we propose a new backtracking algorithm called RRSplit, which at once achieves better practical efficiency and provides a non-trivial theoretical guarantee on the worst-case running time. To achieve the former, we develop a series of reductions and upper bounds for reducing redundant computations, i.e., the time for exploring some unpromising branches of exploration that hold no maximum common subgraph. To achieve the latter, we formally prove that RRSplit incurs a worst-case time complexity which matches the best-known complexity for the problem. Finally, we conduct extensive experiments on four benchmark graph collections, and the results demonstrate that our algorithm outperforms the practical state-of-the-art by several orders of magnitude.}
}


@article{DBLP:journals/pacmmod/XueC25,
	author = {Quan Xue and
                  T.{-}H. Hubert Chan},
	title = {Faster and Efficient Density Decomposition via Proportional Response
                  with Exponential Momentum},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {161:1--161:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725405},
	doi = {10.1145/3725405},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XueC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs are crucial for modeling complex relationships in fields like social networks and biology. A key aspect in graph theory is identifying dense subgraphs, with applications in various domains. Density decomposition refines this by analyzing a graph's global density structure. This concept has been independently rediscovered in research areas like graph mining, algorithm design, and economics. Recent advancements in maximum-flow algorithms allow for nearly-linear time computation of the density vector, but they struggle with large real-world graphs. To address this, iterative first-order methods based on convex optimization, such as the Frank-Wolfe algorithm and momentum-based methods like accelerated FISTA, have been developed to approximate density vectors. This work explores density decomposition through market dynamics, where edges represent buyers and nodes represent sellers in a Fisher market model. The iterative proportional response process, which converges to the Fisher market equilibrium, provides an alternative interpretation of density decomposition. In each step, agents allocate resources based on the proportion of benefit received, moving the system toward equilibrium. Since each proportional response update can be seen as a gradient descent step, we investigate whether momentum methods can speed up convergence. Traditional momentum uses a linear combination of current and previous solutions, whereas our novel exponential momentum variant uses geometric interpolation, aligning better with proportional adjustments. Empirical evaluations on large-scale real-world and synthetic graphs confirm the effectiveness of our methods. Notably, the proportional response algorithm with exponential momentum outperforms existing methods, delivering improvements by several orders of magnitude in some cases. This advancement is significant, as the resulting density vector is crucial for many downstream tasks in graph mining and algorithm design.}
}


@article{DBLP:journals/pacmmod/ZhangPSLLWLGZ25,
	author = {Jiaoyi Zhang and
                  Liqiang Peng and
                  Mo Sha and
                  Weiran Liu and
                  Xiang Li and
                  Sheng Wang and
                  Feifei Li and
                  Mingyu Gao and
                  Huanchen Zhang},
	title = {Femur: {A} Flexible Framework for Fast and Secure Querying from Public
                  Key-Value Store},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {162:1--162:29},
	year = {2025},
	url = {https://doi.org/10.1145/3725299},
	doi = {10.1145/3725299},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangPSLLWLGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With increasing demands for privacy, it becomes necessary to protect sensitive user query data when accessing public key-value databases. Existing Private Information Retrieval (PIR) schemes provide full security but suffer from poor scalability, limiting their applicability in large-scale deployment. We argue that in many real-world scenarios, a more practical solution should allow users to flexibly determine the privacy levels of their queries in a theoretically guided way, balancing security and performance based on specific needs. To formally provide provable guarantees, we introduce a novel concept of distance-based indistinguishability, which can facilitate users to comfortably relax their security requirements. We then design Femur, an efficient framework to securely query public key-value stores with flexible security and performance trade-offs. It uses a space-efficient learned index to convert query keys into storage locations, obfuscates these locations with extra noise provably derived by the distance-based indistinguishability theory, and sends the expanded range to the server. The server then adaptively utilizes the best scheme to retrieve data. We also propose a novel variable-range PIR scheme optimized for bandwidth-constrained environments. Experiments show that Femur outperforms the state-of-the-art designs even when ensuring the same full security level. When users are willing to relax their privacy requirements, Femur can further improve the performance gains to up to 163.9X, demonstrating an effective trade-off between security and performance.}
}


@article{DBLP:journals/pacmmod/MangBHR25,
	author = {Qiuyang Mang and
                  Jinsheng Ba and
                  Pinjia He and
                  Manuel Rigger},
	title = {Finding Logic Bugs in Graph-processing Systems via Graph-cutting},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {163:1--163:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725300},
	doi = {10.1145/3725300},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MangBHR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-processing systems, including Graph Database Management Systems (GDBMSes) and graph libraries, are designed to analyze and manage graph data efficiently. They are widely used in applications such as social networks, recommendation systems, and fraud detection. However, logic bugs in these systems can lead to incorrect results, compromising the reliability of applications. While recent research has explored testing techniques specialized for GDBMSes, it is unclear how to adapt them to graph-processing systems in general. This paper proposes G raph - cutting , a universal approach for detecting logic bugs in both GDBMSes and various algorithms in graph libraries. Our key idea is inspired by the observation that certain graph patterns are critical for various graph-processing tasks. Dividing graph data into subgraphs that preserve those patterns establishes a natural relationship between query results on the original graph and its subgraphs, allowing for the detection of logic bugs when this relationship is violated. We implemented Graph-cutting as a tool, GSlicer, and evaluated it on 3 popular graph-processing systems, NetworkX, Neo4j, and Kùzu. GSlicer detected 39 unique and previously unknown bugs, out of which 34 have been fixed and confirmed by developers. At least 8 logic bugs detected by GSlicer cannot be detected by baseline strategies. Additionally, by leveraging just a few concrete relationships, Graph-cutting can cover over 100 APIs in NetworkX. We expect this technique to be widely applicable and that it can be used to improve the quality of graph-processing systems broadly.}
}


@article{DBLP:journals/pacmmod/DeedsABS25,
	author = {Kyle Deeds and
                  Willow Ahrens and
                  Magdalena Balazinska and
                  Dan Suciu},
	title = {Galley: Modern Query Optimization for Sparse Tensor Programs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {164:1--164:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725301},
	doi = {10.1145/3725301},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DeedsABS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The tensor programming abstraction is a foundational paradigm which allows users to write high performance programs via a high-level imperative interface. Recent work on  sparse tensor compilers  has extended this paradigm to sparse tensors (i.e., tensors where most entries are not explicitly represented). With these systems, users define the semantics of the program and the algorithmic decisions in a concise language that can be compiled to efficient low-level code. However, these systems still require users to make complex decisions about program structure and memory layouts to write efficient programs. This work presents  .Galley , a system for declarative tensor programming that allows users to write efficient tensor programs without making complex algorithmic decisions. Galley is the first system to perform cost based lowering of sparse tensor algebra to the imperative language of sparse tensor compilers, and the first to optimize arbitrary operators beyond Σ and *. First, it decomposes the input program into a sequence of aggregation steps through a novel extension of the FAQ framework. Second, Galley optimizes and converts each aggregation step to a concrete program, which is compiled and executed with a sparse tensor compiler. We show that Galley produces programs that are 1-300x faster than competing methods for machine learning over joins and 5-20x faster than a state-of-the-art relational database for subgraph counting workloads with a minimal optimization overhead.}
}


@article{DBLP:journals/pacmmod/CaoXYQT25,
	author = {Jiaping Cao and
                  Le Xu and
                  Man Lung Yiu and
                  Jianbin Qin and
                  Bo Tang},
	title = {{GPH:} An Efficient and Effective Perfect Hashing Scheme for {GPU}
                  Architectures},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {165:1--165:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725406},
	doi = {10.1145/3725406},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/CaoXYQT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hash tables are widely used to support fast lookup operations for various applications on key-value stores and relational databases. In recent years, hash tables have been significantly improved by utilizing the high memory bandwidth and large parallelism degree offered by Graphics Processing Units (GPUs). However, there is still a lack of comprehensive analysis of the lookup performance on existing GPU-based hash tables. In this work, we develop a micro-benchmark and devise an effective and general performance analysis model, which enables uniform and accurate lookup performance evaluation of GPU-based hash tables. Moreover, we propose GPH, a novel GPU-based hash table, to improve lookup performance with the guidance of the benchmark results from the analysis model devised above. In particular, GPH employs the perfect hashing scheme that ensures exactly 1 bucket probe for every lookup operation. Besides, we optimize the bucket requests to global memory in GPH by devising vectorization and instruction-level parallelism techniques. We also introduce the insert kernel in GPH to support dynamic updates (e.g., processing insert operations) on GPU. Experimentally, GPH achieves over 8500 million operations per second (MOPS) for lookup operation processing in both synthetic and real-world workloads, which outperforms all evaluated GPU-based hash tables.}
}


@article{DBLP:journals/pacmmod/BaiFLPZJ25,
	author = {Wenchao Bai and
                  Wenfei Fan and
                  Shuhao Liu and
                  Kehan Pang and
                  Xiaoke Zhu and
                  Jiahui Jin},
	title = {Rule-Based Graph Cleaning with GPUs on a Single Machine},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {166:1--166:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725303},
	doi = {10.1145/3725303},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BaiFLPZJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies cost-effective graph cleaning with a single machine. We adopt a rule-based method that may embed machine learning models as predicates in the rules. Graph cleaning with the rules involves rule discovery, error detection and correction. These tasks are both computation-heavy and I/O-intensive as they repeatedly invoke costly graph pattern matching, and produce a large amount of a large volume of intermediate results, among other things. In light of these, no existing single-machine system is able to carry out these tasks even on not-too-large graphs, even using GPUs. Thus we develop MiniClean, a single-machine system for cleaning large graphs. It proposes (1) a workflow that better fits a single machine by pipelining CPU, GPU and I/O operations; (2) memory footprint reduction with bundled processing and data compression; and (3) a multi-mode parallel model for SIMD, pipelined and independent parallelism, and their scheduling to maximize CPU--GPU synergy. Using real-life graphs, we empirically verify that MiniClean outperforms the SOTA single-machine systems by at least 65.34× and multi-machine systems with 32 nodes by at least 8.09×.}
}


@article{DBLP:journals/pacmmod/XuC25,
	author = {Mouyi Xu and
                  Lijun Chang},
	title = {Graph Edit Distance Estimation: {A} New Heuristic and {A} Holistic
                  Evaluation of Learning-based Methods},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {167:1--167:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725304},
	doi = {10.1145/3725304},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph edit distance (GED) is an important metric for measuring the distance or similarity between two graphs. It is defined as the minimum number of edit operations required to transform one graph into another. Computing the exact GED between two graphs is an NP-hard problem. With the success of deep learning across various application domains, graph neural networks have also been recently utilized to predict the GED between graphs. However, the existing studies on learning-based methods have two significant limitations. (1)~The development of deep learning models for GED prediction has been explored in various research fields (e.g., databases, machine learning, information retrieval, and computer vision), yet cross-field evaluations have been quite limited. (2)~More importantly, all these advancements have been evaluated against a simple combinatorial heuristic baseline, with their models shown to outperform it. In this paper, we aim to bridge this knowledge gap. We first conduct a holistic review of the existing learning-based methods, categorizing them into non-interpretable and interpretable GED prediction approaches, while highlighting their overarching design principles and relationships among these models. Secondly, we present a simple yet effective combinatorial heuristic algorithm App-BMao for GED estimation, adapted from an existing exact GED computation algorithm. App-BMao provides interpretable GED estimation with controlled time and space complexity. Extensive empirical evaluations on three widely used datasets show that the new heuristic algorithm App-BMao outperforms all existing learning-based approaches for both interpretable and non-interpretable GED prediction.}
}


@article{DBLP:journals/pacmmod/ZhouXRA25,
	author = {Libin Zhou and
                  Lu Xing and
                  Yeasir Rayhan and
                  Walid G. Aref},
	title = {{GTX:} {A} Write-Optimized Latch-free Graph Data System with Transactional
                  Support},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {168:1--168:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725305},
	doi = {10.1145/3725305},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhouXRA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces GTX, a standalone main-memory write-optimized graph data system that specializes in structural and graph property updates while enabling concurrent reads and graph analytics through ACID transactions. Recent graph systems target concurrent read and write support while guaranteeing transaction semantics. However, their performance suffers from updates with real-world temporal locality over the same vertices and edges due to vertex-centric lock contentions. GTX has an adaptive delta-chain locking protocol on top of a carefully designed latch-free graph storage. It eliminates vertex-level locking contention, and adapts to real-life workloads while maintaining sequential access to the graph's adjacency lists storage. GTX's transactions further support cache-friendly block-level concurrency control, and cooperative group commit and garbage collection. This combination of features ensures high update throughput and provides low latency graph analytics. Based on experimental evaluation, in addition to not sacrificing the performance of read-heavy analytical workloads, and having competitive performance similar to state-of-the-art systems, GTX has high read-write transaction throughput. For write-heavy transactional workloads, GTX achieves up to 11X better transaction throughput than the best-performing state-of-the-art system.}
}


@article{DBLP:journals/pacmmod/ZhangLZLTC25,
	author = {Baiyan Zhang and
                  Zepeng Li and
                  Dongxiang Zhang and
                  Huan Li and
                  Kian{-}Lee Tan and
                  Gang Chen},
	title = {High-Throughput Ingestion for Video Warehouse: Comprehensive Configuration
                  and Effective Exploration},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {169:1--169:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725407},
	doi = {10.1145/3725407},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangLZLTC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The innovative concept of Video Extract-Transform-Load (V-ETL), recently proposed in Skyscraper, reinterprets large-scale video analytics as a data warehousing problem. In this study, we aim at enabling real-time and high-throughput ingestion of hundreds of video streams and maximizing the overall accuracy, by constructing a proper ingestion plan for each video stream. To achieve the goal, we construct a comprehensive configuration space that takes into account the configurable components in the entire ingestion pipeline, including numeric parameters and categorical options such as visual inference model selection. The new space is 10 7  times larger than existing approaches, rendering them as sub-optimal points in our space. To effectively explore the huge and heterogeneous configuration space, we devise an accuracy-aware search strategy based on graph embedding and reinforcement learning to establish the runtime-quality Pareto frontier. To reduce the configuration exploration cost for all video streams, we cluster video streams with similar contexts and adopt mixed integer programming to maximize the overall ingestion accuracy while ensuring the real-time ingestion requirement. In the experimental evaluation with one NVIDIA GeForce RTX 4090 GPU card, our Hippo can support real-time ingestion with 300 video streams and secures an ingestion accuracy that exceeds its competitors by more than 30%.}
}


@article{DBLP:journals/pacmmod/WuS25,
	author = {Jiacheng Wu and
                  Dan Suciu},
	title = {HoneyComb: {A} Parallel Worst-Case Optimal Join on Multicores},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {170:1--170:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725307},
	doi = {10.1145/3725307},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To achieve true scalability on massive datasets, a modern query engine needs to be able to take advantage of large, shared-memory, multicore systems.  Binary joins  are conceptually easy to parallelize on a multicore system; however, several applications require a different approach to query evaluation, using a Worst-Case Optimal Join (WCOJ) algorithm. WCOJ is known to outperform traditional query plans for cyclic queries. However, there is no obvious adaptation of WCOJ to parallel architectures. The few existing systems that parallelizeWCOJ do this by partitioning only the top variable of theWCOJ algorithm. This leads to work skew (since some relations end up being read entirely by every thread), possible contention between threads (when the hierarchical trie index is built lazily, which is the case on most recent WCOJ systems), and exacerbates the redundant computations already existing in WCOJ.}
}


@article{DBLP:journals/pacmmod/KangGMS25,
	author = {Dakai Kang and
                  Suyash Gupta and
                  Dahlia Malkhi and
                  Mohammad Sadoghi},
	title = {HotStuff-1: Linear Consensus with One-Phase Speculation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {171:1--171:29},
	year = {2025},
	url = {https://doi.org/10.1145/3725308},
	doi = {10.1145/3725308},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KangGMS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces HotStuff-1, a BFT consensus protocol that improves the latency of HotStuff-1 by two network hops while maintaining linear communication complexity against faults. Furthermore, HotStuff-1 incorporates an incentive-compatible leader rotation design that motivates leaders to propose transactions promptly. HotStuff-1 achieves a reduction of two network hops by  speculatively  sending clients early finality confirmations, after one phase of the protocol. Introducing speculation into streamlined protocols is challenging because, unlike stable-leader protocols, these protocols cannot stop the consensus and recover from failures. Thus, we identify  prefix speculation dilemma  in the context of streamlined protocols; HotStuff-1 is the first streamlined protocol to resolve it. HotStuff-1 embodies an additional mechanism,  slotting , that thwarts delays caused by (1) rationally-incentivized leaders and (2) malicious leaders inclined to sabotage others' progress. The slotting mechanism allows leaders to dynamically drive as many decisions as allowed by network transmission delays before view timers expire, thus mitigating both threats.}
}


@article{DBLP:journals/pacmmod/HeinrichLWKB25,
	author = {Roman Heinrich and
                  Manisha Luthra and
                  Johannes Wehrstein and
                  Harald Kornmayer and
                  Carsten Binnig},
	title = {How Good are Learned Cost Models, Really? Insights from Query Optimization
                  Tasks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {172:1--172:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725309},
	doi = {10.1145/3725309},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HeinrichLWKB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditionally, query optimizers rely on cost models to choose the best execution plan from several candidates, making precise cost estimates critical for efficient query execution. In recent years, cost models based on machine learning have been proposed to overcome the weaknesses of traditional cost models. While these models have been shown to provide better prediction accuracy, only limited efforts have been made to investigate how well  Learned Cost Models  (LCMs) actually perform in query optimization and how they affect overall query performance. In this paper, we address this by a systematic study evaluating LCMs on three of the core query optimization tasks:  join ordering, access path selection,  and  physical operator selection . In our study, we compare seven state-of-the-art LCMs to a traditional cost model and, surprisingly, find that the traditional model often still outperforms LCMs in these tasks. We conclude by highlighting major takeaways and recommendations to guide future research toward making LCMs more effective for query optimization.}
}


@article{DBLP:journals/pacmmod/MoLI25,
	author = {Dingheng Mo and
                  Siqiang Luo and
                  Stratos Idreos},
	title = {How to Grow an LSM-tree? Towards Bridging the Gap Between Theory and
                  Practice},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {173:1--173:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725310},
	doi = {10.1145/3725310},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MoLI25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LSM-tree based key-value stores are widely adopted as the data storage backend in modern big data applications. The LSM-tree  grows  with data ingestion, by either adding levels with fixed level capacities (dubbed as vertical scheme) or increasing level capacities with fixed number of levels (dubbed as horizontal scheme). The vertical scheme leads the trend in recent system designs in RocksDB, LevelDB, and WiredTiger, whereas the horizontal scheme shows a decline in being adopted in the industry. The growth scheme profoundly impacts the LSM system performance in various aspects such as read, write and space costs. This paper attempts to give a new insight into a fundamental design question --  how to grow an LSM-tree  to attain more desirable performance? Our analysis highlights the limitations of the vertical scheme in achieving an optimal read-write trade-off and the horizontal scheme in managing space cost effectively. Building on the analysis, we present a novel approach, Vertiorizon, which combines the strengths of both the vertical and horizontal schemes to achieve a superior balance between lookup, update, and space costs. Its adaptive design makes it highly compatible with a wide spectrum of workloads. Compared to the vertical scheme, Vertiorizon significantly improves the read-write performance trade-off. In contrast to the horizontal scheme, Vertiorizon greatly extends the trade-off range by a non-trivial generalization of Bentley and Saxe's theory, while substantially reducing space costs. When integrated with RocksDB, Vertiorizon demonstrates better write performance than the vertical scheme, while incurring about six times less additional space cost compared to the horizontal scheme.}
}


@article{DBLP:journals/pacmmod/KakkarCSAK25,
	author = {Gaurav Tarlok Kakkar and
                  Jiashen Cao and
                  Aubhro Sengupta and
                  Joy Arulraj and
                  Hyesoon Kim},
	title = {Aero: Adaptive Query Processing of {ML} Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {174:1--174:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725408},
	doi = {10.1145/3725408},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KakkarCSAK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query optimization is critical in relational database management systems (DBMSs) for ensuring efficient query processing. The query optimizer relies on precise selectivity and cost estimates to generate optimal query plans for execution. However, this static query optimization approach falls short for DBMSs handling machine learning (ML) queries. ML-centric DBMSs face distinct challenges in query optimization. First, performance bottlenecks shift to user-defined functions (UDFs), often encapsulating deep learning models, making it difficult to estimate UDF statistics without profiling the query. Second, optimal query plans for ML queries are data-dependent, requiring dynamic plan adjustments during execution. To address these challenges, we introduce Aero, an ML-centric DBMS that utilizes adaptive query processing (AQP) for efficiently processing ML queries. Aero optimizes the evaluation of UDF-based query predicates by dynamically adjusting predicate evaluation order and enhancing UDF execution scalability. By integrating AQP, Aero continuously monitors UDF statistics, routes data to predicates in an optimal order, and dynamically allocates resources for evaluating predicates. Aero achieves up to 6.4x speedup compared to a state-of-the-art ML-centric DBMS across four diverse use cases, with no impact on accuracy.}
}


@article{DBLP:journals/pacmmod/ChenFZ25,
	author = {Haoxian Chen and
                  Wenfei Fan and
                  Jiaye Zheng},
	title = {Incremental Rule Discovery in Response to Parameter Updates},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {175:1--175:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725312},
	doi = {10.1145/3725312},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenFZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies incremental rule discovery. Given a dataset D, rule discovery is to mine the set of the rules on D such that their supports and confidences are above thresholds 𝜎 and 𝛅, respectively. We formulate incremental problems in response to updates Δ𝜎 and/or Δ𝛅, to compute rules added and/or removed with respect to 𝜎 + Δ𝜎 and 𝛅 + Δ𝛅. The need for studying the problems is evident since practitioners often want to adjust their support and confidence thresholds during discovery. The objective is to minimize unnecessary recomputation during the adjustments, not to restart the costly discovery process from scratch. As a testbed, we consider entity enhancing rules, which subsume popular data quality rules as special cases. We develop three incremental algorithms, in response to Δ𝜎, Δ𝜎 and both. We show that relative to a batch discovery algorithm, these algorithms are bounded, i.e., they incur the minimum cost among all incrementalizations of the batch one, and parallelly scalable, i.e., they guarantee to reduce runtime when given more processors. Using real-life data, we empirically verify that the incremental algorithms outperform the batch counterpart by up to 658× when Δ𝜎 and Δ𝜎 are either positive or negative.}
}


@article{DBLP:journals/pacmmod/ZhangLLZQW25,
	author = {Yalong Zhang and
                  Rong{-}Hua Li and
                  Longlong Lin and
                  Qi Zhang and
                  Lu Qin and
                  Guoren Wang},
	title = {Integral Densest Subgraph Search on Directed Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {176:1--176:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725313},
	doi = {10.1145/3725313},
	timestamp = {Fri, 18 Jul 2025 07:33:10 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangLLZQW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The densest subgraph (DS) search over a directed graph focuses on finding the subgraph with the highest density among all subgraphs. This problem has raised numerous applications, such as fraud detection and community detection. The state-of-the-art DS algorithms have prohibitively high costs or poor approximation ratios, making them unsuitable for practical applications. To address these dilemmas, in this paper, we propose a novel model called integral densest subgraph (IDS). We show that IDS can serve as a near-DS model that has a tight floor relationship with the density of the DS. To compute IDS, we first propose a novel flow network named (α,β)-dense network, based on which we design an exact network-flow algorithm GetIDS with O(p • log |V| • |E| 1.5 ) time complexity, where  p  is typically a small constant in real-world graphs. Additionally, we propose several non-trivial pruning techniques to further improve the efficiency. Subsequently, we propose a novel (2 + ε)-approximation algorithm MultiCore with near-linear time complexity, providing a good approximation guarantee with high efficiency. Finally, our extensive experiments on 10 real-world graphs demonstrate the effectiveness of the proposed IDS model, and the high efficiency and scalability of the proposed solutions.}
}


@article{DBLP:journals/pacmmod/LuWT25,
	author = {Shangqi Lu and
                  Ru Wang and
                  Yufei Tao},
	title = {Interactive Graph Search Made Simple},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {177:1--177:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725409},
	doi = {10.1145/3725409},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuWT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interactive graph search (IGS) has proven to be a useful information retrieval paradigm in a diverse set of applications. Robust IGS algorithms are notoriously difficult to design because they are deeply rooted in graph theory. The current state-of-the-art algorithms either fail to achieve an optimal number of interaction rounds or rely on interfaces demanding tedious user inputs. Furthermore, previous research has paid little attention to the underlying computation bottleneck, which is currently dealt with using primitive implementations. This work remedies the above issues altogether. Utilizing novel findings on the problem characteristics, we develop an algorithmic framework for IGS that requires a designer to fill in the details for only two ''black-box'' operations. Our framework, when instantiated with surprisingly simple black-box implementations, yields optimal algorithms not only in all the scenarios explored before but also in new scenarios never studied. We accompany our framework, designed to minimize interaction rounds, with a new algorithm designed to reduce the CPU time complexity significantly. Extensive experiments on both real and synthetic data confirm both the efficacy and efficiency of the proposed techniques.}
}


@article{DBLP:journals/pacmmod/ZhangZM25,
	author = {Xukang Zhang and
                  Huanchen Zhang and
                  Xiaofeng Meng},
	title = {Intra-Query Runtime Elasticity for Cloud-Native Data Analysis},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {178:1--178:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725315},
	doi = {10.1145/3725315},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose the concept of  Intra-Query Runtime Elasticity  (IQRE) for cloud-native data analysis. IQRE enables a cloud-native OLAP engine to dynamically adjust a query\'s  Degree of Parallelism  (DOP) during execution. This capability allows users to utilize cloud computing resources more cost-effectively. We present Accordion, the first IQRE query engine. Accordion can adjust the parallelism of a query at any point during query execution without pausing data processing. It features a user-friendly interface and an auto-tuner backed by a "what-if" service to allow users to adjust the DOP according to their query latency constraints. The design of Accordion follows the execution model in Presto, an open-source distributed SQL query engine developed at Meta. We present the implementation of Accordion and demonstrate its ease of use, showcasing how it enables users to minimize compute resource consumption while meeting their query time constraints.}
}


@article{DBLP:journals/pacmmod/TaoMJZGM25,
	author = {Jeffrey Tao and
                  Natalie Maus and
                  Haydn Thomas Jones and
                  Yimeng Zeng and
                  Jacob R. Gardner and
                  Ryan Marcus},
	title = {Learned Offline Query Planning via Bayesian Optimization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {179:1--179:29},
	year = {2025},
	url = {https://doi.org/10.1145/3725316},
	doi = {10.1145/3725316},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/TaoMJZGM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analytics database workloads often contain queries that are executed repeatedly. Existing optimization techniques generally prioritize keeping optimization cost low, normally well below the time it takes to execute a single instance of a query. If a given query is going to be executed thousands of times, could it be worth investing significantly more optimization time? In contrast to traditional online query optimizers, we propose an offline query optimizer that searches a wide variety of plans and incorporates query execution as a primitive. Our offline query optimizer combines variational auto-encoders with Bayesian optimization to find optimized plans for a given query. We compare our technique to the optimal plans possible with PostgreSQL and recent RL-based systems over several datasets, and show that our technique finds faster query plans.}
}


@article{DBLP:journals/pacmmod/ChanthatrojwongBC25,
	author = {Kasidis Chanthatrojwong and
                  Sourav S. Bhowmick and
                  Byron Choi},
	title = {{LICS:} Towards Theory-Informed Effective Visual Abstraction of Property
                  Graph Schemas},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {180:1--180:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725410},
	doi = {10.1145/3725410},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChanthatrojwongBC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Property graph schemas are essential for organizing property graph data, serving both prescriptive and descriptive roles. This has led to the recent development of property graph schema languages such as PG Schema . While understanding of these languages requires familiarity with complex syntax, this poses usability challenges, particularly for domain experts who are not programmers. Current visual abstractions, such as the  labeled schema graph  (łsg), simplify representation but suffers from visual clutter and limited feature support. To address these challenges, we propose a novel,  generic,  and  extensible  visual abstraction,  labeled iconized composite schema  (łics), whose design is informed by theories and principles from HCI, cognitive psychology, and visualization. A novel łics-based visual interface coined  PASCAL  is also proposed to facilitate visualization of property graph schemas. Under the hood, it leverages the  Map-Paint  algorithm for creating the visual components of łics. A user study demonstrates that łics is superior to the traditional łsg abstraction w.r.t. usability, effectiveness, query formulation efficiency, and schema comprehension.}
}


@article{DBLP:journals/pacmmod/SatrianiVSRVP25,
	author = {Dario Satriani and
                  Enzo Veltri and
                  Donatello Santoro and
                  Sara Rosato and
                  Simone Varriale and
                  Paolo Papotti},
	title = {Logical and Physical Optimizations for {SQL} Query Execution over
                  Large Language Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {181:1--181:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725411},
	doi = {10.1145/3725411},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SatrianiVSRVP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interacting with Large Language Models (LLMs) via declarative queries is increasingly popular for tasks like question answering and data extraction, thanks to their ability to process vast unstructured data. However, LLMs often struggle with answering complex factual questions, exhibiting low precision and recall in the returned data. This challenge highlights that executing queries on LLMs remains a largely unexplored domain, where traditional data processing assumptions often fall short. Conventional query optimization, typically cost-driven, overlooks LLM-specific quality challenges such as contextual understanding. Just as new physical operators are designed to address the unique characteristics of LLMs, optimization must consider these quality challenges. Our results highlight that adhering strictly to conventional query optimization principles fails to generate the best plans in terms of result quality. To tackle this challenge, we present a novel approach to enhance SQL results by applying query optimization techniques specifically adapted for LLMs. We introduce a database system, GALOIS, that sits between the query and the LLM, effectively using the latter as a storage layer. We design alternative physical operators tailored for LLM-based query execution and adapt traditional optimization strategies to this novel context. For example, while pushing down operators in the query plan reduces execution cost (fewer calls to the model), it might complicate the call to the LLM and deteriorate result quality. Additionally, these models lack a traditional catalog for optimization, leading us to develop methods to dynamically gather such metadata during query execution. Our solution is compatible with any LLM and balances the trade-off between query result quality and execution cost. Experiments show up to 144% quality improvement over questions in Natural Language and 29% over direct SQL execution, highlighting the advantages of integrating database solutions with LLMs.}
}


@article{DBLP:journals/pacmmod/HuangZZXW25,
	author = {Kaisong Huang and
                  Jiatang Zhou and
                  Zhuoyue Zhao and
                  Dong Xie and
                  Tianzheng Wang},
	title = {Low-Latency Transaction Scheduling via Userspace Interrupts: Why Wait
                  or Yield When You Can Preempt?},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {182:1--182:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725319},
	doi = {10.1145/3725319},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangZZXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional non-preemptive scheduling can lead to long latency under workloads that mix long-running and short transactions with varying priorities. This occurs because worker threads tend to monopolize CPU cores until they finish processing long-running transactions. Thus, short transactions must wait for the CPU, leading to long latency. As an alternative, cooperative scheduling allows for transaction yielding, but it is difficult to tune for diverse workloads. Although preemption could potentially alleviate this issue, it has seen limited adoption in DBMSs due to the high delivery latency of software interrupts and concerns on wasting useful work induced by read-write lock conflicts in traditional lock-based DBMSs. In this paper, we propose PreemptDB, a new database engine that leverages recent userspace interrupts available in modern CPUs to enable efficient preemptive scheduling. We present an efficient transaction context switching mechanism purely in userspace and scheduling policies that prioritize short, high-priority transactions without significantly affecting long-running queries. Our evaluation demonstrates that PreemptDB significantly reduces end-to-end latency for high-priority transactions compared to non-preemptive FIFO and cooperative scheduling methods.}
}


@article{DBLP:journals/pacmmod/YiTIM25,
	author = {Zixuan Yi and
                  Yao Tian and
                  Zachary G. Ives and
                  Ryan Marcus},
	title = {Low Rank Learning for Offline Query Optimization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {183:1--183:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725412},
	doi = {10.1145/3725412},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YiTIM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent deployments of learned query optimizers use expensive neural networks and ad-hoc search policies. To address these issues, we introduce LimeQO, a framework for offline query optimization leveraging low-rank learning to efficiently explore alternative query plans with minimal resource usage. By modeling the workload as a partially observed, low-rank matrix, we predict unobserved query plan latencies using purely linear methods, significantly reducing computational overhead compared to neural networks. We formalize offline exploration as an active learning problem, and present simple heuristics that reduces a 3-hour workload to 1.5 hours after just 1.5 hours of exploration. Additionally, we propose a transductive Tree Convolutional Neural Network (TCNN) that, despite higher computational costs, achieves the same workload reduction with only 0.5 hours of exploration. Unlike previous approaches that place expensive neural networks directly in the query processing ''hot'' path, our approach offers a low-overhead solution and a no-regressions guarantee, all without making assumptions about the underlying DBMS.}
}


@article{DBLP:journals/pacmmod/ZhangMKOS25,
	author = {Haozhe Zhang and
                  Christoph Mayer and
                  Mahmoud Abo Khamis and
                  Dan Olteanu and
                  Dan Suciu},
	title = {LpBound: Pessimistic Cardinality Estimation Using {\(\mathscr{l}\)}\({}_{\mbox{p}}\)-Norms
                  of Degree Sequences},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {184:1--184:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725321},
	doi = {10.1145/3725321},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangMKOS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality estimation is the problem of estimating the size of the output of a query, without actually evaluating the query. The cardinality estimator is a critical piece of a query optimizer, and is often the main culprit when the optimizer chooses a poor plan. This paper introduces LpBound, a pessimistic cardinality estimator for multi-join queries (acyclic or cyclic) with selection predicates and group-by clauses.  LpBound  computes a guaranteed upper bound on the size of the query output using simple statistics on the input relations, consisting of ℓ p -norms of degree sequences. The bound is the optimal solution of a linear program whose constraints encode data statistics and Shannon inequalities. We introduce two optimizations that exploit the structure of the query in order to speed up the estimation time and make  LpBound  practical. We experimentally evaluate  LpBound  against a range of traditional, pessimistic, and machine learning-based estimators on the JOB, STATS, and subgraph matching benchmarks. Our main finding is that  LpBound  can be orders of magnitude more accurate than traditional estimators used in mainstream open-source and commercial database systems. Yet it has comparable low estimation time and space requirements. When injected the estimates of  LpBound , Postgres derives query plans at least as good as those derived using the true cardinalities.}
}


@article{DBLP:journals/pacmmod/LiFGLWNWZNC25,
	author = {Haoyang Li and
                  Fangcheng Fu and
                  Hao Ge and
                  Sheng Lin and
                  Xuanyu Wang and
                  Jiawen Niu and
                  Yujie Wang and
                  Hailin Zhang and
                  Xiaonan Nie and
                  Bin Cui},
	title = {Malleus: Straggler-Resilient Hybrid Parallel Training of Large-scale
                  Models via Malleable Data and Model Parallelization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {185:1--185:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725322},
	doi = {10.1145/3725322},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiFGLWNWZNC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the scale of models and training data continues to grow, there is an expanding reliance on more GPUs to train large-scale models, which inevitably increases the likelihood of encountering dynamic stragglers that some devices lag behind in performance occasionally. However, hybrid parallel training, one of the de facto paradigms to train large models, is typically sensitive to the stragglers. This paper presents  Malleus , a straggler-resilient hybrid parallel training framework for large-scale models.  Malleus  quantifies the stragglers at the nuanced, per-GPU granularity during training, and develops a novel planning algorithm to deduce the optimal parallelization of GPU devices, pipeline stages, model layers, and training data, maximizing training efficiency when stragglers exist. In addition, once a shift in the straggler situation is detected,  Malleus  adaptively adjusts the parallelization via a re-planning process, and seamlessly and efficiently migrates the model states on the fly, without sacrificing the stability of the training tasks. Empirical results on large language models with up to 110B parameters show that  Malleus  consistently outperforms existing parallel training frameworks under various straggler situations, delivering on average 2.63-5.28x of efficiency improvement.}
}


@article{DBLP:journals/pacmmod/ShiLLDZZ25,
	author = {Zhichao Shi and
                  Youhuan Li and
                  Ziming Li and
                  Yuequn Dou and
                  Xionghu Zhong and
                  Lei Zou},
	title = {MatCo: Computing Match Cover of Subgraph Query over Graph Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {186:1--186:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725323},
	doi = {10.1145/3725323},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShiLLDZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph query can be applied in various scenarios, such as fraud detection and cyberattack pattern analysis. However, computing subgraph queries usually traverses a huge search space. Many efforts have been made to reduce this search space. The size of the answer set can be exponential, providing a substantial lower bound for the search space. Additionally, different answers may overlap, and a single vertex can occur multiple times in different matches. In this paper, we propose a new problem to compute the match cover of a subgraph query. We define the match cover as a subset of answers such that the vertices included are exactly the same as those in the entire set. There can be more than one match covers, however, we only return one, as long as we can avoid the huge overhead of searching the entire set. It is inefficient to apply traditional subgraph query methods for computing match cover. Specifically, existing methods do not prune partial matches that could grow into full matches. For match cover computation, if the vertices in those full matches are already included in previously found matches, continuing the computation over such partial matches is a waste of time. We propose a new framework, called MatCo, to compute the match cover. In MatCo, we design a new data structure, called local candidate space, to determine whether the future search scopes of partial matches have been covered. We can easily maintain local candidate space and efficiently conduct the determination. We also reduce some Cartesian products, which are inevitable in existing methods, into linear enumerations, which significantly improves performance. Extensive experiments over various datasets confirm that our method outperforms comparative ones by 1~3 orders of magnitude. Efficiently computing the minimum match cover could be an interesting future work.}
}


@article{DBLP:journals/pacmmod/KabicCA25,
	author = {Marko Kabic and
                  Shriram Chandran and
                  Gustavo Alonso},
	title = {Maximus: {A} Modular Accelerated Query Engine for Data Analytics on
                  Heterogeneous Systems},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {187:1--187:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725324},
	doi = {10.1145/3725324},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KabicCA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Several trends are changing the underlying fabric for data processing in fundamental ways. On the hardware side, machines are becoming heterogeneous with smart NICs, TPUs, DPUs, etc., but specially with GPUs taking a more dominant role. On the software side, the diversity in workloads, data sources, and data formats has given rise to the notion of composable data processing where the data is processed across a variety of engines and platforms. Finally, on the infrastructure side, different storage types, disaggregated storage, disaggregated memory, networking, and interconnects are all rapidly evolving, which demands a degree of customization to optimize data movement well beyond established techniques. To tackle these challenges, in this paper, we present Maximus, a modular data processing engine that embraces heterogeneity from the ground up. Maximus can run queries on CPUs and GPUs, can split execution between CPUs and GPUs, import and export data in a variety of formats, interact with a wide range of query engines through Substrait, and efficiently manage the execution of complex data processing pipelines. Through the concept of operator-level integration, Maximus can use operators from third-party engines and achieve even better performance with these operators than when they are used with their native engines. The current version of Maximus supports all TPC-H queries on both the GPU and the CPU and optimizes the data movement and kernel execution between them, enabling the overlap of communication and computation to achieve performance comparable to that of the best systems available, but with a far higher degree of completeness and flexibility.}
}


@article{DBLP:journals/pacmmod/VorugantiO25,
	author = {Sairaj Voruganti and
                  M. Tamer {\"{O}}zsu},
	title = {{MIRAGE-ANNS:} Mixed Approach Graph-based Indexing for Approximate
                  Nearest Neighbor Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {188:1--188:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725325},
	doi = {10.1145/3725325},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/VorugantiO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate nearest neighbor search (ANNS) on high dimensional vectors is important for numerous applications, such as search engines, recommendation systems, and more recently, large language models (LLMs), where Retrieval Augmented Generation (RAG) is used to add context to an LLM query. Graph-based indexes built on these vectors have been shown to perform best but have challenges. These indexes can either employ refinement-based construction strategies such as K-Graph and NSG, or increment-based strategies such as HNSW. Refinement-based approaches have fast construction times, but worse search performance and do not allow for incremental inserts, requiring a full reconstruction each time new vectors are added to the index. Increment-based approaches have good search performance and allow for incremental inserts, but suffer from slow construction. This work presents MIRAGE-ANNS ( M ixed  I ncremental  R efinement  A pproach  G raph-based  E xploration for Approximate Nearest Neighbor Search) that constructs the index as fast as refinement-based approaches while retaining search performance comparable or better than increment-based ones. It also allows incremental inserts. We show that MIRAGE achieves state of the art construction and query performance, outperforming existing methods by up to 2x query throughput on real-world datasets.}
}


@article{DBLP:journals/pacmmod/ZhangPXXY25,
	author = {Chenyang Zhang and
                  Junxiong Peng and
                  Chen Xu and
                  Quanqing Xu and
                  Chuanhui Yang},
	title = {Mitigating the Impedance Mismatch between Prediction Query Execution
                  and Database Engine},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {189:1--189:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725326},
	doi = {10.1145/3725326},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangPXXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Prediction queries that apply machine learning (ML) models to perform analysis on data stored in the database are prevalent with the advance of research. Current database systems introduce Python UDFs to express prediction queries and call ML frameworks for inference. However, the impedance mismatch between database engines and prediction query execution imposes a challenge for query performance. First, the database engine is oblivious to the internal semantics of prediction functions and evaluates the UDF holistically, which incurs the repetitive inference context setup. Second, the invocation of prediction functions in the database does not consider that batching inference with a desirable inference batch size achieves a high performance in ML frameworks. To mitigate the mismatch, we propose to employ a prediction-aware operator in database engines, which leverages  inference context reuse cache  to achieve an automatic one-off inference context setup and  batch-aware function invocation  to ensure desirable batching inference. We implement a prototype system, called IMBridge, based on an open-source database OceanBase. Our experiments show that IMBridge achieves a 71.4x speedup on average over OceanBase for prediction query execution and significantly outperforms other solutions.}
}


@article{DBLP:journals/pacmmod/ZhuWMA25,
	author = {Zichen Zhu and
                  Yanpeng Wei and
                  Ju Hyoung Mun and
                  Manos Athanassoulis},
	title = {Mnemosyne: Dynamic Workload-Aware {BF} Tuning via Accurate Statistics
                  in {LSM} trees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {190:1--190:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725327},
	doi = {10.1145/3725327},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhuWMA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log-structured merge (LSM) trees typically employ Bloom Filters (BFs) to prevent unnecessary disk accesses for point queries. The size of BFs can be tuned to navigate a memory vs. performance tradeoff. State-of-the-art memory allocation strategies use a worst-case model for point lookup cost to derive a closed-form solution. However, existing approaches have three limitations: (1) the number of key-value pairs to be ingested must be  known a priori , (2) the closed-form solution only works for a  perfectly shaped  LSM tree, and (3) the model assumes a  uniform query distribution . Due to these limitations, the available memory budget for BFs is sub-optimally utilized, especially when the system is under memory pressure (i.e., less than 7 bits per key). In this paper, we design Mnemosyne, a BF reallocation framework for  evolving  LSM trees that does not require prior workload knowledge. We use a more general query cost model that considers the access pattern  per file , and we find that no system accurately maintains access statistics per file, and that simply maintaining a counter per file significantly deviates from the ground truth for evolving LSM trees. To address this, we propose Merlin, a dynamic sliding-window-based tracking mechanism that accurately captures these statistics. The upgraded Mnemosyne^+ combines Merlin with our new cost model. In our evaluation, Mnemosyne reduces query latency by up to 20% compared to RocksDB under memory pressure, and Mnemosyne^+ further improves throughput by another 10% when workloads exhibit higher skew.}
}


@article{DBLP:journals/pacmmod/NguyenAZL25,
	author = {Lam{-}Duy Nguyen and
                  Adnan Alhomssi and
                  Tobias Ziegler and
                  Viktor Leis},
	title = {Moving on From Group Commit: Autonomous Commit Enables High Throughput
                  and Low Latency on NVMe SSDs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {191:1--191:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725328},
	doi = {10.1145/3725328},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/NguyenAZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Achieving both high throughput and low commit latency has long been a difficult challenge for Database Management Systems (DBMSs). As we show in this paper, existing commit processing protocols fail to fully leverage modern NVMe SSDs to deliver both high throughput and low-latency durable commits. We therefore propose  autonomous commit , the first commit protocol that fully utilizes modern NVMe SSDs to achieve both objectives. Our approach exploits the high parallelism and low write latency of SSDs, enabling workers to explicitly write logs in smaller batches, thereby minimizing the impact of logging I/O on commit latency. Additionally, by parallelizing the acknowledgment procedure, where the DBMS iterates through a set of transactions to inspect their commit state, we mitigate excessive delays resulting from single-threaded commit operations in high-throughput workloads. Our experimental results show that autonomous commit achieves exceptional scalability and low-latency durable commits across a wide range of workloads.}
}


@article{DBLP:journals/pacmmod/ReyRN25,
	author = {Alice Rey and
                  Maximilian Rieger and
                  Thomas Neumann},
	title = {Nested Parquet Is Flat, Why Not Use It? How To Scan Nested Data With
                  On-the-Fly Key Generation and Joins},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {192:1--192:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725329},
	doi = {10.1145/3725329},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ReyRN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parquet is the most commonly used file format to store data in a columnar, binary structure. The format also supports storing nested data in this flattened columnar layout. However, many query engines either do not support nested data or process it with substantially worse performance than relational data. In this work, we close this gap and present a new way to leverage relational query engines for nested data that is stored in this flat columnar file format. Specifically, we demonstrate how to process nested Parquet files much more efficiently. Our approach does not store a copy of the data in an internal format but reads directly from the Parquet file. During query computation, the required flat columns are scanned independently and the nesting is reconstructed using joins with on-the-fly generated join keys. Our approach can be easily integrated into existing query engines to support querying nested Parquet files. Furthermore, we achieve orders of magnitude faster analytical query performance than existing solutions, which makes it a valuable addition.}
}


@article{DBLP:journals/pacmmod/ShiYCL25,
	author = {Jiachen Shi and
                  Jingyi Yang and
                  Gao Cong and
                  Xiaoli Li},
	title = {{NEXT:} {A} New Secondary Index Framework for LSM-based Data Storage},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {193:1--193:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725330},
	doi = {10.1145/3725330},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShiYCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Key-value databases with Log Structured Merge tree are increasingly favored by modern applications. Apart from supporting fast lookup on primary key, efficient queries on non-key attributes are also highly demanded by many of these applications. To enhance query performance, many auxiliary structures like secondary indexing and filters have been developed. However, existing auxiliary structures suffer from three limitations. First, creating filter for every disk component has low lookup efficiency as all components need to be searched during query processing. Second, current secondary index design requires primary table access to fetch the data entries for each output primary key from the index. This indirect entries fetching process involves significant point lookup overhead in the primary table and hence hinders the query performance. Last, maintaining the consistency between the secondary index and the primary table is challenging due to the out-of-place update mechanism of the LSM-tree. To overcome the limitations in existing auxiliary structures for non-key attributes queries, this paper proposes a novel secondary index framework, NEXT, for LSM-based key-value storage system. NEXT utilizes a two-level structure which is integrated with the primary table. In particular, NEXT proposes to create secondary index blocks on each LSM disk component to map the secondary attributes to their corresponding data blocks. In addition, NEXT introduces a global index component which is created on top of all secondary index blocks to direct the secondary index operation to the target secondary index blocks. Finally, NEXT adopts two optimization strategies to further improve the query performance. We implement NEXT on RocksDB and experimentally evaluate its performance against existing methods. Experiments on both static and mixed workloads demonstrate that NEXT outperforms existing methods for different types of non-key attributes.}
}


@article{DBLP:journals/pacmmod/XieXZG25,
	author = {Xiangjin Xie and
                  Guangwei Xu and
                  Lingyan Zhao and
                  Ruijie Guo},
	title = {OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and Consistency
                  Alignment},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {194:1--194:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725331},
	doi = {10.1145/3725331},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XieXZG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although multi-agent collaborative Large Language Models (LLMs) have achieved significant breakthroughs in the Text-to-SQL task, their performance is still constrained by various factors. These factors include the incompleteness of the framework, failure to follow instructions, and model hallucinations. To address these problems, we propose OpenSearch-SQL, which divides the Text-to-SQL task into four main modules: Preprocessing, Extraction, Generation, and Refinement, along with an Alignment module based on a consistency alignment mechanism. This architecture aligns the inputs and outputs of agents through the Alignment module, reducing failures in instruction following and hallucination. Furthermore, we introduce SQL-Like (an intermediate language), optimize the structured Chain-of-Thought (CoT) based on SQL-Like, and develop a dynamic few-shot strategy via self-taught Query-CoT-SQL. In terms of model selection, we directly applied the base LLMs without any post-training, thereby simplifying the task chain and enhancing the framework's portability. Experimental results show that OpenSearch-SQL achieves an execution accuracy(EX) of 69.3% on the BIRD development set, 72.28% on the test set, and a reward-based validity efficiency score (R-VES) of 69.36%, with all three metrics ranking first at the time of submission. These results demonstrate the comprehensive advantages of the proposed method in both effectiveness and efficiency.}
}


@article{DBLP:journals/pacmmod/LiuDGS25,
	author = {Youzhe Liu and
                  Xiaojun Dong and
                  Yan Gu and
                  Yihan Sun},
	title = {Parallel \emph{k}-Core Decomposition: Theory and Practice},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {195:1--195:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725332},
	doi = {10.1145/3725332},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuDGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes efficient solutions for  k -core decomposition with high parallelism. The problem of  k -core decomposition is fundamental in graph analysis and has applications across various domains. However, existing algorithms face significant challenges in achieving work-efficiency in theory and/or high parallelism in practice, and suffer from various performance bottlenecks. We present a simple, work-efficient parallel framework for  k -core decomposition that is easy to implement and adaptable to various strategies for improving work-efficiency. We introduce two techniques to enhance parallelism: a sampling scheme to reduce contention on high-degree vertices, and vertical granularity control (VGC) to mitigate scheduling overhead for low-degree vertices. Furthermore, we design a hierarchical bucket structure to optimize performance for graphs with high coreness values. We evaluate our algorithm on a diverse set of real-world and synthetic graphs. Compared to state-of-the-art parallel algorithms, including ParK, PKC, and Julienne, our approach demonstrates superior performance on 23 out of 25 graphs when tested on a 96-core machine. Our algorithm shows speedups of up to 315× over ParK, 33.4× over PKC, and 52.5× over Julienne.}
}


@article{DBLP:journals/pacmmod/KuffoKB25,
	author = {Leonardo X. Kuffo and
                  Elena Krippner and
                  Peter A. Boncz},
	title = {{PDX:} {A} Data Layout for Vector Similarity Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {196:1--196:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725333},
	doi = {10.1145/3725333},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KuffoKB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose Partition Dimensions Across (PDX), a data layout for vectors (e.g., embeddings) that, similar to PAX [6], stores multiple vectors in one block, using a vertical layout for the dimensions (Figure 1). PDX accelerates exact and approximate similarity search thanks to its dimension-by-dimension search strategy that operates on multiple-vectors-at-a-time in tight loops. It beats SIMD-optimized distance kernels on standard horizontal vector storage (avg 40% faster), only relying on scalar code that gets auto-vectorized. We combined the PDX layout with recent dimension-pruning algorithms ADSampling [19] and BSA [52] that accelerate approximate vector search. We found that these algorithms on the horizontal vector layout can lose to SIMD-optimized linear scans, even if they are SIMD-optimized. However, when used on PDX, their benefit is restored to 2-7x. We find that search on PDX is especially fast if a limited number of dimensions has to be scanned fully, which is what the dimension-pruning approaches do. We finally introduce PDX-BOND, an even more flexible dimension-pruning strategy, with good performance on exact search and reasonable performance on approximate search. Unlike previous pruning algorithms, it can work on vector data ''as-is'' without preprocessing; making it attractive for vector databases with frequent updates.}
}


@article{DBLP:journals/pacmmod/ChenLTRMGNPRW25,
	author = {Yiru Chen and
                  Xupeng Li and
                  Jeffrey Tao and
                  Lana Ramjit and
                  Subrata Mitra and
                  Javad Ghaderi and
                  Ravi Netravali and
                  Aditya G. Parameswaran and
                  Dan Rubenstein and
                  Eugene Wu},
	title = {Physical Visualization Design: Decoupling Interface and System Design},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {197:1--197:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725334},
	doi = {10.1145/3725334},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenLTRMGNPRW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interactive visualization interfaces enable users to efficiently explore, analyze, and make sense of their datasets. However, as data grows in size, it becomes increasingly challenging to build data interfaces that meet the interface designer's desired latency expectations and resource constraints. Cloud DBMSs, while optimized for scalability, often fail to meet latency expectations, necessitating complex, bespoke query execution and optimization techniques for data interfaces. This involves manually navigating a huge optimization space that is sensitive to interface design and resource constraints, such as client vs server data and compute placement, choosing which computations are done offline vs online, and selecting from a large library of visualization-optimized data structures. This paper advocates for a Physical Visualization Design (PVD) tool that decouples interface design from system design to provide design independence. Given an interfaces underlying data flow, interactions with latency expectations, and resource constraints, PVD checks if the interface is feasible and, if so, proposes and instantiates a middleware architecture spanning the client, server, and cloud DBMS that meets the expectations. To this end, this paper presents Jade, the first prototype PVD tool that enables design independence. Jade proposes an intermediate representation called Diffplans to represent the data flows, develops cost estimation models that trade off between latency guarantees and plan feasibility, and implements an optimization framework to search for the middleware architecture that meets the guarantees. We evaluate Jade on six representative data interfaces as compared to Mosaic and Azure SQL database. We find Jade supports a wider range of interfaces, makes better use of available resources, and can meet a wider range of data, latency, and resource conditions.}
}


@article{DBLP:journals/pacmmod/ZhuJBZMK25,
	author = {Yuxuan Zhu and
                  Tengjun Jin and
                  Stefanos Baziotis and
                  Chengsong Zhang and
                  Charith Mendis and
                  Daniel D. Kang},
	title = {PilotDB: Database-Agnostic Online Approximate Query Processing with
                  {A} Priori Error Guarantees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {198:1--198:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725335},
	doi = {10.1145/3725335},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhuJBZMK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {After decades of research in approximate query processing (AQP), its adoption in the industry remains limited. Existing methods struggle to simultaneously provide user-specified error guarantees, eliminate maintenance overheads, and avoid modifications to database management systems. To address these challenges, we introduce two novel techniques, TAQA and BSAP. TAQA is a two-stage online AQP algorithm that achieves all three properties for arbitrary queries. However, it can be slower than exact queries if we use standard row-level sampling. BSAP resolves this by enabling block-level sampling with statistical guarantees in TAQA. We implement TAQA and BSAP in a prototype middleware system, PilotDB, that is compatible with all DBMSs supporting efficient block-level sampling. We evaluate PilotDB on PostgreSQL, SQL Server, and DuckDB over real-world benchmarks, demonstrating up to 126X speedups when running with a 5% guaranteed error.}
}


@article{DBLP:journals/pacmmod/XuHZZSC25,
	author = {Xianghong Xu and
                  Xiao He and
                  Tieying Zhang and
                  Lei Zhang and
                  Rui Shi and
                  Jianjun Chen},
	title = {{PLM4NDV:} Minimizing Data Access for Number of Distinct Values Estimation
                  with Pre-trained Language Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {199:1--199:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725336},
	doi = {10.1145/3725336},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuHZZSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Number of Distinct Values (NDV) estimation of a multiset/column is a basis for many data management tasks, especially within databases. Despite decades of research, most existing methods require either a significant amount of samples through uniform random sampling or access to the entire column to produce estimates, leading to substantial data access costs and potentially ineffective estimations in scenarios with limited data access. In this paper, we propose leveraging semantic information, i.e., schema, to address these challenges. The schema contains rich semantic information that can benefit the NDV estimation. To this end, we propose  PLM4NDV,  a learned method incorporating Pre-trained Language Models (PLMs) to extract semantic schema information for NDV estimation. Specifically,  PLM4NDV  leverages the semantics of the target column and the corresponding table to gain a comprehensive understanding of the column's meaning. By using the semantics,  PLM4NDV  reduces data access costs, provides accurate NDV estimation, and can even operate effectively without any data access. Extensive experiments on a large-scale real-world dataset demonstrate the superiority of  PLM4NDV  over baseline methods. Our code is available at https://github.com/bytedance/plm4ndv.}
}


@article{DBLP:journals/pacmmod/BalakaAWGKF25,
	author = {Muhammad Imam Luthfi Balaka and
                  David Alexander and
                  Qiming Wang and
                  Yue Gong and
                  Adila Krisnadhi and
                  Raul Castro Fernandez},
	title = {Pneuma: Leveraging LLMs for Tabular Data Representation and Retrieval
                  in an End-to-End System},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {200:1--200:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725337},
	doi = {10.1145/3725337},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BalakaAWGKF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding relevant tables among databases, lakes, and repositories is the first step in extracting value from data. Such a task remains difficult because assessing whether a table is relevant to a problem does not always depend only on its content but also on the context, which is usually tribal knowledge known to the individual or team. While tools like data catalogs and academic data discovery systems target this problem, they rely on keyword search or more complex interfaces, limiting non-technical users' ability to find relevant data. The advent of large language models (LLMs) offers a unique opportunity for users to ask questions directly in natural language, making dataset discovery more intuitive, accessible, and efficient. In this paper, we introduce  Pneuma , a retrieval-augmented generation (RAG) system designed to efficiently and effectively discover tabular data.  Pneuma  leverages large language models (LLMs) for both table representation and table retrieval. For table representation,  Pneuma  preserves schema and row-level information to ensure comprehensive data understanding. For table retrieval,  Pneuma  augments LLMs with traditional information retrieval techniques, such as full-text and vector search, harnessing the strengths of both to improve retrieval performance. To evaluate  Pneuma , we generate comprehensive benchmarks that simulate table discovery workload on six real-world datasets including enterprise data, scientific databases, warehousing data, and open data. Our results demonstrate that  Pneuma  outperforms widely used table search systems (such as full-text search and state-of-the-art RAG systems) in accuracy and resource efficiency.}
}


@article{DBLP:journals/pacmmod/ZhangJCFMNCC25,
	author = {Hailin Zhang and
                  Xiaodong Ji and
                  Yilin Chen and
                  Fangcheng Fu and
                  Xupeng Miao and
                  Xiaonan Nie and
                  Weipeng Chen and
                  Bin Cui},
	title = {PQCache: Product Quantization-based KVCache for Long Context {LLM}
                  Inference},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {201:1--201:30},
	year = {2025},
	url = {https://doi.org/10.1145/3725338},
	doi = {10.1145/3725338},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangJCFMNCC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the field of Large Language Models (LLMs) continues to evolve, the context length in inference is steadily growing. Key-Value Cache (KVCache), the intermediate representations of tokens within LLM inference, has now become the primary memory bottleneck due to limited GPU memory. Current methods selectively determine suitable keys and values for self-attention computation in LLMs to address the issue. However, they either fall short in maintaining model quality or result in high serving latency. Drawing inspiration from advanced embedding retrieval techniques prevalent in the data management community, we consider the storage and retrieval of KVCache as a typical embedding retrieval problem. We propose  PQCache , which employs Product Quantization (PQ) to manage KVCache, maintaining model quality while ensuring low serving latency. During the prefilling phase, we apply PQ to tokens' keys for each LLM layer and head. During the autoregressive decoding phase, we use PQ codes and centroids to approximately identify important preceding tokens, then fetch the corresponding key-value pairs for self-attention computation. Through meticulous design of overlapping and caching, we minimize any additional computation and communication overhead during both phases. Extensive experiments demonstrate that PQCache achieves both effectiveness and efficiency, with 4.60% score improvement over existing methods on InfiniteBench and low system latency in both prefilling and decoding.}
}


@article{DBLP:journals/pacmmod/GaoGXYLW25,
	author = {Jianyang Gao and
                  Yutong Gou and
                  Yuexuan Xu and
                  Yongyi Yang and
                  Cheng Long and
                  Raymond Chi{-}Wing Wong},
	title = {Practical and Asymptotically Optimal Quantization of High-Dimensional
                  Vectors in Euclidean Space for Approximate Nearest Neighbor Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {202:1--202:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725413},
	doi = {10.1145/3725413},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GaoGXYLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate nearest neighbor (ANN) query  in high-dimensional Euclidean space is a key operator in database systems. For this query, quantization is a popular family of methods developed for compressing vectors and reducing memory consumption. Among these methods, a recent algorithm called RaBitQ achieves the state-of-the-art performance and provides an asymptotically optimal theoretical error bound. RaBitQ uses 1 bit per dimension for quantization and compresses vectors with a large compression rate. In this paper, we extend RaBitQ to compress vectors with flexible compression rates - it achieves this by using B bits per dimension for quantization with B = 1, 2, ... It inherits the theoretical guarantees of RaBitQ and achieves the asymptotic optimality in terms of the trade-off between space and error bounds as to be proven in this study. Additionally, we present efficient implementations of the extended RaBitQ, enabling its application to ANN queries to reduce both space and time consumption. Extensive experiments on real-world datasets confirm that our method consistently outperforms the state-of-the-art baselines in both accuracy and efficiency when using the same amount of memory.}
}


@article{DBLP:journals/pacmmod/GuanYZXCXXZ25,
	author = {Hong Guan and
                  Lei Yu and
                  Lixi Zhou and
                  Li Xiong and
                  Kanchan Chowdhury and
                  Lulu Xie and
                  Xusheng Xiao and
                  Jia Zou},
	title = {Privacy and Accuracy-Aware {AI/ML} Model Deduplication},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {203:1--203:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725340},
	doi = {10.1145/3725340},
	timestamp = {Fri, 18 Jul 2025 07:29:14 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GuanYZXCXXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. Managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problem of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problem. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines, our approach improved the compression ratio by up to 35× for individual models (including large language models and vision transformers). We also observed up to 43× inference speedup due to the reduction of I/O operations.}
}


@article{DBLP:journals/pacmmod/CaiXY25,
	author = {Kuntai Cai and
                  Xiaokui Xiao and
                  Yin Yang},
	title = {PrivPetal: Relational Data Synthesis via Permutation Relations},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {204:1--204:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725341},
	doi = {10.1145/3725341},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/CaiXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Releasing relational databases while preserving privacy is an important research problem with numerous applications. A canonical approach is to generate synthetic data under differential privacy (DP), which provides a strong, rigorous privacy guarantee. The problem is particularly challenging when the data involve not only entities (e.g., represented by records in tables) but also relationships (represented by foreign-key references), since if we generate random records for each entity independently, the resulting synthetic data usually fail to exhibit realistic relationships. The current state of the art, PrivLava, addresses this issue by generating random join key attributes through a sophisticated expectation-maximization (EM) algorithm. This method, however, is rather costly in terms of privacy budget consumption, due to the numerous EM iterations needed to retain high data utility. Consequently, the privacy cost of PrivLava can be prohibitive for some real-world scenarios. We observe that the utility of the synthetic data is inherently sensitive to the join keys: changing the primary key of a record t, for example, causes t to join with a completely different set of partner records, which may lead to a significant distribution shift of the join result. Consequently, join keys need to be kept highly accurate, meaning that enforcing DP on them inevitably incurs a high privacy cost. In this paper, we explore a different direction: synthesizing a  flattened  relation and subsequently decomposing it down to base relations, which eliminates the need to generate join keys. Realizing this idea is challenging, since naively flattening a relational schema leads to a rather high-dimensional table, which is hard to synthesize accurately with differential privacy. We present a sophisticated PrivPetal approach that addresses the above issues via a novel concept:  permutation relation , which is constructed as a surrogate to synthesize the flattened relation, avoiding the generation of a high-dimensional relation directly. The synthesis is done using a refined Markov random field mechanism, backed by fine-grained privacy analysis. Extensive experiments using multiple real datasets and the TPC-H benchmark demonstrate that PrivPetal significantly outperforms existing methods in terms of aggregate query accuracy on the synthetic data.}
}


@article{DBLP:journals/pacmmod/YuYD25,
	author = {Liantong Yu and
                  Qingqing Ye and
                  Rong Du},
	title = {\emph{PrivRM}: {A} Framework for Range Mean Estimation under Local
                  Differential Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {205:1--205:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725414},
	doi = {10.1145/3725414},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuYD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing collection and analysis of personal data driven by digital technologies has raised concerns about individual privacy. Local Differential Privacy (LDP) has emerged as a promising solution to provide rigorous privacy guarantee for users, without relying on a trusted data collector. In the context of LDP, range mean estimation over numerical values is an important yet challenging problem. Simply applying existing work may introduce overly large noise sensitivity, since all of them focus on statistical tasks (e.g., mean or distribution) across the entire domain. In this paper, we propose a novel framework for <u>Priv</u>ate <u>R</u>ange <u>M</u>ean ( PrivRM ) estimation under LDP. Two implementations of the framework, namely  PrivRM I  and  PrivRM * , are developed, which are adaptable to all existing numerical value perturbation mechanisms. As an optimization of the framework, we also propose a distribution-aware Adaptive Adjustment (AA) strategy to dynamically confine the perturbation space for skewed data distributions. Extensive experimental results show that under the same privacy guarantee and query range, our framework  PrivRM  significantly improve over existing solutions.}
}


@article{DBLP:journals/pacmmod/BourosM25,
	author = {Panagiotis Bouros and
                  Nikos Mamoulis},
	title = {Relevance Queries for Interval Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {206:1--206:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725343},
	doi = {10.1145/3725343},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BourosM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A wide range of applications manage large collections of interval data. For instance, temporal databases manage validity intervals of objects or versions thereof, while in probabilistic databases attribute values of records are associated with confidence or uncertainty intervals. The main search operation on interval data is the retrieval of data intervals that intersect (i.e., overlap with) a query interval (e.g., find records which were valid in September 2020, find temperature readings with non-zero probability to be within [24, 26] degrees). As query results could be many, we need mechanisms that filter or order them based on how relevant they are to the query interval. We define alternative relevance scores between a data and a query interval based on their (relative) overlap. We define relevance queries, which compute only a subset of the most relevant intervals that intersect a query. Then, we propose a framework for evaluating relevance queries that can be applied on popular domain-partitioning interval indices (interval tree and HINT). We present experiments on real datasets that demonstrate the efficiency of our framework over baseline approaches.}
}


@article{DBLP:journals/pacmmod/WangQYZ25,
	author = {Hengrui Wang and
                  Jiansheng Qiu and
                  Fangzhou Yuan and
                  Huanchen Zhang},
	title = {Rethinking The Compaction Policies in LSM-trees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {207:1--207:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725344},
	doi = {10.1145/3725344},
	timestamp = {Thu, 17 Jul 2025 11:27:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangQYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log-structured merge-trees (LSM-trees) are widely used to construct key-value stores. They periodically compact overlapping sorted runs to reduce the read amplification. Prior research on compaction policies has focused on the trade-off between write amplification (WA) and read amplification (RA). In this paper, we propose to treat the compaction operation in LSM-trees as a computational and I/O-bandwidth investment for improving the system's future query throughput, and thus rethink the compaction policy designs. A typical LSM-tree application handles a steady but moderate write stream and prioritizes resources for top-level flushes of small sorted runs to avoid data loss due to write stalls. The goal of the compaction policy, therefore, is to maintain an optimal number of sorted runs to maximize average query throughput. Because compaction and read operations compete for the CPU and I/O resources from the same pool, we must perform a joint optimization to determine the appropriate timing and aggressiveness of the compaction. We introduce a three-level model of an LSM-tree and propose EcoTune, an algorithm based on dynamic programming to find the optimal compaction policy according to workload characterizations. Our evaluation on RocksDB shows that EcoTune improves the average query throughput by 1.5x to 3x over the leveling policy and by up to 2.5x over the lazy-leveling policy on workloads with range/point query ratios.}
}


@article{DBLP:journals/pacmmod/MengSYLCLYZLZ25,
	author = {Lingkai Meng and
                  Yu Shao and
                  Long Yuan and
                  Longbin Lai and
                  Peng Cheng and
                  Xue Li and
                  Wenyuan Yu and
                  Wenjie Zhang and
                  Xuemin Lin and
                  Jingren Zhou},
	title = {Revisiting Graph Analytics Benchmark},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {208:1--208:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725345},
	doi = {10.1145/3725345},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MengSYLCLYZLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of graph analytics platforms has led to the development of various benchmarks for evaluating and comparing platform performance. However, existing benchmarks often fall short of fully assessing performance due to limitations in core algorithm selection, data generation processes (and the corresponding synthetic datasets), as well as the neglect of API usability evaluation. To address these shortcomings, we propose a novel graph analytics benchmark. First, we select eight core algorithms by extensively reviewing both academic and industrial settings. Second, we design an efficient and flexible data generator and produce eight new synthetic datasets as the default datasets for our benchmark. Lastly, we introduce a multi-level large language model (LLM)-based framework for API usability evaluation-the first of its kind in graph analytics benchmarks. We conduct comprehensive experimental evaluations on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra, and G-thinker). The experimental results demonstrate the superiority of our proposed benchmark.}
}


@article{DBLP:journals/pacmmod/ChenYJZWZW25,
	author = {Minxiao Chen and
                  Haitao Yuan and
                  Nan Jiang and
                  Zhihan Zheng and
                  Sai Wu and
                  Ao Zhou and
                  Shangguang Wang},
	title = {{RLOMM:} An Efficient and Robust Online Map Matching Framework with
                  Reinforcement Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {209:1--209:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725346},
	doi = {10.1145/3725346},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenYJZWZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online map matching is a fundamental problem in location-based services, aiming to incrementally match trajectory data step-by-step onto a road network. However, existing methods fail to meet the needs for efficiency, robustness, and accuracy required by large-scale online applications, making this task still challenging. This paper introduces a novel framework that achieves high accuracy and efficient matching while ensuring robustness in handling diverse scenarios. To improve efficiency, we begin by modeling the online map matching problem as an Online Markov Decision Process (OMDP) based on its inherent characteristics. This approach helps efficiently merge historical and real-time data, reducing unnecessary calculations. Next, to enhance robustness, we design a reinforcement learning method, enabling robust handling of real-time data from dynamically changing environments. In particular, we propose a novel model learning process and a comprehensive reward function, allowing the model to make reasonable current matches from a future-oriented perspective, and to continuously update and optimize during the decision-making process based on feedback. Lastly, to address the heterogeneity between trajectories and roads, we design distinct graph structures, facilitating efficient representation learning through graph and recurrent neural networks. To further align trajectory and road data, we introduce contrastive learning to decrease their distance in the latent space, thereby promoting effective integration of the two. Extensive evaluations on three real-world datasets confirm that our method significantly outperforms existing state-of-the-art solutions in terms of accuracy, efficiency and robustness.}
}


@article{DBLP:journals/pacmmod/LuoYDXYY25,
	author = {Qiyao Luo and
                  Jianzhe Yu and
                  Wei Dong and
                  Quanqing Xu and
                  Chuanhui Yang and
                  Ke Yi},
	title = {RM\({}^{\mbox{2}}\): Answer Counting Queries Efficiently under Shuffle
                  Differential Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {210:1--210:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725415},
	doi = {10.1145/3725415},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuoYDXYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Differential privacy (DP) is a leading standard for protecting individual privacy in data collection and analysis. This paper explores the shuffle model of DP, which balances privacy and utility by allowing users to send messages to a trusted shuffler before reaching an untrusted analyzer anonymously. We focus on efficiently implementing the matrix mechanism in shuffle-DP, where efficiency is defined by the number of messages each user sends. Our contributions include a baseline shuffle-DP mechanism that naively adapts the matrix mechanism, followed by an improved mechanism that reduces message complexity while maintaining error levels comparable to central-DP. We demonstrate the versatility of our approach across common query workloads, such as range queries and data cubes, achieving significant improvements in message efficiency. Experimental results confirm that our method outperforms the baseline solution while closely matching the accuracy of central-DP mechanisms.}
}


@article{DBLP:journals/pacmmod/HeWZLZN25,
	author = {Yizhang He and
                  Kai Wang and
                  Wenjie Zhang and
                  Xuemin Lin and
                  Ying Zhang and
                  Wei Ni},
	title = {Robust Privacy-Preserving Triangle Counting under Edge Local Differential
                  Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {211:1--211:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725348},
	doi = {10.1145/3725348},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HeWZLZN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Counting the number of triangles in a graph is a fundamental task and has been extensively studied recently. In real-world applications, continuously releasing the triangle count of a graph poses a significant privacy risk for users. To protect sensitive edge information from a central server, we study the problem of estimating the number of triangles under edge local differential privacy (edge LDP). Existing approaches adopt a multi-round computing scheme, allowing the vertices to perform local triangle counting using the noisy graph constructed in the previous round. However, these algorithms not only restrict the noisy graph that can be downloaded to each vertex, but also have coarse upper bounds for the scale of noise added to the estimates. In this paper, we propose a vertex-centric triangle counting algorithm under edge LDP, which improves data utility by leveraging a larger part of the noisy adjacency matrix. Our approach fully exploits the local graph structure to obtain refined estimates of per-vertex triangle counts. We also devise tight bounds for global sensitivities to not only comply with privacy requirements but also control the scale of added noise. Furthermore, we perform a rigorous analysis of the L2 loss of our unbiased estimators and design optimizations for allocating the privacy budget to minimize L2 loss based on the input graph. Extensive experiments on 12 datasets validate the effectiveness and efficiency of our proposed algorithms.}
}


@article{DBLP:journals/pacmmod/AomarEAAHC25,
	author = {Anas Ait Aomar and
                  Karima Echihabi and
                  Marco Arnaboldi and
                  Ioannis Alagiannis and
                  Damien Hilloulin and
                  Manal Cherkaoui},
	title = {RWalks: Random Walks as Attribute Diffusers for Filtered Vector Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {212:1--212:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725349},
	doi = {10.1145/3725349},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AomarEAAHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analytical tasks in various domains increasingly encode complex information as dense vector data (e.g., embeddings), often requiring filtered vector search (i.e., vector search with attribute filtering). This search is challenging due to the volume and dimensionality of the data, the number and variety of filters, and the difference in distribution and/or update frequency between vectors and filters. Besides, many real applications require answers in a few milliseconds with high recall on large collections. Graph-based methods are considered the best choice for such applications, despite a lack of theoretical guarantees on query accuracy. Existing solutions for filtered vector search are either: 1) ad-hoc, using existing techniques with no or minor modifications; or 2) hybrid, providing specialized indexing and/or search algorithms. We show that neither is satisfactory and propose RWalks, an index-agnostic graph-based filtered vector search method that efficiently supports both filtered and unfiltered vector search. We demonstrate its scalability and robustness against the state-of-the-art with an exhaustive experimental evaluation on four real datasets (up to 100 million vectors), using query workloads with filters of different types (unique/composite), and varied specificity (proportion of points that satisfy a filter). The results show that RWalks can perform filtered search up to 2x faster than the second-best competitor (ACORN), while building the index 76x faster and answering unfiltered search 13x faster.}
}


@article{DBLP:journals/pacmmod/KhanMAM25,
	author = {Abdul Atif Khan and
                  Rashmi Maheshwari and
                  Mohammad Maksood Akhter and
                  Sraban Kumar Mohanty},
	title = {{SBSC:} {A} fast Self-tuned Bipartite proximity graph-based Spectral
                  Clustering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {213:1--213:22},
	year = {2025},
	url = {https://doi.org/10.1145/3725418},
	doi = {10.1145/3725418},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KhanMAM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectral clustering (SC) is well-known for discovering natural groups present in the data by projecting them into Eigen-space based on the proximity graph but incurs cubic time in terms of size (N) of the data as all pair proximity is used. To enhance the efficiency of the SC techniques, the proximity between the data instances and their representatives ( R ) is captured through a bipartite similarity graph. However, extrinsic parameters such as the number of representatives and nearby representatives of data instances, influence the clustering performance, time, and memory usage. Therefore, in this work, we construct a parameter-free bipartite graph to further improve the clustering quality and computational cost of SC by introducing a locality-based sparsification technique. First, the proposed method (SBSC) determines O(√N) numbers of well-distributed representatives in O(N lg N) time by applying Bi-means and  K -means partitioning techniques. Next, SBSC utilizes the local neighbors of  R  to search the nearby representatives, which fastens the search time to O(N). To the best of our knowledge, the proposed bipartite graph is the least (O(N)) sized and therefore, by exploiting the high sparsity of the graph accelerates the Eigen-decomposition step of SBSC. The proposed algorithm takes overall O(N(K 2 +lg N)) time only to detect  K  clusters, and experimental results on eighteen large-sized diversified datasets suggest that SBSC discovers complex clusters much faster than the competing methods with enhanced clustering quality.}
}


@article{DBLP:journals/pacmmod/HanCSLL25,
	author = {Chenxia Han and
                  Chaokun Chang and
                  Srijan Srivastava and
                  Yao Lu and
                  Eric Lo},
	title = {Scalable Complex Event Processing on Video Streams},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {214:1--214:29},
	year = {2025},
	url = {https://doi.org/10.1145/3725419},
	doi = {10.1145/3725419},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HanCSLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid expansion of video streaming content in our daily lives has rendered the real-time processing and analysis of these video streams a critical capability. However, existing deep video analytics systems primarily support only simple queries, such as selection and aggregation. Considering the inherent temporal nature of video streams, queries capable of matching patterns of events could enable a wider range of applications. In this paper, we present Bobsled, a novel video stream processing system designed to efficiently support complex event queries. Experimental results demonstrate that Bobsled can achieve a throughput improvement over state-of-the-art ranging from 2.4× to 11.6×, without any noticeable loss in accuracy.}
}


@article{DBLP:journals/pacmmod/ZhangSHKB25,
	author = {Enhao Zhang and
                  Nicole Sullivan and
                  Brandon Haynes and
                  Ranjay Krishna and
                  Magdalena Balazinska},
	title = {Self-Enhancing Video Data Management System for Compositional Events
                  with Large Language Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {215:1--215:29},
	year = {2025},
	url = {https://doi.org/10.1145/3725352},
	doi = {10.1145/3725352},
	timestamp = {Fri, 18 Jul 2025 07:41:37 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangSHKB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Complex video queries can be answered by decomposing them into modular subtasks. However, existing video data management systems assume the existence of predefined modules for each subtask. We introduce VOCAL-UDF, a novel self-enhancing system that supports compositional queries over videos without the need for predefined modules. VOCAL-UDF automatically identifies and constructs missing modules and encapsulates them as user-defined functions (UDFs), thus expanding its querying capabilities. To achieve this, we formulate a unified UDF model that leverages large language models (LLMs) to aid in new UDF generation. VOCAL UDF handles a wide range of concepts by supporting both program-based UDFs (i.e., Python functions generated by LLMs) and distilled-model UDFs (lightweight vision models distilled from strong pretrained models). To resolve the inherent ambiguity in user intent, VOCAL-UDF generates multiple candidate UDFs and uses active learning to efficiently select the best one. With the self-enhancing capability, VOCAL-UDF significantly improves query performance across three video datasets.}
}


@article{DBLP:journals/pacmmod/LiCLXYCBZ25,
	author = {Ruiyuan Li and
                  Zechao Chen and
                  Ruyun Lu and
                  Xiaolong Xu and
                  Guangchao Yang and
                  Chao Chen and
                  Jie Bao and
                  Yu Zheng},
	title = {\emph{Serf}: Streaming Error-Bounded Floating-Point Compression},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {216:1--216:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725353},
	doi = {10.1145/3725353},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiCLXYCBZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In IoT (Internet of Things) scenarios, massive floating-point time series data are generated in a streaming manner and transmitted within limited bandwidth for real-time analysis. To enhance the efficiency, it is acknowledged to compress the data before transmission. Existing floating-point compression methods are either for batched compression that may cause long delays, or for streaming lossless compression that has an unsatisfactory compression ratio when certain errors are allowed. In this paper, we propose the first <u>S</u>treaming <u>ER</u>ror-bounded <u>F</u>loating-point compression  Serf , which has two implementations:  Serf-Qt  and  Serf-XOR .  Serf-Qt  first quantizes each floating-point value into an integer, and then encodes the integer with Elias gamma coding.  Serf-XOR  is the first lossy floating-point compression based on the XORing operation. To enhance the compression ratio of  Serf-XOR , we propose a novel data offset technique to increase the leading zeros of the XORed values, and design a novel approximation technique to search for an error-qualified value that produces an XORed value with many trailing zeros. To improve the compression efficiency, we propose a pruning strategy to accelerate the process of approximated values search. We further build a streaming transmission prototype system based on a real development board, and deploy the proposed methods to it. Extensive experiments using 13 datasets show that, compared with 17 competitors, both  Serf-Qt  and  Serf-XOR  enjoy remarkable compression ratios with high efficiency in streaming scenarios. The transmission experiments based on the proposed system also showcase that  Serf-XOR  always takes the least overall time when the bandwidth is limited.}
}


@article{DBLP:journals/pacmmod/ThakkarKLKC25,
	author = {Viraj Thakkar and
                  Dongha Kim and
                  Yingchun Lai and
                  Hokeun Kim and
                  Zhichao Cao},
	title = {{SHIELD:} Encrypting Persistent Data of {LSM-KVS} from Monolithic
                  to Disaggregated Storage},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {217:1--217:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725354},
	doi = {10.1145/3725354},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ThakkarKLKC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log-Structured Merge-tree-based Key-Value Stores (LSM-KVS) are widely used to support modern, high-performance, data-intensive applications. In recent years, with the trend of deploying and optimizing LSM-KVS from monolith to Disaggregated Storage (DS) setups, the confidentiality of LSM-KVS persistent data (e.g., WAL and SST files) is vulnerable to unauthorized access from insiders and external attackers and must be protected using encryption. Existing solutions lack a high-performance design for encryption in LSM-KVS, often focus on in-memory data protection with overheads of 3.4-32.5x, and lack the scalability and flexibility considerations required in DS deployments. This paper proposes two novel designs to address the challenges of providing robust security for persistent components of LSM-KVS while maintaining high performance in both monolith and DS deployments - a simple and effective instance-level design suitable for monolithic LSM-KVS deployments, and  SHIELD,  a design that embeds encryption into LSM-KVS components for minimal overhead in both monolithic and DS deployment. We achieve our objective through three contributions: (1) A fine-grained integration of encryption into LSM-KVS write path to minimize performance overhead from exposure-limiting practices like using unique encryption keys per file and regularly re-encrypting using new encryption keys during compaction, (2) Mitigating performance degradation caused by recurring encryption of Write-Ahead Log (WAL) writes by using a buffering solution and (3) Extending confidentiality guarantees to DS by designing a metadata-enabled encryption-key-sharing mechanism and a secure local cache for high scalability and flexibility. We implement both designs on RocksDB, evaluating them in monolithic and DS setups while showcasing an overhead of 0-32% for the instance-level design and 0-36% for SHIELD.}
}


@article{DBLP:journals/pacmmod/AytimurCG25,
	author = {Mehmet Aytimur and
                  Theodoros Chondrogiannis and
                  Michael Grossniklaus},
	title = {{SPACE:} Cardinality Estimation for Path Queries Using Cardinality-Aware
                  Sequence-based Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {218:1--218:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725355},
	doi = {10.1145/3725355},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AytimurCG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality estimation is a central task of cost-based database query optimization. Accurate estimates enable optimizers to identify and avoid expensive plans requiring large intermediate results. While cardinality estimation has been studied extensively in relational databases, research in the setting of graph databases has been more scarce. Furthermore, recent studies have shown that machine-learning-based methods can be utilized for cardinality estimation in both relational and graph databases. In this paper, we focus on the problem of estimating the cardinality of path patterns in graph databases, and we propose the  Sequence-based Path Pattern Cardinality Estimator  (SPACE). Our approach treats path patterns as sequences of node labels and edge types and assign similar cardinalities to path patterns with similar node and edge order. SPACE uses a dual approach: it encodes the sequence of nodes and edges to capture structural characteristics of the path pattern, while also incorporating a cardinality-based encoding to integrate cardinality information throughout learning. In a comprehensive experimental evaluation, we show that our method outperforms the state of the art in terms of both accuracy ( Q -error) and training time.}
}


@article{DBLP:journals/pacmmod/JoT25,
	author = {Saehan Jo and
                  Immanuel Trummer},
	title = {SpareLLM: Automatically Selecting Task-Specific Minimum-Cost Large
                  Language Models under Equivalence Constraint},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {219:1--219:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725356},
	doi = {10.1145/3725356},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/JoT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce SpareLLM, Selecting  P assable  A nd  R esource- E fficient  LLM s, a novel LLM framework designed to minimize the inference costs (i.e., resource-efficient) of large-scale NLP tasks while ensuring sufficient result quality (i.e., passable). It enables users to specify an equivalence constraint in terms of the equivalence of outputs to those of the most powerful LLM. SpareLLM then generates results that deviate from the outputs of this LLM only with a probability below a user-defined threshold. SpareLLM employs a profiling phase that evaluates the performance of multiple LLMs to identify those that meet the user-defined equivalence level. It optimizes the tradeoff between profiling overheads and the anticipated cost savings resulting from profiling. Moreover, SpareLLM further reduces inference costs by strategically leveraging a mix of LLMs. Our experiments on five real-world datasets show that SpareLLM achieves significant cost savings, up to 8.6x, while generating equivalent outputs in 90% of cases compared to GPT-4-Turbo. Compared to recent LLM cascading baselines, SpareLLM demonstrates a superior tradeoff between cost and accuracy, accounting for 91.1% and 83.8% of the points on the Pareto curve for OpenAI and Llama models.}
}


@article{DBLP:journals/pacmmod/YangP25,
	author = {Fan Yang and
                  John Paparrizos},
	title = {{SPARTAN:} Data-Adaptive Symbolic Time-Series Approximation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {220:1--220:30},
	year = {2025},
	url = {https://doi.org/10.1145/3725357},
	doi = {10.1145/3725357},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Symbolic approximations  are dimensionality reduction techniques that convert time series into sequences of discrete symbols, enhancing interpretability while reducing computational and storage costs. To construct symbolic representations, first numeric representations approximate and capture properties of raw time series, followed by a discretization step that converts these numeric dimensions into symbols. Despite decades of development, existing approaches have several key limitations that often result in unsatisfactory performance: they (i) rely on data-agnostic numeric approximations, disregarding intrinsic properties of the time series; (ii) decompose dimensions into equal-sized subspaces, assuming independence among dimensions; and (iii) allocate a uniform encoding budget for discretizing each dimension or subspace, assuming balanced importance. To address these shortcomings, we propose SPARTAN, a novel data-adaptive symbolic approximation method that intelligently allocates the encoding budget according to the importance of the constructed uncorrelated dimensions. Specifically, SPARTAN (i) leverages intrinsic dimensionality reduction properties to derive non-overlapping, uncorrelated latent dimensions; (ii) adaptively distributes the budget based on the importance of each dimension by solving a constrained optimization problem; and (iii) prevents false dismissals in similarity search by ensuring a lower bound on the true distance in the original space. To demonstrate SPARTAN's robustness, we conduct the most comprehensive study to date, comparing SPARTAN with seven state-of-the-art symbolic methods across four tasks: classification, clustering, indexing, and anomaly detection. Rigorous statistical analysis across hundreds of datasets shows that SPARTAN outperforms competing methods significantly on  all  tasks in terms of downstream accuracy, given the same budget. Notably, SPARTAN achieves up to a 2x speedup compared to the most accurate rival. Overall, SPARTAN effectively improves the symbolic representation quality without storage or runtime overheads, paving the way for future advancements.}
}


@article{DBLP:journals/pacmmod/Bach25,
	author = {Jakob Bach},
	title = {Subgroup Discovery with Small and Alternative Feature Sets},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {221:1--221:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725358},
	doi = {10.1145/3725358},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Bach25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgroup-discovery methods find interesting regions in a dataset. In this article, we analyze two constraint types to enhance the interpretability of subgroups: First, we make subgroup descriptions small by limiting the number of features used. Second, we propose the novel problem of finding alternative subgroup descriptions, which cover a similar set of data objects as a given subgroup but use different features. We describe how to integrate both constraint types into heuristic subgroup-discovery methods as well as a novel Satisfiability Modulo Theories (SMT) formulation, which enables a solver-based search for subgroups. Further, we prove  NP -hardness of optimization with either constraint type. Finally, we evaluate unconstrained and constrained subgroup discovery with 27 binary-classification datasets. We observe that heuristic search methods often yield high-quality subgroups fast, even with constraints.}
}


@article{DBLP:journals/pacmmod/PurtzelW25,
	author = {Steven Purtzel and
                  Matthias Weidlich},
	title = {SuSe: Summary Selection for Regular Expression Subsequence Aggregation
                  over Streams},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {222:1--222:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725359},
	doi = {10.1145/3725359},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PurtzelW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Regular expressions (RegEx) are an essential tool for pattern matching over streaming data, e.g., in network and security applications. The evaluation of RegEx queries becomes challenging, though, once subsequences are incorporated, i.e., characters in a sequence may be skipped during matching. Since the number of subsequence matches may grow exponentially in the input length, existing RegEx engines fall short in finding all subsequence matches, especially for queries including Kleene closure. In this paper, we argue that common applications for RegEx queries over streams do not require the enumeration of all distinct matches at any point in time. Rather, only an aggregate over the matches is typically fetched at specific, yet unknown time points. To cater for these scenarios, we present SuSe, a novel architecture for RegEx evaluation that is based on a query-specific summary of the stream. It employs a novel data structure, coined StateSummary, to capture aggregated information about subsequence matches. This structure is maintained by a summary selector, which aims at choosing the stream projections that minimize the loss in the aggregation result over time. Experiments on real-world and synthetic data demonstrate that SuSe is both effective and efficient, with the aggregates being based on several orders of magnitude more matches compared to baseline techniques.}
}


@article{DBLP:journals/pacmmod/SongGLLSZJY25,
	author = {Zhen Song and
                  Yu Gu and
                  Tianyi Li and
                  Yushuai Li and
                  Qing Sun and
                  Yanfeng Zhang and
                  Christian S. Jensen and
                  Ge Yu},
	title = {{SWASH:} {A} Flexible Communication Framework with Sliding Window-Based
                  Cache Sharing for Scalable {DGNN} Training},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {223:1--223:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725360},
	doi = {10.1145/3725360},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SongGLLSZJY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic Graph Neural Networks (DGNNs) are effective at capturing multidimensional data and enable many important applications. As model training is computationally intensive, distributed DGNN training is employed to accommodate large data. Also, when training DGNNs, so-called sliding window training is used predominantly, as it enhances both accuracy and efficiency. However, current distributed frameworks-such as snapshot partitioning, chunk-based partitioning, and  L -hop cache-based communication-free vertex partitioning-are inherently incompatible with sliding window training. While communication-based vertex partitioning supports sliding window training, its design for static graphs limits the effectiveness in distributed DGNN training. Specifically, existing partitioning strategies fail to optimize communication across snapshots, while existing cache reuse and communication scheduling strategies ignore opportunities for optimization between sliding windows. To support distributed sliding window training, we present  SWASH,  a scalable and flexible communication framework that utilizes a  <u>S</u> liding  <u>W</u> indow-based c <u>A</u> che  <u>SH</u> aring technique. Specifically, we propose a flexible communication framework that supports ratio adjustment and timing selection, as well as hyperparameter settings and adaptive scheduling. We also propose a lightweight partitioning strategy tailored to sliding window-based DGNN training to reduce both partitioning and communication overheads. Finally, to alleviate decreases in accuracy due to reduced communication, we propose a cache-sharing technique based on sliding windows for sharing boundary vertex embeddings. Comprehensive experiments show that  SWASH  is capable of training speedups of an average of 9.44× over state-of-the-art frameworks while maintaining the accuracy of fully communicating, non-caching training frameworks.}
}


@article{DBLP:journals/pacmmod/JiangKPA25,
	author = {Wenqi Jiang and
                  Oleh{-}Yevhen Khavrona and
                  Martin Parvanov and
                  Gustavo Alonso},
	title = {SwiftSpatial: Spatial Joins on Modern Hardware},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {224:1--224:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725361},
	doi = {10.1145/3725361},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/JiangKPA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial joins are among the most time-consuming spatial queries, remaining costly even in parallel and distributed systems. In this paper, we explore hardware acceleration for spatial joins by proposing SwiftSpatial, an FPGA-based accelerator that can be deployed in data centers and at the edge. SwiftSpatial contains multiple high-performance join units with innovative hybrid parallelism, several efficient memory management units, and an extensible on-chip join scheduler that supports the popular R-tree synchronous traversal and partition-based spatial-merge (PBSM) algorithms. Benchmarked against various CPU and GPU-based spatial data processing systems, SwiftSpatial demonstrates a latency reduction of up to 41.03x relative to the best-performing baseline, while requiring 6.16x less power. The performance and energy efficiency of SwiftSpatial demonstrate its potential to be used in a variety of configurations (e.g., as an accelerator, near storage, in-network) as well as on different devices (e.g., data centers where FPGAs are widely available or mobile devices, which also contain FPGAs for specialized processing).}
}


@article{DBLP:journals/pacmmod/ZhangL25,
	author = {Zhuoxing Zhang and
                  Sebastian Link},
	title = {Synthesizing Third Normal Form Schemata that Minimize Integrity Maintenance
                  and Update Overheads: Parameterizing 3NF by the Numbers of Minimal
                  Keys and Functional Dependencies},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {225:1--225:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725362},
	doi = {10.1145/3725362},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {State-of-the-art relational schema design generates a lossless, dependency-preserving decomposition into Third Normal Form (3NF), that is in Boyce-Codd Normal Form (BCNF) whenever possible. In particular, dependency-preservation ensures that data integrity can be maintained on individual relation schemata without having to join them, but may need to tolerate a priori unbounded levels of data redundancy and integrity faults. As our main contribution we parameterize 3NF schemata by the numbers of minimal keys and functional dependencies they exhibit. Conceptually, these parameters quantify, already at schema design time, the effort necessary to maintain data integrity, and allow us to break ties between 3NF schemata. Computationally, the parameters enable us to optimize normalization into 3NF according to different strategies. Operationally, we show through experiments that our optimizations translate from the logical level into significantly smaller update overheads during integrity maintenance. Hence, our framework provides access to parameters that guide the computation of logical schema designs which reduce operational overheads.}
}


@article{DBLP:journals/pacmmod/PsarakisCSFK25,
	author = {Kyriakos Psarakis and
                  George Christodoulou and
                  Georgios Siachamis and
                  Marios Fragkoulis and
                  Asterios Katsifodimos},
	title = {Styx: Transactional Stateful Functions on Streaming Dataflows},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {226:1--226:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725363},
	doi = {10.1145/3725363},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PsarakisCSFK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Developing stateful cloud applications, such as low-latency workflows and microservices with strict consistency requirements, remains arduous for programmers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve these use cases. However, existing approaches provide weak transactional guarantees or perform expensive external state accesses requiring inefficient transactional protocols that increase execution latency. In this paper, we present Styx, a novel dataflow-based SFaaS runtime that executes serializable transactions consisting of stateful functions that form arbitrary call-graphs with exactly-once guarantees. Styx extends a deterministic transactional protocol by contributing: i) a function acknowledgment scheme to determine transaction boundaries required in SFaaS workloads, ii) a function-execution caching mechanism, and iii) an early commit-reply mechanism that substantially reduces transaction execution latency. Experiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx outperforms state-of-the-art approaches by achieving at least one order of magnitude higher throughput while exhibiting near-linear scalability and low latency.}
}


@article{DBLP:journals/pacmmod/RiegerN25,
	author = {Maximilian Rieger and
                  Thomas Neumann},
	title = {{T3:} Accurate and Fast Performance Prediction for Relational Database
                  Systems With Compiled Decision Trees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {227:1--227:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725364},
	doi = {10.1145/3725364},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/RiegerN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query performance prediction is used for scheduling, resource scaling, tenant placement, and various other use-cases. Here, the main goal is to estimate the execution time of a query without running it. To be effective, predictors need to be both accurate and fast. In contrast, neural networks that were used in recent work deliver very accurate predictions but suffer from high latency. In this work, we propose the Tuple Time Tree (T3), a new model that is both accurate and fast. It is orders of magnitude faster than comparable methods and has competitive accuracy to state-of-the-art approaches. Additionally, T3 works for new database instances without re-training because it generalizes across database instances. We achieve T3's speed by relying on a low-latency decision tree model that is compiled to native machine code. We maintain high accuracy with two novel techniques: pipeline-based query plan representation and tuple-centric prediction targets. In our pipeline-based query plan representation, T3 decomposes query plans into pipelines. Then, T3 predicts the execution time of each pipeline individually, instead of the whole query in one step. With tuple-centric prediction targets, T3 predicts the expected time it takes to push a single tuple through a pipeline. It then multiplies this predicted value by the input cardinality of the pipeline to estimate its execution time. As a result, T3 achieves state-of-the-art accuracy with a low-latency decision tree model.}
}


@article{DBLP:journals/pacmmod/PugnaloniZPLNS25,
	author = {Francesco Pugnaloni and
                  Luca Zecchini and
                  Matteo Paganelli and
                  Matteo Lissandrini and
                  Felix Naumann and
                  Giovanni Simonini},
	title = {Table Overlap Estimation through Graph Embeddings},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {228:1--228:25},
	year = {2025},
	url = {https://doi.org/10.1145/3725365},
	doi = {10.1145/3725365},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PugnaloniZPLNS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Discovering duplicate or high-overlapping tables in table collections is a crucial task for eliminating redundant information, detecting inconsistencies in the evolution of a table across its multiple versions produced over time, and identifying related tables. Candidate duplicate or related tables to support this task can be identified via the estimation of the largest table overlap. Unfortunately, current solutions for finding it present serious scalability issues for heavy workloads: Sloth, the state of-the-art framework for its estimation, requires more than three days of machine time for computing 100k table overlaps. In this paper, we introduce ARMADILLO, an approach based on graph neural networks that learns table embeddings whose cosine similarity approximates the  overlap ratio  between tables, i.e., the ratio between the area of their largest table overlap and the area of the smaller table in the pair. We also introduce two new annotated datasets based on GitTables and a Wikipedia table corpus containing 1.32 million table pairs overall labeled with their overlap. Evaluating the performance of ARMADILLO on these datasets, we observed that it is able to calculate overlaps between pairs of tables several times faster than the state-of-the-art method while maintaining a good quality in approximating the exact result.}
}


@article{DBLP:journals/pacmmod/RaufFP25,
	author = {Hafiz Tayyab Rauf and
                  Andr{\'{e}} Freitas and
                  Norman W. Paton},
	title = {TableDC: Deep Clustering for Tabular Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {229:1--229:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725366},
	doi = {10.1145/3725366},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/RaufFP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep clustering (DC), a fusion of deep representation learning and clustering, has recently demonstrated positive results in data science, particularly text processing and computer vision. However, joint optimization of feature learning and data distribution in the multi-dimensional space is domain-specific, so existing DC methods struggle to generalize to other application domains (such as data integration). In data management tasks, where high-density embeddings and overlapping clusters dominate, a data management-specific DC algorithm should be able to interact better with the data properties to support data integration tasks. This paper presents a deep clustering algorithm for tabular data (TableDC) that reflects the properties of data management applications that cluster tables (schema inference), rows (entity resolution) and columns (domain discovery). To address overlapping clusters, TableDC integrates Mahalanobis distance, which considers variance and correlation within the data, offering a similarity method suitable for tabular data in high-dimensional latent spaces. TableDC also shows higher tolerance to outliers through its heavy-tailed Cauchy distribution as the similarity kernel. The proposed similarity measure is particularly beneficial where the embeddings of raw data are densely packed and exhibit high degrees of overlap. Data integration tasks may also involve large numbers of clusters, which challenges the scalability of existing DC methods. TableDC learns data embeddings with a large number of clusters more efficiently than baseline DC methods, which scale in quadratic time. We evaluated TableDC with several existing DC, Standard Clustering (SC), and state-of-the-art bespoke methods over benchmark datasets. TableDC consistently outperforms existing DC, SC and bespoke methods.}
}


@article{DBLP:journals/pacmmod/ZhuDSSZY25,
	author = {Jingyu Zhu and
                  Weiwei Deng and
                  Yu Sun and
                  Shaoxu Song and
                  Haiwei Zhang and
                  Xiaojie Yuan},
	title = {The Best of Both Worlds: On Repairing Timestamps and Attribute Values
                  for Multivariate Time Series},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {230:1--230:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725367},
	doi = {10.1145/3725367},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhuDSSZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dirty data are often observed in the multivariate time series, which not only degrades data quality but also adversely affects various downstream applications. Existing studies typically focus on repairing such errors appearing in either timestamps or attribute values alone, relying on the assumption that the other part is clean. However, in real scenarios, owing to various reasons, both timestamps and attribute values can be erroneous. It is intuitive to repair timestamps and attribute values respectively by calling different methods in turn. However, such a strategy may lead to over-repairing and introduce additional errors, by ignoring the mutual reference between timestamps and attribute values. Therefore, in this study, rather than repairing timestamps and attribute values respectively by calling different methods in turn, we consider the repairing for both attribute values and timestamps simultaneously. Our major contributions include (1) defining the multivariate speed constraints and formalizing the optimal repair problem with the NP-hardness analysis, (2) computing the exact solutions with pruning strategies and correctness ensurance, (3) designing the quadratic time approximation algorithm with the performance guarantee, (4) devising the linear time algorithm and ensuring its approximation performance bound. Empirical results over real-world dirty datasets demonstrate the superiority and practicality of our algorithms, against eleven competing methods, where our algorithm not only achieves the best accuracy but also spends the lowest time cost.}
}


@article{DBLP:journals/pacmmod/DengCJZCYW25,
	author = {Yuhao Deng and
                  Chengliang Chai and
                  Kaisen Jin and
                  Linan Zheng and
                  Lei Cao and
                  Ye Yuan and
                  Guoren Wang},
	title = {Two Birds with One Stone: Efficient Deep Learning over Mislabeled
                  Data through Subset Selection},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {231:1--231:28},
	year = {2025},
	url = {https://doi.org/10.1145/3728289},
	doi = {10.1145/3728289},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DengCJZCYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Using a large training dataset to train a big and powerful model -- a typical practice in modern deep learning, often suffers from two major problems: the expensive and slow training process and the error-prone labels. The existing approaches, targeting either speeding up the training by selecting a subset of representative training instances (subset selection) or eliminating the negative effect of mislabels during training (mislabel detection), do not perform well in this scenario due to overlooking one of these two problems. To fill this gap, we propose Deem, a novel data-efficient framework that selects a subset of representative training instances under label uncertainty. The key idea is to leverage the metadata produced during deep learning training, e.g., training losses and gradients, to estimate the label uncertainty and select the representative instances. In particular, we model the problem of subset selection under uncertainty as a problem of finding a subset that closely approximates the gradient of the whole training data set derived on soft labels. We show that it is an NP-hard problem with submodular property and propose a low complexity algorithm to solve this problem with an approximate ratio. Training on this small subset thus improves the training efficiency while guaranteeing the model's accuracy. Moreover, we propose an efficient strategy to dynamically refine this subset during the iterative training process. Extensive experiments on 6 datasets and 10 baselines demonstrate that Deem accelerates the training process up to 10X without sacrificing the model accuracy.}
}


@article{DBLP:journals/pacmmod/GupteP25,
	author = {Suchit Gupte and
                  John Paparrizos},
	title = {Understanding the Black Box: {A} Deep Empirical Dive into Shapley
                  Value Approximations for Tabular Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {232:1--232:31},
	year = {2025},
	url = {https://doi.org/10.1145/3725420},
	doi = {10.1145/3725420},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GupteP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Understanding the decisions made by machine learning models is significant for building trust and enabling the adoption of these models in real-world applications. Shapley values have emerged as a leading method for model interpretability, offering precise insights by quantifying each feature's contribution to predictions. However, computing Shapley values requires exploring all possible combinations of features, which can be computationally expensive, especially for high-dimensional data. This challenge has led to the development of various approximation techniques, often composed of estimation and replacement strategies, to compute the Shapley values efficiently. Our study focuses on the interpretability of machine learning models for tabular datasets, one of the most common and widely used data type. However, the abundance of options has created a substantial gap in determining the most appropriate technique for practical applications. Through this study, we seek to bridge this gap by comprehensively evaluating Shapley value approximations, covering 8 replacement and 17 estimation strategies across diverse regression and classification tasks. The evaluation is conducted exclusively on tabular data, leveraging 200 synthetic and real-world datasets, covering a wide range of model types, from conventional tree-based and linear models to modern neural networks. We focus on computational efficiency and the consistency of Shapley value estimates in handling high-dimensional feature spaces. Our findings reveal that traditional sampling-based approaches significantly reduce computational costs but fail to capture complex feature interactions. On the contrary, model-specific approaches that exploit the structure of the underlying model consistently outperform model-agnostic techniques, delivering higher accuracy and faster computations. Through the study, we aim to encourage further research on Shapley value approximations, advancing data-centric explainable AI.}
}


@article{DBLP:journals/pacmmod/TianOK25,
	author = {Zilu Tian and
                  Dan Olteanu and
                  Christoph Koch},
	title = {Using Process Calculus for Optimizing Data and Computation Sharing
                  in Complex Stateful Parallel Computations},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {233:1--233:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725421},
	doi = {10.1145/3725421},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/TianOK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose novel techniques that exploit data and computation sharing to improve the performance of complex stateful parallel computations, like agent-based simulations. Parallel computations are translated into behavioral equations, a novel formalism layered on top of the foundational process calculus π-calculus. Behavioral equations blend code and data, allowing a system to easily compose and transform parallel programs into specialized programs. We show how optimizations like merging programs, synthesizing efficient message data structures, eliminating local messaging, rewriting communication instructions into local computations, and aggregation pushdown can be expressed as transformations of behavioral equations. We have also built a system called OptiFusion that implements behavioral equations and the aforementioned optimizations. Our experiments showed that OptiFusion is over 10× faster than state-of-the-art stateful systems benchmarked via complex stateful workloads. Generating specialized instructions that are impractical to write by hand allows OptiFusion to outperform even the hand-optimized implementations by up to 2×.}
}


@article{DBLP:journals/pacmmod/SuiYWZZ25,
	author = {Yuan Sui and
                  Xiaochun Yang and
                  Bin Wang and
                  Yujie Zhang and
                  Baihua Zheng},
	title = {Wait and See: {A} Delayed Transactions Partitioning Approach in Deterministic
                  Database Systems for Better Performance},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {234:1--234:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725422},
	doi = {10.1145/3725422},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SuiYWZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deterministic databases are revolutionizing batch transaction processing in shared-nothing architectures, with efficiency largely hinging on minimizing cross-partition operations. However, achieving a universal data partition that eliminates cross-partition operations is often impractical. Thus, developing effective transaction partitioning strategies becomes crucial. Existing methods tend to partition and optimize transactions individually, neglecting the overarching commonalities between transactions within a batch. This oversight results in suboptimal partitioning of transactions that share similar read-write sets, ultimately missing opportunities for global batch execution optimization. In this paper, we present DelayPart, a deterministic database transaction engine that employs a ''wait and see'' strategy to address contextual conflicts between transactions within each batch. DelayPart models transaction batch partitioning as a  k -cut problem based on transaction similarity and employs a LSH forest-based approach to approximate solutions efficiently in linear time, factoring in the global overhead of remote operations for each batch. By postponing the allocation and execution of individual transactions, DelayPart systematically analyzes inter-transaction relationships, enhancing overall performance without compromising execution efficiency. We evaluated DelayPart's performance against various benchmarks on a large-scale cluster, demonstrating that it significantly outperforms state-of-the-art transaction partitioning methods.}
}


@article{DBLP:journals/pacmmod/WangCDYLL25,
	author = {Qichen Wang and
                  Bingnan Chen and
                  Binyang Dai and
                  Ke Yi and
                  Feifei Li and
                  Liang Lin},
	title = {Yannakakis+: Practical Acyclic Query Evaluation with Theoretical Guarantees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {235:1--235:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725423},
	doi = {10.1145/3725423},
	timestamp = {Fri, 18 Jul 2025 07:39:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangCDYLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acyclic conjunctive queries form the backbone of most analytical workloads, and have been extensively studied in the literature from both theoretical and practical angles. However, there is still a large divide between theory and practice. While the 40-year-old Yannakakis algorithm has strong theoretical running time guarantees, it has not been adopted in real systems due to its high hidden constant factor. In this paper, we strive to close this gap by proposing Yannakakis+, an improved version of the Yannakakis algorithm, which is more practically efficient while preserving its theoretical guarantees. Our experiments demonstrate that Yannakakis+ consistently outperforms the original Yannakakis algorithm by 2x to 5x across a wide range of queries and datasets. Another nice feature of our new algorithm is that it generates a traditional DAG query plan consisting of standard relational operators, allowing Yannakakis+ to be easily plugged into any standard SQL engine. Our system prototype currently supports four different SQL engines (DuckDB, PostgreSQL, SparkSQL, and AnalyticDB from Alibaba Cloud), and our experiments show that Yannakakis+ is able to deliver better performance than their native query plans on 160 out of the 162 queries tested, with an average speedup of 2.41x and a maximum speedup of 47,059x.}
}


@article{DBLP:journals/pacmmod/ChesettiSPP25,
	author = {Yuvaraj Chesetti and
                  Benwei Shi and
                  Jeff M. Phillips and
                  Prashant Pandey},
	title = {Zombie Hashing: Reanimating Tombstones in Graveyard},
	journal = {Proc. {ACM} Manag. Data},
	volume = {3},
	number = {3},
	pages = {236:1--236:27},
	year = {2025},
	url = {https://doi.org/10.1145/3725424},
	doi = {10.1145/3725424},
	timestamp = {Thu, 17 Jul 2025 11:27:59 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChesettiSPP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Linear probing-based hash tables offer high data locality and are considered among the fastest in real-world applications. However, they come with an inherent tradeoff between space efficiency and speed, i.e. when the hash table approaches full capacity, its performance tends to decline considerably due to an effect known as  primary clustering.  As a result they are only used at low load factors. Tombstones  (markers for deleted elements) can help mitigate the effect of primary clustering in linear probing hash tables. However, tombstones require periodic redistribution, which, in turn, requires a complete halt of regular operations. This makes linear probing not suitable in practical applications where periodic halts are unacceptable. In this paper, we present a solution to forestall primary clustering in linear probing hash tables, ensuring high data locality and consistent performance even at high load factors. Our approach redistributes tombstones within small windows, deamortizing the cost of mitigating primary clustering and eliminating the need for periodic halts. We provide theoretical guarantees that our deamortization method is asymptotically optimal in efficiency and cost. We also design an efficient implementation within dominant linear-probing hash tables and show performance improvements. We introduce Zombie hashing in two variants: ordered (compact) and unordered (vectorized) linear probing hash tables. Both variants achieve consistent, high throughput and lowest variance in operation latency compared to other state-of-the-art hash tables across numerous churn cycles, while maintaining 95% space efficiency without downtime. Our results show that Zombie hashing overcomes the limitations of linear probing while preserving high data locality.}
}
