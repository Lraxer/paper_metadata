@article{DBLP:journals/pacmmod/AgrawalAC23,
	author = {Divyakant Agrawal and
                  Sihem Amer{-}Yahia and
                  K. Selcuk Candan},
	title = {{PACMMOD} {V1} {N1} Editorial - Welcome to {PACMMOD}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {1:1--1:8},
	year = {2023},
	url = {https://doi.org/10.1145/3588046},
	doi = {10.1145/3588046},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgrawalAC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We are excited to welcome you to Volume 1 of a brand new journal - Proceedings of the ACM on Management of Data, PACMMOD. This new journal is concerned with the principles, algorithms, techniques, systems, and applications of database management systems, data management technology, and science and engineering of data and will include articles reporting cutting-edge data management, data engineering, and data science research. Articles published at PACMMOD address data challenges at various stages of the data lifecycle, from modeling, acquisition, cleaning, integration, indexing, querying, analysis, exploration, visualization, interpretation, and explanation. They focus on data-intensive components of data pipelines; and solve problems in areas of interest to our community (e.g., data curation, optimization, performance, storage, systems), operating within accuracy, privacy, fairness, and diversity constraints. Articles reporting deployed systems and solutions to data science pipelines and/or fundamental experiences and insights from evaluating real-world data engineering problems are especially encouraged.}
}


@article{DBLP:journals/pacmmod/Baunsgaard023,
	author = {Sebastian Baunsgaard and
                  Matthias Boehm},
	title = {{AWARE:} Workload-aware, Redundancy-exploiting Linear Algebra},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {2:1--2:28},
	year = {2023},
	url = {https://doi.org/10.1145/3588682},
	doi = {10.1145/3588682},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Baunsgaard023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compression is an effective technique for fitting data in available memory, reducing I/O, and increasing instruction parallelism. While data systems primarily rely on lossless compression, modern machine learning (ML) systems exploit the approximate nature of ML and mostly use lossy compression via low-precision floating- or fixed-point representations. The resulting unknown impact on learning progress, and model accuracy, however, create trust concerns, that require trial and error, and are problematic for declarative ML pipelines. Given the trend towards increasingly complex, composite ML pipelines---with outer loops for hyper-parameter tuning, feature selection, and data cleaning/augmentation---it is hard for a user to infer the impact of lossy compression. Sparsity exploitation is a common lossless scheme used to improve performance without this uncertainty. Evolving this concept to general redundancy-exploiting compression is a natural next step. Existing work on lossless compression and compressed linear algebra (CLA) enable such exploitation to a degree, but face challenges for general applicability. In this paper, we address these limitations with a workload-aware compression framework, comprising a broad spectrum of new compression schemes and kernels. Instead of a data-centric approach that optimizes compression ratios, our workload-aware compression summarizes the workload of an ML pipeline, and optimizes the compression and execution plan to minimize execution time. On various micro benchmarks and end-to-end ML pipelines, we observe improvements for individual operations up to 10,000x and ML algorithms up to νmprint6.6 x compared to uncompressed operations.}
}


@article{DBLP:journals/pacmmod/TuMLCQ0WH23,
	author = {Rong{-}Cheng Tu and
                  Xian{-}Ling Mao and
                  Kevin Qinghong Lin and
                  Chengfei Cai and
                  Weize Qin and
                  Wei Wei and
                  Hongfa Wang and
                  Heyan Huang},
	title = {Unsupervised Hashing with Semantic Concept Mining},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {3:1--3:19},
	year = {2023},
	url = {https://doi.org/10.1145/3588683},
	doi = {10.1145/3588683},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/TuMLCQ0WH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, to improve the unsupervised image retrieval performance, plenty of unsupervised hashing methods have been proposed by designing a semantic similarity matrix, which is based on the similarities between image features extracted by a pre-trained CNN model. However, most of these methods tend to ignore high-level abstract semantic concepts contained in images. Intuitively, concepts play an important role in calculating the similarity among images. In real-world scenarios, each image is associated with some concepts, and the similarity between two images will be larger if they share more identical concepts. Inspired by the above intuition, in this work, we propose a novel Unsupervised Hashing with Semantic Concept Mining, called UHSCM, which leverages a VLP model to construct a high-quality similarity matrix. Specifically, a set of randomly chosen concepts is first collected. Then, by employing a vision-language pretraining (VLP) model with the prompt engineering which has shown strong power in visual representation learning, the set of concepts is denoised according to the training images. Next, the proposed method UHSCM applies the VLP model with prompting again to mine the concept distribution of each image and construct a high-quality semantic similarity matrix based on the mined concept distributions. Finally, with the semantic similarity matrix as guiding information, a novel hashing loss with a modified contrastive loss based regularization item is proposed to optimize the hashing network. Extensive experiments on three benchmark datasets show that the proposed method outperforms the state-of-the-art baselines in the image retrieval task.}
}


@article{DBLP:journals/pacmmod/ChenZGZSZSD23,
	author = {Zheng Chen and
                  Feng Zhang and
                  Jiawei Guan and
                  Jidong Zhai and
                  Xipeng Shen and
                  Huanchen Zhang and
                  Wentong Shu and
                  Xiaoyong Du},
	title = {CompressGraph: Efficient Parallel Graph Analytics with Rule-Based
                  Compression},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {4:1--4:31},
	year = {2023},
	url = {https://doi.org/10.1145/3588684},
	doi = {10.1145/3588684},
	timestamp = {Mon, 22 Jul 2024 08:23:39 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenZGZSZSD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern graphs exert colossal time and space pressure on graph analytics applications. In 2022, Facebook social graph reaches 2.91 billion users with trillions of edges. Many compression algorithms have been developed to support direct processing on compressed graphs to address this challenge. However, previous graph compression algorithms do not focus on leveraging redundancy in repeated neighbor sequences, so they do not save the amount of computation for graph analytics. We develop CompressGraph, an efficient rule-based graph analytics engine that leverages data redundancy in graphs to achieve both performance boost and space reduction for common graph applications. CompressGraph has three advantages over previous works. First, the rule-based abstraction of CompressGraph supports the reuse of intermediate results during graph traversal, thus saving time. Second, CompressGraph has intense expressiveness to support a wide range of graph applications. Third, CompressGraph scales well under high parallelism because the context-free rules have few dependencies. Experiments show that CompressGraph provides significant performance and space benefits on both CPUs and GPUs. On evaluating six typical graph applications, CompressGraph can achieve 1.97× speedup on the CPU, while 3.95× speedup on the GPU, compared to the state-of-the-art CPU and GPU methods, respectively. Moreover, CompressGraph can save an average of 71.27% memory savings on CPU and 70.36 on GPU.}
}


@article{DBLP:journals/pacmmod/HuangLT23,
	author = {Qiang Huang and
                  Pingyi Luo and
                  Anthony K. H. Tung},
	title = {A New Sparse Data Clustering Method Based On Frequent Items},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {5:1--5:28},
	year = {2023},
	url = {https://doi.org/10.1145/3588685},
	doi = {10.1145/3588685},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangLT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large, sparse categorical data is a natural way to represent complex data like sequences, trees, and graphs. Such data is prevalent in many applications, e.g., Criteo released a terabyte size click log data of 4 billion records with millions of dimensions. While most existing clustering algorithms like k-Means work well on dense, numerical data, there exist relatively few algorithms that can cluster sets of sparse categorical features. In this paper, we propose a new method called k-FreqItems that performs scalable clustering over high-dimensional, sparse data. To make clustering results easily interpretable, k-FreqItems is built upon a novel sparse center representation called FreqItem which will choose a set of high-frequency, non-zero dimensions to represent the cluster. Unlike most existing clustering algorithms, which adopt Euclidean distance as the similarity measure, k-FreqItems uses the popular Jaccard distance for comparing sets. Since the efficiency and effectiveness of k-FreqItems are highly dependent on an initial set of representative seeds, we introduce a new randomized initialization method, SILK, to deal with the seeding problem of k-FreqItems. SILK uses locality-sensitive hash (LSH) functions for oversampling and identifies frequently co-occurred data in LSH buckets to determine a set of promising seeds, allowing k-FreqItems to converge swiftly in an iterative process. Experimental results over seven real-world sparse data sets show that the SILK seeding is around 1.1\\sim3.2× faster yet more effective than the state-of-the-art seeding methods. Notably, SILK scales up well to a billion data objects on a commodity machine with 4 GPUs. The code is available at https://github.com/HuangQiang/k-FreqItems.}
}


@article{DBLP:journals/pacmmod/HuZLHL23,
	author = {Sihao Hu and
                  Zhen Zhang and
                  Shengliang Lu and
                  Bingsheng He and
                  Zhao Li},
	title = {Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {6:1--6:19},
	year = {2023},
	url = {https://doi.org/10.1145/3588686},
	doi = {10.1145/3588686},
	timestamp = {Mon, 19 Jun 2023 16:36:09 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuZLHL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of pump-and-dump schemes (P&Ds) in the cryptocurrency market, it becomes imperative to detect such fraudulent activities in advance to alert potentially susceptible investors. In this paper, we focus on predicting the pump probability of all coins listed in the target exchange before a scheduled pump time, which we refer to as the target coin prediction task. Firstly, we conduct a comprehensive study of the latest 709 P&D events organized in Telegram from Jan. 2019 to Jan. 2022. Our empirical analysis reveals some interesting patterns of P&Ds, such as that pumped coins exhibit intra-channel homogeneity and inter-channel heterogeneity. Here channel refers a form of group in Telegram that is frequently used to coordinate P&D events. This observation inspires us to develop a novel sequence-based neural network, dubbed SNN, which encodes a channel's P&D event history into a sequence representation via the positional attention mechanism to enhance the prediction accuracy. Positional attention helps to extract useful information and alleviates noise, especially when the sequence length is long. Extensive experiments verify the effectiveness and generalizability of proposed methods. Additionally, we release the code and P&D dataset on GitHub https://github.com/Bayi-Hu/Pump-and-Dump-Detection-on-Cryptocurrency, and regularly update the dataset.}
}


@article{DBLP:journals/pacmmod/LeisA0L023,
	author = {Viktor Leis and
                  Adnan Alhomssi and
                  Tobias Ziegler and
                  Yannick Loeck and
                  Christian Dietrich},
	title = {Virtual-Memory Assisted Buffer Management},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {7:1--7:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588687},
	doi = {10.1145/3588687},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LeisA0L023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most database management systems cache pages from storage in a main memory buffer pool. To do this, they either rely on a hash table that translates page identifiers into pointers, or on pointer swizzling which avoids this translation. In this work, we propose vmcache, a buffer manager design that instead uses hardware-supported virtual memory to translate page identifiers to virtual memory addresses. In contrast to existing mmap-based approaches, the DBMS retains control over page faulting and eviction. Our design is portable across modern operating systems, supports arbitrary graph data, enables variable-sized pages, and is easy to implement. One downside of relying on virtual memory is that with fast storage devices the existing operating system primitives for manipulating the page table can become a performance bottleneck. As a second contribution, we therefore propose exmap, which implements scalable page table manipulation on Linux. Together, vmcache and exmap provide flexible, efficient, and scalable buffer management on multi-core CPUs and fast storage devices.}
}


@article{DBLP:journals/pacmmod/ZhangTPCW23,
	author = {Hantian Zhang and
                  Ki Hyun Tae and
                  Jaeyoung Park and
                  Xu Chu and
                  Steven Euijong Whang},
	title = {iFlipper: Label Flipping for Individual Fairness},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {8:1--8:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588688},
	doi = {10.1145/3588688},
	timestamp = {Fri, 23 May 2025 21:09:09 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangTPCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As machine learning becomes prevalent, mitigating any unfairness present in the training data becomes critical. Among the various notions of fairness, this paper focuses on the well-known individual fairness, which states that similar individuals should be treated similarly. While individual fairness can be improved when training a model (in-processing), we contend that fixing the data before model training (pre-processing) is a more fundamental solution. In particular, we show that label flipping is an effective pre-processing technique for improving individual fairness. Our system iFlipper solves the optimization problem of minimally flipping labels given a limit to the individual fairness violations, where a violation occurs when two similar examples in the training data have different labels. We first prove that the problem is NP-hard. We then propose an approximate linear programming algorithm and provide theoretical guarantees on how close its result is to the optimal solution in terms of the number of label flips. We also propose techniques for making the linear programming solution more optimal without exceeding the violations limit. Experiments on real datasets show that iFlipper significantly outperforms other pre-processing baselines in terms of individual fairness and accuracy on unseen test sets. In addition, iFlipper can be combined with in-processing techniques for even better results.}
}


@article{DBLP:journals/pacmmod/KhatiwadaFSCGMR23,
	author = {Aamod Khatiwada and
                  Grace Fan and
                  Roee Shraga and
                  Zixuan Chen and
                  Wolfgang Gatterbauer and
                  Ren{\'{e}}e J. Miller and
                  Mirek Riedewald},
	title = {{SANTOS:} Relationship-based Semantic Table Union Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {9:1--9:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588689},
	doi = {10.1145/3588689},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KhatiwadaFSCGMR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing techniques for unionable table search define unionability using metadata (tables must have the same or similar schemas) or column-based metrics (for example, the values in a table should be drawn from the same domain). In this work, we introduce the use of semantic relationships between pairs of columns in a table to improve the accuracy of the union search. Consequently, we introduce a new notion of unionability that considers relationships between columns, together with the semantics of columns, in a principled way. To do so, we present two new methods to discover the semantic relationships between pairs of columns. The first uses an existing knowledge base (KB), and the second (which we call a "synthesized KB") uses knowledge from the data lake itself. We adopt an existing Table Union Search benchmark and present new (open) benchmarks that represent small and large real data lakes. We show that our new unionability search algorithm, called SANTOS, outperforms a state-of-the-art union search that uses a wide variety of column-based semantics, including word embeddings and regular expressions. We show empirically that our synthesized KB improves the accuracy of union search by representing relationship semantics that may not be contained in an available KB. This result hints at a promising future of creating synthesized KBs from data lakes with limited KB coverage and using them for union search.}
}


@article{DBLP:journals/pacmmod/LiWY0Y0MCU23,
	author = {Yuanpeng Li and
                  Feiyu Wang and
                  Xiang Yu and
                  Yilong Yang and
                  Kaicheng Yang and
                  Tong Yang and
                  Zhuo Ma and
                  Bin Cui and
                  Steve Uhlig},
	title = {LadderFilter: Filtering Infrequent Items with Small Memory and Time
                  Overhead},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {10:1--10:21},
	year = {2023},
	url = {https://doi.org/10.1145/3588690},
	doi = {10.1145/3588690},
	timestamp = {Thu, 13 Feb 2025 09:29:32 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiWY0Y0MCU23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data stream processing is critical in streaming databases. Existing works pay a lot of attention to frequent items. To improve the accuracy for frequent items, existing solutions focus on accurately filtering infrequent items. While these solutions are effective, they keep track of all infrequent items and require multiple hash computations and memory accesses. This increases memory and time overhead. To reduce this overhead, we propose LadderFilter, which candiscard infrequent items efficiently in terms of both memory and time. To achieve memory efficiency, LadderFilter discards (approximately) infrequent items using multiple LRU queues. To achieve time efficiency, we leverage SIMD instructions to implement LRU policy without timestamps. We apply LadderFilter to four types of sketches. Our experimental results show that LadderFilter improves the accuracy by up to 60.6×, and the throughput by up to 1.37×, and can maintain high accuracy with small memory usage. All related code is provided open-source at Github.}
}


@article{DBLP:journals/pacmmod/LiuCFGZ23,
	author = {Shang Liu and
                  Gao Cong and
                  Kaiyu Feng and
                  Wanli Gu and
                  Fuzheng Zhang},
	title = {Effectiveness Perspectives and a Deep Relevance Model for Spatial
                  Keyword Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {11:1--11:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588691},
	doi = {10.1145/3588691},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuCFGZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geo-textual objects with both geographical location and textual description are gaining in prevalence. Over the past decades, substantial research has been conducted on spatial keyword queries, which integrate location into keyword-based querying of geo-textual content. However, existing proposals mostly focus on efficiency for processing spatial keyword queries, and little effort was made to address the effectiveness perspectives. In this work, using two datasets with ground truth query results, we evaluate the effectiveness of standard spatial keyword queries. Our evaluation results show that the TkQ query that ranks objects by a weighted combination of spatial proximity and text relevance is the most effective. Motivated by the finding, we propose a Deep relevance with Weight learning (DrW) model to further improve the effectiveness of the retrieval ranking. DrW is featured with two novel ideas: First, we propose a neural network architecture to learn the text relevance matching over the local interaction between the query and geo-textual objects. Second, we find that a query-dependent weight to balance text relevance and spatial proximity in ranking can improve effectiveness, and we develop a learning-based method to learn the query-dependent weight. Experimental results reveal that our model outperforms state-of-the-art methods on effectiveness, with improvements up to 32.15%, 32.34%, and 33.00% in terms of NDCG@3, NDCG@5, and MRR.}
}


@article{DBLP:journals/pacmmod/Jin0LZMZCC23,
	author = {Tatiana Jin and
                  Boyang Li and
                  Yichao Li and
                  Qihui Zhou and
                  Qianli Ma and
                  Yunjian Zhao and
                  Hongzhi Chen and
                  James Cheng},
	title = {Circinus: Fast Redundancy-Reduced Subgraph Matching},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {12:1--12:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588692},
	doi = {10.1145/3588692},
	timestamp = {Mon, 04 Dec 2023 21:30:04 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Jin0LZMZCC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph matching is one of the most important problems in graph analytics. Many algorithms and systems have been proposed for subgraph matching. Most of these works follow Ullmann's backtracking approach as it is memory-efficient in handling an explosive number of intermediate matching results. However, they have largely overlooked an intrinsic problem of backtracking, namely repeated computation, which contributes to a large portion of the heavy computation in subgraph matching. This paper proposes a subgraph matching system, Circinus, which enables effective computation sharing by a new compression-based backtracking method. Our extensive experiments show that Circinus significantly reduces repeated computation, which transfers to up to several orders of magnitude performance improvement.}
}


@article{DBLP:journals/pacmmod/ZhangCL23,
	author = {Zhuoxing Zhang and
                  Wu Chen and
                  Sebastian Link},
	title = {Composite Object Normal Forms: Parameterizing Boyce-Codd Normal Form
                  by the Number of Minimal Keys},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {13:1--13:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588693},
	doi = {10.1145/3588693},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangCL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We parameterize schemata in Boyce-Codd Normal Form (BCNF) by the number n of minimal keys they exhibit. We show that n quantifies a trade-off between access variety and update complexity. Indeed, access variety refers to the number of different ways by which every entity over the schema is represented uniquely, while update complexity refers to the number of attribute sets for which uniqueness needs to be preserved during updates. As normalization aims at minimizing the level of effort required to preserve data consistency during updates, we establish an algorithm that returns a lossless, dependency-preserving 3NF decomposition where the subset of output schemata not in BCNF is minimized and redundant BCNF schemata are eliminated from the highest to the lowest n exhibited. In particular, if a lossless, dependency-preserving BCNF decomposition exists, our algorithm returns one where the maximum n across all output schemata is minimized. Experiments with synthetic and real-world data quantify the impact of n on the update and query performance over schemata in BCNF with n minimal keys, and show insight into the efficacy of our algorithm suite.}
}


@article{DBLP:journals/pacmmod/HuangWWT23,
	author = {Bo Huang and
                  Victor Junqiu Wei and
                  Raymond Chi{-}Wing Wong and
                  Bo Tang},
	title = {EAR-Oracle: On Efficient Indexing for Distance Queries between Arbitrary
                  Points on Terrain Surface},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {14:1--14:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588694},
	doi = {10.1145/3588694},
	timestamp = {Sat, 30 Sep 2023 10:23:23 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangWWT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the advancement of geo-positioning technology, the terrain data has become increasingly popular and has drawn a lot of research effort from both academia and industry. The distance computation on the terrain surface is a fundamental and important problem that is widely applied in geographical information systems and 3D modeling. As could be observed from the existing studies, online computation of the distance on the terrain surface is very expensive. All existing index-based methods are only efficient under the case where the distance query must be performed among a small set of predefined points-of-interest known apriori. But, in general cases, they could not scale up to sizable datasets due to their intolerable oracle building time and space consumption. In this paper, we studied the arbitrary point-to-arbitrary point distance query on the terrain surface in which no assumption is imposed on the query points, and the distance query could be performed between any two arbitrary points. We propose an indexing structure, namely Efficient Arbitrary Point-to-Arbitrary Point Distance Oracle (EAR-Oracle), with theoretical guarantee on the accuracy, oracle building time, oracle size and query time. Our experiments demonstrate that our oracle enjoys excellent scalability and it scales up to enormous terrain surfaces but none of the existing index-based methods could be able to. Besides, it significantly outperforms all existing online computation methods by orders of magnitude in terms of the query time.}
}


@article{DBLP:journals/pacmmod/YangZZY23,
	author = {Rongjian Yang and
                  Zhijie Zhang and
                  Weiguo Zheng and
                  Jeffrey Xu Yu},
	title = {Fast Continuous Subgraph Matching over Streaming Graphs via Backtracking
                  Reduction},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {15:1--15:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588695},
	doi = {10.1145/3588695},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangZZY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Streaming graphs are drawing increasing attention in both academic and industrial communities as many graphs in real applications evolve over time. Continuous subgraph matching (shorted as CSM) aims to report the incremental matches of a query graph in such streaming graphs. It involves two major steps, i.e., candidate maintenance and incremental match generation, to answer CSM. Throughout the course of continuous subgraph matching, incremental match generation backtracking over the search space dominates the total cost. However, most previous approaches focus on developing techniques for efficient candidate maintenance, while incremental match generation receives less attention despite its importance in CSM. Aiming to minimize the overall cost, we propose two techniques to reduce backtrackings in this paper. We present a cost-effective index CaLiG that yields tighter candidate maintenance, shrinking the search space of backtracking. In addition, we develop a novel incremental matching paradigm KSS that decomposes the query vertices into conditional kernel vertices and shell vertices. With the matches of kernel vertices, the incremental matches can be produced immediately by joining the candidates of shell vertices without any backtrackings. Benefiting from reduced backtrackings, the elapsed time of CSM decreases significantly. Extensive experiments over real graphs show that our method runs faster than the state-of-the-art algorithm orders of magnitude.}
}


@article{DBLP:journals/pacmmod/Yang023,
	author = {Renchi Yang and
                  Jing Tang},
	title = {Efficient Estimation of Pairwise Effective Resistance},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {16:1--16:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588696},
	doi = {10.1145/3588696},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Yang023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given an undirected graph G, the effective resistance r(s,t) measures the dissimilarity of node pair s,t in G, which finds numerous applications in real-world problems, such as recommender systems, combinatorial optimization, molecular chemistry, and electric power networks. Existing techniques towards pairwise effective resistance estimation either trade approximation guarantees for practical efficiency, or vice versa. In particular, the state-of-the-art solution is based on a multitude of Monte Carlo random walks, rendering it rather inefficient in practice, especially on large graphs. Motivated by this, this paper first presents an improved Monte Carlo approach, AMC, which reduces both the length and amount of random walks required without degrading the theoretical accuracy guarantee, through careful theoretical analysis and an adaptive sampling scheme. Further, we develop a greedy approach, GEER, which combines AMC with sparse matrix-vector multiplications in an optimized and non-trivial way. GEER offers significantly improved practical efficiency over AMC without compromising its asymptotic performance and accuracy guarantees. Extensive experiments on multiple benchmark datasets reveal that GEER is orders of magnitude faster than the state of the art in terms of computational time when achieving the same accuracy.}
}


@article{DBLP:journals/pacmmod/00080ZC23,
	author = {Chengyu Wang and
                  Kui Wu and
                  Tongqing Zhou and
                  Zhiping Cai},
	title = {Time2State: An Unsupervised Framework for Inferring the Latent States
                  in Time Series Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {17:1--17:18},
	year = {2023},
	url = {https://doi.org/10.1145/3588697},
	doi = {10.1145/3588697},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/00080ZC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series data from monitoring applications reflect the physical or logical states of the objects, which may produce time series of distinguishable characteristics in different states. Thus, time series data can usually be split into different segments, each reflecting a state of the objects. These states carry rich high-level semantic information, e.g., run, walk, or jump, which helps people better understand the behaviour of the monitored objects. Nevertheless, these states are latent and hard to discover, because the characteristic of time series is complicated and the computational cost is high. This paper develops an efficient and effective unsupervised approach for inferring the latent states of massive multivariate time data. To reduce the computational cost, we present Time2State, a scalable framework that utilizes a sliding window and an encoder to greatly reduce the length of raw time series. To train the encoder, we propose a novel unsupervised loss function, LSE-Loss. Extensive experiments show that compared to the state-of-the-art time series representation learning methods of the same kind, LSE-Loss brings a performance improvement of up to 15% in accuracy.}
}


@article{DBLP:journals/pacmmod/LiuD023,
	author = {Hanmo Liu and
                  Shimin Di and
                  Lei Chen},
	title = {Incremental Tabular Learning on Heterogeneous Feature Space},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {18:1--18:18},
	year = {2023},
	url = {https://doi.org/10.1145/3588698},
	doi = {10.1145/3588698},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuD023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, incremental learning has attracted a lot of interest in both research communities and industries. Generally, given a series of data sets sequentially, it tries to achieve good performance on the new data set while maintaining not bad performance on the old ones. Despite the recent success of incremental learning, existing works mainly assume that the coming data set is from the feature space of old ones, i.e., homogeneous feature space. And they adopt one feature extractor to forcibly project different feature spaces into one space. However, this assumption is hard to hold in real-world scenarios. Especially, the attributes of tables may sequentially increase in tabular learning. Thus, classic incremental learning models may hinder their effectiveness. In this paper, we propose a new method, incremental tabular learning on heterogeneous feature space (ILEAHE) to solve this issue. We first propose the ideas that feature extractors should be decomposed into shared and specific extractors to process the shared and specific features across different data sets respectively. Then, we propose a novel measurement named discriminative ability to measure specific extractors. Thus, two kinds of extractors can be discriminated and the specific extractor will more focus on those domain-specific features. We further demonstrate the effectiveness of ILEAHE through empirical studies.}
}


@article{DBLP:journals/pacmmod/RazaCAA23,
	author = {Aunn Raza and
                  Periklis Chrysogelos and
                  Angelos{-}Christos G. Anadiotis and
                  Anastasia Ailamaki},
	title = {One-shot Garbage Collection for In-memory {OLTP} through Temporality-aware
                  Version Storage},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {19:1--19:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588699},
	doi = {10.1145/3588699},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/RazaCAA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most modern in-memory online transaction processing (OLTP) engines rely on multi-version concurrency control (MVCC) to provide data consistency guarantees in the presence of conflicting data accesses. MVCC improves concurrency by generating a new version of a record on every write, thus increasing the storage requirements. Existing approaches rely on garbage collection and chain consolidation to reduce the length of version chains and reclaim space by freeing unreachable versions. However, finding unreachable versions requires the traversal of long version chains, which incurs random accesses right into the critical path of transaction execution, hence limiting scalability. This paper introduces OneShotGC, a new multi-version storage design that eliminates version traversal during garbage collection, with minimal discovery and memory management overheads. OneShotGC leverages the temporal correlations across versions to opportunistically cluster them into contiguous memory blocks that can be released in one shot. We implement OneShotGC in Proteus and use YCSB and TPC-C to experimentally evaluate its performance with respect to the state-of-the-art, where we observe an improvement of up to 2x in transactional throughput.}
}


@article{DBLP:journals/pacmmod/0004YWMR23,
	author = {Lei Cao and
                  Yizhou Yan and
                  Yu Wang and
                  Samuel Madden and
                  Elke A. Rundensteiner},
	title = {AutoOD: Automatic Outlier Detection},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {20:1--20:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588700},
	doi = {10.1145/3588700},
	timestamp = {Fri, 23 May 2025 14:24:46 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0004YWMR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Outlier detection is critical in real world. Due to the existence of many outlier detection techniques which often return different results for the same data set, the users have to address the problem of determining which among these techniques is the best suited for their task and tune its parameters. This is particularly challenging in the unsupervised setting, where no labels are available for cross-validation needed for such method and parameter optimization. In this work, we propose AutoOD which uses the existing unsupervised detection techniques to automatically produce high quality outliers without any human tuning. AutoOD\'s fundamentally new strategy unifies the merits of unsupervised outlier detection and supervised classification within one integrated solution. It automatically tests a diverse set of unsupervised outlier detectors on a target data set, extracts useful signals from their combined detection results to reliably capture key differences between outliers and inliers. It then uses these signals to produce a "custom outlier classifier" to classify outliers, with its accuracy comparable to supervised outlier classification models trained with ground truth labels - without having access to the much needed labels. On a diverse set of benchmark outlier detection datasets, AutoOD consistently outperforms the best unsupervised outlier detector selected from hundreds of detectors. It also outperforms other tuning-free approaches from 12 to 97 points (out of 100) in the F-1 score.}
}


@article{DBLP:journals/pacmmod/AhujaZGS23,
	author = {Ritesh Ahuja and
                  Sepanta Zeighami and
                  Gabriel Ghinita and
                  Cyrus Shahabi},
	title = {A Neural Approach to Spatio-Temporal Data Release with User-Level
                  Differential Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {21:1--21:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588701},
	doi = {10.1145/3588701},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AhujaZGS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Several "data-for-good" projects [1, 5, 12] initiated by major companies (e.g., Meta, Google) release to the public spatio-temporal datasets to benefit COVID-19 spread modeling [17, 47, 64] and understand human mobility [14, 24]. Most often, spatio-temporal data are provided in the form of snapshot high resolution population density information, where the released statistics capture population counts in small areas for short time periods. Since high resolution is required for utility (e.g., in modeling COVID hotspots) privacy risks are elevated. To prevent malicious actors from using the data to infer sensitive details about individuals, the released datasets must be first sanitized. Typically, [1, 5, 7, 12], differential privacy (DP) is employed as protection model, due to its formal protection guarantees that prevent an adversary to learn whether a particular individual\'s data has been included in the release or not.}
}


@article{DBLP:journals/pacmmod/PingS23,
	author = {Haoyue Ping and
                  Julia Stoyanovich},
	title = {Most Expected Winner: An Interpretation of Winners over Uncertain
                  Voter Preferences},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {22:1--22:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588702},
	doi = {10.1145/3588702},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PingS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It remains an open question how to determine the winner of an election when voter preferences are incomplete or uncertain. One option is to assume some probability space over the voting profile and select the Most Probable Winner (MPW) -- the candidate or candidates with the best chance of winning. In this paper, we propose an alternative winner interpretation, selecting the Most Expected Winner (MEW) according to the expected performance of the candidates. We separate the uncertainty in voter preferences into the generation step and the observation step, which gives rise to a unified voting profile combining both incomplete and probabilistic voting profiles. We use this framework to establish the theoretical hardness of MEW over incomplete voter preferences, and then identify a collection of tractable cases for a variety of voting profiles, including those based on the popular Repeated Insertion Model (RIM) and its special case, the Mallows model. We develop solvers customized for various voter preference types to quantify the candidate performance for the individual voters, and propose a pruning strategy that optimizes computation. The performance of the proposed solvers and pruning strategy is evaluated extensively on real and synthetic benchmarks, showing that our methods are practical.}
}


@article{DBLP:journals/pacmmod/FangSGHWW23,
	author = {Chenguang Fang and
                  Shaoxu Song and
                  Haoquan Guan and
                  Xiangdong Huang and
                  Chen Wang and
                  Jianmin Wang},
	title = {Grouping Time Series for Efficient Columnar Storage},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {23:1--23:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588703},
	doi = {10.1145/3588703},
	timestamp = {Mon, 09 Sep 2024 19:07:28 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FangSGHWW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Columnar storage is now an industry standard design in most open-source or commercial time series database products, making them HTAP systems. The time column of a time series serves as the key for identifying the other value column, namely single-column storage scheme. When multiple time series share a similar set of timestamps, very likely in a module of multiple sensors, it is natural to group them together, i.e., one time column identifies multiple value columns in a single-group storage scheme. While multiple value columns sharing the same time column reduce the space cost of repeating timestamps, it may introduce extra space cost for recording null values. The reason is that time series may not be exactly aligned on each timestamp, owing to missing values, distinct data collection frequencies, unsynchronized clocks and so on. The columngroups storage scheme is thus to divide columns into multiple groups, within which the value columns share the same time column. Unfortunately, the problem of finding the optimal column groups for the minimum space cost is highly challenging, NP-hard according to our analysis. Thereby, we propose a heuristic algorithm for automatically grouping time series for efficient columnar storage. The column groups storage has been deployed in Apache IoTDB, an open-source time series database. The extensive performance analysis, over real-world data from our industrial partners, demonstrates that the proposed column groups achieve near optimal storage, more concise than the storage of single-column or single-group schemes. Interestingly, both the flushing and querying time costs of column groups are comparable to those of single-column or singlegroup, i.e., without incurring extra time cost.}
}


@article{DBLP:journals/pacmmod/ChackoMJ23,
	author = {Jeeta Ann Chacko and
                  Ruben Mayer and
                  Hans{-}Arno Jacobsen},
	title = {How To Optimize My Blockchain? {A} Multi-Level Recommendation Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {24:1--24:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588704},
	doi = {10.1145/3588704},
	timestamp = {Sun, 22 Oct 2023 11:16:15 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChackoMJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aside from the conception of new blockchain architectures, existing blockchain optimizations in the literature primarily focus on system or data-oriented optimizations within prevailing blockchains. However, since blockchains handle multiple aspects ranging from organizational governance to smart contract design, a holistic approach that encompasses all the different layers of a given blockchain system is required to ensure that all optimization opportunities are taken into consideration. In this vein, we define a multi-level optimization recommendation approach that identifies optimization opportunities within a blockchain at the system, data, and user level. Multiple metrics and attributes are derived from a blockchain log and nine optimization recommendations are formalized. We implement an automated optimization recommendation tool, BlockOptR, based on these concepts. The system is extensively evaluated with a wide range of workloads covering multiple real-world scenarios. After implementing the recommended optimizations, we observe an average of 20% improvement in the success rate of transactions and an average of 40% improvement in latency.}
}


@article{DBLP:journals/pacmmod/HouGZ0W23,
	author = {Guanhao Hou and
                  Qintian Guo and
                  Fangyuan Zhang and
                  Sibo Wang and
                  Zhewei Wei},
	title = {Personalized PageRank on Evolving Graphs with an Incremental Index-Update
                  Scheme},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {25:1--25:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588705},
	doi = {10.1145/3588705},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HouGZ0W23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {\\em Personalized PageRank (PPR) stands as a fundamental proximity measure in graph mining. Given an input graph G with the probability of decay α, a source node s and a target node t, the PPR score π(s,t) of target t with respect to source s is the probability that an α-decay random walk starting from s stops at t. A \\em single-source PPR (SSPPR) query takes an input graph G with decay probability α and a source s, and then returns the PPR π(s,v) for each node v ∈ V. Since computing an exact SSPPR query answer is prohibitive, most existing solutions turn to approximate queries with guarantees. The state-of-the-art solutions for approximate SSPPR queries are index-based and mainly focus on static graphs, while real-world graphs are usually dynamically changing. However, existing index-update schemes can not achieve a sub-linear update time. Motivated by this, we present an efficient indexing scheme for single-source PPR queries on evolving graphs. Our proposed solution is based on a classic framework that combines the forward-push technique with a random walk index for approximate PPR queries. Thus, our indexing scheme is similar to existing solutions in the sense that we store pre-sampled random walks for efficient query processing. One of our main contributions is an incremental updating scheme to maintain indexed random walks in expected O(1) time after each graph update. To achieve O(1) update cost, we need to maintain auxiliary data structures for both vertices and edges. To reduce the space consumption, we further revisit the sampling methods and propose a new sampling scheme to remove the auxiliary data structure for vertices while still supporting O(1) index update cost on evolving graphs. Extensive experiments show that our update scheme achieves orders of magnitude speed-up on update performance over existing index-based dynamic schemes without sacrificing the query efficiency.}
}


@article{DBLP:journals/pacmmod/CaoFOXZ23,
	author = {Yang Cao and
                  Wenfei Fan and
                  Weijie Ou and
                  Rui Xie and
                  Wenyue Zhao},
	title = {Transaction Scheduling: From Conflicts to Runtime Conflicts},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {26:1--26:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588706},
	doi = {10.1145/3588706},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CaoFOXZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies how to improve the performance of main memory multicore OLTP systems for executing transactions with conflicts. A promising approach is to partition transaction workloads into mutually conflict-free clusters, and distribute the clusters to different cores for concurrent execution. We show that if transactions in each cluster are properly scheduled, transactions that are traditionally considered conflicting can be executed without conflicts at runtime. In light of this, we propose to schedule transactions and reduce runtime conflicts, instead of partitioning based on the conventional notion of conflicts. We formulate the transaction scheduling problem to minimize runtime conflicts, and show that the problem is NP-complete. This said, we develop an efficient scheduling algorithm to improve parallelism. Moreover, for transactions that are not packed in batches, we show that runtime conflict analysis also helps reduce conflict penalties, by proposing a proactive deferring method. Using standard and enhanced benchmarks, we show that on average our scheduling and proactive deferring methods improve the throughput of existing partitioners and concurrency control protocols by 131% and 109%, respectively, up to 294% and 152%.}
}


@article{DBLP:journals/pacmmod/WuCZHCHZH023,
	author = {Tianhao Wu and
                  Ji Cheng and
                  Chaorui Zhang and
                  Jianfeng Hou and
                  Gengjian Chen and
                  Zhongyi Huang and
                  Weixi Zhang and
                  Wei Han and
                  Bo Bai},
	title = {ClipSim: {A} GPU-friendly Parallel Framework for Single-Source SimRank
                  with Accuracy Guarantee},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {27:1--27:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588707},
	doi = {10.1145/3588707},
	timestamp = {Fri, 11 Apr 2025 09:22:01 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuCZHCHZH023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {SimRank is an important metric to measure the topological similarity between two nodes in a graph. In particular, single-source and top-k SimRank has numerous applications in recommendation systems, network analysis, and web mining, etc. Mathematically, given a vertex, the computation of single-machine and single-source SimRank mainly lies in matrix-matrix operations. However, it is almost impossible to directly compute on large graphs. Thus, existing works yield to two main operations: a series of random walks, and sparse matrix and dense vector multiplication operations. This brings about high computation cost for SimRank on large graphs. In real-world applications, there is always the query time and accuracy trade-off, which hinders the computation of high-precision SimRank on large-scale graphs. To handle this problem, this paper proposesClipSim, the first GPU-friendly parallel framework that accelerates the single-source SimRank on GPU with accuracy guarantee. We design a novel data structure and GPU-friendly parallel algorithms for efficient computation of all the operations of SimRank on GPU. Moreover, our theoretical derivation enables ClipSim to largely reduce the number of random walks required for each node, while maintaining the same theoretical accuracy as the state-of-the-art algorithm, ExactSim. We conduct extensive experiments on real-world and synthetic datasets to demonstrate the accuracy and efficiency of ClipSim. The results show that compared with ExactSim, ClipSim obtains single-source SimRank vectors with the same accuracy and up to 160× faster computation time.}
}


@article{DBLP:journals/pacmmod/Wang0YLMT23,
	author = {Fang Wang and
                  Xiao Yan and
                  Man Lung Yiu and
                  Shuai Li and
                  Zunyao Mao and
                  Bo Tang},
	title = {Speeding Up End-to-end Query Execution via Learning-based Progressive
                  Cardinality Estimation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {28:1--28:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588708},
	doi = {10.1145/3588708},
	timestamp = {Tue, 20 Jun 2023 15:48:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Wang0YLMT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fast query execution requires learning-based cardinality estimators to have short inference time (as model inference time adds to end-to-end query execution time) and high estimation accuracy (which is crucial for finding good execution plan). However, existing estimators cannot meet both requirements due to the inherent tension between model complexity and estimation accuracy. We propose a novel Learning-based Progressive Cardinality Estimator (LPCE), which adopts a query re-optimization methodology. In particular, LPCE consists of an initial model (LPCE-I), which estimates cardinality before query execution, and a refinement model (LPCE-R), which progressively refines the cardinality estimations using the actual cardinalities of the executed operators. During query execution, re-optimization is triggered if the estimations of LPCE-I are found to have large errors, and more efficient execution plans are selected for the remaining operators using the refined estimations provided by LPCE-R. Both LPCE-I and LPCE-R are light-weight query-driven estimators but they achieve both good efficiency and high accuracy when used jointly. Besides designing the models for LPCE-I and LPCE-R, we also integrate re-optimization and LPCE into PostgreSQL, a popular database engine. Extensive experiments show that LPCE yields shorter end-to-end query execution time than state-of-the-art learning-based estimators.}
}


@article{DBLP:journals/pacmmod/ThostrupDBLB23,
	author = {Lasse Thostrup and
                  Gloria Doci and
                  Nils Boeschen and
                  Manisha Luthra and
                  Carsten Binnig},
	title = {Distributed {GPU} Joins on Fast RDMA-capable Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {29:1--29:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588709},
	doi = {10.1145/3588709},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ThostrupDBLB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present a novel pipelined GPU join that accelerates the performance of distributed DBMSs by leveraging GPU resources on fast networks. A key insight is that we enable pipelined join execution by overlapping the network shuffling with the build and probe phases, thereby significantly reducing the GPU idle time. To demonstrate this, we propose novel algorithms for distributed pipelined GPU joins with RDMA and GPUDirect for both arbitrarily large probe- and build-side tables. In our evaluation, we show our pipelined distributed GPU join can reduce the overall runtime of a full query by up to 6× against a state-of-the-art CPU-only join.}
}


@article{DBLP:journals/pacmmod/HulsebosDG23,
	author = {Madelon Hulsebos and
                  {\c{C}}agatay Demiralp and
                  Paul Groth},
	title = {GitTables: {A} Large-Scale Corpus of Relational Tables},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {30:1--30:17},
	year = {2023},
	url = {https://doi.org/10.1145/3588710},
	doi = {10.1145/3588710},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HulsebosDG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The success of deep learning has sparked interest in improving relational table tasks, like data preparation and search, with table representation models trained on large table corpora. Existing table corpora primarily contain tables extracted from HTML pages, limiting the capability to represent offline database tables. To train and evaluate high-capacity models for applications beyond the Web, we need resources with tables that resemble relational database tables. Here we introduce GitTables, a corpus of 1M relational tables extracted from GitHub. Our continuing curation aims at growing the corpus to at least 10M tables. Analyses of GitTables show that its structure, content, and topical coverage differ significantly from existing table corpora. We annotate table columns in GitTables with semantic types, hierarchical relations and descriptions from Schema.org and DBpedia. The evaluation of our annotation pipeline on the T2Dv2 benchmark illustrates that our approach provides results on par with human annotations. We present three applications of GitTables, demonstrating its value for learned semantic type detection models, schema completion methods, and benchmarks for table-to-KG matching, data search, and preparation. We make the corpus and code available at https://gittables.github.io.}
}


@article{DBLP:journals/pacmmod/Li0KLSC23,
	author = {Yifan Li and
                  Xiaohui Yu and
                  Nick Koudas and
                  Shu Lin and
                  Calvin Sun and
                  Chong Chen},
	title = {dbET: Execution Time Distribution-based Plan Selection},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {31:1--31:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588711},
	doi = {10.1145/3588711},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Li0KLSC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While selecting the execution plan for a given query based on a single estimated cost is a generally-adopted strategy, it is usually error-prone and fails to comprehensively profile the plan performance. In this work, we complement existing plan selection methods by proposing a new approach named ET, which produces execution time distributions for query plans utilizing conformal predictions. We develop dbET, a framework that integrates ET into an existing DBMS, requiring no modification to the DBMS and only incurring minor overhead to query processing. Based on the execution time distribution, we design several intuitive yet fundamental query execution objectives and devise the corresponding plan selection strategies. Our experiments on several widely-adopted benchmarks showcase that our design significantly improves the capability of DBMSs in achieving the designated objectives.}
}


@article{DBLP:journals/pacmmod/WuBCH23,
	author = {Renzhi Wu and
                  Alexander Bendeck and
                  Xu Chu and
                  Yeye He},
	title = {Ground Truth Inference for Weakly Supervised Entity Matching},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {32:1--32:28},
	year = {2023},
	url = {https://doi.org/10.1145/3588712},
	doi = {10.1145/3588712},
	timestamp = {Fri, 23 May 2025 21:09:09 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuBCH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity matching (EM) refers to the problem of identifying pairs of data records in one or more relational tables that refer to the same entity in the real world. Supervised machine learning (ML) models currently achieve state-of-the-art matching performance; however, they require a large number of labeled examples, which are often expensive or infeasible to obtain. This has inspired us to approach data labeling for EM using weak supervision. In particular, we use the labeling function abstraction popularized by Snorkel, where each labeling function (LF) is a user-provided program that can generate many noisy match/non-match labels quickly and cheaply. Given a set of user-written LFs, the quality of data labeling depends on a labeling model to accurately infer the ground-truth labels. In this work, we first propose a simple but powerful labeling model for general weak supervision tasks. Then, we tailor the labeling model specifically to the task of entity matching by considering the EM-specific transitivity property. The general form of our labeling model is simple while substantially outperforming the best existing method across ten general weak supervision datasets. To tailor the labeling model for EM, we formulate an approach to ensure that the final predictions of the labeling model satisfy the transitivity property required in EM, utilizing an exact solution where possible and an ML-based approximation in remaining cases. On two single-table and nine two-table real-world EM datasets, we show that our labeling model results in a 9% higher F1 score on average than the best existing method. We also show that a deep learning EM end model (DeepMatcher) trained on labels generated from our weak supervision approach is comparable to an end model trained using tens of thousands of ground-truth labels, demonstrating that our approach can significantly reduce the labeling efforts required in EM.}
}


@article{DBLP:journals/pacmmod/KurmanjiT23,
	author = {Meghdad Kurmanji and
                  Peter Triantafillou},
	title = {Detect, Distill and Update: Learned {DB} Systems Facing Out of Distribution
                  Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {33:1--33:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588713},
	doi = {10.1145/3588713},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KurmanjiT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine Learning (ML) is changing DBs as many DB components are being replaced by ML models. One open problem in this setting is how to update such ML models in the presence of data updates. We start this investigation focusing on data insertions (dominating updates in analytical DBs). We study how to update neural network (NN) models when new data follows a different distribution (a.k.a. it is "out-of-distribution" -- OOD), rendering previously-trained NNs inaccurate. A requirement in our problem setting is that learned DB components should ensure high accuracy for tasks on old and new data (e.g., for approximate query processing (AQP), cardinality estimation (CE), synthetic data generation (DG), etc.). This paper proposes a novel updatability framework (DDUp). DDUp can provide updatability for different learned DB system components, even based on different NNs, without the high costs to retrain the NNs from scratch. DDUp entails two components: First, a novel, efficient, and principled statistical-testing approach to detect OOD data. Second, a novel model updating approach, grounded on the principles of transfer learning with knowledge distillation, to update learned models efficiently, while still ensuring high accuracy. We develop and showcase DDUp\'s applicability for three different learned DB components, AQP, CE, and DG, each employing a different type of NN. Detailed experimental evaluation using real and benchmark datasets for AQP, CE, and DG detail DDUp\'s performance advantages.}
}


@article{DBLP:journals/pacmmod/WangLLS0023,
	author = {Zhibin Wang and
                  Longbin Lai and
                  Yixue Liu and
                  Bing Shui and
                  Chen Tian and
                  Sheng Zhong},
	title = {I/O-Efficient Butterfly Counting at Scale},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {34:1--34:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588714},
	doi = {10.1145/3588714},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangLLS0023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Butterfly (a cyclic graph motif) counting is a fundamental task with many applications in graph analysis, which aims at computing the number of butterflies in a large graph. With the rapid growth of graph data, it is more and more challenging to do butterfly counting due to the super-linear time complexity and large memory consumption. In this paper, we study I/O-efficient algorithms for doing butterfly counting on hierarchical memory. Existing algorithms of the kind cannot guarantee I/O optimality. Observing that in order to count butterflies, it suffices to "witness" a subgraph instead of the whole structure, a new class of algorithms called semi-witnessing algorithm is proposed. We prove that a semi-witnessing algorithm is not restricted by the lower bound Ømega(|E|2/MB) of a witnessing algorithm, and give a new bound of Ømega(min(|E|2/MB, |E|/|V| √M B)). We further develop the IOBufs algorithm that manages to approach the I/O lower bound, and thus claim its optimality. Finally, we make efforts to parallelize IOBufs to further improve the performance and scalability. We show in the experiment that IOBufs significantly outperforms the state-of-the-art algorithms EMRC and BFC-EM. In addition, IOBufs can scale to conducting butterfly counting on the Clueweb graph with 37 billion edges and quintillions (10^18 ) of butterflies.}
}


@article{DBLP:journals/pacmmod/ZengTC23,
	author = {Yuxiang Zeng and
                  Yongxin Tong and
                  Lei Chen},
	title = {LiteHST: {A} Tree Embedding based Method for Similarity Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {35:1--35:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588715},
	doi = {10.1145/3588715},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZengTC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Similarity search is getting increasingly useful in real applications. This paper focuses on the in-memory similarity search, i.e., the range query and k nearest neighbor (kNN) query, under arbitrary metric spaces, where the only known information is the distance function to measure the similarity between two objects. Although lots of research has studied this problem, the query efficiency of existing solutions is still unsatisfactory. To further improve the query efficiency, we are inspired by the tree embeddings, which map each object into a unique leaf of a well-structured tree solely based on the distances. Unlike existing embedding techniques (e.g., Lipschitz embeddings and pivot mapping) for similarity search, where an extra multi-dimensional index is needed to index the embedding space (e.g., Lp metrics), we directly use this tree to answer similarity search. This seems to be promising, but it is challenging to tailor tree embeddings for efficient similarity search. Specifically, we present a novel index called LiteHST, which is based on the most popular tree embedding (HST) and heavily customized for similarity search in the node structure and storage scheme. We propose a new construction algorithm with lower time complexity than existing methods and prove the optimality of LiteHST in the distance bound. Based on this new index, we also design optimization techniques that heavily reduce the number of distance computations and hence save running time. Finally, extensive experiments demonstrate that our solution outperforms the state-of-the-art in the query efficiency by a large margin.}
}


@article{DBLP:journals/pacmmod/GeorgiadisM23,
	author = {Thanasis Georgiadis and
                  Nikos Mamoulis},
	title = {Raster Intervals: An Approximation Technique for Polygon Intersection
                  Joins},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {36:1--36:18},
	year = {2023},
	url = {https://doi.org/10.1145/3588716},
	doi = {10.1145/3588716},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GeorgiadisM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many data science applications, most notably Geographic Information Systems, require the computation of spatial joins between large object collections. The objective is to find pairs of objects that intersect, i.e., share at least one common point. The intersection test is very expensive especially for polygonal objects. Therefore, the objects are typically approximated by their minimum bounding rectangles (MBRs) and the join is performed in two steps. In the filter step, all pairs of objects whose MBRs intersect are identified as candidates; in the refinement step, each of the candidate pairs is verified for intersection. The refinement step has been shown notoriously expensive, especially for polygon-polygon joins, constituting the bottleneck of the entire process. We propose a novel approximation technique for polygons, which (i) rasterizes them using a fine grid, (ii) models groups of nearby cells that intersect a polygon as an interval, and (iii) encodes each interval by a bitstring that captures the overlap of each cell in it with the polygon. We also propose an efficient intermediate filter, which is applied on the object approximations before the refinement step, to avoid it for numerous object pairs. Via experimentation with real data, we show that the end-to-end spatial join cost can be reduced by up to one order of magnitude with the help of our filter and by at least three times compared to using alternative intermediate filters.}
}


@article{DBLP:journals/pacmmod/SchleichSS23,
	author = {Maximilian Schleich and
                  Amir Shaikhha and
                  Dan Suciu},
	title = {Optimizing Tensor Programs on Flexible Storage},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {37:1--37:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588717},
	doi = {10.1145/3588717},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SchleichSS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tensor programs often need to process large tensors (vectors, matrices, or higher order tensors) that require a specialized storage format for their memory layout. Several such layouts have been proposed in the literature, such as the Coordinate Format, the Compressed Sparse Row format, and many others, that were especially designed to optimally store tensors with specific sparsity properties. However, existing tensor processing systems require specialized extensions in order to take advantage of every new storage format. In this paper we describe a system that allows users to define flexible storage formats in a declarative tensor query language, similar to the language used by the tensor program. The programmer only needs to write storage mappings, which describe, in a declarative way, how the tensors are laid out in main memory. Then, we describe a cost-based optimizer that optimizes the tensor program for the specific memory layout. We demonstrate empirically significant performance improvements compared to state-of-the-art tensor processing systems.}
}


@article{DBLP:journals/pacmmod/FanKOW23,
	author = {Zhiwei Fan and
                  Paraschos Koutris and
                  Xiating Ouyang and
                  Jef Wijsen},
	title = {LinCQA: Faster Consistent Query Answering with Linear Time Guarantees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {38:1--38:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588718},
	doi = {10.1145/3588718},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanKOW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most data analytical pipelines often encounter the problem of querying inconsistent data that violate pre-determined integrity constraints. Data cleaning is an extensively studied paradigm that singles out a consistent repair of the inconsistent data. Consistent query answering (CQA) is an alternative approach to data cleaning that asks for all tuples guaranteed to be returned by a given query on all (in most cases, exponentially many) repairs of the inconsistent data. In this paper, we identify a class of acyclic select-project-join (SPJ) queries for which CQA can be solved via SQL rewriting with a linear time guarantee. Our rewriting method can be viewed as a generalization of Yannakakis' algorithm for acyclic joins to the inconsistent setting. We present LinCQA, a system that takes as input any query in our class and outputs rewritings in both SQL and non-recursive Datalog with negation. We show that LinCQA often outperforms the existing CQA systems on both synthetic and real-world workloads, and in some cases, by orders of magnitude.}
}


@article{DBLP:journals/pacmmod/TsamouraLU23,
	author = {Efthymia Tsamoura and
                  Jaehun Lee and
                  Jacopo Urbani},
	title = {Probabilistic Reasoning at Scale: Trigger Graphs to the Rescue},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {39:1--39:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588719},
	doi = {10.1145/3588719},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/TsamouraLU23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The role of uncertainty in data management has become more prominent than ever before, especially because of the growing importance of machine learning-driven applications that produce large uncertain databases. A well-known approach to querying such databases is to blend rule-based reasoning with uncertainty. However, techniques proposed so far struggle with large databases. In this paper, we address this problem by presenting a new technique for probabilistic reasoning that exploits Trigger Graphs (TGs) -- a notion recently introduced for the non-probabilistic setting. The intuition is that TGs can effectively store a probabilistic model by avoiding an explicit materialization of the lineage and by grouping together similar derivations of the same fact. Firstly, we show how TGs can be adapted to support the possible world semantics. Then, we describe techniques for efficiently computing a probabilistic model and formally establish the correctness of our approach. We also present an extensive empirical evaluation using a prototype called LTGs. Our comparison against other leading engines shows that LTGs is not only faster, even against approximate reasoning techniques, but can also reason over probabilistic databases that existing engines cannot scale to.}
}


@article{DBLP:journals/pacmmod/SvingosHGPI23,
	author = {Christoforos Svingos and
                  Andr{\'{e}} Hernich and
                  Hinnerk Gildhoff and
                  Yannis Papakonstantinou and
                  Yannis E. Ioannidis},
	title = {Foreign Keys Open the Door for Faster Incremental View Maintenance},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {40:1--40:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588720},
	doi = {10.1145/3588720},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SvingosHGPI23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serverless cloud-based warehousing systems enable users to create materialized views in order to speed up predictable and repeated query workloads. Incremental view maintenance (IVM) minimizes the time needed to bring a materialized view up-to-date. It allows the refresh of a materialized view solely based on the base table changes since the last refresh. In serverless cloud-based warehouses, IVM uses computations defined as SQL scripts that update the materialized view based on updates to its base tables. However, the scripts set up for materialized views with inner joins are not optimal in the presence of foreign key constraints. For instance, for a join of two tables, the state of the art IVM computations use a UNION ALL operator of two joins - one computing the contributions to the join from updates to the first table and the other one computing the remaining contributions from the second table. Knowing that one of the join keys is a foreign-key would allow us to prune all but one of the UNION ALL branches and obtain a more efficient IVM script. In this work, we explore ways of incorporating knowledge about foreign key into IVM in order to speed up its performance. Experiments in Redshift showed that the proposed technique improved the execution times of the whole refresh process up to 2 times, and up to 2.7 times the process of calculating the necessary changes that will be applied into the materialized view.}
}


@article{DBLP:journals/pacmmod/WuNAKM23,
	author = {Ziniu Wu and
                  Parimarjan Negi and
                  Mohammad Alizadeh and
                  Tim Kraska and
                  Samuel Madden},
	title = {FactorJoin: {A} New Cardinality Estimation Framework for Join Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {41:1--41:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588721},
	doi = {10.1145/3588721},
	timestamp = {Tue, 08 Aug 2023 10:54:19 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuNAKM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality estimation is one of the most fundamental and challenging problems in query optimization. Neither classical nor learning-based methods yield satisfactory performance when estimating the cardinality of the join queries. They either rely on simplified assumptions leading to ineffective cardinality estimates or build large models to understand the complicated data distributions, leading to long planning times and a lack of generalizability across queries. In this paper, we propose a new framework FactorJoin for estimating join queries. FactorJoin combines the idea behind the classical join-histogram method to efficiently handle joins with the learning-based methods to accurately capture attribute correlation Specifically, FactorJoin scans every table in a DB and builds single-table conditional distributions during an offline preparation phase. When a join query comes, FactorJoin translates it into a factor graph model over the learned distributions to effectively and efficiently estimate its cardinality. Unlike existing learning-based methods, FactorJoin does not need to de-normalize joins upfront or require executed query workloads to train the model. Since it only relies on single-table statistics, FactorJoin has a small space overhead and is extremely easy to train and maintain. In our evaluation, FactorJoin can produce more effective estimates than the previous state-of-the-art learning-based methods, with 40x less estimation latency, 100x smaller model size, and 100x faster training speed at comparable or better accuracy. In addition, FactorJoin can estimate 10,000 sub-plan queries within one second to optimize the query plan, which is very close to the traditional cardinality estimators in commercial DBMS.}
}


@article{DBLP:journals/pacmmod/GenossarSG23,
	author = {Bar Genossar and
                  Roee Shraga and
                  Avigdor Gal},
	title = {FlexER: Flexible Entity Resolution for Multiple Intents},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {42:1--42:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588722},
	doi = {10.1145/3588722},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GenossarSG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity resolution, a longstanding problem of data cleaning and integration, aims at identifying data records that represent the same real-world entity. Existing approaches treat entity resolution as a universal task, assuming the existence of a single interpretation of a real-world entity and focusing only on finding matched records, separating corresponding from non-corresponding ones, with respect to this single interpretation. However, in real-world scenarios, where entity resolution is part of a more general data project, downstream applications may have varying interpretations of real-world entities relating, for example, to various user needs. In what follows, we introduce the problem of multiple intents entity resolution (MIER), an extension to the universal (single intent) entity resolution task. As a solution, we propose FlexER, utilizing contemporary solutions to universal entity resolution tasks to solve MIER. FlexER addresses the problem as a multi-label classification problem. It combines intent-based representations of tuple pairs using a multiplex graph representation that serves as an input to a graph neural network (GNN). FlexER learns intent representations and improves the outcome to multiple resolution problems. A large-scale empirical evaluation introduces a new benchmark and, using also two well-known benchmarks, shows that FlexER effectively solves the MIER problem and outperforms the state-of-the-art for a universal entity resolution.}
}


@article{DBLP:journals/pacmmod/Faria023,
	author = {Nuno Faria and
                  Jos{\'{e}} Pereira},
	title = {MRVs: Enforcing Numeric Invariants in Parallel Updates to Hotspots
                  with Randomized Splitting},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {43:1--43:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588723},
	doi = {10.1145/3588723},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Faria023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Performance of transactional systems is degraded by update hotspots as conflicts lead to waiting and wasted work. This is particularly challenging in emerging large-scale database systems, as latency increases the probability of conflicts, state-of-the-art lock-based mitigations are not available, and most alternatives provide only weak consistency and cannot enforce lower bound invariants. We address this challenge with Multi-Record Values (MRVs), a technique that can be layered on existing database systems and that uses randomization to split and access numeric values in multiple records such that the probability of conflict can be made arbitrarily small. The only coordination needed is the underlying transactional system, meaning it retains existing isolation guarantees. The proposal is tested on five different systems ranging from DBx1000 (scale-up) to MySQL GR and a cloud-native NewSQL system (scale-out). The experiments explore design and configuration trade-offs and, with the TPC-C and STAMP Vacation benchmarks, demonstrate improved throughput and reduced abort rates when compared to alternatives.}
}


@article{DBLP:journals/pacmmod/YeHCY23,
	author = {Chenhao Ye and
                  Wuh{-}Chwen Hwang and
                  Keren Chen and
                  Xiangyao Yu},
	title = {Polaris: Enabling Transaction Priority in Optimistic Concurrency Control},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {44:1--44:24},
	year = {2023},
	url = {https://doi.org/10.1145/3588724},
	doi = {10.1145/3588724},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YeHCY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transaction priority is a critical feature for real-world database systems. Under high contention, certain classes of transactions should be given a higher chance to commit than others. Such a prioritization mechanism is commonly implemented in locking-based concurrency control protocols as some lock scheduling mechanisms, but it is rarely supported in the world of optimistic concurrency control. We present Polaris, an optimistic concurrency control protocol that supports multiple priority levels. To enforce priority, Polaris introduces a minimal amount of pessimism through a lightweight reservation mechanism. The protocol is fully optimistic among transactions within the same priority level and preserves the high throughput advantage of optimistic protocols. Our evaluation with YCSB workload shows that Polaris can make the p999 tail latency of high-priority transactions 13x lower than that of low-priority ones. With an abort-aware priority assignment policy, Polaris can deliver 1.9x higher throughput and 17x lower tail latency compared to Silo for high-contention workloads.}
}


@article{DBLP:journals/pacmmod/Liu0ZX23,
	author = {Kaixin Liu and
                  Sibo Wang and
                  Yong Zhang and
                  Chunxiao Xing},
	title = {An Efficient Algorithm for Distance-based Structural Graph Clustering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {45:1--45:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588725},
	doi = {10.1145/3588725},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Liu0ZX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Structural graph clustering (SCAN) is a classic graph clustering algorithm. In SCAN, a key step is to compute the structural similarity between vertices according to the overlap ratio of one-hop neighborhoods. Given two vertices u and v, existing studies only consider the case when u and v are neighbors. However, the structural similarity between non-neighboring vertices in SCAN is always zero, and using only one-hop neighbors on weighted graphs discards the weights on each edge. Both may not reflect the true closeness of two vertices and may fail to return high-quality clustering results. To tackle this issue, we define and study the distance-based structural graph clustering problem. Given a distance threshold d and two vertices u and v, the structural similarity between u and v is defined as the ratio of their respective neighbors within a distance of no more than d. We show that the newly defined distance-based SCAN achieves better clustering results compared to the vanilla version of SCAN. However, the new definition brings challenges in the computation of final clustering results. To tackle this efficiency issue, we propose DistanceSCAN, an efficient approximate algorithm for solving the distance-based SCAN problem. The main idea of DistanceSCAN is to use all-distances bottom-k sketches (ADS) to speed up the computation of similarities. Given the ADS, we can derive the similarity between two vertices with a bounded cost of O(k). However, to ensure that the estimated similarity has an approximation guarantee, the value of k still needs to be set to as large as thousands. This brings high computational costs when computing the similarities between neighboring vertices. To tackle this issue, we further construct histograms to prune the structural similarity computations of vertices pairs. Extensive experiments on real datasets validate the effectiveness and efficiency of DistanceSCAN.}
}


@article{DBLP:journals/pacmmod/ConwayFJ23,
	author = {Alex Conway and
                  Martin Farach{-}Colton and
                  Rob Johnson},
	title = {SplinterDB and Maplets: Improving the Tradeoffs in Key-Value Store
                  Compaction Policy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {46:1--46:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588726},
	doi = {10.1145/3588726},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ConwayFJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A critical aspect of modern key-value stores is the interaction between compaction policy and filters. Aggressive compaction reduces the on-disk footprint of a key-value store and can improve query performance, but can reduce insertion throughput because it is I/O and CPU expensive. Filters can mitigate the query costs of lazy compaction, but only if they fit in RAM, limiting the scalability of queries with lazy compaction. And, with fast storage devices, the CPU costs of querying filters in a lazy compacting system can be significant. In this work, we present Mapped SplinterDB, a key-value store that achieves excellent insertion performance, query performance, space efficiency, and scalability by replacing filters with maplets, space-efficient data structures that act as lossy maps with false positives. Critically, we use quotient maplets, which can be merged and resized without access to the underlying data, enabling us to decouple compaction of the data from compaction of the quotient maplets. Thus Mapped SplinterDB can compact data lazily and quotient maplets aggressively, so that each level has multiple sorted runs of data but only one quotient maplet. Quotient maplets are so small that compacting them aggressively is still cheaper than compacting the (much larger) data lazily, so overall we get the insertion performance of a lazily compacted system. And, since there is only one quotient maplet to query on each level, we get the query performance of an aggressively compacted system. Furthermore, quotient maplets can accelerate queries even when they don't fit in RAM, improving scalability to huge datasets. We also show how to use quotient maplets to estimate when a compaction could resolve a high density of updates, enabling Mapped SplinterDB to perform targeted compactions for space recovery. In our benchmarks, Mapped SplinterDB matches the insertion performance of SplinterDB, a state-of-the-art lazily compacted system, and beats RocksDB, an aggressive compacting system, by up to 9×. On queries, Mapped SplinterDB outperforms SplinterDB and RocksDB by up to 89% and 83%, respectively, and scales gracefully to huge datasets. Mapped SplinterDB is able to dynamically trade update performance for space efficiency, resulting in space overheads on update-heavy workloads as low as 15-61%, whereas RocksDB had 80-117% and SplinterDB had up to 137% space overhead.}
}


@article{DBLP:journals/pacmmod/0001BCFKTJ23,
	author = {Prashant Pandey and
                  Michael A. Bender and
                  Alex Conway and
                  Martin Farach{-}Colton and
                  William Kuszmaul and
                  Guido Tagliavini and
                  Rob Johnson},
	title = {IcebergHT: High Performance Hash Tables Through Stability and Low
                  Associativity},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {47:1--47:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588727},
	doi = {10.1145/3588727},
	timestamp = {Wed, 23 Oct 2024 08:55:27 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0001BCFKTJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern hash table designs for DRAM and PMEM strive to minimize space while maximizing speed. The most important factor in speed is the number of cache lines accessed during updates and queries. On PMEM, there is an additional consideration, which is to minimize the number of writes, because on PMEM writes are more expensive than reads. This paper proposes two design objectives, stability and low-associativity, that enable us to build hash tables that minimize cache-line accesses for all operations. A hash table is stable if it does not move items around, and a hash table has low associativity if there are only a few locations where an item can be stored. Low associativity ensures that queries need to examine only a few memory locations, and stability ensures that insertions write to very few cache lines. Stability also simplifies concurrency and, on PMEM, crash safety. We present IcebergHT, a fast, concurrent, space-efficient, and crash-safe (for PMEM) hash table based on the design principles of stability and low associativity. IcebergHT combines in-memory metadata with a new hashing technique, iceberg hashing, that is (1) space efficient, (2) stable, and (3) supports low associativity. In contrast, existing hash-tables either modify numerous cache lines during insertions (e.g. cuckoo hashing), access numerous cache lines during queries (e.g. linear probing), or waste space (e.g. chaining). Moreover, the combination of (1)-(3) yields several emergent benefits: IcebergHT scales better than other hash tables, has excellent performance, and supports crash-safety on PMEM. Our benchmarks show that IcebergHT has excellent performance both in DRAM and PMEM. In PMEM, IcebergHT insertions are 50% to 3× faster than state-of-the-art PMEM hash tables, such as Dash and CLHT, and queries are 20% to 2× faster. IcebergHT space overhead is 17%, whereas Dash and CLHT have space overheads of 2× and 3×, respectively. IcebergHT also scaled linearly throughout our experiments and is crash safe. In DRAM, IcebergHT outperforms state-of-the-art hash tables libcuckoo and CLHT by almost 2× on insertions while offering good query throughput and much better space efficiency.}
}


@article{DBLP:journals/pacmmod/0006SL0P023,
	author = {Jiayao Zhang and
                  Qiheng Sun and
                  Jinfei Liu and
                  Li Xiong and
                  Jian Pei and
                  Kui Ren},
	title = {Efficient Sampling Approaches to Shapley Value Approximation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {48:1--48:24},
	year = {2023},
	url = {https://doi.org/10.1145/3588728},
	doi = {10.1145/3588728},
	timestamp = {Fri, 28 Feb 2025 17:23:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/0006SL0P023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shapley value provides a unique way to fairly assess each player's contribution in a coalition and has enjoyed many applications. However, the exact computation of Shapley value is #P-hard due to the combinatoric nature of Shapley value. Many existing applications of Shapley value are based on Monte-Carlo approximation, which requires a large number of samples and the assessment of utility on many coalitions to reach high quality approximation, and thus is still far from being efficient. Can we achieve an efficient approximation of Shapley value by smartly obtaining samples? In this paper, we treat the sampling approach to Shapley value approximation as a stratified sampling problem. Our main technical contributions are a novel stratification design and two sample allocation methods based on Neyman allocation and empirical Bernstein bound, respectively. Experimental results on several real data sets and synthetic data sets demonstrate the effectiveness and efficiency of our novel stratification design and sampling approaches.}
}


@article{DBLP:journals/pacmmod/YuL23,
	author = {Kaiqiang Yu and
                  Cheng Long},
	title = {Maximum k-Biplex Search on Bipartite Graphs: {A} Symmetric-BK Branching
                  Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {49:1--49:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588729},
	doi = {10.1145/3588729},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enumerating maximal k-biplexes (MBPs) of a bipartite graph has been used for applications such as fraud detection. Nevertheless, there usually exists an exponential number of MBPs, which brings up two issues when enumerating MBPs, namely the effectiveness issue (many MBPs are of low values) and the efficiency issue (enumerating all MBPs is not affordable on large graphs). Existing proposals of tackling this problem impose constraints on the number of vertices of each MBP to be enumerated, yet they are still not sufficient (e.g., they require to specify the constraints, which is often not user-friendly, and cannot control the number of MBPs to be enumerated directly). Therefore, in this paper, we study the problem of finding K MBPs with the most edges called MaxBPs, where K is a positive integral user parameter. The new proposal well avoids the drawbacks of existing proposals (i.e., the number of MBPs to be enumerated is directly controlled and the MBPs to be enumerated tend to have high values since they have more edges than the majority of MBPs). We formally prove the NP-hardness of the problem. We then design two branch-and-bound algorithms, among which, the better one called FastBB improves the worst-case time complexity to O*(γkn), where O* suppresses the polynomials, γk is a real number that relies on k and is strictly smaller than 2, and n is the number of vertices in the graph. For example, for k=1, γk is equal to 1.754. We further introduce three techniques for boosting the performance of the branch-and-bound algorithms, among which, the best one called PBIE can further improve the time complexity to O*(γkd3) for large sparse graphs, where d is the maximum degree of the graph (note that d< <n for sparse graphs). We conduct extensive experiments on both real and synthetic datasets, and the results show that our algorithm is up to four orders of magnitude faster than all baselines and finding MaxBPs works better than finding all MBPs for a fraud detection application.}
}


@article{DBLP:journals/pacmmod/FanFYWMSJFHBBFW23,
	author = {Yangxin Fan and
                  Xuanji Yu and
                  Raymond Wieser and
                  David Meakin and
                  Avishai Shaton and
                  Jean{-}Nicolas Jaubert and
                  Robert Flottemesch and
                  Michael Howell and
                  Jennifer Braid and
                  Laura S. Bruckman and
                  Roger H. French and
                  Yinghui Wu},
	title = {Spatio-Temporal Denoising Graph Autoencoders with Data Augmentation
                  for Photovoltaic Data Imputation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {50:1--50:19},
	year = {2023},
	url = {https://doi.org/10.1145/3588730},
	doi = {10.1145/3588730},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanFYWMSJFHBBFW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of the global Photovoltaic (PV) market with real time data-loggers has enabled large scale PV data analytical pipelines for power forecasting and reliability assessment of PV fleets. Nevertheless, the performance of PV data analysis depends on the quality of PV timeseries data. We propose a novel Spatio-Temporal Denoising Graph Autoencoder STD-GAE framework to impute missing PV Power Data. STD-GAE exploits temporal correlation, spatial coherence, and value dependencies from domain knowledge to recover missing data. It is empowered by two modules. (1) To cope with sparse yet various scenarios of missing data, STD-GAE incorporates a domain-knowledge aware data augmentation module to create plausible variations of missing data patterns. This generalizes STD-GAE to robust imputation over different seasons and environment. (2) STD-GAE nontrivially integrates spatiotemporal graph convolution layers and denoising autoencoder to improve the accuracy of imputation accuracy at PV fleet level. Experimental results on two PV datasets show that STD-GAE can achieve a gain of 43.14% in imputation accuracy and remains less sensitive to missing rate, different seasons, and missing scenarios, compared with state-of-the-art data imputation methods.}
}


@article{DBLP:journals/pacmmod/Huang00TZ023,
	author = {Kai Huang and
                  Haibo Hu and
                  Qingqing Ye and
                  Kai Tian and
                  Bolong Zheng and
                  Xiaofang Zhou},
	title = {{TED:} Towards Discovering Top-k Edge-Diversified Patterns in a Graph
                  Database},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {51:1--51:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588736},
	doi = {10.1145/3588736},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Huang00TZ023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With an exponentially growing number of graphs from disparate repositories, there is a strong need to analyze a graph database containing an extensive collection of small- or medium-sized data graphs (e.g., chemical compounds). Although subgraph enumeration and subgraph mining have been proposed to bring insights into a graph database by a set of subgraph structures, they often end up with similar or homogenous topologies, which is undesirable in many graph applications. To address this limitation, we propose the Top-k Edge-Diversified Patterns Discovery problem to retrieve a set of subgraphs that cover the maximum number of edges in a database. To efficiently process such query, we present a generic and extensible framework called Ted which achieves a guaranteed approximation ratio to the optimal result. Two optimization strategies are further developed to improve the performance. Experimental studies on real-world datasets demonstrate the superiority of Ted to traditional techniques.}
}


@article{DBLP:journals/pacmmod/LiSCY23,
	author = {Yiming Li and
                  Yanyan Shen and
                  Lei Chen and
                  Mingxuan Yuan},
	title = {Orca: Scalable Temporal Graph Neural Network Training with Theoretical
                  Guarantees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {52:1--52:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588737},
	doi = {10.1145/3588737},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiSCY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Representation learning over dynamic graphs is critical for many real-world applications such as social network services and recommender systems. Temporal graph neural networks (T-GNNs) are powerful representation learning methods and have achieved remarkable effectiveness on continuous-time dynamic graphs. However, T-GNNs still suffer from high time complexity, which increases linearly with the number of timestamps and grows exponentially with the model depth, causing them not scalable to large dynamic graphs. To address the limitations, we propose Orca, a novel framework that accelerates T-GNN training by non-trivially caching and reusing intermediate embeddings. We design an optimal cache replacement algorithm, named MRU, under a practical cache limit. MRU not only improves the efficiency of training T-GNNs by maximizing the number of cache hits but also reduces the approximation errors by avoiding keeping and reusing extremely stale embeddings. Meanwhile, we develop profound theoretical analyses of the approximation error introduced by our reuse schemes and offer rigorous convergence guarantees. Extensive experiments have validated that Orca can obtain two orders of magnitude speedup over the state-of-the-art baselines while achieving higher precision on large dynamic graphs.}
}


@article{DBLP:journals/pacmmod/DeedsSB23,
	author = {Kyle B. Deeds and
                  Dan Suciu and
                  Magdalena Balazinska},
	title = {SafeBound: {A} Practical System for Generating Cardinality Bounds},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {53:1--53:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588907},
	doi = {10.1145/3588907},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DeedsSB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent work has reemphasized the importance of cardinality estimates for query optimization. While new techniques have continuously improved in accuracy over time, they still generally allow for under-estimates which often lead optimizers to make overly optimistic decisions. This can be very costly for expensive queries. An alternative approach to estimation is cardinality bounding, also called pessimistic cardinality estimation, where the cardinality estimator provides guaranteed upper bounds of the true cardinality. By never underestimating, this approach allows the optimizer to avoid potentially inefficient plans. However, existing pessimistic cardinality estimators are not yet practical: they use very limited statistics on the data, and cannot handle predicates. In this paper, we introduce SafeBound, the first practical system for generating cardinality bounds. SafeBound builds on a recent theoretical work that uses degree sequences on join attributes to compute cardinality bounds, extends this framework with predicates, introduces a practical compression method for the degree sequences, and implements an efficient inference algorithm. Across four workloads, SafeBound achieves up to 80% lower end-to-end runtimes than PostgreSQL, and is on par or better than state of the art ML-based estimators and pessimistic cardinality estimators, by improving the runtime of the expensive queries. It also saves up to 500x in query planning time, and uses up to 6.8x less space compared to state of the art cardinality estimation methods.}
}


@article{DBLP:journals/pacmmod/PengCCYX23,
	author = {Yun Peng and
                  Byron Choi and
                  Tsz Nam Chan and
                  Jianye Yang and
                  Jianliang Xu},
	title = {Efficient Approximate Nearest Neighbor Search in Multi-dimensional
                  Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {54:1--54:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588908},
	doi = {10.1145/3588908},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/PengCCYX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate nearest neighbor (ANN) search is a fundamental search in multi-dimensional databases, which has numerous real-world applications, such as image retrieval, recommendation, entity resolution, and sequence matching. Proximity graph (PG) has been the state-of-the-art index for ANN search. However, the search on existing PGs either suffers from a high time complexity or has no performance guarantee on the search result. In this paper, we propose a novel τ-monotonic graph (τ- MG) to address the limitations. The novelty of τ-MG lies in a τ-monotonic property. Based on this property, we prove that if the distance between a query q and its nearest neighbor is less than a constant τ, the search on τ-MG guarantees to find the exact nearest neighbor of q and the time complexity of the search is smaller than all existing PG-based methods. For index construction efficiency, we propose an approximate variant of τ-MG, namely τ-monotonic neighborhood graph (τ- MNG), which only requires the neighborhood of each node to be τ-monotonic. We further propose an optimization to reduce the number of distance computations in search. Our extensive experiments show that our techniques outperform all existing methods on well-known real-world datasets.}
}


@article{DBLP:journals/pacmmod/TangWZ0023,
	author = {Xiu Tang and
                  Sai Wu and
                  Dongxiang Zhang and
                  Feifei Li and
                  Gang Chen},
	title = {Detecting Logic Bugs of Join Optimizations in {DBMS}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {55:1--55:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588909},
	doi = {10.1145/3588909},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/TangWZ0023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generation-based testing techniques have shown their effectiveness in detecting logic bugs of DBMS, which are often caused by improper implementation of query optimizers. Nonetheless, existing generation-based debug tools are limited to single-table queries and there is a substantial research gap regarding multi-table queries with join operators. In this paper, we propose TQS, a novel testing framework targeted at detecting logic bugs derived by queries involving multi-table joins. Given a target DBMS, TQS achieves the goal with two key components: Data-guided Schema and Query Generation (DSG) and Knowledge-guided Query Space Exploration (KQE). DSG addresses the key challenge of multi-table query debugging: how to generate ground-truth (query, result) pairs for verification. It adopts the database normalization technique to generate a testing schema and maintains a bitmap index for result tracking. To improve debug efficiency, DSG also artificially inserts some noises into the generated data. To avoid repetitive query space search, KQE forms the problem as isomorphic graph set discovery and combines the graph embedding and weighted random walk for query generation. We evaluated TQS on four popular DBMSs: MySQL, MariaDB, TiDB and PolarDB. Experimental results show that TQS is effective in finding logic bugs of join optimization in database management systems. It successfully detected 115 bugs within 24 hours, including 31 bugs in MySQL, 30 in MariaDB, 31 in TiDB, and 23 in PolarDB respectively.}
}


@article{DBLP:journals/pacmmod/LiuZZZ0XWL023,
	author = {Zirui Liu and
                  Yixin Zhang and
                  Yifan Zhu and
                  Ruwen Zhang and
                  Tong Yang and
                  Kun Xie and
                  Sha Wang and
                  Tao Li and
                  Bin Cui},
	title = {TreeSensing: Linearly Compressing Sketches with Flexibility},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {56:1--56:28},
	year = {2023},
	url = {https://doi.org/10.1145/3588910},
	doi = {10.1145/3588910},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuZZZ0XWL023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A Sketch is an excellent probabilistic data structure, which records the approximate statistics of data streams. Linear additivity is an important property of sketches. This paper studies how to keep the linear property after sketch compression. Most existing compression methods do not keep the linear property. We propose TreeSensing, an accurate, efficient, and flexible framework to linearly compress sketches. In TreeSensing, we first separate a sketch into two parts according to counter values. For the sketch with small counters, we propose a technique called TreeEncoding to compress it into a hierarchical structure. For the sketch with large counters, we propose a technique called SketchSensing to compress it using compressive sensing. We theoretically analyze the accuracy of TreeSensing. We use TreeSensing to compress 7 sketches and conduct two end-to-end experiments: distributed measurement and distributed machine learning. Experimental results show that TreeSensing outperforms prior art on both accuracy and efficiency, which achieves up to 100× smaller error and 5.1× higher speed than state-of-the-art Cluster-Reduce. All related codes are open-sourced.}
}


@article{DBLP:journals/pacmmod/OmarDK023,
	author = {Reham Omar and
                  Ishika Dhall and
                  Panos Kalnis and
                  Essam Mansour},
	title = {A Universal Question-Answering Platform for Knowledge Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {57:1--57:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588911},
	doi = {10.1145/3588911},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/OmarDK023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge from diverse application domains is organized as knowledge graphs (KGs) that are stored in RDF engines accessible in the web via SPARQL endpoints. Expressing a well-formed SPARQL query requires information about the graph structure and the exact URIs of its components, which is impractical for the average user. Question answering (QA) systems assist by translating natural language questions to SPARQL. Existing QA systems are typically based on application-specific human-curated rules, or require prior information, expensive pre-processing and model adaptation for each targeted KG. Therefore, they are hard to generalize to a broad set of applications and KGs. In this paper, we propose KGQAn, a universal QA system that does not need to be tailored to each target KG. Instead of curated rules, KGQAn introduces a novel formalization of question understanding as a text generation problem to convert a question into an intermediate abstract representation via a neural sequence-to-sequence model. We also develop a just-in-time linker that maps at query time the abstract representation to a SPARQL query for a specific KG, using only the publicly accessible APIs and the existing indices of the RDF store, without requiring any pre-processing. Our experiments with several real KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin the state-of-the-art in terms of quality of answers and processing time, especially for arbitrary KGs, unseen during the training.}
}


@article{DBLP:journals/pacmmod/HuangM23,
	author = {Xiaogang Huang and
                  Tiefeng Ma},
	title = {Fast Density-Based Clustering: Geometric Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {58:1--58:24},
	year = {2023},
	url = {https://doi.org/10.1145/3588912},
	doi = {10.1145/3588912},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DBSCAN is a fundamental density-based clustering algorithm with extensive applications. However, a bottleneck of DBSCAN is its O(n2) worst-case time complexity. In this paper, we propose an algorithm called GAP-DBC, which exploits the geometric relationships between points to solve this problem. GAP-DBC introduces an efficient partitioning algorithm to partition the data set with a limited number of range queries and then establishes an initial cluster structure based on the partition. GAP-DBC proceeds to iteratively refine the cluster structure by additional range queries. Finally, the cluster structure is accomplished using an iterative algorithm that utilizes the spatial relationships among points to reduce unnecessary distance calculations. We further demonstrate theoretically that GAP-DBC has an excellent guarantee in terms of computational efficiency. We conducted experiments on both synthetic and real-world data sets to evaluate the performance of GAP-DBC. The results show that our algorithm is competitive with other state-of-the-art algorithms.}
}


@article{DBLP:journals/pacmmod/Mao00LM23,
	author = {Yancan Mao and
                  Jianjun Zhao and
                  Shuhao Zhang and
                  Haikun Liu and
                  Volker Markl},
	title = {MorphStream: Adaptive Scheduling for Scalable Transactional Stream
                  Processing on Multicores},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {59:1--59:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588913},
	doi = {10.1145/3588913},
	timestamp = {Thu, 21 Sep 2023 13:07:21 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Mao00LM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transactional stream processing engines (TSPEs) differ significantly in their designs, but all rely on non- adaptive scheduling strategies for processing concurrent state transactions. Subsequently, none exploit multicore parallelism to its full potential due to complex workload dependencies. This paper introduces MorphStream, which adopts a novel approach by decomposing scheduling strategies into three dimensions and then strives to make the right decision along each dimension, based on analyzing the decision trade-offs under varying workload characteristics. Compared to the state-of-the-art, MorphStream achieves up to 3.4 times higher throughput and 69.1% lower processing latency for handling real-world use cases with complex and dynamically changing workload dependencies.}
}


@article{DBLP:journals/pacmmod/WangYXZLTG23,
	author = {Pinghui Wang and
                  Chengjin Yang and
                  Dongdong Xie and
                  Junzhou Zhao and
                  Hui Li and
                  Jing Tao and
                  Xiaohong Guan},
	title = {An Effective and Differentially Private Protocol for Secure Distributed
                  Cardinality Estimation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {60:1--60:24},
	year = {2023},
	url = {https://doi.org/10.1145/3588914},
	doi = {10.1145/3588914},
	timestamp = {Wed, 12 Feb 2025 14:31:22 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangYXZLTG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Counting the number of distinct elements distributed over multiple data holders is a fundamental problem with many real-world applications ranging from crowd counting to network monitoring. Although a number of space and computationally efficient sketch methods (e.g., the Flajolet-Martin sketch and the HyperLogLog sketch) for cardinality estimation have been proposed to solve the above problem, these sketch methods are insecure when considering privacy concerns related to the use of each data holder's personal dataset. Despite a recently proposed protocol that successfully implements the well-known Flajolet-Martin (FM) sketch on a secret-sharing based multiparty computation (MPC) framework for solving the problem of private distributed cardinality estimation (PDCE), we observe that this MPC-FM protocol is not differentially private. In addition, the MPC-FM protocol is computationally expensive, which limits its applications to data holders with limited computation resources. To address the above issues, in this paper we propose a novel protocol DP-DICE, which is computationally efficient and differentially private for solving the problem of PDCE. Experimental results show that our DP-DICE achieves orders of magnitude speedup and reduces the estimation error by several times in comparison with state-of-the-arts under the same security requirements.}
}


@article{DBLP:journals/pacmmod/CaiLZ023,
	author = {Yuzheng Cai and
                  Siyuan Liu and
                  Weiguo Zheng and
                  Xuemin Lin},
	title = {Towards Generating Hop-constrained s-t Simple Path Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {61:1--61:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588915},
	doi = {10.1145/3588915},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CaiLZ023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs have been widely used in real-world applications, in which investigating relations between vertices is an important task. In this paper, we study the problem of generating the k-hop-constrained s-t simple path graph, i.e., the subgraph consisting of all simple paths from vertex s to vertex t of length no larger than k. To our best knowledge, we are the first to formalize this problem and prove its NP-hardness on directed graphs. To tackle this challenging problem, we propose an efficient algorithm namedEVE, which exploits the paradigm of edge-wise examination rather than exhaustively enumerating all paths. Powered by essential vertices appearing in all simple paths between vertex pairs,EVE distinguishes the edges that are definitely (or not) contained in the desired simple path graph, producing a tight upper-bound graph in the time cost O(k2|E|). Each remaining undetermined edge is further verified to deliver the exact answer. Extensive experiments are conducted on 15 real networks. The results show thatEVE significantly outperforms all baselines by several orders of magnitude. Moreover, by takingEVE as a built-in block, state-of-the-art for hop-constrained simple path enumeration can be accelerated by up to an order of magnitude.}
}


@article{DBLP:journals/pacmmod/ZhouPZZRLFCLWHW23,
	author = {Weixing Zhou and
                  Qi Peng and
                  Zijie Zhang and
                  Yanfeng Zhang and
                  Yang Ren and
                  Sihao Li and
                  Guo Fu and
                  Yulong Cui and
                  Qiang Li and
                  Caiyi Wu and
                  Shangjun Han and
                  Shengyi Wang and
                  Guoliang Li and
                  Ge Yu},
	title = {GeoGauss: Strongly Consistent and Light-Coordinated {OLTP} for Geo-Replicated
                  {SQL} Database},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {62:1--62:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588916},
	doi = {10.1145/3588916},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhouPZZRLFCLWHW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multinational enterprises conduct global business that has a demand for geo-distributed transactional databases. Existing state-of-the-art databases adopt a sharded master-follower replication architecture. However, the single-master serving mode incurs massive cross-region writes from clients, and the sharded architecture requires multiple round-trip acknowledgments (e.g., 2PC) to ensure atomicity for cross-shard transactions. These limitations drive us to seek yet another design choice. In this paper, we propose a strongly consistent OLTP database GeoGauss with full replica multi-master architecture. To efficiently merge the updates from different master nodes, we propose a multi-master OCC that unifies data replication and concurrent transaction processing. By leveraging an epoch-based delta state merge rule and the optimistic asynchronous execution, GeoGauss ensures strong consistency with light-coordinated protocol and allows more concurrency with weak isolation, which are sufficient to meet our needs. Our geo-distributed experimental results show that GeoGauss achieves 7.06X higher throughput and 17.41X lower latency than the state-of-the-art geo-distributed database CockroachDB on the TPC-C benchmark.}
}


@article{DBLP:journals/pacmmod/GuFCL0W23,
	author = {Tu Gu and
                  Kaiyu Feng and
                  Gao Cong and
                  Cheng Long and
                  Zheng Wang and
                  Sheng Wang},
	title = {The RLR-Tree: {A} Reinforcement Learning Based R-Tree for Spatial
                  Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {63:1--63:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588917},
	doi = {10.1145/3588917},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GuFCL0W23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learned indexes have been proposed to replace classic index structures like B-Tree with machine learning (ML) models. They require to replace both the indexes and query processing algorithms currently deployed by the databases, and such a radical departure is likely to encounter challenges and obstacles. In contrast, we propose a fundamentally different way of using ML techniques to build a better R-Tree without the need to change the structure or query processing algorithms of traditional R-Tree. Specifically, we develop reinforcement learning (RL) based models to decide how to choose a subtree for insertion and how to split a node when building and updating an R-Tree, instead of relying on hand-crafted heuristic rules currently used by the R-Tree and its variants. Experiments on real and synthetic datasets with up to more than 100 million spatial objects show that our RL based index outperforms the R-Tree and its variants in terms of query processing time.}
}


@article{DBLP:journals/pacmmod/0004COWX23,
	author = {Peng Jia and
                  Shaofeng Cai and
                  Beng Chin Ooi and
                  Pinghui Wang and
                  Yiyuan Xiong},
	title = {Robust and Transferable Log-based Anomaly Detection},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {64:1--64:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588918},
	doi = {10.1145/3588918},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/0004COWX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log messages provide a valuable source of runtime information for ensuring the safety and consistency of systems. Recently, many machine learning and deep learning methods have been proposed to automatically detect anomalous log messages, obviating the need for manual detection by experts. However, we find that in practice, the effectiveness of existing learning-based methods is severely affected by incomplete information and distribution shift. Specifically, each log message can actually be parsed into a fixed number of key information fields, while existing methods analyze log messages using only the log event information and ignore other useful information fields that can be critical to anomaly detection. Further, the distribution of real-world log messages changes continuously due to the dynamic nature of the runtime environment and thus, a detection model conventionally trained based on the unrealistic i.i.d. assumption may not provide the expected and consistent performance. In this paper, we present a robust and transferable anomaly detection framework RT-Log to address the above problems. To perform a comprehensive analysis of log messages, we introduce an adaptive relation modeling technique, which captures feature interactions among log information fields selectively and dynamically for effective and interpretable log representations. To establish its robustness and transferability, we propose a general environment generalization technique for learning the environment invariant representations that can generalize across different runtime environments. We evaluate the anomaly detection performance of RT-Log on large real-world datasets. Extensive experimental results demonstrate that RT-Log consistently outperforms state-of-the-art methods by a significant margin under different settings.}
}


@article{DBLP:journals/pacmmod/BornemannBKNNS23,
	author = {Leon Bornemann and
                  Tobias Bleifu{\ss} and
                  Dmitri V. Kalashnikov and
                  Fatemeh Nargesian and
                  Felix Naumann and
                  Divesh Srivastava},
	title = {Matching Roles from Temporal Data: Why Joe Biden is not only President,
                  but also Commander-in-Chief},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {65:1--65:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588919},
	doi = {10.1145/3588919},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BornemannBKNNS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present role matching, a novel, fine-grained integrity constraint on temporal fact data, i.e., (subject, predicate, object, timestamp)-quadruples. A role is a combination of subject and predicate and can be associated with different objects as the real world evolves and the data changes over time. A role matching states that the associated object of two or more roles should always match across time. Once discovered, role matchings can serve as integrity constraints to improve data quality, for instance of structured data in Wikipedia[3]. If violated, role matchings can alert data owners or editors and thus allow them to correct the error. Finding all role matchings is challenging due both to the inherent quadratic complexity of the matching problem and the need to identify true matches based on the possibly short history of the facts observed so far. To address the first challenge, we introduce several blocking methods both for clean and dirty input data. For the second challenge, the matching stage, we show how the entity resolution method Ditto[27] can be adapted to achieve satisfactory performance for the role matching task. We evaluate our method on datasets from Wikipedia infoboxes, showing that our blocking approaches can achieve 95% recall, while maintaining a reduction ratio of more than 99.99%, even in the presence of dirty data. In the matching stage, we achieve a macro F1-score of 89% on our datasets, using automatically generated labels.}
}


@article{DBLP:journals/pacmmod/TawoseDY023,
	author = {Olamide Timothy Tawose and
                  Jun Dai and
                  Lei Yang and
                  Dongfang Zhao},
	title = {Toward Efficient Homomorphic Encryption for Outsourced Databases through
                  Parallel Caching},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {66:1--66:23},
	year = {2023},
	url = {https://doi.org/10.1145/3588920},
	doi = {10.1145/3588920},
	timestamp = {Sun, 22 Oct 2023 11:16:15 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/TawoseDY023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many applications deployed to public clouds are concerned about the confidentiality of their outsourced data, such as financial services and electronic patient records. A plausible solution to this problem is homomorphic encryption (HE), which supports certain algebraic operations directly over the ciphertexts. The downside of HE schemes is their significant, if not prohibitive, performance overhead for data-intensive workloads that are very common for outsourced databases, or database-as-a-serve in cloud computing. The objective of this work is to mitigate the performance overhead incurred by the HE module in outsourced databases. To that end, this paper proposes a radix-based parallel caching optimization for accelerating the performance of homomorphic encryption (HE) of outsourced databases in cloud computing. The key insight of the proposed optimization is caching selected radix-ciphertexts in parallel without violating existing security guarantees of the primitive/base HE scheme. We design the radix HE algorithm and apply it to both batch- and incremental-HE schemes; we demonstrate the security of those radix-based HE schemes by showing that the problem of breaking them can be reduced to the problem of breaking their base HE schemes that are known IND-CPA (i.e. Indistinguishability under Chosen-Plaintext Attack). We implement the radix-based schemes as middleware of a 10-node Cassandra cluster on CloudLab; experiments on six workloads show that the proposed caching can boost state-of-the-art HE schemes, such as Paillier and Symmetria, by up to five orders of magnitude.}
}


@article{DBLP:journals/pacmmod/ZhuSHA23,
	author = {Yiwen Zhu and
                  Rathijit Sen and
                  Robert Horton and
                  John Mark Agosta},
	title = {Runtime Variation in Big Data Analytics},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {67:1--67:20},
	year = {2023},
	url = {https://doi.org/10.1145/3588921},
	doi = {10.1145/3588921},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhuSHA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The dynamic nature of resource allocation and runtime conditions on Cloud can result in high variability in a job's runtime across multiple iterations, leading to a poor experience. Identifying the sources of such variation and being able to predict and adjust for them is crucial to cloud service providers to design reliable data processing pipelines, provision and allocate resources, adjust pricing services, meet SLOs and debug performance hazards. In this paper, we analyze the runtime variation of millions of production Scope jobs on Cosmos, an exabyte-scale internal analytics platform at Microsoft. We propose an innovative 2-step approach to predict job runtime distribution by characterizing typical distribution shapes combined with a classification model with an average accuracy of >96%, using an innovative interpretable machine-learning algorithm out-performing traditional regression models and better capturing long tails. We examine factors such as job plan characteristics and inputs, resource allocation, physical cluster heterogeneity and utilization, and scheduling policies. To the best of our knowledge, this is the first study on predicting categories of runtime distributions for enterprise analytics workloads at scale. Furthermore, we examine how our methods can be used to analyze what-if scenarios, focusing on the impact of resource allocation, scheduling, and physical cluster provisioning decisions on a job's runtime consistency and predictability.}
}


@article{DBLP:journals/pacmmod/LiaoLDCQW23,
	author = {Meihao Liao and
                  Rong{-}Hua Li and
                  Qiangqiang Dai and
                  Hongyang Chen and
                  Hongchao Qin and
                  Guoren Wang},
	title = {Efficient Resistance Distance Computation: The Power of Landmark-based
                  Approaches},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {68:1--68:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588922},
	doi = {10.1145/3588922},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiaoLDCQW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resistance distance is a fundamental metric to measure the similarity between two nodes in graphs which has been widely used in many real-world applications. In this paper, we study two problems on approximately computing resistance distance: (i) single-pair query which aims at calculating the resistance distance r(s, t) for a given pair of nodes (s, t); and (ii) single-source query which is to compute all the resistance distances r(s, u) for all nodes u in the graph with a given source node s. Existing algorithms for these two resistance distance query problems are often costly on large graphs. To efficiently solve these problems, we first establish several interesting connections among resistance distance, a new concept called v-absorbed random walk, random spanning forests, and a newly-developed v-absorbed push procedure. Based on such new connections, we propose three novel and efficient sampling-based algorithms as well as a deterministic algorithm for single-pair query; and we develop an online and two index-based approximation algorithms for single-source query. We show that the two index-based algorithms for single-source query take almost the same running time as the algorithms for single-pair query with the aid of a linear-size index. The striking feature of all our algorithms is that they are allowed to select an easy-to-hit node by random walks on the graph. Such an easy-to-hit landmark node v can make the v-absorbed random walk sampling, spanning tree sampling, as well as the v-absorbed push more efficient, thus significantly improving the performance of our algorithms. Extensive experiments on 5 real-life datasets show that our algorithms substantially outperform the state-of-the-art algorithms for two resistance distance query problems in terms of both running time and estimation errors.}
}


@article{DBLP:journals/pacmmod/HeW00023,
	author = {Yizhang He and
                  Kai Wang and
                  Wenjie Zhang and
                  Xuemin Lin and
                  Ying Zhang},
	title = {Scaling Up k-Clique Densest Subgraph Detection},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {69:1--69:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588923},
	doi = {10.1145/3588923},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HeW00023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the k-clique densest subgraph problem, which detects the subgraph that maximizes the ratio between the number of k-cliques and the number of vertices in it. The problem has been extensively studied in the literature and has many applications in a wide range of fields such as biology and finance. Existing solutions rely heavily on repeatedly computing all the k-cliques, which are not scalable to handle large k values on large-scale graphs. In this paper, by adapting the idea of "pivoting", we propose the SCT*-Index to compactly organize the k-cliques. Based on the SCT*-Index, our SCTL algorithm can directly obtain the k-cliques from the index and efficiently achieve near-optimal approximation. To further improve SCTL, we propose SCTL* that includes novel graph reductions and batch-processing optimizations to reduce the search space and decrease the number of visited k-cliques, respectively. As evaluated in our experiments, SCTL* significantly outperform existing approaches by up to two orders of magnitude. In addition, we propose a sampling-based approximate algorithm that can provide reasonable approximations for any k value on billion-scale graphs. Extensive experiments on 12 real-world graphs validate both the efficiency and effectiveness of the proposed techniques.}
}


@article{DBLP:journals/pacmmod/FanHWX23,
	author = {Wenfei Fan and
                  Ziyan Han and
                  Yaoshu Wang and
                  Min Xie},
	title = {Discovering Top-k Rules using Subjective and Objective Criteria},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {70:1--70:29},
	year = {2023},
	url = {https://doi.org/10.1145/3588924},
	doi = {10.1145/3588924},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanHWX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies two questions about rule discovery. Can we characterize the usefulness of rules using quantitative criteria? How can we discover rules using those criteria? As a testbed, we consider entity enhancing rules (REEs), which subsume common association rules and data quality rules as special cases. We characterize REEs using a bi-criteria model, with both objective measures such as support and confidence, and subjective measures for the user's needs; we learn the subjective measure and the weight vectors via active learning. Based on the bi-criteria model, we develop a top-k algorithm to discover top-ranked REEs, and an any-time algorithm for successive discovery via lazy evaluation. We parallelize these algorithms such that they guarantee to reduce runtime when more processors are used. Using real-life and synthetic datasets, we show that the algorithms are able to find top-ranked rules and speed up conventional rule-discovery methods by 134X on average.}
}


@article{DBLP:journals/pacmmod/ThielKAHMS23,
	author = {Konstantin Emil Thiel and
                  Daniel Kocher and
                  Nikolaus Augsten and
                  Thomas H{\"{u}}tter and
                  Willi Mann and
                  Daniel Ulrich Schmitt},
	title = {{FINEX:} {A} Fast Index for Exact {\&} Flexible Density-Based
                  Clustering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {71:1--71:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588925},
	doi = {10.1145/3588925},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ThielKAHMS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Density-based clustering aims to find groups of similar objects (i.e., clusters) in a given dataset. Applications include, e.g., process mining and anomaly detection. It comes with two user parameters (ε, MinPts) that determine the clustering result, but are typically unknown in advance. Thus, users need to interactively test various settings until satisfying clusterings are found. However, existing solutions suffer from the following limitations: (a) Ineffective pruning of expensive neighborhood computations. (b) Approximate clustering, where objects are falsely labeled noise. (c) Restricted parameter tuning that is limited to ε whereas MinPts is constant, which reduces the explorable clusterings. (d) Inflexibility in terms of applicable data types and distance functions. We propose FINEX, a linear-space index that overcomes these limitations. Our index provides exact clusterings and can be queried with either of the two parameters. FINEX avoids neighborhood computations where possible and reduces the complexities of the remaining computations by leveraging fundamental properties of density-based clusters. Hence, our solution is efficient and flexible regarding data types and distance functions. Moreover, FINEX respects the original and straightforward notion of density-based clustering. In our experiments on 12 large real-world datasets from various domains, FINEX frequently outperforms state-of-the-art techniques for exact clustering by orders of magnitude.}
}


@article{DBLP:journals/pacmmod/HuangWZTL023,
	author = {Shiyue Huang and
                  Ziwei Wang and
                  Xinyi Zhang and
                  Yaofeng Tu and
                  Zhongliang Li and
                  Bin Cui},
	title = {{DBPA:} {A} Benchmark for Transactional Database Performance Anomalies},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {72:1--72:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588926},
	doi = {10.1145/3588926},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangWZTL023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly diagnosis is vital to the performance of online transaction processing (OLTP) systems. In the meanwhile, machine learning techniques can reason complex relationships beyond human abilities and perform well on such problems. However, they rely on a large number of training samples for anomalies, which are in serious shortage in both industry and academia due to the difficulty of collection. The problem raises the demand of a benchmark for anomaly reproduction and data collection. In this paper, we propose DBPA, a benchmark for transactional database performance anomalies. Specifically, we identify nine common anomalies rooted in the diverse influence factors. For each anomaly, we carefully design a reproduction procedure, which consists with its root cause in real-world databases. With the reproduction procedures, users can easily generate a dataset in a new environment and extend new anomaly types. For compound anomalies, we provide a generation algorithm that allows users to generate compound anomalies data of any possible combinations with existing collected data. We also provide a large dataset of both normal and anomalous monitoring data collected from various environments, facilitating the training of machine learning models and the evaluation of new algorithms for anomaly diagnosis.}
}


@article{DBLP:journals/pacmmod/HaffnerD23,
	author = {Immanuel Haffner and
                  Jens Dittrich},
	title = {Efficiently Computing Join Orders with Heuristic Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {73:1--73:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588927},
	doi = {10.1145/3588927},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HaffnerD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Join order optimization is one of the most fundamental problems in processing queries on relational data. It has been studied extensively for almost four decades now. Still, because of its NP hardness, no generally efficient solution exists and the problem remains an important topic of research. The scope of algorithms to compute join orders ranges from exhaustive enumeration, to combinatorics based on graph properties, to greedy search, to genetic algorithms, to recently investigated machine learning. A few works exist that use heuristic search to compute join orders. However, a theoretical argument why and how heuristic search is applicable to join order optimization is lacking. In this work, we investigate join order optimization via heuristic search. In particular, we provide a strong theoretical framework, in which we reduce join order optimization to the shortest path problem. We then thoroughly analyze the properties of this problem and the applicability of heuristic search. We devise crucial optimizations to make heuristic search tractable. We implement join ordering via heuristic search in a real DBMS and conduct an extensive empirical study. Our findings show that for star- and clique-shaped queries, heuristic search finds optimal plans an order of magnitude faster than current state of the art. Our suboptimal solutions further extend the cost/time Pareto frontier.}
}


@article{DBLP:journals/pacmmod/Yuan0QAKLW23,
	author = {Lyuheng Yuan and
                  Da Yan and
                  Wenwen Qu and
                  Saugat Adhikari and
                  Jalal Khalil and
                  Cheng Long and
                  Xiaoling Wang},
	title = {{T-FSM:} {A} Task-Based System for Massively Parallel Frequent Subgraph
                  Pattern Mining from a Big Graph},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {74:1--74:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588928},
	doi = {10.1145/3588928},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Yuan0QAKLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding frequent subgraph patterns in a big graph is an important problem with many applications such as classifying chemical compounds and building indexes to speed up graph queries. Since this problem is NP-hard, some recent parallel systems have been developed to accelerate the mining. However, they often have a huge memory cost, very long running time, suboptimal load balancing, and possibly inaccurate results. In this paper, we propose an efficient system called T-FSM for parallel mining of frequent subgraph patterns in a big graph. T-FSM adopts a novel task-based execution engine design to ensure high concurrency, bounded memory consumption, and effective load balancing. It also supports a new anti-monotonic frequentness measure called Fraction-Score, which is more accurate than the widely used MNI measure. Our experiments show that T-FSM is orders of magnitude faster than SOTA systems for frequent subgraph pattern mining. Our system code has been released at https://github.com/lyuheng/T-FSM.}
}


@article{DBLP:journals/pacmmod/KaminskyPN23,
	author = {Youri Kaminsky and
                  Eduardo H. M. Pena and
                  Felix Naumann},
	title = {Discovering Similarity Inclusion Dependencies},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {75:1--75:24},
	year = {2023},
	url = {https://doi.org/10.1145/3588929},
	doi = {10.1145/3588929},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KaminskyPN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inclusion dependencies (INDs) are a well-known type of data dependency, specifying that the values of one column are contained in those of another column. INDs can be used for various purposes, such as foreign-key candidate selection or join partner discovery. The traditional notion of INDs is based on clean data, where the dependencies hold without exceptions. Unfortunately, data often contain errors, preventing otherwise valid INDs from being discovered. A typical response to this problem is to relax the dependency definition using a similarity measure to account for minor data errors, such as typos or different formatting. While this relaxation is known for functional dependencies, for inclusion dependencies no such relaxation has been defined. We formally introduce similarity inclusion dependencies, which relax the inclusion by demanding the existence only of sufficiently similar values. Similarity inclusion dependencies can fulfill traditional IND use cases, such as foreign-key candidate discovery, even in the presence of dirty data. We present Sawfish, the first algorithm to discover all similarity inclusion dependencies in a given dataset efficiently. Our algorithm combines approaches for the discovery of traditional INDs and string similarity joins with a novel sliding-window approach and lazy candidate validation. Our experimental evaluation shows that Sawfish can outperform a baseline by a factor of up to 6.5.}
}


@article{DBLP:journals/pacmmod/0004Y0Y023,
	author = {Shiqi Zhang and
                  Renchi Yang and
                  Xiaokui Xiao and
                  Xiao Yan and
                  Bo Tang},
	title = {Effective and Efficient PageRank-based Positioning for Graph Visualization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {76:1--76:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588930},
	doi = {10.1145/3588930},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0004Y0Y023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph visualization is a vital component in many real-world applications (e.g., social network analysis, web mining, and bioinformatics) that enables users to unearth crucial insights from complex data. Lying in the core of graph visualization is the node distance measure, which determines how the nodes are placed on the screen. A favorable node distance measure should be informative in reflecting the full structural information between nodes and effective in optimizing visual aesthetics. However, existing node distance measures yield sub-par visualization quality as they fall short of these requirements. Moreover, most existing measures are computationally inefficient, incurring a long response time when visualizing large graphs. To overcome such deficiencies, we propose a new node distance measure, PDist, geared towards graph visualization by exploiting a well-known node proximity measure,personalized PageRank. Moreover, we propose an efficient algorithm Tau-Push for estimating PDist under both single- and multi-level visualization settings. With several carefully-designed techniques, TauPush offers non-trivial theoretical guarantees for estimation accuracy and computation complexity. Extensive experiments show that our proposal significantly outperforms 13 state-of-the-art graph visualization solutions on 12 real-world graphs in terms of both efficiency and effectiveness (including aesthetic criteria and user feedback). In particular, our proposal can interactively produce satisfactory visualizations within one second for billion-edge graphs.}
}


@article{DBLP:journals/pacmmod/DaiLLW23,
	author = {Qiangqiang Dai and
                  Rong{-}Hua Li and
                  Meihao Liao and
                  Guoren Wang},
	title = {Maximal Defective Clique Enumeration},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {77:1--77:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588931},
	doi = {10.1145/3588931},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DaiLLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Maximal clique enumeration is a fundamental operator in graph analysis. The model of clique, however, is typically too restrictive for real-world applications as it requires an edge for every pair of vertices. To remedy this restriction, practical graph analysis applications often resort to find relaxed cliques as alternatives. In this work, we investigate a notable relaxed clique model, called s-defective clique, which allows at most s edges to be missing. Similar to the complexity of maximal clique enumeration, the problem of enumerating all maximal s-defective cliques is also NP-hard. To solve this problem, we first develop a new polynomial-delay algorithm based on a carefully-designed reverse search technique, which can output two consecutive results within polynomial time. To achieve better practical efficiency, we propose a branch-and-bound algorithm with a novel pivoting technique. We prove that the time complexity of this algorithm depends only on O(α_sn) or O(αsδ) when using a degeneracy ordering optimization, where αs is a positive real number strictly less than 2, and δ (δ <n) is the degeneracy of the graph. To our knowledge, this is the first algorithm that can break the O(2n) time complexity to enumerate all maximal s-defective cliques (s>0). We also develop several new pruning techniques to further improve the efficiency of our branch-and-bound algorithm to enumerate all relatively-large maximal s-defective cliques. In addition, we further generalize our pivot-based branch-and-bound algorithm to enumerate all maximal subgraphs satisfying a hereditary property. Here we call a graph meeting the hereditary property if all its subgraphs have the same property as itself. Finally, extensive experiments on 11 datasets demonstrate the efficiency, effectiveness, and scalability of the proposed solutions.}
}


@article{DBLP:journals/pacmmod/YeLDQW23,
	author = {Xiaowei Ye and
                  Rong{-}Hua Li and
                  Qiangqiang Dai and
                  Hongchao Qin and
                  Guoren Wang},
	title = {Efficient Biclique Counting in Large Bipartite Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {78:1--78:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588932},
	doi = {10.1145/3588932},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YeLDQW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A (p,q)-biclique is a complete subgraph (X,Y) that |X|=p, |Y|=q. Counting (p,q)-bicliques in bipartite graphs is an important operator for many bipartite graph analysis applications. However, getting the count of (p,q)-bicliques for large p and q (e.g., p,q ≥ 10) is extremely difficult, because the number of (p,q)-bicliques increases exponentially with respect to p and q. The state-of-the-art algorithm for this problem is based on the (p,q)-biclique enumeration technique which is often costly due to the exponential blowup in the enumeration space of (p,q)-bicliques. To overcome this problem, we first propose a novel exact algorithm, called EPivoter, based on a newly-developed edge-pivoting technique. The striking feature of EPivoter is that it can count (p,q)-bicliques for all pairs of (p,q) using a combinatorial technique, instead of exhaustively enumerating all (p,q)-bicliques. Second, we propose a novel dynamic programming (DP) based h-zigzag sampling technique to provably approximate the count of the (p,q)-bicliques for all pairs of (p,q), where an h-zigzag is an ordered simple path in G with length 2h-1 (h = min{p,q}). We show that our DP-based sampling technique is very efficient. Third, to further improve the efficiency, we also propose a hybrid framework that integrates both the exact EPivoter algorithm and sampling-based algorithms. Extensive experiments on 7 real-world graphs show that our algorithms are several orders of magnitude faster than the state-of-the-art algorithm.}
}


@article{DBLP:journals/pacmmod/ZhaoHZ00023,
	author = {Yikai Zhao and
                  Wenchen Han and
                  Zheng Zhong and
                  Yinda Zhang and
                  Tong Yang and
                  Bin Cui},
	title = {Double-Anonymous Sketch: Achieving Top-K-fairness for Finding Global
                  Top-K Frequent Items},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {79:1--79:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588933},
	doi = {10.1145/3588933},
	timestamp = {Thu, 27 Mar 2025 18:54:49 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhaoHZ00023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding top-K frequent items has been a hot topic in data stream processing in recent years, which has a wide range of applications. However, most of existing sketch algorithms focuses on finding local top-K in a single data stream. In this paper, we work on finding global top-K in multiple disjoint data streams. We find that directly deploying prior sketch algorithms is often unfair under global scenarios, which will degrade the accuracy of global top-K. We define top-K-fairness and show that it is important for finding global top-K. To achieve top-K-fairness, we propose a new sketch framework, called the Double-Anonymous sketch. The process of finding global top-K items is similar to that of paper reviewing and democratic elections. In these scenarios, double-anonymity is often an effective strategy to achieve top-K-fairness. We also propose two techniques, hot panning, and early freezing, to further improve the accuracy. We theoretically prove that the Double-Anonymous sketch achieves top-K-fairnesswhile keeping high accuracy. We perform extensive experiments to verify top-K-fairness in the scenario of disjoint data streams. The experimental results show that the Double-Anonymous sketch's error is up to 129 times (60 times on average) smaller than the state-of-the-art. All the related source code is open-sourced and available at Github.}
}


@article{DBLP:journals/pacmmod/HuangGBL23,
	author = {Shixun Huang and
                  Junhao Gan and
                  Zhifeng Bao and
                  Wenqing Lin},
	title = {Managing Conflicting Interests of Stakeholders in Influencer Marketing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {80:1--80:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588934},
	doi = {10.1145/3588934},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangGBL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A successful campaign should be able to attract investment from the brand, and meanwhile manage the conflicting interests in the campaign cost between the brand and the influencers. As such, the agency between these two stakeholders plays a vital role. Motivated by the above, we stand in the agency's shoes to formulate an interesting yet practical problem, namely Profit Divergence Minimization in Investment-Persuasive Influencer Marketing Campaign (PDMIC). This problem aims to (i) minimize the divergence of the actual hiring prices from the asking prices of the influencers and meanwhile (ii) maintain the attractiveness of the pricing scheme for the influencers to the brand. We show that this problem is NP-hard. To mitigate the challenge of the extremely large searching space of the hiring prices of the influencers, we solve this problem by firstly considering a restrictive searching sub-space and then gradually expanding the searching sub-space to the whole space in the end (specifically, from binary price choices to a set of integer prices and then to any price in the feasible price range). We propose effective yet efficient approximate algorithms for solving the problem in each of these settings. Extensive experiments demonstrate the superiority of our methods.}
}


@article{DBLP:journals/pacmmod/WangCLYTY023,
	author = {Feiyu Wang and
                  Qizhi Chen and
                  Yuanpeng Li and
                  Tong Yang and
                  Yaofeng Tu and
                  Lian Yu and
                  Bin Cui},
	title = {JoinSketch: {A} Sketch Algorithm for Accurate and Unbiased Inner-Product
                  Estimation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {81:1--81:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588935},
	doi = {10.1145/3588935},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangCLYTY023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inner-product estimation is the base of many important tasks in a variety of big data scenarios, including measuring similarity of streams in data stream processing, estimating join size in database, and analyzing cosine similarity in various applications. Sketch, as a class of probability algorithms, is promising in inner-product estimation. However, existing sketch solutions suffer from low accuracy due to their neglect of the high skewness of real data. In this paper, we design a new sketch algorithm for accurate and unbiased inner-product estimation, namely JoinSketch. To improve accuracy, JoinSketch consists of multiple components, and records items with different frequency in different components. We theoretically prove that JoinSketch is unbiased, and has lower variance compared with the well-known AGMS and Fast-AGMS sketch. The experimental results show that JoinSketch improves the accuracy by 10 times in average while maintaining a comparable speed. All code is open-sourced at Github.}
}


@article{DBLP:journals/pacmmod/LuoCWO23,
	author = {Zhaojing Luo and
                  Shaofeng Cai and
                  Yatong Wang and
                  Beng Chin Ooi},
	title = {Regularized Pairwise Relationship based Analytics for Structured Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {82:1--82:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588936},
	doi = {10.1145/3588936},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuoCWO23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In line with the increasing machine learning model inference accuracy, deep learning (DL) models have been increasingly applied to structured data for a wide spectrum of real-world applications, including product recommendations, online advertisement, healthcare analytics and risk analysis. However, unlike unstructured data, structured data is high-dimensional and sparse and therefore engenders a large number of parameters in DL, making DL models more prone to overfitting. To alleviate the overfitting problem, various regularization methods have been designed to constrain the model parameters as a means to control the model complexity. Unfortunately, these methods are often restricted to regularizing the parameter values directly without considering the intrinsic correlations and dependencies between attribute fields of structured data which is however key to effective structured data modeling. In this paper, we re-examine DL for structured data from a new perspective of attribute interactions. In particular, we seek to explicitly model and regularize the pairwise relationships between attribute fields of structured data, in a field-adaptive manner, via a proposed attentive and interpretable framework called ATT-Reg. Specifically, in this framework, a set of attentive weight matrices are introduced to each attribute field for modeling obviously different relationships with its neighboring attribute fields. Further, we derive from the Bayesian viewpoint a novel Attentive Regularization method for imposing adaptive regularization strengths on different pairs of attribute fields, based on the informativeness of their relationship, which is calculated using both data-driven information and functional dependency (FD) knowledge. Such adaptive regularization facilitates each attribute field to learn discriminative and diversified representations for more effective predictive analytics. We also develop a feature attribution method for supporting more interpretable predictions We validate the effectiveness of our ATT-Reg on six real-world datasets. Extensive experimental results show that ATT-Reg achieves significant improvement over state-of-the-art graph models, attentive models as well as regularization methods and supports an excellent degree of interpretation.}
}


@article{DBLP:journals/pacmmod/ShahoutFB23,
	author = {Rana Shahout and
                  Roy Friedman and
                  Ran Ben Basat},
	title = {Together is Better: Heavy Hitters Quantile Estimation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {83:1--83:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588937},
	doi = {10.1145/3588937},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShahoutFB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stream monitoring is fundamental in many data stream applications, such as financial data trackers, security, anomaly detection, and load balancing. In that respect, quantiles are of particular interest, as they often capture the user's utility. For example, if a video connection has high tail (e.g., 99'th percentile) latency, the perceived quality will suffer, even if the average and median latencies are low. In this work, we consider the problem of approximating the per-item quantiles. Elements in our stream are (ID, value) tuples, and we wish to track the quantiles for each ID. Existing quantile sketches are designed for a plain number stream (i.e., containing just a value). While one could allocate a separate sketch instance for each ID, this may require an infeasible amount of memory. Instead, we consider tracking the quantiles for the heavy hitters (most frequent items), which are often considered particularly important, without knowing them beforehand. We first present a couple of simple and effective algorithms that serve as baselines, a sampling approach and a sketching approach. Then, we present SQUAD, an algorithm that combines sampling and sketching while improving the asymptotic space complexity. Intuitively, SQUAD uses a background sampling process to capture the behaviour of the quantiles of an item before it is allocated with a sketch, thereby allowing us to use fewer samples and sketches. The algorithms are rigorously analyzed, and we demonstrate SQUAD's superiority using extensive~simulations on real-world traces.}
}


@article{DBLP:journals/pacmmod/TuFTWL0JG23,
	author = {Jianhong Tu and
                  Ju Fan and
                  Nan Tang and
                  Peng Wang and
                  Guoliang Li and
                  Xiaoyong Du and
                  Xiaofeng Jia and
                  Song Gao},
	title = {Unicorn: {A} Unified Multi-tasking Model for Supporting Matching Tasks
                  in Data Integration},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {84:1--84:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588938},
	doi = {10.1145/3588938},
	timestamp = {Mon, 19 Jun 2023 16:36:09 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/TuFTWL0JG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data matching - which decides whether two data elements (e.g., string, tuple, column, or knowledge graph entity) are the "same" (a.k.a. a match) - is a key concept in data integration, such as entity matching and schema matching. The widely used practice is to build task-specific or even dataset-specific solutions, which are hard to generalize and disable the opportunities of knowledge sharing that can be learned from different datasets and multiple tasks. In this paper, we propose Unicorn, a unified model for generally supporting common data matching tasks. Unicorn can enable knowledge sharing by learning from multiple tasks and multiple datasets, and can also support zero-shot prediction for new tasks with zero labeled matching/non-matching pairs. However, building such a unified model is challenging due to heterogeneous formats of input data elements and various matching semantics of multiple tasks. To address the challenges, Unicorn employs one generic Encoder that converts any pair of data elements (a, b) into a learned representation, and uses a Matcher, which is a binary classifier, to decide whether a matches b. To align matching semantics of multiple tasks, Unicorn adopts a mixture-of-experts model that enhances the learned representation into a better representation. We conduct extensive experiments using 20 datasets on seven well-studied data matching tasks, and find that our unified model can achieve better performance on most tasks and on average, compared with the state-of-the-art specific models trained for ad-hoc tasks and datasets separately. Moreover, Unicorn can also well serve new matching tasks with zero-shot learning.}
}


@article{DBLP:journals/pacmmod/SuGS23,
	author = {Yunxiang Su and
                  Yikun Gong and
                  Shaoxu Song},
	title = {Time Series Data Validity},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {85:1--85:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588939},
	doi = {10.1145/3588939},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SuGS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a key step of data preparation, it is always necessary to first assert the quality of data before conducting any data application. Given a set of constraints, the validity measure evaluates the degree of data meeting the constraints, e.g., whether the values are in the specified range or fluctuate drastically over time in a series. It is worth noting that simply counting all the data points in violation to the constraints may over claim the data validity issue. Following the minimum change criteria in data repairing, we propose to study the minimum number of data points that need to be changed in order to satisfy the constraints, or equivalently, the maximum rate of data that can be reserved without change, as the validity measure. To our best knowledge, this is the first study on defining and evaluating time series data validity. We devise algorithms for computing the validity measure in quadratic time and linear space. Remarkably, the validity measure has been deployed and included as a function in SQL statements, in Apache IoTDB, an open-source time series database. The algorithm fully adapts to the LSM-based storage of time series in multiple segments. Extensive experiments over 8 real-world datasets show up to 4 orders of magnitude improvement in time cost compared to the related method SCREEN.}
}


@article{DBLP:journals/pacmmod/FanFJLL023,
	author = {Wenfei Fan and
                  Wenzhi Fu and
                  Ruochun Jin and
                  Muyang Liu and
                  Ping Lu and
                  Chao Tian},
	title = {Making It Tractable to Catch Duplicates and Conflicts in Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {86:1--86:28},
	year = {2023},
	url = {https://doi.org/10.1145/3588940},
	doi = {10.1145/3588940},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanFJLL023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes an approach for entity resolution (ER) and conflict resolution (CR) in large-scale graphs. It is based on a class of Graph Cleaning Rules (GCRs), which support the primitives of relational data cleaning rules, and may embed machine learning classifiers as predicates. As opposed to previous graph rules, GCRs are defined with a dual graph pattern to accommodate irregular structures of schemaless graphs, and adopt patterns of a star form to reduce the complexity. We show that the satisfiability, implication and validation problems are all in polynomial time (PTIME) for GCRs, as opposed to the intractability of these classical problems for previous graph dependencies. We develop a parallel algorithm to discover GCRs by combining the generations of patterns and predicates, and a parallel PTIME algorithm for "deep" ER and CR by recursively applying the mined GCRs. We show that these algorithms guarantee to reduce runtime when more processors are used. Using real-life and synthetic graphs, we experimentally verify that rule discovery and error detection with GCRs are substantially faster than with previous graph dependencies, with improved accuracy.}
}


@article{DBLP:journals/pacmmod/LiuT0WH23,
	author = {Kaiqi Liu and
                  Panrong Tong and
                  Mo Li and
                  Yue Wu and
                  Jianqiang Huang},
	title = {{ST4ML:} Machine Learning Oriented Spatio-Temporal Data Processing
                  at Scale},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {87:1--87:28},
	year = {2023},
	url = {https://doi.org/10.1145/3588941},
	doi = {10.1145/3588941},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuT0WH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data scientists and researchers utilize enormous spatio-temporal data and build machine learning models to solve practical problems in diverse domains including intelligent transportation, urban planning, epidemic prediction, and many more. Extracting application-specific features from big spatio-temporal data poses system requirements of heterogeneous data support, efficient and scalable computing over spatial and temporal dimensions, as well as a user-friendly programming interface. This paper presents ST4ML, a distributed spatio-temporal data processing system to support scalable machine-learning-oriented applications. We propose a three-stage pipelining computing framework, namely "selection-conversion-extraction" to abstract the distributed computing flow and implement it based on Apache Spark. To the best of our knowledge, ST4ML is the first of its kind to realize our design considerations. Extensive experiments with real-world datasets evidence that ST4ML outperforms straightforward extensions of existing ST data processing systems by up to an order of magnitude. ST4ML is open-sourced at https://github.com/Panrong/st4ml.}
}


@article{DBLP:journals/pacmmod/LuoZ00CS23,
	author = {Yuyu Luo and
                  Yihui Zhou and
                  Nan Tang and
                  Guoliang Li and
                  Chengliang Chai and
                  Leixian Shen},
	title = {Learned Data-aware Image Representations of Line Charts for Similarity
                  Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {88:1--88:29},
	year = {2023},
	url = {https://doi.org/10.1145/3588942},
	doi = {10.1145/3588942},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuoZ00CS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding line-chart images similar to a given line-chart image query is a common task in data exploration and image query systems, e.g. finding similar trends in stock markets or medical Electroencephalography images. The state-of-the-art approaches consider either data-level similarity (when the underlying data is present) or image-level similarity (when the underlying data is absent). In this paper, we study the scenario that during query time, only line-chart images are available. Our goal is to train a neural network that can turn these line-chart images into representations that are aware of the data used to generate these line charts, so as to learn better representations. Our key idea is that we can collect both data and line-chart images to learn such a neural network (at training step), while during query (or inference) time, we support the case that only line-chart images are provided. To this end, we present LineNet, a Vision Transformer-based Triplet Autoencoder model to learn data-aware image representations of line charts for similarity search. We design a novel pseudo labels selection mechanism to guide LineNet to capture both data-aware and image-level similarity of line charts. We further propose a diversified training samples selection strategy to optimize the learning process and improve the performance. We conduct both quantitative evaluation and case studies, showing that LineNet significantly outperforms the state-of-the-art methods for searching similar line-chart images.}
}


@article{DBLP:journals/pacmmod/DarH023,
	author = {Chen Dar and
                  Moshik Hershcovitch and
                  Adam Morrison},
	title = {{RLS} Side Channels: Investigating Leakage of Row-Level Security Protected
                  Data Through Query Execution Time},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {89:1--89:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588943},
	doi = {10.1145/3588943},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DarH023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many modern use cases of relational databases involve multi-tenancy. To allow a tenant to only access its data, relational database systems (RDBMSs) introduced row-level security (RLS). RLS enables specifying per-row access controls, which the database enforces by rewriting tenant queries to add an RLS policy filter that filters out rows the tenant is not allowed to view. Unfortunately, while RLS blocks queries from returning unauthorized data, side-effects of query execution can form a side-channel that leaks information about such secret data. This paper investigates how RLS query execution time can leak information about rows that the querying tenant is restricted from viewing. We show that in PostgreSQL and SQL Server, an attacker can craft index-using queries to learn whether a value they are not authorized to view exists in an RLS-protected table, and in some cases, how many times such a value exists in the table. Our attack succeeds in a realistic cloud setting: we successfully attack managed PostgreSQL and SQL Server database instances on AWS from virtual machines in the same and different data centers. To block the RLS time side-channel, we design a data-oblivious query scheme for the case of unique keys. We also analyze the trade-offs created by the data-oblivious approach for non-unique keys. To facilitate the evaluation of RLS attacks and defenses, we introduce a benchmark that supports multi-tenancy and RLS, which are not supported by established benchmarks such as YCSB. We implement our solution in PostgreSQL and show that it achieves security with minimal performance impact.}
}


@article{DBLP:journals/pacmmod/TanCCHW23,
	author = {Hongshi Tan and
                  Xinyu Chen and
                  Yao Chen and
                  Bingsheng He and
                  Weng{-}Fai Wong},
	title = {LightRW: {FPGA} Accelerated Graph Dynamic Random Walks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {90:1--90:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588944},
	doi = {10.1145/3588944},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/TanCCHW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph dynamic random walks (GDRWs) have recently emerged as a powerful paradigm for graph analytics and learning applications, including graph embedding and graph neural networks. Despite the fact that many existing studies optimize the performance of GDRWs on multi-core CPUs, massive random memory accesses and costly synchronizations cause severe resource underutilization, and the processing of GDRWs is usually the key performance bottleneck in many graph applications. This paper studies an alternative architecture, FPGA, to address these issues in GDRWs, as FPGA has the ability of hardware customization so that we are able to explore fine-grained pipeline execution and specialized memory access optimizations. Specifically, we propose LightRW, a novel FPGA-based accelerator for GDRWs. LightRW embraces a series of optimizations to enable fine-grained pipeline execution on the chip and to exploit the massive parallelism of FPGA while significantly reducing memory accesses. As current commonly used sampling methods in GDRWs do not efficiently support fine-grained pipeline execution, we develop a parallelized reservoir sampling method to sample multiple vertices per cycle for efficient pipeline execution. To address the random memory access issues, we propose a degree-aware configurable caching method that buffers hot vertices on-chip to alleviate random memory accesses and a dynamic burst access engine that efficiently retrieves neighbors. Experimental results show that our optimization techniques are able to improve the performance of GDRWs on FPGA significantly. Moreover, LightRW delivers up to 9.55x and 9.10x speedup over the state-of-the-art CPU-based MetaPath and Node2vec random walks, respectively. This work is open-sourced on GitHub at https://github.com/Xtra-Computing/LightRW.}
}


@article{DBLP:journals/pacmmod/Chen0FYCL023,
	author = {Sibei Chen and
                  Nan Tang and
                  Ju Fan and
                  Xuemi Yan and
                  Chengliang Chai and
                  Guoliang Li and
                  Xiaoyong Du},
	title = {HAIPipe: Combining Human-generated and Machine-generated Pipelines
                  for Data Preparation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {91:1--91:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588945},
	doi = {10.1145/3588945},
	timestamp = {Mon, 19 Jun 2023 16:36:09 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Chen0FYCL023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data preparation is crucial in achieving optimized results for machine learning (ML). However, having a good data preparation pipeline is highly non-trivial for ML practitioners, which is not only domain-specific, but also dataset-specific. There are two common practices. Human-generated pipelines (HI-pipelines) typically use a wide range of any operations or libraries but are highly experience- and heuristic-based. In contrast, machine-generated pipelines (AI-pipelines), a.k.a. AutoML, often adopt a predefined set of sophisticated operations and are search-based and optimized. These two common practices are mutually complementary. In this paper, we study a new problem that, given an HI-pipeline and an AI-pipeline for the same ML task, can we combine them to get a new pipeline (HAI-pipeline) that is better than the provided HI-pipeline and AI-pipeline? We propose HAIPipe, a framework to address the problem, which adopts an enumeration-sampling strategy to carefully select the best performing combined pipeline. We also introduce a reinforcement learning (RL) based approach to search an optimized AI-pipeline. Extensive experiments using 1400+ real-world HI-pipelines (Jupyter notebooks from Kaggle) verify that HAIPipe can significantly outperform the approaches using either HI-pipelines or AI-pipelines alone.}
}


@article{DBLP:journals/pacmmod/SchonbergerSM23,
	author = {Manuel Sch{\"{o}}nberger and
                  Stefanie Scherzinger and
                  Wolfgang Mauerer},
	title = {Ready to Leap (by Co-Design)? Join Order Optimisation on Quantum Hardware},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {92:1--92:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588946},
	doi = {10.1145/3588946},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SchonbergerSM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prospect of achieving computational speedups by exploiting quantum phenomena makes the use of quantum processing units (QPUs) attractive for many algorithmic database problems. Query optimisation, which concerns problems that typically need to explore large search spaces, seems like an ideal match for quantum algorithms. We present the first quantum implementation of join ordering, one of the most investigated and fundamental query optimisation problems, based on a reformulation to quadratic binary unconstrained optimisation problems. We empirically characterise our method on two state-of-the-art approaches (gate-based quantum computing and quantum annealing), and identify speed-ups compared to the best know classical join ordering approaches for input sizes conforming to current quantum annealers. Yet, we also confirm that limits of early-stage technology are quickly reached. Current QPUs are classified as noisy, intermediate scale quantum computers (NISQ), and are restricted by a variety of limitations that reduce their capabilities as compared to ideal future QPUs, which prevents us from scaling up problem dimensions and reaching practical utility. To overcome these challenges, our formulation accounts for specific QPU properties and limitations, and allows us to trade between achievable solution quality and problem size. In contrast to all prior work on quantum computing for query optimisation and database-related challenges, we go beyond currently available QPUs, and explicitly target the scalability limitations: Using insights gained from numerical simulations and our experimental analysis, we identify key criteria for co-designing QPUs to improve their usefulness for join ordering, and show how even relatively minor physical architectural improvements can result in substantial enhancements. Finally, we outline a path towards practical utility of custom-designed QPUs.}
}


@article{DBLP:journals/pacmmod/Balsebre0CHH23,
	author = {Pasquale Balsebre and
                  Dezhong Yao and
                  Gao Cong and
                  Weiming Huang and
                  Zhen Hai},
	title = {Mining Geospatial Relationships from Text},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {93:1--93:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588947},
	doi = {10.1145/3588947},
	timestamp = {Fri, 07 Feb 2025 10:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Balsebre0CHH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A geospatial Knowledge Graph (KG) is a heterogeneous information network, capable of representing relationships between spatial entities in a machine-interpretable format, and has tremendous applications in logistics and social networks. Existing efforts to build a geospatial KG, have mainly used sparse spatial relationships, e.g., a district located inside a city, which provide only marginal benefits compared to a traditional database. In spite of the substantial advances in the tasks of link prediction and knowledge graph completion, identifying geospatial relationships remains challenging, particularly due to the fact that spatial entities are represented with single-point geometries, and textual attributes are frequently missing. In this study, we present GTMiner, a novel framework capable of jointly modeling Geospatial and Textual information to construct a knowledge graph, by mining three useful spatial relationships from a geospatial database, in an end-to-end fashion. The system is divided into three components: (1) a Candidate Selection module, to efficiently select a small number of candidate pairs; (2) a Relation Prediction component to predict spatial relationships between the entities; (3) a KG Refinement procedure, to improve both coverage and correctness of a geospatial knowledge graph. We carry out experiments on four cities' geospatial databases, from publicly-available sources and compare with existing algorithms for link prediction and geospatial data integration. Finally, we conduct an ablation study to motivate our design choices and an efficiency analysis to show that the time required by GTMiner for training and inference is comparable, or even shorter, than existing solutions.}
}


@article{DBLP:journals/pacmmod/ZhouLFLG23,
	author = {Xuanhe Zhou and
                  Guoliang Li and
                  Jianhua Feng and
                  Luyang Liu and
                  Wei Guo},
	title = {Grep: {A} Graph Learning Based Database Partitioning System},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {94:1--94:24},
	year = {2023},
	url = {https://doi.org/10.1145/3588948},
	doi = {10.1145/3588948},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhouLFLG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database partitioning is a fundamental but challenging task in distributed databases, which selects specific columns as a partitioning key for each table and uses the partitioning key to allocate the table data into different compute nodes in order to maximize the performance. However, this problem is NP-hard and existing distributed databases require users to manually specify the partitioning keys, which may cause potential performance degradation. Although reinforcement learning based methods have been proposed, they have several limitations. First, they do not capture the complex data distributions and query access patterns, and thus involve high computation cost across different compute nodes to answer a query. Second, they involve an expensive step to repetitively partition the data into different compute nodes in order to train a learned key-selection model, which is a waste of time and resources. To address these limitations, we propose a practical learned database partitioning system Grep. We first adopt a graph model to encode data and query features, where vertices are columns, edges are query relations, and the weights of columns are computed based on the localized graph structures (e.g., data diversity, joined columns). We then utilize graph neural networks to embed the partitioning factors into embedding vectors in order to capture the data and query correlations. Next we propose a key-selection model to select appropriate partitioning keys based on the graph model. Finally, we propose an evaluation model to estimate the partitioning performance without actually partitioning the database. We have implemented Grep in a commercial distributed database, and experiments show the effectiveness of our system (e.g., 68% higher throughput for 30K queries in a real banking scenario).}
}


@article{DBLP:journals/pacmmod/ChenYLLRZHWS23,
	author = {Chaoyu Chen and
                  Hang Yu and
                  Zhichao Lei and
                  Jianguo Li and
                  Shaokang Ren and
                  Tingkai Zhang and
                  Silin Hu and
                  Jianchao Wang and
                  Wenhui Shi},
	title = {{BALANCE:} Bayesian Linear Attribution for Root Cause Localization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {95:1--95:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588949},
	doi = {10.1145/3588949},
	timestamp = {Mon, 19 Jun 2023 16:36:09 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenYLLRZHWS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Root Cause Analysis (RCA) plays an indispensable role in distributed data system maintenance and operations, as it bridges the gap between fault detection and system recovery. Existing works mainly study multidimensional localization or graph-based root cause localization. This paper opens up the possibilities of exploiting the recently developed framework of explainable AI (XAI) for the purpose of RCA. In particular, we propose BALANCE (BAyesian Linear AttributioN for root CausE localization), which formulates the problem of RCA through the lens of attribution in XAI and seeks to explain the anomalies in the target KPIs by the behavior of the candidate root causes. BALANCE consists of three innovative components. First, we propose a Bayesian multicollinear feature selection (BMFS) model to predict the target KPIs given the candidate root causes in a forward manner while promoting sparsity and concurrently paying attention to the correlation between the candidate root causes. Second, we introduce attribution analysis to compute the attribution score for each candidate in a backward manner. Third, we merge the estimated root causes related to each KPI if there are multiple KPIs. We extensively evaluate the proposed BALANCE method on one synthesis dataset as well as three real-world RCA tasks, that is, bad SQL localization, container fault localization, and fault type diagnosis for Exathlon. Results show that BALANCE outperforms the state-of-the-art (SOTA) methods in terms of accuracy with the least amount of running time, and achieves at least 6% notably higher accuracy than SOTA methods for real tasks. BALANCE has been deployed to production to tackle real-world RCA problems, and the online results further advocate its usage for real-time diagnosis in distributed data systems.}
}


@article{DBLP:journals/pacmmod/Du00H23,
	author = {Xinyu Du and
                  Xingyi Zhang and
                  Sibo Wang and
                  Zengfeng Huang},
	title = {Efficient Tree-SVD for Subset Node Embedding over Large Dynamic Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {96:1--96:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588950},
	doi = {10.1145/3588950},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Du00H23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subset embedding is the task to learn low-dimensional representations for a subset of nodes according to the graph topology. It has applications when we focus on a subset of users, e.g., young adults, and aim to make better recommendations for these target users. In real-world scenarios, graphs are dynamically changing. Thus, it is more desirable to dynamically maintain the subset embeddings to reflect graph updates. The state-of-the-art methods, e.g., DynPPE, still adopt a hashing-based method, while hashing-based solutions are shown to be less effective than matrix factorization (MF)-based methods in existing studies. At the same time, MF-based methods in the literature are too expensive to update the embedding when the graph changes, making them inapplicable on dynamic graphs. Motivated by this, we present Tree-SVD, an efficient and effective MF-based method for dynamic subset embedding. If we simply maintain the whole proximity matrix, then we need to re-do the MF, e.g., truncated Singular Value Decomposition (SVD), on the whole matrix after graph updates, which is prohibitive. To tackle this issue, our main idea is to do hierarchical SVD (HSVD) on the proximity matrix of the given subset, which vertically divides the proximity matrix into multiple sub-matrices, and then repeatedly do SVD on sub-matrices and merge the intermediate results to obtain the final embedding. We first present Tree-SVD, which combines a sparse randomized SVD with an HSVD. Our theoretical analysis shows that our Tree-SVD gains the efficiency of sparse randomized SVD and the flexibility of the HSVD with theoretical guarantees. To further reduce update costs, we present a lazy-update strategy. In this strategy, we only update sub-matrices that changes remarkably in terms of the Frobenius norm. We present theoretical analysis to show the guarantees with our lazy-update strategy. Extensive experiments show the efficiency and effectiveness of Tree-SVD on node classification and link prediction tasks.}
}


@article{DBLP:journals/pacmmod/Wu0ZG0J23,
	author = {Xinle Wu and
                  Dalin Zhang and
                  Miao Zhang and
                  Chenjuan Guo and
                  Bin Yang and
                  Christian S. Jensen},
	title = {AutoCTS+: Joint Neural Architecture and Hyperparameter Search for
                  Correlated Time Series Forecasting},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {97:1--97:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588951},
	doi = {10.1145/3588951},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Wu0ZG0J23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sensors in cyber-physical systems often capture interconnected processes and thus emit correlated time series (CTS), the forecasting of which enables important applications. The key to successful CTS forecasting is to uncover the temporal dynamics of time series and the spatial correlations among time series. Deep learning-based solutions exhibit impressive performance at discerning these aspects. In particular, automated CTS forecasting, where the design of an optimal deep learning architecture is automated, enables forecasting accuracy that surpasses what has been achieved by manual approaches. However, automated CTS solutions remain in their infancy and are only able to find optimal architectures for predefined hyperparameters and scale poorly to large-scale CTS. To overcome these limitations, we propose AutoCTS+, a joint, scalable framework, to automatically devise effective CTS forecasting models. Specifically, we encode each candidate architecture and accompanying hyperparameters into a joint graph representation. We introduce an efficient Architecture-Hyperparameter Comparator (AHC) to rank all architecture-hyperparameter pairs, and we then further evaluate the top-ranked pairs to select an architecture-hyperparameter pair as the final model. Extensive experiments on six benchmark datasets demonstrate that AutoCTS+ not only eliminates manual efforts but also is capable of better performance than manually designed and existing automatically designed CTS models. In addition, it shows excellent scalability to large CTS.}
}


@article{DBLP:journals/pacmmod/LaiL023,
	author = {Ziliang Lai and
                  Chris Liu and
                  Eric Lo},
	title = {When Private Blockchain Meets Deterministic Database},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {98:1--98:28},
	year = {2023},
	url = {https://doi.org/10.1145/3588952},
	doi = {10.1145/3588952},
	timestamp = {Sun, 22 Oct 2023 11:16:15 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LaiL023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Private blockchain as a replicated transactional system shares many commonalities with distributed database. However, the intimacy between private blockchain and deterministic database has never been studied. In essence, private blockchain and deterministic database both ensure replica consistency by determinism. In this paper, we present a comprehensive analysis to uncover the connections between private blockchain and deterministic database. While private blockchains have started to pursue deterministic transaction executions recently, deterministic databases have already studied deterministic concurrency control protocols for almost a decade. This motivates us to propose Harmony, a novel deterministic concurrency control protocol designed for blockchain use. We use Harmony to build a new relational blockchain, namely HarmonyBC, which features low abort rates, hotspot resiliency, and inter-block parallelism, all of which are especially important to disk-oriented blockchain. Empirical results on Smallbank, YCSB, and TPC-C show that HarmonyBC offers 2.0x to 3.5x throughput better than the state-of-the-art private blockchains.}
}


@article{DBLP:journals/pacmmod/BarbarioliMSK23,
	author = {Bruno Barbarioli and
                  Gabriel Mersy and
                  Stavros Sintos and
                  Sanjay Krishnan},
	title = {Hierarchical Residual Encoding for Multiresolution Time Series Compression},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {99:1--99:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588953},
	doi = {10.1145/3588953},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BarbarioliMSK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data compression is a key technique for reducing the cost of data transfer from storage to compute nodes. Increasingly, modern data scales necessitate lossy compression techniques, where exactness is sacrificed for a smaller compressed representation. One challenge in lossy compression is that different applications may have different accuracy demands. Today's compression techniques struggle in this setting either forcing the user to compress at the strictest accuracy demand, or to re-encode the data at multiple resolutions. This paper proposes a simple, but effective multiresolution compression algorithm for time series data, where a single encoding can effectively be decompressed at multiple output resolutions. There are a number of benefits over current state-of-the-art techniques for time series compression. (1) The storage footprint of this encoding is smaller than re-encoding the data at multiple resolutions. (2) Similarly, the compression latency is generally smaller than re-encoding at multiple resolutions. (3) Finally, the decompression latency of our encoding is significantly faster than single encodings at the strictest accuracy demand.}
}


@article{DBLP:journals/pacmmod/ZeighamiSS23,
	author = {Sepanta Zeighami and
                  Cyrus Shahabi and
                  Vatsal Sharan},
	title = {NeuroSketch: Fast and Approximate Evaluation of Range Aggregate Queries
                  with Neural Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {100:1--100:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588954},
	doi = {10.1145/3588954},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZeighamiSS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Range aggregate queries (RAQs) are an integral part of many real-world applications, where, often, fast and approximate answers for the queries are desired. Recent work has studied answering RAQs using machine learning (ML) models, where a model of the data is learned to answer the queries. However, there is no theoretical understanding of why and when the ML based approaches perform well. Furthermore, since the ML approaches model the data, they fail to capitalize on any query specific information to improve performance in practice. In this paper, we focus on modeling "queries" rather than data and train neural networks to learn the query answers. This change of focus allows us to theoretically study our ML approach to provide a distribution and query dependent error bound for neural networks when answering RAQs. We confirm our theoretical results by developing NeuroSketch, a neural network framework to answer RAQs in practice. Extensive experimental study on real-world, TPC-benchmark and synthetic datasets show that NeuroSketch answers RAQs multiple orders of magnitude faster than state-of-the-art and with better accuracy.}
}


@article{DBLP:journals/pacmmod/AkiliP023,
	author = {Samira Akili and
                  Steven Purtzel and
                  Matthias Weidlich},
	title = {INEv: In-Network Evaluation for Event Stream Processing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {101:1--101:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588955},
	doi = {10.1145/3588955},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AkiliP023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Complex event processing (CEP) detects situations of interest by evaluating queries over event streams. Once CEP is used in networked applications, the distribution of query evaluation among the event sources enables performance optimization. Instead of collecting all events at one location for query evaluation, sub-queries are placed at network nodes to reduce the data transmission overhead. Yet, existing techniques either place such sub-queries at exactly one node in the network, which neglects the benefits of truly distributed evaluation, or are agnostic to the network structure, which ignores transmission costs due to the absence of direct network links. To overcome the above limitations, we propose INEV graphs for in-network evaluation of CEP queries with rich semantics, including Kleene closure and negation. Our idea is to introduce fine-granular routing of partial results of sub-queries as an additional degree of freedom in query evaluation: We exploit events already disseminated in the network as part of one sub-query, when evaluating another one. We show how to instantiate INEv graphs by splitting a query workload into sub-queries, placing them at network nodes, and forwarding of their results to other nodes. Also, we characterize INEv graphs that guarantee correct and complete query evaluation, and discuss their construction based on a cost model that unifies transmission and processing latency. Our experimental results indicate that INEv graphs can reduce transmission costs for distributed CEP by up to eight orders of magnitude compared to baseline strategies.}
}


@article{DBLP:journals/pacmmod/WangICWNY23,
	author = {Guangjing Wang and
                  Nikolay Ivanov and
                  Bocheng Chen and
                  Qi Wang and
                  ThanhVu Nguyen and
                  Qiben Yan},
	title = {Graph Learning for Interactive Threat Detection in Heterogeneous Smart
                  Home Rule Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {102:1--102:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588956},
	doi = {10.1145/3588956},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangICWNY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The interactions among automation configuration rule data have led to undesired and insecure issues in smart homes, which are known as interactive threats. Most existing solutions use program analysis to identify interactive threats among automation rules, which is not suitable for closed-source platforms. Meanwhile, security policy-based solutions suffer from low detection accuracy because the pre-defined security policies in a single platform can hardly cover diverse interactive threat types across heterogeneous platforms. In this paper, we propose Glint, the first graph learning-based system for interactive threat detection in smart homes. We design a multi-scale graph representation learning model, called ITGNN, for both homogeneous and heterogeneous interaction graph pattern learning. To facilitate graph learning, we build large interaction graph training datasets by multi-domain data fusion from five different platforms. Moreover, Glint detects drifting samples with contrastive learning and improves the generalization ability with transfer learning across heterogeneous platforms. Our evaluation shows that Glint achieves 95.5% accuracy in detecting interactive threats across the five platforms. Besides, we examine a set of user-designed blueprints in the Home Assistant platform and reveal four new types of real-world interactive threats, called "action block", "action ablation", "trigger intake", and "condition duplicate", which are cross-platform interactive threats captured by Glint.}
}


@article{DBLP:journals/pacmmod/SaeedanEZ23,
	author = {Majid Saeedan and
                  Ahmed Eldawy and
                  Zhijia Zhao},
	title = {dsJSON: {A} Distributed {SQL} {JSON} Processor},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {103:1--103:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588957},
	doi = {10.1145/3588957},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SaeedanEZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The popularity of JSON as a data interchange format resulted in big amounts of datasets available for processing. Users would like to analyze this data using SQL queries but existing distributed systems limit their users to only two specific formats, JSONLine and GeoJSON. The complexity of JSON schema makes it challenging to parse arbitrary files in a modern distributed system while producing records with unified schema that can be processed with SQL. To address these challenges, this paper introduces dsJSON, a state-of-the-art distributed JSON processor that overcomes limitations in existing systems and scales to big and complex data. dsJSON introduces the projection tree, a novel data structure that applies selective parsing of nested attributes to produce records that are ready for SQL processors. The key objective of the projection tree is to parse a big JSON file in parallel to produce records with a unified schema that can be processed with SQL. dsJSON is integrated into SparkSQL which enables users to run arbitrary SQL queries on complex JSON files. It also pushes projection and filter down into the parser for full integration between the parser and the processor. Experiments on up-to two terabytes of real data show that dsJSON performs several times faster than existing systems. It can also efficiently parse extremely large files not supported by existing distributed parsers}
}


@article{DBLP:journals/pacmmod/MiaoWPGY23,
	author = {Xiaoye Miao and
                  Yangyang Wu and
                  Jiazhen Peng and
                  Yunjun Gao and
                  Jianwei Yin},
	title = {Efficient and Effective Cardinality Estimation for Skyline Family},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {104:1--104:21},
	year = {2023},
	url = {https://doi.org/10.1145/3588958},
	doi = {10.1145/3588958},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MiaoWPGY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality estimation, predicting the query result size, is a fundamental problem in databases. Existing skyline cardinality estimation methods are computationally infeasible for massive skyline queries over the large-scale database. In this paper, we introduce a unified skyline family w.r.t. various skyline variants. We propose an efficient and effective skyline family cardinality estimation model, named EECE, in an end-to-end manner. EECE consists of two modules, unsupervised data distribution learning (DDL) and supervised monotonic cardinality estimation (MCE). DDL leverages the mixture data guided transformer to learn the distribution of database and query parameters for model pre-training. MCE further incorporates supervised learning and parameter clamping to enhance the estimation under monotonicity guarantees. We develop an efficient incremental learning algorithm for EECE to adapt the database and query logs update. Extensive experiments on several real-world and synthetic datasets demonstrate that, EECE speeds up the cardinality estimation by six orders of magnitude, with more than 39% accuracy gain, compared to the state-of-the-art approaches.}
}


@article{DBLP:journals/pacmmod/WangYLZ023,
	author = {Ke Wang and
                  Guanqun Yang and
                  Yiwei Li and
                  Huanchen Zhang and
                  Mingyu Gao},
	title = {When Tree Meets Hash: Reducing Random Reads for Index Structures on
                  Persistent Memories},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {105:1--105:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588959},
	doi = {10.1145/3588959},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangYLZ023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indexing structures are widely used in modern data-processing applications to support high-performance queries, and there are a variety of recent designs specifically optimized for the newly available persistent memory (PM). The primary focus of previous PM indexes is on reducing the expensive PM writes for persisting data. However, we find that in tree-based PM indexes, because of the smaller performance gap between writes and random reads on real PM devices, the read-intensive tree traversal phase dominates the overall latency. This observation calls for further optimizations on existing indexing structures for PM. In this paper, we propose Extendible Radix Tree (ERT), an efficient indexing structure for PM that significantly reduces tree heights to minimize random reads, while still maintaining fast in-node search speed. The key idea is to use extendible hashing for each node in a radix tree. This design allows us to have a relatively large fanout of the radix tree to keep the tree height small, and also to realize constant-time lookups within a node. Using extendible hashing also allows for incremental node modification without excessive writes during inserts and updates. Range queries are efficiently and robustly handled by enforcing partial ordering among the keys in the hash table of each node without introducing more hash collisions. Our experiments on both synthetic and real-world data sets demonstrate that ERT achieves up to 2.65×, 4.41×, and 2.43× speedups for search, insert, and range queries over the respectively state-of-the-art PM index.}
}


@article{DBLP:journals/pacmmod/Zhang0DZXX023,
	author = {Zhengxin Zhang and
                  Qing Li and
                  Guanglin Duan and
                  Dan Zhao and
                  Jingyu Xiao and
                  Guorui Xie and
                  Yong Jiang},
	title = {Pontus: Finding Waves in Data Streams},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {106:1--106:26},
	year = {2023},
	url = {https://doi.org/10.1145/3588960},
	doi = {10.1145/3588960},
	timestamp = {Tue, 13 Aug 2024 14:11:15 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Zhang0DZXX023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The bumps and dips in data streams are valuable patterns for data mining and networking scenarios such as online advertising and botnet detection. In this paper, we define the wave, a data stream pattern with a serious deviation from the stable arrival rate for a period of time. We then propose Pontus, an efficient framework for wave detection and estimation. In Pontus, a lightweight data structure is utilized for the preliminary processing of incoming packets in the data plane to take advantage of its high processing speed; then, the powerful control plane carries out computationally intensive wave detection and estimation. In particular, we propose the Multi-Stage Progressive Tracking strategy which detects waves in stages and removes any disqualified items promptly to save memory. Hash collisions are addressed by a Stage Variance Maximization technique to reduce estimation error. Moreover, we prove the theoretical error bound and establish upper bounds of false positive and false negative. Experiment results show that the software version of Pontus can achieve around 97% F1-Score even under scarce memory when baselines fail. Furthermore, the implemented prototype of Pontus based on P4 achieves 842x higher throughput than the baseline strawman solution.}
}


@article{DBLP:journals/pacmmod/FuWX023,
	author = {Rui Fu and
                  Yuncheng Wu and
                  Quanqing Xu and
                  Meihui Zhang},
	title = {{FEAST:} {A} Communication-efficient Federated Feature Selection Framework
                  for Relational Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {107:1--107:28},
	year = {2023},
	url = {https://doi.org/10.1145/3588961},
	doi = {10.1145/3588961},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FuWX023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vertical federated learning (VFL) is an emerging paradigm for cross-silo organizations to build more accurate machine learning (ML) models. In this setting, multiple organizations (i.e., parties) hold the same set of samples with different features. However, different parties may have redundant or highly correlated features, leading to inefficient and ineffective VFL model training. Effective feature selection in VFL is therefore essential to mitigate such a problem and improve model effectiveness, as well as computation and communication efficiency. To this end, in this paper, we propose a federated feature selection framework, called FEAST, which leverages conditional mutual information (CMI) to select more informative features while having low redundancy. Furthermore, we design a communication-efficient method to reduce the information exchanged among the parties while protecting the parties' raw data. Extensive experiments on four real-world datasets demonstrate that the proposed framework achieves state-of-the-art performance in terms of accuracy, communication and computation costs.}
}


@article{DBLP:journals/pacmmod/LiuC23,
	author = {Zhuoxuan Liu and
                  Shimin Chen},
	title = {Pea Hash: {A} Performant Extendible Adaptive Hashing Index},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {108:1--108:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588962},
	doi = {10.1145/3588962},
	timestamp = {Thu, 15 Jun 2023 21:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hashing index is widely used to support efficient point operations. We observe that there is a conflict between performance and memory utilization goals. Existing hashing indices often have to trade off hash table access latency for better memory utilization. Moreover, many designs support only unique keys, and their performance is often suboptimal with skew workloads. In this paper, we propose Pea Hash with two techniques to address the above two problems: (i) adaptive hashing strategy that holistically optimizes both access latency and memory utilization, and (ii) data-aware adaptive buckets that accommodate unique keys, and keys with various numbers of duplicates. We develop both an NVM-optimized Pea Hash and a DRAM-based Pea Hash index. Experiments on a machine equipped with Intel Optane DC Persistent memory show that compared to state-of-the-art NVM-optimized hashing indices, the NVM-optimized Pea Hash achieves up to 13.8x performance improvements with similar memory utilization. The DRAM-based Pea Hash outperforms existing in-DRAM hashing index designs, showing the generality of the proposed techniques.}
}


@article{DBLP:journals/pacmmod/DoshiZJMHABF23,
	author = {Lyric Doshi and
                  Vincent Zhuang and
                  Gaurav Jain and
                  Ryan Marcus and
                  Haoyu Huang and
                  Deniz Altinb{\"{u}}ken and
                  Eugene Brevdo and
                  Campbell Fraser},
	title = {Kepler: Robust Learning for Parametric Query Optimization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {109:1--109:25},
	year = {2023},
	url = {https://doi.org/10.1145/3588963},
	doi = {10.1145/3588963},
	timestamp = {Thu, 15 Jun 2023 21:57:48 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DoshiZJMHABF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most existing parametric query optimization (PQO) techniques rely on traditional query optimizer cost models, which are often inaccurate and result in suboptimal query performance. We propose Kepler, an end-to-end learning-based approach to PQO that demonstrates significant speedups in query latency over a traditional query optimizer. Central to our method is Row Count Evolution (RCE), a novel plan generation algorithm based on perturbations in the sub-plan cardinality space. While previous approaches require accurate cost models, we bypass this requirement by evaluating candidate plans via actual execution data and training anML model to predict the fastest plan given parameter binding values. Our models leverage recent advances in neural network uncertainty in order to robustly predict faster plans while avoiding regressions in query performance. Experimentally, we show that Kepler achieves significant improvements in query runtime on multiple datasets on PostgreSQL.}
}


@article{DBLP:journals/pacmmod/NieMWYXMC023,
	author = {Xiaonan Nie and
                  Xupeng Miao and
                  Zilong Wang and
                  Zichao Yang and
                  Jilong Xue and
                  Lingxiao Ma and
                  Gang Cao and
                  Bin Cui},
	title = {FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via
                  Dynamic Device Placement},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {110:1--110:19},
	year = {2023},
	url = {https://doi.org/10.1145/3588964},
	doi = {10.1145/3588964},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/NieMWYXMC023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing data volume, there is a trend of using large-scale pre-trained models to store the knowledge into an enormous number of model parameters. The training of these models is composed of lots of dense algebras, requiring a huge amount of hardware resources. Recently, sparsely-gated Mixture-of-Experts (MoEs) are becoming more popular and have demonstrated impressive pretraining scalability in various downstream tasks. However, such a sparse conditional computation may not be effective as expected in practical systems due to the routing imbalance and fluctuation problems. Generally, MoEs are becoming a new data analytics paradigm in the data life cycle and suffering from unique challenges at scales, complexities, and granularities never before possible. In this paper, we propose a novel DNN training framework, FlexMoE, which systematically and transparently address the inefficiency caused by dynamic dataflow. We first present an empirical analysis on the problems and opportunities of training MoE models, which motivates us to overcome the routing imbalance and fluctuation problems by a dynamic expert management and device placement mechanism. Then we introduce a novel scheduling module over the existing DNN runtime to monitor the data flow, make the scheduling plans, and dynamically adjust the model-to-hardware mapping guided by the real-time data traffic. A simple but efficient heuristic algorithm is exploited to dynamically optimize the device placement during training. We have conducted experiments on both NLP models (e.g., BERT and GPT) and vision models (e.g., Swin). And results show FlexMoE can achieve superior performance compared with existing systems on real-world workloads --- FlexMoE outperforms DeepSpeed by 1.70x on average and up to 2.10x, and outperforms FasterMoE by 1.30x on average and up to 1.45x.}
}


@article{DBLP:journals/pacmmod/Wang0WP023,
	author = {Zeyu Wang and
                  Qitong Wang and
                  Peng Wang and
                  Themis Palpanas and
                  Wei Wang},
	title = {Dumpy: {A} Compact and Adaptive Index for Large Data Series Collections},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {1},
	pages = {111:1--111:27},
	year = {2023},
	url = {https://doi.org/10.1145/3588965},
	doi = {10.1145/3588965},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Wang0WP023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data series indexes are necessary for managing and analyzing the increasing amounts of data series collections that are nowadays available. These indexes support both exact and approximate similarity search, with approximate search providing high-quality results within milliseconds, which makes it very attractive for certain modern applications. Reducing the pre-processing (i.e., index building) time and improving the accuracy of search results are two major challenges. DSTree and the iSAX index family are state-of-the-art solutions for this problem. However, DSTree suffers from long index building times, while iSAX suffers from low search accuracy. In this paper, we identify two problems of the iSAX index family that adversely affect the overall performance. First, we observe the presence of a proximity-compactness trade-off related to the index structure design (i.e., the node fanout degree), significantly limiting the efficiency and accuracy of the resulting index. Second, a skewed data distribution will negatively affect the performance of iSAX. To overcome these problems, we propose Dumpy, an index that employs a novel multi-ary data structure with an adaptive node splitting algorithm and an efficient building workflow. Furthermore, we devise Dumpy-Fuzzy as a variant of Dumpy which further improves search accuracy by proper duplication of series. Experiments with a variety of large, real datasets demonstrate that the Dumpy solutions achieve considerably better efficiency, scalability and search accuracy than its competitors.}
}


@article{DBLP:journals/pacmmod/AgrawalACFH23,
	author = {Divyakant Agrawal and
                  Sihem Amer{-}Yahia and
                  K. Sel{\c{c}}uk Candan and
                  Avrilia Floratou and
                  Hakan Hacig{\"{u}}m{\"{u}}s},
	title = {{PACMMOD} {V1} {N2} Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {112:1--112:2},
	year = {2023},
	url = {https://doi.org/10.1145/3589257},
	doi = {10.1145/3589257},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgrawalACFH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We are excited to welcome you to the second issue of Volume 1 of the Proceedings of the ACM on Management of Data, PACMMOD. In addition to the 76 research track articles (out of 279 Cycle C submissions), this issue also includes peer-reviewed industrial track papers.}
}


@article{DBLP:journals/pacmmod/LimLCLPKLK23,
	author = {Chaemin Lim and
                  Suhyun Lee and
                  Jinwoo Choi and
                  Jounghoo Lee and
                  Seongyeon Park and
                  Hanjun Kim and
                  Jinho Lee and
                  Youngsok Kim},
	title = {Design and Analysis of a Processing-in-DIMM Join Algorithm: {A} Case
                  Study with {UPMEM} DIMMs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {113:1--113:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589258},
	doi = {10.1145/3589258},
	timestamp = {Wed, 21 May 2025 18:33:20 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LimLCLPKLK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern dual in-line memory modules (DIMMs) support processing-in-memory (PIM) by implementing in-DIMM processors (IDPs) located near memory banks. PIM can greatly accelerate in-memory join, whose performance is frequently bounded by main-memory accesses, by offloading the operations of join from host central processing units (CPUs) to the IDPs. As real PIM hardware has not been available until very recently, the prior PIM-assisted join algorithms have relied on PIM hardware simulators which assume fast shared memory between the IDPs and fast inter-IDP communication; however, on commodity PIM-enabled DIMMs, the IDPs do not share memory and demand the CPUs to mediate inter-IDP communication. Such discrepancies in the architectural characteristics make the prior studies incompatible with the DIMMs. Thus, to exploit the high potential of PIM on commodity PIM-enabled DIMMs, we need a new join algorithm designed and optimized for the DIMMs and their architectural characteristics. In this paper, we design and analyze Processing-In-DIMM Join (PID-Join), a fast in-memory join algorithm which exploits UPMEM DIMMs, currently the only publicly-available PIM-enabled DIMMs. The DIMMs impose several key challenges on efficient acceleration of join including the shared-nothing nature and limited compute capabilities of the IDPs, the lack of hardware support for fast inter-IDP communication, and the slow IDP-wise data transfers between the IDPs and the main memory. PID-Join overcomes the challenges by prototyping and evaluating hash, sort-merge, and nested-loop algorithms optimized for the IDPs, enabling fast inter-IDP communication using host CPU cache streaming and vector instructions, and facilitating fast rank-wise data transfers between the IDPs and the main memory. Our evaluation using a real system equipped with eight UPMEM DIMMs and 1,024 IDPs shows that PID-Join greatly improves the performance of in-memory join over various CPU-based in-memory join algorithms.}
}


@article{DBLP:journals/pacmmod/Wang00023,
	author = {Letong Wang and
                  Xiaojun Dong and
                  Yan Gu and
                  Yihan Sun},
	title = {Parallel Strong Connectivity Based on Faster Reachability},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {114:1--114:29},
	year = {2023},
	url = {https://doi.org/10.1145/3589259},
	doi = {10.1145/3589259},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Wang00023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computing strongly connected components (SCC) is among the most fundamental problems in graph analytics. Given the large size of today's real-world graphs, parallel SCC implementation is increasingly important. SCC is challenging in the parallel setting and is particularly hard on large-diameter graphs. Many existing parallel SCC implementations can be even slower than Tarjan's sequential algorithm on large-diameter graphs. To tackle this challenge, we propose an efficient parallel SCC implementation using a new parallel reachability approach. Our solution is based on a novel idea referred to as vertical granularity control (VGC). It breaks the synchronization barriers to increase parallelism and hide scheduling overhead. To use VGC in our SCC algorithm, we also design an efficient data structure called the parallel hash bag. It uses parallel dynamic resizing to avoid redundant work in maintaining frontiers (vertices processed in a round). We implement the parallel SCC algorithm by Blelloch et al. (J. ACM, 2020) using our new parallel reachability approach. We compare our implementation to the state-of-the-art systems, including GBBS, iSpan, Multi-step, and our highly optimized Tarjan's (sequential) algorithm, on 18 graphs, including social, web, k-NN, and lattice graphs. On a machine with 96 cores, our implementation is the fastest on 16 out of 18 graphs. On average (geometric means) over all graphs, our SCC is 6.0× faster than the best previous parallel code (GBBS), 12.8× faster than Tarjan's sequential algorithms, and 2.7× faster than the best existing implementation on each graph. We believe that our techniques are of independent interest. We also apply our parallel hash bag and VGC scheme to other graph problems, including connectivity and least-element lists (LE-lists). Our implementations improve the performance of the state-of-the-art parallel implementations for these two problems.}
}


@article{DBLP:journals/pacmmod/WangS23,
	author = {Zhiqi Wang and
                  Zili Shao},
	title = {ForestTI: {A} Scalable Inverted-Index-Oriented Timeseries Management
                  System with Flexible Memory Efficiency},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {115:1--115:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589260},
	doi = {10.1145/3589260},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Timeseries management systems play an important role in IoT and performance monitoring. As the data volume scales up, absorbing data memory efficiently with high throughput becomes a growing requirement for timeseries management systems. However, the designs of the existing systems, especially the in-memory data structures, suffer from two issues. First, they suffer from the trade-off between memory efficiency and performance. Second, they are not scalable because of lock contention where they cannot benefit from parallel insertion and querying. In this paper, we propose ForestTI, a scalable inverted-index-oriented timeseries management system where the balance point between memory efficiency and performance can be flexibly adjusted under the increasing memory pressure. First, we present a two-level inverted index, which is scalable with optimistic lock coupling, and its internal structure can be gradually converted to more memory efficient representations. Second, we propose a two-level pointer swizzling mechanism to actively swap out the cold posting lists and in-memory timeseries objects as the number of timeseries increases. Finally, we further optimize the on-disk data structures (i.e. write-ahead logs and LSM-tree) to adapt to the high insertion throughput from the in-memory components. We prototype ForestTI with C++ from scratch, and compared to the storage engine of Prometheus, ForestTI achieves 1.79x higher insertion throughput, 52.1% lower query latency, and 56.9% lower memory occupation. We have released the open-source code of ForestTI for public access.}
}


@article{DBLP:journals/pacmmod/LiYS23,
	author = {Yiran Li and
                  Renchi Yang and
                  Jieming Shi},
	title = {Efficient and Effective Attributed Hypergraph Clustering via K-Nearest
                  Neighbor Augmentation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {116:1--116:23},
	year = {2023},
	url = {https://doi.org/10.1145/3589261},
	doi = {10.1145/3589261},
	timestamp = {Fri, 07 Feb 2025 15:20:40 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiYS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hypergraphs are an omnipresent data structure used to represent high-order interactions among entities. Given a hypergraph H wherein nodes are associated with attributes, attributed hypergraph clustering (AHC) aims to partition the nodes in H into k disjoint clusters, such that intra-cluster nodes are closely connected and share similar attributes, while inter-cluster nodes are far apart and dissimilar. It is highly challenging to capture multi-hop connections via nodes or attributes on large attributed hypergraphs for accurate clustering. Existing AHC solutions suffer from issues of prohibitive computational costs, sub-par clustering quality, or both. In this paper, we present AHCKA, an efficient approach to AHC, which achieves state-of-the-art result quality via several algorithmic designs. Under the hood, AHCKA includes three key components: (i) a carefully-crafted K-nearest neighbor augmentation strategy for the optimized exploitation of attribute information on hypergraphs, (ii) a joint hypergraph random walk model to devise an effective optimization objective towards AHC, and (iii) a highly efficient solver with speedup techniques for the problem optimization. Extensive experiments, comparing AHCKA against 15 baselines over 8 real attributed hypergraphs, reveal that AHCKA is superior to existing competitors in terms of clustering quality, while often being up to orders of magnitude faster.}
}


@article{DBLP:journals/pacmmod/0001CBM23,
	author = {Tianyu Li and
                  Badrish Chandramouli and
                  Sebastian Burckhardt and
                  Samuel Madden},
	title = {{DARQ} Matter Binds Everything: Performant and Composable Cloud Programming
                  via Resilient Steps},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {117:1--117:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589262},
	doi = {10.1145/3589262},
	timestamp = {Tue, 08 Aug 2023 10:54:19 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0001CBM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Providing strong fault-tolerant guarantees for the modern cloud is difficult, as application developers must coordinate between independent stateful services and ephemeral compute and handle various failure-induced anomalies. We propose Composable Resilient Steps (CReSt), a new abstraction for resilient cloud applications. CReSt uses fault-tolerant steps as its core building block, which allows participants to receive, process, and send messages as a single uninterruptible atomic unit. Composability and reliability are orthogonally achieved by reusable CReSt implementations, for example, leveraging reliable message queues. Thus, CReSt application builders focus solely on translating application logic into steps, and infrastructure builders focus on efficient CReSt implementations. We propose one such implementation called DARQ (for Deduplicated Asynchronously Recoverable Queues). At its core, DARQ is a storage service that encapsulates CReSt participant state and enforces CReSt semantics; developers attach ephemeral compute nodes to DARQ instances to implement stateful distributed components. Services built with DARQ are resilient by construction, and CReSt-compatible services naturally compose without loss of resilience. For performance, we propose a novel speculative execution scheme to execute CReSt steps without waiting for message persistence in DARQ, effectively eliding cloud persistence overheads; our scheme maintains CReSt's fault-tolerance guarantees and automatically restores to a consistent system state upon failure. We showcase the generality of CReSt and DARQ using two applications: cloud streaming and workflow processing. Experiments show that DARQ is able to achieve extremely low latency and high throughput across these use cases, often beating state-of-the-art customized solutions.}
}


@article{DBLP:journals/pacmmod/KuschewskiSAL23,
	author = {Maximilian Kuschewski and
                  David Sauerwein and
                  Adnan Alhomssi and
                  Viktor Leis},
	title = {BtrBlocks: Efficient Columnar Compression for Data Lakes},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {118:1--118:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589263},
	doi = {10.1145/3589263},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KuschewskiSAL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analytics is moving to the cloud and data is moving into data lakes. These reside on object storage services like S3 and enable seamless data sharing and system interoperability. To support this, many systems build on open storage formats like Apache Parquet. However, these formats are not optimized for remotely-accessed data lakes and today's high-throughput networks. Inefficient decompression makes scans CPU-bound and thus increases query time and cost. With this work we present BtrBlocks, an open columnar storage format designed for data lakes. BtrBlocks uses a set of lightweight encoding schemes, achieving fast and efficient decompression and high compression ratios.}
}


@article{DBLP:journals/pacmmod/Xiang0L023,
	author = {Zihang Xiang and
                  Tianhao Wang and
                  Wanyu Lin and
                  Di Wang},
	title = {Practical Differentially Private and Byzantine-resilient Federated
                  Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {119:1--119:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589264},
	doi = {10.1145/3589264},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Xiang0L023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy and Byzantine resilience are two indispensable requirements for a federated learning (FL) system. Although there have been extensive studies on privacy and Byzantine security in their own track, solutions that consider both remain sparse. This is due to difficulties in reconciling privacy-preserving and Byzantine-resilient algorithms. In this work, we propose a solution to such a two-fold issue. We use our version of differentially private stochastic gradient descent (DP-SGD) algorithm to preserve privacy and then apply our Byzantine-resilient algorithms. We note that while existing works follow this general approach, an in-depth analysis on the interplay between DP and Byzantine resilience has been ignored, leading to unsatisfactory performance. Specifically, for the random noise introduced by DP, previous works strive to reduce its seemingly detrimental impact on the Byzantine aggregation. In contrast, we leverage the random noise to construct a first-stage aggregation that effectively rejects many existing Byzantine attacks. Moreover, based on another property of our DP variant, we form a second-stage aggregation which provides a final sound filtering. Our protocol follows the principle of co-designing both DP and Byzantine resilience. We provide both theoretical proof and empirical experiments to show our protocol is effective: retaining high accuracy while preserving the DP guarantee and Byzantine resilience. Compared with the previous work, our protocol 1) achieves significantly higher accuracy even in a high privacy regime; 2) works well even when up to 90% distributive workers are Byzantine.}
}


@article{DBLP:journals/pacmmod/Fathollahzadeh023,
	author = {Saeed Fathollahzadeh and
                  Matthias Boehm},
	title = {{GIO:} Generating Efficient Matrix and Frame Readers for Custom Data
                  Formats by Example},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {120:1--120:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589265},
	doi = {10.1145/3589265},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Fathollahzadeh023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data Scientists deal with a wide variety of file data formats and data representations. Probably the most difficult to handle are custom data formats that liberally define their own particular flat or nested structure with multiple custom delimiters, multi-line records, or undocumented semantics of attribute sequences, co-appearances, and repetitions. As a prerequisite for exploratory ML model training, data scientists need to map these data representations into regular frames or matrices. Unfortunately, existing tools and frameworks provide only limited support for aiding this process, which causes redundant manual efforts and unnecessary data quality issues. In this paper, we initiate work on automatic matrix and frame reader generation by example. A user provides a sample of raw text data and its mapped matrix or frame representation. Our GIO framework then first identifies the mapping rules from raw to structured data, and subsequently generates source code of an efficient, multi-threaded reader for reading full raw datasets of this format. In order to facilitate manual improvements, both the mapping rules, and generated reader can be modified as needed. Our experiments show that GIO is able to correctly identify the mapping rules for basic text formats like CSV, LibSVM, MatrixMarket; custom text formats from publishing, automotive, and health care; as well as various nested formats such as JSON and XML. Additionally, the automatically generated readers yield competitive performance compared to hand-coded readers and tuned libraries like RapidJSON.}
}


@article{DBLP:journals/pacmmod/BlacherKSLLG23,
	author = {Mark Blacher and
                  Julien Klaus and
                  Christoph Staudt and
                  S{\"{o}}ren Laue and
                  Viktor Leis and
                  Joachim Giesen},
	title = {Efficient and Portable Einstein Summation in {SQL}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {121:1--121:19},
	year = {2023},
	url = {https://doi.org/10.1145/3589266},
	doi = {10.1145/3589266},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BlacherKSLLG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computational problems ranging from artificial intelligence to physics require efficient computations of large tensor expressions. These tensor expressions can often be represented in Einstein notation. To evaluate tensor expressions in Einstein notation, that is, for the actual Einstein summation, usually external libraries are used. Surprisingly, Einstein summation operations on tensors fit well with fundamental SQL constructs. We show that by applying only four mapping rules and a simple decomposition scheme using common table expressions, large tensor expressions in Einstein notation can be translated to portable and efficient SQL code. The ability to execute large Einstein summation queries opens up new possibilities to process data within SQL. We demonstrate the power of Einstein summation queries on four use cases, namely querying triplestore data, solving Boolean satisfiability problems, performing inference in graphical models, and simulating quantum circuits. The performance of Einstein summation queries, however, depends on the query engine implemented in the database system. Therefore, supporting efficient Einstein summation computations in database systems presents new research challenges for the design and implementation of query engines.}
}


@article{DBLP:journals/pacmmod/ZhangW0P23,
	author = {Juntao Zhang and
                  Sheng Wang and
                  Yuan Sun and
                  Zhiyong Peng},
	title = {Prerequisite-driven Fair Clustering on Heterogeneous Information Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {122:1--122:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589267},
	doi = {10.1145/3589267},
	timestamp = {Tue, 11 Feb 2025 08:46:07 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangW0P23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies the problem of fair clustering on heterogeneous information networks (HINs) by considering constraints on structural and sensitive attributes. We propose a Prerequisite-driven Fair Clustering (PDFC ) algorithm to solve this problem. Specifically, we define the structural constraint on the connection among nodes in HINs by combining meta-paths and prerequisite meta-paths and introduce Fairlets as the balance constraint. Under two constraints, we learn node embeddings based on graph models and perform theCholesky decomposition to obtain their orthogonal embeddings. We fuse node embeddings under constraints, define the loss function of PDFC, and perform k-means to achieve clustering. In addition, we design an update strategy of the adjacency matrix to achieve dynamic PDFC over time. Compared with several fair clustering algorithms on three real-world datasets, our experimental results verify the effectiveness and efficiency of PDFC.}
}


@article{DBLP:journals/pacmmod/0007S023,
	author = {Wei Dong and
                  Dajun Sun and
                  Ke Yi},
	title = {Better than Composition: How to Answer Multiple Relational Queries
                  under Differential Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {123:1--123:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589268},
	doi = {10.1145/3589268},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0007S023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Answering relational queries under differential privacy has attracted a lot of attention in recent years due to growing concerns on personal privacy, and instance-optimal mechanisms have been developed for a single query. However, most real-world data analytical tasks require multiple queries to be answered under a total privacy budget. The standard solution to extend the single-query mechanism to multiple queries is via privacy composition. However, we observe that this may yield an error bound that could be a d0.5-factor worse from the optimal, where d is the number of queries. In this paper, we present a different, more holistic approach that closes this gap. In addition to theoretical optimality, our new mechanism also significantly outperforms privacy composition in practice, especially on more skewed data and large d.}
}


@article{DBLP:journals/pacmmod/SheoranCCWVP23,
	author = {Nikhil Sheoran and
                  Supawit Chockchowwat and
                  Arav Chheda and
                  Suwen Wang and
                  Riya Verma and
                  Yongjoo Park},
	title = {A Step Toward Deep Online Aggregation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {124:1--124:28},
	year = {2023},
	url = {https://doi.org/10.1145/3589269},
	doi = {10.1145/3589269},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SheoranCCWVP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For exploratory data analysis, it is often desirable to know what answers you are likely to get before actually obtaining those answers. This can potentially be achieved by designing systems to offer the estimates of a data operation result-say op(data)-earlier in the process based on partial data processing. Those estimates continuously refine as more data is processed and finally converge to the exact answer. Unfortunately, the existing techniques-called Online Aggregation (OLA)-are limited to a single operation; that is, we cannot obtain the estimates for op(op(data)) or op(...(op(data))). If this Deep OLA becomes possible, data analysts will be able to explore data more interactively using complex cascade operations. In this work, we take a step toward Deep OLA with evolving data frames (edf), a novel data model to offer OLA for nested ops-op(...(op(data)))-by representing an evolving structured data (with converging estimates) that is closed under set operations. That is, op(edf) produces yet another edf; thus, we can freely apply successive operations to edf and obtain an OLA output for each op. We evaluate its viability with Wake, an edf-based OLA system, by examining against state-of-the-art OLA and non-OLA systems. In our experiments on TPC-H dataset, Wake produces its first estimates 4.93× faster (median)-with 1.3× median slowdown for exact answers-compared to conventional systems. Besides its generality, Wake is also 1.92× faster (median) than existing OLA systems in producing estimates of under 1% relative errors.}
}


@article{DBLP:journals/pacmmod/000100J0023,
	author = {Zhichen Lai and
                  Dalin Zhang and
                  Huan Li and
                  Christian S. Jensen and
                  Hua Lu and
                  Yan Zhao},
	title = {LightCTS: {A} Lightweight Framework for Correlated Time Series Forecasting},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {125:1--125:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589270},
	doi = {10.1145/3589270},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/000100J0023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Correlated time series (CTS) forecasting plays an essential role in many practical applications, such as traffic management and server load control. Many deep learning models have been proposed to improve the accuracy of CTS forecasting. However, while models have become increasingly complex and computationally intensive, they struggle to improve accuracy. Pursuing a different direction, this study aims instead to enable much more efficient, lightweight models that preserve accuracy while being able to be deployed on resource-constrained devices. To achieve this goal, we characterize popular CTS forecasting models and yield two observations that indicate directions for lightweight CTS forecasting. On this basis, we propose the LightCTS framework that adopts plain stacking of temporal and spatial operators instead of alternate stacking that is much more computationally expensive. Moreover, LightCTS features light temporal and spatial operator modules, called L-TCN and GL-Former, that offer improved computational efficiency without compromising their feature extraction capabilities. LightCTS also encompasses a last-shot compression scheme to reduce redundant temporal features and speed up subsequent computations. Experiments with single-step and multi-step forecasting benchmark datasets show that LightCTS is capable of nearly state-of-the-art accuracy at much reduced computational and storage overheads.}
}


@article{DBLP:journals/pacmmod/XiaoL23,
	author = {Xingxing Xiao and
                  Jianzhong Li},
	title = {rkHit: Representative Query with Uncertain Preference},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {126:1--126:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589271},
	doi = {10.1145/3589271},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/XiaoL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A top-k query retrieves the k tuples with highest scores according to a user preference, defined as a scoring function. It is difficult for a user to precisely specify the scoring function. Instead, obtaining the distribution on scoring functions, i.e., the preference distribution, has been extensively explored in many fields. Motivated by this, we introduce the uniform (r,k)-hit (UrkHit) problem. Given a preference distribution, UrkHit aims to select a representative set of r tuples to maximize the probability of containing a tuple attractive to the user. We say a tuple attracts a user, if it is a top-k tuple for the scoring function adopted by the user. Further, we generalize UrkHit and propose the (r,k)-hit (rkHit) problem with an additional penalty function to model the user satisfaction with the tuple ranked i-th. rkHit aims to maximize the expected user satisfaction with the representative set. In 2D space, we design an exact algorithm 2DH for rkHit, indicating rkHit is in P for d=2. We show that rkHit is NP-hard when d\\ge3. In 3D space, assuming a uniform preference distribution, we propose a (1-1/e)-approximation algorithm 3DH based on space partitioning. In addition, we propose an approximate algorithm MDH suitable for any dimension and distribution, which creatively combines the ideas of sampling and clustering. It relaxes the approximation guarantee slightly. Comprehensive experiments demonstrate the efficiency and effectiveness of our algorithms.}
}


@article{DBLP:journals/pacmmod/YangLZW0023,
	author = {Yajun Yang and
                  Hanxiao Li and
                  Xiangju Zhu and
                  Junhu Wang and
                  Xin Wang and
                  Hong Gao},
	title = {HR-Index: An Effective Index Method for Historical Reachability Queries
                  over Evolving Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {127:1--127:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589272},
	doi = {10.1145/3589272},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangLZW0023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reachability query is a fundamental problem and has been well studied on static graphs. However, in the real world, the graphs are not static but always evolving over time. In this paper, we study the problem of historical reachability query on evolving graphs. We propose a novel index, named HR-Index, which integrates complete and correct historical reachability information of the evolving graph. A historical reachability query on an evolving graph can be converted into a static reachability query on its HR-Index and thus query efficiency can be improved significantly. We also propose two optimization techniques to reduce the size of HR-Index effectively. We confirm the effectiveness and efficiency of our method through conducting extensive experiments on real-life datasets. Experimental results show both vertex and edge size of HR-Index are far smaller than that of the evolving graphs and our method has at least an order of magnitude improvement in time and space efficiency compared to the state-of-the-art method.}
}


@article{DBLP:journals/pacmmod/GrafbergerGS23,
	author = {Stefan Grafberger and
                  Paul Groth and
                  Sebastian Schelter},
	title = {Automating and Optimizing Data-Centric What-If Analyses on Native
                  Machine Learning Pipelines},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {128:1--128:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589273},
	doi = {10.1145/3589273},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GrafbergerGS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software systems that learn from data with machine learning (ML) are used in critical decision-making processes. Unfortunately, real-world experience shows that the pipelines for data preparation, feature encoding and model training in ML systems are often brittle with respect to their input data. As a consequence, data scientists have to run different kinds of data centric what-if analyses to evaluate the robustness and reliability of such pipelines, e.g., with respect to data errors or preprocessing techniques. These what-if analyses follow a common pattern: they take an existing ML pipeline, create a pipeline variant by introducing a small change, and execute this pipeline variant to see how the change impacts the pipeline's output score. The application of existing analysis techniques to ML pipelines is technically challenging as they are hard to integrate into existing pipeline code and their execution introduces large overheads due to repeated work. We propose mlwhatif to address these integration and efficiency challenges for data-centric what-if analyses on ML pipelines. mlwhatif enables data scientists to declaratively specify what-if analyses for an ML pipeline, and to automatically generate, optimize and execute the required pipeline variants. Our approach employs pipeline patches to specify changes to the data, operators and models of a pipeline. Based on these patches, we define a multi-query optimizer for efficiently executing the resulting pipeline variants jointly, with four subsumption-based optimization rules. Subsequently, we detail how to implement the pipeline variant generation and optimizer of mlwhatif. For that, we instrument native ML pipelines written in Python to extract dataflow plans with re-executable operators. We experimentally evaluate mlwhatif, and find that its speedup scales linearly with the number of pipeline variants in applicable cases, and is invariant to the input data size. In end-to-end experiments with four analyses on more than 60 pipelines, we show speedups of up to 13x compared to sequential execution, and find that the speedup is invariant to the model and featurization in the pipeline. Furthermore, we confirm the low instrumentation overhead of mlwhatif.}
}


@article{DBLP:journals/pacmmod/XuCPXB23,
	author = {Lyu Xu and
                  Byron Choi and
                  Yun Peng and
                  Jianliang Xu and
                  Sourav S. Bhowmick},
	title = {A Framework for Privacy Preserving Localized Graph Pattern Query Processing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {129:1--129:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589274},
	doi = {10.1145/3589274},
	timestamp = {Wed, 18 Dec 2024 13:51:13 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuCPXB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies privacy preserving graph pattern query services in a cloud computing paradigm. In such a paradigm, data owner stores the large data graph to a powerful cloud hosted by a service provider (SP) and users send their queries to SP for query processing. However, as SP may not always be trusted, the sensitive information of users' queries, importantly, the query structures, should be protected. In this paper, we study how to outsource the localized graph pattern queries (LGPQs) on the SP side with privacy preservation. LGPQs include a rich set of semantics, such as subgraph homomorphism, subgraph isomorphism, and strong simulation, for which each matched graph pattern is located in a subgraph called ball that have a restriction on its size. To provide privacy preserving query service for LGPQs, this paper proposes the first framework, called Prilo, that enables users to privately obtain the query results. To further optimize Prilo, we propose Prilo* that comprises the first bloom filter for trees in the trust execution environment (TEE) on SP, a query-oblivious twiglet-based technique for pruning non-answers, and a secure retrieval scheme of balls that enables user to obtain query results early. We conduct detailed experiments on real world datasets to show that Prilo* is on average 4x faster than the baseline, and meanwhile, preserves query privacy.}
}


@article{DBLP:journals/pacmmod/HuangZCS23,
	author = {Silu Huang and
                  Erkang Zhu and
                  Surajit Chaudhuri and
                  Leonhard Spiegelberg},
	title = {T-Rex: Optimizing Pattern Search on Time Series},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {130:1--130:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589275},
	doi = {10.1145/3589275},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangZCS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pattern search is an important class of queries for time series data. Time series patterns often match variable-length segments with a large search space, thereby posing a significant performance challenge. The existing pattern search systems, for example, SQL query engines supporting MATCH_RECOGNIZE, are ineffective in pruning the large search space of variable-length segments. In many cases, the issue is due to the use of a restrictive query language modeled on time series points and a computational model that limits search space pruning. We built T-ReX to address this problem using two main building blocks: first, a MATCH_RECOGNIZE language extension that exposes the notion of segment variable and adds new operators, lending itself to better optimization; second, an executor capable of pruning the search space of matches and minimizing total query time using an optimizer. We conducted experiments using 5 real-world datasets and 11 query templates, including those from existing works. T-ReX outperformed an optimized NFA-based pattern search executor by 6x in median query time and an optimized tree-based executor by 19X.}
}


@article{DBLP:journals/pacmmod/0001NLB23,
	author = {Tobias Ziegler and
                  Jacob Nelson{-}Slivon and
                  Viktor Leis and
                  Carsten Binnig},
	title = {Design Guidelines for Correct, Efficient, and Scalable Synchronization
                  using One-Sided {RDMA}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {131:1--131:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589276},
	doi = {10.1145/3589276},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0001NLB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Remote data structures built with one-sided Remote Direct Memory Access (RDMA) are at the heart of many disaggregated database management systems today. Concurrent access to these data structures by thousands of remote workers necessitates a highly efficient synchronization scheme. Remarkably, our investigation reveals that existing synchronization schemes display substantial variations in performance and scalability. Even worse, some schemes do not correctly synchronize, resulting in rare and hard-to-detect data corruption. Motivated by these observations, we conduct the first comprehensive analysis of one-sided synchronization techniques and provide general principles for correct synchronization using one-sided RDMA. Our research demonstrates that adherence to these principles not only guarantees correctness but also results in substantial performance enhancements.}
}


@article{DBLP:journals/pacmmod/MaBCT23,
	author = {Jiebing Ma and
                  Sourav S. Bhowmick and
                  Byron Choi and
                  Lester Tay},
	title = {Theories and Principles Matter: Towards Visually Appealing and Effective
                  Abstraction of Property Graph Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {132:1--132:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589277},
	doi = {10.1145/3589277},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MaBCT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing visual abstraction of a property graph query by representing it as a labeled atomic graph (LAG) has great potential to democratize the usage of property graph databases as it enables user-friendly visual query formulation without demanding the need to learn a property graph query language e.g., Cypher. Unfortunately, existing LAG-based query interfaces do not embrace HCI principles and psychology theories to inform their design and as a result may have adverse impact on their usability and aesthetics. In this paper, we depart from the classical theory- and principles-oblivious LAG abstraction to present a novel theory-informed visual abstraction called labeled composite graph (LCG) to address this limitation. It realizes a novel and extensible visual shape definition language called VEDA to create and maintain an LCG systematically, guided by a variety of theories and principles from HCI, visualization and psychology. We build a novel LCG-based visual property graph query interface for Cypher called SIERRA and demonstrate through a user study its superiority to an industrial-strength LAG-based query interface for property graphs w.r.t. usability, aesthetics and efficient query formulation.}
}


@article{DBLP:journals/pacmmod/Sun0LX23,
	author = {Zitan Sun and
                  Xin Huang and
                  Qing Liu and
                  Jianliang Xu},
	title = {Efficient Star-based Truss Maintenance on Dynamic Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {133:1--133:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589278},
	doi = {10.1145/3589278},
	timestamp = {Tue, 21 May 2024 17:42:37 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Sun0LX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {K-truss is a useful notion of dense subgraphs, which can represent cohesive parts of a graph in a hierarchical way. In practice, in order to enable various truss-based applications to answer queries faster, the edge trussnesses are computed in advance. However, real-world graphs may not always be static and often have edges inserted or removed, leading to costly truss maintenance of recomputing all edge trussnesses. In this paper, we focus on dynamic graphs with star insertions/deletions, where a star insertion can represent a newly joined user with friend connections in social networks or a recently published paper with cited references in citation networks. To tackle such star-based truss maintenance, we propose a new structure of AffBall based on the local structure of an inserted/deleted star motif. With AffBall, we make use of the correlation of inserted edges to compute the trussnesses of the inner edges surrounding the star. Then, we analyze the onion layer of k-truss and conduct truss maintenance for the edges beyond the star, which can be efficiently achieved with a time complexity related to the number of the edges that change the onion layer. Moreover, we extend star-based truss maintenance to handle general updates and single-edge insertions/deletions. Extensive experiments on real-world dynamic graphs verify the effectiveness and efficiency of proposed algorithms against state-of-the-art truss maintenance algorithms.}
}


@article{DBLP:journals/pacmmod/ParkTH23,
	author = {Yeonsu Park and
                  Byungchul Tak and
                  Wook{-}Shin Han},
	title = {QaaD (Query-as-a-Data): Scalable Execution of Massive Number of Small
                  Queries in Spark},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {134:1--134:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589279},
	doi = {10.1145/3589279},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ParkTH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spark big data processing platform is heavily used in today's IT services for various critical applications such as machine learning tasks for service recommendations or massive volumes of raw sales data analysis. Spark is designed to deliver high performance by enabling a high degree of parallelism while processing various heavy-weight queries that require homogeneous operations on large data. However, it has been observed that workloads made of small and short-running queries coming from various sources are becoming dominant in practice. Unfortunately, the current Spark architecture is unfit to process workloads made of a large number of small queries optimally due to excessive I/Os with small computations. We present a technique, called QaaD, that addresses this problem fundamentally by applying i) transparent conversion of workloads made of small queries into one with large queries and ii) dynamic partition size adjustment for runtime overhead minimization. For this, we introduce a new abstraction, microRDD, to support our design of query merging, the embedding of queries as part of data, and an opportunistic sharing of common input data among queries. Comprehensive evaluation using real-world data shows that QaaD is able to deliver 10.6x to 36.6x speed-up against standard Spark executions for small query workloads.}
}


@article{DBLP:journals/pacmmod/ShresthaHTP23,
	author = {Rajesh Shrestha and
                  Omeed Habibelahian and
                  Arash Termehchy and
                  Paolo Papotti},
	title = {Exploratory Training: When Annotators Learn About Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {135:1--135:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589280},
	doi = {10.1145/3589280},
	timestamp = {Mon, 06 Jan 2025 07:42:37 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShresthaHTP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data systems often present examples and solicit labels from users to learn a target model, i.e., active learning. However, due to the complexity of the underlying data, users may not initially have a perfect understanding of the effective model and do not know the accurate labeling. For example, a user who is training a model for detecting noisy or abnormal values may not perfectly know the properties of typical and clean values in the data. Users may improve their knowledge about the data and target model as they observe examples during training. As users gradually learn about the data and model, they may revise their labeling strategies. Current systems assume that users always provide correct labeling with potentially a fixed and small chance of annotation mistakes. Nonetheless, if the trainer revises its belief during training, such mistakes become significant and non-stationarity. Hence, current systems consume incorrect labels and may learn inaccurate models. In this paper, we build theoretical underpinnings and design algorithms to develop systems that collaborate with users to learn the target model accurately and efficiently. At the core of our proposal, a game-theoretic framework models the joint learning of user and system to reach a desirable eventual stable state, where both user and system share the same belief about the target model. We extensively evaluate our system using user studies over various real-world datasets and show that our algorithms lead to accurate results with a smaller number of interactions compared to existing methods.}
}


@article{DBLP:journals/pacmmod/YanLH23,
	author = {Cong Yan and
                  Yin Lin and
                  Yeye He},
	title = {Predicate Pushdown for Data Science Pipelines},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {136:1--136:28},
	year = {2023},
	url = {https://doi.org/10.1145/3589281},
	doi = {10.1145/3589281},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YanLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicate pushdown is a widely adopted query optimization. Existing systems and prior work mostly use pattern-matching rules to decide when a predicate can be pushed through certain operators like join or groupby. However, challenges arise in optimizing for data science pipelines due to the widely used non-relational operators and user-defined functions (UDF) that existing rules would fail to cover. In this paper, we present MagicPush, which decides predicate pushdown using a search-verification approach.MagicPush searches for candidate predicates on pipeline input, which is often not the same as the predicate to be pushed down, and verifies that the pushdown does not change pipeline output with full correctness guarantees. Our evaluation on TPC-H queries and 200 real-world pipelines sampled from GitHub Notebooks shows that MagicPush substantially outperforms a strong baseline that uses a union of rules from prior work - it is able to discover new pushdown opportunities and better optimize 42 real-world pipelines with up to 99% reduction in running time, while discovering all pushdown opportunities found by the existing baseline on remaining cases.}
}


@article{DBLP:journals/pacmmod/GaoL23,
	author = {Jianyang Gao and
                  Cheng Long},
	title = {High-Dimensional Approximate Nearest Neighbor Search: with Reliable
                  and Efficient Distance Comparison Operations},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {137:1--137:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589282},
	doi = {10.1145/3589282},
	timestamp = {Tue, 22 Oct 2024 20:38:18 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GaoL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate K nearest neighbor (AKNN) search in the high-dimensional Euclidean vector space is a fundamental and challenging problem. We observe that in high-dimensional space, the time consumption of nearly all AKNN algorithms is dominated by that of the distance comparison operations (DCOs). For each operation, it scans full dimensions of an object and thus, runs in linear time wrt the dimensionality. To speed it up, we propose a randomized algorithm named ADSampling which runs in logarithmic time wrt the dimensionality for the majority of DCOs and succeeds with high probability. In addition, based on ADSampling we develop one generic and two algorithm-specific techniques as plugins to enhance existing AKNN algorithms. Both theoretical and empirical studies confirm that: (1) our techniques introduce nearly no accuracy loss and (2) they consistently improve the efficiency.}
}


@article{DBLP:journals/pacmmod/DaiLYLZW23,
	author = {Qiangqiang Dai and
                  Rong{-}Hua Li and
                  Xiaowei Ye and
                  Meihao Liao and
                  Weipeng Zhang and
                  Guoren Wang},
	title = {Hereditary Cohesive Subgraphs Enumeration on Bipartite Graphs: The
                  Power of Pivot-based Approaches},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {138:1--138:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589283},
	doi = {10.1145/3589283},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DaiLYLZW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding cohesive subgraphs from a bipartite graph is a fundamental operator in bipartite graph analysis. In this paper, we focus on the problem of mining cohesive subgraphs from a bipartite graph that satisfy a hereditary property. Here a cohesive subgraph meets the hereditary property if all of its subgraphs satisfy the same property as itself. We show that several important cohesive subgraph models, such as maximal biclique and maximal k-biplex, satisfy the hereditary property. The problem of enumerating all maximal hereditary subgraphs was known to be NP-hard. To solve this problem, we first propose a novel and general pivot-based enumeration framework to efficiently enumerate all maximal hereditary subgraphs in a bipartite graph. Then, based on our general framework, we develop a new pivot-based algorithm with several pruning techniques to enumerate all maximal bicliques. We prove that the worst-case time complexity of our pivot-based maximal biclique enumeration algorithm is O(m x 2n/2 ) (or O(m\\times 1.414^n)) which is near optimal since there exist up to O(2n/2 ) maximal bicliques in a bipartite graph with n vertices and m edges. Moreover, we also show that our algorithm can achieve polynomial-delay time complexity with a slight modification. Third, on the basis of our general framework, we also devise a novel pivot-based algorithm with several non-trivial pruning techniques to enumerate maximal k-biplexes in a bipartite graph. Finally, we conduct extensive experiments using 11 real-world bipartite graphs to evaluate the proposed algorithms. The results show that our pivot-based solutions can achieve one order of magnitude (three orders of magnitude) faster than the state-of-the-art maximal biclique enumeration algorithms (maximal k-biplex enumeration algorithms).}
}


@article{DBLP:journals/pacmmod/LanBCB23,
	author = {Hai Lan and
                  Zhifeng Bao and
                  J. Shane Culpepper and
                  Renata Borovica{-}Gajic},
	title = {Updatable Learned Indexes Meet Disk-Resident {DBMS} - From Evaluations
                  to Design Choices},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {139:1--139:22},
	year = {2023},
	url = {https://doi.org/10.1145/3589284},
	doi = {10.1145/3589284},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LanBCB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although many updatable learned indexes have been proposed in recent years, whether they can outperform traditional approaches on disk remains unknown. In this study, we revisit and implement four state-of-the-art updatable learned indexes on disk, and compare them against the B+-tree under a wide range of settings. Through our evaluation, we make some key observations: 1) Overall, the B+-tree performs well across a range of workload types and datasets. 2) A learned index could outperform B+-tree or other learned indexes on disk for a specific workload. For example, PGM achieves the best performance in write-only workloads while LIPP significantly outperforms others in lookup-only workloads. We further conduct a detailed performance analysis to reveal the strengths and weaknesses of these learned indexes on disk. Moreover, we summarize the observed common shortcomings in five categories and propose four design principles to guide future design of on-disk, updatable learned indexes: (1) reducing the index's tree height, (2) better data structures to lower operation overheads, (3) improving the efficiency of scan operations, and (4) more efficient storage layout.}
}


@article{DBLP:journals/pacmmod/DayanBRP23,
	author = {Niv Dayan and
                  Ioana O. Bercea and
                  Pedro Reviriego and
                  Rasmus Pagh},
	title = {InfiniFilter: Expanding Filters to Infinity and Beyond},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {140:1--140:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589285},
	doi = {10.1145/3589285},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DayanBRP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Filter data structures have been used ubiquitously since the 1970s to answer approximate set-membership queries in various areas of computer science including architecture, networks, operating systems, and databases. Such filters need to be allocated with a given capacity in advance to provide a guarantee over the false positive rate. In many applications, however, the data size is not known in advance, requiring filters to dynamically expand. This paper shows that existing methods for expanding filters exhibit at least one of the following flaws: (1) they entail an expensive scan over the whole data set, (2) they require a lavish memory footprint, (3) their query, delete and/or insertion performance plummets, (4) their false positive rate skyrockets, and/or (5)~they cannot expand indefinitely. We introduce InfiniFilter, a new method for expanding filters that addresses these shortcomings. InfiniFilter is a hash table that stores a fingerprint for each entry. It doubles in size when it reaches capacity, and it sacrifices one bit from each fingerprint to map it to the expanded hash table. The core novelty is a new and flexible hash slot format that sets longer fingerprints to newer entries. This keeps the average fingerprint length long and thus the false positive rate stable. At the same time, InfiniFilter provides stable insertion/query/delete performance as it is comprised of a unified hash table. We implement InfiniFilter on top of Quotient Filter, and we demonstrate theoretically and empirically that it offers superior cost properties compared to existing methods: it better scales performance, the false positive rate, and the memory footprint, all at the same time.}
}


@article{DBLP:journals/pacmmod/HuangB23,
	author = {Shixun Huang and
                  Zhifeng Bao},
	title = {Shortest Paths Discovery in Uncertain Networks via Transfer Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {141:1--141:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589286},
	doi = {10.1145/3589286},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to various reasons such as noisy measurement and privacy preservation, a network/graph is often uncertain such that each edge in the network has a probability of existence. In this paper, we study finding the most probable shortest path which has the highest probability of being the shortest path between a given pair of nodes in an uncertain network. Despite significant progress being made, this problem still suffers from the efficiency and scalability issue. To solve this problem, the state-of-the-art adopts a two-phase approach where Phase 1 generates some candidate paths and Phase 2 estimates their probabilities of being the shortest path and returns the one with the highest probability as the solution. Notably, Phase 2 requires a large number of simulations over all edges in the network and can easily dominate the cost of the whole process. In this paper, we aim to resolve the efficiency and scalability issue by optimizing Phase 2. Specifically, we first propose a non-learning based fast approximation technique which significantly reduces the number of samples for the probability estimation in each simulation. Afterwards, we further propose a learning-based method which can directly estimate the probability of each candidate path without costly simulations. Extensive experiments show that (1) compared to the state-of-the-art, our fast approximation technique and learning-based method can achieve up to 5x and 210x speedups in Phase 2 respectively while maintaining highly competitive or even equivalent results, (2) the training process is highly scalable and (3) the prediction function can work effectively under the problem settings different from the one it was trained.}
}


@article{DBLP:journals/pacmmod/CaiXC23,
	author = {Kuntai Cai and
                  Xiaokui Xiao and
                  Graham Cormode},
	title = {PrivLava: Synthesizing Relational Data with Foreign Keys under Differential
                  Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {142:1--142:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589287},
	doi = {10.1145/3589287},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/CaiXC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Answering database queries while preserving privacy is an important problem that has attracted considerable research attention in recent years. A canonical approach to this problem is to use synthetic data. That is, we replace the input database R with a synthetic database R* that preserves the characteristics of R, and use R* to answer queries. Existing solutions for relational data synthesis, however, either fail to provide strong privacy protection, or assume that R contains a single relation. In addition, it is challenging to extend the existing single-relation solutions to the case of multiple relations, because they are unable to model the complex correlations induced by the foreign keys. Therefore, multi-relational data synthesis with strong privacy guarantees is an open problem. In this paper, we address the above open problem by proposing PrivLava, the first solution for synthesizing relational data with foreign keys under differential privacy, a rigorous privacy framework widely adopted in both academia and industry. The key idea of PrivLava is to model the data distribution in R using graphical models, with latent variables included to capture the inter-relational correlations caused by foreign keys. We show that PrivLava supports arbitrary foreign key references that form a directed acyclic graph, and is able to tackle the common case when R contains a mixture of public and private relations. Extensive experiments on census data sets and the TPC-H benchmark demonstrate that PrivLava significantly outperforms its competitors in terms of the accuracy of aggregate queries processed on the synthetic data.}
}


@article{DBLP:journals/pacmmod/WanXLJ0023,
	author = {Xinchen Wan and
                  Kaiqiang Xu and
                  Xudong Liao and
                  Yilun Jin and
                  Kai Chen and
                  Xin Jin},
	title = {Scalable and Efficient Full-Graph {GNN} Training for Large Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {143:1--143:23},
	year = {2023},
	url = {https://doi.org/10.1145/3589288},
	doi = {10.1145/3589288},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WanXLJ0023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have emerged as powerful tools to capture structural information from graph-structured data, achieving state-of-the-art performance on applications such as recommendation, knowledge graph, and search. Graphs in these domains typically contain hundreds of millions of nodes and billions of edges. However, previous GNN systems demonstrate poor scalability because large and interleaved computation dependencies in GNN training cause significant overhead in current parallelization methods. We present G3, a distributed system that can efficiently train GNNs over billion-edge graphs at scale. G3 introduces GNN hybrid parallelism which synthesizes three dimensions of parallelism to scale out GNN training by sharing intermediate results peer-to-peer in fine granularity, eliminating layer-wise barriers for global collective communication or neighbor replications as seen in prior works. G3 leverages locality-aware iterative partitioning and multi-level pipeline scheduling to exploit acceleration opportunities by distributing balanced workload among workers and overlapping computation with communication in both inter-layer and intra-layer training processes. We show via a prototype implementation and comprehensive experiments that G3 can achieve as much as 2.24x speedup in a 16-node cluster, and better final accuracy over prior works.}
}


@article{DBLP:journals/pacmmod/Treder-Tschechlov23,
	author = {Dennis Treder{-}Tschechlov and
                  Manuel Fritz and
                  Holger Schwarz and
                  Bernhard Mitschang},
	title = {{ML2DAC:} Meta-Learning to Democratize AutoML for Clustering Analysis},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {144:1--144:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589289},
	doi = {10.1145/3589289},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Treder-Tschechlov23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analysts often struggle with the combined algorithm selection and hyperparameter optimization problem, a.k.a. CASH problem in literature. Typically, they execute several algorithms with varying hyperparameter settings to find configurations that show valuable results. Efficiently finding these configurations is a major challenge. In clustering analyses, analysts face the additional challenge to select a cluster validity index that allows them to evaluate clustering results in a purely unsupervised fashion. Many different cluster validity indices exist and each one has its benefits depending on the dataset characteristics. While experienced analysts might address these challenges using their domain knowledge and experience, especially novice analysts struggle with them. In this paper, we propose a new meta-learning approach to address these challenges. Our approach uses knowledge from past clustering evaluations to apply strategies that experienced analysts would exploit. In particular, we use meta-learning to (a) select a suitable clustering validity index, (b) efficiently select well-performing clustering algorithm and hyperparameter configurations, and (c) reduce the search space to suitable clustering algorithms. In the evaluation, we show that our approach significantly outperforms state-of-the-art approaches regarding accuracy and runtime.}
}


@article{DBLP:journals/pacmmod/WangWCZZ0F023,
	author = {Yunhai Wang and
                  Yuchun Wang and
                  Xin Chen and
                  Yue Zhao and
                  Fan Zhang and
                  Eugene Wu and
                  Chi{-}Wing Fu and
                  Xiaohui Yu},
	title = {{OM3:} An Ordered Multi-level Min-Max Representation for Interactive
                  Progressive Visualization of Time Series},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {145:1--145:24},
	year = {2023},
	url = {https://doi.org/10.1145/3589290},
	doi = {10.1145/3589290},
	timestamp = {Wed, 12 Jul 2023 16:44:02 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangWCZZ0F023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a novel multi-level representation of time series called OM3 that facilitates efficient interactive progressive visualization of large data stored in a database and supports various interactions such as resizing, panning, zooming, and visual query. Based on our proposed line-segment aggregation, this representation can produce error-free line visualizations that preserve the shape of a time series in windows of arbitrary sizes. To reduce the interaction latency, we develop an incremental tree-based query strategy to support progressive visualizations, allowing a finer control on the accuracy-time tradeoff. We quantitatively compare OM3 with state-of-the-art methods, including a method implemented on a leading time-series database InfluxDB, in two settings with databases residing either in the local area network or on the cloud. Results show that OM^3 maintains a low latency within 300~ms on the web browser and a high data reduction ratio regardless of the data size (ranging from millions to billions of records), achieving around 1,000 times faster than the state-of-the-art methods on the largest dataset experimented with.}
}


@article{DBLP:journals/pacmmod/Wang0LZ023,
	author = {Yexin Wang and
                  Zhi Yang and
                  Junqi Liu and
                  Wentao Zhang and
                  Bin Cui},
	title = {Scapin: Scalable Graph Structure Perturbation by Augmented Influence
                  Maximization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {146:1--146:21},
	year = {2023},
	url = {https://doi.org/10.1145/3589291},
	doi = {10.1145/3589291},
	timestamp = {Tue, 11 Feb 2025 20:50:07 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Wang0LZ023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generating data perturbations to graphs has become a useful tool for analyzing the robustness of Graph Neural Networks (GNNs). However, existing model-driven methodologies can be prohibitively expensive to apply in large graphs, which hinders the understanding of GNN robustness at scale. In this paper, we present Scapin, a data-driven methodology that opens up a new perspective by connecting graph structure perturbation for GNNs with augmented influence maximization-to either facilitate desirable spreads or curtail undesirable ones by adding or deleting a small set of edges. This connection not only allows us to perform data perturbation on GNNs with computation scalability but also provides nice interpretations. To transform such connections into efficient perturbation approaches for the new GNN setting, Scapin introduces a novel edge influence model, decomposed influence maximization objectives, and a principled algorithm for edge addition by exploiting submodularity of the objectives. Empirical studies demonstrate that Scapin can give orders of magnitude improvement over state-of-art methods in terms of runtime and memory efficiency, with comparable or even better performance.}
}


@article{DBLP:journals/pacmmod/GuF00JM023,
	author = {Zihui Gu and
                  Ju Fan and
                  Nan Tang and
                  Lei Cao and
                  Bowen Jia and
                  Sam Madden and
                  Xiaoyong Du},
	title = {Few-shot Text-to-SQL Translation using Structure and Content Prompt
                  Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {147:1--147:28},
	year = {2023},
	url = {https://doi.org/10.1145/3589292},
	doi = {10.1145/3589292},
	timestamp = {Mon, 04 Nov 2024 22:25:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GuF00JM023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A common problem with adopting Text-to-SQL translation in database systems is poor generalization. Specifically, when there is limited training data on new datasets, existing few-shot Text-to-SQL techniques, even with carefully designed textual prompts on pre-trained language models (PLMs), tend to be ineffective. In this paper, we present a divide-and-conquer framework to better support few-shot Text-to-SQL translation, which divides Text-to-SQL translation into two stages (or sub-tasks), such that each sub-task is simpler to be tackled. The first stage, called the structure stage, steers a PLM to generate an SQL structure (including SQL commands such as SELECT, FROM, WHERE and SQL operators such as <", ?>") with placeholders for missing identifiers. The second stage, called the content stage, guides a PLM to populate the placeholders in the generated SQL structure with concrete values (including SQL identifies such as table names, column names, and constant values). We propose a hybrid prompt strategy that combines learnable vectors and fixed vectors (i.e., word embeddings of textual prompts), such that the hybrid prompt can learn contextual information to better guide PLMs for prediction in both stages. In addition, we design keyword constrained decoding to ensure the validity of generated SQL structures, and structure guided decoding to guarantee the model to fill correct content. Extensive experiments, by comparing with ten state-of-the-art Text-to-SQL solutions at the time of writing, show that SC-Prompt significantly outperforms them in the few-shot scenario. In particular, on the widely-adopted Spider dataset, given less than 500 labeled training examples (5% of the official training set), SC-Prompt outperforms the previous SOTA methods by around 5% on accuracy.}
}


@article{DBLP:journals/pacmmod/NguyenMA23,
	author = {Cuong D. T. Nguyen and
                  Johann K. Miller and
                  Daniel J. Abadi},
	title = {Detock: High Performance Multi-region Transactions at Scale},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {148:1--148:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589293},
	doi = {10.1145/3589293},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/NguyenMA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many globally distributed data stores need to replicate data across large geographic distances. Since synchronously replicating data across such distances is slow, those systems with high consistency requirements often geo-partition data and direct all linearizable requests to the primary region of the accessed data. This significantly improves performance for workloads where most transactions access data close to where they originate from. However, supporting serializable multi-geo-partition transactions is a challenge, and they often degrade the performance of the whole system. This becomes even more challenging when they conflict with single-partition requests, where optimistic protocols lead to high numbers of aborts, and pessimistic protocols lead to high numbers of distributed deadlocks. In this paper, we describe the design of concurrency control and deadlock resolution protocols, built within a practical, complete implementation of a geographically replicated database system called Detock, that enables processing strictly-serializable multi-region transactions with near-zero performance degradation at extremely high conflict and order of magnitude higher throughput relative to state-of-the art geo-replication approaches, while improving latency by up to a factor of 5.}
}


@article{DBLP:journals/pacmmod/CareyDEJKKMMNVZ23,
	author = {CJ Carey and
                  Travis Dick and
                  Alessandro Epasto and
                  Adel Javanmard and
                  Josh Karlin and
                  Shankar Kumar and
                  Andres Mu{\~{n}}oz Medina and
                  Vahab Mirrokni and
                  Gabriel Henrique Nunes and
                  Sergei Vassilvitskii and
                  Peilin Zhong},
	title = {Measuring Re-identification Risk},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {149:1--149:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589294},
	doi = {10.1145/3589294},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CareyDEJKKMMNVZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compact user representations (such as embeddings) form the backbone of personalization services. In this work, we present a new theoretical framework to measure re-identification risk in such user representations. Our framework, based on hypothesis testing, formally bounds the probability that an attacker may be able to obtain the identity of a user from their representation. As an application, we show how our framework is general enough to model important real-world applications such as the Chrome's Topics API for interest-based advertising. We complement our theoretical bounds by showing provably good attack algorithms for re-identification that we use to estimate the re-identification risk in the Topics API. We believe this work provides a rigorous and interpretable notion of re-identification risk and a framework to measure it that can be used to inform real-world applications.}
}


@article{DBLP:journals/pacmmod/WangWS23,
	author = {Yisu Remy Wang and
                  Max Willsey and
                  Dan Suciu},
	title = {Free Join: Unifying Worst-Case Optimal and Traditional Joins},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {150:1--150:23},
	year = {2023},
	url = {https://doi.org/10.1145/3589295},
	doi = {10.1145/3589295},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangWS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the last decade, worst-case optimal join (WCOJ) algorithms have emerged as a new paradigm for one of the most fundamental challenges in query processing: computing joins efficiently. Such an algorithm can be asymptotically faster than traditional binary joins, all the while remaining simple to understand and implement. However, they have been found to be less efficient than the old paradigm, traditional binary join plans, on the typical acyclic queries found in practice. Some database systems that support WCOJ use a hybrid approach: use WCOJ to process the cyclic subparts of the query (if any), and rely on traditional binary joins otherwise. In this paper we propose a new framework, called Free Join, that unifies the two paradigms. We describe a new type of plan, a new data structure (which unifies the hash tables and tries used by the two paradigms), and a suite of optimization techniques. Our system, implemented in Rust, matches or outperforms both traditional binary joins and WCOJ on standard query benchmarks.}
}


@article{DBLP:journals/pacmmod/Tian0N0T23,
	author = {Zilu Tian and
                  Peter Lindner and
                  Markus Nissl and
                  Christoph Koch and
                  Val Tannen},
	title = {Generalizing Bulk-Synchronous Parallel Processing for Data Science:
                  From Data to Threads and Agent-Based Simulations},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {151:1--151:28},
	year = {2023},
	url = {https://doi.org/10.1145/3589296},
	doi = {10.1145/3589296},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Tian0N0T23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We generalize the bulk-synchronous parallel (BSP) processing model to make it better support agent-based simulations. Such simulations frequently exhibit hierarchical structure in their communication patterns which can be exploited to improve performance. We allow for the creation of temporary artificial network partitions during which agents synchronize only locally within their group in a way that does not compromise the correctness of a simulation. We have built a distributed engine, CloudCity, which uses this idea to improve the locality of computation, communication, and synchronization in such simulations. We experimentally evaluate the performance of our system on a benchmark of simulation workloads and compare it against other popular BSP-like systems, obtaining insights into the impact of various system design choices and optimization on simulation engine performance.}
}


@article{DBLP:journals/pacmmod/ZhangDFGPZ23,
	author = {Ling Zhang and
                  Shaleen Deep and
                  Avrilia Floratou and
                  Anja Gruenheid and
                  Jignesh M. Patel and
                  Yiwen Zhu},
	title = {Exploiting Structure in Regular Expression Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {152:1--152:28},
	year = {2023},
	url = {https://doi.org/10.1145/3589297},
	doi = {10.1145/3589297},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangDFGPZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Regular expression, or regex, is widely used to extract critical information from a large corpus of formatted text by finding patterns of interest. In tasks like log processing, the speed of regex matching is crucial. Data scientists and developers regularly use regex libraries that implement optimized regular expression matching using modern automata theory. However, computing state transitions in the underlying regex evaluation engine can be inefficient when a regex query contains a multitude of string literals. This inefficiency is further exasperated when analyzing large data volumes. This paper presents BLARE, Blazingly Fast Regular Expression, a regular expression matching framework that is inspired by the mechanisms that are used in database engines, which use a declarative framework to explore multiple equivalent execution plans, all of which produce the correct final result. Similarly, BLARE decomposes a regex into multiple regex and string components and then creates evaluation strategies in which the components can be evaluated in an order that is not strictly a left-to-right translation of the input regex query. Rather than using a cost-based optimization approach, BLARE uses an adaptive runtime strategy based on a multi-armed bandit approach to find an efficient execution plan. BLARE is also modular and can be built on top of any existing regex library. We implemented BLARE on four commonly used regex libraries, RE2, PCRE2, Boost Regex, and ICU Regex, and evaluated it using two production workloads and one open-source workload. BLARE was 1.6× to 3.7× faster than RE2 and 3.4× to 7.9× faster than Boost Regex. PCRE2 did not finish on one of the workloads, but on the remaining two workloads, BLARE improved the performance of PCRE2 by 3.1× to over 100×. For the open-source dataset, BLARE provided a speed up of 61.7× for ICU Regex. BLARE code is publicly available at https://github.com/mush-zhang/Blare.}
}


@article{DBLP:journals/pacmmod/0005023,
	author = {Xiao Hu and
                  Qichen Wang},
	title = {Computing the Difference of Conjunctive Queries Efficiently},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {153:1--153:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589298},
	doi = {10.1145/3589298},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/0005023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate how to efficiently compute the difference result of two (or multiple) conjunctive queries, which is the last operator in relational algebra to be unraveled. The standard approach in practical database systems is to materialize the results for every input query as a separate set, and then compute the difference of two (or multiple) sets. This approach is bottlenecked by the complexity of evaluating every input query individually, which could be very expensive, particularly when there are only a few results in the difference. In this paper, we introduce a new approach by exploiting the structural property of input queries and rewriting the original query by pushing the difference operator down as much as possible. We show that for a large class of difference queries, this approach can lead to a linear-time algorithm, in terms of the input size and (final) output size, i.e., the number of query results that survive from the difference operator. We complete this result by showing the hardness of computing the remaining difference queries in linear time. Although a linear-time algorithm is hard to achieve in general, we also provide some heuristics that can provably improve the standard approach. At last, we compare our approach with standard SQL engines over graph and benchmark datasets. The experiment results demonstrate order-of-magnitude speedups achieved by our approach over the vanilla SQL engine.}
}


@article{DBLP:journals/pacmmod/BritoFFMMS23,
	author = {Felipe T. Brito and
                  Victor A. E. de Farias and
                  Cheryl J. Flynn and
                  Subhabrata Majumdar and
                  Javam C. Machado and
                  Divesh Srivastava},
	title = {Global and Local Differentially Private Release of Count-Weighted
                  Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {154:1--154:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589299},
	doi = {10.1145/3589299},
	timestamp = {Tue, 31 Oct 2023 13:06:40 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BritoFFMMS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many complex natural and technological systems are commonly modeled as count-weighted graphs, where nodes represent entities, edges model relationships between them, and edge weights define some counting statistics associated with each relationship. As graph data usually contain sensitive information about entities, preserving privacy when releasing this type of data becomes an important issue. In this context, differential privacy (DP) has become the de facto standard for data release under strong privacy guarantees. When dealing with DP for weighted graphs, most state-of-the-art works assume that the graph topology is known. However, in several real-world applications, the privacy of the graph topology also needs to be ensured. In this paper, we aim to bridge the gap between DP and count-weighted graph data release, considering both graph structure and edge weights as private information. We first adapt the weighted graph DP definition to take into account the privacy of the graph structure. We then develop two novel approaches to privately releasing count-weighted graphs under the notions of global and local DP. We also leverage the post-processing property of DP to improve the accuracy of the proposed techniques considering graph domain constraints. Experiments using real-world graph data demonstrate the superiority of our approaches in terms of utility over existing techniques, enabling subsequent computation of a variety of statistics on the released graph with high utility, in some cases comparable to the non-private results.}
}


@article{DBLP:journals/pacmmod/WangW23,
	author = {Libin Wang and
                  Raymond Chi{-}Wing Wong},
	title = {{QHL:} {A} Fast Algorithm for Exact Constrained Shortest Path Search
                  on Road Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {155:1--155:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589300},
	doi = {10.1145/3589300},
	timestamp = {Mon, 13 Jan 2025 18:00:07 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Route planning is fundamental in our daily life. However, existing mapping applications focus on recommending routes by optimizing one single objective, which is inconsistent with some scenarios where users prefer the optimal route under a constraint. The constrained shortest path (CSP) query matches this requirement, but the query efficiencies of previous solutions are often low due to CSP's NP-hardness. In the era of big data, state-of-the-art indexes are getting larger to support faster query processing. Recent attempts to preprocess more intermediate results and reduce the number of table lookups have proved successful in solving the CSP. However, the best-known algorithm ignores some information in the CSP queries and tries to solve a more general problem before tackling the exact CSP. In this paper, we propose by far the fastest algorithm called QHL, which fully utilizes the pruning power of the CSP query information. Specifically, we preprocess our index by generating pruning conditions that can improve query efficiency. We also conducted extensive experiments on real-world datasets to demonstrate the superiority of our proposed algorithm. QHL could answer each CSP query in around 50 μs and run faster than the best-known algorithm by orders of magnitude.}
}


@article{DBLP:journals/pacmmod/000400H023,
	author = {Pingchuan Ma and
                  Rui Ding and
                  Shuai Wang and
                  Shi Han and
                  Dongmei Zhang},
	title = {XInsight: eXplainable Data Analysis Through The Lens of Causality},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {156:1--156:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589301},
	doi = {10.1145/3589301},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/000400H023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In light of the growing popularity of Exploratory Data Analysis (EDA), understanding the underlying causes of the knowledge acquired by EDA is crucial. However, it remains under-researched. This study promotes a transparent and explicable perspective on data analysis, called eXplainable Data Analysis (XDA). For this reason, we present XInsight, a general framework for XDA. XInsight provides data analysis with qualitative and quantitative explanations of causal and non-causal semantics. This way, it will significantly improve human understanding and confidence in the outcomes of data analysis, facilitating accurate data interpretation and decision making in the real world. XInsight is a three-module, end-to-end pipeline designed to extract causal graphs, translate causal primitives into XDA semantics, and quantify the quantitative contribution of each explanation to a data fact. XInsight uses a set of design concepts and optimizations to address the inherent difficulties associated with integrating causality into XDA. Experiments on synthetic and real-world datasets as well as a user study demonstrate the highly promising capabilities of XInsight.}
}


@article{DBLP:journals/pacmmod/ChaiL0FM0L023,
	author = {Chengliang Chai and
                  Jiabin Liu and
                  Nan Tang and
                  Ju Fan and
                  Dongjing Miao and
                  Jiayi Wang and
                  Yuyu Luo and
                  Guoliang Li},
	title = {GoodCore: Data-effective and Data-efficient Machine Learning through
                  Coreset Selection over Incomplete Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {157:1--157:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589302},
	doi = {10.1145/3589302},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChaiL0FM0L023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a dataset with incomplete data (e.g., missing values), training a machine learning model over the incomplete data requires two steps. First, it requires a data-effective step that cleans the data in order to improve the data quality (and the model quality on the cleaned data). Second, it requires a data-efficient step that selects a core subset of the data (called coreset) such that the trained models on the entire data and the coreset have similar model quality, in order to improve the training efficiency. The first-data-effective-then-data-efficient methods are too costly, because they are expensive to clean the whole data; while the first-data-efficient-then-data-effective methods have low model quality, because they cannot select high-quality coreset for incomplete data. In this paper, we investigate the problem of coreset selection over incomplete data for data-effective and data-efficient machine learning. The essential challenge is how to model the incomplete data for selecting high-quality coreset. To this end, we propose the GoodCore framework towards selecting a good coreset over incomplete data with low cost. To model the unknown complete data, we utilize the combinations of possible repairs as possible worlds of the incomplete data. Based on possible worlds, GoodCore selects an expected optimal coreset through gradient approximation without training ML models. We formally define the expected optimal coreset selection problem, prove its NP-hardness, and propose a greedy algorithm with an approximation ratio. To make GoodCore more efficient, we further propose optimization methods that incorporate human-in-the-loop imputation or automatic imputation method into our framework. Experimental results show the effectiveness and efficiency of our framework with low cost.}
}


@article{DBLP:journals/pacmmod/WangWC0O23,
	author = {Yatong Wang and
                  Yuncheng Wu and
                  Xincheng Chen and
                  Gang Feng and
                  Beng Chin Ooi},
	title = {Incentive-Aware Decentralized Data Collaboration},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {158:1--158:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589303},
	doi = {10.1145/3589303},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangWC0O23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data collaboration enables multiple parties to pool data for deriving meaningful data insights. However, data misuse and unlawful data collection have led to precautionary measures being imposed by individual organizations to guide against data leakage and abuse. As a response, decentralized federated learning (DFL) has emerged as an attractive paradigm to facilitate data collaboration while being amenable to privacy-preserving data and knowledge sharing, cost reduction, and prediction accuracy improvement. Unfortunately, the participating parties in DFL tend to be heterogeneous with skew datasets and uneven capabilities. Inevitably, training and transmission costs, and the presence of free-riders pose challenges to the adoption and participation of DFL. The absence of centralized parameter servers further exacerbates the problem of evaluating the contribution of each individual party. Therefore, an effective incentive mechanism is essential to promote data collaboration. In this paper, we propose a novel Incentive-aware Decentralized fEderated leArning (IDEA) framework for facilitating data collaboration. Specifically, we first design a customizable reward scheme for heterogeneous parties to optimize their respective objectives such as higher model accuracy, communication efficiency, and computational efficiency. To reward fairly to deserving parties while offering flexibility, we propose a novel multi-agent reinforcement learning (MARL) incentive mechanism, which enables heterogeneous parties to learn their own optimal collaboration policy. We then design an efficient decentralized data collaboration algorithm that supports the customizable reward scheme based on individual objective-specific collaboration policy. We theoretically prove that the algorithm achieves a Nash equilibrium, which ensures the fairness of the corresponding rewards for parties. We conduct extensive experiments to evaluate the performance of our proposed framework against four baselines on five real-world datasets. The results show that IDEA outperforms state-of-the-art methods in terms of effectiveness, efficiency, and accumulated reward.}
}


@article{DBLP:journals/pacmmod/HuangSCXR023,
	author = {Jiacheng Huang and
                  Zequn Sun and
                  Qijin Chen and
                  Xiaozhou Xu and
                  Weijun Ren and
                  Wei Hu},
	title = {Deep Active Alignment of Knowledge Graph Entities and Schemata},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {159:1--159:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589304},
	doi = {10.1145/3589304},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangSCXR023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graphs (KGs) store rich facts about the real world. In this paper, we study KG alignment, which aims to find alignment between not only entities but also relations and classes in different KGs. Alignment at the entity level can cross-fertilize alignment at the schema level. We propose a new KG alignment approach, called DAAKG, based on deep learning and active learning. With deep learning, it learns the embeddings of entities, relations and classes, and jointly aligns them in a semi-supervised manner. With active learning, it estimates how likely an entity, relation or class pair can be inferred, and selects the best batch for human labeling. We design two approximation algorithms for efficient solution to batch selection. Our experiments on benchmark datasets show the superior accuracy and generalization of DAAKG and validate the effectiveness of all its modules.}
}


@article{DBLP:journals/pacmmod/LiaoLDCQW23a,
	author = {Meihao Liao and
                  Rong{-}Hua Li and
                  Qiangqiang Dai and
                  Hongyang Chen and
                  Hongchao Qin and
                  Guoren Wang},
	title = {Efficient Personalized PageRank Computation: The Power of Variance-Reduced
                  Monte Carlo Approaches},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {160:1--160:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589305},
	doi = {10.1145/3589305},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiaoLDCQW23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized PageRank (PPR) computation is a fundamental problem in graph analysis. The state-of-the-art algorithms for PPR computation are based on a bidirectional framework which include a deterministic forward push and a Monte Carlo sampling procedure. The Monte Carlo sampling procedure, however, often has a relatively-large variance, thus reducing the performance of the PPR computation algorithms. To overcome this issue, we develop two novel variance-reduced Monte Carlo techniques for PPR computation. Our first technique is to apply power iterations to reduce the variance of the Monte Carlo sampling procedure. We prove that conducting few power iterations can significantly reduce the variance of existing Monte Carlo estimators, only with few additional costs. Moreover, we show that such a simple and novel variance-reduced Monte Carlo technique can achieve comparable estimation accuracy and the same time complexity as the state-of-the-art bidirectional algorithms. Our second technique is a novel progressive sampling method which uses the historical information of former samples to reduce the variance of the Monte Carlo estimator. We develop several novel PPR computation algorithms by integrating both of these variance reduction techniques with two existing Monte Carlo sampling approaches, including random walk sampling and spanning forests sampling. Finally, we conduct extensive experiments on 5 real-life large graphs to evaluate our solutions. The results show that our algorithms can achieve much higher PPR estimation accuracy by using much less time, compared to the state-of-the-art bidirectional algorithms.}
}


@article{DBLP:journals/pacmmod/BianSA23,
	author = {Haoqiong Bian and
                  Tiannan Sha and
                  Anastasia Ailamaki},
	title = {Using Cloud Functions as Accelerator for Elastic Data Analytics},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {161:1--161:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589306},
	doi = {10.1145/3589306},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BianSA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud function (CF) services, such as AWS Lambda, have been applied as the new computing infrastructure in implementing analytical query engines. For bursty and sparse workloads, CF-based query engine is more elastic than the traditional query engines running in servers, i.e., virtual machines (VMs), and might provide a higher performance/price ratio. However, it is still controversial whether CF services are good suites for general analytical workloads, in respect of the limitations of CFs in storage, network, and lifetime, as well as the much higher resource unit prices than VMs. In this paper, we first present micro-benchmark evaluations of the features of CF and VM. We reveal that for query processing, though CF is more elastic than VM, it is less scalable and is more expensive for continuous workloads. Then, to get the best of both worlds, we propose Pixels-Turbo - a hybrid query engine that processes queries in a scalable VM cluster by default and invokes CFs to accelerate the processing of unpredictable workload spikes. In the query engine, we propose several optimizations to improve the performance and scalability of the CF-based operators and a cost-based optimizer to select the appropriate algorithm and parallelism for the physical query plan. Evaluations on TPC-H and real-world workload show that our query engine has a 1-2 orders of magnitude higher performance/price ratio than state-of-the-art serverless query engines for sustained workloads while not compromising the elasticity for workload spikes.}
}


@article{DBLP:journals/pacmmod/WangWWZSL23,
	author = {Xin Wang and
                  Zhengru Wang and
                  Zhenyu Wu and
                  Shuhao Zhang and
                  Xuanhua Shi and
                  Li Lu},
	title = {Data Stream Clustering: An In-depth Empirical Study},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {162:1--162:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589307},
	doi = {10.1145/3589307},
	timestamp = {Fri, 12 Jul 2024 19:38:53 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangWWZSL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data Stream Clustering (DSC) plays an important role in mining continuous and unlabeled data streams in real-world applications. Over the last decades, numerous DSC algorithms have been proposed with promising clustering accuracy and efficiency. Despite the significant differences among existing DSC algorithms, they are commonly built around four key design aspects: summarizing data structure, window model, outlier detection mechanism, and offline refinement strategy. However, there is a lack of empirical studies on these key design aspects in the same codebase using real-world workloads with distinct characteristics. As a result, it is difficult for researchers to improve upon the state-of-the-art. In this paper, we conduct such a study of DSC on its four key design aspects. We implemented state-of-the-art variants of all of these design choices in an open-sourced platform from scratch and evaluated them using both real-world and synthetic workloads. Our analysis identifies the fundamental issues and trade-offs of each design choice in terms of both accuracy and efficiency. We even find that combining flexible design choices led to the development of a new algorithm called Benne, which can be tuned to achieve either better accuracy or better efficiency compared to the state-of-the-art.}
}


@article{DBLP:journals/pacmmod/Li023,
	author = {Haoyang Li and
                  Lei Chen},
	title = {{EARLY:} Efficient and Reliable Graph Neural Network for Dynamic Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {163:1--163:28},
	year = {2023},
	url = {https://doi.org/10.1145/3589308},
	doi = {10.1145/3589308},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Li023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks have been widely used to learn node representations for many real-world static graphs. In general, they learn node representations by recursively aggregating information from neighbors. However, graphs in many applications are dynamic, evolving with continuous graph events, such as node feature and graph structure updates. These events require the node representations to be updated accordingly. Currently, due to the real-time requirement, how to efficiently and reliably update node representations under continuous graph events is still an open problem. Recent studies propose two solutions to partially address this problem, but their performance is still limited. First, local-based GNNs only update the nodes directly involved in events, suffering from the quality-deficit issue, since they neglect the other nodes affected by these events. Second, neighbor-sampling GNNs propose to sample neighbors to accelerate neighbor aggregation computations, encountering the neighbor-redundant issue. These sampled neighbors may be similar and cannot reflect the distribution of all neighbors, leading that node representations aggregated on these redundant neighbors may differ from those aggregated on all neighbors. In this paper, we propose an efficient and reliable graph neural network, namely EARLY, to update node representations for dynamic graphs. We first identify the top-k influential nodes that are most affected by graph events. Then, to sample neighbors diversely, we propose a diversity-aware layer-wise sampling technique. We theoretically demonstrate that this technique can decrease the sampling expectation error and learn more reliable node representations. Therefore, the top-k nodes selection and diversity-aware sampling enable EARLY to efficiently update node representations in a reliable way. Extensive experiments on the five real-world graphs demonstrate the effectiveness and efficiency of our proposed EARLY.}
}


@article{DBLP:journals/pacmmod/LiaoBWZ00023,
	author = {Hao Liao and
                  Sheng Bi and
                  Jiao Wu and
                  Wei Zhang and
                  Mingyang Zhou and
                  Rui Mao and
                  Wei Chen},
	title = {Popularity Ratio Maximization: Surpassing Competitors through Influence
                  Propagation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {164:1--164:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589309},
	doi = {10.1145/3589309},
	timestamp = {Fri, 24 Jan 2025 16:47:12 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiaoBWZ00023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present an algorithmic study on how to surpass competitors in popularity by strategic promotions in social networks. We first propose a novel model, in which we integrate the Preferential Attachment (PA) model for popularity growth with the Independent Cascade (IC) model for influence propagation in social networks called PA-IC model. In PA-IC, a popular item and a novice item grab shares of popularity from the natural popularity growth via the PA model, while the novice item tries to gain extra popularity via influence cascade in a social network. The popularity ratio is defined as the ratio of the popularity measure between the novice item and the popular item. We formulate Popularity Ratio Maximization (PRM) as the problem of selecting seeds in multiple rounds to maximize the popularity ratio in the end. We analyze the popularity ratio and show that it is monotone but not submodular. To provide an effective solution, we devise a surrogate objective function and show that empirically it is very close to the original objective function while theoretically, it is monotone and submodular. We design two efficient algorithms, one for the overlapping influence and non-overlapping seeds (across rounds) setting and the other for the non-overlapping influence and overlapping seed setting, and further discuss how to deal with other models and problem variants. Our empirical evaluation further demonstrates that our proposed method consistently achieves the best popularity promotion compared to other methods. Our theoretical and empirical analyses shed light on the interplay between influence maximization and preferential attachment in social networks.}
}


@article{DBLP:journals/pacmmod/Ma0CHWC23,
	author = {Kaihao Ma and
                  Xiao Yan and
                  Zhenkun Cai and
                  Yuzhen Huang and
                  Yidi Wu and
                  James Cheng},
	title = {{FEC:} Efficient Deep Recommendation Model Training with Flexible
                  Embedding Communication},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {165:1--165:21},
	year = {2023},
	url = {https://doi.org/10.1145/3589310},
	doi = {10.1145/3589310},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Ma0CHWC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Embedding-based deep recommendation models (EDRMs), which contain small dense models and large embedding tables, are widely used in industry. Embedding communication constitutes the main cost for the distributed training of EDRMs, and thus we propose two strategies to improve its efficiency, i.e.,embedding tiering andpre-fetching. In particular, embedding tiering uses AllReduce to communicate popular embeddings that are accessed frequently. This is counter-intuitive as embeddings belong to the sparse embedding tables, but reasonable because the access pattern of popular embeddings resembles dense models. Pre-fetching starts communication early for embeddings that receive no updates such that they are removed from the critical path of training. We implement embedding tiering and pre-fetching in a system called FEC and compare it with the state-of-the-art systems on real datasets. The results show that FEC consistently outperforms the existing methods on all datasets, and its speed can be up to 6.65x and 2.42x in terms of embedding communication time and training throughput compared with the best performing baseline.}
}


@article{DBLP:journals/pacmmod/ZhangSS023,
	author = {Xin Zhang and
                  Yanyan Shen and
                  Yingxia Shao and
                  Lei Chen},
	title = {{DUCATI:} {A} Dual-Cache Training System for Graph Neural Networks
                  on Giant Graphs with the {GPU}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {166:1--166:24},
	year = {2023},
	url = {https://doi.org/10.1145/3589311},
	doi = {10.1145/3589311},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangSS023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently Graph Neural Networks (GNNs) have achieved great success in many applications. The mini-batch training has become the de-facto way to train GNNs on giant graphs. However, the mini-batch generation task is extremely expensive which slows down the whole training process. Researchers have proposed several solutions to accelerate the mini-batch generation, however, they (1) fail to exploit the locality of the adjacency matrix, (2) cannot fully utilize the GPU memory, and (3) suffer from the poor adaptability to diverse workloads. In this work, we propose DUCATI, aDual-Cache system to overcome these drawbacks. In addition to the traditionalNfeat-Cache, DUCATI introduces a newAdj-Cache to further accelerate the mini-batch generation and better utilize GPU memory. DUCATI develops a workload-awareDual-Cache Allocator which adaptively finds the best cache allocation plan under different settings. We compare DUCATI with various GNN training systems on four billion-scale graphs under diverse workload settings. The experimental results show that in terms of training time, DUCATI can achieve up to 3.33 times speedup (2.07 times on average) compared to DGL and up to 1.54 times speedup (1.32 times on average) compared to the state-of-the-artSingle-Cache systems. We also analyze the time-accuracy trade-offs of DUCATI and four state-of-the-art GNN training systems. The analysis results offer users some guidelines on system selection regarding different input sizes and hardware resources.}
}


@article{DBLP:journals/pacmmod/AraiFO23,
	author = {Junya Arai and
                  Yasuhiro Fujiwara and
                  Makoto Onizuka},
	title = {GuP: Fast Subgraph Matching by Guard-based Pruning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {167:1--167:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589312},
	doi = {10.1145/3589312},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AraiFO23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph matching, which finds subgraphs isomorphic to a query, is crucial for information retrieval from data represented as a graph. To avoid redundant explorations in the data, existing methods restrict the search space by extracting candidate vertices and edges that may constitute isomorphic subgraphs. However, expensive computation is still required because candidate vertices induce many non-isomorphic subgraphs. In this paper, we propose GuP, a subgraph matching algorithm with pruning based on guards. A guard represents a pattern of intermediate search states that never lead to isomorphic subgraphs. By attaching a guard to each candidate vertex and edge, GuP adaptively filters out unnecessary candidates depending on the search state at each step. The experimental results show that GuP effectively reduces the search space and can answer difficult queries that state-of-the-art methods cannot answer within a practical time frame.}
}


@article{DBLP:journals/pacmmod/WuZLH23,
	author = {Zhaomin Wu and
                  Junhui Zhu and
                  Qinbin Li and
                  Bingsheng He},
	title = {DeltaBoost: Gradient Boosting Decision Trees with Efficient Machine
                  Unlearning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {168:1--168:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589313},
	doi = {10.1145/3589313},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuZLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As machine learning (ML) has been widely developed in real-world applications, the privacy of ML models draws an increasing concern. In this paper, we study how to forget specific data records from ML models to preserve the privacy of these data. Although some studies propose efficient unlearning algorithms on random forests and extremely randomized trees, Gradient Boosting Decision Trees (GBDT), which are widely used in practice, have not been explored. The efficient unlearning of GBDT faces two major challenges: 1) the training of each tree is deterministic and non-robust; 2) the training of a tree depends on all the previous trees. To solve the first challenge, we propose a robust GBDT-like ML model DeltaBoost that enables efficient and accurate deletion according to our theoretical analysis. For the second challenge, we design a training algorithm for DeltaBoost that minimizes the dependency among trees. Our experiments on five datasets demonstrate that DeltaBoost can remove data records from the trained model efficiently and effectively. Our unlearning approach achieves up to two orders of magnitude speedup compared to retraining GBDT. Besides, DeltaBoost produces competitive performance to existing decision-tree-based ML models.}
}


@article{DBLP:journals/pacmmod/XuMFB23,
	author = {Yichen Xu and
                  Chenhao Ma and
                  Yixiang Fang and
                  Zhifeng Bao},
	title = {Efficient and Effective Algorithms for Generalized Densest Subgraph
                  Discovery},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {169:1--169:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589314},
	doi = {10.1145/3589314},
	timestamp = {Wed, 19 Jul 2023 10:44:53 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuMFB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The densest subgraph problem (DSP) is of great significance due to its wide applications in different domains. Meanwhile, diverse requirements in various applications lead to different density variants for DSP. Unfortunately, existing DSP algorithms cannot be easily extended to handle those variants efficiently and accurately. To fill this gap, we first unify different density metrics into a generalized density definition. We further propose a new model, c-core, to locate the general densest subgraph and show its advantage in accelerating the searching process. Extensive experiments show that our c-core-based optimization can provide up to three orders of magnitude speedup over baselines. Moreover, we study an important variant of DSP under a size constraint, namely the densest-at-least-k-subgraph (DalkS) problem. We propose an algorithm based on graph decomposition, and it is likely to give a solution that is at least 0.8 of the optimal density in our experiments, while the state-of-the-art method can only ensure a solution with density at least 0.5 of the optimal density. Our experiments show that our DalkS algorithm can achieve at least 0.99 of the optimal density for over one-third of all possible size constraints.}
}


@article{DBLP:journals/pacmmod/XieFXLM23,
	author = {Haoxuan Xie and
                  Yixiang Fang and
                  Yuyang Xia and
                  Wensheng Luo and
                  Chenhao Ma},
	title = {On Querying Connected Components in Large Temporal Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {170:1--170:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589315},
	doi = {10.1145/3589315},
	timestamp = {Wed, 19 Jul 2023 10:44:53 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XieFXLM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, for the first time, we introduce the concepts of window-CCs and window-SCCs on undirected and directed temporal graphs, respectively. We then study the queries of window-CC and window-SCC by developing several efficient index-based query solutions. The space costs of the best indices are linear to the sizes of the temporal graphs. The extensive experimental evaluation on 12 real-world datasets demonstrates the high efficiency and effectiveness of the proposed solutions. In the future, we will develop distributed index construction algorithms, which would be useful for very large temporal graphs containing billions of edges. In the future, we will implement our algorithms by using a distributed computing platform (e.g., Pregel), which would be very useful when the temporal graph is too large to be kept by a single machine.}
}


@article{DBLP:journals/pacmmod/0002Z0KGJ23,
	author = {David Campos and
                  Miao Zhang and
                  Bin Yang and
                  Tung Kieu and
                  Chenjuan Guo and
                  Christian S. Jensen},
	title = {LightTS: Lightweight Time Series Classification with Adaptive Ensemble
                  Distillation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {171:1--171:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589316},
	doi = {10.1145/3589316},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0002Z0KGJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the sweeping digitalization of processes, increasingly vast amounts of time series data are being produced. Accurate classification of such time series facilitates decision making in multiple domains. State-of-the-art classification accuracy is often achieved by ensemble learning where results are synthesized from multiple base models. This characteristic implies that ensemble learning needs substantial computing resources, preventing their use in resource-limited environments, such as in edge devices. To extend the applicability of ensemble learning, we propose the LightTS framework that compresses large ensembles into lightweight models while ensuring competitive accuracy. First, we propose adaptive ensemble distillation that assigns adaptive weights to different base models such that their varying classification capabilities contribute purposefully to the training of the lightweight model. Second, we propose means of identifying Pareto optimal settings w.r.t. model accuracy and model size, thus enabling users with a space budget to select the most accurate lightweight model. We report on experiments using 128 real-world time series sets and different types of base models that justify key decisions in the design of LightTS and provide evidence that LightTS is able to outperform competitors.}
}


@article{DBLP:journals/pacmmod/Fernandez23,
	author = {Raul Castro Fernandez},
	title = {Data-Sharing Markets: Model, Protocol, and Algorithms to Incentivize
                  the Formation of Data-Sharing Consortia},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {172:1--172:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589317},
	doi = {10.1145/3589317},
	timestamp = {Sat, 14 Oct 2023 20:14:27 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Fernandez23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Organizations that would mutually benefit from pooling their data are otherwise wary of sharing. This is because sharing data is costly-in time and effort-and, at the same time, the benefits of sharing are not clear. Without a clear cost-benefit analysis, participants default in not sharing. As a consequence, many opportunities to create valuable data-sharing consortia never materialize, and the value of data remains locked. We introduce a new sharing model, market protocol, and algorithms to incentivize the creation of data-sharing markets. The combined contributions of this paper, which we call DSC, incentivize the creation of data-sharing markets that unleash the value of data for its participants. The sharing model introduces two incentives; one that guarantees that participating is better than not doing so and another that compensates participants according to how valuable their data is. Because operating the consortia is costly, we are also concerned with ensuring its operation is sustainable: we design a protocol that ensures that a valuable data-sharing consortium forms when it is sustainable. We introduce algorithms to elicit the value of data from the participants, which is used first to cover the costs of operating the consortia and second to compensate for data contributions. For the latter, we challenge using the Shapley value to allocate revenue. We offer analytical and empirical evidence for this and introduce an alternative method that compensates participants better and leads to the formation of data-sharing consortia.}
}


@article{DBLP:journals/pacmmod/FangG0XGJ23,
	author = {Ziquan Fang and
                  Shenghao Gong and
                  Lu Chen and
                  Jiachen Xu and
                  Yunjun Gao and
                  Christian S. Jensen},
	title = {Ghost: {A} General Framework for High-Performance Online Similarity
                  Queries over Distributed Trajectory Streams},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {173:1--173:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589318},
	doi = {10.1145/3589318},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FangG0XGJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory similarity queries, including similarity search and similarity join, offer a foundation for many geo-spatial applications. With the rapid increase of streaming trajectory data volumes, e.g., data from mobile phones, vessel monitoring, or traffic systems, many location-based services benefit from online similarity analytics over trajectory data streams, where moving objects continually emit real-time position data. However, most existing studies focus on offline settings, and thus several major challenges remain unanswered in an online setting. To this end, we describe Ghost, a distributed stream processing framework that enables generic, efficient, and scalable online trajectory similarity search and join. We propose a novel incremental online similarity computation (IOSC) mechanism to accelerate pair-wise streaming trajectory distance calculation, which supports a broad range of trajectory distance metrics. Compared with previous studies, IOSC reduces the complexity from quadratic to linear in terms of trajectory length. Building on this foundation, we propose histogram-based algorithms that exploit histogram indexes and a series of pruning bounds to enable streaming trajectory similarity search and join. Finally, we extend our methods to the distributed platform Flink for scalability, where a CostPartitioner is developed to ensure parallel processing and workload balancing. An experimental study using two real-life and one synthetic datasets shows that Ghost (i) acquires 6-20× efficiency/throughput gains and one order of magnitude memory overhead savings over state-of-the-art baselines, (ii) achieves 3--8× workload balancing gains on Flink, and (iii) exhibits low parameter sensitivity and high robustness.}
}


@article{DBLP:journals/pacmmod/SancaCA23,
	author = {Viktor Sanca and
                  Periklis Chrysogelos and
                  Anastasia Ailamaki},
	title = {LAQy: Efficient and Reusable Query Approximations via Lazy Sampling},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {174:1--174:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589319},
	doi = {10.1145/3589319},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SancaCA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern analytical engines rely on Approximate Query Processing (AQP) to provide faster response times than the hardware allows for exact query answering. However, existing AQP methods impose steep performance penalties as workload unpredictability increases. Specifically, offline AQP relies on predictable workloads to create samples that match the queries in a priori to query execution, reducing query response times when queries match the expected workload. As soon as workload predictability diminishes, existing online AQP methods create query-specific samples with little reuse across queries, producing significantly smaller gains in response times. As a result, existing approaches cannot fully exploit the benefits of sampling under increased unpredictability. We analyze sample creation and propose LAQy, a framework for building, expanding, and merging samples to adapt to the changes in workload predicates. We show the main parameters that affect the sample creation time and propose lazy sampling to overcome the unpredictability issues that cause fast-but-specialized samples to be query-specific. We evaluate LAQy by implementing it in an in-memory code-generation-based scale-up analytical engine to show the adaptivity and practicality of our framework in a modern system. LAQy speeds up online sampling processing as a function of sample reuse ranging from practically zero to full online sampling time and from 2.5x to 19.3x in a simulated exploratory workload.}
}


@article{DBLP:journals/pacmmod/Banerjee0L23,
	author = {Prithu Banerjee and
                  Wei Chen and
                  Laks V. S. Lakshmanan},
	title = {Mitigating Filter Bubbles Under a Competitive Diffusion Model},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {175:1--175:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589320},
	doi = {10.1145/3589320},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Banerjee0L23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While social networks greatly facilitate information dissemination, they are well known to have contributed to the phenomena of filter bubbles and echo chambers. This in turn can lead to societal polarization and erosion of trust in public institutions. Mitigating filter bubbles is an urgent open problem. Recently, approaches based on the influence maximization paradigm have been proposed in our community for mitigating filter bubbles by balancing exposure to opposing viewpoints. However, existing works ignore the inherent competition between the adoption of opposing viewpoints by users. In this paper, we propose a realistic model for the filter bubble problem, which unlike previous work, captures thecompetition between opposing opinions propagating in a network as well as thecomplementary nature of the reward forexposing users to both those opinions. We formulate an optimization problem for mitigating filter bubbles under our model. We establish several evidences of the intrinsic difficulty in developing constant approximation to the problem and develop a heuristic and two instance-dependent approximation algorithms. Our experiments over 4 real datasets show that our heuristic far outperforms two state-of-the-art baselines as well as other algorithms in both efficiency and mitigating filter bubbles. We also empirically demonstrate that our best heuristic performs close to the optimal objective, which is obtained by utilizing the theoretical bounds of our approximation algorithms.}
}


@article{DBLP:journals/pacmmod/LiS0N23,
	author = {Jia Li and
                  Yanyan Shen and
                  Lei Chen and
                  Charles Wang Wai Ng},
	title = {{SSIN:} Self-Supervised Learning for Rainfall Spatial Interpolation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {176:1--176:21},
	year = {2023},
	url = {https://doi.org/10.1145/3589321},
	doi = {10.1145/3589321},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiS0N23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The acquisition of accurate rainfall distribution in space is an important task in hydrological analysis and natural disaster pre-warning. However, it is impossible to install rain gauges on every corner. Spatial interpolation is a common way to infer rainfall distribution based on available raingauge data. However, the existing works rely on some unrealistic pre-settings to capture spatial correlations, which limits their performance in real scenarios. To tackle this issue, we propose the SSIN, which is a novel data-driven self-supervised learning framework for rainfall spatial interpolation by mining latent spatial patterns from historical observation data. Inspired by the Cloze task and BERT, we fully consider the characteristics of spatial interpolation and design the SpaFormer model based on the Transformer architecture as the core of SSIN. Our main idea is: by constructing rich self-supervision signals via random masking, SpaFormer can learn informative embeddings for raw data and then adaptively model spatial correlations based on rainfall spatial context. Extensive experiments on two real-world raingauge datasets show that our method outperforms the state-of-the-art solutions. In addition, we take traffic spatial interpolation as another use case to further explore the performance of our method, and SpaFormer achieves the best performance on one large real-world traffic dataset, which further confirms the effectiveness and generality of our method.}
}


@article{DBLP:journals/pacmmod/OrogatE23,
	author = {Abdelghny Orogat and
                  Ahmed El{-}Roby},
	title = {Maestro: Automatic Generation of Comprehensive Benchmarks for Question
                  Answering Over Knowledge Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {177:1--177:24},
	year = {2023},
	url = {https://doi.org/10.1145/3589322},
	doi = {10.1145/3589322},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/OrogatE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, there has been an upsurge in the number of knowledge graphs (KG) that can only be accessed by experts. Non-expert users lack an adequate understanding of the queried knowledge graph's vocabulary and structure, as well as the syntax of the structured query language used to express the user's information needs. To increase the user base of these KGs, a set of Question Answering (QA) systems that use natural language to query these knowledge graphs have been introduced. However, finding a benchmark that accurately evaluates the quality of a QA system is a difficult task due to (1) the high degree of variation in the fine-grained properties among the existing benchmarks, (2) the static nature of the existing benchmarks versus the evolving nature of KGs, and (3) the limited number of KGs targeted by existing benchmarks, which hinders the usability of QA systems in real-world deployment over KGs that are different from those that were used in the evaluation of the QA systems. In this paper, we introduce Maestro, a benchmark generation system for question answering over knowledge graphs. Maestro can generate a new benchmark for any KG given the KG and, optionally, a text corpus that covers this KG. The benchmark generated by Maestro is guaranteed to cover all the properties of the natural language questions and queries that were encountered in the literature as long as the targeted KG includes these properties. Maestro also generates high-quality natural language questions with various utterances that are on par with manually-generated ones to better evaluate QA systems.}
}


@article{DBLP:journals/pacmmod/LiLC23,
	author = {Yinan Li and
                  Jianan Lu and
                  Badrish Chandramouli},
	title = {Selection Pushdown in Column Stores using Bit Manipulation Instructions},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {178:1--178:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589323},
	doi = {10.1145/3589323},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern analytical database systems predominantly rely on column-oriented storage, which offers superior compression efficiency due to the nature of the columnar layout. This compression, however, creates challenges in decoding speed during query processing. Previous research has explored predicate pushdown on encoded values to avoid decoding, but these techniques are restricted to specific encoding schemes and predicates, limiting their practical use. In this paper, we propose a generic predicate pushdown approach that supports arbitrary predicates by leveraging selection pushdown to reduce decoding costs. At the core of our approach is a fast select operator capable of directly extracting selected encoded values without decoding, by using Bit Manipulation Instructions, an instruction set extension to the X86 architecture. We empirically evaluate the proposed techniques in the context of Apache Parquet using both micro-benchmarks and the TPC-H benchmark, and show that our techniques improve the query performance of Parquet by up to one order of magnitude with representative scan queries. Further experimentation using Apache Spark demonstrates speed improvements of up to 5.5X even for end-to-end queries involving complex joins.}
}


@article{DBLP:journals/pacmmod/PengW023,
	author = {Zhencan Peng and
                  Zhizhi Wang and
                  Dong Deng},
	title = {Near-Duplicate Sequence Search at Scale for Large Language Model Memorization
                  Evaluation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {179:1--179:18},
	year = {2023},
	url = {https://doi.org/10.1145/3589324},
	doi = {10.1145/3589324},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/PengW023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent studies show that large language models (LLM) unintendedly memorize part of the training data, which brings serious privacy risks. For example, it has been shown that over 1% of tokens generated unprompted by an LLM are part of sequences in the training data. However, current studies mainly focus on the exact memorization behaviors. In this paper, we propose to evaluate how many generated texts have near-duplicates (e.g., only differ by a couple of tokens out of 100) in the training corpus. A major challenge of conducting this evaluation is the huge computation cost incurred by near-duplicate sequence searches. This is because modern LLMs are trained on larger and larger corpora with up to 1 trillion tokens. What's worse is that the number of sequences in a text is quadratic to the text length. To address this issue, we develop an efficient and scalable near-duplicate sequence search algorithm in this paper. It can find (almost) all the near-duplicate sequences of the query sequence in a large corpus with guarantees. Specifically, the algorithm generates and groups the min-hash values of all the sequences with at least t tokens (as very short near-duplicates are often irrelevant noise) in the corpus in linear time to the corpus size. We formally prove that only 2 n+1/t+1 -1 min-hash values are generated for a text with n tokens in expectation. Thus the index time and size are reasonable. When a query arrives, we find all the sequences sharing enough min-hash values with the query using inverted indexes and prefix filtering. Extensive experiments on a few large real-world LLM training corpora show that our near-duplicate sequence search algorithm is efficient and scalable.}
}


@article{DBLP:journals/pacmmod/DrienFAA23,
	author = {Osnat Drien and
                  Matanya Freiman and
                  Antoine Amarilli and
                  Yael Amsterdamer},
	title = {Query-Guided Resolution in Uncertain Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {180:1--180:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589325},
	doi = {10.1145/3589325},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DrienFAA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a novel framework for uncertain data management. We start with a database whose tuple correctness is uncertain and an oracle that can resolve the uncertainty, i.e., decide if a tuple is correct or not. Such an oracle may correspond, e.g., to a data expert or to a crowdsourcing platform. We wish to use the oracle to clean the database with the goal of ensuring the correct answer for specific mission-critical queries. To avoid the prohibitive cost of cleaning the entire database and to minimize the expected number of calls to the oracle, we must carefully select tuples whose resolution would suffice to resolve the uncertainty in query results. In other words, we need a query-guided process for the resolution of uncertain data. We develop an end-to-end solution to this problem, based on the derivation of query answers and on correctness probabilities for the uncertain data. At a high level, we first track Boolean provenance to identify which input tuples contribute to the derivation of each output tuple, and in what ways. We then design an active learning solution for iteratively choosing tuples to resolve, based on the provenance structure and on an evolving estimation of tuple correctness probabilities. We conduct an extensive experimental study to validate our framework in different use cases.}
}


@article{DBLP:journals/pacmmod/Sun023,
	author = {Xibo Sun and
                  Qiong Luo},
	title = {Efficient GPU-Accelerated Subgraph Matching},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {181:1--181:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589326},
	doi = {10.1145/3589326},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Sun023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph matching is a basic operation in graph analytics, finding all occurrences of a query graph Q in a data graph G. A common approach is to first filter out non-candidate vertices in G, and then order the vertices in Q to enumerate results. Recent work has started to utilize the GPU to accelerate subgraph matching. However, the effectiveness of current GPU-based filtering and ordering methods is limited, and the result enumeration often runs out of memory quickly. To address these problems, we propose EGSM, an efficient approach to GPU-based subgraph matching. Specifically, we design a data structure Cuckoo trie to support dynamic maintenance of candidates for filtering, and order query vertices based on estimated numbers of candidate vertices on the fly. Furthermore, we perform a hybrid breadth-first and depth-first search with memory management for result enumeration. Consequently, EGSM significantly outperforms the state-of-the-art GPU-accelerated algorithms, including GSI and CuTS.}
}


@article{DBLP:journals/pacmmod/KargarN23,
	author = {Saeed Kargar and
                  Faisal Nawab},
	title = {Hamming Tree: The Case for Energy-Aware Indexing for NVMs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {182:1--182:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589327},
	doi = {10.1145/3589327},
	timestamp = {Fri, 07 Jul 2023 23:32:32 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KargarN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-volatile memory (NVM) technologies are widely adopted in data storage solutions and battery-powered mobile and IoT devices. Wear-out and energy efficiency are two vital challenges facing the use of NVM. In Hamming Tree, we propose a software-level memory-aware solution that picks the memory segment of where a write operation is applied judiciously to minimize bit flipping. It has been shown that reducing bit flips leads to reducing energy consumption and improving write endurance. We performed real evaluations on an Optane memory device that show that Hamming Tree can achieve up to 67.8% reduction in energy consumption.}
}


@article{DBLP:journals/pacmmod/LiCC023,
	author = {Peng Li and
                  Zhiyi Chen and
                  Xu Chu and
                  Kexin Rong},
	title = {DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning
                  over Tabular Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {183:1--183:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589328},
	doi = {10.1145/3589328},
	timestamp = {Fri, 23 May 2025 21:09:09 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiCC023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data preprocessing is a crucial step in the machine learning process that transforms raw data into a more usable format for downstream ML models. However, it can be costly and time-consuming, often requiring the expertise of domain experts. Existing automated machine learning (AutoML) frameworks claim to automate data preprocessing. However, they often use a restricted search space of data preprocessing pipelines which limits the potential performance gains, and they are often too slow as they require training the ML model multiple times. In this paper, we propose DiffPrep, a method that can automatically and efficiently search for a data preprocessing pipeline for a given tabular dataset and a differentiable ML model such that the performance of the ML model is maximized. We formalize the problem of data preprocessing pipeline search as a bi-level optimization problem. To solve this problem efficiently, we transform and relax the discrete, non-differential search space into a continuous and differentiable one, which allows us to perform the pipeline search using gradient descent with training the ML model only once. Our experiments show that DiffPrep achieves the best test accuracy on 15 out of the 18 real-world datasets evaluated and improves the model's test accuracy by up to 6.6 percentage points.}
}


@article{DBLP:journals/pacmmod/HusseinLRBBC23,
	author = {Rana Hussein and
                  Alberto Lerner and
                  Andr{\'{e}} Ryser and
                  Lucas David B{\"{u}}rgi and
                  Albert Blarer and
                  Philippe Cudr{\'{e}}{-}Mauroux},
	title = {GraphINC: Graph Pattern Mining at Network Speed},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {184:1--184:28},
	year = {2023},
	url = {https://doi.org/10.1145/3589329},
	doi = {10.1145/3589329},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HusseinLRBBC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Pattern Mining (GPM) is a class of algorithms that identifies given shapes within a graph, e.g., cliques of a certain size. Any area of a graph can contain a shape of interest, but in real-world graphs, these shapes tend to be concentrated in areas deemed skewed. Because mining skewed areas can dominate GPM computations, the overwhelming majority of state-of-the-art GPM techniques break such areas into many small parts and load balance them across servers. This paper takes a diametrically opposite approach: we suggest a framework that concentrates rather than divides the skewed areas. Our framework, called GraphINC, relies on two key innovations. First, it introduces a new graph partitioning scheme capable of separating the skewed area from the rest of the graph. Second, it offloads the skewed part onto a new class of hardware accelerator, a programmable network switch. We implemented our framework to leverage a commercial 100 Gbps switch and obtained results 6.5 to 52.4× faster thanks to our novel offloading technique.}
}


@article{DBLP:journals/pacmmod/ZhaoZG23,
	author = {Junyi Zhao and
                  Huanchen Zhang and
                  Yihan Gao},
	title = {Efficient Query Re-optimization with Judicious Subquery Selections},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {185:1--185:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589330},
	doi = {10.1145/3589330},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhaoZG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query re-optimization is an adaptive query processing technique that re-invokes the optimizer at certain points in query execution. The goal is to dynamically correct the cardinality estimation errors using the statistics collected at runtime to adjust the query plan to improve the overall performance. We identify a key weakness in existing re-optimization algorithms: their subquery division and re-optimization trigger strategies rely heavily on the optimizer\'s initial plan, which can be far away from optimal. We, therefore, propose QuerySplit, a novel re-optimization algorithm that skips the potentially misleading global plan and instead generates subqueries directly from the logical plan as the basic re-optimization units. By developing a cost function that prioritizes the execution of less "damaging" subqueries, QuerySplit successfully postpones (sometimes avoids) the execution of complex large joins to maximize their probability of having smaller input sizes. We implemented QuerySplit in PostgreSQL and compared our solution against four state-of-the-art re-optimization algorithms using the Join Order Benchmark. Our experiments show that QuerySplit reduces the benchmark execution time by 35% compared to the second-best alternative. The performance gap between QuerySplit and an optimal optimizer is within 4%.}
}


@article{DBLP:journals/pacmmod/ZhangCWLCTL023,
	author = {Xinyi Zhang and
                  Zhuo Chang and
                  Hong Wu and
                  Yang Li and
                  Jia Chen and
                  Jian Tan and
                  Feifei Li and
                  Bin Cui},
	title = {A Unified and Efficient Coordinating Framework for Autonomous {DBMS}
                  Tuning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {186:1--186:26},
	year = {2023},
	url = {https://doi.org/10.1145/3589331},
	doi = {10.1145/3589331},
	timestamp = {Wed, 19 Mar 2025 21:16:37 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangCWLCTL023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently using machine learning (ML) based techniques to optimize the performance of modern database management systems (DBMSs) has attracted intensive interest from both industry and academia. With an objective to tune a specific component of a DBMS (e.g., index selection, knobs tuning), the ML-based tuning agents have shown to be able to find better configurations than experienced database administrators (DBAs). However, one critical yet challenging question remains unexplored -- how to make those ML-based tuning agents work collaboratively. Existing methods do not consider the dependencies among the multiple agents, and the model used by each agent only studies the effect of changing the configurations in a single component. To tune different components for DBMS, a coordinating mechanism is needed to make the multiple agents be cognizant of each other. Also, we need to decide how to allocate the limited tuning budget (e.g., time and resources) among the agents to maximize the performance. Such a decision is difficult to make since the distribution of the reward (i.e., performance improvement) corresponding to each agent is unknown and non-stationary. In this paper, we study the above question and present a unified coordinating framework to efficiently utilize existing ML-based agents. First, we propose a message propagation protocol that specifies the collaboration behaviors for agents and encapsulates the global tuning messages in each agent's model. Second, we combine Thompson Sampling, a well-studied reinforcement learning algorithm with a memory buffer so that our framework can allocate the tuning budget judiciously in a non-stationary environment. Our framework defines the interfaces adapted to a broad class of ML-based tuning agents, yet simple enough for integration with existing implementations and future extensions. Based on extensive evaluations, we show that this framework can effectively utilize different ML-based agents and find better configurations with 1.4~14.1x speedups on the workload execution time compared with baselines.}
}


@article{DBLP:journals/pacmmod/Sheng0F00C023,
	author = {Yufan Sheng and
                  Xin Cao and
                  Yixiang Fang and
                  Kaiqi Zhao and
                  Jianzhong Qi and
                  Gao Cong and
                  Wenjie Zhang},
	title = {{WISK:} {A} Workload-aware Learned Index for Spatial Keyword Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {187:1--187:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589332},
	doi = {10.1145/3589332},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Sheng0F00C023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial objects often come with textual information, such as Points of Interest (POIs) with their descriptions, which are referred to as geo-textual data. To retrieve such data, spatial keyword queries that take into account both spatial proximity and textual relevance have been extensively studied. Existing indexes designed for spatial keyword queries are mostly built based on the geo-textual data without considering the distribution of queries already received. However, previous studies have shown that utilizing the known query distribution can improve the index structure for future query processing. In this paper, we propose WISK, a learned index for spatial keyword queries, which self-adapts for optimizing querying costs given a query workload. One key challenge is how to utilize both structured spatial attributes and unstructured textual information during learning the index. We first divide the data objects into partitions, aiming to minimize the processing costs of the given query workload. We prove the NP-hardness of the partitioning problem and propose a machine learning model to find the optimal partitions. Then, to achieve more pruning power, we build a hierarchical structure based on the generated partitions in a bottom-up manner with a reinforcement learning-based approach. We conduct extensive experiments on real-world datasets and query workloads with various distributions, and the results show that WISK outperforms all competitors, achieving up to 8× speedup in querying time with comparable storage overhead.}
}


@article{DBLP:journals/pacmmod/Ren0RTPL23,
	author = {Xiaobin Ren and
                  Kaiqi Zhao and
                  Patricia J. Riddle and
                  Katerina Taskova and
                  Qingyi Pan and
                  Lianyan Li},
	title = {{DAMR:} Dynamic Adjacency Matrix Representation Learning for Multivariate
                  Time Series Imputation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {188:1--188:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589333},
	doi = {10.1145/3589333},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Ren0RTPL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Missing data imputation for location-based sensor data has attracted much attention in recent years. The state-of-the-art imputation methods based on graph neural networks have a priori assumption that the spatial correlations between sensor locations are static. However, real-world data sets often exhibit dynamic spatial correlations. This paper proposes a novel approach to capturing the dynamics of spatial correlations between geographical locations as a composition of the constant, long-term trends and periodic patterns. To this end, we design a new method called Dynamic Adjacency Matrix Representation (DAMR) that extracts various dynamic patterns of spatial correlations and represents them as adjacency matrices. The adjacency matrices are then aggregated and fed into a well-designed graph representation learning layer for predicting the missing values. Through extensive experiments on six real-world data sets, we demonstrate that DAMR reduces the MAE by up to 19.4% compared with the state-of-the-art methods for the missing value imputation task}
}


@article{DBLP:journals/pacmmod/SunMSXBERFZTCWA23,
	author = {Yutian Sun and
                  Tim Meehan and
                  Rebecca Schlussel and
                  Wenlei Xie and
                  Masha Basmanova and
                  Orri Erling and
                  Andrii Rosa and
                  Shixuan Fan and
                  Rongrong Zhong and
                  Arun Thirupathi and
                  Nikhil Collooru and
                  Ke Wang and
                  Sameer Agarwal and
                  Arjun Gupta and
                  Dionysios Logothetis and
                  Kostas Xirogiannopoulos and
                  Amit Dutta and
                  Varun Gajjala and
                  Rohit Jain and
                  Ajay Palakuzhy and
                  Prithvi Pandian and
                  Sergey Pershin and
                  Abhisek Saikia and
                  Pranjal Shankhdhar and
                  Neerad Somanchi and
                  Swapnil Tailor and
                  Jialiang Tan and
                  Sreeni Viswanadha and
                  Zac Wen and
                  Biswapesh Chattopadhyay and
                  Bin Fan and
                  Deepak Majeti and
                  Aditi Pandit},
	title = {Presto: {A} Decade of {SQL} Analytics at Meta},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {189:1--189:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589769},
	doi = {10.1145/3589769},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SunMSXBERFZTCWA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Presto is an open-source distributed SQL query engine that supports analytics workloads involving multiple exabyte-scale data sources. Presto is used for low-latency interactive use cases as well as long-running ETL jobs at Meta. It was originally launched at Meta in 2013 and donated to the Linux Foundation in 2019. Over the last ten years, upholding query latency and scalability with the hyper growth of data volume at Meta as well as new SQL analytics requirements have raised impressive challenges for Presto. A top priority has been ensuring query reliability does not regress with the shift towards smaller, more elastic container allocation, which requires queries to run with substantially smaller memory headroom and can be preempted at any time. Additionally, new demands from machine learning, privacy, and graph analytics have driven Presto maintainers to think beyond traditional data analytics. In this paper, we discuss several successful evolutions in recent years that have improved Presto latency as well as scalability by several orders of magnitude in production at Meta. Some of the notable ones are hierarchical caching, native vectorized execution engines, materialized views, and Presto on Spark. With these new capabilities, we have deprecated or are in the process of deprecating various legacy query engines so that Presto becomes the single piece to serve interactive, ad-hoc, ETL, and graph processing workloads for the entire data warehouse.}
}


@article{DBLP:journals/pacmmod/XuGD23,
	author = {Zhichen Xu and
                  Ying Gao and
                  Andrew Davidson},
	title = {Keep Your Distributed Data Warehouse Consistent at a Minimal Cost},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {190:1--190:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589770},
	doi = {10.1145/3589770},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuGD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large data warehouses store interdependent tables that are updated independently in response to business logic changes or late arrival of critical data. To keep the warehouse consistent, changes to upstream tables need to be propagated to downstream tables in a timely fashion. However, a naive change propagation algorithm can cause many unnecessary updates or recalculations of downstream tables, which drives up the cost of data warehouse management. In this paper, we describe our solution that can ensure the eventual consistency of the data warehouse while avoiding unnecessary table updates. We also show that the optimal trade-off between computational cost reduction and meeting data freshness constraints can be found by solving a dynamic programming problem. The proposed solution is currently in production to manage the YouTube Data Warehouse and has reduced update requests by 25% by eliminating non-trivial duplicates. These requests would have been carried out by large batch jobs over big data. Eliminating them has led to a proportionate reduction in computing resources. One key advantage of our approach is that it can be used in a heterogeneous, distributed data warehouse environment where the operator software may not have complete control over the query processors. This is because our approach only relies on having dependency information for tables and can operate on the post-state of data sources.}
}


@article{DBLP:journals/pacmmod/PanWZZPLZFZ23,
	author = {Zhenxuan Pan and
                  Tao Wu and
                  Qingwen Zhao and
                  Qiang Zhou and
                  Zhiwei Peng and
                  Jiefeng Li and
                  Qi Zhang and
                  Guanyu Feng and
                  Xiaowei Zhu},
	title = {GeaFlow: {A} Graph Extended and Accelerated Dataflow System},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {191:1--191:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589771},
	doi = {10.1145/3589771},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PanWZZPLZFZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {GeaFlow is a distributed dataflow system optimized for streaming graph processing, and has been widely adopted at Ant Group, serving various scenarios ranging from risk control of financial activities to analytics on social networks and knowledge graphs. It is built on top of a base with full-fledged stateful stream processing capabilities, extended with a series of graph-aware optimizations to address the space explosion and programming complexity issues of conventional join-based approaches. We propose new state backends and streaming operators that facilitate processing on dynamic graph-structured datasets, reducing space consumed by states. We develop a hybrid domain-specific language that embeds Gremlin into SQL, supporting both table and graph abstractions over streaming data. In addition to streaming workloads, GeaFlow is also extensively used for some batch processing jobs. In the largest deployments to date, GeaFlow is able to process tens of millions of events per second and manage hundreds of terabytes of states.}
}


@article{DBLP:journals/pacmmod/DongPPAESDPJKPZ23,
	author = {Siying Dong and
                  Shiva Shankar P. and
                  Satadru Pan and
                  Anand Ananthabhotla and
                  Dhanabal Ekambaram and
                  Abhinav Sharma and
                  Shobhit Dayal and
                  Nishant Vinaybhai Parikh and
                  Yanqin Jin and
                  Albert Kim and
                  Sushil Patil and
                  Jay Zhuang and
                  Sam Dunster and
                  Akanksha Mahajan and
                  Anirudh Chelluri and
                  Chaitanya Datye and
                  Lucas Vasconcelos Santana and
                  Nitin Garg and
                  Omkar Gawde},
	title = {Disaggregating RocksDB: {A} Production Experience},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {192:1--192:24},
	year = {2023},
	url = {https://doi.org/10.1145/3589772},
	doi = {10.1145/3589772},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DongPPAESDPJKPZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As in the general industry, there is a trend in Meta's data centers to migrate data from locally attached SSDs to cloud storage. We extended RocksDB [26], a widely used open-source storage engine designed and built for local SSDs, to leverage disaggregated storage. RocksDB's design, such as its data and log files' access patterns, makes an append-only distributed file system a desirable underlying storage. At Meta, we built disaggregated RocksDB using Tectonic File System [35], which so far had mainly been used for our data warehouse and blob storage stacks. We identified that metadata overhead and tail latencies were Tectonic's major performance gaps and addressed them accordingly. We improved the reliability, performance and other requirements with both general and customized optimizations to the core engine in RocksDB. We also took the time to deeply understand the common challenges presented by applications running on RocksDB and implemented enhancements to address them. This architecture enabled RocksDB to adapt to a more distributed architecture for performance enhancements.}
}


@article{DBLP:journals/pacmmod/Zhao0CTRXYCLZLL23,
	author = {Hanyu Zhao and
                  Zhi Yang and
                  Yu Cheng and
                  Chao Tian and
                  Shiru Ren and
                  Wencong Xiao and
                  Man Yuan and
                  Langshi Chen and
                  Kaibo Liu and
                  Yang Zhang and
                  Yong Li and
                  Wei Lin},
	title = {GoldMiner: Elastic Scaling of Training Data Pre-Processing Pipelines
                  for Deep Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {193:1--193:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589773},
	doi = {10.1145/3589773},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Zhao0CTRXYCLZLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Training data pre-processing pipelines are essential to deep learning (DL). As the performance of model training keeps increasing with both hardware advancements (e.g., faster GPUs) and various software optimizations, the data pre-processing on CPUs is becoming more resource-intensive and a severe bottleneck of the pipeline. This problem is even worse in the cloud, where training jobs exhibit diverse CPU-GPU demands that usually result in mismatches with fixed hardware configurations and resource fragmentation, degrading both training performance and cluster utilization. We introduce GoldMiner, an input data processing service for stateless operations used in pre-processing data for DL model training. GoldMiner decouples data pre-processing from model training into a new role called the data worker. Data workers facilitate scaling of data pre-processing to anywhere in a cluster, effectively pooling the resources across the cluster to satisfy the diverse requirements of training jobs. GoldMiner achieves this decoupling in a fully automatic and elastic manner. The key insight is that data pre-processing is inherently stateless, thus can be executed independently and elastically. This insight guides GoldMiner to automatically extract stateless computation out of a monolithic training program, efficiently disaggregate it across data workers, and elastically scale data workers to tune the resource allocations across jobs to optimize cluster efficiency. We have applied GoldMiner to industrial workloads, and our evaluation shows that GoldMiner can transform unmodified training programs to use data workers, accelerating individual training jobs by up to 12.1x. GoldMiner also improves average job completion time and aggregate GPU utilization by up to 2.5x and 2.1x in a 64-GPU cluster, respectively, by scheduling data workers with elasticity.}
}


@article{DBLP:journals/pacmmod/YangZYLOGZY23,
	author = {Xinying Yang and
                  Ruide Zhang and
                  Cong Yue and
                  Yang Liu and
                  Beng Chin Ooi and
                  Qun Gao and
                  Yuan Zhang and
                  Hao Yang},
	title = {VeDB: {A} Software and Hardware Enabled Trusted Relational Database},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {194:1--194:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589774},
	doi = {10.1145/3589774},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangZYLOGZY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain-like ledger databases emerge in recent years as a more efficient alternative to permissioned blockchains. Conventional ledger databases mostly rely on authenticated structures such as the Merkle tree and transparency logs for supporting auditability, and hence they suffer from the performance problem. As opposed to conventional ledger DBMSes, we design VeDB - a high-performance verifiable software (Ve-S) and hardware (Ve-H) enabled DBMS with rigorous auditability for better user options and broad applications. In Ve-S, we devise a novel verifiable Shrubs array (VSA) with two-layer ordinals (serial numbers) which outperforms conventional Merkle tree-based models due to lower CPU and I/O cost. It enables rigorous auditability through its efficient credible timestamp range authentication method, and fine-grained data verification at the client side, which are lacking in state-of-the-art relational ledger databases. In Ve-H, we devise a non-intrusive trusted affiliation by TEE leveraging digest signing, monotonic counters, and trusted timestamps in VeDB, which supports both data notarization and lineage applications. The experimental results show that VeDB-VSA outperforms Merkle tree-based authenticated data structures (ADS) up to 70× and 3.7× for insertion and verification; and VeDB Ve-H data lineage verification is 8.5× faster than Ve-S.}
}


@article{DBLP:journals/pacmmod/0018QHSHJR0023,
	author = {Chen Wang and
                  Jialin Qiao and
                  Xiangdong Huang and
                  Shaoxu Song and
                  Haonan Hou and
                  Tian Jiang and
                  Lei Rui and
                  Jianmin Wang and
                  Jiaguang Sun},
	title = {Apache IoTDB: {A} Time Series Database for IoT Applications},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {195:1--195:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589775},
	doi = {10.1145/3589775},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/0018QHSHJR0023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A typical industrial scenario encounters thousands of devices with millions of sensors, consistently generating billions of data points. It poses new requirements of time series data management, not well addressed in existing solutions, including (1) device-defined ever-evolving schema, (2) mostly periodical data collection, (3) strongly correlated series, (4) variously delayed data arrival, and (5) highly concurrent data ingestion. In this paper, we present a time series database management system, Apache IoTDB. It consists of (i) a time series native file format, TsFile, with specially designed data encoding, and (ii) an IoTDB engine for efficiently handling delayed data arrivals and processing queries. The system achieves a throughput of 10 million inserted values per second. Queries such as 1-day data selection of 0.1 million points and 3-year data aggregation over 10 million points can be processed in 100 ms. Comparisons with InfluxDB, TimescaleDB, KairosDB, Parquet and ORC over real world data loads demonstrate the superiority of IoTDB and TsFile.}
}


@article{DBLP:journals/pacmmod/AkidauBCHJLMPPS23,
	author = {Tyler Akidau and
                  Paul Barbier and
                  Istvan Cseri and
                  Fabian Hueske and
                  Tyler Jones and
                  Sasha Lionheart and
                  Daniel Mills and
                  Dzmitry Pauliukevich and
                  Lukas Probst and
                  Niklas Semmler and
                  Dan Sotolongo and
                  Boyuan Zhang},
	title = {What's the Difference? Incremental Processing with Change Queries
                  in Snowflake},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {196:1--196:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589776},
	doi = {10.1145/3589776},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AkidauBCHJLMPPS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incremental algorithms are the heart and soul of stream processing. Low latency results depend on the ability to react to the subset of changes in a dataset over time rather than reprocessing the entirety of a dataset as it evolves. But while the SQL language is well suited for representing streams of changes (via tables) and their application to tables over time (via DML), it entirely lacks a method to query the changes to a table or view in the first place. In this paper, we present CHANGES queries and STREAM objects, Snowflake's primitives for querying and consuming incremental changes to table objects over time. CHANGES queries and STREAMs have been in use within Snowflake for three years, and see broad adoption across our customers. We describe the semantics of these primitives, discuss the implementation challenges, present an analysis of their usage at Snowflake, and contrast with other offerings.}
}


@article{DBLP:journals/pacmmod/MohoneyPCMIMPR23,
	author = {Jason Mohoney and
                  Anil Pacaci and
                  Shihabur Rahman Chowdhury and
                  Ali Mousavi and
                  Ihab F. Ilyas and
                  Umar Farooq Minhas and
                  Jeffrey Pound and
                  Theodoros Rekatsinas},
	title = {High-Throughput Vector Similarity Search in Knowledge Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {197:1--197:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589777},
	doi = {10.1145/3589777},
	timestamp = {Fri, 08 Dec 2023 20:16:43 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/MohoneyPCMIMPR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is an increasing adoption of machine learning for encoding data into vectors to serve online recommendation and search use cases. As a result, recent data management systems propose augmenting query processing with online vector similarity search. In this work, we explore vector similarity search in the context of Knowledge Graphs (KGs). Motivated by the tasks of finding related KG queries and entities for past KG query workloads, we focus on hybrid vector similarity search (hybrid queries for short) where part of the query corresponds to vector similarity search and part of the query corresponds to predicates over relational attributes associated with the underlying data vectors. For example, given past KG queries for a song entity, we want to construct new queries for new song entities whose vector representations are close to the vector representation of the entity in the past KG query. But entities in a KG also have non-vector attributes such as a song associated with an artist, a genre, and a release date. Therefore, suggested entities must also satisfy query predicates over non-vector attributes beyond a vector-based similarity predicate. While these tasks are central to KGs, our contributions are generally applicable to hybrid queries. In contrast to prior works that optimize online queries, we focus on enabling efficient batch processing of past hybrid query workloads. We present our system, HQI, for high-throughput batch processing of hybrid queries. We introduce a workload-aware vector data partitioning scheme to tailor the vector index layout to the given workload and describe a multi-query optimization technique to reduce the overhead of vector similarity computations. We evaluate our methods on industrial workloads and demonstrate that HQI yields a 31× improvement in throughput for finding related KG queries compared to existing hybrid query processing approaches.}
}


@article{DBLP:journals/pacmmod/AnglesBD0GHLLMM23,
	author = {Renzo Angles and
                  Angela Bonifati and
                  Stefania Dumbrava and
                  George Fletcher and
                  Alastair Green and
                  Jan Hidders and
                  Bei Li and
                  Leonid Libkin and
                  Victor Marsault and
                  Wim Martens and
                  Filip Murlak and
                  Stefan Plantikow and
                  Ognjen Savkovic and
                  Michael Schmidt and
                  Juan Sequeda and
                  Slawek Staworko and
                  Dominik Tomaszuk and
                  Hannes Voigt and
                  Domagoj Vrgoc and
                  Mingxi Wu and
                  Dusan Zivkovic},
	title = {PG-Schema: Schemas for Property Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {198:1--198:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589778},
	doi = {10.1145/3589778},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AnglesBD0GHLLMM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Property graphs have reached a high level of maturity, witnessed by multiple robust graph database systems as well as the ongoing ISO standardization effort aiming at creating a new standard Graph Query Language (GQL). Yet, despite documented demand, schema support is limited both in existing systems and in the first version of the GQL Standard. It is anticipated that the second version of the GQL Standard will include a rich DDL. Aiming to inspire the development of GQL and enhance the capabilities of graph database systems, we propose PG-Schema, a simple yet powerful formalism for specifying property graph schemas. It features PG-Schema with flexible type definitions supporting multi-inheritance, as well as expressive constraints based on the recently proposed PG-Keys formalism. We provide the formal syntax and semantics of PG-Schema, which meet principled design requirements grounded in contemporary property graph management scenarios, and offer a detailed comparison of its features with those of existing schema languages and graph database systems.}
}


@article{DBLP:journals/pacmmod/WangLSYZLYWLYWC23,
	author = {Jianying Wang and
                  Tongliang Li and
                  Haoze Song and
                  Xinjun Yang and
                  Wenchao Zhou and
                  Feifei Li and
                  Baoyue Yan and
                  Qianqian Wu and
                  Yukun Liang and
                  Chengjun Ying and
                  Yujie Wang and
                  Baokai Chen and
                  Chang Cai and
                  Yubin Ruan and
                  Xiaoyi Weng and
                  Shibin Chen and
                  Liang Yin and
                  Chengzhong Yang and
                  Xin Cai and
                  Hongyan Xing and
                  Nanlong Yu and
                  Xiaofei Chen and
                  Dapeng Huang and
                  Jianling Sun},
	title = {PolarDB-IMCI: {A} Cloud-Native {HTAP} Database System at Alibaba},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {199:1--199:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589785},
	doi = {10.1145/3589785},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangLSYZLYWLYWC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud-native databases have become the de-facto choice for mission-critical applications on the cloud due to the need for high availability, resource elasticity, and cost efficiency. Meanwhile, driven by the increasing connectivity between data generation and analysis, users prefer a single database to efficiently process both OLTP and OLAP workloads, which enhances data freshness and reduces the complexity of data synchronization and the overall business cost. In this paper, we summarize five crucial design goals for a cloud-native HTAP database based on our experience and customers' feedback, i.e., transparency, competitive OLAP performance, minimal perturbation on OLTP workloads, high data freshness, and excellent resource elasticity. As our solution to realize these goals, we present PolarDB-IMCI, a cloud-native HTAP database system designed and deployed at Alibaba Cloud. Our evaluation results show that PolarDB-IMCI is able to handle HTAP efficiently on both experimental and production workloads; notably, it speeds up analytical queries up to ×149 on TPC-H (100GB). PolarDB-IMCI introduces low visibility delay and little performance perturbation on OLTP workloads (<5%), and resource elasticity can be achieved by scaling out in tens of seconds.}
}


@article{DBLP:journals/pacmmod/YuHWMCZLZ23,
	author = {Wenyuan Yu and
                  Tao He and
                  Lei Wang and
                  Ke Meng and
                  Ye Cao and
                  Diwen Zhu and
                  Sanhong Li and
                  Jingren Zhou},
	title = {Vineyard: Optimizing Data Sharing in Data-Intensive Analytics},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {200:1--200:27},
	year = {2023},
	url = {https://doi.org/10.1145/3589780},
	doi = {10.1145/3589780},
	timestamp = {Wed, 19 Mar 2025 21:16:37 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuHWMCZLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern data analytics and AI jobs become increasingly complex and involve multiple tasks performed on specialized systems. Sharing of intermediate data between different systems is often a significant bottleneck in such jobs. When the intermediate data is large, it is mostly exchanged through files in standard formats (e.g., CSV and ORC), causing high I/O and (de)serialization overheads. To solve these problems, we develop Vineyard, a high-performance, extensible, and cloud-native object store, trying to provide an intuitive experience for users to share data across systems in complex real-life workflows. Since different systems usually work on data structures (e.g., dataframes, graphs, hashmaps) with similar interfaces, and their computation logic is often loosely-coupled with how such interfaces are implemented over specific memory layouts, it enables Vineyard to conduct data sharing efficiently at a high level via memory mapping and method sharing. Vineyard provides an IDL named VCDL to facilitate users to register their own intermediate data types into Vineyard such that objects of the registered types can then be efficiently shared across systems in a polyglot workflow. As a cloud-native system, Vineyard is designed to work closely with Kubernetes, as well as achieve fault-tolerance and high performance in production environments. Evaluations on real-life datasets and data analytics jobs show that the above optimizations of Vineyard can significantly improve the end-to-end performance of data analytics jobs, by reducing their data-sharing time up to 68.4x.}
}


@article{DBLP:journals/pacmmod/LangeneckerSSB23,
	author = {Sven Langenecker and
                  Christoph Sturm and
                  Christian Schalles and
                  Carsten Binnig},
	title = {Steered Training Data Generation for Learned Semantic Type Detection},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {201:1--201:25},
	year = {2023},
	url = {https://doi.org/10.1145/3589786},
	doi = {10.1145/3589786},
	timestamp = {Fri, 07 Jul 2023 23:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LangeneckerSSB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we introduce STEER to adapt learned semantic type extraction approaches to a new, unseen data lake. STEER provides a data programming framework for semantic labeling which is used to generate new labeled training data with minimal overhead. At its core, STEER comes with a novel training data generation procedure called Steered-Labeling that can generate high quality training data not only for non-numeric but also for numerical columns. With this generated training data STEER is able to fine-tune existing learned semantic type extraction models. We evaluate our approach on four different data lakes and show that we can significantly improve the performance of two different types of learned models across all data lakes.}
}


@article{DBLP:journals/pacmmod/GuoJKS0ASSRC0G23,
	author = {Jinsong Guo and
                  Aditya Jami and
                  Markus Kr{\"{o}}ll and
                  Lukas Schweizer and
                  Sergey Paramonov and
                  Eric Aichinger and
                  Stefano Sferrazza and
                  Mattia Scaccia and
                  St{\'{e}}phane Reissfelder and
                  Eda Cicek and
                  Giovanni Grasso and
                  Georg Gottlob},
	title = {When Automatic Filtering Comes to the Rescue: Pre-Computing Company
                  Competitor Pairs in Owler},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {2},
	pages = {202:1--202:23},
	year = {2023},
	url = {https://doi.org/10.1145/3589787},
	doi = {10.1145/3589787},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GuoJKS0ASSRC0G23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Competitor data constitutes information significantly valuable for many business applications. Meltwater provides users with access to a large Company Information System (CIS), Owler, which contains competitor pairs and other useful information about companies. Meltwater has been seeking a practical solution to discover more competitor pairs in Owler. The first attempt, a fully-manual workflow (called MW_Manual) for finding more competitor pairs in Owler consisted of two manual steps: a filtering step that excludes obvious non-competitor company pairs, and a further inspection process that inspects each left company pair after the filtering step. MW_Manual was cost prohibitive because the results of the filtering step contained too many non-competitor pairs. Inspecting such non-competitor pairs caused an overhead to the overall workload. To reduce the manual workload, especially the required human effort in the manual inspection process, Meltwater has transformed MW_Manual into a semi-automatic workflow (called MW_CPFilter) by replacing the manual filtering with an automatic yet more precise process that adopts a system called CPFilter. This paper presents CPFilter, a system used in the filtering process of MW_CPFilter. CPFilter automatically pre-computes likely competitor pairs from existing competitor pairs in Owler. CPFilter combines (i) the generation of new competitor candidate pairs by inference from existing competitors and other company-specific knowledge, with (ii) the validation of each candidate competitor pair of two companies by checking whether or not empirical evidence that indicates the competitor relationships of these two companies can be found. CPFilter has three key advantages compared with the manual filtering process and previous works: (i) it resulted in a high workload reduction rate of 0.81, (ii) it is domain-independent so that it can be applied to different sectors in Owler, and (iii) its results are explainable so that humans can easily understand its results.}
}


@article{DBLP:journals/pacmmod/AgrawalMS23,
	author = {Divyakant Agrawal and
                  Alexandra Meliou and
                  S. Sudarshan},
	title = {{PACMMOD} Volume 1, Issue 3: Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {203:1--203:2},
	year = {2023},
	url = {https://doi.org/10.1145/3617307},
	doi = {10.1145/3617307},
	timestamp = {Sat, 13 Jan 2024 17:37:15 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgrawalMS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We are excited to introduce this new issue of PACMMOD (Proceedings of the ACM on Management of Data). PACMMOD is a new journal, concerned with the principles, algorithms, techniques, systems, and applications of database management systems, data management technology, and science and engineering of data. It includes articles reporting cutting-edge data management, data engineering, and data science research. Articles published at PACMMOD address data challenges at various stages of the data lifecycle, from modeling, acquisition, cleaning, integration, indexing, querying, analysis, exploration, visualization, interpretation, and explanation. They focus on dataintensive components of data pipelines, and solve problems in areas of interest to our community (e.g., data curation, optimization, performance, storage, systems), operating within accuracy, privacy, fairness, and diversity constraints. Articles reporting deployed systems and solutions to data science pipelines and/or fundamental experiences and insights from evaluating real-world data engineering problems are especially encouraged.}
}


@article{DBLP:journals/pacmmod/ChockchowwatLP23,
	author = {Supawit Chockchowwat and
                  Wenjie Liu and
                  Yongjoo Park},
	title = {AirIndex: Versatile Index Tuning Through Data and Storage},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {204:1--204:26},
	year = {2023},
	url = {https://doi.org/10.1145/3617308},
	doi = {10.1145/3617308},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChockchowwatLP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The end-to-end lookup latency of a hierarchical index---such as a B-tree or a learned index---is determined by its structure such as the number of layers, the kinds of branching functions appearing in each layer, the amount of data we must fetch from layers, etc. Our primary observation is that by optimizing those structural parameters (or designs) specifically to a target system's I/O characteristics (e.g., latency, bandwidth), we can offer a faster lookup compared to the ones that are not optimized. Can we develop a systematic method for finding those optimal design parameters? Ideally, the method must have the potential to generate almost any existing index or a novel combination of them for the fastest possible lookup. In this work, we present new data and an I/O-aware index builder (called AirIndex) that can find high-speed hierarchical index designs in a principled way. Specifically, AirIndex minimizes an objective function expressing the end-to-end latency in terms of various designs---the number of layers, types of layers, and more---for given data and a storage profile, using a graph-based optimization method purpose-built to address the computational challenges rising from the inter-dependencies among index layers and the exponentially many candidate parameters in a large search space. Our empirical studies confirm that AirIndex can find optimal index designs, build optimal indexes within the times comparable to existing methods, and deliver up to 4.1x faster lookup than a lightweight B-tree library (LMDB), 3.3x--46.3x faster than state-of-the-art learned indexes (RMI/CDFShop, PGM-index, ALEX/APEX, PLEX), and 2.0 faster than Data Calculator's suggestion on various dataset and storage settings.}
}


@article{DBLP:journals/pacmmod/ZhuWYZ23,
	author = {Rui Zhu and
                  Bin Wang and
                  Xiaochun Yang and
                  Baihua Zheng},
	title = {Closest Pairs Search Over Data Stream},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {205:1--205:26},
	year = {2023},
	url = {https://doi.org/10.1145/3617326},
	doi = {10.1145/3617326},
	timestamp = {Sat, 13 Jan 2024 17:37:15 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhuWYZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {k-closest pair (KCP for short) search is a fundamental problem in database research. Given a set of d-dimensional streaming data S, KCP search aims to retrieve k pairs with the shortest distances between them. While existing works have studied continuous 1-closest pair query (i.e., k=1) over dynamic data environments, which allow for object insertions/deletions, they require high computational costs and cannot easily support KCP search with k>1. This paper investigates the problem of KCP search over data stream, aiming to incrementally maintain as few pairs as possible to support KCP search with arbitrarily k. To achieve this, we introduce the concept of NNS (short for  N earest  N eighbour pair- S et), which consists of all the nearest neighbour pairs and allows us to support KCP search via only accessing O(k) objects. We further observe that in most cases, we only need to use a small portion of NNS to answer KCP search as typically kłl n. Based on this observation, we propose TNNS (short for  T hreshold-based  NN pair  S et), which contains a small number of high-quality NN pairs, and a partition named τ-DLBP (short for τ- D istance  L ower- B ound based  P artition) to organize objects, with τ being an integer significantly smaller than n. τ-DLBP organizes objects using up to O(łog n / τ) partitions and is able to support the construction and update of TNNS efficiently.}
}


@article{DBLP:journals/pacmmod/ZhengPWZZGQSBZDZL23,
	author = {Zhen Zheng and
                  Zaifeng Pan and
                  Dalin Wang and
                  Kai Zhu and
                  Wenyi Zhao and
                  Tianyou Guo and
                  Xiafei Qiu and
                  Minmin Sun and
                  Junjie Bai and
                  Feng Zhang and
                  Xiaoyong Du and
                  Jidong Zhai and
                  Wei Lin},
	title = {BladeDISC: Optimizing Dynamic Shape Machine Learning Workloads via
                  Compiler Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {206:1--206:29},
	year = {2023},
	url = {https://doi.org/10.1145/3617327},
	doi = {10.1145/3617327},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhengPWZZGQSBZDZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compiler optimization plays an increasingly important role to boost the performance of machine learning models for data processing and management. With increasingly complex data, the dynamic tensor shape phenomenon emerges for ML models. However, existing ML compilers either can only handle static shape models or expose a series of performance problems for both operator fusion optimization and code generation in dynamic shape scenes. This paper tackles the main challenges of dynamic shape optimization: the fusion optimization without shape value, and code generation supporting arbitrary shapes. To tackle the fundamental challenge of the absence of shape values, it systematically abstracts and excavates the shape information and designs a cross-level symbolic shape representation. With the insight that what fusion optimization relies upon is tensor shape relationships between adjacent operators rather than exact shape values, it proposes the dynamic shape fusion approach based on shape information propagation. To generate code that adapts to arbitrary shapes efficiently, it proposes a compile-time and runtime combined code generation approach. Finally, it presents a complete optimization pipeline for dynamic shape models and implements an industrial-grade ML compiler, named BladeDISC. The extensive evaluation demonstrates that BladeDISC outperforms PyTorch, TorchScript, TVM, ONNX Runtime, XLA, Torch Inductor (dynamic shape), and TensorRT by up to 6.95×, 6.25×, 4.08×, 2.04×, 2.06×, 7.92×, and 4.16× (3.54×, 3.12×, 1.95×, 1.47×, 1.24×, 2.93×, and 1.46× on average) in terms of end-to-end inference speedup on the A10 and T4 GPU, respectively. BladeDISC's source code is publicly available at https://github.com/alibaba/BladeDISC.}
}


@article{DBLP:journals/pacmmod/GuoFZW23,
	author = {Qintian Guo and
                  Chen Feng and
                  Fangyuan Zhang and
                  Sibo Wang},
	title = {Efficient Algorithm for Budgeted Adaptive Influence Maximization:
                  An Incremental RR-set Update Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {207:1--207:26},
	year = {2023},
	url = {https://doi.org/10.1145/3617328},
	doi = {10.1145/3617328},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GuoFZW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a graph G, a cost associated with each node, and a budget B, the budgeted influence maximization (BIM) aims to find the optimal set S of seed nodes that maximizes the influence among all possible sets such that the total cost of nodes in S is no larger than B. Existing solutions mainly follow the non-adaptive idea, i.e., determining all the seeds before observing any actual diffusion. Due to the absence of actual diffusion information, they may result in unsatisfactory influence spread. Motivated by the limitation of existing solutions, in this paper, we make the first attempt to solve the BIM problem under the adaptive setting, where seed nodes are iteratively selected after observing the diffusion result of the previous seeds. We design the first practical algorithm which achieves an expected approximation guarantee by probabilistically adopting a cost-aware greedy idea or a single influential node. Further, we develop an optimized version to improve its practical performance in terms of influence spread. Besides, the scalability issues of the adaptive IM-related problems still remain open. It is because they usually involve multiple rounds (e.g., equal to the number of seeds) and in each round, they have to construct sufficient new reverse-reachable set (RR-set) samples such that the claimed approximation guarantee can actually hold. However, this incurs prohibitive computation, imposing limitations on real applications. To solve this dilemma, we propose an incremental update approach. Specifically, it maintains extra construction information when building RR-sets, and then it can quickly correct a problematic RR-set from the very step where it is first affected. As a result, we recycle the RR-sets at a small computational cost, while still providing correctness guarantee. Finally, extensive experiments on large-scale real graphs demonstrate the superiority of our algorithms over baselines in terms of both influence spread and running time.}
}


@article{DBLP:journals/pacmmod/LuoYFZ23,
	author = {Wensheng Luo and
                  Qiaoyuan Yang and
                  Yixiang Fang and
                  Xu Zhou},
	title = {Efficient Core Maintenance in Large Bipartite Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {208:1--208:26},
	year = {2023},
	url = {https://doi.org/10.1145/3617329},
	doi = {10.1145/3617329},
	timestamp = {Sat, 13 Jan 2024 17:37:15 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuoYFZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an important cohesive subgraph model in bipartite graphs, the (α, β)-core (a.k.a. bi-core) has found a wide spectrum of real-world applications, such as product recommendation, fraudster detection, and community search. In these applications, the bipartite graphs are often large and dynamic, where vertices and edges are inserted and deleted frequently, so it is costly to recompute (α, β)-cores from scratch when the graph has changed. Recently, a few works have attempted to study how to maintain (α, β)-cores in the dynamic bipartite graph, but their performance is still far from perfect, due to the huge size of graphs and their frequent changes. To alleviate this issue, in this paper we present efficient (α, β)-core maintenance algorithms over bipartite graphs. We first introduce a novel concept, called bi-core numbers, for the vertices of bipartite graphs. Based on this concept, we theoretically analyze the effect of inserting and deleting edges on the changes of vertices' bi-core numbers, which can be further used to narrow down the scope of the updates, thereby reducing the computational redundancy. We then propose efficient (α, β)-core maintenance algorithms for handling the edge insertion and edge deletion respectively, by exploiting the above theoretical analysis results. Finally, extensive experimental evaluations are performed on both real and synthetic datasets, and the results show that our proposed algorithms are up to two orders of magnitude faster than the state-of-the-art approaches.}
}


@article{DBLP:journals/pacmmod/Chang23,
	author = {Lijun Chang},
	title = {Efficient Maximum k-Defective Clique Computation with Improved Time
                  Complexity},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {209:1--209:26},
	year = {2023},
	url = {https://doi.org/10.1145/3617313},
	doi = {10.1145/3617313},
	timestamp = {Sat, 13 Jan 2024 17:37:15 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Chang23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {k-defective cliques relax cliques by allowing up-to k missing edges from being a complete graph. This relaxation enables us to find larger near-cliques and has applications in link prediction, cluster detection, social network analysis and transportation science. The problem of finding the largest k-defective clique has been recently studied with several algorithms being proposed in the literature. However, the currently fastest algorithm KDBB does not improve its time complexity from being the trivial O(2n), and also, KDBB's practical performance is still not satisfactory. In this paper, we advance the state of the art for exact maximum k-defective clique computation, in terms of both time complexity and practical performance. Moreover, we separate the techniques required for achieving the time complexity from others purely used for practical performance consideration; this design choice may facilitate the research community to further improve the practical efficiency while not sacrificing the worst case time complexity. In specific, we first develop a general framework kDC that beats the trivial time complexity of O(2n) and achieves a better time complexity than all existing algorithms. The time complexity of kDC is solely achieved by our newly designed non-fully-adjacent-first branching rule, excess-removal reduction rule and high-degree reduction rule. Then, to make kDC practically efficient, we further propose a new upper bound, two new reduction rules, and an algorithm for efficiently computing a large initial solution. Extensive empirical studies on three benchmark graph collections with 290 graphs in total demonstrate that kDC outperforms the currently fastest algorithm KDBB by several orders of magnitude.}
}


@article{DBLP:journals/pacmmod/FanFLTY23,
	author = {Lihang Fan and
                  Wenfei Fan and
                  Ping Lu and
                  Chao Tian and
                  Qiang Yin},
	title = {Enriching Recommendation Models with Logic Conditions},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {210:1--210:28},
	year = {2023},
	url = {https://doi.org/10.1145/3617330},
	doi = {10.1145/3617330},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanFLTY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes RecLogic, a framework for improving the accuracy of machine learning (ML) models for recommendation. It aims to enhance existing ML models with logic conditions to reduce false positives and false negatives, without training a new model. Underlying RecLogic are (a) a class of prediction rules on graphs, denoted by TIEs, (b) a new approach to learning TIEs, and (c) a new paradigm for recommendation with TIEs. TIEs may embed ML recommendation models as predicates; as opposed to prior graph rules, it is tractable to decide whether a graph satisfies a set of TIEs. To enrich ML models, RecLogic iteratively trains a generator with feedback from each round, to learn TIEs with a probabilistic bound. RecLogic also provides a PTIME parallel algorithm for making recommendations with the learned TIEs. Using real-life data, we empirically verify that RecLogic improves the accuracy of ML predictions by 22.89% on average in an area where the prediction strength is neither sufficiently large nor sufficiently small, up to 33.10%.}
}


@article{DBLP:journals/pacmmod/YuL23a,
	author = {Kaiqiang Yu and
                  Cheng Long},
	title = {Fast Maximal Quasi-clique Enumeration: {A} Pruning and Branching Co-Design
                  Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {211:1--211:26},
	year = {2023},
	url = {https://doi.org/10.1145/3617331},
	doi = {10.1145/3617331},
	timestamp = {Tue, 22 Oct 2024 20:38:18 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuL23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mining cohesive subgraphs from a graph is a fundamental problem in graph data analysis. One notable cohesive structure is γ-quasi-clique (QC), where each vertex connects at least a fraction γ of the other vertices inside. Enumerating maximal γ-quasi-cliques (MQCs) of a graph has been widely studied and used for many applications such as community detection and significant biomolecule structure discovery. One common practice of finding all MQCs is to (1) find a set of QCs containing all MQCs and then (2) filter out non-maximal QCs. While quite a few algorithms have been developed (which are branch-and-bound algorithms) for finding a set of QCs that contains all MQCs, all focus on sharpening the pruning techniques and devote little effort to improving the branching part. As a result, they provide no guarantee on pruning branches and all have the worst-case time complexity of O*(2n), where O* suppresses the polynomials and n is the number of vertices in the graph. In this paper, we focus on the problem of finding a set of QCs containing all MQCs but deviate from further sharpening the pruning techniques as existing methods do. We pay attention to both the pruning and branching parts and develop new pruning techniques and branching methods that would suit each other better towards pruning more branches both theoretically and practically. Specifically, we develop a new branch-and-bound algorithm called FastQC based on newly developed pruning techniques and branching methods, which improves the worst-case time complexity to O*(αkn), where αk is a positive real number strictly smaller than 2. Furthermore, we develop a divide-and-conquer strategy for boosting the performance of FastQC. Finally, we conduct extensive experiments on both real and synthetic datasets, and the results show that our algorithms are up to two orders of magnitude faster than the state-of-the-art on real datasets.}
}


@article{DBLP:journals/pacmmod/LiCGPGY23,
	author = {Anran Li and
                  Yue Cao and
                  Jiabao Guo and
                  Hongyi Peng and
                  Qing Guo and
                  Han Yu},
	title = {FedCSS: Joint Client-and-Sample Selection for Hard Sample-Aware Noise-Robust
                  Federated Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {212:1--212:24},
	year = {2023},
	url = {https://doi.org/10.1145/3617332},
	doi = {10.1145/3617332},
	timestamp = {Thu, 01 May 2025 20:35:10 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiCGPGY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables a large number of data owners (a.k.a. FL clients) to jointly train a machine learning model without disclosing private local data. The importance of local data samples to the FL model vary widely. This is exacerbated by the presence of noisy data, which exhibit large losses similar to important (hard) samples. Currently, there lacks an FL approach that can effectively distinguish hard samples (which are beneficial) from noisy samples (which are harmful). To bridge this gap, we propose the Federated Client and Sample Selection (FedCSS) approach. It is a bilevel optimization approach for FL client-and-sample selection to achieve hard sample-aware noise-robust learning in a privacy preserving manner. It performs meta-learning based online approximation to iteratively update global FL models, select the most positively influential samples and deal with training data noise. Theoretical analysis shows that it is guaranteed to converge in an efficient manner. Experimental comparison against six state-of-the-art baselines on five real-world datasets in the presence of data noise and heterogeneity shows that it achieves up to 26.4% higher test accuracy, while saving communication and computation costs by at least 41.5% and 1.2%, respectively.}
}


@article{DBLP:journals/pacmmod/MoCLS23,
	author = {Dingheng Mo and
                  Fanchao Chen and
                  Siqiang Luo and
                  Caihua Shan},
	title = {Learning to Optimize LSM-trees: Towards {A} Reinforcement Learning
                  based Key-Value Store for Dynamic Workloads},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {213:1--213:25},
	year = {2023},
	url = {https://doi.org/10.1145/3617333},
	doi = {10.1145/3617333},
	timestamp = {Sat, 13 Jan 2024 17:37:15 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/MoCLS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LSM-trees are widely adopted as the storage backend of key-value stores. However, optimizing the system performance under dynamic workloads has not been sufficiently studied or evaluated in previous work. To fill the gap, we present RusKey, a key-value store with the following new features: (1) RusKey is a first attempt to orchestrate LSM-tree structures online to enable robust performance under the context of dynamic workloads; (2) RusKey is the first study to use Reinforcement Learning (RL) to guide LSM-tree transformations; (3) RusKey includes a new LSM-tree design, named FLSM-tree, for an efficient transition between different compaction policies -- the bottleneck of dynamic key-value stores. We justify the superiority of the new design with theoretical analysis; (4) RusKey requires no prior workload knowledge for system adjustment, in contrast to state-of-the-art techniques. Experiments show that RusKey exhibits strong performance robustness in diverse workloads, achieving up to 4x better end-to-end performance than the RocksDB system under various settings.}
}


@article{DBLP:journals/pacmmod/HuangYDLDS23,
	author = {He Huang and
                  Jiakun Yu and
                  Yang Du and
                  Jia Liu and
                  Haipeng Dai and
                  Yu{-}E Sun},
	title = {Memory-Efficient and Flexible Detection of Heavy Hitters in High-Speed
                  Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {214:1--214:24},
	year = {2023},
	url = {https://doi.org/10.1145/3617334},
	doi = {10.1145/3617334},
	timestamp = {Wed, 12 Mar 2025 16:02:49 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangYDLDS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heavy-hitter detection is a fundamental task in network traffic measurement and security. Existing work faces the dilemma of suffering dynamic and imbalanced traffic characteristics or lowering the detection efficiency and flexibility. In this paper, we propose a flexible sketch called SwitchSketch that embraces dynamic and skewed traffic for efficient and accurate heavy-hitter detection. The key idea of SwitchSketch is allowing the sketch to dynamically switch among different modes and take full use of each bit of the memory. We present an encoding-based switching scheme together with a flexible bucket structure to jointly achieve this goal by using a combination of design features, including variable-length cells, shrunk counters, embedded metadata, and switchable modes. We further implement SwitchSketch on the NetFPGA-1G-CML board. Experimental results based on real Internet traces show that SwitchSketch achieves a high Fβ-Score of threshold-t detection (consistently higher than 0.938) and over 99% precision rate of top-k detection under a tight memory size (e.g., 100KB). Besides, it outperforms the state-of-the-art by reducing the ARE by 30.77%\\sim99.96%. All related implementations are open-sourced.}
}


@article{DBLP:journals/pacmmod/FengQC23,
	author = {Zijin Feng and
                  Miao Qiao and
                  Hong Cheng},
	title = {Modularity-based Hypergraph Clustering: Random Hypergraph Model, Hyperedge-cluster
                  Relation, and Computation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {215:1--215:25},
	year = {2023},
	url = {https://doi.org/10.1145/3617335},
	doi = {10.1145/3617335},
	timestamp = {Sat, 13 Jan 2024 17:37:15 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FengQC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A graph models the connections among objects. One important graph analytical task is clustering which partitions a data graph into clusters with dense innercluster connections. A line of clustering maximizes a function called modularity. Modularity-based clustering is widely adopted on dyadic graphs due to its scalability and clustering quality which depends highly on its selection of a random graph model. The random graph model decides not only which clustering is preferred - modularity measures the quality of a clustering based on its alignment to the edges of a random graph, but also the cost of computing such an alignment. Existing random hypergraph models either measure the hyperedge-cluster alignment in an All-Or-Nothing (AON) manner, losing important group-wise information, or introduce expensive alignment computation, refraining the clustering from scaling up. This paper proposes a new random hypergraph model called Hyperedge Expansion Model (HEM), a non-AON hypergraph modularity function called Partial Innerclusteredge modularity (PI) based on HEM, a clustering algorithm called Partial Innerclusteredge Clustering (PIC) that optimizes PI, and novel computation optimizations. PIC is a scalable modularity-based hypergraph clustering that can effectively capture the non-AON hyperedge-cluster relation. Our experiments show that PIC outperforms eight state-of-the-art methods on real-world hypergraphs in terms of both clustering quality and scalability and is up to five orders of magnitude faster than the baseline methods.}
}


@article{DBLP:journals/pacmmod/ShiYW23,
	author = {Ge Shi and
                  Ziyi Yan and
                  Tianzheng Wang},
	title = {OptiQL: Robust Optimistic Locking for Memory-Optimized Indexes},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {216:1--216:26},
	year = {2023},
	url = {https://doi.org/10.1145/3617336},
	doi = {10.1145/3617336},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShiYW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern memory-optimized indexes often use optimistic locks for concurrent accesses. Read operations can proceed optimistically without taking the lock, greatly improving performance on multicore CPUs. But this is at the cost of robustness against contention where many threads contend on a small set of locks, causing excessive cacheline invalidation, interconnect traffic and eventually performance collapse. Yet existing solutions often sacrifice desired properties such as compact 8-byte lock size and fairness among lock requesters. This paper presents optimistic queuing lock (OptiQL), a new optimistic lock for database indexing to solve this problem. OptiQL extends the classic MCS lock---a fair, compact and robust mutual exclusion lock---with optimistic read capabilities for index workloads to achieve both robustness and high performance while maintaining various desirable properties. Evaluation using memory-optimized B+-trees on a 40-core, dual-socket server shows that OptiQL matches existing optimistic locks for read operations, while avoiding performance collapse under high contention.}
}


@article{DBLP:journals/pacmmod/LinWHGYLJ23,
	author = {Yan Lin and
                  Huaiyu Wan and
                  Jilin Hu and
                  Shengnan Guo and
                  Bin Yang and
                  Youfang Lin and
                  Christian S. Jensen},
	title = {Origin-Destination Travel Time Oracle for Map-based Services},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {217:1--217:27},
	year = {2023},
	url = {https://doi.org/10.1145/3617337},
	doi = {10.1145/3617337},
	timestamp = {Sat, 13 Jan 2024 17:37:15 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LinWHGYLJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given an origin (O), a destination (D), and a departure time (T), an Origin-Destination (OD) travel time oracle~(ODT-Oracle) returns an estimate of the time it takes to travel from O to D when departing at T. ODT-Oracles serve important purposes in map-based services. To enable the construction of such oracles, we provide a travel-time estimation (TTE) solution that leverages historical trajectories to estimate time-varying travel times for OD pairs. The problem is complicated by the fact that multiple historical trajectories with different travel times may connect an OD pair, while trajectories may vary from one another. To solve the problem, it is crucial to remove outlier trajectories when doing travel time estimation for future queries. We propose a novel, two-stage framework called Diffusion-based Origin-destination Travel Time Estimation (DOT), that solves the problem. First, DOT employs a conditioned Pixelated Trajectories (PiT) denoiser that enables building a diffusion-based PiT inference process by learning correlations between OD pairs and historical trajectories. Specifically, given an OD pair and a departure time, we aim to infer a PiT. Next, DOT encompasses a Masked Vision Transformer~(MViT) that effectively and efficiently estimates a travel time based on the inferred PiT. We report on extensive experiments on two real-world datasets that offer evidence that DOT is capable of outperforming baseline methods in terms of accuracy, scalability, and explainability.}
}


@article{DBLP:journals/pacmmod/SiddiqiKB23,
	author = {Shafaq Siddiqi and
                  Roman Kern and
                  Matthias Boehm},
	title = {{SAGA:} {A} Scalable Framework for Optimizing Data Cleaning Pipelines
                  for Machine Learning Applications},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {218:1--218:26},
	year = {2023},
	url = {https://doi.org/10.1145/3617338},
	doi = {10.1145/3617338},
	timestamp = {Sat, 13 Jan 2024 17:37:15 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SiddiqiKB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the exploratory data science lifecycle, data scientists often spent the majority of their time finding, integrating, validating and cleaning relevant datasets. Despite recent work on data validation, and numerous error detection and correction algorithms, in practice, data cleaning for ML remains largely a manual, unpleasant, and labor-intensive trial and error process, especially in large-scale, distributed computation. The target ML application---such as classification or regression models---can be used as a signal of valuable feedback though, for selecting effective data cleaning strategies. In this paper, we introduce SAGA, a framework for automatically generating the top-K most effective data cleaning pipelines. SAGA adopts ideas from Auto-ML, feature selection, and hyper-parameter tuning. Our framework is extensible for user-provided constraints, new data cleaning primitives, and ML applications; automatically generates hybrid runtime plans of local and distributed operations; and performs pruning by interesting properties (e.g., monotonicity). Instead of full automation---which is rather unrealistic---SAGA simplifies the mechanical aspects of data cleaning. Our experiments show that SAGA yields robust accuracy improvements over state-of-the-art, and good scalability regarding increasing data sizes and number of evaluated pipelines.}
}


@article{DBLP:journals/pacmmod/LuoWYWL23,
	author = {Qiyao Luo and
                  Yilei Wang and
                  Ke Yi and
                  Sheng Wang and
                  Feifei Li},
	title = {Secure Sampling for Approximate Multi-party Query Processing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {219:1--219:27},
	year = {2023},
	url = {https://doi.org/10.1145/3617339},
	doi = {10.1145/3617339},
	timestamp = {Thu, 08 Aug 2024 08:05:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuoWYWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of random sampling in the secure multi-party computation (MPC) model. In MPC, taking a sample securely must have a cost Ω(n) irrespective to the sample size s. This is in stark contrast with the plaintext setting, where a sample can be taken in O(s) time trivially. Thus, the goal of approximate query processing (AQP) with sublinear costs seems unachievable under MPC. To get around this inherent barrier, in this paper we take a two-stage approach: In the offline stage, we generate a batch of n/s samples with (n) total cost, which can then be consumed to answer queries as they arrive online. Such an approach allows us to achieve an Õ(s) amortized cost per query, similar to the plaintext setting. Based on our secure batch sampling algorithms, we build MASQUE, an MPC-AQP system that achieves sublinear online query costs by running an MPC protocol to evaluate the queries on pre-generated samples. MASQUE achieves the strong security guarantee of the MPC model, i.e., nothing is revealed beyond the query result, which itself can be further protected by (amplified) differential privacy}
}


@article{DBLP:journals/pacmmod/SioulasMA23,
	author = {Panagiotis Sioulas and
                  Ioannis Mytilinis and
                  Anastasia Ailamaki},
	title = {{SH2O:} Efficient Data Access for Work-Sharing Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {220:1--220:26},
	year = {2023},
	url = {https://doi.org/10.1145/3617340},
	doi = {10.1145/3617340},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SioulasMA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interactive applications require processing tens to hundreds of concurrent analytical queries within tight time constraints. In such setups, where high concurrency causes contention, work-sharing databases are critical for improving scalability and for bounding the increase in response time. However, as such databases share data access using full scans and expensive shared filters, they suffer from a data-access bottleneck that jeopardizes interactivity. We present SH2O: a novel data-access operator that addresses the data-access bottleneck of work-sharing databases. SH2O is based on the idea that an access pattern based on judiciously selected multidimensional ranges can replace a set of shared filters. To exploit the idea in an efficient and scalable manner, SH2O uses a three-tier approach: i) it uses spatial indices to efficiently access the ranges without overfetching, ii) it uses an optimizer to choose which filters to replace such that it maximizes cost-benefit for index accesses, and iii) it exploits partitioning schemes and independently accesses each data partition to reduce the number of filters in the access pattern. Furthermore, we propose a tuning strategy that chooses a partitioning and indexing scheme that minimizes SH2O's cost for a target workload. Our evaluation shows a speedup of 1.8-22.2 for batches of hundreds of data-access-bound queries.}
}


@article{DBLP:journals/pacmmod/DhulipalaLLM23,
	author = {Laxman Dhulipala and
                  Jakub Lacki and
                  Jason Lee and
                  Vahab Mirrokni},
	title = {TeraHAC: Hierarchical Agglomerative Clustering of Trillion-Edge Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {3},
	pages = {221:1--221:27},
	year = {2023},
	url = {https://doi.org/10.1145/3617341},
	doi = {10.1145/3617341},
	timestamp = {Mon, 05 Feb 2024 20:26:49 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DhulipalaLLM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce TeraHAC, a (1+ε)-approximate hierarchical agglomerative clustering (HAC) algorithm which scales to trillion-edge graphs. Our algorithm is based on a new approach to computing (1+ε)-approximate HAC, which is a novel combination of the nearest-neighbor chain algorithm and the notion of (1+ε)-approximate HAC. Our approach allows us to partition the graph among multiple machines and make significant progress in computing the clustering within each partition before any communication with other partitions is needed. We evaluate TeraHAC on a number of real-world and synthetic graphs of up to 8 trillion edges. We show that TeraHAC requires over 100x fewer rounds compared to previously known approaches for computing HAC. It is up to 8.3x faster than SCC, the state-of-the-art distributed algorithm for hierarchical clustering, while achieving 1.16x higher quality. In fact, TeraHAC essentially retains the quality of the celebrated HAC algorithm while significantly improving the running time.}
}


@article{DBLP:journals/pacmmod/AgrawalMS23a,
	author = {Divyakant Agrawal and
                  Alexandra Meliou and
                  S. Sudarshan},
	title = {{PACMMOD} Volume 1 Issue 4: Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {222:1--222:2},
	year = {2023},
	url = {https://doi.org/10.1145/3626709},
	doi = {10.1145/3626709},
	timestamp = {Sat, 13 Jan 2024 17:37:15 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgrawalMS23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Welcome to this issue of the Proceedings of the ACM on Management of Data (Volume 1, Issue 4 (SIGMOD)). While this issue has papers from the SIGMOD track, PACMMOD will soon also have issues with papers from the newly created PODS track. Out of 189 submissions to the round of reviewing for the PACMMOD SIGMOD track whose submission deadline was April 15, 2023, a total of 49 articles were accepted, and are presented in this issue.}
}


@article{DBLP:journals/pacmmod/HaynesAPLJT23,
	author = {Brandon Haynes and
                  Rana Alotaibi and
                  Anna Pavlenko and
                  Jyoti Leeka and
                  Alekh Jindal and
                  Yuanyuan Tian},
	title = {GEqO: ML-Accelerated Semantic Equivalence Detection},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {223:1--223:25},
	year = {2023},
	url = {https://doi.org/10.1145/3626710},
	doi = {10.1145/3626710},
	timestamp = {Mon, 03 Feb 2025 10:38:34 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HaynesAPLJT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large scale analytics engines have become a core dependency for modern data-driven enterprises to derive business insights and drive actions. These engines support a large number of analytic jobs processing huge volumes of data on a daily basis, and workloads are often inundated with overlapping computations across multiple jobs. Reusing common computation is crucial for efficient cluster resource utilization and reducing job execution time. Detecting common computation is the first and key step for reducing this computational redundancy. However, detecting equivalence on large-scale analytics engines requires efficient and scalable solutions that are fully automated. In addition, to maximize computation reuse, equivalence needs to be detected at the semantic level instead of just the syntactic level (i.e., the ability to detect semantic equivalence of seemingly different-looking queries). Unfortunately, existing solutions fall short of satisfying these requirements. In this paper, we take a major step towards filling this gap by proposing GEqO, a portable and lightweight machine-learning-based framework for efficiently identifying semantically equivalent computations at scale. GEqO introduces two machine-learning-based filters that quickly prune out nonequivalent subexpressions and employs a semi-supervised learning feedback loop to iteratively improve its model with an intelligent sampling mechanism. Further, with its novel database-agnostic featurization method, GEqO can transfer the learning from one workload and database to another. Our extensive empirical evaluation shows that, on TPC-DS-like queries, GEqO yields significant performance gains-up to 200x faster than automated verifiers-and finds up to 2x more equivalences than optimizer and signature-based equivalence detection approaches.}
}


@article{DBLP:journals/pacmmod/GenossarGS23,
	author = {Bar Genossar and
                  Avigdor Gal and
                  Roee Shraga},
	title = {The Battleship Approach to the Low Resource Entity Matching Problem},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {224:1--224:25},
	year = {2023},
	url = {https://doi.org/10.1145/3626711},
	doi = {10.1145/3626711},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GenossarGS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity matching, a core data integration problem, is the task of deciding whether two data tuples refer to the same real-world entity. Recent advances in deep learning methods, using pre-trained language models, were proposed for resolving entity matching. Although demonstrating unprecedented results, these solutions suffer from a major drawback as they require large amounts of labeled data for training, and, as such, are inadequate to be applied to low resource entity matching problems. To overcome the challenge of obtaining sufficient labeled data we offer a new active learning approach, focusing on a selection mechanism that exploits unique properties of entity matching. We argue that a distributed representation of a tuple pair indicates its informativeness when considered among other pairs. This is used consequently in our approach that iteratively utilizes space-aware considerations. Bringing it all together, we treat the low resource entity matching problem as a Battleship game, hunting indicative samples, focusing on positive ones, through awareness of the latent space along with careful planning of next sampling iterations. An extensive experimental analysis shows that the proposed algorithm outperforms state-of-the-art active learning solutions to low resource entity matching, and although using less samples, can be as successful as state-of-the-art fully trained known algorithms.}
}


@article{DBLP:journals/pacmmod/HuangWL23,
	author = {Yicong Huang and
                  Zuozhi Wang and
                  Chen Li},
	title = {Udon: Efficient Debugging of User-Defined Functions in Big Data Systems
                  with Line-by-Line Control},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {225:1--225:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626712},
	doi = {10.1145/3626712},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many big data systems are written in languages such as C, C++, Java, and Scala to process large amounts of data efficiently, while data analysts often use Python to conduct data wrangling, statistical analysis, and machine learning. User-defined functions (UDFs) are commonly used in these systems to bridge the gap between the two ecosystems. In this paper, we propose Udon, a novel debugger to support fine-grained debugging of UDFs. Udon encapsulates the modern line-by-line debugging primitives, such as the ability to set breakpoints, perform code inspections, and make code modifications while executing a UDF on a single tuple. It includes a novel debug-aware UDF execution model to ensure the responsiveness of the operator during debugging. It utilizes advanced state-transfer techniques to satisfy breakpoint conditions that span across multiple UDFs. It incorporates various optimization techniques to reduce the runtime overhead. We conduct experiments with multiple UDF workloads on various datasets and show its high efficiency and scalability.}
}


@article{DBLP:journals/pacmmod/ChenLCJJSS23,
	author = {Zehao Chen and
                  Bingzhe Li and
                  Xiaojun Cai and
                  Zhiping Jia and
                  Lei Ju and
                  Zili Shao and
                  Zhaoyan Shen},
	title = {ChainKV: {A} Semantics-Aware Key-Value Store for Ethereum System},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {226:1--226:23},
	year = {2023},
	url = {https://doi.org/10.1145/3626713},
	doi = {10.1145/3626713},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenLCJJSS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Log-Structure Merged tree (LSM-tree) based key-value (KV) store has been widely adopted as the storage engine for blockchain systems, such as Ethereum, in which blockchain data are uniformly transformed into randomly distributed KV items for persistence. However, blockchain semantics are ignored during this process, making the blockchain storage suffer from heavy read/write amplification problems. Moreover, as the Ethereum network scales up, tremendous data further exacerbates its storage burden. Until now, most studies have focused on sharding, data archiving, decentralized distributed storage, etc., to mitigate the burden of the storage layer. However, the incompatibility between Ethereum semantics and the characteristics of the storage engine is ignored. In this paper, we present ChainKV, a new semantics-aware storage paradigm to improve the storage management performance for the Ethereum system. Firstly, based on Ethereum blockchain semantics, ChainKV separately stores different types of data in multiple storage zones in the KV store to mitigate the read/write amplification problem. Secondly, following the mechanism of the verification process in the authenticated data structure (ADS), a new ADS data transformer is proposed to exploit the data locality when persisting ADS. Moreover, a new space gaming caching policy is adopted to coordinate the cache space management for two independent storage zones. Finally, we propose an optional lightweight node crash recovery mechanism to eliminate functional redundancy between the Ethereum protocol and the storage engine. The experimental results indicate that ChainKV outperforms the prior Ethereum systems by up to 1.99× and 4.20× for synchronization and query operations, respectively}
}


@article{DBLP:journals/pacmmod/DingWYZXCPL23,
	author = {Haoran Ding and
                  Zhaoguo Wang and
                  Yicun Yang and
                  Dexin Zhang and
                  Zhenglin Xu and
                  Haibo Chen and
                  Ruzica Piskac and
                  Jinyang Li},
	title = {Proving Query Equivalence Using Linear Integer Arithmetic},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {227:1--227:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626768},
	doi = {10.1145/3626768},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DingWYZXCPL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Proving the equivalence between SQL queries is a fundamental problem in database research. Existing solvers model queries using algebraic representations and convert such representations into first-order logic formulas so that query equivalence can be verified by solving a satisfiability problem. The main challenge lies in "unbounded summations", which appear commonly in a query\'s algebraic representation in order to model common SQL features, such as projection and aggregate functions. Unfortunately, existing solvers handle unbounded summations in an ad-hoc manner based on heuristics or syntax comparison, which severely limits the set of queries that can be supported. This paper develops a new SQL equivalence prover called SQLSolver, which can handle unbounded summations in a principled way. Our key insight is to use the theory of LIA^*, which extends linear integer arithmetic formulas with unbounded sums and provides algorithms to translate a LIA^* formula to a LIA formula that can be decided using existing SMT solvers. We augment the basic LIA^* theory to handle several complex scenarios (such as nested unbounded summations) that arise from modeling real-world queries. We evaluate SQLSolver with 359 equivalent query pairs derived from the SQL rewrite rules in Calcite and Spark SQL. SQLSolver successfully proves 346 pairs of them, which significantly outperforms existing provers.}
}


@article{DBLP:journals/pacmmod/MakhijaG23,
	author = {Neha Makhija and
                  Wolfgang Gatterbauer},
	title = {A Unified Approach for Resilience and Causal Responsibility with Integer
                  Linear Programming {(ILP)} and {LP} Relaxations},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {228:1--228:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626715},
	doi = {10.1145/3626715},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/MakhijaG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {What is a minimal set of tuples to delete from a database in order to eliminate all query answers? This problem is called "the resilience of a query" and is one of the key algorithmic problems underlying various forms of reverse data management, such as view maintenance, deletion propagation and causal responsibility. A long-open question is determining the conjunctive queries (CQs) for which resilience can be solved in PTIME. We shed new light on this problem by proposing a unified Integer Linear Programming (ILP) formulation. It is unified in that it can solve both previously studied restrictions (e.g., self-join-free CQs under set semantics that allow a PTIME solution) and new cases (all CQs under set or bag semantics). It is also unified in that all queries and all database instances are treated with the same approach, yet the algorithm is guaranteed to terminate in PTIME for all known PTIME cases. In particular, we prove that for all known easy cases, the optimal solution to our ILP is identical to a simpler Linear Programming (LP) relaxation, which implies that standard ILP solvers return the optimal solution to the original ILP in PTIME. Our approach allows us to explore new variants and obtain new complexity results. 1) It works under bag semantics, for which we give the first dichotomy results in the problem space. 2) We extend our approach to the related problem of causal responsibility and give a more fine-grained analysis of its complexity. 3) We recover easy instances for generally hard queries, including instances with read-once provenance and instances that become easy because of Functional Dependencies in the data. 4) We solve an open conjecture about a unified hardness criterion from PODS 2020 and prove the hardness of several queries of previously unknown complexity. 5) Experiments confirm that our findings accurately predict the asymptotic running times, and that our universal ILP is at times even quicker than a previously proposed dedicated flow algorithm.}
}


@article{DBLP:journals/pacmmod/SongGLSZJY23,
	author = {Zhen Song and
                  Yu Gu and
                  Tianyi Li and
                  Qing Sun and
                  Yanfeng Zhang and
                  Christian S. Jensen and
                  Ge Yu},
	title = {{ADGNN:} Towards Scalable {GNN} Training with Aggregation-Difference
                  Aware Sampling},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {229:1--229:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626716},
	doi = {10.1145/3626716},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SongGLSZJY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed computing is promising to enable large-scale graph neural network (GNN) model training. However, care is needed to avoid excessive computational and communication overheads. Sampling is promising in terms of enabling scalability, and sampling techniques have been proposed to reduce training costs. However, online sampling introduces large overheads, and while offline sampling that is done only once can eliminate such overheads, it instead introduces information loss and accuracy degradation. Thus, existing sampling techniques are unable to improve simultaneously both efficiency and accuracy, particularly at low sampling rates. We develop a distributed system, ADGNN, for full-batch based GNN training that adopts a hybrid sampling architecture to enable a trade-off between efficiency and accuracy. Specifically, ADGNN employs sampling result reuse techniques to reduce the cost associated with sampling and thus improve training efficiency. To alleviate accuracy degradation, we introduce a new metric,Aggregation Difference (AD), that quantifies the gap between sampled and full neighbor set aggregation. We present so-called AD-Sampling that aims to minimize the Aggregation Difference with an adaptive sampling frequency tuner. Finally, ADGNN employs anAD -importance-based sampling technique for remote neighbors to further reduce communication costs. Experiments on five real datasets show that ADGNN is able to outperform the state-of-the-art by up to nearly 9 times in terms of efficiency, while achieving comparable accuracy to the non-sampling methods.}
}


@article{DBLP:journals/pacmmod/AfroozehKB23,
	author = {Azim Afroozeh and
                  Leonardo X. Kuffo and
                  Peter A. Boncz},
	title = {{ALP:} Adaptive Lossless floating-Point Compression},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {230:1--230:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626717},
	doi = {10.1145/3626717},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AfroozehKB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IEEE 754 doubles do not exactly represent most real values, introducing rounding errors in computations and [de]serialization to text. These rounding errors inhibit the use of existing lightweight compression schemes such as Delta and Frame Of Reference (FOR), but recently new schemes were proposed: Gorilla, Chimp128, PseudoDecimals (PDE), Elf and Patas. However, their compression ratios are not better than those of general-purpose compressors such as Zstd; while [de]compression is much slower than Delta and FOR. We propose and evaluate ALP, that significantly improves these previous schemes in both speed and compression ratio (Figure 1). We created ALP after carefully studying the datasets used to evaluate the previous schemes. To obtain speed, ALP is designed to fit vectorized execution. This turned out to be key for also improving the compression ratio, as we found in-vector commonalities to create compression opportunities. ALP is an adaptive scheme that uses a strongly enhanced version of PseudoDecimals [31] to losslessly encode doubles as integers if they originated as decimals, and otherwise uses vectorized compression of the doubles' front bits. Its high speeds stem from our implementation in scalar code that auto-vectorizes, using building blocks provided by our FastLanes library [6], and an efficient two-stage compression algorithm that first samples row-groups and then vectors.}
}


@article{DBLP:journals/pacmmod/StavrakakisGBSIB23,
	author = {Dimitrios Stavrakakis and
                  Dimitra Giantsidi and
                  Maurice Bailleu and
                  Philip S{\"{a}}ndig and
                  Shady Issa and
                  Pramod Bhatotia},
	title = {Anchor: {A} Library for Building Secure Persistent Memory Systems},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {231:1--231:31},
	year = {2023},
	url = {https://doi.org/10.1145/3626718},
	doi = {10.1145/3626718},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/StavrakakisGBSIB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud infrastructure is experiencing a shift towards disaggregated setups, especially with the introduction of the Compute Express Link (CXL) technology, where byte-addressable ersistent memory (PM) is becoming prominent. To fully utilize the potential of such devices, it is a necessity to access them through network stacks with equivalently high levels of performance (e.g., kernel-bypass, RDMA). While, these advancements are enabling the development of high-performance data management systems, their deployment on untrusted cloud environments also increases the security threats. To this end, we present Anchor, a library for building secure PM systems. Anchor provides strong hardware-assisted security properties, while ensuring crash consistency. Anchor exposes APIs for secure data management within the realms of the established PM programming model, targeting byte-addressable storage devices. Anchor leverages trusted execution environments (TEE) and extends their security properties on PM. While TEE's protected memory region provides a strong foundation for building secure systems, the key challenge is that: TEEs are fundamentally incompatible with PM and kernel-bypass networking approaches-in particular, TEEs are neither designed to protect untrusted non-volatile PM, nor the protected region can be accessed via an untrusted DMA connection. To overcome this challenge, we design a PM engine that ensures strong security properties for the PM data, using confidential and authenticated PM data structures, while preserving crash consistency through a secure logging protocol. We further extend the PM engine to provide remote PM data operations via a secure network stack and a formally verified remote attestation protocol to form an end-to-end system. Our evaluation shows that Anchor incurs reasonable overheads, while providing strong security properties.}
}


@article{DBLP:journals/pacmmod/ChenWCW23,
	author = {Xiaolei Chen and
                  Peng Wang and
                  Jia Chen and
                  Wei Wang},
	title = {AS-Parser: Log Parsing Based on Adaptive Segmentation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {232:1--232:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626719},
	doi = {10.1145/3626719},
	timestamp = {Tue, 11 Feb 2025 14:01:13 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenWCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {System logs have long been recognized as valuable data for analyzing and diagnosing system failures. One fundamental task of log processing is to convert unstructured logs into structured logs through log parsing. All previous log parsing approaches follow a general framework that first segments each log into a token sequence and then computes similarity between two sequences. However, all existing approaches share the common drawback: the flat segmentation with fixed delimiters fails to understand the structural information of logs, which causes low parsing accuracy. To address this problem, we propose a novel log parsing approach, AS-Parser. Our approach introduces a hierarchical log segmentation mechanism that can adaptively segment logs into a tree structure. It can automatically recognize the appropriate delimiters and capture the common structural information. Moreover, we propose three improvements that enhance both the effectiveness and efficiency of our approach. On the public benchmark, AS-Parser performs best on 14 out of 16 datasets, with an average parsing accuracy of 0.943, far exceeding existing approaches.}
}


@article{DBLP:journals/pacmmod/PerronFDCM23,
	author = {Matthew Perron and
                  Raul Castro Fernandez and
                  David J. DeWitt and
                  Michael J. Cafarella and
                  Samuel Madden},
	title = {Cackle: Analytical Workload Cost and Performance Stability With Elastic
                  Pools},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {233:1--233:25},
	year = {2023},
	url = {https://doi.org/10.1145/3626720},
	doi = {10.1145/3626720},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/PerronFDCM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analytical query workloads are prone to rapid fluctuations in resource demands. These rapid, hard to predict resource demand changes make provisioning a challenge. Users must either over provision at excessive cost or suffer poor query latency when demand spikes. Prior work shows the viability of using cloud functions to match the supply of compute to the workload demand without provisioning resources ahead of time. For low query volumes, this approach is less costly at reasonable performance compared to provisioned systems, but as query volumes increase the cost overhead of cloud functions outweighs the benefit gained by rapid elasticity. In this work, we propose a novel strategy combining rapidly scalable but expensive resources with slow to start but inexpensive virtual machines to gain the benefit of elasticity without losing out on the cost savings of provisioned resources. We demonstrate a technique that minimizes cost over a wide range of workloads, environmental conditions, and compute costs while providing stable query performance. We implement these ideas in Cackle and demonstrate that it achieves similar performance and cost per query across a wide range of workloads, avoiding the cost and performance cliffs of alternative approaches.}
}


@article{DBLP:journals/pacmmod/LiWCJWZYA23,
	author = {Haoyu Li and
                  Liuhui Wang and
                  Qizhi Chen and
                  Jianan Ji and
                  Yuhan Wu and
                  Yikai Zhao and
                  Tong Yang and
                  Aditya Akella},
	title = {ChainedFilter: Combining Membership Filters by Chain Rule},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {234:1--234:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626721},
	doi = {10.1145/3626721},
	timestamp = {Thu, 27 Mar 2025 18:54:49 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiWCJWZYA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Membership (membership query/membership testing) is a fundamental problem across databases, networks and security. However, previous research has primarily focused on either approximate solutions, such as Bloom Filters, or exact methods, like perfect hashing and dictionaries, without attempting to develop an integral theory. In this paper, we propose a unified and complete theory, namely chain rule, for general membership problems, which encompasses both approximate and exact membership as extreme cases. Building upon the chain rule, we introduce a straightforward yet versatile algorithm framework, namely ChainedFilter, to combine different elementary filters without losing information. Our evaluation results demonstrate that ChainedFilter improves performance of many applications including static dictionary, lossless data compression, Cuckoo Hashing, LSM-Tree and Learned Filters.}
}


@article{DBLP:journals/pacmmod/NikooBH23,
	author = {AmirReza Alizade Nikoo and
                  Michael H. B{\"{o}}hlen and
                  Sven Helmer},
	title = {Correlation Joins over Time Series Data Streams Utilizing Complementary
                  Dimension Reduction and Transformation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {235:1--235:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626722},
	doi = {10.1145/3626722},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/NikooBH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A common analysis task over a stream of time series is to find all pairs of windows whose correlation is above a given threshold. For a large number of streams, doing so naively, i.e., checking the Cartesian product, is too expensive. In essence, finding correlated pairs in a non-naive way boils down to a high-dimensional similarity join in a Euclidean space. While there are similarity join algorithms, such as Quickjoin and ε-kdB tree, they are inefficient for high-dimensional data. We propose CorrJoin, short for Correlation Join, that combines a complementary dimension reduction and transformation step with a subsequent double-filtering step. In the first step, we reduce the dimensionality of data stream windows by combining a fast but inaccurate method, Piecewise Aggregate Approximation (PAA), with an accurate and slow one, Singular Value Decomposition (SVD). Not only does SVD compensate for the weaknesses of PAA, it also transforms the data to make the first filter based on bucketing more effective. The second filter, which uses Euclidean distances, reduces the number of false positives before computing exact correlations. Our experiments reveal that in common settings, CorrJoin is an order of magnitude faster than state-of-the-art approaches (up to 20 times faster than Quickjoin).}
}


@article{DBLP:journals/pacmmod/LiDBYWZBYXW23,
	author = {Yanan Li and
                  Guangqing Deng and
                  Changming Bai and
                  Jingyu Yang and
                  Gang Wang and
                  Hao Zhang and
                  Jin Bai and
                  Haitao Yuan and
                  Mengwei Xu and
                  Shangguang Wang},
	title = {Demystifying the QoS and QoE of Edge-hosted Video Streaming Applications
                  in the Wild with SNESet},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {236:1--236:29},
	year = {2023},
	url = {https://doi.org/10.1145/3626723},
	doi = {10.1145/3626723},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiDBYWZBYXW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video streaming applications (VSAs) are increasingly being deployed on large-scale edge platforms, which have the potential to significantly improve the quality of service (QoS) and end-user experience (QoE), ultimately maximizing business outcomes. However, there is currently very little understanding of how QoS, QoE, and the impact of QoS on QoE for VSAs on edge platforms in the wild and at scale. To close the knowledge gap, we collect SNESet, an active measurement dataset comprising QoS and QoE telemetry metrics of 8 VSAs over four months, covering end-users from 798 edge sites,30 cities, and 3 ISPs in one country.We characterize and compare the QoS and QoE metrics in SNESet with existing publicly available datasets, highlighting that SNESet includes a significantly greater number of metrics (horizontal diversity and vertical hierarchy) and provides more comprehensive coverage of specific metrics.Moreover, we qualitatively and quantitatively analyze the impact of QoS on QoE in both domain-general and domain-specific scenarios. Our findings can inform the system design decisions that different entities in the video ecosystem (content providers, video player designers, third-party optimizers, edge vendors) make to maximize end-users experience and ultimately maximize the business outcomes. We hope SNESet can attract more research efforts in the data management community, computer network community, and beyond.}
}


@article{DBLP:journals/pacmmod/ChenLW23,
	author = {Fahao Chen and
                  Peng Li and
                  Celimuge Wu},
	title = {{DGC:} Training Dynamic Graphs with Spatio-Temporal Non-Uniformity
                  using Graph Partitioning by Chunks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {237:1--237:25},
	year = {2023},
	url = {https://doi.org/10.1145/3626724},
	doi = {10.1145/3626724},
	timestamp = {Thu, 18 Apr 2024 16:10:16 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic Graph Neural Network (DGNN) has shown a strong capability of learning dynamic graphs by exploiting both spatial and temporal features. Although DGNN has recently received considerable attention by AI community and various DGNN models have been proposed, building a distributed system for efficient DGNN training is still challenging. It has been well recognized that how to partition the dynamic graph and assign workloads to multiple GPUs plays a critical role in training acceleration. Existing works partition a dynamic graph into snapshots or temporal sequences, which only work well when the graph has uniform spatio-temporal structures. However, dynamic graphs in practice are not uniformly structured, with some snapshots being very dense while others are sparse. To address this issue, we propose DGC, a distributed DGNN training system that achieves a 1.25× - 7.52× speedup over the state-of-the-art in our testbed. DGC's success stems from a new graph partitioning method that partitions dynamic graphs into chunks, which are essentially subgraphs with modest training workloads and few inter connections. This partitioning algorithm is based on graph coarsening, which can run very fast on large graphs. In addition, DGC has a highly efficient run-time, powered by the proposed chunk fusion and adaptive stale aggregation techniques. Extensive experimental results on 3 typical DGNN models and 4 popular dynamic graph datasets are presented to show the effectiveness of DGC.}
}


@article{DBLP:journals/pacmmod/FuLLLC23,
	author = {Congcong Fu and
                  Hui Li and
                  Jian Lou and
                  Huizhen Li and
                  Jiangtao Cui},
	title = {DP-starJ: {A} Differential Private Scheme towards Analytical Star-Join
                  Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {238:1--238:24},
	year = {2023},
	url = {https://doi.org/10.1145/3626725},
	doi = {10.1145/3626725},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FuLLLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Star-join query is the fundamental task in data warehouse and has wide applications in On-line Analytical Processing (olap) scenarios. Due to the large number of foreign key constraints and the asymmetric effect in the neighboring instance between the fact and dimension tables, even those latest dp efforts specifically designed for join, if directly applied to star-join query, will suffer from extremely large estimation errors and expensive computational cost. In this paper, we are thus motivated to propose DP-starJ, a novel Differentially Private framework for star-Join queries. DP-starJ consists of a series of strategies tailored to specific features of star-join, including 1) we unveil the different effects of fact and dimension tables on the neighboring database instances, and accordingly revisit the definitions tailored to different cases of star-join; 2) we propose Predicate Mechanism (PM), which utilizes predicate perturbation to inject noise into the join procedure instead of the results; 3) to further boost the robust performance, we propose a dp-compliant star-join algorithm for various types of star-join tasks based on PM. We provide both theoretical analysis and empirical study, which demonstrate the superiority of the proposed methods over the state-of-the-art solutions in terms of accuracy, efficiency, and scalability.}
}


@article{DBLP:journals/pacmmod/ChenZHW23,
	author = {Xingguang Chen and
                  Fangyuan Zhang and
                  Jinchao Huang and
                  Sibo Wang},
	title = {Efficient Approximation Framework for Attribute Recommendation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {239:1--239:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626726},
	doi = {10.1145/3626726},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenZHW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trend analysis is a fundamental type of analytical query in online analytical processing (OLAP) systems. In trend analysis, a key step is to identify k valuable attributes whose distributions in two subsets under different predicates significantly differ for further investigation, where the difference is measured by metric functions. However, the exact solution that involves scanning all records is prohibitively expensive, particularly when handling large datasets in the era of big data. To minimize unnecessary data access, the existing state-of-the-art solution TopKAttr adopts sampling to avoid the expensive data scan. However, their solution still has two main drawbacks. Firstly, their solution is tailored only for two limited metric functions: the Earth Mover distance and Euclidean distance, and cannot be generalized to more complicated metric functions. Besides, their solution still aims to return the exact top-k answers via the sampling method, which still causes high running costs as shown in our experiment. Motivated by these limitations, we propose a general approximation framework for attribute recommendation that efficiently returns the top-k attributes with theoretical guarantees while supporting an extensive range of metric functions, such as the Kolmogorov-Smirnov test (KS-test), Chebyshev distance, the Earth Mover distance, Euclidean distance, and with the potential to more metrics. The key to our framework is a new bound estimation strategy that can be applied to a wide spectrum of metrics, as we listed above. Based on our estimation framework, we further devise an efficient approximation algorithm with theoretical guarantees to answer the top-k queries, which is widely used in attribute recommendation. Extensive experiments on four real large datasets show that our framework gains up to an order of magnitude speed-up and consistently high accuracy compared to TopKAttr, providing a promising alternative for attribute recommendation in OLAP systems.}
}


@article{DBLP:journals/pacmmod/IslamAR23,
	author = {Md Mouinul Islam and
                  Mahsa Asadi and
                  Senjuti Basu Roy},
	title = {Equitable Top-k Results for Long Tail Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {240:1--240:24},
	year = {2023},
	url = {https://doi.org/10.1145/3626727},
	doi = {10.1145/3626727},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/IslamAR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For datasets exhibiting long tail phenomenon, we identify a fairness concern in existing top-k algorithms, that return a "fixed" set of k results for a given query. This causes a handful of popular records (products, items, etc) getting overexposed and always be returned to the user query, whereas, there exists a long tail of niche records that may be equally desirable (have similar utility). To alleviate this, we propose θ-Equiv-top-k-MMSP inside existing top-k algorithms - instead of returning a fixed top-k set, it generates all (or many) top-k sets that are equivalent in utility and creates a probability distribution over those sets. The end user will be returned one of these sets during the query time proportional to its associated probability, such that, after many draws from many end users, each record will have as equal exposure as possible (governed by uniform selection probability). θ-Equiv-top-k-MMSP is formalized with two sub-problems. (a) θ-Equiv-top-k-Sets to produce a set S of sets, each set has k records, where the sets are equivalent in utility with the top-k set; (b) MaxMinFair to produce a probability distribution over S, that is, PDF(S), such that the records in S have uniform selection probability. We formally study the hardness of θ-Equiv-top-k-MMSP. We present multiple algorithmic results - (a) An exact solution for θ-Equiv-top-k-Sets, and MaxMinFair. (b) We design highly scalable algorithms that solve θ-Equiv-top-k-Sets through a random walk and is backed by probability theory, as well as a greedy solution designed for MaxMinFair. (c) We finally present an adaptive random walk based algorithm that solves θ-Equiv-top-k-Sets and MaxMinFair at the same time. We empirically study how θ-Equiv-top-k-MMSP can alleviate a equitable exposure concerns that group fairness suffers from. We run extensive experiments using 6 datasets and design intuitive baseline algorithms that corroborate our theoretical analysis.}
}


@article{DBLP:journals/pacmmod/ZhuXZWSYYP23,
	author = {Shengkun Zhu and
                  Quanqing Xu and
                  Jinshan Zeng and
                  Sheng Wang and
                  Yuan Sun and
                  Zhifeng Yang and
                  Chuanhui Yang and
                  Zhiyong Peng},
	title = {{F3KM:} Federated, Fair, and Fast k-means},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {241:1--241:25},
	year = {2023},
	url = {https://doi.org/10.1145/3626728},
	doi = {10.1145/3626728},
	timestamp = {Tue, 11 Feb 2025 08:46:07 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhuXZWSYYP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a federated, fair, and fast k-means algorithm (F3KM) to solve the fair clustering problem efficiently in scenarios where data cannot be shared among different parties. The proposed algorithm decomposes the fair k-means problem into multiple subproblems and assigns each subproblem to a client for local computation. Our algorithm allows each client to possess multiple sensitive attributes (or have no sensitive attributes). We propose an in-processing method that employs the alternating direction method of multipliers (ADMM) to solve each subproblem. During the procedure of solving subproblems, only the computation results are exchanged between the server and the clients, without exchanging the raw data. Our theoretical analysis shows that F3KM is efficient in terms of both communication and computation complexities. Specifically, it achieves a better trade-off between utility and communication complexity, and reduces the computation complexity to linear with respect to the dataset size. Our experiments show that F3KM achieves a better trade-off between utility and fairness than other methods. Moreover, F3KM is able to cluster five million points in one hour, highlighting its impressive efficiency.}
}


@article{DBLP:journals/pacmmod/VanNostrandZHR23,
	author = {Peter M. VanNostrand and
                  Huayi Zhang and
                  Dennis M. Hofmann and
                  Elke A. Rundensteiner},
	title = {{FACET:} Robust Counterfactual Explanation Analytics},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {242:1--242:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626729},
	doi = {10.1145/3626729},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/VanNostrandZHR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning systems are deployed in domains such as hiring and healthcare, where undesired classifications can have serious ramifications for the user. Thus, there is a rising demand for explainable AI systems which provide actionable steps for lay users to obtain their desired outcome. To meet this need, we propose FACET, the first explanation analytics system which supports a user in interactively refining counterfactual explanations for decisions made by tree ensembles. As FACET's foundation, we design a novel type of counterfactual explanation called the counterfactual region. Unlike traditional counterfactuals, FACET's regions concisely describe portions of the feature space where the desired outcome is guaranteed, regardless of variations in exact feature values. This property, which we coin explanation robustness, is critical for the practical application of counterfactuals. We develop a rich set of novel explanation analytics queries which empower users to identify personalized counterfactual regions that account for their real-world circumstances. To process these queries, we develop a compact high-dimensional counterfactual region index along with index-aware query processing strategies for near real-time explanation analytics. We evaluate FACET against state-of-the-art explanation techniques on eight public benchmark datasets and demonstrate that FACET generates actionable explanations of similar quality in an order of magnitude less time while providing critical robustness guarantees. Finally, we conduct a preliminary user study which suggests that FACET's regions lead to higher user understanding than traditional counterfactuals.}
}


@article{DBLP:journals/pacmmod/BussottiVSP23,
	author = {Jean{-}Flavien Bussotti and
                  Enzo Veltri and
                  Donatello Santoro and
                  Paolo Papotti},
	title = {Generation of Training Examples for Tabular Natural Language Inference},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {243:1--243:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626730},
	doi = {10.1145/3626730},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BussottiVSP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tabular data is becoming increasingly important in Natural Language Processing (NLP) tasks, such as Tabular Natural Language Inference (TNLI). Given a table and a hypothesis expressed in NL text, the goal is to assess if the former structured data supports or refutes the latter. In this work, we focus on the role played by the annotated data in training the inference model. We introduce a system, Tenet, for the automatic augmentation and generation of training examples for TNLI. Given the tables, existing approaches are either based on human annotators, and thus expensive, or on methods that produce simple examples that lack data variety and complex reasoning. Instead, our approach is built around the intuition that SQL queries are the right tool to achieve variety in the generated examples, both in terms of data variety and reasoning complexity. The first is achieved by evidence-queries that identify cell values over tables according to different data patterns. Once the data for the example is identified, semantic-queries describe the different ways such data can be identified with standard SQL clauses. These rich descriptions are then verbalized as text to create the annotated examples for the TNLI task. The same approach is also extended to create counterfactual examples, i.e., examples where the hypothesis is false, with a method based on injecting errors in the original (clean) table. For all steps, we introduce generic generation algorithms that take as input only the tables. For our experimental study, we use three datasets from the TNLI literature and two crafted by us on more complex tables. Tenet generates human-like examples, which lead to the effective training of several inference models with results comparable to those obtained by training the same models with manually-written examples.}
}


@article{DBLP:journals/pacmmod/FarhanKOW23,
	author = {Muhammad Farhan and
                  Henning Koehler and
                  Robert Ohms and
                  Qing Wang},
	title = {Hierarchical Cut Labelling - Scaling Up Distance Queries on Road Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {244:1--244:25},
	year = {2023},
	url = {https://doi.org/10.1145/3626731},
	doi = {10.1145/3626731},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FarhanKOW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Answering the shortest-path distance between two arbitrary locations is a fundamental problem in road networks. Labelling-based solutions are the current state-of-the-arts to render fast response time, which can generally be categorised into hub-based labellings, highway-based labellings, and tree decomposition labellings. Hub-based and highway-based labellings exploit hierarchical structures of road networks with the aim to reduce labelling size for improving query efficiency. However, these solutions still result in large search spaces on distance labels at query time, particularly when road networks are large. Tree decomposition labellings leverage a hierarchy of vertices to reduce search spaces over distance labels at query time, but such a hierarchy is generated using tree decomposition techniques, which may yield very large labelling sizes and slow querying. In this paper, we propose a novel solution hierarchical cut 2-hop labelling (HC2L) to address the drawbacks of the existing works. Our solution combines the benefits of hierarchical structures from both perspectives - reduce the size of a distance labelling at preprocessing time and further reduce the search space on a distance labelling at query time. At its core, we propose a new hierarchy, balanced tree hierarchy, which enables a fast, efficient data structure to reduce the size of distance labelling and to select a very small subset of labels to compute the shortest-path distance at query time. To speed up the construction process of HC2L, we further propose a parallel variant of our method, namely HC2L^p. We have evaluated our solution on 10 large real-world road networks through extensive experiments. The results show that our method is 1.5-4 times faster in terms of query processing while being comparable in terms of labelling construction time and achieving up to 60% smaller labelling size compared to the state-of-the-art approaches.}
}


@article{DBLP:journals/pacmmod/ZhangSYMXJLSZL23,
	author = {Jiujing Zhang and
                  Zhitao Shen and
                  Shiyu Yang and
                  Lingkai Meng and
                  Chuan Xiao and
                  Wei Jia and
                  Yue Li and
                  Qinhui Sun and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {High-Ratio Compression for Machine-Generated Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {245:1--245:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626732},
	doi = {10.1145/3626732},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangSYMXJLSZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine-generated data is rapidly growing and poses challenges for data-intensive systems, especially as the growth of data outpaces the growth of storage space. To cope with the storage issue, compression plays a critical role in storage engines, particularly for data-intensive applications, where a high compression ratio and efficient random access are essential. However, existing compression techniques tend to focus on general-purpose and data block approaches, but overlook the inherent structure of machine-generated data and hence result in low compression ratios or limited lookup efficiency. To address these limitations, we introduce the Pattern-Based Compression (PBC) algorithm, which specifically targets patterns in machine-generated data to achieve Pareto-optimality in most cases. Unlike traditional data block-based methods, PBC compresses data on a per-record basis, facilitating rapid random access. Our experimental evaluation demonstrates that PBC, on average, achieves a compression ratio twice as high as the state-of-the-art techniques while maintaining competitive compression and decompression speeds. We also integrate PBC to a production database system and achieve improvements on both comparison ratio and throughput.}
}


@article{DBLP:journals/pacmmod/WangCWH23,
	author = {Qiange Wang and
                  Yao Chen and
                  Weng{-}Fai Wong and
                  Bingsheng He},
	title = {HongTu: Scalable Full-Graph {GNN} Training on Multiple GPUs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {246:1--246:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626733},
	doi = {10.1145/3626733},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangCWH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Full-graph training on graph neural networks (GNN) has emerged as a promising training method for its effectiveness. Full-graph training requires extensive memory and computation resources. To accelerate this training process, researchers have proposed employing multi-GPU processing. However the scalability of existing frameworks is limited as they necessitate maintaining the training data for every layer in GPU memory. To efficiently train on large graphs, we present HongTu, a scalable full-graph GNN training system running on GPU-accelerated platforms. HongTu stores vertex data in CPU memory and offloads training to GPUs. HongTu employs a memory-efficient full-graph training framework that reduces runtime memory consumption by using partition-based training and recomputation-caching-hybrid intermediate data management. To address the issue of increased host-GPU communication caused by duplicated neighbor access among partitions, HongTu employs a deduplicated communication framework that converts the redundant host-GPU communication to efficient inter/intra-GPU data access. Further, HongTu uses a cost model-guided graph reorganization method to minimize communication overhead. Experimental results on a 4XA100 GPU server show that HongTu effectively supports billion-scale full-graph GNN training while reducing host-GPU data communication by 25%-71%. Compared to the full-graph GNN system DistGNN running on 16 CPU nodes, HongTu achieves speedups ranging from 7.8X to 20.2X. For small graphs where the training data fits into the GPUs, HongTu achieves performance comparable to existing GPU-based GNN systems.}
}


@article{DBLP:journals/pacmmod/MoCWCB23,
	author = {Songsong Mo and
                  Yile Chen and
                  Hao Wang and
                  Gao Cong and
                  Zhifeng Bao},
	title = {Lemo: {A} Cache-Enhanced Learned Optimizer for Concurrent Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {247:1--247:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626734},
	doi = {10.1145/3626734},
	timestamp = {Tue, 06 Aug 2024 09:17:50 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MoCWCB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the expansion of modern database services, multi-user access has become a crucial feature in various practical application scenarios, including enterprise applications and e-commerce platforms. However, if multiple users submit queries within a short time frame, it can result in potential issues such as redundant computation and query concurrency. Unfortunately, most existing multi-query optimization methods, which aim to enhance query processing efficiency, have not adequately addressed these two problems, especially in the setting where multiple queries are being executed concurrently. To this end, we propose a novel method named Lemo for the multi-query optimization problem. Specifically, we propose a novel value network to predict latencies of concurrent queries as the foundation model for query plan generation. Furthermore, we introduce a shared buffer manager component to cache the intermediate results of sub-queries. The shared buffer manager applies a novel replacement policy to maintain the cached buffer with the objective of maximizing the opportunity for the reuse of the cached sub-queries. Based on the shared buffer, our proposed value network can incorporate the cached results into cost estimation to further guide Lemo in generating query plans, thus avoiding redundant computation. Lemo has been integrated into PostgreSQL and experiments conducted on real datasets with PostgreSQL show that it outperforms all the baselines in efficiency.}
}


@article{DBLP:journals/pacmmod/HuangW23,
	author = {Zezhou Huang and
                  Eugene Wu},
	title = {Lightweight Materialization for Fast Dashboards Over Joins},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {248:1--248:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626735},
	doi = {10.1145/3626735},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dashboards are vital in modern business intelligence tools, providing non-technical users with an interface to access comprehensive business data. With the rise of cloud technology, there is an increased number of data sources to provide enriched contexts for various analytical tasks, leading to a demand for interactive dashboards over a large number of joins. Nevertheless, joins are among the most expensive operations in DBMSes, making the support of interactive dashboards over joins challenging. In this paper, we present Treant, a dashboard accelerator for queries over large joins. Treant uses factorized query execution to handle aggregation queries over large joins, which alone is still insufficient for interactive speeds. To address this, we exploit the incremental nature of user interactions using Calibrated Junction Hypertree (CJT), a novel data structure that applies lightweight materialization of the intermediates during factorized execution. CJT ensures that the work needed to compute a query is proportional to how different it is from the previous query, rather than the overall complexity. Treant manages CJTs to share work between queries and performs materialization offline or during user "think-times." Implemented as a middleware that rewrites SQL, Treant is portable to any SQL-based DBMS. Our experiments on single node and cloud DBMSes show that Treant improves dashboard interactions by two orders of magnitude, and provides 10x improvement for ML augmentation compared to SOTA factorized ML system.}
}


@article{DBLP:journals/pacmmod/WangS23a,
	author = {Zhiqi Wang and
                  Zili Shao},
	title = {MirrorKV: An Efficient Key-Value Store on Hybrid Cloud Storage with
                  Balanced Performance of Compaction and Querying},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {249:1--249:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626736},
	doi = {10.1145/3626736},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangS23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LSM-based key-value stores have been leveraged in many state-of-the-art data-intensive applications as storage engines. As data volume scales up, a cost-efficient approach is to deploy these applications on hybrid cloud storage with hot/cold separation, which splits the LSM-tree into two parts and thus brings new challenges on how to split and how to close the significant performance gap between these two parts. Existing LSM-tree key-value stores mainly focus on the optimizations of local storage, which incurs sub-optimal performance when directly applied to hybrid storage. In this paper, we present MirrorKV for efficient compaction and querying on hybrid cloud storage. First, based on the capacities of fast and slow cloud storage, MirrorKV vertically separates hot/cold data of different levels stored in different cloud storage with different compaction mechanisms. To avoid compaction in slow storage being the bottleneck of the write path, MirrorKV proposes a novel virtual split to only compact the metadata during the compaction, which postpones the actual compaction until it reaches deep enough levels. Second, to reduce accessing slow storage during querying, MirrorKV horizontally separates keys and values into two mirrored LSM-trees to differentiate caching priorities; the maintained tree structures preserve the data locality for efficient sequential reading without incurring the overhead of the traditional key-value separation solutions. Finally, MirrorKV leverages cached data to guide the compaction where the hot data is retained in the fast storage while the cold data is compacted to deeper levels in slow storage. Compared with RocksDB-cloud, MirrorKV achieves 2.4× higher random insertion throughput, 29% higher random read throughput, and 99% less compaction time.}
}


@article{DBLP:journals/pacmmod/YangC23,
	author = {Zehai Yang and
                  Shimin Chen},
	title = {{MOST:} Model-Based Compression with Outlier Storage for Time Series
                  Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {250:1--250:29},
	year = {2023},
	url = {https://doi.org/10.1145/3626737},
	doi = {10.1145/3626737},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series data are used in a wide variety of applications. The explosive growth of the amount of time series data poses a significant challenge in efficient data storage and query processing. Unfortunately, existing compression techniques either show only low to medium compression ratio on time series data, or incur significant decompression overhead during query processing. We propose a novel compression technique, MOST (Model-based compression with Outlier STorage) for time series data. As measurement values often change smoothly in a period of time, we divide a time series into segments of smooth changes, then compute a linear model for each segment. Since tiny errors are often acceptable in analysis tasks, we omit data points whose computed values are within a pre-specified error threshold from the actual values, thereby effectively reducing the data size. Outliers are rare but important for many applications, and therefore we store outliers explicitly. Moreover, for processing MOST compressed data, we propose a segment-outlier dual-mode query engine that computes segments as a whole as much as possible, and build a prototype MostDB. Experimental results on real-world data sets show that MOST achieves 9.45-15.04x compression ratios. Compared to existing time series databases, MostDB achieves up to 11.68x speedups for common queries from the IoTDB Benchmark.}
}


@article{DBLP:journals/pacmmod/ZhuHA23,
	author = {Zichen Zhu and
                  Xiao Hu and
                  Manos Athanassoulis},
	title = {{NOCAP:} Near-Optimal Correlation-Aware Partitioning Joins},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {252:1--252:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626739},
	doi = {10.1145/3626739},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhuHA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Storage-based joins are still commonly used today because the memory budget does not always scale with the data size. One of the many join algorithms developed that has been widely deployed and proven to be efficient is the Hybrid Hash Join (HHJ), which is designed to exploit any available memory to maximize the data that is joined directly in memory. However, HHJ cannot fully exploit detailed knowledge of the join attribute correlation distribution. In this paper, we show that given a correlation skew in the join attributes, HHJ partitions data in a suboptimal way. To do that, we derive the optimal partitioning using a new cost-based analysis of partitioning-based joins that is tailored for primary key - foreign key (PK-FK) joins, one of the most common join types. This optimal partitioning strategy has a high memory cost, thus, we further derive an approximate algorithm that has tunable memory cost and leads to near-optimal results. Our algorithm, termed NOCAP (Near-Optimal Correlation-Aware Partitioning) join, outperforms the state of the art for skewed correlations by up to 30%, and the textbook Grace Hash Join by up to 4×. Further, for a limited memory budget, NOCAP outperforms HHJ by up to 10%, even for uniform correlation. Overall, NOCAP dominates state-of-the-art algorithms and mimics the best algorithm for a memory budget varying from below √||relation|| to more than ||relation||.}
}


@article{DBLP:journals/pacmmod/YangC23a,
	author = {Jingyi Yang and
                  Gao Cong},
	title = {{PLATON:} Top-down R-tree Packing with Learned Partition Policy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {253:1--253:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626742},
	doi = {10.1145/3626742},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangC23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The exponential growth of spatial data poses new challenges to the performance of spatial databases. Spatial indexes like R-tree greatly accelerate the query performance and can be effectively constructed through packing, i.e., loading all data into the index at once. However, existing R-tree packing methods rely on a set of fixed heuristic rules, which may not be suitable for different data distributions and workload patterns. To address the limitations of existing R-tree packing methods, we propose PLATON, a top-down R-tree packing method with learned partition policy that explicitly optimizes the query performance with regard to the given data and workload instance. We develop a learned partition policy based on Monte Carlo Tree Search and carefully make design choices for the MCTS exploration strategy and simulation strategy to improve algorithm convergence. We propose a divide and conquer strategy and two optimization techniques, early termination and level-wise sampling, to drastically reduce the MCTS algorithm's time complexity and make it a linear-time algorithm. Experiments on both synthetic and real-world datasets demonstrate the superior performance of PLATON over existing R-tree variants and recently proposed learned/workload-aware spatial indexes.}
}


@article{DBLP:journals/pacmmod/RumbaughX23,
	author = {Douglas B. Rumbaugh and
                  Dong Xie},
	title = {Practical Dynamic Extension for Sampling Indexes},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {254:1--254:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626744},
	doi = {10.1145/3626744},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/RumbaughX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The execution of analytical queries on massive datasets presents challenges due to long response times and high computational costs. As a result, the analysis of representative samples of data has emerged as an attractive alternative; this avoids the cost of processing queries against the entire dataset, while still producing statistically valid results. Unfortunately, the sampling techniques in common use sacrifice either sample quality or performance, and so are poorly suited for this task. However, it is possible to build high quality sample sets efficiently with the assistance of indexes. This introduces a new challenge: real-world data is subject to continuous update, and so the indexes must be kept up to date. This is difficult, because existing sampling indexes present a dichotomy; efficient sampling indexes are difficult to update, while easily updatable indexes have poor sampling performance. This paper seeks to address this gap by proposing a general and practical framework for extending most sampling indexes with efficient update support, based on splitting indexes into smaller shards, combined with a systematic approach to the periodic reconstruction. The framework's design space is examined, with an eye towards exploring trade-offs between update performance, sampling performance, and memory usage. Three existing static sampling indexes are extended using this framework to support updates, and the generalization of the framework to concurrent operations and larger-than-memory data is discussed. Through a comprehensive suite of benchmarks, the extended indexes are shown to match or exceed the update throughput of state-of-the-art dynamic baselines, while presenting significant improvements in sampling latency.}
}


@article{DBLP:journals/pacmmod/YangWZDLC23,
	author = {Jiani Yang and
                  Sai Wu and
                  Dongxiang Zhang and
                  Jian Dai and
                  Feifei Li and
                  Gang Chen},
	title = {Rethinking Learned Cost Models: Why Start from Scratch?},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {255:1--255:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626769},
	doi = {10.1145/3626769},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangWZDLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent work has applied learning-based approaches to replace the conventional cost model, but these approaches are expensive to train and result in high inference overheads. Furthermore, due to a lack of explainability, models trained for one database may not be easily transferred to another, requiring a complete re-training process. In this paper, we propose a new approach to tuning the conventional formula-based cost model for DBMS. Our approach involves identifying important parameters within the cost model rules and using a fast-learning model to adjust them for each specific hardware and software configuration of the DBMS deployment. We dynamically partition the search space of hardware and software configurations to gradually refine the cost model estimation. To apply our cost model to a new DBMS instance, we start with a rough estimation and progressively refine it with finer granularity. Our experiments with different hardware and software configurations show that our approach enables the conventional cost model to be quickly transferred to any database instance, achieving comparable results to a fine-tuned learning-based model. Overall, our approach provides a practical solution to tuning the conventional cost model for DBMS, with significant benefits in terms of reduced cost and improved performance.}
}


@article{DBLP:journals/pacmmod/SongZLPC23,
	author = {Haoze Song and
                  Wenchao Zhou and
                  Feifei Li and
                  Xiang Peng and
                  Heming Cui},
	title = {Rethink Query Optimization in {HTAP} Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {256:1--256:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626750},
	doi = {10.1145/3626750},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SongZLPC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of data-intensive applications has fueled the evolution of hybrid transactional and analytical processing (HTAP). To support mixed workloads, distributed HTAP databases typically maintain two data copies that are specially tailored for data freshness and performance isolation. In particular, a copy in a row-oriented format is well-suited for OLTP workloads, and a second copy in a column-oriented format is optimized for OLAP workloads. Such a hybrid design opens up a new design space for query optimization: plans can be optimized over different data formats and can be executed over isolated resources, which we term hybrid plans. In this paper, we demonstrate that hybrid plans can largely benefit query execution (e.g., up to 11x speedups in our evaluation). However, we also found these benefits will potentially be at the cost of sacrificing data freshness or performance isolation since traditional optimizers may not precisely model and schedule the execution of hybrid plans on real-time updated HTAP databases. Therefore, we propose Metis, an HTAP-aware optimizer. We show, both theoretically and experimentally, that using the proposed optimizations, a system can largely benefit from hybrid plans while preserving isolated performance for OLTP and OLAP, and these optimizations are robust to the changes in workloads.}
}


@article{DBLP:journals/pacmmod/PrammerP23,
	author = {Martin Prammer and
                  Jignesh M. Patel},
	title = {Rethinking the Encoding of Integers for Scans on Skewed Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {257:1--257:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626751},
	doi = {10.1145/3626751},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/PrammerP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bit-parallel scanning techniques are characterized by their ability to accelerate compute through the process known as early pruning. Early pruning techniques iterate over the bits of each value, searching for opportunities to safely prune compute early, before processing each data value in its entirety. However, because of this iterative evaluation, the effectiveness of early pruning depends on the relative position of bits that can be used for pruning within each value. Due to this behavior, bit-parallel techniques have faced significant challenges when processing skewed data, especially when values contain many leading zeroes. This problem is further amplified by the inherent trade-off that bit-parallel techniques make between columnar scan and fetch performance: a storage layer that supports early pruning requires multiple memory accesses to fetch a single value. Thus, in the case of skewed data, bit-parallel techniques increase fetch latency without significantly improving scan performance when compared to baseline columnar implementations. To remedy this shortcoming, we transform the values in bit-parallel columns using novel encodings. We propose the concept of forward encodings: a family of encodings that shift pruning-relevant bits closer to the most significant bit. Using this concept, we propose two particular encodings: the Data Forward Encoding and the Extended Data Forward Encoding. We demonstrate the impact of these encodings using multiple real-world datasets. Across these datasets, forward encodings improve the current state-of-the-art bit-parallel technique's scan and fetch performance in many cases by 1.4x and 1.3x, respectively.}
}


@article{DBLP:journals/pacmmod/GeZSLGCCP23,
	author = {Jiake Ge and
                  Huanchen Zhang and
                  Boyu Shi and
                  Yuanhui Luo and
                  Yunda Guo and
                  Yunpeng Chai and
                  Yuxing Chen and
                  Anqun Pan},
	title = {{SALI:} {A} Scalable Adaptive Learned Index Framework based on Probability
                  Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {258:1--258:25},
	year = {2023},
	url = {https://doi.org/10.1145/3626752},
	doi = {10.1145/3626752},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GeZSLGCCP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growth in data storage capacity and the increasing demands for high performance have created several challenges for concurrent indexing structures. One promising solution is the learned index, which uses a learning-based approach to fit the distribution of stored data and predictively locate target keys, significantly improving lookup performance. Despite their advantages, prevailing learned indexes exhibit constraints and encounter issues of scalability on multi-core data storage. This paper introduces SALI, the Scalable Adaptive Learned Index framework, which incorporates two strategies aimed at achieving high scalability, improving efficiency, and enhancing the robustness of the learned index. Firstly, a set of node-evolving strategies is defined to enable the learned index to adapt to various workload skews and enhance its concurrency performance in such scenarios. Secondly, a lightweight strategy is proposed to maintain statistical information within the learned index, with the goal of further improving the scalability of the index. Furthermore, to validate their effectiveness, SALI applied the two strategies mentioned above to the learned index structure that utilizes fine-grained write locks, known as LIPP. The experimental results have demonstrated that SALI significantly enhances the insertion throughput with 64 threads by an average of 2.04x compared to the second-best learned index. Furthermore, SALI accomplishes a lookup throughput similar to that of LIPP+.}
}


@article{DBLP:journals/pacmmod/ZhangCWYG23,
	author = {Fangyuan Zhang and
                  Dechuang Chen and
                  Sibo Wang and
                  Yin Yang and
                  Junhao Gan},
	title = {Scalable Approximate Butterfly and Bi-triangle Counting for Large
                  Bipartite Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {259:1--259:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626753},
	doi = {10.1145/3626753},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangCWYG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A bipartite graph is a graph that consists of two disjoint sets of vertices and only edges between vertices from different vertex sets. In this paper, we study the counting problems of two common types of em motifs in bipartite graphs: (i) butterflies (2x2 bicliques) and (ii) bi-triangles (length-6 cycles). Unlike most of the existing algorithms that aim to obtain exact counts, our goal is to obtain precise enough estimations of these counts in bipartite graphs, as such estimations are already sufficient and of great usefulness in various applications. While there exist approximate algorithms for butterfly counting, these algorithms are mainly based on the techniques designed for general graphs, and hence, they are less effective on bipartite graphs. Not to mention that there is still a lack of study on approximate bi-triangle counting. Motivated by this, we first propose a novel butterfly counting algorithm, called one-sided weighted sampling, which is tailored for bipartite graphs. The basic idea of this algorithm is to estimate the total butterfly count with the number of butterflies containing two randomly sampled vertices from the same side of the two vertex sets. We prove that our estimation is unbiased, and our technique can be further extended (non-trivially) for bi-triangle count estimation. Theoretical analyses under a power-law random bipartite graph model and extensive experiments on multiple large real datasets demonstrate that our proposed approximate counting algorithms can reach high accuracy, yet achieve up to three orders (resp. four orders) of magnitude speed-up over the state-of-the-art exact butterfly (resp. bi-triangle) counting algorithms. Additionally, we present an approximate clustering coefficient estimation framework for bipartite graphs, which shows a similar speed-up over the exact solutions with less than 1% relative error.}
}


@article{DBLP:journals/pacmmod/MollFMGC23,
	author = {Oscar R. Moll and
                  Manuel Favela and
                  Samuel Madden and
                  Vijay Gadepally and
                  Michael J. Cafarella},
	title = {SeeSaw: Interactive Ad-hoc Search Over Image Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {260:1--260:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626754},
	doi = {10.1145/3626754},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/MollFMGC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As image datasets become ubiquitous, the problem of ad-hoc searches over image data is increasingly important. Many high-level data tasks in machine learning, such as constructing datasets for training and testing object detectors, imply finding ad-hoc objects or scenes within large image datasets as a key sub-problem. New foundational visual-semantic embeddings trained on massive web datasets such as Contrastive Language-Image Pre-Training (CLIP) can help users start searches on their own data, but we find there is a long tail of queries where these models fall short in practice. Seesaw is a system for interactive ad-hoc searches on image datasets that integrates state-of-the-art embeddings like CLIP with user feedback in the form of box annotations to help users quickly locate images of interest in their data even in the long tail of harder queries. One key challenge for Seesaw is that, in practice, many sensible approaches to incorporating feedback into future results, including state-of-the-art active-learning algorithms, can worsen results compared to introducing no feedback, partly due to CLIP's high-average performance. Therefore, Seesaw includes several algorithms that empirically result in larger and also more consistent improvements. We compare Seesaw's accuracy to both using CLIP alone and to a state-of-the-art active-learning baseline and find Seesaw consistently helps improve results for users across four datasets and more than a thousand queries. Seesaw increases Average Precision (AP) on search tasks by an average of .08 on a wide benchmark (from a base of .72), and by a .27 on a subset of more difficult queries where CLIP alone performs poorly.}
}


@article{DBLP:journals/pacmmod/MengCC23,
	author = {Zizhong Meng and
                  Xin Cao and
                  Gao Cong},
	title = {Selectivity Estimation for Queries Containing Predicates over Set-Valued
                  Attributes},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {261:1--261:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626755},
	doi = {10.1145/3626755},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/MengCC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Selectivity estimation aims to estimate the size of query results size accurately and efficiently. Despite being a well-researched area for decades, most existing estimators are designed to handle comparison predicates over numeric and categorical data. Nevertheless, Set-valued data are ubiquitous in many applications such as information retrieval and recommendation systems. However, these estimators may not be effective for handling predicates over set-valued data. In this work, we presents novel techniques for selectivity estimation on queries involving predicates over set-valued attributes. We first propose the set-valued column factorization problem, whereby each each set-valued column is converted to multiple numeric subcolumns, and set containment predicates are converted to numeric comparison predicates. This enables us to leverage any existing estimator to perform selectivity estimation. We then develop two methods for column factorization and query conversion, namely ST and STH. We integrate ST and STH into three estimators, Postgres, Neurocard, and DeepDB. We then conduct a comprehensive empirical analysis by comparing our approach against three baselines across three different datasets. The experimental results demonstrate that our methods exhibit superior estimation accuracy while maintaining high efficiency compared to the baseline techniques.}
}


@article{DBLP:journals/pacmmod/WangF23,
	author = {Qiming Wang and
                  Raul Castro Fernandez},
	title = {Solo: Data Discovery Using Natural Language Questions Via {A} Self-Supervised
                  Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {262:1--262:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626756},
	doi = {10.1145/3626756},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most deployed data discovery systems, such as Google Datasets, and open data portals only support keyword search. Keyword search is geared towards general audiences but limits the types of queries the systems can answer. We propose a new system that lets users write natural language questions directly. A major barrier to using this learned data discovery system is it needs expensive-to-collect training data, thus limiting its utility. In this paper, we introduce a self-supervised approach to assemble training datasets and train learned discovery systems without human intervention. It requires addressing several challenges, including the design of self-supervised strategies for data discovery, table representation strategies to feed to the models, and relevance models that work well with the synthetically generated questions. We combine all the above contributions into a system, Solo, that solves the problem end to end. The evaluation results demonstrate the new techniques outperform state-of-the-art approaches on well-known benchmarks. All in all, the technique is a stepping stone towards building learned discovery systems.}
}


@article{DBLP:journals/pacmmod/ShaLWLT23,
	author = {Mo Sha and
                  Jialin Li and
                  Sheng Wang and
                  Feifei Li and
                  Kian{-}Lee Tan},
	title = {TEE-based General-purpose Computational Backend for Secure Delegated
                  Data Processing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {263:1--263:28},
	year = {2023},
	url = {https://doi.org/10.1145/3626757},
	doi = {10.1145/3626757},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShaLWLT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing prevalence of data breaches necessitates robust data protection measures in computational tasks. Secure computation outsourcing (SCO) presents a viable solution by safeguarding the confidentiality of inputs and outputs in data processing without disclosure. Nonetheless, this approach assumes the existence of a trustworthy coordinator to orchestrate and oversee the process, typically implying that data owners must fulfill this role themselves. In this paper, we consider secure delegated data processing (SDDP), an expanded data processing scenario wherein data owners simply delegate their data to SDDP providers for subsequent value mining or other downstream applications, eliminating the necessary involvement of data owners or trusted entities to dive into data processing deeply. However, general-purpose SDDP poses significant challenges in permitting the discretionary execution of computational tasks by SDDP providers on sensitive data while ensuring confidentiality. Existing approaches are insufficient to support SDDP in either efficiency or universality. To tackle this issue, we propose TGCB, a TEE-based General-purpose Computational Backend, designed to endow general-purpose computation with SDDP capabilities from an engineering perspective, powered by TEE-based code integrity and data confidentiality. Central to TGCB is the Encryption Programming Language (EPL) that defines computational tasks in SDDP. Specifically, SDDP providers can express arbitrary computable functions as EPL scripts, processed by TGCB's interfaces, securely interpreted and executed in TEE, ensuring data confidentiality throughout the process. As a universal computational backend, TGCB extensively bolsters data security in existing general-purpose computational tasks, allowing data owners to leverage SDDP without privacy concerns.}
}


@article{DBLP:journals/pacmmod/TianYZHZZ23,
	author = {Yao Tian and
                  Tingyun Yan and
                  Ruiyuan Zhang and
                  Kai Huang and
                  Bolong Zheng and
                  Xiaofang Zhou},
	title = {A Learned Cuckoo Filter for Approximate Membership Queries over Variable-sized
                  Sliding Windows on Data Streams},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {264:1--264:26},
	year = {2023},
	url = {https://doi.org/10.1145/3626758},
	doi = {10.1145/3626758},
	timestamp = {Fri, 08 Nov 2024 08:36:36 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/TianYZHZZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Designing a space-efficient data structure to answer membership queries while ensuring high accuracy and real-time response is a challenging task in the field of stream processing. Many techniques have been developed to answer these queries in a sliding windows manner. However, assuming the user will conduct the query with the presupposed window size is not always practical. In this paper, we introduce a novel data structure called Learned Cuckoo Filter (LCF). It can provide satisfactory results for the approximate membership query on data streams, regardless of the user-defined query windows. LCF operates by adaptively maintaining cuckoo filters with the assistance of a well-trained oracle that learned the frequency feature of the data within the stream. To further enhance memory utilization, we develop a compact version of LCF (denoted by LCF_C), which selectively removes redundant information to reduce space consumption without compromising query accuracy. Furthermore, we conduct a thorough theoretical analysis of query accuracy and provide detailed guidelines for optimal parameter selection (denoted by LCF_O). Extensive experimental studies on synthetic and real-world datasets demonstrate the superiority of the proposed methods in terms of both space consumption and accuracy. Compared to the state-of-the-art algorithms, LCF_O can reduce up to 61% of space cost at the same error level, and achieve up to 12× improved accuracy with the same space cost.}
}


@article{DBLP:journals/pacmmod/HanCGMS23,
	author = {Shanshan Han and
                  Vishal Chakraborty and
                  Michael T. Goodrich and
                  Sharad Mehrotra and
                  Shantanu Sharma},
	title = {Veil: {A} Storage and Communication Efficient Volume-Hiding Algorithm},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {265:1--265:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626759},
	doi = {10.1145/3626759},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HanCGMS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses volume leakage (i.e., leakage of the number of records in the answer set) when processing keyword queries in encrypted key-value (KV) datasets. Volume leakage, coupled with prior knowledge about data distribution and/or previously executed queries, can reveal both ciphertexts and current user queries. We develop a solution to prevent volume leakage, entitled Veil, that partitions the dataset by randomly mapping keys to a set of equi-sized buckets. Veil provides a tunable mechanism for data owners to explore a trade-off between storage and communication overheads. To make buckets indistinguishable to the adversary, Veil uses a novel padding strategy that allow buckets to overlap, reducing the need to add fake records. Both theoretical and experimental results show Veil to significantly outperform existing state-of-the-art.}
}


@article{DBLP:journals/pacmmod/MaiyyaVAAK23,
	author = {Sujaya Maiyya and
                  Sharath Chandra Vemula and
                  Divyakant Agrawal and
                  Amr El Abbadi and
                  Florian Kerschbaum},
	title = {Waffle: An Online Oblivious Datastore for Protecting Data Access Patterns},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {266:1--266:25},
	year = {2023},
	url = {https://doi.org/10.1145/3626760},
	doi = {10.1145/3626760},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/MaiyyaVAAK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present Waffle, a datastore that protects an application's data access patterns from a passive persistent adversary. Waffle achieves this without prior knowledge of the input data access distribution, making it the first of its kind to adaptively handle input sequences under a passive persistent adversary. Waffle maintains a constant bandwidth and client-side storage overhead, which can be adjusted to suit the application owner's preferences. This flexibility allows the owner to fine-tune system parameters and strike a balance between security and performance. Our evaluation, utilizing the Yahoo! Cloud Serving Benchmark (YCSB) benchmark and Redis as the backend storage, demonstrates promising results. The insecure baseline outperforms Waffle by a mere 5-6x, whereas Waffle outperforms Pancake-a state-of-the-art oblivious datastore under passive persistent adversaries-by 45-57%, and a concurrent ORAM system, TaoStore, by 102x.}
}


@article{DBLP:journals/pacmmod/ZhangH23,
	author = {Shufan Zhang and
                  Xi He},
	title = {DProvDB: Differentially Private Query Processing with Multi-Analyst
                  Provenance},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {267:1--267:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626761},
	doi = {10.1145/3626761},
	timestamp = {Tue, 01 Apr 2025 17:56:50 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the adoption of differential privacy (DP) in practical database systems like PINQ, FLEX, and PrivateSQL. Such systems allow data analysts to query sensitive data while providing a rigorous and provable privacy guarantee. However, the existing design of these systems does not distinguish data analysts of different privilege levels or trust levels. This design can have an unfair apportion of the privacy budget among the data analyst if treating them as a single entity, or waste the privacy budget if considering them as non-colluding parties and answering their queries independently. In this paper, we propose DProvDB, a fine-grained privacy provenance framework for the multi-analyst scenario that tracks the privacy loss to each single data analyst. Under this framework, when given a fixed privacy budget, we build algorithms that maximize the number of queries that could be answered accurately and apportion the privacy budget according to the privilege levels of the data analysts.}
}


@article{DBLP:journals/pacmmod/ShahMTKJBM23,
	author = {Raunak Shah and
                  Koyel Mukherjee and
                  Atharv Tyagi and
                  Sai Keerthana Karnam and
                  Dhruv Joshi and
                  Shivam Pravin Bhosale and
                  Subrata Mitra},
	title = {{R2D2:} Reducing Redundancy and Duplication in Data Lakes},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {268:1--268:25},
	year = {2023},
	url = {https://doi.org/10.1145/3626762},
	doi = {10.1145/3626762},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShahMTKJBM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enterprise data lakes often suffer from substantial amounts of duplicate and redundant data, with data volumes ranging from terabytes to petabytes. This leads to both increased storage costs and unnecessarily high maintenance costs for these datasets. In this work, we focus on identifying and reducing redundancy in enterprise data lakes by addressing the problem of "dataset containment". To the best of our knowledge, this is one of the first works that addresses table-level containment at a large scale. We propose R2D2: a three-step hierarchical pipeline that efficiently identifies almost all instances of containment by progressively reducing the search space in the data lake. It first builds (i) a schema containment graph, followed by (ii) statistical min-max pruning, and finally, (iii) content level pruning. We further propose minimizing the total storage and access costs by optimally identifying redundant datasets that can be deleted (and reconstructed on demand) while respecting latency constraints. We implement our system on Azure Databricks clusters using Apache Spark for enterprise data stored in ADLS Gen2, and on AWS clusters for open-source data. In contrast to existing modified baselines that are inaccurate or take several days to run, our pipeline can process an enterprise customer data lake at the TB scale in approximately 5 hours with high accuracy. We present theoretical results as well as extensive empirical validation on both enterprise (scale of TBs) and open-source datasets (scale of MBs - GBs), which showcase the effectiveness of our pipeline.}
}


@article{DBLP:journals/pacmmod/FanHRWWXY23,
	author = {Wenfei Fan and
                  Ziyan Han and
                  Weilong Ren and
                  Ding Wang and
                  Yaoshu Wang and
                  Min Xie and
                  Mengyi Yan},
	title = {Splitting Tuples of Mismatched Entities},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {269:1--269:29},
	year = {2023},
	url = {https://doi.org/10.1145/3626763},
	doi = {10.1145/3626763},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanHRWWXY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There has been a host of work on entity resolution (ER), to identify tuples that refer to the same entity. This paper studies the inverse of ER, to identify tuples to which distinct real-world entities are matched by mistake, and split such tuples into a set of tuples, one for each entity. We formulate the tuple splitting problem. We propose a scheme to decide what tuples to split and what tuples to correct without splitting, fix errors/assign attribute values to the split tuples, and impute missing values. The scheme introduces a class of rules, which embed predicates for aligning entities across relations and knowledge graphs G, assessing correlation between attributes, and extracting data from G. It unifies logic deduction, correlation models, and data extraction by chasing the data with the rules. We train machine learning models to assess attribute correlation and predict missing values. We develop algorithms for the tuple splitting scheme. Using real-life data, we empirically verify that the scheme is efficient and accurate, with F-measure 0.92 on average.}
}


@article{DBLP:journals/pacmmod/ZhaoPCDLO23,
	author = {Zhanhao Zhao and
                  Hexiang Pan and
                  Gang Chen and
                  Xiaoyong Du and
                  Wei Lu and
                  Beng Chin Ooi},
	title = {VeriTxn: Verifiable Transactions for Cloud-Native Databases with Storage
                  Disaggregation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {270:1--270:27},
	year = {2023},
	url = {https://doi.org/10.1145/3626764},
	doi = {10.1145/3626764},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhaoPCDLO23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud-native databases become increasingly popular while exposing to greater data security and correctness risks. Existing verifiable outsourced databases overlook either the correctness risk of transactions, or the disaggregation architecture: a key design consideration of cloud-native databases for performance and elasticity, or both. We present VeriTxn, a novel cloud-native database that efficiently provides verifiability of transaction correctness. VeriTxn relies on the trusted hardware (i.e., Intel SGX) to enable verifiable transaction processing. We build a page-structure cache in the trusted domain, where transactions can be verified with low, constant overhead. VeriTxn further optimizes the read-only transactions by exploiting disaggregation to fit the read-heavy workload in the cloud. We also integrate our proposal into MySQL, a popular open-source database. We conduct extensive experiments to compare VeriTxn against state-of-the-art verifiable databases and evaluate the performance of VeriTxn on MySQL. The results show that VeriTxn introduces tolerable performance degradation for verifiable transactions, while achieving up to 7.03× and 7.93× higher throughput than Litmus and LedgerDB, and its sustainable performance when integrated with MySQL.}
}


@article{DBLP:journals/pacmmod/GuanZMCHCPD23,
	author = {Jiawei Guan and
                  Feng Zhang and
                  Siqi Ma and
                  Kuangyu Chen and
                  Yihua Hu and
                  Yuxing Chen and
                  Anqun Pan and
                  Xiaoyong Du},
	title = {Homomorphic Compression: Making Text Processing on Compression Unlimited},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {271:1--271:28},
	year = {2023},
	url = {https://doi.org/10.1145/3626765},
	doi = {10.1145/3626765},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GuanZMCHCPD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Lossless data compression is an effective way to handle the huge transmission and storage overhead of massive text data. Its utility is even more significant today when data volumes are skyrocketing. The concept of operating on compressed data infuses new blood into efficient text management by enabling mainly access-oriented text processing tasks to be done directly on compressed data without decompression. Facing limitations of the existing compressed text processing schemes such as limited types of operations supported, low efficiency, and high space occupation, we address these problems by proposing a homomorphic compression theory. It enables the generalization and characterization of algorithms with compression processing capabilities. On this basis, we develop HOCO, an efficient text data management engine that supports a variety of processing tasks on compressed text. We select three representative compression schemes and implement them combined with homomorphism in HOCO. HOCO supports the extension of homomorphic compression schemes through a modular and object-oriented design and has convenient interfaces for text processing tasks. We evaluate HOCO on six real-world datasets. The three schemes implemented in HOCO show trade-offs in terms of compression ratio, supported operation types, and efficiency. Experiments also show that HOCO can achieve higher throughput in random access and modification operations (averagely 9.18× than the state-of-the-art) and lower latency in text analytic tasks (averagely 7.16× than processing on uncompressed text) without compromising compression efficacy.}
}


@article{DBLP:journals/pacmmod/MiaoW23,
	author = {Zhengjie Miao and
                  Jin Wang},
	title = {Watchog: {A} Light-weight Contrastive Learning based Framework for
                  Column Annotation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {1},
	number = {4},
	pages = {272:1--272:24},
	year = {2023},
	url = {https://doi.org/10.1145/3626766},
	doi = {10.1145/3626766},
	timestamp = {Sat, 13 Jan 2024 17:37:16 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/MiaoW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relational Web tables provide valuable resources for numerous downstream applications, making table understanding, especially column annotation that identifies semantic types and relations of columns, a hot topic in the field of data management. Despite recent efforts to improve different tasks in table understanding by using the power of large pre-trained language models, existing methods heavily rely on large-scale and high-quality labeled instances, while they still suffer from the data sparsity problem due to the imbalanced data distribution among different classes. In this paper, we propose the Watchog framework, which employs contrastive learning techniques to learn robust representations for tables by leveraging a large-scale unlabeled table corpus with minimal overhead. Our approach enables the learned table representations to enhance fine tuning with much fewer additional labeled instances than in prior studies for downstream column annotation tasks. Besides, we further proposed optimization techniques for semi-supervised settings. Experimental results on popular benchmarking datasets illustrate the superiority of our proposed techniques in two column annotation tasks under different settings. In particular, our Watchog framework effectively alleviates the class imbalance issue caused by a long-tailed label distribution. In the semi-supervised setting, Watchog outperforms the best-known method by up to 26% and 41% in Micro and Macro F1 scores, respectively, on the task of semantic type detection.}
}
