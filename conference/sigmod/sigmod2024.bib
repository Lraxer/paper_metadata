@article{DBLP:journals/pacmmod/AgrawalMS24,
	author = {Divy Agrawal and
                  Alexandra Meliou and
                  S. Sudarshan},
	title = {{PACMMOD} Volume 2 Issue 1: Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {1:1--1:2},
	year = {2024},
	url = {https://doi.org/10.1145/3639256},
	doi = {10.1145/3639256},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgrawalMS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Welcome to Issue 1 of Volume 2 of the Proceedings of the ACM on Management of Data, which has papers from the third round of submissions to the SIGMOD research track. Out of 230 submissions in this round, whose submission deadline was July 15, 2023, a total of 71 articles were accepted and are presented in this issue.}
}


@article{DBLP:journals/pacmmod/ChuPLKLSCHH24,
	author = {David C. Y. Chu and
                  Rithvik Panchapakesan and
                  Shadaj Laddad and
                  Lucky E. Katahanas and
                  Chris Liu and
                  Kaushik Shivakumar and
                  Natacha Crooks and
                  Joseph M. Hellerstein and
                  Heidi Howard},
	title = {Optimizing Distributed Protocols with Query Rewrites},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {2:1--2:25},
	year = {2024},
	url = {https://doi.org/10.1145/3639257},
	doi = {10.1145/3639257},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChuPLKLSCHH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed protocols such as 2PC and Paxos lie at the core of many systems in the cloud, but standard implementations do not scale. New scalable distributed protocols are developed through careful analysis and rewrites, but this process is ad hoc and error-prone. This paper presents an approach for scaling any distributed protocol by applying rule-driven rewrites, borrowing from query optimization. Distributed protocol rewrites entail a new burden: reasoning about spatiotemporal correctness. We leverage order-insensitivity and data dependency analysis to systematically identify correct coordination-free scaling opportunities. We apply this analysis to create preconditions and mechanisms for coordination-free decoupling and partitioning, two fundamental vertical and horizontal scaling techniques. Manual rule-driven applications of decoupling and partitioning improve the throughput of 2PC by 5× and Paxos by 3×, and match state-of-the-art throughput in recent work. These results point the way toward automated optimizers for distributed protocols based on correct-by-construction rewrite rules.}
}


@article{DBLP:journals/pacmmod/CostaFV24,
	author = {Marco Costa and
                  Paolo Ferragina and
                  Giorgio Vinciguerra},
	title = {Grafite: Taming Adversarial Queries with Optimal Range Filters},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {3:1--3:23},
	year = {2024},
	url = {https://doi.org/10.1145/3639258},
	doi = {10.1145/3639258},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/CostaFV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Range filters allow checking whether a query range intersects a given set of keys with a chance of returning a false positive answer, thus generalising the functionality of Bloom filters from point to range queries. Existing practical range filters have addressed this problem heuristically, resulting in high false positive rates and query times when dealing with adversarial inputs, such as in the common scenario where queries are correlated with the keys. We introduce Grafite, a novel range filter that solves these issues with a simple design and clear theoretical guarantees that hold regardless of the input data and query distribution: given a fixed space budget of B bits per key, the query time is O(1), and the false positive probability is upper bounded by l/2B-2, where l is the query range size. Our experimental evaluation shows that Grafite is the only range filter to date to achieve robust and predictable false positive rates across all combinations of datasets, query workloads, and range sizes, while providing faster queries and construction times, and dominating all competitors in the case of correlated queries. As a further contribution, we introduce a very simple heuristic range filter whose performance on uncorrelated queries is very close to or better than the one achieved by the best heuristic range filters proposed in the literature so far.}
}


@article{DBLP:journals/pacmmod/LiuDZLJJHWCC24,
	author = {Jinyang Liu and
                  Sheng Di and
                  Kai Zhao and
                  Xin Liang and
                  Sian Jin and
                  Zizhe Jian and
                  Jiajun Huang and
                  Shixun Wu and
                  Zizhong Chen and
                  Franck Cappello},
	title = {High-performance Effective Scientific Error-bounded Lossy Compression
                  with Auto-tuned Multi-component Interpolation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {4:1--4:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639259},
	doi = {10.1145/3639259},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuDZLJJHWCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Error-bounded lossy compression has been identified as a promising solution for significantly reducing scientific data volumes upon users' requirements on data distortion. For the existing scientific error-bounded lossy compressors, some of them (such as SPERR and FAZ) can reach fairly high compression ratios and some others (such as SZx, SZ, and ZFP) feature high compression speeds, but they rarely exhibit both high ratio and high speed meanwhile. In this paper, we propose HPEZ with newly-designed interpolations and quality-metric-driven auto-tuning, which features significantly improved compression quality upon the existing high-performance compressors, meanwhile being exceedingly faster than high-ratio compressors. The key contributions lie as follows: (1) We develop a series of advanced techniques such as interpolation re-ordering, multi-dimensional interpolation, and natural cubic splines to significantly improve compression qualities with interpolation-based data prediction. (2) The auto-tuning module in HPEZ has been carefully designed with novel strategies, including but not limited to block-wise interpolation tuning, dynamic dimension freezing, and Lorenzo tuning. (3) We thoroughly evaluate HPEZ compared with many other compressors on six real-world scientific datasets. Experiments show that HPEZ outperforms other high-performance error-bounded lossy compressors in compression ratio by up to 140% under the same error bound, and by up to 360% under the same PSNR. In parallel data transfer experiments on the distributed database, HPEZ achieves a significant performance gain with up to 40% time cost reduction over the second-best compressor.}
}


@article{DBLP:journals/pacmmod/ZhangHJZW24,
	author = {Siyuan Zhang and
                  Zhenying He and
                  Yinan Jing and
                  Kai Zhang and
                  X. Sean Wang},
	title = {{MWP:} Multi-Window Parallel Evaluation of Regular Path Queries on
                  Streaming Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {5:1--5:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639260},
	doi = {10.1145/3639260},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangHJZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A persistent Regular Path Query (RPQ) on a streaming graph is to continuously find every pair of vertices that are connected by a path in the graph within a sliding window, such that the edge label sequence of this path matches a given regular expression. The existing RPQ evaluation algorithm in the literature incrementally maintains a set of spanning-tree-like data structures to quickly form query results and to avoid reprocessing edges that are shared by multiple sliding windows. This approach allows parallel processing of the graph edges within a sliding window but requires a blocking expiration phase between sliding windows to remove the old edges. This blocking phase can significantly degrade the query performance, especially when the edges arrive quickly and the sliding windows overlap significantly. This paper presents a new RPQ evaluation strategy called Multi-Window Parallel (MWP) method leveraging a new data structure called Timestamped Rooted Digraph (TRD). The novel idea is to incrementally maintain TRDs for the quick formulation of query results, like the aforementioned spanning trees, but simultaneously contain needed information for multiple sliding windows. MWP eliminates the forced blocking expiration phase. Only when memory runs low, a quick "dirty garbage collection" (DGC) process is done to remove some unneeded edges and nodes on TRDs, without incurring large costs. Extensive experiments on real graph datasets show that MWP significantly outperforms the existing algorithm in terms of throughput, tail latency, and scalability, and that DGC provides an effective solution for releasing memory with minimum impact.}
}


@article{DBLP:journals/pacmmod/YanW24,
	author = {Yinzhao Yan and
                  Raymond Chi{-}Wing Wong},
	title = {Proximity Queries on Point Clouds using Rapid Construction Path Oracle},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {6:1--6:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639261},
	doi = {10.1145/3639261},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YanW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prevalence of computer graphics technology boosts the developments of point clouds in recent years, which offer advantages over terrain surfaces (represented by Triangular Irregular Networks, i.e., TINs) in proximity queries, including the shortest path query, the k-Nearest Neighbor (kNN) query and the range query. Since (1) all existing on-the-fly and oracle-based shortest path query algorithms on a TIN are very expensive, (2) all existing on-the-fly shortest path query algorithms on a point cloud are still not efficient, and (3) there are no oracle-based shortest path query algorithms on a point cloud, we propose an efficient (1+ε)-approximate shortest path oracle that answers the shortest path query for a set of Points-Of-Interests (POIs) on the point cloud, which has a good performance (in terms of the oracle construction time, oracle size and shortest path query time) due to the concise information about the pairwise shortest paths between any pair of POIs stored in the oracle. Our oracle can be easily adapted to answering the shortest path query for any points on the point cloud if POIs are not given as input, and also achieve a good performance. Then, we propose efficient algorithms for answering the (1+ε)-approximate kNN and range query with the assistance of our oracle. Our experimental results show that when POIs are given (resp. not given) as input, our oracle is up to 390 times, 30 times and 6 times (resp. 500 times, 140 times and 50 times) better than the best-known oracle on a TIN in terms of the oracle construction time, oracle size and shortest path query time, respectively. Our algorithms for the other two proximity queries are both up to 100 times faster than the best-known algorithms.}
}


@article{DBLP:journals/pacmmod/WangYL24,
	author = {Kaixin Wang and
                  Kaiqiang Yu and
                  Cheng Long},
	title = {Efficient k-Clique Listing: An Edge-Oriented Branching Strategy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {7:1--7:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639262},
	doi = {10.1145/3639262},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {k-clique listing is a vital graph mining operator with diverse applications in various networks. The state-of-the-art algorithms all adopt a branch-and-bound (BB) framework with a vertex-oriented branching strategy (called VBBkC), which forms a sub-branch by expanding a partial k-clique with a vertex. These algorithms have the time complexity of O(k · m · (δ/2)k-2 ), where m is the number of edges in the graph and δ is the degeneracy of the graph. In this paper, we propose a BB framework with a new edge-oriented branching (called EBBkC), which forms a sub-branch by expanding a partial k-clique with two vertices that connect each other (which correspond to an edge ). We explore various edge orderings for EBBkC such that it achieves a time complexity of O( m · δ + k · m · (τ/2)k-2 ), where τ is an integer related to the maximum truss number of the graph and we have τ < δ. The time complexity of EBBkC is better than that of VBBkC algorithms for k>3 since both O(m · δ) and O(k · m · (τ/2)k-2 ) are bounded by O(k · m · (δ/2)k-2 ). Furthermore, we develop specialized algorithms for sub-branches on dense graphs so that we can early-terminate them and apply the specialized algorithms. We conduct extensive experiments on 19 real graphs, and the results show that our newly developed EBBkC based algorithms with the early termination technique consistently and largely outperform the state-of-the-art (VBBkC based) algorithms.}
}


@article{DBLP:journals/pacmmod/AnC24,
	author = {Shuai An and
                  Yang Cao},
	title = {Relative Keys: Putting Feature Explanation into Context},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {8:1--8:28},
	year = {2024},
	url = {https://doi.org/10.1145/3639263},
	doi = {10.1145/3639263},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AnC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Formal feature explanations strictly maintain perfect conformity but are intractable to compute, while heuristic methods are much faster but can lead to problematic explanations due to lack of conformity guarantees. We propose relative keys that have the best of both worlds. Relative keys associate feature explanations with a set of instances as context, and warrant perfect conformity over the context as formal explanations do, whilst being orders of magnitudes faster and working for complex blackbox models. Based on it, we develop CCE, a prototype that computes explanations with provably bounded conformity and succinctness, without accessing the models. We show that computing the most succinct relative keys is NP-complete and develop various algorithms for it under the batch and online models. Using 9 real-life datasets and 7 state-of-the-art explanation methods, we demonstrate that CCE explains cases where existing methods cannot, and provides more succinct explanations with perfect conformity for cases they can; moreover, it is 2 orders of magnitude faster.}
}


@article{DBLP:journals/pacmmod/LiuMWB24,
	author = {Si Liu and
                  Luca Multazzu and
                  Hengfeng Wei and
                  David A. Basin},
	title = {{NOC-NOC:} Towards Performance-optimal Distributed Transactions},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {9:1--9:25},
	year = {2024},
	url = {https://doi.org/10.1145/3639264},
	doi = {10.1145/3639264},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuMWB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Substantial research efforts have been devoted to studying the performance optimality problem for distributed database transactions. However, they focus just on optimizing transactional reads, and thus overlook crucial factors, such as the efficiency of writes, which also impact the overall system performance. Motivated by a recent study on Twitter's workloads showing the prominence of write-heavy workloads in practice, we make a substantial step towards performance-optimal distributed transactions by also aiming to optimize writes, a fundamentally new dimension to this problem. We propose a new design objective and establish impossibility results with respect to the achievable isolation levels. Guided by these results, we present two new transaction algorithms with different isolation guarantees that fulfill this design objective. Our evaluation demonstrates that these algorithms outperform the state of the art.}
}


@article{DBLP:journals/pacmmod/ZhengLLZLW24,
	author = {Yihang Zheng and
                  Chen Lin and
                  Xian Lyu and
                  Xuanhe Zhou and
                  Guoliang Li and
                  Tianqing Wang},
	title = {Robustness of Updatable Learning-based Index Advisors against Poisoning
                  Attack},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {V2mod010:1--V2mod010:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639265},
	doi = {10.1145/3639265},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhengLLZLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite the promising performance of recent learning-based Index Advisors (IAs), they exhibited the robustness issue when poisoning attacks polluted training data. This paper presents the first attempt to study the robustness of updatable learning-based IAs against poisoning attack, i.e., whether the IAs can maintain robust performance if their training/updating is disturbed by injecting an extraneous toxic workload. The goal is to provide an opaque-box stress test that is generally effective in evaluating the robustness of different learning-based IAs without using the users\' private data. There are three challenges, i.e., how to probe "index preference" from opaque-box IAs, how to design effective injecting strategies even if the IAs can be fine-tuned, and how to generate queries to meet the specific constraints for IA probing and injecting. The presented stress-test framework PIPA consists of a probing stage, an injecting stage, and a query generator. To address the first challenge, the probing stage estimates the IA\'s indexing preference by observing its responses to the probing workload. To address the second challenge, the injecting stage injects workloads that spoof the IA to demote the top-ranked indexes in the estimated indexing preference and promote mid-ranked indexes. The stress test is effective because the IA is trapped in a local optimum even after fine-tuning. To address the third challenge, PIPA utilizes IABART (Index Aware BART) to generate queries that can be optimized by building indexes on a given set of indexes. Extensive experiments on different benchmarks against various learning-based IAs demonstrate the effectiveness of PIPA and that existing learning-based IAs are non-robust when faced with even a subtle amount of injected extraneous toxic workloads.}
}


@article{DBLP:journals/pacmmod/ZhangWXPX24,
	author = {Xinyi Zhang and
                  Qichen Wang and
                  Cheng Xu and
                  Yun Peng and
                  Jianliang Xu},
	title = {FedKNN: Secure Federated k-Nearest Neighbor Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {V2mod011:1--V2mod011:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639266},
	doi = {10.1145/3639266},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangWXPX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nearest neighbor search is a fundamental task in various domains, such as federated learning, data mining, information retrieval, and biomedicine. With the increasing need to utilize data from different organizations while respecting privacy regulations, private data federation has emerged as a promising solution. However, it is costly to directly apply existing approaches to federated k-nearest neighbor (kNN) search with difficult-to-compute distance functions, like graph or sequence similarity. To address this challenge, we propose FedKNN, a system that supports secure federated kNN search queries with a wide range of similarity measurements. Our system is equipped with a new Distribution-Aware kNN (DANN) algorithm to minimize unnecessary local computations while protecting data privacy. We further develop DANN*, a secure version of DANN that satisfies differential obliviousness. Extensive evaluations show that FedKNN outperforms state-of-the-art solutions, achieving up to 4.8× improvement on federated graph kNN search and up to 2.7× improvement on federated sequence kNN search. Additionally, our approach offers a trade-off between privacy and efficiency, providing strong privacy guarantees with minimal overhead.}
}


@article{DBLP:journals/pacmmod/JiXWZXL24,
	author = {Haojie Ji and
                  Kun Xie and
                  Jigang Wen and
                  Qingyi Zhang and
                  Gaogang Xie and
                  Wei Liang},
	title = {FineMon: An Innovative Adaptive Network Telemetry Scheme for Fine-Grained,
                  Multi-Metric Data Monitoring with Dynamic Frequency Adjustment and
                  Enhanced Data Recovery},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {V2mod012:1--V2mod012:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639267},
	doi = {10.1145/3639267},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/JiXWZXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network telemetry, characterized by its efficient push model and high-performance communication protocol (gRPC), offers a new avenue for collecting fine-grained real-time data. Despite its advantages, existing network telemetry systems lack a theoretical basis for setting measurement frequency, struggle to capture informative samples, and face challenges in setting a uniform frequency for multi-metric monitoring. We introduce FineMon, an innovative adaptive network telemetry scheme for precise, fine-grained, multi-metric data monitoring. FineMon leverages a novel Two-sided Frequency Adjustment (TFA) to dynamically adjust the measurement frequency on the Network Management System (NMS) and infrastructure sides. On the NMS side, we provide a theoretical basis for frequency determination, drawing on changes in the rank of multi-metric data to minimize monitoring overhead. On the infrastructure side, we adjust the frequency in real-time to capture significant data fluctuations. We propose a robust Enhanced-Subspace-based Tensor Completion (ESTC) to ensure accurate recovery of fine-grained data, even with noise or outliers. Through extensive experimentation with three real datasets, we demonstrate FineMon's superiority over existing schemes in reduced measurement overhead, enhanced accuracy, and effective capture of intricate temporal features.}
}


@article{DBLP:journals/pacmmod/ZengZZZLZC24,
	author = {Xianzhi Zeng and
                  Shuhao Zhang and
                  Hongbin Zhong and
                  Hao Zhang and
                  Mian Lu and
                  Zhao Zheng and
                  Yuqiang Chen},
	title = {{PECJ:} Stream Window Join on Disorder Data Streams with Proactive
                  Error Compensation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {V2mod013:1--V2mod013:24},
	year = {2024},
	url = {https://doi.org/10.1145/3639268},
	doi = {10.1145/3639268},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZengZZZLZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stream Window Join (SWJ), a vital operation in stream analytics, struggles with achieving a balance between accuracy and latency due to out-of-order data arrivals. Existing methods predominantly rely on adaptive buffering, but often fall short in performance, thereby constraining practical applications. We introduce PECJ, a solution that proactively incorporates unobserved data to enhance accuracy while reducing latency, thus requiring robust predictive modeling of stream oscillation. At the heart of PECJ lies a mathematical formulation of the posterior distribution approximation (PDA) problem using variational inference (VI). This approach circumvents error propagation while meeting the low-latency demands of SWJ. We detail the implementation of PECJ, striking a balance between complexity and generality, and discuss both analytical and learning-based approaches. Experimental evaluations reveal PECJ's superior performance. The successful integration of PECJ into a multi-threaded SWJ benchmark testbed further establishes its practical value, demonstrating promising advancements in enhancing data stream processing capabilities amidst out-of-order data.}
}


@article{DBLP:journals/pacmmod/WangXYWPKGXGX24,
	author = {Mengzhao Wang and
                  Weizhi Xu and
                  Xiaomeng Yi and
                  Songlin Wu and
                  Zhangyang Peng and
                  Xiangyu Ke and
                  Yunjun Gao and
                  Xiaoliang Xu and
                  Rentong Guo and
                  Charles Xie},
	title = {Starling: An I/O-Efficient Disk-Resident Graph Index Framework for
                  High-Dimensional Vector Similarity Search on Data Segment},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {V2mod014:1--V2mod014:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639269},
	doi = {10.1145/3639269},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangXYWPKGXGX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-dimensional vector similarity search (HVSS) is gaining prominence as a powerful tool for various data science and AI applications. As vector data scales up, in-memory indexes pose a significant challenge due to the substantial increase in main memory requirements. A potential solution involves leveraging disk-based implementation, which stores and searches vector data on high-performance devices like NVMe SSDs. However, implementing HVSS for data segments proves to be intricate in vector databases where a single machine comprises multiple segments for system scalability. In this context, each segment operates with limited memory and disk space, necessitating a delicate balance between accuracy, efficiency, and space cost. Existing disk-based methods fall short as they do not holistically address all these requirements simultaneously. In this paper, we present Starling, an I/O-efficient disk-resident graph index framework that optimizes data layout and search strategy within the segment. It has two primary components: (1) a data layout incorporating an in-memory navigation graph and a reordered disk-based graph with enhanced locality, reducing the search path length and minimizing disk bandwidth wastage; and (2) a block search strategy designed to minimize costly disk I/O operations during vector query execution. Through extensive experiments, we validate the effectiveness, efficiency, and scalability of Starling. On a data segment with 2GB memory and 10GB disk capacity, Starling can accommodate up to 33 million vectors in 128 dimensions, offering HVSS with over 0.9 average precision and top-10 recall rate, and latency under 1 millisecond. The results showcase Starling's superior performance, exhibiting 43.9x higher throughput with 98% lower query latency compared to state-of-the-art methods while maintaining the same level of accuracy.}
}


@article{DBLP:journals/pacmmod/LiWL24,
	author = {Yingze Li and
                  Hongzhi Wang and
                  Xianglong Liu},
	title = {One Seed, Two Birds: {A} Unified Learned Structure for Exact and Approximate
                  Counting},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {15:1--15:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639270},
	doi = {10.1145/3639270},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The modern database has many precise and approximate counting requirements. Nevertheless, a solitary multidimensional index or cardinality estimator is insufficient to cater to the escalating demands across all counting scenarios. Such approaches are constrained either by query selectivity or by the compromise between query accuracy and efficiency. We propose CardIndex, a unified learned structure to solve the above problems. CardIndex serves as a versatile solution that not only functions as a multidimensional learned index for accurate counting but also doubles as an adaptive cardinality estimator, catering to varying counting scenarios with diverse requirements for precision and efficiency. Rigorous experimentation has showcased its superiority. Compared to the state-of-the-art (SOTA) autoregressive data-driven cardinality estimation baselines, our structure achieves training and updating times that are two orders of magnitude faster. Additionally, our CPU-based query estimation latency surpasses GPU-based baselines by two to three times. Notably, the estimation accuracy of low-selectivity queries is up to 314 times better than the current SOTA estimator. In terms of indexing tasks, the construction speed of our structure is two orders of magnitude faster than RSMI and 1.9 times faster than R-tree. Furthermore, it exhibits a point query processing speed that is 3%-17% times faster than RSMI and 1.07 to 2.75 times faster than R-tree and KDB-tree. Range queries under specific loads are 20% times faster than the SOTA indexes.}
}


@article{DBLP:journals/pacmmod/ShaikhhaSSN24,
	author = {Amir Shaikhha and
                  Dan Suciu and
                  Maximilian Schleich and
                  Hung Q. Ngo},
	title = {Optimizing Nested Recursive Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {16:1--16:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639271},
	doi = {10.1145/3639271},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShaikhhaSSN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Datalog is a declarative programming language that has gained popularity in various domains due to its simplicity, expressiveness, and efficiency. But "pure" Datalog is limited to monotone queries, and cannot be used in most practical applications. For that reason, newer systems are relaxing the language by allowing non-monotone queries to be freely combined with recursion. But by departing from the elegant fixpoint semantics of pure datalog, these systems often result in inefficient query execution, for example they perform redundant computations, or use redundant storage. In this paper, we propose Temporel, a system that allows recursion to be freely combined with non-monotone operators. Temporel optimizes the program by compiling it into a novel intermediate representation that we call TempoDL. Our experimental results show that our system outperforms a state-of-the-art Datalog engine as well as a vectorized and a compiled in-memory database system for a wide range of applications from machine learning to graph processing.}
}


@article{DBLP:journals/pacmmod/IzenovDTR24,
	author = {Yesdaulet Izenov and
                  Asoke Datta and
                  Brian Tsan and
                  Florin Rusu},
	title = {Sub-optimal Join Order Identification with L1-error},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {17:1--17:24},
	year = {2024},
	url = {https://doi.org/10.1145/3639272},
	doi = {10.1145/3639272},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/IzenovDTR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Q-error -- the standard metric for quantifying the error of individual cardinality estimates -- has been widely adopted as a surrogate for query plan optimality in recent work on learning-based cardinality estimation. However, the only result connecting Q-error with plan optimality is an upper-bound on the cost of the worst possible query plan computed from a set of cardinality estimates---there is no connection between Q-error and the real plans generated by standard query optimizers. Therefore, in order to identify sub-optimal query plans, we propose a learning-based method having as its main feature a novel measure called L1-error. Similar to Q-error, L1-error requires complete knowledge of true cardinalities and estimates for all the sub-plans of a query plan. Unlike Q-error, which considers the estimates independently, L1-error is defined as a permutation distance between true cardinalities and estimates for all the sub-plans having the same number of joins. Moreover, L1-error takes into account errors relative to the magnitude of their cardinalities and gives larger weight to small multi-way joins. Our experimental results confirm that, when L1-error is integrated into a standard decision tree classifier, it leads to the accurate identification of sub-optimal plans across four different benchmarks. This accuracy can be further improved by combining L1-error with Q-error into a composite feature that can be computed without overhead from the same data.}
}


@article{DBLP:journals/pacmmod/FujiwaraKINNK24,
	author = {Yasuhiro Fujiwara and
                  Atsutoshi Kumagai and
                  Yasutoshi Ida and
                  Masahiro Nakano and
                  Makoto Nakatsuji and
                  Akisato Kimura},
	title = {Efficient Algorithm for K-Multiple-Means},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {18:1--18:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639273},
	doi = {10.1145/3639273},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FujiwaraKINNK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {K-Multiple-Means is an extension of K-means for the clustering of multiple means used in many applications, such as image segmentation, load balancing, and blind-source separation. Since K-means uses only one mean to represent each cluster, it fails to capture non-spherical cluster structures of data points. However, since K-Multiple-Means represents the cluster by computing multiple means and grouping them into specified c clusters, it can effectively capture the non-spherical clusters of the data points. To obtain the clusters, K-Multiple-Means updates a similarity matrix of a bipartite graph between the data points and the multiple means by iteratively computing the leading c singular vectors of the matrix. K-Multiple-Means, however, incurs a high computation cost for large-scale data due to the iterative SVD computations. Our proposal, F-KMM, increases the efficiency of K-Multiple-Means by computing the singular vectors from a smaller similarity matrix between the multiple means obtained from the similarity matrix of the bipartite graph. To compute the similarity matrix of the bipartite graph efficiently, we skip unnecessary distance computations and estimate lower bounding distances between the data points and the multiple means. Theoretically, the proposed approach guarantees the same clustering results as K-Multiple-Means since it can exactly compute the singular vectors from the similarity matrix between the multiple means. Experiments show that our approach is several orders of magnitude faster than previous clustering approaches that use multiple means.}
}


@article{DBLP:journals/pacmmod/XuZZWFC24,
	author = {Yanchao Xu and
                  Dongxiang Zhang and
                  Shuhao Zhang and
                  Sai Wu and
                  Zexu Feng and
                  Gang Chen},
	title = {Predictive and Near-Optimal Sampling for View Materialization in Video
                  Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {19:1--19:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639274},
	doi = {10.1145/3639274},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuZZWFC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scalable video query optimization has re-emerged as an attractive research topic in recent years. The OTIF system, a video database with cutting-edge efficiency, has introduced a new paradigm of utilizing view materialization to facilitate online query processing. Specifically, it stores the results of multi-object tracking queries to answer common video queries with sub-second latency. However, the cost associated with view materialization in OTIF is prohibitively high for supporting large-scale video streams. In this paper, we study efficient MOT-based view materialization in video databases. We first conduct a theoretical analysis and establish two types of optimality measures that serve as lower bounds for video frame sampling. In order to minimize the number of processed video frames, we propose a novel predictive sampling framework, namely LEAP, exhibits near-optimal sampling performance. Its efficacy relies on a data-driven motion manager that enables accurate trajectory prediction, a compact object detection model via knowledge distillation, and a robust cross-frame associator to connect moving objects in two frames with a large time gap. Extensive experiments are conducted in 7 real datasets, with 7 baselines and a comprehensive query set, including selection, aggregation and top-k queries. The results show that with comparable query accuracy to OTIF, our LEAP can reduce the number of processed video frames by up to 9× and achieve 5× speedup in query processing time. Moreover, LEAP demonstrates impressive throughput when handling large-scale video streams, as it leverages a single NVIDIA RTX 3090ti GPU to support real-time MOT-based view materialization from 160 video streams simultaneously.}
}


@article{DBLP:journals/pacmmod/ChristodoulouBM24,
	author = {George Christodoulou and
                  Panagiotis Bouros and
                  Nikos Mamoulis},
	title = {{LIT:} Lightning-fast In-memory Temporal Indexing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {20:1--20:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639275},
	doi = {10.1145/3639275},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChristodoulouBM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of temporal database indexing, i.e., indexing versions of a database table in an evolving database. With the larger and cheaper memory chips nowadays, we can afford to keep track of all versions of an evolving table in memory. This raises the question of how to index such a table effectively. We depart from the classic indexing approach, where both current (i.e., live) and past (i.e., dead) data versions are indexed in the same data structure, and propose LIT, a hybrid index, which decouples the management of the current and past states of the indexed column. LIT includes optimized indexing modules for dead and live records, which support efficient queries and updates, and gracefully combines them. We experimentally show that LIT is orders of magnitude faster than the state-of-the-art temporal indices. Furthermore, we demonstrate that LIT uses linear space to the number of record indexed versions, making it suitable for main-memory temporal data management.}
}


@article{DBLP:journals/pacmmod/YangJYMB24,
	author = {Junran Yang and
                  Hyekang Kevin Joo and
                  Sai S. Yerramreddy and
                  Dominik Moritz and
                  Leilani Battle},
	title = {Optimizing Dataflow Systems for Scalable Interactive Visualization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {21:1--21:25},
	year = {2024},
	url = {https://doi.org/10.1145/3639276},
	doi = {10.1145/3639276},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangJYMB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Supporting the interactive exploration of large datasets is a popular and challenging use case for data management systems. Traditionally, the interface and the back-end system are built and optimized separately, and interface design and system optimization require different skill sets that are difficult for one person to master. To enable analysts to focus on visualization design, we contribute VegaPlus, a system that automatically optimizes interactive dashboards to support large datasets. To achieve this, VegaPlus leverages two core ideas. First, we introduce an optimizer that can reason about execution plans in Vega, a back-end DBMS, or a mix of both environments. The optimizer also considers how user interactions may alter execution plan performance, and can partially or fully rewrite the plans when needed. Through a series of benchmark experiments on seven different dashboard designs, our results show that VegaPlus provides superior performance and versatility compared to standard dashboard optimization techniques.}
}


@article{DBLP:journals/pacmmod/ZengFMZL24,
	author = {Yuanyuan Zeng and
                  Yixiang Fang and
                  Chenhao Ma and
                  Xu Zhou and
                  Kenli Li},
	title = {Efficient Distributed Hop-Constrained Path Enumeration on Large-Scale
                  Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {22:1--22:25},
	year = {2024},
	url = {https://doi.org/10.1145/3639277},
	doi = {10.1145/3639277},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZengFMZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The enumeration of hop-constrained simple paths is a building block in many graph-based areas. Due to the enormous search spaces in large-scale graphs, a single machine can hardly satisfy the requirements of both efficiency and memory, which causes an urgent need for efficient distributed methods. In practice, it is inevitable to produce plenty of intermediate results when directly extending centralized methods to the distributed environment, thereby causing a memory crisis and weakening the query performance. The state-of-the-art distributed method HybridEnum designed a hybrid search paradigm to enumerate simple paths. However, it makes massive exploration for the redundant vertices not located in any simple path, thereby resulting in poor query performance. To alleviate this problem, we design a distributed approach DistriEnum to optimize query performance and scalability with well-bound memory consumption. Firstly, DistriEnum adopts a graph reduction strategy to rule out the redundant vertices without satisfying the constraint of hop number. Then, a core search paradigm is designed to simultaneously reduce the traversal of shared subpaths and the storage of intermediate results. Moreover, DistriEnum is equipped with a task division strategy to theoretically achieve workload balance. Finally, a vertex migration strategy is devised to reduce the communication cost during the enumeration. The comprehensive experimental results on 10 real-world graphs demonstrate that DistriEnum achieves up to 3 orders of magnitude speedup than HybridEnum in query performance and exhibits superior performances on scalability, communication cost, and memory consumption.}
}


@article{DBLP:journals/pacmmod/YangS24,
	author = {Renchi Yang and
                  Jieming Shi},
	title = {Efficient High-Quality Clustering for Large Bipartite Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {23:1--23:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639278},
	doi = {10.1145/3639278},
	timestamp = {Fri, 07 Feb 2025 15:20:40 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A bipartite graph contains inter-set edges between two disjoint vertex sets, and is widely used to model real-world data, such as user-item purchase records, author-article publications, and biological interactions between drugs and proteins. k-Bipartite Graph Clustering (k-BGC) is to partition the target vertex set in a bipartite graph into k disjoint clusters. The clustering quality is important to the utility of k-BGC in various applications like social network analysis, recommendation systems, text mining, and bioinformatics, to name a few. Existing approaches to k-BGC either output clustering results with compromised quality due to inadequate exploitation of high-order information between vertices, or fail to handle sizable bipartite graphs with billions of edges. Motivated by this, this paper presents two efficient k-BGC solutions, HOPE and HOPE+, which achieve state-of-the-art performance on large-scale bipartite graphs. HOPE obtains high scalability and effectiveness through a new k-BGC problem formulation based on the novel notion of high-order perspective (HOP) vectors and an efficient technique for low-rank approximation of HOP vectors. HOPE+ further elevates the k-BGC performance to another level with a judicious problem transformation and a highly efficient two-stage optimization framework. Two variants, HOPE+ (FNEM) and HOPE+ (SNEM) are designed when either the Frobenius norm or spectral norm is applied in the transformation. Extensive experiments, comparing HOPE and HOPE+ against 13 competitors on 10 real-world datasets, exhibit that our solutions, especially HOPE+, are superior to existing methods in terms of result quality, while being up to orders of magnitude faster. On the largest dataset MAG with 1.1 billion edges, HOPE+ is able to produce clusters with the highest clustering accuracy within 31 minutes, which is unmatched by any existing solution for k-BGC.}
}


@article{DBLP:journals/pacmmod/NobariR24,
	author = {Arash Dargahi Nobari and
                  Davood Rafiei},
	title = {{DTT:} An Example-Driven Tabular Transformer for Joinability by Leveraging
                  Large Language Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {24:1--24:24},
	year = {2024},
	url = {https://doi.org/10.1145/3639279},
	doi = {10.1145/3639279},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/NobariR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many organizations rely on data from government and third-party sources, and those sources rarely follow the same data formatting. This introduces challenges in integrating data from multiple sources or aligning external sources with internal databases. Commercial database systems do not offer adequate support for integrating data from heterogeneous sources, and manual integration is both time-consuming and inefficient. State-of-the-art data integration approaches that rely on similarity functions and textual transformations often fail to handle challenging cases where multiple mappings are required, or the mappings go beyond simple textual transformations. In this paper, we study the potentials of deep neural models for transforming tables for joinability. In particular, we cast the problem as a prediction task and develop a framework that leverages large deep-learning language models to transform tabular data from a source formatting to a desired target representation. Our framework can efficiently learn the patterns for mapping a source formatting into an expected target using just a few examples, which can then be used for tasks such as table joining, filling in missing values, and error detection. Compared to state-of-the-art mapping and joining approaches, our framework delivers noticeably more accurate and scalable performance on both real-world and synthetic datasets. Our experimental evaluation also shows that the performance of the proposed framework using our fine-tuned model is at par or better than large language models such as GPT-3, despite the significant difference in size, and that using large language models within our framework improves their performance.}
}


@article{DBLP:journals/pacmmod/ChenGSHWW24,
	author = {Ziling Chen and
                  Haoquan Guan and
                  Shaoxu Song and
                  Xiangdong Huang and
                  Chen Wang and
                  Jianmin Wang},
	title = {Determining Exact Quantiles with Randomized Summaries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {25:1--25:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639280},
	doi = {10.1145/3639280},
	timestamp = {Mon, 09 Sep 2024 19:07:28 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenGSHWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantiles are fundamental statistics in various data science tasks, but costly to compute, e.g., by loading the entire data in memory for ranking. With limited memory space, prevalent in end devices or databases with heavy loads, it needs to scan the data in multiple passes. The idea is to gradually shrink the range of the queried quantile till it is small enough to fit in memory for ranking the result. Existing methods use deterministic sketches to determine the exact range of quantile, known as deterministic filter, which could be inefficient in range shrinking. In this study, we propose to shrink the ranges more aggressively, using randomized summaries such as KLL sketch. That is, with a high probability the quantile lies in a smaller range, namely probabilistic filter, determined by the randomized sketch. Specifically, we estimate the expected passes for determining the exact quantiles with probabilistic filters, and select a proper probability that can minimize the expected passes. Analyses show that our exact quantile determination method can terminate in P passes with 1-δ confidence, storing O(N 1/P logP-1/2P (1/δ)) items, close to the lower bound Ømega(N1/P) for a fixed δ. The approach has been deployed as a function in an LSM-tree based time-series database Apache IoTDB. Remarkably, the randomized sketches can be pre-computed for the immutable SSTables in LSM-tree. Moreover, multiple quantile queries could share the data passes for probabilistic filters in range estimation. Extensive experiments on real and synthetic datasets demonstrate the superiority of our proposal compared to the existing methods with deterministic filters. On average, our method takes 0.48 fewer passes and 18% of the time compared with the state-of-the-art deterministic sketch (GK sketch).}
}


@article{DBLP:journals/pacmmod/WangLLL24,
	author = {Pinghui Wang and
                  Yitong Liu and
                  Zhicheng Li and
                  Rundong Li},
	title = {An {LDP} Compatible Sketch for Securely Approximating Set Intersection
                  Cardinalities},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {26:1--26:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639281},
	doi = {10.1145/3639281},
	timestamp = {Thu, 08 Aug 2024 12:56:51 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given two sets of elements held by two different parties separately, computing the cardinality (i.e., the number of distinct elements) of their intersection set is a fundamental task in applications such as network monitoring and database systems. To handle large sets with limited space, computation, and communication costs, lightweight probabilistic methods (i.e., sketch methods) such as the Flajolet-Martin (FM) sketch and the HyperLogLog (HLL) sketch are extensively used. However, when a set's probabilistic data summary and the hash functions used to construct the sketch are disclosed to an untrusted third party, the set's privacy is compromised. Directly applyingLocal Differential Privacy (LDP) techniques to safeguard the sketch collection results in extremely large estimation errors of set intersection cardinalities. To address this issue, we propose a novel sketch method that makes it easier to incorporate noise into the constructed sketch to achieve differential privacy. More importantly, our sketch method is compatible with the LDP noise. In other words, the probabilistic model underlying our LDP-based data summary is quite basic, allowing us to eliminate the estimation error generated by the noise. We perform extensive experiments on various synthetic and real-world datasets and the experimental results demonstrate that our method is orders of magnitude more accurate and several times faster than state-of-the-art methods.}
}


@article{DBLP:journals/pacmmod/ShiWX24,
	author = {Jifan Shi and
                  Biao Wang and
                  Yun Xu},
	title = {Spruce: a Fast yet Space-saving Structure for Dynamic Graph Storage},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {27:1--27:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639282},
	doi = {10.1145/3639282},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShiWX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic graphs have been gaining increasing popularity across various application domains. With the growing size of these graphs, the update performance as well as space occupancy is becoming a crucial aspect of dynamic graph storage. Although existing dynamic graph systems can handle massive streaming updates (e.g., insertions and deletions), they cannot achieve both high throughput and low memory footprint. Drawing inspiration from the basic operations of the van Emde Boas (vEB) tree in double-logarithmic time, we designed Spruce, a high-performance yet space-saving in-memory structure to store dynamic graphs. Spruce uses a compact representation to construct the tree-like multilevel structure, which shares the common prefixes of vertices and has no merging or splitting of nodes to achieve the requirements of low memory consumption and high-efficiency dynamic operations. Furthermore, Spruce incorporates a read-optimized concurrency protocol, which refines ROWEX and Optimistic Locking, to facilitate efficient simultaneous read/write operations. Our experiment demonstrates that compared to Sortledton (the best of competitors), Spruce is up to 2.4X faster in ingesting graph updates, while saving up to 38.5% of memory space. As for graph analytics, Spruce shows high adaptability to different analytical workloads, and achieves comparable performance to other state-of-the-art dynamic graph structures.}
}


@article{DBLP:journals/pacmmod/LiuFTLD24,
	author = {Tongyu Liu and
                  Ju Fan and
                  Nan Tang and
                  Guoliang Li and
                  Xiaoyong Du},
	title = {Controllable Tabular Data Synthesis Using Diffusion Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {28:1--28:29},
	year = {2024},
	url = {https://doi.org/10.1145/3639283},
	doi = {10.1145/3639283},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuFTLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Controllable tabular data synthesis plays a crucial role in numerous applications by allowing users to generate synthetic data with specific conditions. These conditions can include synthesizing tuples with predefined attribute values or creating tuples that exhibit a particular correlation with an external table. However, existing approaches lack the flexibility to support new conditions and can be time-consuming when dealing with multiple conditions. To overcome these limitations, we propose a novel approach that leverages diffusion models to first learn an unconditional generative model. Subsequently, we introduce lightweight controllers to guide the unconditional generative model in generating synthetic data that satisfies different conditions. The primary research challenge lies in effectively supporting controllability using lightweight solutions while ensuring the realism of the synthetic data. To address this challenge, we design an unconditional diffusion model tailored specifically for tabular data. Additionally, we propose a new sampling method that enables correlation-aware controls throughout the data generation process. We conducted extensive experiments across various applications for controllable tabular data synthesis, which show that our approach outperforms the state-of-the-art methods.}
}


@article{DBLP:journals/pacmmod/YangZLCW24,
	author = {Boyu Yang and
                  Weiguo Zheng and
                  Xiang Lian and
                  Yuzheng Cai and
                  X. Sean Wang},
	title = {{HERO:} {A} Hierarchical Set Partitioning and Join Framework for Speeding
                  up the Set Intersection Over Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {29:1--29:25},
	year = {2024},
	url = {https://doi.org/10.1145/3639284},
	doi = {10.1145/3639284},
	timestamp = {Tue, 06 May 2025 16:06:34 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YangZLCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the most primitive operators in graph algorithms, such as the triangle counting, maximal clique enumeration, and subgraph listing, a set intersection operator returns common vertices between any two given sets of vertices in data graphs. It is therefore very important to accelerate the set intersection, which will benefit a bunch of tasks that take it as a built-in block. Existing works on the set intersection usually followed the merge intersection or galloping-search framework, and most optimization research focused on how to leverage the SIMD hardware instructions. In this paper, we propose a novel multi-level set intersection framework, namely hierarchical set partitioning and join (HERO), by using our well-designed set intersection bitmap tree (SIB-tree) index, which is independent of SIMD instructions and completely orthogonal to the merge intersection framework. We recursively decompose the set intersection task into small-sized subtasks and solve each subtask using bitmap and boolean AND operations. To sufficiently achieve the acceleration brought by our proposed intersection approach, we formulate a graph reordering problem, prove its NP-hardness, and then develop a heuristic algorithm to tackle this problem. Extensive experiments on real-world graphs have been conducted to confirm the efficiency and effectiveness of our HERO approach. The speedup over classic merge intersection achieves up to 188x and 176x for triangle counting and maximal clique enumeration, respectively.}
}


@article{DBLP:journals/pacmmod/LiLLHZQR24,
	author = {Xiaochen Li and
                  Weiran Liu and
                  Jian Lou and
                  Yuan Hong and
                  Lei Zhang and
                  Zhan Qin and
                  Kui Ren},
	title = {Local Differentially Private Heavy Hitter Detection in Data Streams
                  with Bounded Memory},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {30:1--30:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639285},
	doi = {10.1145/3639285},
	timestamp = {Wed, 05 Mar 2025 08:16:09 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiLLHZQR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Top-k frequent items detection is a fundamental task in data stream mining. Many promising solutions are proposed to improve memory efficiency while still maintaining high accuracy for detecting the Top-k items. Despite the memory efficiency concern, the users could suffer from privacy loss if participating in the task without proper protection, since their contributed local data streams may continually leak sensitive individual information. However, most existing works solely focus on addressing either the memory-efficiency problem or the privacy concerns but seldom jointly, which cannot achieve a satisfactory tradeoff between memory efficiency, privacy protection, and detection accuracy. In this paper, we present a novel framework HG-LDP to achieve accurate Top-k item detection at bounded memory expense, while providing rigorous local differential privacy (LDP) protection. Specifically, we identify two key challenges naturally arising in the task, which reveal that directly applying existing LDP techniques will lead to an inferior "accuracy-privacy-memory efficiency" tradeoff. Therefore, we instantiate three advanced schemes under the framework by designing novel LDP randomization methods, which address the hurdles caused by the large size of the item domain and by the limited space of the memory. We conduct comprehensive experiments on both synthetic and real-world datasets to show that the proposed advanced schemes achieve a superior "accuracy-privacy-memory efficiency" tradeoff, saving 2300× memory over baseline methods when the item domain size is 41,270. Our code is anonymously open-sourced via the link.}
}


@article{DBLP:journals/pacmmod/HaoZYS24,
	author = {Xiangpeng Hao and
                  Xinjing Zhou and
                  Xiangyao Yu and
                  Michael Stonebraker},
	title = {Towards Buffer Management with Tiered Main Memory},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {31:1--31:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639286},
	doi = {10.1145/3639286},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HaoZYS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The scaling of per-GB DRAM cost has slowed down in recent years. Recent research has suggested that adding remote memory to a system can further reduce the overall memory cost while maintaining good performance. Remote memory (i.e., tiered memory), connected to host servers via high-speed interconnect protocols such as RDMA and CXL, is expected to deliver 100x (less than 1µs) lower latency than SSD and be more cost-effective than local DRAM through pooling or adopting cheaper memory technologies. Tiered memory opens up a large number of potential use cases within database systems. But previous work has only explored limited ways of using tiered memory. Our study provides a systematic study for DBMS to build tiered memory buffer management with respect to a wide range of hardware performance characteristics. Specifically, we study five different indexing designs that leverage remote memory in different ways and evaluate them through a wide range of metrics including performance, tiered-memory latency sensitivity, and cost-effectiveness. In addition, we propose a new memory provisioning strategy that allocates an optimal amount of local and remote memory for a given workload. Our evaluations show that while some designs achieve higher performance than others, no design can win in all measured dimensions.}
}


@article{DBLP:journals/pacmmod/ShiDS24,
	author = {Jessica Shi and
                  Laxman Dhulipala and
                  Julian Shun},
	title = {Parallel Algorithms for Hierarchical Nucleus Decomposition},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {32:1--32:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639287},
	doi = {10.1145/3639287},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShiDS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nucleus decompositions have been shown to be a useful tool for finding dense subgraphs. The coreness value of a clique represents its density based on the number of other cliques it is adjacent to. One useful output of nucleus decomposition is to generate a hierarchy among dense subgraphs at different resolutions. However, existing parallel algorithms for nucleus decomposition do not generate this hierarchy, and only compute the coreness values. This paper presents a scalable parallel algorithm for hierarchy construction, with practical optimizations, such as interleaving the coreness computation with hierarchy construction and using a concurrent union-find data structure in an innovative way to generate the hierarchy. We also introduce a parallel approximation algorithm for nucleus decomposition, which achieves much lower span in theory and better performance in practice. We prove strong theoretical bounds on the work and span (parallel time) of our algorithms. On a 30-core machine with two-way hyper-threading, our parallel hierarchy construction algorithm achieves up to a 58.84x speedup over the state-of-the-art sequential hierarchy construction algorithm by Sariyuce et al. and up to a 30.96x self-relative parallel speedup. On the same machine, our approximation algorithm achieves a 3.3x speedup over our exact algorithm, while generating coreness estimates with a multiplicative error of 1.33x on average.}
}


@article{DBLP:journals/pacmmod/YeLSG24,
	author = {Chang Ye and
                  Yuchen Li and
                  Shixuan Sun and
                  Wentian Guo},
	title = {gSWORD: GPU-accelerated Sampling for Subgraph Counting},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {33:1--33:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639288},
	doi = {10.1145/3639288},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YeLSG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph counting is a fundamental component for many downstream applications such as graph representation learning and query optimization.Since obtaining the exact count is often intractable,there have been a plethora of approximation methods on graph sampling techniques. Nonetheless, the state-of-the-art sampling methods still require massive samples to produce accurate approximations on large data graphs.We propose gSWORD, a GPU framework that leverages the massive parallelism of GPUs to accelerate iterative sampling algorithms for subgraph counting. Despite the embarrassingly parallel nature of the samples, there are unique challenges in accelerating subgraph counting due to its irregular computation logic. To address these challenges, we introduce two GPU-centric optimizations: (1) sample inheritance, enabling threads to inherit samples from neighboring threads to avoid idling, and (2) warp streaming, effectively distributing workloads among threads through a streaming process. Moreover, we propose a CPU-GPU co-processing pipeline that overlaps the sampling and enumeration processes to mitigate the underestimation issue. Experimental results demonstrate that deploying state-of-the-art sampling algorithms on gSWORD can perform millions of samples per second. The co-processing pipeline substantially improves the estimation accuracy in the cases where existing methods encounter severe underestimations with negligible overhead.}
}


@article{DBLP:journals/pacmmod/FangY24,
	author = {Juanru Fang and
                  Ke Yi},
	title = {Privacy Amplification by Sampling under User-level Differential Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {34:1--34:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639289},
	doi = {10.1145/3639289},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FangY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Random sampling is an effective tool for reducing the computational costs of query processing in large databases. It has also been used frequently for private data analysis, in particular, under differential privacy (DP). An interesting phenomenon that the literature has identified, is that sampling can amplify the privacy guarantee of a mechanism, which in turn leads to reduced noise scales that have to be injected. All existing privacy amplification results only hold in the standard, record-level DP model. Recently, user-level differential privacy (user-DP) has gained a lot of attention as it protects all data records contributed by any particular user, thus offering stronger privacy protection. Sampling-based mechanisms under user-DP have not been explored so far, except naively running the mechanism on a sample without privacy amplification, which results in large DP noises. In fact, sampling is in even more demand under user-DP, since all state-of-the-art user-DP mechanisms have high computational costs due to the complex relationships between users and records. In this paper, we take the first step towards the study of privacy amplification by sampling under user-DP, and give the amplification results for two common user-DP sampling strategies: simple sampling and sample-and-explore. The experimental results show that these sampling-based mechanisms can be a useful tool to obtain some quick and reasonably accurate estimates on large private datasets.}
}


@article{DBLP:journals/pacmmod/RuiHSKWW24,
	author = {Lei Rui and
                  Xiangdong Huang and
                  Shaoxu Song and
                  Yuyuan Kang and
                  Chen Wang and
                  Jianmin Wang},
	title = {Time Series Representation for Visualization in Apache IoTDB},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {35:1--35:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639290},
	doi = {10.1145/3639290},
	timestamp = {Mon, 09 Sep 2024 19:07:28 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/RuiHSKWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When analyzing time series, often interactively, the analysts frequently demand to visualize instantly large-scale data stored in databases. M4 visualization selects the first, last, bottom and top data points in each pixel column to ensure pixel-perfectness of the two-color line chart visualization. While M4 already shows its preciseness of encasing time series in different scales into a fixed size of pixels, how to efficiently support M4 representation in a time series native database is still absent. It is worth noting that, to enable fast writes, the commodity time series database systems, such as Apache IoTDB or InfluxDB, employ LSM-Tree based storage. That is, a time series is segmented and stored in a number of chunks, with possibly out-of-order arrivals, i.e., disordered on timestamps. To implement M4, a natural idea is to merge online the chunks as a whole series, with costly merge sort on timestamps, and then perform M4 representation as in relational databases. In this study, we propose a novel chunk merge free approach called M4-LSM to accelerate M4 representation and visualization. In particular, we utilize the metadata of chunks to prune and avoid the costly merging of any chunk. Moreover, intra-chunk indexing and pruning are enabled for efficiently accessing the representation points, referring to the special properties of time series. Remarkably, the time series database native operator M4-LSM has been implemented in Apache IoTDB, an open-source time series database, and deployed in companies across various industries. In the experiments over real-world datasets, the proposed M4-LSM operator demonstrates high efficiency without sacrificing preciseness.}
}


@article{DBLP:journals/pacmmod/JasnyTTKIB24,
	author = {Matthias Jasny and
                  Lasse Thostrup and
                  Sajjad Tamimi and
                  Andreas Koch and
                  Zsolt Istv{\'{a}}n and
                  Carsten Binnig},
	title = {Zero-sided {RDMA:} Network-driven Data Shuffling for Disaggregated
                  Heterogeneous Cloud DBMSs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {36:1--36:28},
	year = {2024},
	url = {https://doi.org/10.1145/3639291},
	doi = {10.1145/3639291},
	timestamp = {Fri, 31 May 2024 21:08:02 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/JasnyTTKIB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present a novel communication scheme called zero-sided RDMA, enabling data exchange as a native network service using a programmable switch. In contrast to one- or two-sided RDMA, in zero-sided RDMA, neither the sender nor the receiver is actively involved in data exchange. Zero-sided RDMA thus enables efficient RDMA-based data shuffling between heterogeneous hardware devices in a disaggregated setup without the need to implement a complete RDMA stack on each heterogeneous device or the need for a CPU that is co-located with the accelerator to coordinate the data transfer. As such, we think that zero-sided RDMA is a major building block to make efficient use of heterogeneous accelerators in future cloud DBMSs. In our evaluation, we show that zero-sided RDMA can outperform existing one-sided RDMA-based schemes for accelerator-to-accelerator communication and thus speed up typical distributed database operations such as joins.}
}


@article{DBLP:journals/pacmmod/ZhangZLC24,
	author = {Jintao Zhang and
                  Chao Zhang and
                  Guoliang Li and
                  Chengliang Chai},
	title = {{PACE:} Poisoning Attacks on Learned Cardinality Estimation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {37:1--37:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639292},
	doi = {10.1145/3639292},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangZLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality estimation (CE) plays a crucial role in database optimizer. We have witnessed the emergence of numerous learned CE models recently which can outperform traditional methods such as histograms and samplings. However, learned models also bring many security risks. For example, a query-driven learned CE model learns a query-to-cardinality mapping based on the historical workload. Such a learned model could be attacked by poisoning queries, which are crafted by malicious attackers and woven into the historical workload, leading to performance degradation of CE. In this paper, we explore the potential security risks in learned CE and study a new problem of poisoning attacks on learned CE in a black-box setting. There are three challenges. First, the interior details of the CE model are hidden in the black-box setting, making it difficult to attack the model. Second, the attacked CE model's parameters will be updated with the poisoning queries, i.e., a variable varying with the optimization variable, so the problem cannot be modeled as a univariate optimization problem and thus is hard to solve by an efficient algorithm. Third, to make an imperceptible attack, it requires to generate poisoning queries that follow a similar distribution to historical workload. We propose a poisoning attack system, PACE, to address these challenges. To tackle the first challenge, we propose a method of speculating and training a surrogate model, which transforms the black-box attack into a near-white-box attack. To address the second challenge, we model the poisoning problem as a bivariate optimization problem, and design an effective and efficient algorithm to solve it. To overcome the third challenge, we propose an adversarial approach to train a poisoning query generator alongside an anomaly detector, ensuring that the poisoning queries follow similar distribution to historical workload. Experiments show that PACE reduces the accuracy of the learned CE models by 178×, leading to a 10× decrease in the end-to-end performance of the target database.}
}


@article{DBLP:journals/pacmmod/WuI24,
	author = {Peizhi Wu and
                  Zachary G. Ives},
	title = {Modeling Shifting Workloads for Learned Database Systems},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {38:1--38:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639293},
	doi = {10.1145/3639293},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learned database systems address several weaknesses of traditional cost estimation techniques in query optimization: they learn a model of a database instance, e.g., as queries are executed. However, when the database instance has skew and correlation, it is nontrivial to create an effective training set that anticipates workload shifts, where query structure changes and/or different regions of the data contribute to query answers. Our predictive model may perform poorly with these out-of-distribution inputs. In this paper, we study how the notion of a replay buffer can be managed through online algorithms to build a concise yet representative model of the workload distribution --- allowing for rapid adaptation and effective prediction of cardinalities and costs. We experimentally validate our methods over several data domains.}
}


@article{DBLP:journals/pacmmod/ArroyueloBGHNR24,
	author = {Diego Arroyuelo and
                  Benjamin Bustos and
                  Adri{\'{a}}n G{\'{o}}mez{-}Brand{\'{o}}n and
                  Aidan Hogan and
                  Gonzalo Navarro and
                  Juan L. Reutter},
	title = {Worst-Case-Optimal Similarity Joins on Graph Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {39:1--39:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639294},
	doi = {10.1145/3639294},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ArroyueloBGHNR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We extend the concept of worst-case optimal equijoins in graph databases to the case where some nodes are required to be within the k-nearest neighbors (kNN) of others under some similarity function. We model the problem by superimposing the database graph with the kNN graph and show that a variant of Leapfrog TrieJoin (LTJ) implemented over a compact data structure called the Ring can be seamlessly extended to integrate similarity clauses with the equijoins in the LTJ query process, retaining worst-case optimality in many relevant cases. Our experiments on a benchmark that combines Wikidata and IMGpedia show that our enhanced LTJ algorithm outperforms by a considerable margin a baseline that first applies classic LTJ and then completes the query by applying the similarity predicates. The difference is more pronounced on queries where the similarity clauses are more densely connected to the query, becoming of an order of magnitude in some cases.}
}


@article{DBLP:journals/pacmmod/ChenQWKKG24,
	author = {Tingyang Chen and
                  Dazhuo Qiu and
                  Yinghui Wu and
                  Arijit Khan and
                  Xiangyu Ke and
                  Yunjun Gao},
	title = {View-based Explanations for Graph Neural Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {40:1--40:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639295},
	doi = {10.1145/3639295},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenQWKKG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generating explanations for graph neural networks (GNNs) has been studied to understand their behaviors in analytical tasks such as graph classification. Existing approaches aim to understand the overall results of GNNs rather than providing explanations for specific class labels of interest, and may return explanation structures that are hard to access, nor directly queryable. We propose GVEX, a novel paradigm that generates Graph Views for GNN EXplanation. (1) We design a two-tier explanation structure called explanation views. An explanation view consists of a set of graph patterns and a set of induced explanation subgraphs. Given a database G of multiple graphs and a specific class label l assigned by a GNN-based classifier M, it concisely describes the fraction of G that best explains why l is assigned by M. (2) We propose quality measures and formulate an optimization problem to compute optimal explanation views for GNN explanation. We show that the problem is Σ2P-hard. (3) We present two algorithms. The first one follows an explain-and-summarize strategy that first generates high-quality explanation subgraphs which best explain GNNs in terms of feature influence maximization, and then performs a summarization step to generate patterns. We show that this strategy provides an approximation ratio of 1/2. Our second algorithm performs a single-pass to an input node stream in batches to incrementally maintain explanation views, having an anytime quality guarantee of 1/4-approximation. Using real-world benchmark data, we experimentally demonstrate the effectiveness, efficiency, and scalability of GVEX. Through case studies, we showcase the practical applications of GVEX.}
}


@article{DBLP:journals/pacmmod/LiangYHCH24,
	author = {Liang Liang and
                  Guang Yang and
                  Ali Hadian and
                  Luis Alberto Croquevielle and
                  Thomas Heinis},
	title = {{SWIX:} {A} Memory-efficient Sliding Window Learned Index},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {41:1--41:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639296},
	doi = {10.1145/3639296},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiangYHCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data stream processing systems enable querying over sliding windows of streams of data. Efficient index structures for the streaming window are a crucial building block to enable querying the sliding window for operations such as aggregation and joins. This paper proposes SWIX, a novel memory-efficient learned index for sliding windows. Unlike conventional learned indexes that rely on tree structures to achieve logarithmic query cost, SWIX has a flat structure that uses substantially less memory and enables efficient query execution while having a low cost for index maintenance when inserting (and retraining). SWIX dynamically adapts itself to the real-time distribution shifts of data streams. SWIX outperforms existing indexes in terms of query execution time and memory footprint for workloads characterized by very frequent updates. Our results show that SWIX has a significantly smaller memory footprint than conventional, streaming, and learned indexes, using only 22% to 42% of the size compared to state-of-the-art approaches, yet outperforming them by up 1.2× to 1.6× on average (and up to 52×) in terms of query time, making it a space- and time-efficient method for indexing data streams. For concurrent learned indexes, Parallel SWIX can achieve up to 3.45× throughput with only 34% of memory consumption.}
}


@article{DBLP:journals/pacmmod/KooKKCHKM24,
	author = {Kyoseung Koo and
                  Sohyun Kim and
                  Wonhyeon Kim and
                  Yoojin Choi and
                  Juhee Han and
                  Bogyeong Kim and
                  Bongki Moon},
	title = {PreVision: An Out-of-Core Matrix Computation System with Optimal Buffer
                  Replacement},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {42:1--42:25},
	year = {2024},
	url = {https://doi.org/10.1145/3639297},
	doi = {10.1145/3639297},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KooKKCHKM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale matrix computations have become indispensable in artificial intelligence and scientific applications. It is of paramount importance to efficiently perform out-of-core computations that often entail an excessive amount of disk I/O. Unfortunately, however, most existing systems do not focus on disk I/O aspects and are vulnerable to performance degradation when the scale of input matrices and intermediate data grows large. To address this problem, we present a new out-of-core matrix computation system called PreVision. The PreVision system can achieve optimal buffer replacement by leveraging the deterministic characteristics of data access patterns, and it can also avoid redundant I/O operations by proactively evicting the pages that are no longer referenced. Through extensive evaluations, we demonstrate that PreVision outperforms the existing out-of-core matrix computation systems and significantly reduces disk I/O operations.}
}


@article{DBLP:journals/pacmmod/BleifussPBSN24,
	author = {Tobias Bleifu{\ss} and
                  Thorsten Papenbrock and
                  Thomas Bl{\"{a}}sius and
                  Martin Schirneck and
                  Felix Naumann},
	title = {Discovering Functional Dependencies through Hitting Set Enumeration},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {43:1--43:24},
	year = {2024},
	url = {https://doi.org/10.1145/3639298},
	doi = {10.1145/3639298},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BleifussPBSN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Functional dependencies (FDs) are among the most important integrity constraints in databases. They serve to normalize datasets and thus resolve redundancies, they contribute to query optimization, and they are frequently used to guide data cleaning efforts. Because the FDs of a particular dataset are usually unknown, automatic profiling algorithms are needed to discover them. These algorithms have made considerable advances in the past few years, but they still require a significant amount of time and memory to process datasets of practically relevant sizes. We present FDHits, a novel FD discovery algorithm that finds all valid, minimal FDs in a given relational dataset. FDHits is based on several discovery optimizations that include a hybrid validation approach, effective hitting set enumeration techniques, one-pass candidate validations, and parallelization. Our experiments show that FDHits, even without parallel execution, has a median speedup of 8.1 compared to state-of-the-art FD discovery algorithms while using significantly less memory. This allows the discovery of all FDs even on datasets that could not be processed by the current state-of-the-art.}
}


@article{DBLP:journals/pacmmod/SchwabeA24,
	author = {Tim Schwabe and
                  Maribel Acosta},
	title = {Cardinality Estimation over Knowledge Graphs with Embeddings and Graph
                  Neural Networks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {44:1--44:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639299},
	doi = {10.1145/3639299},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SchwabeA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality Estimation over Knowledge Graphs (KG) is crucial for query optimization, yet remains a challenging task due to the semi-structured nature and complex correlations of data in typical KGs. In this work, we propose GNCE, a novel approach that leverages knowledge graph embeddings and Graph Neural Networks (GNN) to accurately predict the cardinality of conjunctive queries over KGs. GNCE first creates semantically meaningful embeddings for all entities in the KG, which are then used to learn a representation of a query using a GNN to estimate the cardinality of the query. We evaluate GNCE on several KGs in terms of q-Error and demonstrate that it outperforms state-of-the-art approaches based on sampling, summaries, and (machine) learning in terms of estimation accuracy while also having a low execution time and few parameters. Additionally, we show that GNCE performs similarly well on real-world queries and can inductively generalize to unseen entities, making it suitable for use in dynamic query processing scenarios. Our proposed approach has the potential to significantly improve query optimization and related applications that rely on accurate cardinality estimates of conjunctive queries.}
}


@article{DBLP:journals/pacmmod/KimLKH24,
	author = {Kyoungmin Kim and
                  Sangoh Lee and
                  Injung Kim and
                  Wook{-}Shin Han},
	title = {{ASM:} Harmonizing Autoregressive Model, Sampling, and Multi-dimensional
                  Statistics Merging for Cardinality Estimation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {45:1--45:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639300},
	doi = {10.1145/3639300},
	timestamp = {Sat, 24 Aug 2024 12:34:36 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KimLKH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent efforts in learned cardinality estimation (CE) have substantially improved estimation accuracy and query plans inside query optimizers. However, achieving decent efficiency, scalability, and the support of a wide range of queries at the same time, has remained questionable. Rather than falling back to traditional approaches to trade off one criterion with another, we present a new learned approach that achieves all these. Our method, called ASM, harmonizes autoregressive models for per-table statistics estimation, sampling for merging these statistics for join queries, and multi-dimensional statistics merging that extends the sampling for estimating thousands of sub-queries, without assuming independence between join keys. Extensive experiments show that ASM significantly improves query plans under a similar or smaller overhead than the previous learned methods and supports a wider range of queries.}
}


@article{DBLP:journals/pacmmod/LinM24,
	author = {Yiming Lin and
                  Sharad Mehrotra},
	title = {{PLAQUE:} Automated Predicate Learning at Query Time},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {46:1--46:25},
	year = {2024},
	url = {https://doi.org/10.1145/3639301},
	doi = {10.1145/3639301},
	timestamp = {Thu, 17 Oct 2024 14:48:26 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LinM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicate pushing down is a key optimization used to speed up query processing. Much of the existing practice is restricted to pushing predicates explicitly listed in the query. In this paper, we consider the challenge of learning predicates during query execution which are then exploited to accelerate execution. Prior related approaches with a similar goal are restricted (e.g., learn only from only join columns or from specific data statistics). We significantly expand the realm of predicates that can be learned from different query operators (aggregations, joins, grouping, etc.) and develop a system, entitled PLAQUE, that learns such predicates during query execution. Comprehensive evaluations on both synthetic and real datasets demonstrate that the learned predicate approach adopted by PLAQUE can significantly accelerate query execution by up to 33x, and this improvement increases to up to 100x when User-Defined Functions (UDFs) are utilized in queries.}
}


@article{DBLP:journals/pacmmod/ChatterjeePKI24,
	author = {Subarna Chatterjee and
                  Mark F. Pekala and
                  Lev Kruglyak and
                  Stratos Idreos},
	title = {Limousine: Blending Learned and Classical Indexes to Self-Design Larger-than-Memory
                  Cloud Storage Engines},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {47:1--47:28},
	year = {2024},
	url = {https://doi.org/10.1145/3639302},
	doi = {10.1145/3639302},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChatterjeePKI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present Limousine, a self-designing key-value storage engine, that can automatically morph to the near-optimal storage engine architecture shape given a workload, a cloud budget, and target performance. At its core, Limousine identifies the fundamental design principles of storage engines as combinations of learned and classical data structures that collaborate through algorithms for data storage and access. By unifying these principles over diverse hardware and three major cloud providers (AWS, GCP, and Azure), Limousine creates a massive design space of quindecillion (1048) storage engine designs the vast majority of which do not exist in literature or industry. Limousine contains a distribution-aware IO model to accurately evaluate any candidate design. Using these models, Limousine searches within the exhaustive design space to construct a navigable continuum of designs connected along a Pareto frontier of cloud cost and performance. If storage engines contain learned components, Limousine also introduces efficient lazy write algorithms to optimize the holistic read-write performance. Once the near-optimal design is decided for the given context, Limousine automatically materializes the corresponding design in Rust code. Using the YCSB benchmark, we demonstrate that storage engines automatically designed and generated by Limousine scale better by up to 3 orders of magnitude when compared with state-of-the-art industry-leading engines such as RocksDB, WiredTiger, FASTER, and Cosine, over diverse workloads, data sets, and cloud budgets.}
}


@article{DBLP:journals/pacmmod/ZecchiniBSBN24,
	author = {Luca Zecchini and
                  Tobias Bleifu{\ss} and
                  Giovanni Simonini and
                  Sonia Bergamaschi and
                  Felix Naumann},
	title = {Determining the Largest Overlap between Tables},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {48:1--48:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639303},
	doi = {10.1145/3639303},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZecchiniBSBN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Both on the Web and in data lakes, it is possible to detect much redundant data in the form of largely overlapping pairs of tables. In many cases, this overlap is not accidental and provides significant information about the relatedness of the tables. Unfortunately, efficiently quantifying the overlap between two tables is not trivial. In particular, detecting their largest overlap, i.e., their largest common subtable, is a computationally challenging problem. As the information overlap may not occur in contiguous portions of the tables, only the ability to permute columns and rows can reveal it. The detection of the largest overlap can help us in relevant tasks such as the discovery of multiple coexisting versions of the same table, which can present differences in the completeness and correctness of the conveyed information. Automatically detecting these highly similar, matching tables would allow us to guarantee their consistency through data cleaning or change propagation, but also to eliminate redundancy to free up storage space or to save additional work for the editors. We present the first formal definition of this problem, and with it Sloth, our solution to efficiently detect the largest overlap between two tables. We experimentally demonstrate on real-world datasets its efficacy in solving this task, analyzing its performance and showing its impact on multiple use cases.}
}


@article{DBLP:journals/pacmmod/KurmanjiTT24,
	author = {Meghdad Kurmanji and
                  Eleni Triantafillou and
                  Peter Triantafillou},
	title = {Machine Unlearning in Learned Databases: An Experimental Analysis},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {49:1--49:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639304},
	doi = {10.1145/3639304},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KurmanjiTT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning models based on neural networks (NNs) are enjoying ever-increasing attention in the Database (DB) community, both in research and practice. However, an important issue has been largely overlooked, namely the challenge of dealing with the inherent, highly dynamic nature of DBs, where data updates are fundamental, highly-frequent operations (unlike, for instance, in ML classification tasks). Although some recent research has addressed the issues of maintaining updated NN models in the presence of new data insertions, the effects of data deletions (a.k.a., "machine unlearning") remain a blind spot. With this work, for the first time to our knowledge, we pose and answer the following key questions: What is the effect of unlearning algorithms on NN-based DB models? How do these effects translate to effects on key downstream DB tasks, such as cardinality/selectivity estimation (SE), approximate query processing (AQP), data generation (DG), and upstream tasks like data classification (DC)? What metrics should we use to assess the impact and efficacy of unlearning algorithms in learned DBs? Is the problem of (and solutions for) machine unlearning in DBs different from that of machine learning in DBs in the face of data insertions? Is the problem of (and solutions for) machine unlearning for DBs different from unlearning in the ML literature? what are the overhead and efficiency of unlearning algorithms (versus the naive solution of retraining from scratch)? What is the sensitivity of unlearning on batching delete operations (in order to reduce model updating overheads)? If we have a suitable unlearning algorithm (forgetting old knowledge), can we combine it with an algorithm handling data insertions (new knowledge) en route to solving the general adaptability/updatability requirement in learned DBs in the face of both data inserts and deletes? We answer these questions using a comprehensive set of experiments, various unlearning algorithms, a variety of downstream DB tasks (such as SE, AQP, and DG), and an upstream task (DC), each with different NNs, and using a variety of metrics (model-internal, and downstream-task specific) on a variety of real datasets, making this also a first key step towards a benchmark for learned DB unlearning.}
}


@article{DBLP:journals/pacmmod/BrucatoSWNC24,
	author = {Matteo Brucato and
                  Tarique Siddiqui and
                  Wentao Wu and
                  Vivek R. Narasayya and
                  Surajit Chaudhuri},
	title = {Wred: Workload Reduction for Scalable Index Tuning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {50:1--50:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639305},
	doi = {10.1145/3639305},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BrucatoSWNC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern database systems offer index-tuning advisors that automatically identify a set of indexes to improve workload performance. Advisors leverage the optimizer's what-if API to optimize a query for a hypothetical index configuration. Because what-if calls constitute a major bottleneck of index tuning, existing techniques, such as workload compression, help reduce the number of what-if calls to speed up tuning. Unfortunately, even with small workloads and few what-if calls, tuning can still take hours due to the complexity of the queries (e.g., the number of joins, filters, group-by and order-by clauses), which increases their optimization time. This paper introduces workload reduction, a new complementary technique aimed at expediting index tuning by decreasing individual what-if call time without significantly affecting the quality of index tuning. We present an efficient workload reduction algorithm, called Wred, which rewrites each query in the original workload to eliminate column and table expressions unlikely to benefit from indexes, thereby accelerating what-if calls. We study its complexity and ability to maintain high index quality. We perform an extensive evaluation over industry benchmarks and real-world customer workloads, which shows that Wred results in a 3x median speedup in tuning efficiency over an industrial-strength state-of-the-art index advisor, with only a 3.7% median loss in improvement---where improvement is the total workload cost as estimated by the query optimizer---and results in up to 24.7x speedup with 1.8% improvement loss. Furthermore, combining Wred and Isum (a state-of-the-art workload compression technique for index tuning) results in higher speedups than either of the two techniques alone, with 10.5x median speedup and 5% median improvement loss.}
}


@article{DBLP:journals/pacmmod/ZhangLCZZYC24,
	author = {Hailin Zhang and
                  Zirui Liu and
                  Boxuan Chen and
                  Yikai Zhao and
                  Tong Zhao and
                  Tong Yang and
                  Bin Cui},
	title = {{CAFE:} Towards Compact, Adaptive, and Fast Embedding for Large-scale
                  Recommendation Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {51:1--51:28},
	year = {2024},
	url = {https://doi.org/10.1145/3639306},
	doi = {10.1145/3639306},
	timestamp = {Thu, 27 Mar 2025 18:54:49 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangLCZZYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment. Existing embedding compression solutions cannot simultaneously meet three key design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive, and Fast Embedding compression framework that addresses the above requirements. The design philosophy of CAFE is to dynamically allocate more memory resources to important features (called hot features), and allocate less memory to unimportant ones. In CAFE, we propose a fast and lightweight sketch data structure, named HotSketch, to capture feature importance and report hot features in real time. For each reported hot feature, we assign it a unique embedding. For the non-hot features, we allow multiple features to share one embedding by using hash embedding technique. Guided by our design philosophy, we further propose a multi-level hash embedding framework to optimize the embedding tables of non-hot features. We theoretically analyze the accuracy of HotSketch, and analyze the model convergence against deviation. Extensive experiments show that CAFE significantly outperforms existing embedding compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The source codes of CAFE are available at GitHub.}
}


@article{DBLP:journals/pacmmod/SirinI24,
	author = {Utku Sirin and
                  Stratos Idreos},
	title = {The Image Calculator: 10x Faster Image-AI Inference by Replacing {JPEG}
                  with Self-designing Storage Format},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {52:1--52:31},
	year = {2024},
	url = {https://doi.org/10.1145/3639307},
	doi = {10.1145/3639307},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SirinI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Numerous applications today rely on artificial intelligence over images. Image AI is, however, extremely expensive. In particular, the inference cost of image AI dominates the end-to-end cost. We observe that the image storage format lies at the root of the problem. Images today are predominantly stored in JPEG format. JPEG is a storage format designed for the human eye; it maximally compresses images without distorting the components of an image that are visible to the human eye. However, our observation is that during image AI, images are "seen\'\' by algorithms, not humans. In addition, every AI application is different regarding which data components of the images are the most relevant. We present the Image Calculator, a self-designing image storage format that adapts to the given AI task, i.e., the specific neural network, the dataset, and the applications\' specific accuracy, inference time, and storage requirements. Contrary to the state-of-the-art, the Image Calculator does not use a fixed storage format like JPEG. Instead, it designs and constructs a new storage format tailored to the context. It does so by constructing a massive design space of candidate storage formats from first principles, within which it searches efficiently using composite performance models (inference time, accuracy, storage). This way, it leverages the given AI task\'s unique characteristics to compress the data maximally. We evaluate the Image Calculator across a diverse set of data, image analysis tasks, AI models, and hardware. We show that the Image Calculator can generate image storage formats that reduce inference time by up to 14.2x and storage by up to 8.2x with a minimal loss in accuracy or gain, compared to JPEG and its state-of-the-art variants.}
}


@article{DBLP:journals/pacmmod/HuangSACLJZCT24,
	author = {Hanxian Huang and
                  Tarique Siddiqui and
                  Rana Alotaibi and
                  Carlo Curino and
                  Jyoti Leeka and
                  Alekh Jindal and
                  Jishen Zhao and
                  Jes{\'{u}}s Camacho{-}Rodr{\'{\i}}guez and
                  Yuanyuan Tian},
	title = {Sibyl: Forecasting Time-Evolving Query Workloads},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {53:1--53:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639308},
	doi = {10.1145/3639308},
	timestamp = {Mon, 03 Feb 2025 10:38:34 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuangSACLJZCT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database systems often rely on historical query traces to perform workload-based performance tuning. However, real production workloads are time-evolving, making historical queries ineffective for optimizing future workloads. To address this challenge, we propose SIBYL, an end-to-end machine learning-based framework that accurately forecasts a sequence of future queries, with the entire query statements, in various prediction windows. Drawing insights from real-workloads, we propose template-based featurization techniques and develop a stacked-LSTM with an encoder-decoder architecture for accurate forecasting of query workloads. We also develop techniques to improve forecasting accuracy over large prediction windows and achieve high scalability over large workloads with high variability in arrival rates of queries. Finally, we propose techniques to handle workload drifts. Our evaluation on four real workloads demonstrates that SIBYL can forecast workloads with an 87.3% median F1 score, and can result in 1.7× and 1.3× performance improvement when applied to materialized view selection and index selection applications, respectively.}
}


@article{DBLP:journals/pacmmod/AytimurRWCG24,
	author = {Mehmet Aytimur and
                  Silvan Reiner and
                  Leonard W{\"{o}}rteler and
                  Theodoros Chondrogiannis and
                  Michael Grossniklaus},
	title = {{LPLM:} {A} Neural Language Model for Cardinality Estimation of LIKE-Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {54:1--54:25},
	year = {2024},
	url = {https://doi.org/10.1145/3639309},
	doi = {10.1145/3639309},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AytimurRWCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality estimation is an important step in cost-based database query optimization. The accuracy of the estimates directly affects the ability of an optimizer to identify the most efficient query execution plan correctly. In this paper, we study cardinality estimation of LIKE-queries, i.e., queries that use the LIKE-operator to match a pattern with wildcards against string-valued attributes. While both traditional and machine-learning-based approaches have been proposed to tackle this problem, we argue that they all suffer from drawbacks. Most importantly, many state-of-the-art approaches are not designed for patterns that contain wildcards in-between characters. Based on past research on neural language models, we introduce the LIKE-Pattern Language Model (LPLM) that uses a new language and a novel probability distribution function to capture the semantics of general LIKE-patterns. We also propose a method to generate training data for our model. We demonstrate that our method outperforms state-of-the-art approaches in terms of precision (Q-error), while offering comparable runtime performance and memory requirements.}
}


@article{DBLP:journals/pacmmod/BangDCH24,
	author = {Tiemo Bang and
                  Chris Douglas and
                  Natacha Crooks and
                  Joseph M. Hellerstein},
	title = {SkyPIE: {A} Fast {\&} Accurate Oracle for Object Placement},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {55:1--55:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639310},
	doi = {10.1145/3639310},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BangDCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud object stores offer vastly different price points for object storage as a function of workload and geography. Poor object placement can thus lead to significant cost overheads. Prior cost-saving techniques attempt to optimize placement policies on the fly, deciding object placements for each object individually. In practice, these techniques do not scale to the size of the modern cloud. In this work, we leverage the static nature and pay-per-use pricing model of cloud environments to explore a different approach. Rather than computing object placements on the fly, we precompute a SkyPIE oracle---a lookup structure representing all possible placement policies and the workloads for which they are optimal. Internally, SkyPIE represents placement policies as a matrix of cost-hyperplanes, which we effectively precompute through pruning and convex optimization. By leveraging a fast geometric algorithm, online queries then are 1 to 8 orders of magnitude faster but as accurate as Integer-Linear-Programming. This makes exact optimization tractable for real workloads and we show >10x cost savings compared to state-of-the-art heuristic approaches.}
}


@article{DBLP:journals/pacmmod/LuoPXZX24,
	author = {Xuan Luo and
                  Jian Pei and
                  Cheng Xu and
                  Wenjie Zhang and
                  Jianliang Xu},
	title = {Fast Shapley Value Computation in Data Assemblage Tasks as Cooperative
                  Simple Games},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {56:1--56:28},
	year = {2024},
	url = {https://doi.org/10.1145/3639311},
	doi = {10.1145/3639311},
	timestamp = {Fri, 28 Feb 2025 17:23:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuoPXZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we tackle the challenging problem of Shapley value computation in data markets in a novel setting of data assemblage tasks with binary utility functions among data owners. By modeling these scenarios as cooperative simple games, we leverage pivotal probabilities to transform the computation into a problem of counting beneficiaries. Moreover, we make an insightful observation that the Shapley values can be computed using subsets of minimal syntheses within the inclusion-exclusion framework in combinatorics. Based on this insight, we develop a game decomposition approach and utilize techniques in Boolean function decomposition into disjunctive normal form. One interesting property of our method is that the time complexity depends only on the data owners participating in those minimal syntheses, rather than all the data owners. Extensive experiments with real data sets demonstrate a significant efficiency improvement for computing the Shapley values in data assemblage tasks modeled as simple games.}
}


@article{DBLP:journals/pacmmod/ChenC24,
	author = {Yiyuan Chen and
                  Shimin Chen},
	title = {Cabin: {A} Compressed Adaptive Binned Scan Index},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {57:1--57:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639312},
	doi = {10.1145/3639312},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scan is a fundamental operation widely used in main-memory analytical database systems. To accelerate scans, previous studies build either record-order or sort-order structures known as scan indices. While achieving good performance, scan indices often incur significant space overhead, limiting their use in main-memory databases. For example, the most recent and best performing scan index, BinDex, consists of a sort-order position array, which is an array of rowIDs in the value order, and a set of record-order bit vectors, representing records in pre-defined value intervals. The structures can be much larger than the base data column size. In this paper, we propose a novel scan index, Cabin, that exploits the following three techniques for better time-space tradeoff. 1) filter sketches that represent every 2^w-2 value intervals with a w-bit sketched vector, thereby exponentially reducing the space for the bit vectors; 2) selective position array that removes the rowID array for a fraction of intervals in order to lower the space overhead for the position array; and 3) data-aware intervals that judiciously select interval boundaries based on the data characteristics to better support popular values in skewed data distributions or categorical attributes. Experimental results show that compared with state-of-the-art scan solutions, Cabin achieves better time-space tradeoff, and attains 1.70 -- 4.48x improvement for average scan performance given the same space budget.}
}


@article{DBLP:journals/pacmmod/BaziotisKM24,
	author = {Stefanos Baziotis and
                  Daniel D. Kang and
                  Charith Mendis},
	title = {Dias: Dynamic Rewriting of Pandas Code},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {58:1--58:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639313},
	doi = {10.1145/3639313},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BaziotisKM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, dataframe libraries, such as pandas have exploded in popularity. Due to their flexibility, they are increasingly used in ad-hoc exploratory data analysis (EDA) workloads. These workloads are diverse, including custom functions which can span libraries or be written in pure Python. The majority of systems available to accelerate EDA workloads focus on bulk-parallel workloads, which contain vastly different computational patterns, typically within a single library. As a result, they can introduce excessive overheads for ad-hoc EDA workloads due to their expensive optimization techniques. Instead, we identify source-to-source, external program rewriting as a lightweight technique which can optimize across representations, and offer substantial speedups while also avoiding slowdowns. We implemented Dias, which rewrites notebook cells to be more efficient for ad-hoc EDA workloads. We develop techniques for efficient rewrites in Dias, including checking the preconditions under which rewrites are correct, dynamically, at fine-grained program points. We show that Dias can rewrite individual cells to be 57× faster compared to pandas and 1909× faster compared to optimized systems such as modin. Furthermore, Dias can accelerate whole notebooks by up to 3.6× compared to pandas and 27.1× compared to modin.}
}


@article{DBLP:journals/pacmmod/CamachoRodriguezAGGPAFCR24,
	author = {Jes{\'{u}}s Camacho{-}Rodr{\'{\i}}guez and
                  Ashvin Agrawal and
                  Anja Gruenheid and
                  Ashit Gosalia and
                  Cristian Petculescu and
                  Josep Aguilar{-}Saborit and
                  Avrilia Floratou and
                  Carlo Curino and
                  Raghu Ramakrishnan},
	title = {LST-Bench: Benchmarking Log-Structured Tables in the Cloud},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {59:1--59:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639314},
	doi = {10.1145/3639314},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CamachoRodriguezAGGPAFCR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data processing engines increasingly leverage distributed file systems for scalable, cost-effective storage. While the Apache Parquet columnar format has become a popular choice for data storage and retrieval, the immutability of Parquet files renders it impractical to meet the demands of frequent updates in contemporary analytical workloads. Log-Structured Tables (LSTs), such as Delta Lake, Apache Iceberg, and Apache Hudi, offer an alternative for scenarios requiring data mutability, providing a balance between efficient updates and the benefits of columnar storage. They provide features like transactions, time-travel, and schema evolution, enhancing usability and enabling access from multiple engines. Moreover, engines like Apache Spark and Trino can be configured to leverage the optimizations and controls offered by LSTs to meet specific business needs. Conventional benchmarks and tools are inadequate for evaluating the transformative changes in the storage layer resulting from these advancements, as they do not allow us to measure the impact of design and optimization choices in this new setting. In this paper, we propose a novel benchmarking approach and metrics that build upon existing benchmarks, aiming to systematically assess LSTs. We develop a framework, LST-Bench, which facilitates effective exploration and evaluation of the collaborative functioning of LSTs and data processing engines through tailored benchmark packages. A package is a mix of use patterns reflecting a target workload; LST-Bench makes it easy to define a wide range of use patterns and combine them into a package, and we include a baseline package for completeness. Our assessment demonstrates the effectiveness of our framework and benchmark packages in extracting valuable insights across diverse environments. The code for LST-Bench is open source and is available at https://github.com/microsoft/lst-bench/.}
}


@article{DBLP:journals/pacmmod/ZhangLZL24,
	author = {Zhijie Zhang and
                  Yujie Lu and
                  Weiguo Zheng and
                  Xuemin Lin},
	title = {A Comprehensive Survey and Experimental Study of Subgraph Matching:
                  Trends, Unbiasedness, and Interaction},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {60:1--60:29},
	year = {2024},
	url = {https://doi.org/10.1145/3639315},
	doi = {10.1145/3639315},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangLZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph matching is a fundamental problem in graph analysis. In recent years, many subgraph matching algorithms have been proposed, making it pressing and challenging to compare their performance and identify their strengths and weaknesses. We observe that (1) The embedding enumeration in the classic filtering-ordering-enumerating framework dominates the overall performance, and thus enhancing the backtracking paradigm is becoming a current research trend; (2) Simply changing the limitation of output size results in a substantial variation in the ranking of different methods, leading to biased performance evaluation; (3) The techniques employed at different stages of subgraph matching interact with each other, making it less feasible to replace and evaluate a single technique in isolation. Therefore, a comprehensive survey and experimental study of subgraph matching is necessary to identify the current trends, ensure unbiasedness, and investigate the potential interactions. In this paper, we comprehensively review the methods in the current trend and experimentally confirm their advantage over prior approaches. We unbiasedly evaluate the performance of these algorithms by using an effective metric, namely embeddings per second. To fully investigate the interactions between various techniques, we select 10 representative techniques for each stage and evaluate all the feasible combinations.}
}


@article{DBLP:journals/pacmmod/GatterbauerD24,
	author = {Wolfgang Gatterbauer and
                  Cody Dunne},
	title = {On The Reasonable Effectiveness of Relational Diagrams: Explaining
                  Relational Query Patterns and the Pattern Expressiveness of Relational
                  Languages},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {61:1--61:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639316},
	doi = {10.1145/3639316},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GatterbauerD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Comparing relational languages by their logical expressiveness is well understood. Less well understood is how to compare relational languages by their ability to represent relational query patterns. Indeed, what are query patterns other than "a certain way of writing a query"? And how can query patterns be defined across procedural and declarative languages, irrespective of their syntax? To the best of our knowledge, we provide the first semantic definition of relational query patterns by using a variant of structure-preserving mappings between the relational tables of queries. This formalism allows us to analyze the relative pattern expressiveness of relational language fragments and create a hierarchy of languages with equal logical expressiveness yet different pattern expressiveness. Notably, for the non-disjunctive language fragment, we show that relational calculus can express a larger class of patterns than the basic operators of relational algebra. Our language-independent definition of query patterns opens novel paths for assisting database users. For example, these patterns could be leveraged to create visual query representations that faithfully represent query patterns, speed up interpretation, and provide visual feedback during query editing. As a concrete example, we propose Relational Diagrams, a complete and sound diagrammatic representation of safe relational calculus that is provably (i) unambiguous, (ii) relationally complete, and (iii) able to represent all query patterns for unions of non-disjunctive queries. Among all diagrammatic representations for relational queries that we are aware of, ours is the only one with these three properties. Furthermore, our anonymously preregistered user study shows that Relational Diagrams allow users to recognize patterns meaningfully faster and more accurately than SQL.}
}


@article{DBLP:journals/pacmmod/LiangCMIL24,
	author = {Jiaming Liang and
                  Lei Cao and
                  Samuel Madden and
                  Zack Ives and
                  Guoliang Li},
	title = {{RITA:} Group Attention is All You Need for Timeseries Analytics},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {62:1--62:28},
	year = {2024},
	url = {https://doi.org/10.1145/3639317},
	doi = {10.1145/3639317},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiangCMIL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Timeseries analytics is important in many real-world applications. Recently, the Transformer model, popular in natural language processing, has been leveraged to learn high quality feature embeddings from timeseries: embeddings are key to the performance of various timeseries analytics tasks such as similarity-based timeseries queries within vector databases. However, quadratic time and space complexities limit Transformers' scalability, especially for long timeseries. To address these issues, we develop a timeseries analytics tool, RITA, which uses a novel attention mechanism, named group attention, to address this scalability issue. Group attention dynamically clusters the objects based on their similarity into a small number of groups and approximately computes the attention at the coarse group granularity. It thus significantly reduces the time and space complexity, yet provides a theoretical guarantee on the quality of the computed attention. The dynamic scheduler of RITA continuously adapts the number of groups and the batch size in the training process, ensuring group attention always uses the fewest groups needed to meet the approximation quality requirement. Extensive experiments on various timeseries datasets and analytics tasks demonstrate that RITA outperforms the state-of-the-art in accuracy and is significantly faster --- with speedups of up to 63X.}
}


@article{DBLP:journals/pacmmod/ChangY24,
	author = {Lijun Chang and
                  Kai Yao},
	title = {Maximum k-Plex Computation: Theory and Practice},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {63:1--63:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639318},
	doi = {10.1145/3639318},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChangY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The k-plex model relaxes the clique model by allowing each vertex to miss up to k neighbors, including the vertex itself. A 1-plex is a clique. Many exact algorithms have been recently designed for finding the k-plex with the largest number of vertices, known as the maximum k-plex computation problem. However, all the existing algorithms, except BS, has the trivial worst-case time complexity of O*(2n) when ignoring polynomial factors. On the other hand, although BS improves the time complexity to O*(βkn) where βk < 2 is a constant depending only on k, its practical performance is not satisfactory. In this paper, we study the maximum k-plex computation problem from both theory and practice. We first propose two new reduction rules and a new branching rule and prove that the base of the exponential time complexity is reduced to γk when the new reduction and branching rules are incorporated into a standard backtracking algorithm; here γk < βk. We then design a two-stage approach kPlexT to improve the exponent of the time complexity by separating the search of large k-plexes from the search of small ones. We prove that kPlexT runs in O*((α Δ)k+1 γ_kα) time when the maximum k-plex size Ωk(G) is at least 2k-1, and in O*((α Δ)k+1 γ_kα + min(γkn, n2k-2)) time otherwise; here, α is the degeneracy and Δ is the maximum degree of the input graph. We also prove that with slight modification, kPlexT runs in O*((αΔ)k+1 (k+1)α+k-Ωk(G)) time when ømega_k(G) ≥ 2k-1. Finally, we propose another reduction rule and a better initialization method to improve the practical performance of kPlexT. Extensive empirical studies demonstrate that kPlexT achieves state-of-the-art practical performance. We also show that our improved time complexity carries over to other related problems such as enumerating all maximal k-plexes, quasi-cliques, and k-biplexes.}
}


@article{DBLP:journals/pacmmod/HuWTSDYC24,
	author = {Gansen Hu and
                  Zhaoguo Wang and
                  Chuzhe Tang and
                  Jiahuan Shen and
                  Zhiyuan Dong and
                  Sheng Yao and
                  Haibo Chen},
	title = {WeBridge: Synthesizing Stored Procedures for Large-Scale Real-World
                  Web Applications},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {64:1--64:29},
	year = {2024},
	url = {https://doi.org/10.1145/3639319},
	doi = {10.1145/3639319},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuWTSDYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern web applications use databases to store their data. When processing user requests, these applications retrieve and store data in the database server, which incurs network round trips. These round trips significantly increase the application's latency. Previous approaches have attempted to reduce these round trips by prefetching query results or batching database accesses. However, neither method can efficiently reduce the latency when some queries depend on previous queries' results. In real-world applications, nearly 50% of the queries depend on the result of other queries. This paper presents WeBridge, the first system capable of synthesizing stored procedures for large-scale real-world web applications. First, WeBridge employs concolic execution technique to analyze the applications and generate stored procedures for hot program paths. Then, it seamlessly integrates the stored procedures into the application by extending the database access library. Finally, it improves the efficiency of the stored procedures with speculative execution. Evaluation using real-world web applications and workloads show that WeBridge achieves up to 79.8% median latency reduction and up to 2× peak throughput.}
}


@article{DBLP:journals/pacmmod/LiuZZ24,
	author = {Yihao Liu and
                  Xinyu Zeng and
                  Huanchen Zhang},
	title = {LeCo: Lightweight Compression via Learning Serial Correlations},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {65:1--65:28},
	year = {2024},
	url = {https://doi.org/10.1145/3639320},
	doi = {10.1145/3639320},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Lightweight data compression is a key technique that allows column stores to exhibit superior performance for analytical queries. Despite a comprehensive study on dictionary-based encodings to approach Shannon's entropy, few prior works have systematically exploited the serial correlation in a column for compression. In this paper, we propose LeCo (i.e., Learned Compression), a framework that uses machine learning to remove the serial redundancy in a value sequence automatically to achieve an outstanding compression ratio and decompression performance. LeCo presents a general approach to this end, making existing algorithms such as Frame-of-Reference (FOR), Delta Encoding, and Run-Length Encoding (RLE) special cases under our framework. Our microbenchmark with three synthetic and eight real-world data sets shows that a prototype of LeCo achieves a Pareto improvement on both compression ratio and random access speed over the existing solutions. When integrating LeCo into widely-used applications, we observe up to 5.2× speed up in a data analytical query in the Arrow columnar execution engine, and a 16% increase in RocksDB's throughput.}
}


@article{DBLP:journals/pacmmod/TsanDIR24,
	author = {Brian Tsan and
                  Asoke Datta and
                  Yesdaulet Izenov and
                  Florin Rusu},
	title = {Approximate Sketches},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {66:1--66:24},
	year = {2024},
	url = {https://doi.org/10.1145/3639321},
	doi = {10.1145/3639321},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/TsanDIR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sketches are single-pass small-space data summaries that can quickly estimate the cardinality of join queries. However, sketches are not directly applicable to join queries with dynamic filter conditions --- where arbitrary selection predicate(s) are applied --- since a sketch is limited to a fixed selection. While multiple sketches for various selections can be used in combination, they each incur individual storage and maintenance costs. Alternatively, exact sketches can be built during runtime for every selection. To make this process scale, a high-degree of parallelism --- available in hardware accelerators such as GPUs --- is required. Therefore, sketch usage for cardinality estimation in query optimization is limited. Following recent work that applies transformers to cardinality estimation, we design a novel learning-based method to approximate the sketch of any arbitrary selection, enabling sketches for join queries with filter conditions. We train a transformer on each table to estimate the sketch of any subset of the table, i.e., any arbitrary selection. Transformers achieve this by learning the joint distribution amongst table attributes, which is equivalent to a multidimensional sketch. Subsequently, transformers can approximate any sketch, enabling sketches for join cardinality estimation. In turn, estimating joins via approximate sketches allows tables to be modeled individually and thus scales linearly with the number of tables. We evaluate the accuracy and efficacy of approximate sketches on queries with selection predicates consisting of conjunctions of point and range conditions. Approximate sketches achieve similar accuracy to exact sketches with at least one order of magnitude less overhead.}
}


@article{DBLP:journals/pacmmod/LeeHHPPHL24,
	author = {Wonseok Lee and
                  Jaehyun Ha and
                  Wook{-}Shin Han and
                  Changgyoo Park and
                  Myunggon Park and
                  Juhyeng Han and
                  Juchang Lee},
	title = {DoppelGanger++: Towards Fast Dependency Graph Generation for Database
                  Replay},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {67:1--67:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639322},
	doi = {10.1145/3639322},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LeeHHPPHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A database replay system (DRS) captures workloads on a production system and then replays them in a test system to test various system changes, avoiding any risk before realizing them in production. The dependency graph generation in a DRS is crucial in preserving output determinism while maximizing concurrency. The state-of-the-art dependency graph generation algorithm deployed in a commercial DBMS uses a generate-and-prune strategy. It first generates a dependency graph by performing backward scans for each request in a workload. It then prunes all redundant edges using an expensive, transitive reduction algorithm. However, we notice that this generates a large dependency graph that contains many redundant edges and its worst-case time complexity is quadratic to the number of requests in a workload. In order to solve these challenging problems, we formally propose four classes of dependency graphs for DRSs. We then present a stateful single forward scan algorithm, SSFS, to generate any class of dependency graphs by performing a single scan over all requests while succinctly maintaining states. Here, states refer to information that is stored and maintained for efficient dependency graph generation. We also propose the parallel SSFS to utilize the computation power with multi-core CPUs while balancing the loads. We implemented our DRS in a leading commercial DBMS. Extensive experiments using the TPC-C, SD benchmarks, and a real-world customer workload show that our DRS significantly improves the dependency graph generation time by up to two orders of magnitude, compared to the state-of-the-art.}
}


@article{DBLP:journals/pacmmod/FangSWC24,
	author = {Jingzhi Fang and
                  Yanyan Shen and
                  Yue Wang and
                  Lei Chen},
	title = {STile: Searching Hybrid Sparse Formats for Sparse Deep Learning Operators
                  Automatically},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {68:1--68:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639323},
	doi = {10.1145/3639323},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FangSWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparse operators, i.e., operators that take sparse tensors as input, are of great importance in deep learning models. Due to the diverse sparsity patterns in different sparse tensors, it is challenging to optimize sparse operators by seeking an optimal sparse format, i.e., leading to the lowest operator latency. Existing works propose to decompose a sparse tensor into several parts and search for a hybrid of sparse formats to handle diverse sparse patterns. However, they often make a trade-off between search space and search time: their search spaces are limited in some cases, resulting in limited operator running efficiency they can achieve. In this paper, we try to extend the search space in its breadth (by doing flexible sparse tensor transformations) and depth (by enabling multi-level decomposition). We formally define the multi-level sparse format decomposition problem, which is NP-hard, and we propose a framework STile for it. To search efficiently, a greedy algorithm is used, which is guided by a cost model about the latency of computing a sub-task of the original operator after decomposing the sparse tensor. Experiments of two common kinds of sparse operators, SpMM and SDDMM, are conducted on various sparsity patterns, and we achieve 2.1-18.0× speedup against cuSPARSE on SpMMs and 1.5 - 6.9× speedup against DGL on SDDMM. The search time is less than one hour for any tested sparse operator, which can be amortized.}
}


@article{DBLP:journals/pacmmod/ZuoQZLD24,
	author = {Chaoji Zuo and
                  Miao Qiao and
                  Wenchao Zhou and
                  Feifei Li and
                  Dong Deng},
	title = {SeRF: Segment Graph for Range-Filtering Approximate Nearest Neighbor
                  Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {69:1--69:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639324},
	doi = {10.1145/3639324},
	timestamp = {Fri, 17 May 2024 21:40:31 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZuoQZLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Effective vector representation models, e.g., word2vec and node2vec, embed real-world objects such as images and documents in high dimensional vector space. In the meanwhile, the objects are often associated with attributes such as timestamps and prices. Many scenarios need to jointly query the vector representations of the objects together with their attributes. These queries can be formalized as range-filtering approximate nearest neighbor search (ANNS) queries. Specifically, given a collection of data vectors, each associated with an attribute value whose domain has a total order. The range-filtering ANNS consists of a query range and a query vector. It finds the approximate nearest neighbors of the query vector among all the data vectors whose attribute values fall in the query range. Existing approaches suffer from a rapidly degrading query performance when the query range width shifts. The query performance can be optimized by a solution that builds an ANNS index for every possible query range; however, the index time and index size become prohibitive -- the number of query ranges is quadratic to the number n of data vectors. To overcome these challenges, for the query range contains all attribute values smaller than a user-provided threshold, we design a structure called the segment graph whose index time and size are the same as a single ANNS index, yet can losslessly compress the n ANNS indexes, reducing the indexing cost by a factor of Ω(n). To handle general range queries, we propose a 2D segment graph with average-case index size O(n log n) to compress n segment graphs, breaking the quadratic barrier. Extensive experiments conducted on real-world datasets show that our proposed structures outperformed existing methods significantly; our index also exhibits superior scalability.}
}


@article{DBLP:journals/pacmmod/PeriniN24,
	author = {Massimo Perini and
                  Milos Nikolic},
	title = {In-Database Data Imputation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {70:1--70:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639326},
	doi = {10.1145/3639326},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/PeriniN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Missing data is a widespread problem in many domains, creating challenges in data analysis and decision making. Traditional techniques for dealing with missing data, such as excluding incomplete records or imputing simple estimates (e.g., mean), are computationally efficient but may introduce bias and disrupt variable relationships, leading to inaccurate analyses. Model-based imputation techniques offer a more robust solution that preserves the variability and relationships in the data, but they demand significantly more computation time, limiting their applicability to small datasets. This work enables efficient, high-quality, and scalable data imputation within a database system using the widely used MICE method. We adapt this method to exploit computation sharing and a ring abstraction for faster model training. To impute both continuous and categorical values, we develop techniques for in-database learning of stochastic linear regression and Gaussian discriminant analysis models. Our MICE implementations in PostgreSQL and DuckDB outperform alternative MICE implementations and model-based imputation techniques by up to two orders of magnitude in terms of computation time, while maintaining high imputation quality.}
}


@article{DBLP:journals/pacmmod/YoungmannCGR24,
	author = {Brit Youngmann and
                  Michael J. Cafarella and
                  Amir Gilad and
                  Sudeepa Roy},
	title = {Summarized Causal Explanations For Aggregate Views},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {71:1--71:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639328},
	doi = {10.1145/3639328},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YoungmannCGR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {SQL queries with group-by and average are frequently used and plotted as bar charts in several data analysis applications. Understanding the reasons behind the results in such an aggregate view may be a highly nontrivial and time-consuming task, especially for large datasets with multiple attributes. Hence, generating automated explanations for aggregate views can allow users to gain better insights into the results while saving time in data analysis. When providing explanations for such views, it is paramount to ensure that they are succinct yet comprehensive, reveal different types of insights that hold for different aggregate answers in the view, and, most importantly, they reflect reality and arm users to make informed data-driven decisions, i.e., the explanations do not only consider correlations but are causal. In this paper, we present CauSumX, a framework for generating summarized causal explanations for the entire aggregate view. Using background knowledge captured in a causal DAG, CauSumX finds the most effective causal treatments for different groups in the view. We formally define the framework and the optimization problem, study its complexity, and devise an efficient algorithm using the Apriori algorithm, LP rounding, and several optimizations. We experimentally show that our system generates useful summarized causal explanations compared to prior work and scales well for large high-dimensional data.}
}


@article{DBLP:journals/pacmmod/CopulFMR24,
	author = {Roni Copul and
                  Nave Frost and
                  Tova Milo and
                  Kathy Razmadze},
	title = {TabEE: Tabular Embeddings Explanations},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {1},
	pages = {72:1--72:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639329},
	doi = {10.1145/3639329},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CopulFMR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tabular embedding methods have become increasingly popular due to their effectiveness in improving the results of various tasks, including classic databases tasks and machine learning predictions. However, most current methods treat these embedding models as "black boxes" making it difficult to understand the insights captured by the models. Our research proposes a novel approach to interpret these models, aiming to provide local and global explanations for the original data and detect potential flaws in the embedding models. The proposed solution is appropriate for every tabular embedding algorithm, as it fits the black box view of the embedding model. Furthermore, we propose methods for comparing different embedding models, which can help identify data biases that might impact the models\' credibility without the user\'s knowledge. Our approach is evaluated on multiple datasets and multiple embeddings, demonstrating that our proposed explanations provide valuable insights into the behavior of tabular embedding methods. By making these models more transparent, we believe our research will contribute to the development of more effective and reliable embedding methods for a wide range of applications.}
}


@article{DBLP:journals/pacmmod/GeertsMN24,
	author = {Floris Geerts and
                  Wim Martens and
                  Matthias Niewerth},
	title = {{PACMMOD} Volume 2 Issue 2: Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {73},
	year = {2024},
	url = {https://doi.org/10.1145/3651136},
	doi = {10.1145/3651136},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GeertsMN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We are excited to announce the first issue dedicated to the PODS research track of the Proceedings of the ACM on Management of Data, or PACMMOD, journal. In its current form, this new journal hosts a SIGMOD and a PODS research track. The PODS research track aims to provide a solid scientific basis for methods, techniques, and solutions for the data management challenges that continually arise in our data-driven society. Articles for the PODS track of PACMMOD present principled contributions to modeling, application, system building, and both theoretical and experimental validation in the context of data management. Such articles might be based, among others, on establishing theoretical results, developing new concepts and frameworks that deserve further exploration, providing experimental work that sheds light on the scientific foundations of the discipline, or a rigorous analysis of both widely used and recently developed industry artifacts. At a time when computer science is increasingly data centric, it is essential to promote an active exchange of tools and techniques between principles of database systems and other communities focused on data management. The PODS track thus pays special attention to those papers that help in the urgent process of integrating data management techniques within broader computer science. Articles published in this track will be invited for presentation to the ACM Symposium on Principles of Database Systems (PODS), which is held jointly with SIGMOD each year.}
}


@article{DBLP:journals/pacmmod/PadmanabhaSS24,
	author = {Anantha Padmanabha and
                  Luc Segoufin and
                  Cristina Sirangelo},
	title = {A Dichotomy in the Complexity of Consistent Query Answering for Two
                  Atom Queries With Self-Join},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {74},
	year = {2024},
	url = {https://doi.org/10.1145/3651137},
	doi = {10.1145/3651137},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PadmanabhaSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the dichotomy conjecture for consistent query answering under primary key constraints. It states that, for every fixed Boolean conjunctive query q, testing whether q is certain (i.e. whether it evaluates to true over all repairs of a given inconsistent database) is either PTime or CoNP-complete. This conjecture has been verified for self-join-free and path queries. We show that it also holds for queries with two atoms.}
}


@article{DBLP:journals/pacmmod/ZhaoFOK24,
	author = {Hangdong Zhao and
                  Austen Z. Fan and
                  Xiating Ouyang and
                  Paraschos Koutris},
	title = {Conjunctive Queries with Negation and Aggregation: {A} Linear Time
                  Characterization},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {75},
	year = {2024},
	url = {https://doi.org/10.1145/3651138},
	doi = {10.1145/3651138},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhaoFOK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the complexity of evaluating Conjunctive Queries with negation (\\cqneg). First, we present an algorithm with linear preprocessing time and constant delay enumeration for a class of CQs with negation called free-connex signed-acyclic queries. We show that no other queries admit such an algorithm subject to lower-bound conjectures. Second, we extend our algorithm to Conjunctive Queries with negation and aggregation over a general semiring, which we call Functional Aggregate Queries with negation (\\faqneg). Such an algorithm achieves constant delay enumeration for the same class of queries but with a slightly increased preprocessing time, which includes an inverse Ackermann function. We show that this surprising appearance of the Ackermmann function is probably unavoidable for general semirings but can be removed when the semiring has a specific structure. Finally, we show an application of our results to computing the difference of CQs.}
}


@article{DBLP:journals/pacmmod/KoutrisOW24,
	author = {Paraschos Koutris and
                  Xiating Ouyang and
                  Jef Wijsen},
	title = {Consistent Query Answering for Primary Keys on Rooted Tree Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {76},
	year = {2024},
	url = {https://doi.org/10.1145/3651139},
	doi = {10.1145/3651139},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KoutrisOW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the data complexity of consistent query answering (CQA) on databases that may violate the primary key constraints. A repair is a maximal subset of the database satisfying the primary key constraints. For a Boolean query q, the problem fCERTAINTY(q) takes a database as input, and asks whether or not each repair satisfies q. The computational complexity of fCERTAINTY(q) has been established whenever q is a self-join-free Boolean conjunctive query, or a (not necessarily self-join-free) Boolean path query. In this paper, we take one more step towards a general classification for all Boolean conjunctive queries by considering the class of rooted tree queries. In particular, we show that for every rooted tree query q, fCERTAINTY(q) is in FO, NL-hard ∩ LPFL, or coNP-complete, and it is decidable (in polynomial time), given q, which of the three cases applies. We also extend our classification to larger classes of queries with simple primary keys. Our classification criteria rely on query homomorphisms and our polynomial-time fixpoint algorithm is based on a novel use of context-free grammar (CFG).}
}


@article{DBLP:journals/pacmmod/Gutierrez-Basulto24,
	author = {V{\'{\i}}ctor Guti{\'{e}}rrez{-}Basulto and
                  Albert Gutowski and
                  Yazm{\'{\i}}n Ang{\'{e}}lica Ib{\'{a}}{\~{n}}ez{-}Garc{\'{\i}}a and
                  Filip Murlak},
	title = {Containment of Graph Queries Modulo Schema},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {77},
	year = {2024},
	url = {https://doi.org/10.1145/3651140},
	doi = {10.1145/3651140},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Gutierrez-Basulto24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With multiple graph database systems on the market and a new Graph Query Language standard on the horizon, it is time to revisit some classic static analysis problems. Query containment, arguably the workhorse of static analysis, has already received a lot of attention in the context of graph databases, but not so in the presence of schemas. We aim to change this. Because there is no universal agreement yet on what graph schemas should be, we rely on an abstract formalism borrowed from the knowledge representation community: we assume that schemas are expressed in a description logic (DL). We identify a suitable DL that capture both basic constraints on the labels of incident nodes and edges, and more refined schema features such as participation, cardinality, and unary key constraints. Basing upon, and extending, the rich body of work on DLs, we solve the containment modulo schema problem for unions of conjunctive regular path queries (UCRPQs) and schemas whose descriptions do not mix inverses and counting. For two-way UCRPQs (UC2RPQs) we solve the problem under additional assumptions that tend to hold in practice: we restrict the use of concatenation in queries and participation constraints in schemas.}
}


@article{DBLP:journals/pacmmod/LohreyS24,
	author = {Markus Lohrey and
                  Markus L. Schmid},
	title = {Enumeration for MSO-Queries on Compressed Trees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {78},
	year = {2024},
	url = {https://doi.org/10.1145/3651141},
	doi = {10.1145/3651141},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LohreyS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a linear preprocessing and output-linear delay enumeration algorithm for MSO-queries over trees that are compressed in the well-established grammar-based framework. Time bounds are measured with respect to the size of the compressed representation of the tree. Our result extends previous work on the enumeration of MSO-queries over uncompressed trees and on the enumeration of document spanners over compressed text documents.}
}


@article{DBLP:journals/pacmmod/0002OS24,
	author = {Ahmet Kara and
                  Dan Olteanu and
                  Dan Suciu},
	title = {From Shapley Value to Model Counting and Back},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {79},
	year = {2024},
	url = {https://doi.org/10.1145/3651142},
	doi = {10.1145/3651142},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0002OS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we investigate the problem of quantifying the contribution of each variable to the satisfying assignments of a Boolean function based on the Shapley value. Our main result is a polynomial-time equivalence between computing Shapley values and model counting for any class of Boolean functions that are closed under substitutions of variables with disjunctions of fresh variables. This result settles an open problem raised in prior work, which sought to connect the Shapley value computation to probabilistic query evaluation. We show two applications of our result. First, the Shapley values can be computed in polynomial time over deterministic and decomposable circuits, since they are closed under OR-substitutions. Second, there is a polynomial-time equivalence between computing the Shapley value for the tuples contributing to the answer of a Boolean conjunctive query and counting the models in the lineage of the query. This equivalence allows us to immediately recover the dichotomy for Shapley value computation in case of self-join-free Boolean conjunctive queries; in particular, the hardness for non-hierarchical queries can now be shown using a simple reduction from the \\#P-hard problem of model counting for lineage in positive bipartite disjunctive normal form.}
}


@article{DBLP:journals/pacmmod/ThompsonF24,
	author = {Sam M. Thompson and
                  Dominik D. Freydenberger},
	title = {Generalized Core Spanner Inexpressibility via Ehrenfeucht-Fra{\"{\i}}ss{\'{e}}
                  Games for {FC}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {80},
	year = {2024},
	url = {https://doi.org/10.1145/3651143},
	doi = {10.1145/3651143},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ThompsonF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite considerable research on document spanners, little is known about the expressive power of generalized core spanners. In this paper, we use Ehrenfeucht-Fraïssé games to obtain general inexpressibility lemmas for the logic FC (a finite model variant of the theory of concatenation). Applying these lemmas give inexpressibility results for FC that we lift to generalized core spanners. In particular, we give several relations that cannot be selected by generalized core spanners, thus demonstrating the effectiveness of the inexpressibility lemmas. As an immediate consequence, we also gain new insights into the expressive power of core spanners.}
}


@article{DBLP:journals/pacmmod/AgarwalHSY24,
	author = {Pankaj K. Agarwal and
                  Xiao Hu and
                  Stavros Sintos and
                  Jun Yang},
	title = {On Reporting Durable Patterns in Temporal Proximity Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {81},
	year = {2024},
	url = {https://doi.org/10.1145/3651144},
	doi = {10.1145/3651144},
	timestamp = {Tue, 06 Aug 2024 08:17:52 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgarwalHSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding patterns in graphs is a fundamental problem in databases and data mining. In many applications, graphs are temporal and evolve over time, so we are interested in finding durable patterns, such as triangles and paths, which persist over a long time. While there has been work on finding durable simple patterns, existing algorithms do not have provable guarantees and run in strictly super-linear time. The paper leverages the observation that many graphs arising in practice are naturally proximity graphs or can be approximated as such, where nodes are embedded as points in some high-dimensional space, and two nodes are connected by an edge if they are close to each other. We work with an implicit representation of the proximity graph, where nodes are additionally annotated by time intervals, and design near-linear-time algorithms for finding (approximately) durable patterns above a given durability threshold. We also consider an interactive setting where a client experiments with different durability thresholds in a sequence of queries; we show how to compute incremental changes to result patterns efficiently in time near-linear to the size of the changes.}
}


@article{DBLP:journals/pacmmod/JayaramWZ24,
	author = {Rajesh Jayaram and
                  David P. Woodruff and
                  Samson Zhou},
	title = {Streaming Algorithms with Few State Changes},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {82},
	year = {2024},
	url = {https://doi.org/10.1145/3651145},
	doi = {10.1145/3651145},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/JayaramWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study streaming algorithms that minimize the number of changes made to their internal state (i.e., memory contents). While the design of streaming algorithms typically focuses on minimizing space and update time, these metrics fail to capture the asymmetric costs, inherent in modern hardware and database systems, of reading versus writing to memory. In fact, most streaming algorithms write to their memory on every update, which is undesirable when writing is significantly more expensive than reading. This raises the question of whether streaming algorithms with small space and number of memory writes are possible. We first demonstrate that, for the fundamental F p  moment estimation problem with p ≥ 1, any streaming algorithm that achieves a constant factor approximation must make Ω(n 1-1/p ) internal state changes, regardless of how much space it uses. Perhaps surprisingly, we show that this lower bound can be matched by an algorithm which also has near-optimal space complexity. Specifically, we give a (1+ε)-approximation algorithm for F p  moment estimation that use a near-optimal ~O ε (n 1-1/p ) number of state changes, while simultaneously achieving near-optimal space, i.e., for p∈[1,2), our algorithm uses poly(log n,1/ε) bits of space for, while for p>2, the algorithm uses ~O ε (n 1-1/p ) space. We similarly design streaming algorithms that are simultaneously near-optimal in both space complexity and the number of state changes for the heavy-hitters problem, sparse support recovery, and entropy estimation. Our results demonstrate that an optimal number of state changes can be achieved without sacrificing space complexity.}
}


@article{DBLP:journals/pacmmod/CalauttiLPS24,
	author = {Marco Calautti and
                  Ester Livshits and
                  Andreas Pieris and
                  Markus Schneider},
	title = {The Complexity of Why-Provenance for Datalog Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {83},
	year = {2024},
	url = {https://doi.org/10.1145/3651146},
	doi = {10.1145/3651146},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CalauttiLPS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Datalog is a powerful rule-based language that allows us to express complex recursive queries and has found numerous applications over the years. Explaining why a result to a Datalog query is obtained is an essential task towards explainable and transparent data-intensive applications that rely on Datalog. A standard way of explaining a query result is the so-called why-provenance, which provides information about the witnesses to a query result in the form of subsets of the input database that as a whole can be used to derive that result. To our surprise, despite the fact that the notion of why-provenance for Datalog queries has been around for decades and intensively studied, its computational complexity remains unexplored. Our goal is to fill this gap in the why-provenance literature. Towards this end, we pinpoint the data complexity of why-provenance for Datalog queries and key subclasses thereof. The takeaway of our work is that why-provenance for recursive queries, even if the recursion is limited to be linear, is an intractable problem, whereas for non-recursive queries is highly tractable.}
}


@article{DBLP:journals/pacmmod/0001J0S24,
	author = {Peter Lindner and
                  Sachin Basil John and
                  Christoph Koch and
                  Dan Suciu},
	title = {The Moments Method for Approximate Data Cube Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {84},
	year = {2024},
	url = {https://doi.org/10.1145/3651147},
	doi = {10.1145/3651147},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0001J0S24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate an approximation algorithm for various aggregate queries on partially materialized data cubes. Data cubes are interpreted as probability distributions, and cuboids from a partial materialization populate the terms of a series expansion of the target query distribution. Unknown terms in the expansion are just assumed to be 0 in order to recover an approximate query result. We identify this method as a variant of related approaches from other fields of science, that is, the Bahadur representation and, more generally, (biased) Fourier expansions of Boolean functions. Existing literature indicates a rich but intricate theoretical landscape. Focusing on the data cube application, we start by investigating worst-case error bounds. We build upon prior work to obtain provably optimal materialization strategies with respect to query workloads. In addition, we propose a new heuristic method governing materialization decisions. Finally, we show that well-approximated queries are guaranteed to have well-approximated roll-ups.}
}


@article{DBLP:journals/pacmmod/0002LLTWZ24,
	author = {Yu Cheng and
                  Max Li and
                  Honghao Lin and
                  Zi{-}Yi Tai and
                  David P. Woodruff and
                  Jason Zhang},
	title = {Tight Lower Bounds for Directed Cut Sparsification and Distributed
                  Min-Cut},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {85},
	year = {2024},
	url = {https://doi.org/10.1145/3651148},
	doi = {10.1145/3651148},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/0002LLTWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider two fundamental cut approximation problems on large graphs. We prove new lower bounds for both problems that are optimal up to logarithmic factors. The first problem is approximating cuts in balanced directed graphs. In this problem, we want to build a data structure that can provide (1 ± ε)-approximation of cut values on a graph with n vertices. For arbitrary directed graphs, such a data structure requires Ω(n 2 ) bits even for constant ε. To circumvent this, recent works study β-balanced graphs, meaning that for every directed cut, the total weight of edges in one direction is at most β times the total weight in the other direction. We consider the for-each model, where the goal is to approximate each cut with constant probability, and the for-all model, where all cuts must be preserved simultaneously. We improve the previous Ømega(n √β/ε) lower bound in the for-each model to ~Ω (n √β /ε) and we improve the previous Ω(n β/ε) lower bound in the for-all model to Ω(n β/ε 2 ). This resolves the main open questions of (Cen et al., ICALP, 2021). The second problem is approximating the global minimum cut in a local query model, where we can only access the graph via degree, edge, and adjacency queries. We prove an ΩL(min m, m/ε 2  k R) lower bound for this problem, which improves the previous ΩL(m/k R) lower bound, where m is the number of edges, k is the minimum cut size, and we seek a (1+ε)-approximation. In addition, we show that existing upper bounds with minor modifications match our lower bound up to logarithmic factors.}
}


@article{DBLP:journals/pacmmod/0001GR24,
	author = {Andreas G{\"{o}}bel and
                  Leslie Ann Goldberg and
                  Marc Roth},
	title = {The Weisfeiler-Leman Dimension of Conjunctive Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {86},
	year = {2024},
	url = {https://doi.org/10.1145/3651587},
	doi = {10.1145/3651587},
	timestamp = {Fri, 02 Aug 2024 21:41:11 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0001GR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A graph parameter is a function f on graphs with the property that, for any pair of isomorphic graphs G 1  and G 2 , f(G 1 )=f(G 2 ). The Weisfeiler--Leman (WL) dimension of f is the minimum k such that, if G 1  and G 2  are indistinguishable by the k-dimensional WL-algorithm then f(G 1 )=f(G 2 ). The WL-dimension of f is ∞ if no such k exists. We study the WL-dimension of graph parameters characterised by the number of answers from a fixed conjunctive query to the graph. Given a conjunctive query φ, we quantify the WL-dimension of the function that maps every graph G to the number of answers of φ in G. The works of Dvorak (J. Graph Theory 2010), Dell, Grohe, and Rattan (ICALP 2018), and Neuen (ArXiv 2023) have answered this question for full conjunctive queries, which are conjunctive queries without existentially quantified variables. For such queries φ, the WL-dimension is equal to the treewidth of the Gaifman graph of φ. In this work, we give a characterisation that applies to all conjunctive queries. Given any conjunctive query φ, we prove that its WL-dimension is equal to the semantic extension width sew(φ), a novel width measure that can be thought of as a combination of the treewidth of φ and its quantified star size, an invariant introduced by Durand and Mengel (ICDT 2013) describing how the existentially quantified variables of φ are connected with the free variables. Using the recently established equivalence between the WL-algorithm and higher-order Graph Neural Networks (GNNs) due to Morris et al. (AAAI 2019), we obtain as a consequence that the function counting answers to a conjunctive query φ cannot be computed by GNNs of order smaller than sew(φ). The majority of the paper is concerned with establishing a lower bound of the WL-dimension of a query. Given any conjunctive query φ with semantic extension width k, we consider a graph F of treewidth k obtained from the Gaifman graph of φ by repeatedly cloning the vertices corresponding to existentially quantified variables. Using a modification due to Furer (ICALP 2001) of the Cai-Fürer-Immerman construction (Combinatorica 1992), we then obtain a pair of graphs χ(F) and ^χ(F) that are indistinguishable by the (k-1)-dimensional WL-algorithm since F has treewidth k. Finally, in the technical heart of the paper, we show that φ has a different number of answers in χ(F) and ^χ(F). Thus, φ can distinguish two graphs that cannot be distinguished by the (k-1)-dimensional WL-algorithm, so the WL-dimension of φ is at least k.}
}


@article{DBLP:journals/pacmmod/FanKZ24,
	author = {Austen Z. Fan and
                  Paraschos Koutris and
                  Hangdong Zhao},
	title = {Tight Bounds of Circuits for Sum-Product Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {87},
	year = {2024},
	url = {https://doi.org/10.1145/3651588},
	doi = {10.1145/3651588},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanKZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we ask the following question: given a Boolean Conjunctive Query (CQ), what is the smallest circuit that computes the provenance polynomial of the query over a given semiring? We answer this question by giving upper and lower bounds. Notably, it is shown that any circuit F that computes a CQ over the tropical semiring must have size log |F| ≥ (1-ε) · da-entw for any ε >0, where da-entw is the degree-aware entropic width of the query. We show a circuit construction that matches this bound when the semiring is idempotent. The techniques we use combine several central notions in database theory: provenance polynomials, tree decompositions, and disjunctive Datalog programs. We extend our results to lower and upper bounds for formulas (i.e., circuits where each gate has outdegree one), and to bounds for non-Boolean CQs.}
}


@article{DBLP:journals/pacmmod/DaiQL24,
	author = {Yizhou Dai and
                  Miao Qiao and
                  Rong{-}Hua Li},
	title = {On Density-based Local Community Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {88},
	year = {2024},
	url = {https://doi.org/10.1145/3651589},
	doi = {10.1145/3651589},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DaiQL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Local community search (LCS) finds a community in a given graph G local to a set R of seed nodes by optimizing an objective function. The objective function f(S) for an induced subgraph S encodes the set inclusion criteria of R to a classic community measurement of S such as the conductance and the density. An ideal algorithm for optimizing f(S) is strongly local, that is, the complexity is dependent on R as opposed to G. This paper formulates a general form of objective functions for LCS using configurations and then focuses on a set C of density-based configurations, each corresponding to a density-based LCS objective function. The paper has two main results. i) A constructive classification of C: a configuration in C has a strongly local algorithm for optimizing its corresponding objective function if and only if it is in C L  ⊆ C. ii) A linear programming-based general solution for density-based LCS that is strongly local and practically efficient. This solution is different from the existing strongly local LCS algorithms, which are all based on flow networks.}
}


@article{DBLP:journals/pacmmod/AiswaryaCCM24,
	author = {C. Aiswarya and
                  Diego Calvanese and
                  Francesco Di Cosmo and
                  Marco Montali},
	title = {Verification of Unary Communicating Datalog Programs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {89},
	year = {2024},
	url = {https://doi.org/10.1145/3651590},
	doi = {10.1145/3651590},
	timestamp = {Sun, 19 Jan 2025 15:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AiswaryaCCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study verification of reachability properties over Communicating Datalog Programs (CDPs), which are networks of relational nodes connected through unordered channels and running Datalog-like computations. Each node manipulates a local state database (DB), depending on incoming messages and additional input DBs from external services. Decidability of verification for CDPs has so far been established only under boundedness assumptions on the state and channel sizes, showing at the same time undecidability of reachability for unbounded states with only two unary relations or unbounded channels with a single binary relation. The goal of this paper is to study the open case of CDPs with bounded states and unbounded channels, under the assumption that channels carry unary relations only. We discuss the significance of the resulting model and prove the decidability of verification of variants of reachability, captured in fragments of first-order CTL. We do so through a novel reduction to coverability problems in a class of high-level Petri Nets that manipulate unordered data identifiers. We study the tightness of our results, showing that minor generalizations of the considered reachability properties yield undecidability of verification, both for CDPs and the corresponding Petri Net model.}
}


@article{DBLP:journals/pacmmod/ZhaoDKRT24,
	author = {Hangdong Zhao and
                  Shaleen Deep and
                  Paraschos Koutris and
                  Sudeepa Roy and
                  Val Tannen},
	title = {Evaluating Datalog over Semirings: {A} Grounding-based Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {90},
	year = {2024},
	url = {https://doi.org/10.1145/3651591},
	doi = {10.1145/3651591},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhaoDKRT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Datalog is a powerful yet elegant language that allows expressing recursive computation. Although Datalog evaluation has been extensively studied in the literature, so far, only loose upper bounds are known on how fast a Datalog program can be evaluated. In this work, we ask the following question: given a Datalog program over a naturally-ordered semiring σ, what is the tightest possible runtime? To this end, our main contribution is a general two-phase framework for analyzing the data complexity of Datalog over σ: first ground the program into an equivalent system of polynomial equations (i.e. grounding) and then find the least fixpoint of the grounding over σ. We present algorithms that use structure-aware query evaluation techniques to obtain the smallest possible groundings. Next, efficient algorithms for fixpoint evaluation are introduced over two classes of semirings: (1) finite-rank semirings and (2) absorptive semirings of total order. Combining both phases, we obtain state-of-the-art and new algorithmic results. Finally, we complement our results with a matching fine-grained lower bound.}
}


@article{DBLP:journals/pacmmod/VandevoortKN24,
	author = {Brecht Vandevoort and
                  Bas Ketsman and
                  Frank Neven},
	title = {When View- and Conflict-Robustness Coincide for Multiversion Concurrency
                  Control},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {91},
	year = {2024},
	url = {https://doi.org/10.1145/3651592},
	doi = {10.1145/3651592},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/VandevoortKN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A DBMS allows trading consistency for efficiency through the allocation of isolation levels that are strictly weaker than serializability. The robustness problem asks whether, for a given set of transactions and a given allocation of isolation levels, every possible interleaved execution of those transactions that is allowed under the provided allocation, is always safe. In the literature, safe is interpreted as conflict-serializable (to which we refer here as conflict-robustness). In this paper, we study the view-robustness problem, interpreting safe as view-serializable. View-serializability is a more permissive notion that allows for a greater number of schedules to be serializable and aligns more closely with the intuitive understanding of what it means for a database to be consistent. However, view-serializability is more complex to analyze (e.g., conflict-serializability can be decided in polynomial time whereas deciding view-serializability is NP-complete). While conflict-robustness implies view-robustness, the converse does not hold in general. In this paper, we provide a sufficient condition for isolation levels guaranteeing that conflict- and view-robustness coincide and show that this condition is satisfied by the isolation levels occurring in Postgres and Oracle: read committed (RC), snapshot isolation (SI) and serializable snapshot isolation (SSI). It hence follows that for these systems, widening from conflict- to view-serializability does not allow for more sets of transactions to become robust. Interestingly, the complexity of deciding serializability within these isolation levels is still quite different. Indeed, deciding conflict-serializability for schedules allowed under RC and SI remains in polynomial time, while we show that deciding view-serializability within these isolation levels remains NP-complete.}
}


@article{DBLP:journals/pacmmod/KarmakarMSB24,
	author = {Pratik Karmakar and
                  Mika{\"{e}}l Monet and
                  Pierre Senellart and
                  St{\'{e}}phane Bressan},
	title = {Expected Shapley-Like Scores of Boolean functions: Complexity and
                  Applications to Probabilistic Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {92},
	year = {2024},
	url = {https://doi.org/10.1145/3651593},
	doi = {10.1145/3651593},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KarmakarMSB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shapley values, originating in game theory and increasingly prominent in explainable AI, have been proposed to assess the contribution of facts in query answering over databases, along with other similar power indices such as Banzhaf values. In this work we adapt these Shapley-like scores to probabilistic settings, the objective being to compute their expected value. We show that the computations of expected Shapley values and of the expected values of Boolean functions are interreducible in polynomial time, thus obtaining the same tractability landscape. We investigate the specific tractable case where Boolean functions are represented as deterministic decomposable circuits, designing a polynomial-time algorithm for this setting. We present applications to probabilistic databases through database provenance, and an effective implementation of this algorithm within the ProvSQL system, which experimentally validates its feasibility over a standard benchmark.}
}


@article{DBLP:journals/pacmmod/HanischK24,
	author = {Philipp Hanisch and
                  Markus Kr{\"{o}}tzsch},
	title = {Chase Termination Beyond Polynomial Time},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {93},
	year = {2024},
	url = {https://doi.org/10.1145/3651594},
	doi = {10.1145/3651594},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HanischK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The chase is a widely implemented approach to reason with tuple-generating dependencies (tgds), used in data exchange, data integration, and ontology-based query answering. However, it is merely a semi-decision procedure, which may fail to terminate. Many decidable conditions have been proposed for tgds to ensure chase termination, typically by forbidding some kind of "cycle\'\' in the chase process. We propose a new criterion that explicitly allows some such cycles, and yet ensures termination of the standard chase under reasonable conditions. This leads to new decidable fragments of tgds that are not only syntactically more general but also strictly more expressive than the fragments defined by prior acyclicity conditions. Indeed, while known terminating fragments are restricted to PTime data complexity, our conditions yield decidable languages for any k- ExpTime. We further refine our syntactic conditions to obtain fragments of tgds for which an optimised chase procedure decides query entailment in PSpace or k- ExpSpace, respectively.}
}


@article{DBLP:journals/pacmmod/BunGN024,
	author = {Mark Bun and
                  Marco Gaboardi and
                  Marcel Neunhoeffer and
                  Wanrong Zhang},
	title = {Continual Release of Differentially Private Synthetic Data from Longitudinal
                  Data Collections},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {94},
	year = {2024},
	url = {https://doi.org/10.1145/3651595},
	doi = {10.1145/3651595},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BunGN024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by privacy concerns in long-term longitudinal studies in medical and social science research, we study the problem of continually releasing differentially private synthetic data from longitudinal data collections. We introduce a model where, in every time step, each individual reports a new data element, and the goal of the synthesizer is to incrementally update a synthetic dataset in a consistent way to capture a rich class of statistical properties. We give continual synthetic data generation algorithms that preserve two basic types of queries: fixed time window queries and cumulative time queries. We show nearly tight upper bounds on the error rates of these algorithms and demonstrate their empirical performance on realistically sized datasets from the U.S. Census Bureau's Survey of Income and Program Participation.}
}


@article{DBLP:journals/pacmmod/0001JB24,
	author = {Bart Bogaerts and
                  Maxime Jakubowski and
                  Jan Van den Bussche},
	title = {Postulates for Provenance: Instance-based provenance for first-order
                  logic},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {95},
	year = {2024},
	url = {https://doi.org/10.1145/3651596},
	doi = {10.1145/3651596},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/0001JB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Instance-based provenance is an explanation for a query result in the form of a subinstance of the database. We investigate different desiderata one may want to impose on these subinstances. Concretely we consider seven basic postulates for provenance. Six of them relate subinstances to provenance polynomials, three-valued semantics, and Halpern-Pearl causality. Determinism of the provenance mechanism is the seventh basic postulate. Moreover, we consider the postulate of minimality, which can be imposed with respect to any set of basic postulates. Our main technical contribution is an analysis and characterisation of which combinations of postulates are jointly satisfiable. Our main conceptual contribution is an approach to instance-based provenance through three-valued instances, which makes it applicable to first-order logic queries involving negation.}
}


@article{DBLP:journals/pacmmod/KhamisNOS24,
	author = {Mahmoud Abo Khamis and
                  Vasileios Nakos and
                  Dan Olteanu and
                  Dan Suciu},
	title = {Join Size Bounds using l\({}_{\mbox{p}}\)-Norms on Degree Sequences},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {96},
	year = {2024},
	url = {https://doi.org/10.1145/3651597},
	doi = {10.1145/3651597},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KhamisNOS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating the output size of a query is a fundamental yet longstanding problem in database query processing. Traditional cardinality estimators used by database systems can routinely underestimate the true output size by orders of magnitude, which leads to significant system performance penalty. Recently, upper bounds have been proposed that are based on information inequalities and incorporate sizes and max-degrees from input relations, yet their main benefit is limited to cyclic queries, because they degenerate to rather trivial formulas on acyclic queries. We introduce a significant extension of the upper bounds, by incorporating l p -norms of the degree sequences of join attributes. Our bounds are significantly lower than previously known bounds, even when applied to acyclic queries. These bounds are also based on information theory, they come with a matching query evaluation algorithm, are computable in exponential time in the query size, and are provably tight when all degrees are ''simple''.}
}


@article{DBLP:journals/pacmmod/0005K24,
	author = {Xiao Hu and
                  Paraschos Koutris},
	title = {Topology-aware Parallel Joins},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {97},
	year = {2024},
	url = {https://doi.org/10.1145/3651598},
	doi = {10.1145/3651598},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0005K24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the design and analysis of parallel join algorithms in a topology-aware computational model. In this model, the network is modeled as a directed graph, where each edge is associated with a cost function that depends on the data transferred between the two endpoints and the link bandwidth. The computation proceeds in synchronous rounds and the cost of each round is measured as the maximum cost over all the edges in the network. Our main result is an asymptotically optimal join algorithm over symmetric tree topologies. The algorithm generalizes prior topology-aware protocols for set intersection and cartesian product to a binary join over an arbitrary input distribution with possible data skew.}
}


@article{DBLP:journals/pacmmod/Hu24,
	author = {Xiao Hu},
	title = {Fast Matrix Multiplication for Query Processing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {98},
	year = {2024},
	url = {https://doi.org/10.1145/3651599},
	doi = {10.1145/3651599},
	timestamp = {Fri, 09 Aug 2024 16:05:02 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Hu24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies how to use fast matrix multiplication to speed up query processing. As observed, computing a two-table join and then projecting away the join attribute is essentially the Boolean matrix multiplication problem, which can be significantly improved with fast matrix multiplication. Moving beyond this basic two-table query, we introduce output-sensitive algorithms for general join-project queries using fast matrix multiplication. These algorithms have achieved a polynomially large improvement over the classic Yannakakis framework. To the best of our knowledge, this is the first theoretical improvement for general acyclic join-project queries since 1981.}
}


@article{DBLP:journals/pacmmod/CalauttiLPS24a,
	author = {Marco Calautti and
                  Ester Livshits and
                  Andreas Pieris and
                  Markus Schneider},
	title = {Combined Approximations for Uniform Operational Consistent Query Answering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {99},
	year = {2024},
	url = {https://doi.org/10.1145/3651600},
	doi = {10.1145/3651600},
	timestamp = {Tue, 20 Aug 2024 07:59:07 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/CalauttiLPS24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Operational consistent query answering (CQA) is a recent framework for CQA based on revised definitions of repairs, which are built by applying a sequence of operations (e.g., fact deletions) starting from an inconsistent database until we reach a database that is consistent w.r.t. the given set of constraints. It has been recently shown that there is an efficient approximation for computing the percentage of repairs that entail a given query when we focus on primary keys, conjunctive queries, and assuming the query is fixed (i.e., in data complexity). However, it has been left open whether such an approximation exists when the query is part of the input (i.e., in combined complexity). We show that this is the case when we focus on self-join-free conjunctive queries of bounded generelized hypertreewidth. We also show that it is unlikely that efficient approximation schemes exist once we give up one of the adopted syntactic restrictions, i.e., self-join-freeness or bounding the generelized hypertreewidth. Towards the desired approximation, we introduce a counting complexity class, called SpanTL, show that each problem in it admits an efficient approximation scheme by using a recent approximability result about tree automata, and then place the problem of interest in SpanTL.}
}


@article{DBLP:journals/pacmmod/DavidFM24,
	author = {Claire David and
                  Nadime Francis and
                  Victor Marsault},
	title = {Distinct Shortest Walk Enumeration for RPQs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {100},
	year = {2024},
	url = {https://doi.org/10.1145/3651601},
	doi = {10.1145/3651601},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DavidFM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the Distinct Shortest Walks problem. Given two vertices s and t of a graph database D and a regular path query, we want to enumerate all walks of minimal length from s to t that carry a label that conforms to the query. Usual theoretical solutions turn out to be inefficient when applied to graph models that are closer to real-life systems, in particular because edges may carry multiple labels. Indeed, known algorithms may repeat the same answer exponentially many times. We propose an efficient algorithm for graph databases with multiple labels. The preprocessing runs in O(DxA) and the delay between two consecutive outputs is in O(λxA), where A is a nondeterministic automaton representing the query and L is the minimal length. The algorithm can handle epsilon-transitions in A or queries given as regular expressions at no additional cost.}
}


@article{DBLP:journals/pacmmod/BenderCFKK24,
	author = {Michael A. Bender and
                  Alex Conway and
                  Martin Farach{-}Colton and
                  Hanna Koml{\'{o}}s and
                  William Kuszmaul},
	title = {Layered List Labeling},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {101},
	year = {2024},
	url = {https://doi.org/10.1145/3651602},
	doi = {10.1145/3651602},
	timestamp = {Wed, 23 Oct 2024 08:55:27 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BenderCFKK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The list-labeling problem is one of the most basic and well-studied algorithmic primitives in data structures, with an extensive literature spanning upper bounds, lower bounds, and data management applications. The classical algorithm for this problem, dating back to 1981, has amortized cost O(log bn). Subsequent work has led to improvements in three directions: low-latency (worst-case) bounds; high-throughput (expected) bounds; and (adaptive) bounds for important workloads. Perhaps surprisingly, these three directions of research have remained almost entirely disjoint---this is because, so far, the techniques that allow for progress in one direction have forced worsening bounds in the others. Thus there would appear to be a tension between worst-case, adaptive, and expected bounds. List labeling has been proposed for use in databases at least as early as PODS'99, but a database needs good throughput, response time, and needs to adapt to common workloads (e.g., bulk loads), and no current list-labeling algorithm achieve good bounds for all three. We show that this tension is not fundamental. In fact, with the help of new data-structural techniques, one can actually combine any three list-labeling solutions in order to cherry-pick the best worst-case, adaptive, and expected bounds from each of them.}
}


@article{DBLP:journals/pacmmod/Pavan0VM24,
	author = {Aduri Pavan and
                  Sourav Chakraborty and
                  N. V. Vinodchandran and
                  Kuldeep S. Meel},
	title = {On the Feasibility of Forgetting in Data Streams},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {102},
	year = {2024},
	url = {https://doi.org/10.1145/3651603},
	doi = {10.1145/3651603},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Pavan0VM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In today\'s digital age, it is becoming increasingly prevalent to retain digital footprints in the cloud indefinitely. Nonetheless, there is a valid argument that entities should have the authority to decide whether their personal data remains within a specific database or is expunged. Indeed, nations across the globe are increasingly enacting legislation to uphold the "Right To Be Forgotten" for individuals. Investigating computational challenges, including the formalization and implementation of this notion, is crucial due to its relevance in the domains of data privacy and management. This work introduces a new streaming model: the \'Right to be Forgotten Data Streaming Model\' (RFDS model). The main feature of this model is that any element in the stream has the right to have its history removed from the stream. Formally, the input is a stream of updates of the form (a, Δ) where Δ ∈ {+, ⊥} and a is an element from a universe U. When the update Δ=+ occurs, the frequency of a, denoted as f a , is incremented to f a  +1. When the update Δ=⊥, occurs, f a  is set to 0. This feature, which represents the forget request, distinguishes the present model from existing data streaming models. This work systematically investigates computational challenges that arise while incorporating the notion of the right to be forgotten. Our initial considerations reveal that even estimating F 1  (sum of the frequencies of elements) of the stream is a non-trivial problem in this model. Based on the initial investigations, we focus on a modified model which we call α-RFDS where we limit the number of forget operations to be at most α fraction. In this modified model, we focus on estimating F 0  (number of distinct elements) and F 1 . We present algorithms and establish almost-matching lower bounds on the space complexity for these computational tasks.}
}


@article{DBLP:journals/pacmmod/MarcinkowskiO24,
	author = {Jerzy Marcinkowski and
                  Mateusz Orda},
	title = {Bag Semantics Conjunctive Query Containment. Four Small Steps Towards
                  Undecidability},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {103},
	year = {2024},
	url = {https://doi.org/10.1145/3651604},
	doi = {10.1145/3651604},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MarcinkowskiO24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query Containment Problem (QCP) is one of the most fundamental decision problems in database query processing and optimization. Complexity of QCP for conjunctive queries has been fully understood since 1970s. But, as Chaudhuri and Vardi noticed in their classical 1993 paper this understanding is based on the assumption that query answers are sets of tuples, and it does not transfer to the situation when multi-set (bag) semantics is considered. Now, 30 years later, decidability of QCP for bag semantics remains an open question, one of the most intriguing open questions in database theory. In this paper we show a series of undecidability results for some generalizations of this problem. We show, for example, that the problem whether, for given two boolean conjunctive queries φ s  and φ b , and a linear function F, the inequality F(φ s (D)) =< φ b (D) holds for each database instance D, is undecidable.}
}


@article{DBLP:journals/pacmmod/MakhijaG24,
	author = {Neha Makhija and
                  Wolfgang Gatterbauer},
	title = {Minimally Factorizing the Provenance of Self-join Free Conjunctive
                  Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {104},
	year = {2024},
	url = {https://doi.org/10.1145/3651605},
	doi = {10.1145/3651605},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/MakhijaG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of finding the minimal-size factorization of the provenance of self-join-free conjunctive queries, i.e.,we want to find a formula that minimizes the number of variable repetitions. This problem is equivalent to solving the fundamental Boolean formula factorization problem for the restricted setting of the provenance formulas of self-join free queries. While general Boolean formula minimization is Σ p 2 -complete, we show that the problem is NP-Complete in our case. Additionally, we identify a large class of queries that can be solved in PTIME, expanding beyond the previously known tractable cases of read-once formulas and hierarchical queries. We describe connections between factorizations, Variable Elimination Orders (VEOs), and minimal query plans. We leverage these insights to create an Integer Linear Program (ILP) that can solve the minimal factorization problem exactly. We also propose a Max-Flow Min-Cut (MFMC) based algorithm that gives an efficient approximate solution. Importantly, we show that both the Linear Programming (LP) relaxation of our ILP, and our MFMC-based algorithm are always correct for all currently known PTIME cases. Thus, we present two unified algorithms (ILP and MFMC) that can both recover all known PTIME cases in PTIME, yet also solve NP-Complete cases either exactly (ILP) or approximately (MFMC), as desired.}
}


@article{DBLP:journals/pacmmod/BienvenuFL24,
	author = {Meghyn Bienvenu and
                  Diego Figueira and
                  Pierre Lafourcade},
	title = {When is Shapley Value Computation a Matter of Counting?},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {105},
	year = {2024},
	url = {https://doi.org/10.1145/3651606},
	doi = {10.1145/3651606},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BienvenuFL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Shapley value provides a natural means of quantifying the contributions of facts to database query answers. In this work, we seek to broaden our understanding of Shapley value computation (SVC) in the database setting by revealing how it relates to Fixed-size Generalized Model Counting (FGMC), which is the problem of computing the number of sub-databases of a given size and containing a given set of assumed facts that satisfy a fixed query. Our focus will be on explaining the difficulty of SVC via FGMC, and to this end, we identify general conditions on queries which enable reductions from FGMC to SVC. As a byproduct, we not only obtain alternative explanations for existing hardness results for SVC, but also new complexity results. In particular, we establish FP-#P complexity dichotomies for constant-free unions of connected CQs and connected homomorphism-closed graph queries. We also consider some variants of the SVC problem, by disallowing assumed facts or quantifying the contributions of constants rather than facts.}
}


@article{DBLP:journals/pacmmod/0001024,
	author = {Christoph Koch and
                  Peter Lindner},
	title = {Query Optimization by Quantifier Elimination},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {106},
	year = {2024},
	url = {https://doi.org/10.1145/3651607},
	doi = {10.1145/3651607},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0001024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query optimizers have a limited arsenal of techniques for optimizing nested queries. In this paper, we develop a new approach for query optimization based on quantifier elimination. Quantifier elimination is a well-established tool for proving the decidability of logical theories. Here, however, we show that it can be turned into an effective query optimization technique that may yield asymptotic improvements in query processing efficiency. In addition, the technique establishes a foundation for certain well-known but previously little-understood aggregation based techniques for optimizing nested queries.}
}


@article{DBLP:journals/pacmmod/AtseriasK24,
	author = {Albert Atserias and
                  Phokion G. Kolaitis},
	title = {Consistency of Relations over Monoids},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {107},
	year = {2024},
	url = {https://doi.org/10.1145/3651608},
	doi = {10.1145/3651608},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AtseriasK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The interplay between local consistency and global consistency has been the object of study in several different areas, including probability theory, relational databases, and quantum information. For relational databases, Beeri, Fagin, Maier, and Yannakakis showed that a database schema is acyclic if and only if it has the local-to-global consistency property for relations, which means that every collection of pairwise consistent relations over the schema is globally consistent. More recently, the same result has been shown under bag semantics. In this paper, we carry out a systematic study of local vs. global consistency for relations over positive commutative monoids, which is a common generalization of ordinary relations and bags. Let K be an arbitrary positive commutative monoid. We begin by showing that acyclicity of the schema is a necessary condition for the local-to-global consistency property for K-relations to hold. Unlike the case of ordinary relations and bags, however, we show that acyclicity is not always sufficient. After this, we characterize the positive commutative monoids for which acyclicity is both necessary and sufficient for the local-to-global consistency property to hold; this characterization involves a combinatorial property of monoids, which we call the transportation property. We then identify several different classes of monoids that possess the transportation property. As our final contribution, we introduce a modified notion of local consistency of K-relations, which we call pairwise consistency up to the free cover. We prove that, for all positive commutative monoids K, even those without the transportation property, acyclicity is both necessary and sufficient for every family of K-relations that is pairwise consistent up to the free cover to be globally consistent.}
}


@article{DBLP:journals/pacmmod/BenderFGK24,
	author = {Michael A. Bender and
                  Mart{\'{\i}}n Farach{-}Colton and
                  Michael T. Goodrich and
                  Hanna Koml{\'{o}}s},
	title = {History-Independent Dynamic Partitioning: Operation-Order Privacy
                  in Ordered Data Structures},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {108},
	year = {2024},
	url = {https://doi.org/10.1145/3651609},
	doi = {10.1145/3651609},
	timestamp = {Fri, 02 Aug 2024 21:41:11 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/BenderFGK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A data structure is history independent if its internal representation reveals nothing about the history of operations beyond what can be determined from the current contents of the data structure. History independence is typically viewed as a security or privacy guarantee, with the intent being to minimize risks incurred by a security breach or audit. Despite widespread advances in history independence, there is an important data-structural primitive that previous work has been unable to replace with an equivalent history-independent alternative---dynamic partitioning. In dynamic partitioning, we are given a dynamic set S of ordered elements and a size-parameter B, and the objective is to maintain a partition of S into ordered groups, each of size Θ(B). Dynamic partitioning is important throughout computer science, with applications to B-tree rebalancing, write-optimized dictionaries, log-structured merge trees, other external-memory indexes, geometric and spatial data structures, cache-oblivious data structures, and order-maintenance data structures. The lack of a history-independent dynamic-partitioning primitive has meant that designers of history-independent data structures have had to resort to complex alternatives. In this paper, we achieve history-independent dynamic partitioning. Our algorithm runs asymptotically optimally against an oblivious adversary, processing each insert/delete with O(1) operations in expectation and O(B log N/loglog N) with high probability in set size N.}
}


@article{DBLP:journals/pacmmod/GribelyukSWY24,
	author = {Elena Gribelyuk and
                  Pachara Sawettamalya and
                  Hongxun Wu and
                  Huacheng Yu},
	title = {Simple {\&} Optimal Quantile Sketch: Combining Greenwald-Khanna
                  with Khanna-Greenwald},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {109},
	year = {2024},
	url = {https://doi.org/10.1145/3651610},
	doi = {10.1145/3651610},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GribelyukSWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating the ε-approximate quantiles or ranks of a stream is a fundamental task in data monitoring. Given a stream x_1,..., x_n from a universe \\mathcalU with total order, an additive-error quantile sketch \\mathcalM allows us to approximate the rank of any query y\\in \\mathcalU up to additive ε n error. In 2001, Greenwald and Khanna gave a deterministic algorithm (GK sketch) that solves the ε-approximate quantiles estimation problem using O(ε^-1 łog(ε n)) space \\citegreenwald2001space ; recently, this algorithm was shown to be optimal by Cormode and Vesleý in 2020 \\citecormode2020tight. However, due to the intricacy of the GK sketch and its analysis, over-simplified versions of the algorithm are implemented in practical applications, often without any known theoretical guarantees. In fact, it has remained an open question whether the GK sketch can be simplified while maintaining the optimal space bound. In this paper, we resolve this open question by giving a simplified deterministic algorithm that stores at most (2 + o(1))ε^-1 łog (ε n) elements and solves the additive-error quantile estimation problem; as a side benefit, our algorithm achieves a smaller constant factor than the \\frac11 2 ε^-1 łog(ε n) space bound in the original GK sketch~\\citegreenwald2001space. Our algorithm features an easier analysis and still achieves the same optimal asymptotic space complexity as the original GK sketch. Lastly, our simplification enables an efficient data structure implementation, with a worst-case runtime of O(łog(1/ε) + łog łog (ε n)) per-element for the ordinary ε-approximate quantile estimation problem. Also, for the related "weighted\'\' quantile estimation problem, we give efficient data structures for our simplified algorithm which guarantee a worst-case per-element runtime of O(łog(1/ε) + łog łog (ε W_n/w_\\textrmmin )), achieving an improvement over the previous upper bound of \\citeassadi2023generalizing.}
}


@article{DBLP:journals/pacmmod/0002P24,
	author = {Christoph Dorn and
                  Haikal Pribadi},
	title = {TypeQL: {A} Type-Theoretic {\&} Polymorphic Query Language},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {110},
	year = {2024},
	url = {https://doi.org/10.1145/3651611},
	doi = {10.1145/3651611},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/0002P24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relational data modeling can often be restrictive as it provides no direct facility for modeling polymorphic types, reified relations, multi-valued attributes, and other common high-level structures in data. This creates many challenges in data modeling and engineering tasks, and has led to the rise of more flexible NoSQL databases, such as graph and document databases. In the absence of structured schemas, however, we can neither express nor validate the intention of data models, making long-term maintenance of databases substantially more difficult. To resolve this dilemma, we argue that, parallel to the role of classical predicate logic for relational algebra, contemporary foundations of mathematics rooted in type theory can guide us in the development of powerful new high-level data models and query languages. To this end, we introduce a new polymorphic entity-relation-attribute (PERA) data model, grounded in type-theoretic principles and accessible through classical conceptual modeling, with a near-natural query language: TypeQL. We illustrate the syntax of TypeQL as well as its denotation in the PERA model, formalize our model as an algebraic theory with dependent types, and describe its stratified semantics.}
}


@article{DBLP:journals/pacmmod/Meel0024,
	author = {Kuldeep S. Meel and
                  Sourav Chakraborty and
                  Umang Mathur},
	title = {A faster {FPRAS} for {\#}NFA},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {112},
	year = {2024},
	url = {https://doi.org/10.1145/3651613},
	doi = {10.1145/3651613},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Meel0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a non-deterministic finite automaton (NFA) A with m states, and a natural number n (presented in unary), the #NFA problem asks to determine the size of the set L(A,n) of words of length n accepted by A. While the corresponding decision problem of checking the emptiness of L(A,n) is solvable in polynomial time, the #NFA problem is known to be #P-hard. Recently, the long-standing open question --- whether there is an FPRAS (fully polynomial time randomized approximation scheme) for #NFA --- was resolved by Arenas, Croquevielle, Jayaram, and Riveros in [ACJR19]. The authors demonstrated the existence of a fully polynomial randomized approximation scheme with a time complexity of ~O(m 17  n 17  • 1/ε 14  • log (1/δ)), for a given tolerance ε and confidence parameter δ. Given the prohibitively high time complexity in terms of each of the input parameters, and considering the widespread application of approximate counting (and sampling) in various tasks in Computer Science, a natural question arises: is there a faster FPRAS for #NFA that can pave the way for the practical implementation of approximate #NFA tools? In this work, we answer this question in the positive. We demonstrate that significant improvements in time complexity are achievable, and propose an FPRAS for #NFA that is more efficient in terms of both time and sample complexity. A key ingredient in the FPRAS due to Arenas, Croquevielle, Jayaram, and Riveros [ACJR19] is inter-reducibility of sampling and counting, which necessitates a closer look at the more informative measure --- the number of samples maintained for each pair of state q and length i <= n. In particular, the scheme of [ACJR19] maintains O(m 7 /n 7  ε 7  ) samples per pair of state and length. In the FPRAS we propose, we systematically reduce the number of samples required for each state to be only poly-logarithmically dependent on m, with significantly less dependence on n and ε, maintaining only ~O(n 4 /ε 2 ) samples per state. Consequently, our FPRAS runs in time ~O((m 2 n 10  + m 3 n 6 ) • 1/ε 4  • log 2 (1/δ)). The FPRAS and its analysis use several novel insights. First, our FPRAS maintains a weaker invariant about the quality of the estimate of the number of samples for each state q and length i <= n. Second, our FPRAS only requires that the distribution of the samples maintained is close to uniform distribution only in total variation distance (instead of maximum norm). We believe our insights may lead to further reductions in time complexity and thus open up a promising avenue for future work towards the practical implementation of tools for approximate #NFA.}
}


@article{DBLP:journals/pacmmod/FockeGRZ24,
	author = {Jacob Focke and
                  Leslie Ann Goldberg and
                  Marc Roth and
                  Stanislav Zivn{\'{y}}},
	title = {Counting Answers to Unions of Conjunctive Queries: Natural Tractability
                  Criteria and Meta-Complexity},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {2},
	pages = {113},
	year = {2024},
	url = {https://doi.org/10.1145/3651614},
	doi = {10.1145/3651614},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FockeGRZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of counting answers to unions of conjunctive queries (UCQs) under structural restrictions on the input query. Concretely, given a class C of UCQs, the problem #UCQ (C) provides as input a UCQ Ψ ∈ C and a database D and the problem is to compute the number of answers of Ψ in D. Chen and Mengel [PODS'16] have shown that for any recursively enumerable class C, the problem #UCQ (C) is either fixed-parameter tractable or hard for one of the parameterised complexity classes W[1] or #W[1]. However, their tractability criterion is unwieldy in the sense that, given any concrete class C of UCQs, it is not easy to determine how hard it is to count answers to queries in C. Moreover, given a single specific UCQ Ψ, it is not easy to determine how hard it is to count answers to Ψ. In this work, we address the question of finding a natural tractability criterion: The combined conjunctive query of a UCQ Ψ=φ 1  ∨ ... ∨ φ l  is the conjunctive query ^ Ψ = φ_1 ∧ ... ∧ φ l . We show that under natural closure properties of C, the problem #UCQ (C) is fixed-parameter tractable if and only if the combined conjunctive queries of UCQs in C, and their contracts, have bounded treewidth. A contract of a conjunctive query is an augmented structure, taking into account how the quantified variables are connected to the free variables --- if all variables are free, then a conjunctive query is equal to its contract; in this special case the criterion for fixed-parameter tractability of #UCQ (C) thus simplifies to the combined queries having bounded treewidth. Finally, we give evidence that a closure property on C is necessary for obtaining a natural tractability criterion: We show that even for a single UCQ Ψ, the meta problem of deciding whether #UCQ (Ψ) can be solved in time O(|D| d ) is NP-hard for any fixed d ≥ 1. Moreover, we prove that a known exponential-time algorithm for solving the meta problem is optimal under assumptions from fine-grained complexity theory. As a corollary of our reduction, we also establish that approximating the Weisfeiler-Leman-Dimension of a UCQ is NP-hard.}
}


@article{DBLP:journals/pacmmod/AgrawalM024,
	author = {Divyakant Agrawal and
                  Alexandra Meliou and
                  S. Sudarshan},
	title = {{PACMMOD} Volume 2 Issue 3: Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {114},
	year = {2024},
	url = {https://doi.org/10.1145/3654917},
	doi = {10.1145/3654917},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgrawalM024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Welcome to Issue 3 of Volume 2 of the Proceedings of the ACM on Management of Data. This issue has papers submitted to the SIGMOD research track of PACMMOD, whose submission deadline was October 15, 2023. Out of 287 submissions in this round, a total of 70 articles were accepted and are presented in this issue. We provide statistics of paper submissions and acceptance, by primary subject area, across all submissions to the SIGMOD research track of PACMMOD in 2023. Accepted papers from this pool have been invited for presentation at the ACM SIGMOD 2024 conference to be held from 9th to 14th June 2024, in Santiago, Chile.}
}


@article{DBLP:journals/pacmmod/Wu00SWZ0024,
	author = {Yang Wu and
                  Yao Wan and
                  Hongyu Zhang and
                  Yulei Sui and
                  Wucai Wei and
                  Wei Zhao and
                  Guandong Xu and
                  Hai Jin},
	title = {Automated Data Visualization from Natural Language via Large Language
                  Models: An Exploratory Study},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {115},
	year = {2024},
	url = {https://doi.org/10.1145/3654992},
	doi = {10.1145/3654992},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Wu00SWZ0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Natural Language to Visualization (NL2Vis) task aims to transform natural-language descriptions into visual representations for a grounded table, enabling users to gain insights from vast amounts of data. Recently, many deep learning-based approaches have been developed for NL2Vis. Despite the considerable efforts made by these approaches, challenges persist in visualizing data sourced from unseen databases or spanning multiple tables. Taking inspiration from the remarkable generation capabilities of Large Language Models (LLMs), this paper conducts an empirical study to evaluate their potential in generating visualizations, and explore the effectiveness of in-context learning prompts for enhancing this task. In particular, we first explore the ways of transforming structured tabular data into sequential text prompts, as to feed them into LLMs and analyze which table content contributes most to the NL2Vis. Our findings suggest that transforming structured tabular data into programs is effective, and it is essential to consider the table schema when formulating prompts. Furthermore, we evaluate two types of LLMs: finetuned models (e.g., T5-Small) and inference-only models (e.g., GPT-3.5), against state-of-the-art methods, using the NL2Vis benchmarks (i.e., nvBench). The experimental results reveal that LLMs outperform baselines, with inference-only models consistently exhibiting performance improvements, at times even surpassing fine-tuned models when provided with certain few-shot demonstrations through in-context learning. Finally, we analyze when the LLMs fail in NL2Vis, and propose to iteratively update the results using strategies such as chain-of-thought, role-playing, and code-interpreter. The experimental results confirm the efficacy of iterative updates and hold great potential for future study.}
}


@article{DBLP:journals/pacmmod/ChoiYC24,
	author = {Minguk Choi and
                  Seehwan Yoo and
                  Jongmoo Choi},
	title = {Can Learned Indexes be Built Efficiently? {A} Deep Dive into Sampling
                  Trade-offs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {116},
	year = {2024},
	url = {https://doi.org/10.1145/3654919},
	doi = {10.1145/3654919},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChoiYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By embedding the distribution of keys in indexing structure, learned indexes can minimize the index size and maximize the lookup performance. Yet, one of the problems in the present learned index is the long index-building time. The conventional learned index requires a complete traversal of the entire dataset, which makes it less practical than traditional index. This paper challenges the efficiency of build time to make the learned index practical. Our approach for a build time-efficient learned index is to employ sampled learning. In this paper, we present two error-bounded sampling schemes: Sample EB-PLA, and Sample EB-Histogram. Although sampling is a simple idea, there are several considerations to make it practical. For example, sampling interval, error-boundness, and index hyper-parameters are inter-related each other, presenting complicated trade-offs between build-time, index size, accuracy and lookup latency. Throughout the extensive experiments over six real-world datasets, we show that the index-building time can be efficiently reduced over an order of magnitude by our sampling schemes. The results reveal that the sampling expands the design space of learned indexes, including the build-time as well as lookup performance and index size. Our Pareto analysis shows that a learned index can be built more efficiently than a traditional index through sampling.}
}


@article{DBLP:journals/pacmmod/WangZW0024,
	author = {Jianwei Wang and
                  Ying Zhang and
                  Kai Wang and
                  Xuemin Lin and
                  Wenjie Zhang},
	title = {Missing Data Imputation with Uncertainty-Driven Network},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {117},
	year = {2024},
	url = {https://doi.org/10.1145/3654920},
	doi = {10.1145/3654920},
	timestamp = {Wed, 04 Dec 2024 15:15:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangZW0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of missing data imputation, which is a fundamental task in the area of data quality that aims to impute the missing data to achieve the completeness of datasets. Though the recent distribution-modeling-based techniques (e.g., distribution generation and distribution matching) can achieve state-of-the-art performance in terms of imputation accuracy, we notice that (1) they deploy a sophisticated deep learning model that tends to be overfitting for missing data imputation; (2) they directly rely on a global data distribution while overlooking the local information. Driven by the inherent variability in both missing data and missing mechanisms, in this paper, we explore the uncertain nature of this task and aim to address the limitations of existing works by proposing an u<u>N</u>certainty-driven netw<u>O</u>rk for <u>M</u>issing data <u>I</u>mputation, termed NOMI. NOMI has three key components, i.e., the retrieval module, the neural network gaussian process imputator (NNGPI) and the uncertainty-based calibration module. NOMI~ runs these components sequentially and in an iterative manner to achieve a better imputation performance. Specifically, in the retrieval module, NOMI~ retrieves local neighbors of the incomplete data samples based on the pre-defined similarity metric. Subsequently, we design NNGPI~ that merges the advantages of both the Gaussian Process and the universal approximation capacity of neural networks. NNGPI~ models the uncertainty by learning the posterior distribution over the data to impute missing values while alleviating the overfitting issue. Moreover, we further propose an uncertainty-based calibration module that utilizes the uncertainty of the imputator on its prediction to help the retrieval module obtain more reliable local information, thereby further enhancing the imputation performance. We also demonstrate that our NOMI~ can be reformulated as an instance of the well-known Expectation Maximization (EM) algorithm, highlighting the strong theoretical foundation of our proposed methods. Extensive experiments are conducted over 12 real-world datasets. The results demonstrate the excellent performance of NOMI in terms of both accuracy and efficiency.}
}


@article{DBLP:journals/pacmmod/Dai0024,
	author = {Binyang Dai and
                  Xiao Hu and
                  Ke Yi},
	title = {Reservoir Sampling over Joins},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {118},
	year = {2024},
	url = {https://doi.org/10.1145/3654921},
	doi = {10.1145/3654921},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Dai0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sampling over joins is a fundamental task in large-scale data analytics. Instead of computing the full join results, which could be massive, a uniform sample of the join results would suffice for many purposes, such as answering analytical queries or training machine learning models. In this paper, we study the problem of how to maintain a random sample over joins while the tuples are streaming in. Without the join, this problem can be solved by some simple and classical reservoir sampling algorithms. However, the join operator makes the problem significantly harder, as the join size can be polynomially larger than the input. We present a new algorithm for this problem that achieves a near-linear complexity. The key technical components are a generalized reservoir sampling algorithm that supports a predicate, and a dynamic index for sampling over joins. We also conduct extensive experiments on both graph and relational data over various join queries, and the experimental results demonstrate significant performance improvement over the state of the art.}
}


@article{DBLP:journals/pacmmod/ZhouGF024,
	author = {Yingli Zhou and
                  Qingshuo Guo and
                  Yixiang Fang and
                  Chenhao Ma},
	title = {A Counting-based Approach for Efficient k-Clique Densest Subgraph
                  Discovery},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {119},
	year = {2024},
	url = {https://doi.org/10.1145/3654922},
	doi = {10.1145/3654922},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhouGF024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Densest subgraph discovery (DSD) is a fundamental topic in graph mining. It has been extensively studied in the literature and has found many real applications in a wide range of fields, such as biology, finance, and social networks. As a typical problem of DSD, the k-clique densest subgraph (CDS) problem aims to detect a subgraph from a graph, such that the ratio of the number of k-cliques over the number of its vertices is maximized. This problem has received plenty of attention in the literature, and is widely used in identifying larger ''near-cliques''. Existing CDS solutions, either k-core or convex programming based solutions, often need to enumerate almost all the k-cliques, which is very inefficient because real-world graphs usually have a vast number of k-cliques. To improve the efficiency, in this paper, we propose a novel framework based on the Frank-Wolfe algorithm, which only needs k-clique counting, rather than k-clique enumeration, where the former one is often much faster than the latter one. Based on the framework, we develop an efficient approximation algorithm, by employing the state-of-the-art k-clique counting algorithm and proposing some optimization techniques. We have performed extensive experimental evaluation on 14 real-world large graphs and the results demonstrate the high efficiency of our algorithms. Particularly, our algorithm is up to seven orders of magnitude faster than the state-of-the-art algorithm with the same accuracy guarantee.}
}


@article{DBLP:journals/pacmmod/PatelKGZ24,
	author = {Liana Patel and
                  Peter Kraft and
                  Carlos Guestrin and
                  Matei Zaharia},
	title = {{ACORN:} Performant and Predicate-Agnostic Search Over Vector Embeddings
                  and Structured Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {120},
	year = {2024},
	url = {https://doi.org/10.1145/3654923},
	doi = {10.1145/3654923},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/PatelKGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Applications increasingly leverage mixed-modality data, and must jointly search over vector data, such as embedded images, text and video, as well as structured data, such as attributes and keywords. Proposed methods for this hybrid search setting either suffer from poor performance or support a severely restricted set of search predicates (e.g., only small sets of equality predicates), making them impractical for many applications. To address this, we present ACORN, an approach for performant and predicate-agnostic hybrid search. ACORN builds on Hierarchical Navigable Small Worlds (HNSW), a state-of-the-art graph-based approximate nearest neighbor index, and can be implemented efficiently by extending existing HNSW libraries. ACORN introduces the idea of predicate subgraph traversal to emulate a theoretically ideal, but impractical, hybrid search strategy. ACORN's predicate-agnostic construction algorithm is designed to enable this effective search strategy, while supporting a wide array of predicate sets and query semantics. We systematically evaluate ACORN on both prior benchmark datasets, with simple, low-cardinality predicate sets, and complex multi-modal datasets not supported by prior methods. We show that ACORN achieves state-of-the-art performance on all datasets, outperforming prior methods with 2--1,000× higher throughput at a fixed recall. Our code is available at: https://github.com/stanford-futuredata/ACORN.}
}


@article{DBLP:journals/pacmmod/HanXHWWW24,
	author = {Xiaoyu Han and
                  Haoran Xiong and
                  Zhenying He and
                  Peng Wang and
                  Chen Wang and
                  X. Sean Wang},
	title = {Akane: Perplexity-Guided Time Series Data Cleaning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {121},
	year = {2024},
	url = {https://doi.org/10.1145/3654993},
	doi = {10.1145/3654993},
	timestamp = {Fri, 09 Aug 2024 16:05:02 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HanXHWWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dirty data are prevalent in time series, such as energy consumption or stock data. Existing data cleaning algorithms present shortcomings in dirty data identification and unsatisfactory cleaning decisions. To handle these drawbacks, we leverage inherent recurrent patterns in time series, analogize them as fixed combinations in textual data, and incorporate the concept of perplexity. The cleaning problem is thus transformed to minimize the perplexity of the time series under a given cleaning cost, and we design a four-phase algorithmic framework to tackle this problem. To ensure the framework's feasibility, we also conduct a brief analysis of the impact of dirty data and devise an automatic budget selection strategy. Moreover, to make it more generic, we additionally introduce advanced solutions, including an ameliorative probability calculation method grounded in the homomorphic pattern aggregation and a greedy-based heuristic algorithm for resource savings. Experiments on 12 real-world datasets demonstrate the superiority of our methods.}
}


@article{DBLP:journals/pacmmod/ChenHCFGZ0C24,
	author = {Sibei Chen and
                  Yeye He and
                  Weiwei Cui and
                  Ju Fan and
                  Song Ge and
                  Haidong Zhang and
                  Dongmei Zhang and
                  Surajit Chaudhuri},
	title = {Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive
                  Learning for Table Representations},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {122},
	year = {2024},
	url = {https://doi.org/10.1145/3654925},
	doi = {10.1145/3654925},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenHCFGZ0C24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spreadsheets are widely recognized as the most popular end-user programming tools, which blend the power of formula-based computation, with an intuitive table-based interface. Today, spreadsheets are used by billions of users to manipulate tables, most of whom are neither database experts nor professional programmers. Despite the success of spreadsheets, authoring complex formulas remains challenging, as non-technical users need to look up and understand non-trivial formula syntax. To address this pain point, we leverage the observation that there is often an abundance of similar-looking spreadsheets in the same organization, which not only have similar data, but also share similar computation logic encoded as formulas. We develop an Auto-Formula system that can accurately predict formulas that users want to author in a target spreadsheet cell, by learning and adapting formulas that already exist in similar spreadsheets, using contrastive-learning techniques inspired by "similar-face recognition" from compute vision. Extensive evaluations on over 2K test formulas extracted from real enterprise spreadsheets show the effectiveness of Auto-Formula over alternatives. Our benchmark data is available at https://github.com/microsoft/Auto-Formula to facilitate future research.}
}


@article{DBLP:journals/pacmmod/AbramovichDF0O24,
	author = {Omer Abramovich and
                  Daniel Deutch and
                  Nave Frost and
                  Ahmet Kara and
                  Dan Olteanu},
	title = {Banzhaf Values for Facts in Query Answering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {123},
	year = {2024},
	url = {https://doi.org/10.1145/3654926},
	doi = {10.1145/3654926},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AbramovichDF0O24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantifying the contribution of database facts to query answers has been studied as means of explanation. The Banzhaf value, originally developed in Game Theory, is a natural measure of fact contribution, yet its efficient computation for select-project-join-union queries is challenging. In this paper, we introduce three algorithms to compute the Banzhaf value of database facts: an exact algorithm, an anytime deterministic approximation algorithm with relative error guarantees, and an algorithm for ranking and top-k. They have three key building blocks: compilation of query lineage into an equivalent function that allows efficient Banzhaf value computation; dynamic programming computation of the Banzhaf values of variables in a Boolean function using the Banzhaf values for constituent functions; and a mechanism to compute efficiently lower and upper bounds on Banzhaf values for any positive DNF function. We complement the algorithms with a dichotomy for the Banzhaf-based ranking problem: given two facts, deciding whether the Banzhaf value of one is greater than of the other is tractable for hierarchical queries and intractable for non-hierarchical queries. We show experimentally that our algorithms significantly outperform exact and approximate algorithms from prior work, most times up to two orders of magnitude. Our algorithms can also cover challenging problem instances that are beyond reach for prior work.}
}


@article{DBLP:journals/pacmmod/YuGZTW024,
	author = {Qiaolin Yu and
                  Chang Guo and
                  Jay Zhuang and
                  Viraj Thakkar and
                  Jianguo Wang and
                  Zhichao Cao},
	title = {CaaS-LSM: Compaction-as-a-Service for LSM-based Key-Value Stores in
                  Storage Disaggregated Infrastructure},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {124},
	year = {2024},
	url = {https://doi.org/10.1145/3654927},
	doi = {10.1145/3654927},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuGZTW024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optimizing LSM-based Key-Value Stores (LSM-KVS) for disaggregated storage is essential to achieve better resource utilization, performance, and flexibility. Most of the existing studies focus on offloading the compaction to the storage nodes to mitigate the performance penalties caused by heavy network traffic between computing and storage. However, several critical issues are not addressed including the strong dependency between offloaded compaction and LSM-KVS, resource load-balancing, compaction scheduling, and complex transient errors. To address the aforementioned issues and limitations, in this paper, we propose CaaS-LSM, a novel disaggregated LSM-KVS with a new idea of Compaction-as-a-Service. CaaS-LSM brings three key contributions. First, CaaS-LSM decouples the compaction from LSM-KVS and achieves stateless execution to ensure high flexibility and avoid coordination overhead with LSM-KVS. Second, CaaS-LSM introduces a performance- and resource-optimized control plane to guarantee better performance and resource utilization via an adaptive run-time scheduling and management strategy. Third, CaaS-LSM addresses different levels of transient and execution errors via sophisticated error-handling logic. We implement the prototype of CaaS-LSM based on RocksDB and evaluate it with different LSM-based distributed databases (Kvrocks and Nebula). In the storage disaggregated setup, CaaS-LSM achieves up to 8X throughput improvement and reduces the P99 latency up to 98% compared with the conventional LSM-KVS, and up to 61% of improvement compared with state-of-the-art LSM-KVS optimized for disaggregated storage.}
}


@article{DBLP:journals/pacmmod/PaponCZA24,
	author = {Tarikul Islam Papon and
                  Taishan Chen and
                  Shuo Zhang and
                  Manos Athanassoulis},
	title = {{CAVE:} Concurrency-Aware Graph Processing on SSDs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {125},
	year = {2024},
	url = {https://doi.org/10.1145/3654928},
	doi = {10.1145/3654928},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/PaponCZA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale graph analytics has become increasingly common in areas like social networks, physical sciences, transportation networks, and recommendation systems. Since many such practical graphs do not fit in main memory, graph analytics performance depends on efficiently utilizing underlying storage devices. These out-of-core graph processing systems employ sharding and sub-graph partitioning to optimize for storage while relying on efficient sequential access of traditional hard disks. However, today's storage is increasingly based on solid-state drives (SSDs) that exhibit high internal parallelism and efficient random accesses. Yet, state-of-the-art graph processing systems do not explicitly exploit those properties, resulting in subpar performance. In this paper, we develop CAVE, the first graph processing engine that optimally exploits underlying SSD-based storage by harnessing the available storage device parallelism via carefully selecting which I/Os to graph data can be issued concurrently. Thus, CAVE traverses multiple paths and processes multiple nodes and edges concurrently, achieving parallelization at a granular level. We identify two key ways to parallelize graph traversal algorithms based on the graph structure and algorithm: intra-subgraph and inter-subgraph parallelization. The first identifies subgraphs that contain vertices that can be accessed in parallel, while the latter identifies subgraphs that can be processed in their entirety in parallel. To showcase the benefit of our approach, we build within CAVE parallelized versions of five popular graph algorithms (Breadth-First Search, Depth-First Search, Weakly Connected Components, PageRank, Random Walk) that exploit the full bandwidth of the underlying device. CAVE uses a blocked file format based on adjacency lists and employs a concurrent cache pool that is essential to the parallelization of graph algorithms. By experimenting with different types of graphs on three SSD devices, we demonstrate that CAVE utilizes the available parallelism, and scales to diverse real-world graph datasets. CAVE achieves up to one order of magnitude speedup compared to the popular out-of-core systems Mosaic and GridGraph, and up to three orders of magnitude speedup in runtime compared to GraphChi.}
}


@article{DBLP:journals/pacmmod/ZhenATC24,
	author = {Cheng Zhen and
                  Nischal Aryal and
                  Arash Termehchy and
                  Amandeep Singh Chabada},
	title = {Certain and Approximately Certain Models for Statistical Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {126},
	year = {2024},
	url = {https://doi.org/10.1145/3654929},
	doi = {10.1145/3654929},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhenATC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-world data is often incomplete and contains missing values. To train accurate models over real-world datasets, users need to spend a substantial amount of time and resources imputing and finding proper values for missing data items. In this paper, we demonstrate that it is possible to learn accurate models directly from data with missing values for certain training data and target models. We propose a unified approach for checking the necessity of data imputation to learn accurate models across various widely-used machine learning paradigms. We build efficient algorithms with theoretical guarantees to check this necessity and return accurate models in cases where imputation is unnecessary. Our extensive experiments indicate that our proposed algorithms significantly reduce the amount of time and effort needed for data imputation without imposing considerable computational overhead.}
}


@article{DBLP:journals/pacmmod/LiZLFZZWP0024,
	author = {Haoyang Li and
                  Jing Zhang and
                  Hanbing Liu and
                  Ju Fan and
                  Xiaokang Zhang and
                  Jun Zhu and
                  Renjie Wei and
                  Hongyan Pan and
                  Cuiping Li and
                  Hong Chen},
	title = {CodeS: Towards Building Open-source Language Models for Text-to-SQL},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {127},
	year = {2024},
	url = {https://doi.org/10.1145/3654930},
	doi = {10.1145/3654930},
	timestamp = {Sat, 20 Jul 2024 15:05:02 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiZLFZZWP0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique. We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications. The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks.}
}


@article{DBLP:journals/pacmmod/0007CLS024,
	author = {Wei Dong and
                  Zijun Chen and
                  Qiyao Luo and
                  Elaine Shi and
                  Ke Yi},
	title = {Continual Observation of Joins under Differential Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {128},
	year = {2024},
	url = {https://doi.org/10.1145/3654931},
	doi = {10.1145/3654931},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/0007CLS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of continual observation under differential privacy has been studied extensively in the literature. However, all existing works, with the exception of [28,51], have only studied the simple counting query and its derivatives. Join queries, which are arguably the most important class of queries in relational databases, have only been considered in [28,51], but the solutions offered there have two limitations: First, they only support a few specific graph pattern queries, which are special cases of joins. Second, they require hard degree/frequency constraints on the graph/database instance, and the privatized query answers have errors proportional to these constraints. In this paper, we propose a new differentially private mechanism for continual observation of joins that overcomes these two limitations. Our mechanism supports arbitrary joins and predicates, and do not require any constraints to be given in advance, even over an infinite stream. More importantly, it yields an error that is proportional to the actual maximum degree/frequencies in the graph/database instance at the current time of observation. Such an instance-specific utility guarantee is much preferred for the continual observation problem, where the database size and the query answer may change significantly over time.}
}


@article{DBLP:journals/pacmmod/HeddesNGN24,
	author = {Mike Heddes and
                  Igor Nunes and
                  Tony Givargis and
                  Alex Nicolau},
	title = {Convolution and Cross-Correlation of Count Sketches Enables Fast Cardinality
                  Estimation of Multi-Join Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {129},
	year = {2024},
	url = {https://doi.org/10.1145/3654932},
	doi = {10.1145/3654932},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HeddesNGN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing rate of data generated by critical systems, estimating functions on streaming data has become essential. This demand has driven numerous advancements in algorithms designed to efficiently query and analyze one or more data streams while operating under memory constraints. The primary challenge arises from the rapid influx of new items, requiring algorithms that enable efficient incremental processing of streams in order to keep up. A prominent algorithm in this domain is the AMS sketch. Originally developed to estimate the second frequency moment of a data stream, it can also estimate the cardinality of the equi-join between two relations. Since then, two important advancements are the Count sketch, a method which significantly improves upon the sketch update time, and secondly, an extension of the AMS sketch to accommodate multi-join queries. However, combining the strengths of these methods to maintain sketches for multi-join queries while ensuring fast update times is a non-trivial task, and has remained an open problem for decades as highlighted in the existing literature. In this work, we successfully address this problem by introducing a novel sketching method which has fast updates, even for sketches capable of accurately estimating the cardinality of complex multi-join queries. We prove that our estimator is unbiased and has the same error guarantees as the AMS-based method. Our experimental results confirm the significant improvement in update time complexity, resulting in orders of magnitude faster estimates, with equal or better estimation accuracy.}
}


@article{DBLP:journals/pacmmod/An024,
	author = {Shuai An and
                  Yang Cao},
	title = {Counterfactual Explanation at Will, with Zero Privacy Leakage},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {130},
	year = {2024},
	url = {https://doi.org/10.1145/3654933},
	doi = {10.1145/3654933},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/An024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While counterfactuals have been extensively studied as an intuitive explanation of model predictions, they still have limited adoption in practice due to two obstacles: (a) They rely on excessive access to the model for explanation that the model owner may not provide; and (b) counterfactuals carry information that adversarial users can exploit to launch model extraction attacks. To address the challenges, we propose CPC, a data-driven approach to counterfactual. CPC works at the client side and gives full control and right-to-explain to model users, even when model owners opt not to. Moreover, CPC warrants that adversarial users cannot exploit counterfactuals to extract models. We formulate properties and fundamental problems underlying CPC, study their complexity and develop effective algorithms. Using real-world datasets and user study, we verify that CPC does prevent adversaries from exploiting counterfactuals for model extraction attacks, and is orders of magnitude faster than existing explainers, while maintaining comparable and often higher quality.}
}


@article{DBLP:journals/pacmmod/00060K24,
	author = {Yifan Li and
                  Xiaohui Yu and
                  Nick Koudas},
	title = {Data Acquisition for Improving Model Confidence},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {131},
	year = {2024},
	url = {https://doi.org/10.1145/3654934},
	doi = {10.1145/3654934},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/00060K24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, there has been a growing recognition that high-quality training data is crucial for the performance of machine learning models. This awareness has catalyzed both research endeavors and industrial initiatives dedicated to data acquisition to enhance diverse dimensions of model performance. Among these dimensions, model confidence holds paramount importance; however, it has often been overlooked in prior investigations into data acquisition methodologies. To address this gap, our work focuses on improving the data acquisition process with the goal of enhancing the confidence of Machine Learning models. Specifically, we operate within a practical context where limited samples can be obtained from a large data pool. We employ well-established model confidence metrics as our foundation, and we propose two methodologies, Bulk Acquisition (BA) and Sequential Acquisition (SA), each geared towards identifying the sets of samples that yield the most substantial gains in model confidence. Recognizing the complexity of BA and SA, we introduce two efficient approximate methods, namely kNN-BA and kNN-SA, restricting data acquisition to promising subsets within the data pool. To broaden the applicability of our solutions, we introduce a Distribution-based Acquisition approach that makes minimal assumption regarding the data pool and facilitates the data acquisition across various settings. Through extensive experimentation encompassing diverse datasets, models, and parameter configurations, we demonstrate the efficacy of our proposed methods across a range of tasks. Comparative experiments with alternative applicable baselines underscore the superior performance of our proposed approaches.}
}


@article{DBLP:journals/pacmmod/AkiliP024,
	author = {Samira Akili and
                  Steven Purtzel and
                  Matthias Weidlich},
	title = {DecoPa: Query Decomposition for Parallel Complex Event Processing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {132},
	year = {2024},
	url = {https://doi.org/10.1145/3654935},
	doi = {10.1145/3654935},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/AkiliP024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Systems for Complex Event Processing (CEP) enable the detection of predefined patterns in event streams. While the evaluation of CEP queries is computationally hard, scalability may be achieved by parallelization. Yet, existing approaches for parallel CEP are driven by static query properties, such as partitioning keys and states of the evaluation model. They largely neglect the rates with which processing units may ingest and compare events for query evaluation. In this paper, we present an approach for parallel CEP that is based on a flexible decomposition of CEP queries. Our idea is to guide the decomposition by the sustainable throughput of each processing unit, in order to maximize the overall performance. To this end, we introduce DecoPa plans for parallel CEP, provide a cost model for them, elaborate on their correctness and optimality, and present an algorithm for their construction. Experiments using a DecoPa implementation in Flink illustrate throughput gains of up to 12 orders of magnitude compared to state-of-the-art approaches.}
}


@article{DBLP:journals/pacmmod/LiaoZLDCW24,
	author = {Meihao Liao and
                  Junjie Zhou and
                  Rong{-}Hua Li and
                  Qiangqiang Dai and
                  Hongyang Chen and
                  Guoren Wang},
	title = {Efficient and Provable Effective Resistance Computation on Large Graphs:
                  An Index-based Approach},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {133},
	year = {2024},
	url = {https://doi.org/10.1145/3654936},
	doi = {10.1145/3654936},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiaoZLDCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Effective resistance (ER) is a fundamental metric for measuring node similarities in a graph, and it finds applications in various domains including graph clustering, recommendation systems, link prediction, and graph neural networks. The state-of-the-art algorithm for computing effective resistance relies on a landmark technique, which involves selecting a node that is easy to reach by all the other nodes as a landmark. The performance of this technique heavily depends on the chosen landmark node. However, in many real-life graphs, it is not always possible to find an easily reachable landmark node, which can significantly hinder the algorithm's efficiency. To overcome this problem, we propose a novel multiple landmarks technique which involves selecting a set of landmark nodes V l  such that the other nodes in the graph can easily reach any one of a landmark node in V l . Specifically, we first propose several new formulas to compute ER with multiple landmarks, utilizing the concept of Schur complement. These new formulas allow us to pre-compute and maintain several small-sized matrices related to V l  as a compact index. With this powerful index technique, we demonstrate that both single-pair and single-source ER queries can be efficiently answered using a newly-developed V l -absorbed random walk sampling or V l -absorbed push technique. Comprehensive theoretical analysis shows that all proposed index-based algorithms achieve provable performance guarantees for both single-pair and single-source ER queries. Extensive experiments on 5 real-life datasets demonstrate the high efficiency of our multiple landmarks-based index techniques. For instance, our algorithms, with a 1.5 GB index size, can be up to 4 orders of magnitude faster than the state-of-the-art algorithms while achieving the same accuracy on a large road network.}
}


@article{DBLP:journals/pacmmod/XiaZ24,
	author = {Haisong Xia and
                  Zhongzhi Zhang},
	title = {Efficient Approximation of Kemeny's Constant for Large Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {134},
	year = {2024},
	url = {https://doi.org/10.1145/3654937},
	doi = {10.1145/3654937},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/XiaZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For an undirected graph, its Kemeny's constant is defined as the mean hitting time of random walks from one vertex to another chosen randomly according to the stationary distribution. Kemeny's constant exhibits numerous explanations from different perspectives and has found various applications in the field of complex networks. Due to the requirement of computing the inverse of the normalized Laplacian matrix, it is infeasible to get the accurate Kemeny's constant of large networks with millions of vertices. Existing methods either consume excessive memory space that are impractical for large-scale networks, or involve redundant simulation, leaving room for further optimization. In this paper, we propose two scalable Monte Carlo algorithms RefinedMC and ForestMC to approximate Kemeny's constant. RefinedMC makes several refinements based on the simulation of truncated random walks, significantly reducing the amount of required random walks, while ForestMC utilizes the newly discovered paradigm connecting Kemeny's constant with the inverse of corresponding Laplacian submatrix, which is considerably accurate. Extensive numerical experiments on model and realistic networks demonstrate that our approximation algorithms evidently outperform the baseline methods in terms of efficiency and accuracy.}
}


@article{DBLP:journals/pacmmod/DaiLCLQW24,
	author = {Qiangqiang Dai and
                  Rong{-}Hua Li and
                  Donghang Cui and
                  Meihao Liao and
                  Yu{-}Xuan Qiu and
                  Guoren Wang},
	title = {Efficient Maximal Biplex Enumerations with Improved Worst-Case Time
                  Guarantee},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {135},
	year = {2024},
	url = {https://doi.org/10.1145/3654938},
	doi = {10.1145/3654938},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DaiLCLQW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A k-biplex is an induced subgraph of a bipartite graph which requires every vertex on the one side disconnecting at most k vertices on the other side. Enumerating all maximal k-biplexes in a bipartite graph is a fundamental operator in bipartite graph analysis and finds applications in various domains, including community detection, online recommendation, and fraud detection in finance networks. The state-of-the-art solutions for maximal k-biplex enumeration suffer from efficiency issues as k increases (k ≥ 2), with the time complexity of O(m 2 n ), where n (m) denotes the number of vertices (edges) in the bipartite graph. To address this issue, we propose two theoretically and practically efficient enumeration algorithms based on novel branching techniques. Specifically, we first devise a new branching rule as a fundamental component. Building upon this, we then develop a novel branch-and-bound enumeration algorithm to efficiently enumerate maximal k-biplexes. We prove that our algorithm achieves a worst-case time complexity of O(mα k n ), where α k  < 2, thus significantly improving the time complexity compared to previous algorithms. To enhance the performance, we further propose an improved enumeration algorithm based on a novel pivot-based branching rule. Theoretical analysis reveals that our improved algorithm has a time complexity of O(mβ k n ), where β k  is strictly less than α k . In addition, we also present several non-trivial optimization techniques, including graph reduction, upper-bounds based pruning, and ordering-based optimization, to further improve the efficiency of our algorithms. Finally, we conduct extensive experiments on 6 large real-world bipartite graphs to evaluate the efficiency and scalability of the proposed solutions. The results demonstrate that our improved algorithm achieves up to 5 orders of magnitude faster than the state-of-the-art solutions.}
}


@article{DBLP:journals/pacmmod/ShahbaziSA24,
	author = {Nima Shahbazi and
                  Stavros Sintos and
                  Abolfazl Asudeh},
	title = {FairHash: {A} Fair and Memory/Time-efficient Hashmap},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {136},
	year = {2024},
	url = {https://doi.org/10.1145/3654939},
	doi = {10.1145/3654939},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShahbaziSA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hashmap is a fundamental data structure in computer science. There has been extensive research on constructing hashmaps that minimize the number of collisions leading to efficient lookup query time. Recently, the data-dependant approaches, construct hashmaps tailored for a target data distribution that guarantee to uniformly distribute data across different buckets and hence minimize the collisions. Still, to the best of our knowledge, none of the existing technique guarantees group fairness among different groups of items stored in the hashmap. Therefore, in this paper, we introduce FairHash, a data-dependant hashmap that guarantees uniform distribution at the group-level across hash buckets, and hence, satisfies the statistical parity notion of group fairness. We formally define, three notions of fairness and, unlike existing work, FairHash satisfies all three of them simultaneously. We propose three families of algorithms to design fair hashmaps, suitable for different settings. Our ranking-based algorithms reduce the unfairness of data-dependant hashmaps without any memory-overhead. The cut-based algorithms guarantee zero-unfairness in all cases, irrespective of how the data is distributed, but those introduce an extra memory-overhead. Last but not least, the discrepancy-based algorithms enable trading off between various fairness notions. In addition to the theoretical analysis, we perform extensive experiments to evaluate the efficiency and efficacy of our algorithms on real datasets. Our results verify the superiority of FairHash compared to the other baselines on fairness at almost no performance cost.}
}


@article{DBLP:journals/pacmmod/KurkureSWGS24,
	author = {Yash Kurkure and
                  Miles Shamo and
                  Joseph Wiseman and
                  Sainyam Galhotra and
                  Stavros Sintos},
	title = {Faster Algorithms for Fair Max-Min Diversification in R\({}^{\mbox{d}}\)},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {137},
	year = {2024},
	url = {https://doi.org/10.1145/3654940},
	doi = {10.1145/3654940},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/KurkureSWGS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The task of extracting a diverse subset from a dataset, often referred to as maximum diversification, plays a pivotal role in various real-world applications that have far-reaching consequences. In this work, we delve into the realm of fairness-aware data subset selection, specifically focusing on the problem of selecting a diverse set of size k from a large collection of n data points (FairDiv). The FairDiv problem is well-studied in the data management and theory community. In this work, we develop the first constant approximation algorithm for FairDiv that runs in near-linear time using only linear space. In contrast, all previously known constant approximation algorithms run in super-linear time (with respect to n or k) and use super-linear space. Our approach achieves this efficiency by employing a novel combination of the Multiplicative Weight Update method and advanced geometric data structures to implicitly and approximately solve a linear program. Furthermore, we improve the efficiency of our techniques by constructing a coreset. Using our coreset, we also propose the first efficient streaming algorithm for the FairDiv problem whose efficiency does not depend on the distribution of data points. Empirical evaluation on million-sized datasets demonstrates that our algorithm achieves the best diversity within a minute. All prior techniques are either highly inefficient or do not generate a good solution.}
}


@article{DBLP:journals/pacmmod/KozarMZM24,
	author = {Anastasiia Kozar and
                  Bonaventura Del Monte and
                  Steffen Zeuch and
                  Volker Markl},
	title = {Fault Tolerance Placement in the Internet of Things},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {138},
	year = {2024},
	url = {https://doi.org/10.1145/3654941},
	doi = {10.1145/3654941},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KozarMZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today's IoT applications exploit the capabilities of three different computation environments: sensors, edge, and cloud. Ensuring fault tolerance at the edge level presents unique challenges due to complex network hierarchies and the presence of resource-constrained computing devices. In contrast to the Cloud, the Edge lacks high availability standards and a persistent upstream backup. To ensure reliability, fault tolerance mechanisms have to be deployed on the edge devices along with processing operators competing for available resources. However, existing operator placement strategies are not aware of fault tolerance resource requirements, and existing fault tolerance approaches are not aware of available resources. This miscommunication in resource-constrained environments like the Edge leads to underprovisioning and failures. In this paper, we present a resource-aware fault-tolerance approach that takes the unique characteristics of the Edge into account to provide reliable stream processing. To this end, we model fault tolerance as an operator placement problem that uses multi-objective optimization to decide where to backup data. As opposed to existing approaches that treat operator placement and fault tolerance as two separate steps, we combine them and showcase that this is especially important for low-end edge devices. Overall, our approach effectively mitigates potential failures and outperforms state-of-the-art fault tolerance approaches by up to an order of magnitude in throughput.}
}


@article{DBLP:journals/pacmmod/ZhangWQLL24,
	author = {Tianping Zhang and
                  Zhaoyang Wang and
                  Chen Qian and
                  Jian Li and
                  Yin Lou},
	title = {FeatureLTE: Learning to Estimate Feature Importance},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {139},
	year = {2024},
	url = {https://doi.org/10.1145/3654942},
	doi = {10.1145/3654942},
	timestamp = {Thu, 08 Aug 2024 16:16:26 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangWQLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature importance scores (FIS) estimation is an important problem in many data-intensive applications. Traditional approaches can be divided into two types; model-specific methods and model-agnostic methods. In this work, we present FeatureLTE, a novel learning-based approach to FIS estimation. For the first time, as we demonstrate through extensive experiments, it is possible to build general-purpose pre-trained models for FIS estimation. Therefore, FIS estimation reduces to prediction outputs from a pre-trained FeatureLTE model. Pre-trained FeatureLTE models enjoy several desired advantages, including accuracy, robustness, efficiency, and evolvability, and FeatureLTE models really begin to shine on large datasets where traditional methods often find themselves unable to scale. We build our pre-trained models for binary classification and regression problems using observations from nearly 1,000 public datasets. We systematically evaluate various design choices of FeatureLTE model construction and carefully design meta features to make sure that they are computationally lightweight. Based on our evaluation, FeatureLTE is on par with the best existing FIS estimators in terms of FIS quality, and achieves up to 339.48x speedup without sacrificing the quality of FIS estimates on large-scale datasets. Finally, we release two pre-trained FeatureLTE models for binary classification and regression problems that are ready to use on almost all tabular datasets, along with the repository of 701 binary classification datasets and 256 regression datasets with pre-computed feature importance scores to promote future research along this direction.}
}


@article{DBLP:journals/pacmmod/Chu0ZZ024,
	author = {Deming Chu and
                  Fan Zhang and
                  Wenjie Zhang and
                  Ying Zhang and
                  Xuemin Lin},
	title = {Graph Summarization: Compactness Meets Efficiency},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {140},
	year = {2024},
	url = {https://doi.org/10.1145/3654943},
	doi = {10.1145/3654943},
	timestamp = {Mon, 29 Jul 2024 07:57:21 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Chu0ZZ024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the volume and ubiquity of graphs increase, a compact graph representation becomes essential for enabling efficient storage, transfer, and processing of graphs. Given a graph, the graph summarization problem asks for a compact representation that consists of a summary graph and the corrections, such that we can recreate the original graph from the representation exactly. Although this problem has been studied extensively, the existing works either trade summary compactness for efficiency, or vice versa. In particular, a well-known greedy method provides the most compact summary but incurs prohibitive time cost, while the state-of-the-art algorithms with practical overheads are more than 20% behind in summary compactness in our comparison with the greedy method. This paper presents Mags and Mags-DM, two algorithms that aim to bridge the compactness and efficiency in graph summarization. Mags adopts the existing greedy paradigm that provides state-of-the-art compactness, but significantly improves its efficiency with a novel algorithm design. Meanwhile, Mags-DM follows a different paradigm with practical efficiency and overcomes its limitations in compactness. Moreover, both algorithms can support parallel computing environments. We evaluate Mags and Mags-DM on graphs up to billion-scale and demonstrate that they achieve state-of-the-art in both compactness and efficiency, rather than in one of them. Compared with the method that offers state-of-the-art compactness, Mags and Mags-DM have a small difference (< 0.1% and < 2.1%) in compactness. For efficiency, Mags is on average 11.1x and 4.2x faster than the two state-of-the-art algorithms with practical overheads, while Mags-DM can further reduce the running time by 13.4x compared with Mags. This shows that graph summarization algorithms can be made practical while still offering a compact summary.}
}


@article{DBLP:journals/pacmmod/WangGYZ24,
	author = {Hengrui Wang and
                  Te Guo and
                  Junzhao Yang and
                  Huanchen Zhang},
	title = {{GRF:} {A} Global Range Filter for LSM-Trees with Shape Encoding},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {141},
	year = {2024},
	url = {https://doi.org/10.1145/3654944},
	doi = {10.1145/3654944},
	timestamp = {Thu, 27 Mar 2025 15:41:37 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangGYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log-structured merge-trees (LSM-trees) are widely used in key-value stores because of its excellent write performance. To reduce LSM-tree's read amplification due to overlapping sorted runs, each file (i.e., SSTable) in an LSM-tree is typically associated with a point or range filter to reduce unnecessary I/Os to the runs that do not contain the target key (range). However, as modern SSDs get faster, probing multiple in-memory filters per query often makes the system CPU bottlenecked, thus compromising the system's throughput. In this paper, we developed the Global Range Filter (GRF) for RocksDB that reduces the number of filter probes per query to one. We follow the pioneering Chucky's approach by storing the sorted run IDs within the filter. However, we identify two practical challenges in building a global range filter: correctness in multi-version concurrency control and efficiency in frequent updates. We solve both challenges by the novel Shape Encoding algorithm. With further optimizations, GRF achieves a dominating performance over the state-of-the-art filters under different workloads when integrated into RocksDB.}
}


@article{DBLP:journals/pacmmod/ZhuMZK0G24,
	author = {Yifan Zhu and
                  Ruiyao Ma and
                  Baihua Zheng and
                  Xiangyu Ke and
                  Lu Chen and
                  Yunjun Gao},
	title = {{GTS:} GPU-based Tree Index for Fast Similarity Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {142},
	year = {2024},
	url = {https://doi.org/10.1145/3654945},
	doi = {10.1145/3654945},
	timestamp = {Mon, 20 Jan 2025 08:41:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhuMZK0G24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Similarity search, the task of identifying objects most similar to a given query object under a specific metric, has gathered significant attention due to its practical applications. However, the absence of coordinate information to accelerate similarity search and the high computational cost of measuring object similarity hinder the efficiency of existing CPU-based methods. Additionally, these methods struggle to meet the demand for high throughput data management. To address these challenges, we propose GTS, a GPU-based tree index designed for the parallel processing of similarity search in general metric spaces, where only the distance metric for measuring object similarity is known. The GTS index utilizes a pivot-based tree structure to efficiently prune objects and employs list tables to facilitate GPU computing. To efficiently manage concurrent similarity queries with limited GPU memory, we have developed a two-stage search method that combines batch processing and sequential strategies to optimize memory usage. The paper also introduces an effective update strategy for the proposed GPU-based index, encompassing streaming data updates and batch data updates. Additionally, we present a cost model to evaluate search performance. Extensive experiments on five real-life datasets demonstrate that GTS achieves efficiency gains of up to two orders of magnitude over existing CPU baselines and up to 20x efficiency improvements compared to state-of-the-art GPU-based methods.}
}


@article{DBLP:journals/pacmmod/Zhu0SY24,
	author = {Jingyu Zhu and
                  Yu Sun and
                  Shaoxu Song and
                  Xiaojie Yuan},
	title = {High Precision {\(\not =\)} High Cost: Temporal Data Fusion for Multiple
                  Low-Precision Sensors},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {143},
	year = {2024},
	url = {https://doi.org/10.1145/3654946},
	doi = {10.1145/3654946},
	timestamp = {Fri, 19 Jul 2024 23:17:57 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Zhu0SY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-quality data are crucial for practical applications, but obtaining them through high-precision sensors comes at a high cost. To guarantee the trade-off between cost and precision, we may use multiple low-precision sensors to obtain the nearly accurate data fusion results at an affordable cost. The commonly used techniques, such as the Kalman filter and truth discovery methods, typically compute fusion values by combining all the observations according to predictions or sensor reliability. However, low-precision sensors can often cause outliers, and such methods combining all observations are susceptible to interference. To handle this problem, we select a single observation from multiple sensor readings as the fusion result for each timestamp. The selection strategy is guided by the maximum likelihood estimation, to determine the most probable changing trends of fusion results with adjacent timestamps. Our major contributions include (1) the problem formalization and NP-hardness analysis on finding the fusion result with the maximum likelihood w.r.t. local fusion models, (2) exact algorithms based on dynamic programming for tackling the problem, (3) efficient approximation methods with performance guarantees. Experiments on various real datasets and downstream applications demonstrate the superiority and practicality of our work in low-precision sensor data fusion.}
}


@article{DBLP:journals/pacmmod/LiCZC0WL024,
	author = {Anran Li and
                  Yuanyuan Chen and
                  Jian Zhang and
                  Mingfei Cheng and
                  Yihao Huang and
                  Yueming Wu and
                  Anh Tuan Luu and
                  Han Yu},
	title = {Historical Embedding-Guided Efficient Large-Scale Federated Graph
                  Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {144},
	year = {2024},
	url = {https://doi.org/10.1145/3654947},
	doi = {10.1145/3654947},
	timestamp = {Fri, 07 Mar 2025 20:53:43 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiCZC0WL024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph convolutional networks (GCNs) are promising for graph learning tasks. For privacy-preserving graph learning tasks involving distributed graph datasets, federated learning (FL)-based GCN (FedGCN) training is required. An important open challenge for FedGCN is scaling to large graphs, which typically incurs 1) high computation overhead for handling the explosively-increasing number of neighbors, and 2) high communication overhead of training GCNs involving multiple FL clients. Thus, neighbor sampling is being studied to enhance the scalability of FedGCNs. Existing FedGCN training techniques with neighbor sampling often produce extremely large communication and computation overhead and inaccurate node embeddings, leading to poor model performance. To bridge this gap, we propose the <u>Fed</u>erated <u>A</u>daptive <u>A</u>ttention-based <u>S</u>ampling (FedAAS) approach. It achieves substantial cost savings by efficiently leveraging historical embedding estimators and focusing the limited communication resources on transmitting the most influential neighbor node embeddings across FL clients. We further design an adaptive embedding synchronization scheme to optimize the efficiency and accuracy of FedAAS on large-scale datasets. Theoretical analysis shows that the approximation error induced by the staleness of historical embedding is upper bounded, and the model is guaranteed to converge in an efficient manner. Extensive experimental evaluation against four state-of-the-art baselines on six real-world graph datasets show that FedAAS achieves up to 5.12% higher test accuracy, while saving communication and computation costs by 95.11% and 94.76%, respectively.}
}


@article{DBLP:journals/pacmmod/ZhangQYB24,
	author = {Shunkang Zhang and
                  Ji Qi and
                  Xin Yao and
                  Andr{\'{e}} Brinkmann},
	title = {Hyper: {A} High-Performance and Memory-Efficient Learned Index via
                  Hybrid Construction},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {145},
	year = {2024},
	url = {https://doi.org/10.1145/3654948},
	doi = {10.1145/3654948},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangQYB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learned indexes use machine learning techniques to improve index construction. However, they often face a fundamental trade-off between performance and memory consumption, especially in dynamic environments with frequent insert and delete operations. This trade-off stems from the construction approaches used in learned indexes: The top-down approach increases performance at the cost of significant memory overhead, while the bottom-up approach focuses on memory efficiency but introduces performance issues due to prediction errors. % A unified solution that simultaneously optimizes performance and memory consumption in dynamic data management scenarios is therefore highly desirable. We propose Hyper, a highly efficient learned index with a novel two-phase hybrid construction approach. Our approach combines bottom-up construction for leaf nodes with top-down construction for inner nodes to achieve an optimal balance between performance and memory consumption. Hyper effectively handles concurrent writes and structure adjustments without sacrificing query performance. We evaluated Hyper on both simple and complex real-world datasets and compared it to seven state-of-the-art learned indexes and several traditional data structures for dynamic workloads. The evaluation results show that Hyper achieves a remarkable performance boost of up to 3.75× with significantly reduced index memory consumption of up to 1610× in the single-thread evaluation. In high concurrency scenarios, Hyper even achieves improvements up to 5.73×, 3.72×, and 3.99× in read-only, read-write, and write-only workloads.}
}


@article{DBLP:journals/pacmmod/HanI24,
	author = {Soonbo Han and
                  Zachary G. Ives},
	title = {Implementation Strategies for Views over Property Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {146},
	year = {2024},
	url = {https://doi.org/10.1145/3654949},
	doi = {10.1145/3654949},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HanI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The need to query complex interactions and relationships has motivated interest in property graph database platforms. For some graph applications, graph views are required to abstract the data, e.g., to capture individual-level vs. organization-level relationships; or show single computational steps vs. composite workflows. Emerging efforts to standardize graph query languages have developed semantics and language constructs for graph views. This paper considers the task of implementing such views using rewriting techniques --- both using existing property graph DBMSs and converting to relational RDBMSs. We consider both virtual and materialized views, ways of rewriting queries, and structures for indexing data. We also note a common use case of graph views, which involves preserving a graph except minor local transformations; we develop novel extensions and semantics for this. We evaluate and compare the performance of our techniques under a variety of workloads, and we compare existing graph and relational DBMS platforms.}
}


@article{DBLP:journals/pacmmod/Lee0LH24,
	author = {Yukyoung Lee and
                  Kyoungmin Kim and
                  Wonseok Lee and
                  Wook{-}Shin Han},
	title = {In-depth Analysis of Continuous Subgraph Matching in a Common Delta
                  Query Compilation Framework},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {147},
	year = {2024},
	url = {https://doi.org/10.1145/3654950},
	doi = {10.1145/3654950},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Lee0LH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The continuous subgraph matching (CSM) problem aims to continuously detect patterns on a dynamic graph, with real-world applications such as fraud detection. Numerous methods have been proposed to address CSM, yet they lack fair comparisons. Furthermore, an existing unified framework for CSM shows misleading experimental results due to its suboptimal implementations. In this paper, we propose a new framework that generates CSM code from the logical and physical plans of delta queries with stacked views. By expressing each CSM method as a delta query plan, our framework enables fair comparisons of CSM methods. Through our comprehensive experiments, we make cause-and-effect arguments for the divergent performance trends from the previous papers and further analyze the individual impacts of various techniques on overall performance. Specifically, our CSM code for an old method significantly outperforms the most recent CSM method, CaLiG, by up to 48.6 times.}
}


@article{DBLP:journals/pacmmod/LiL0WD24,
	author = {Jiajun Li and
                  Runlin Lei and
                  Sibo Wang and
                  Zhewei Wei and
                  Bolin Ding},
	title = {Learning-based Property Estimation with Polynomials},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {148},
	year = {2024},
	url = {https://doi.org/10.1145/3654994},
	doi = {10.1145/3654994},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiL0WD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of estimating data properties using sampling frequency histograms has attracted extensive interest in the area of databases. The properties include the number of distinct values (NDV), entropy, and so on. In the field of databases, property estimation is fundamental to complex applications. For example, NDV estimation is the foundation of query optimization, and entropy estimation is the foundation of data compression. Among them, methods originating from statistics exhibit desirable theoretical guarantees but rely on specific assumptions about the distribution of data, resulting in poor performance in real-world applications. Learning-based methods, which use information from training data, are adaptable in the real world but often lack theoretical guarantees or explainability. In addition, a unified framework for estimating these frequency-based estimators with machine learning is lacking. Given the aforementioned challenges, it is natural to wonder if a unified framework with theoretical guarantees can be established for property estimation. The recent literature has presented theoretical studies that propose estimation frameworks based on polynomials. These studies also prove estimation errors with respect to the sample size. Motivated by the above polynomial estimation framework, we propose a learning-based estimation framework with polynomial approximation, which aims to learn the coefficients of the polynomial, providing theoretical guarantees to the learning framework. Through comprehensive experiments on both synthetic and real-world datasets for estimating various data properties like NDV, entropy, and power sum, our results show the superiority of our algorithms over previous estimators.}
}


@article{DBLP:journals/pacmmod/GlazeMZGSBK24,
	author = {Nick Glaze and
                  Tria McNeely and
                  Yiwen Zhu and
                  Matthew Gleeson and
                  Helen Serr and
                  Rajeev Bhopi and
                  Subru Krishnan},
	title = {Lorentz: Learned {SKU} Recommendation Using Profile Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {149},
	year = {2024},
	url = {https://doi.org/10.1145/3654952},
	doi = {10.1145/3654952},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GlazeMZGSBK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In response to diverse demands, cloud operators have significantly expanded the array of service offerings, often referred to as Stock Keeping Units (SKUs) available for computing resource configurations. Such diversity has led to increased complexity for customers to choose the appropriate SKU. In the analyzed system, only 43% of the resource capacity was rightly chosen. Although various automated solutions have attempted to resolve this issue, they often rely on the availability of enriched data, such as workload traces, which are unavailable for newly established services. Since these services amass a substantial volume of telemetry from existing users, cloud operators can leverage this information to better understand customer needs and mitigate the risk of over- or under-provisioning. Furthermore, customer satisfaction feedback serves as a crucial resource for continuous learning and improving the recommendation mechanism. In this paper, we present Lorentz, an intelligent SKU recommender for provisioning new compute resources that circumvents the need for workload traces. Lorentz leverages customer profile data to forecast resource capacities for new users based on detailed profiling of existing users. Furthermore, using a continuous learned feedback loop, Lorentz tailors capacity recommendations according to customer performance vs. cost preferences captured through satisfaction signals. Validated using the production data from provisioned VMs supporting Database Platform X, we demonstrate that Lorentz outperforms user selections and existing defaults, reducing slack by >60% without increasing throttling. Evaluated using synthetic data, Lorentz's personalization stage iteratively learns the user preferences over time with high accuracy.}
}


@article{DBLP:journals/pacmmod/WangZZSLH24,
	author = {Qihang Wang and
                  Decheng Zuo and
                  Zhan Zhang and
                  Yanjun Shu and
                  Xin Liu and
                  Mingxuan He},
	title = {Low-Latency Adaptive Distributed Stream Join System Based on a Flexible
                  Join Model},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {150},
	year = {2024},
	url = {https://doi.org/10.1145/3654953},
	doi = {10.1145/3654953},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangZZSLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stream join is a fundamental operation in stream processing and has attracted extensive research due to its large resource consumption and serious impact on system performance. As the theoretical basis of stream join systems, the stream join model greatly affects system performance. State-of-the-art stream join models either consume too much computing resources or too much storage resources, thus resulting in lower throughput or higher latency. In this paper, we propose a new stream join model for processing arbitrary join predicates, called CoModel, which offers a flexible trade-off between memory and computing resource consumption. More importantly, CoModel can achieve the minimum sum of the number of store operations and join operations among all existing join models, and thus can achieve the lowest latency and highest throughput when the overheads associated with the local stream join for each input tuple are approximately constant. We give a trade-off strategy for CoModel and theoretically prove its performance advantages based on queuing theory. Furthermore, we design and implement an adaptive distributed stream join system, CoStream, based on CoModel. CoStream can adaptively adjust its structure according to resource constraints and statistics of input data. We conduct extensive experiments for CoStream to evaluate its performance and adaptivity, and the results show that CoStream has the lowest latency and highest throughput in various scenarios.}
}


@article{DBLP:journals/pacmmod/ZhangSZ24,
	author = {Jiaoyi Zhang and
                  Kai Su and
                  Huanchen Zhang},
	title = {Making In-Memory Learned Indexes Efficient on Disk},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {151},
	year = {2024},
	url = {https://doi.org/10.1145/3654954},
	doi = {10.1145/3654954},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangSZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learned indexes have been demonstrated to outperform traditional ones in memory-resident scenarios. However, recent studies show that they fail to outperform B+tree when extended to disks directly. In this paper, we argue that it is feasible to create efficient disk-based learned indexes by applying a set of general transformations and optimizations to existing in-memory ones. Through theoretical analysis and controlled experiments, we propose six transformation guidelines applicable to various state-of-the-art learned index structures to fully leverage the characteristics of disk storage. Our evaluation shows that the indexes developed by applying our guidelines achieve a Pareto improvement in both throughput and space efficiency compared to the traditional B+tree and previous implementations of disk-based learned indexes.}
}


@article{DBLP:journals/pacmmod/Pang0YY24,
	author = {Yue Pang and
                  Lei Zou and
                  Jeffrey Xu Yu and
                  Linglin Yang},
	title = {Materialized View Selection {\&} View-Based Query Planning for
                  Regular Path Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {152},
	year = {2024},
	url = {https://doi.org/10.1145/3654955},
	doi = {10.1145/3654955},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Pang0YY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A regular path query (RPQ) returns node pairs connected by a path whose edge label sequence satisfies the given regular expression. Given a workload of RPQs, selecting the shared subqueries as materialized views to precompute offline can speed up the online processing. Since the available memory is limited, we define the materialized view selection (MVS) problem for RPQs as minimizing the total workload query cost within a memory budget. To tackle the problem's NP-hardness, we design an efficient MVS algorithm based on heuristics. To prevent redundancies in the selected views, we devise the AND-OR directed acyclic graph with closure (AODC) as the multi-RPQ query plan representation for the workload, which encodes the relations between subqueries. In addition to detecting view redundancy, the AODC also incrementally updates itself during view selection. To support query planning, we design a scalable cost and cardinality estimation scheme for full-fledged RPQs, including Kleene closures. Our method, when applied to the Wikidata Query Logs, shows a 9.73× speedup in the total query processing time compared to ad-hoc processing, using the views it selects.}
}


@article{DBLP:journals/pacmmod/LuoZ0GCX24,
	author = {Chengyang Luo and
                  Yifan Zhu and
                  Qing Liu and
                  Yunjun Gao and
                  Lu Chen and
                  Jianliang Xu},
	title = {MCR-Tree: An Efficient Index for Multi-dimensional Core Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {153},
	year = {2024},
	url = {https://doi.org/10.1145/3654956},
	doi = {10.1145/3654956},
	timestamp = {Fri, 14 Feb 2025 11:26:25 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LuoZ0GCX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Core models are well-known cohesive subgraph models for graph analytics that have been extensively studied. These models, including (α, β)-core, (k, l)-core, and k -core, have multiple parameters, which are referred to as multi-dimensional cores. The goal of core search is to retrieve subgraphs from a graph that satisfy the semantics of a given core model. In the literature, various indexes have been proposed to accelerate core search for different core models. However, existing indexes suffer from several limitations, such as significant redundancy, lack of scalability with respect to the number of parameters, limited generality, and inadequate consideration of index maintenance. To address these limitations, in this paper, we thoroughly investigate the problem of multi-dimensional core search. In particular, we propose a novel index called MCR-Tree, which can be applied to different core models. The MCR-Tree projects all vertices into a multi-dimensional space by leveraging the skyline corenesses, which are indexed by an R-tree. Furthermore, the MCR-Tree integrates the connectivity information of subgraphs into the nodes of the R-tree to facilitate multi-dimensional core search. Subsequently, an efficient branch-and-bound algorithm is designed to perform multi-dimensional core search by traversing the MCR-Tree. Additionally, we discuss how to maintain the MCR-Tree for graph updates. Extensive experiments demonstrate that the MCR-Tree is up to two orders of magnitude smaller than existing indexes and the MCR-Tree-based core search method is up to an order of magnitude faster than existing algorithms.}
}


@article{DBLP:journals/pacmmod/GongGF24,
	author = {Yue Gong and
                  Sainyam Galhotra and
                  Raul Castro Fernandez},
	title = {Nexus: Correlation Discovery over Collections of Spatio-Temporal Tabular
                  Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {154},
	year = {2024},
	url = {https://doi.org/10.1145/3654957},
	doi = {10.1145/3654957},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GongGF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Causal analysis is essential for gaining insights into complex real-world processes and making informed decisions. However, performing accurate causal analysis on observational data is generally infeasible, and therefore, domain experts start exploration with the identification of correlations. The increased availability of data from open government websites, organizations, and scientific studies presents an opportunity to harness observational datasets in assisting domain experts during this exploratory phase. In this work, we introduce Nexus, a system designed to align large repositories of spatio-temporal datasets and identify correlations, facilitating the exploration of causal relationships. Nexus addresses the challenges of aligning tabular datasets across space and time, handling missing data, and identifying correlations deemed "interesting". Empirical evaluation on Chicago Open Data and United Nations datasets demonstrates the effectiveness of Nexus in exposing interesting correlations, many of which have undergone extensive scrutiny by social scientists.}
}


@article{DBLP:journals/pacmmod/0002CWP0T24,
	author = {Mo Sha and
                  Yifan Cai and
                  Sheng Wang and
                  Linh Thi Xuan Phan and
                  Feifei Li and
                  Kian{-}Lee Tan},
	title = {Object-oriented Unified Encrypted Memory Management for Heterogeneous
                  Memory Architectures},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {155},
	year = {2024},
	url = {https://doi.org/10.1145/3654958},
	doi = {10.1145/3654958},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/0002CWP0T24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In contemporary database applications, the demand for memory resources is intensively high. To enhance adaptability to varying resource needs and improve cost efficiency, the integration of diverse storage technologies within heterogeneous memory architectures emerges as a promising solution. Despite the potential advantages, there exists a significant gap in research related to the security of data within these complex systems. This paper endeavors to fill this void by exploring the intricacies and challenges of ensuring data security in object-oriented heterogeneous memory systems. We introduce the concept of Unified Encrypted Memory (UEM) management, a novel approach that provides unified object references essential for data management platforms, while simultaneously concealing the complexities of physical scheduling from developers. At the heart of UEM lies the seamless and efficient integration of data encryption techniques, which are designed to ensure data integrity and guarantee the freshness of data upon access. Our research meticulously examines the security deficiencies present in existing heterogeneous memory system designs. By advancing centralized security enforcement strategies, we aim to achieve efficient object-centric data protection. Through extensive evaluations conducted across a variety of memory configurations and tasks, our findings highlight the effectiveness of UEM. The security features of UEM introduce low and acceptable overheads, and UEM outperforms conventional security measures in terms of speed and space efficiency.}
}


@article{DBLP:journals/pacmmod/Lin0F0LM24,
	author = {Chunxu Lin and
                  Wensheng Luo and
                  Yixiang Fang and
                  Chenhao Ma and
                  Xilin Liu and
                  Yuchi Ma},
	title = {On Efficient Large Sparse Matrix Chain Multiplication},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {156},
	year = {2024},
	url = {https://doi.org/10.1145/3654959},
	doi = {10.1145/3654959},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Lin0F0LM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparse matrices are often used to model the interactions among different objects and they are prevalent in many areas including e-commerce, social network, and biology. As one of the fundamental matrix operations, the sparse matrix chain multiplication (SMCM) aims to efficiently multiply a chain of sparse matrices, which has found various real-world applications in areas like network analysis, data mining, and machine learning. The efficiency of SMCM largely hinges on the order of multiplying the matrices, which further relies on the accurate estimation of the sparsity values of intermediate matrices. Existing matrix sparsity estimators often struggle with large sparse matrices, because they suffer from the accuracy issue in both theory and practice. To enable efficient SMCM, in this paper we introduce a novel row-wise sparsity estimator (RS-estimator), a straightforward yet effective estimator that leverages matrix structural properties to achieve efficient, accurate, and theoretically guaranteed sparsity estimation. Based on the RS-estimator, we propose a novel ordering algorithm for determining a good order of efficient SMCM. We further develop an efficient parallel SMCM algorithm by effectively utilizing multiple CPU threads. We have conducted experiments by multiplying various chains of large sparse matrices extracted from five real-world large graph datasets, and the results demonstrate the effectiveness and efficiency of our proposed methods. In particular, our SMCM algorithm is up to three orders of magnitude faster than the state-of-the-art algorithms.}
}


@article{DBLP:journals/pacmmod/SongWXQZ024,
	author = {Jingyi Song and
                  Dong Wen and
                  Lantian Xu and
                  Lu Qin and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {On Querying Historical Connectivity in Temporal Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {157},
	year = {2024},
	url = {https://doi.org/10.1145/3654960},
	doi = {10.1145/3654960},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SongWXQZ024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the historical connectivity query in temporal graphs where edges continuously arrive. Given an arbitrary time window, and two query vertices, the problem aims to identify if two vertices are connected by a path in the snapshot of the window. The state-of-the-art method designs an index based on the two-hop cover, and updating the index is costly when new edges arrive. In this paper, we propose a new framework and design a novel forest-based index for historical connectivity queries. The index enables us to answer queries by searching if two vertices are connected in the forest. We update the index by modifying a forest structure. Our techniques also work for connectivity query processing in a sliding window of temporal graphs. Extensive experiments have been conducted to show the considerable advantages of our approach compared with the state-of-the-art methods in both historical connectivity queries and sliding-window connectivity queries.}
}


@article{DBLP:journals/pacmmod/Kim024,
	author = {Albert Kim and
                  Samuel Madden},
	title = {Optimizing Disjunctive Queries with Tagged Execution},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {158},
	year = {2024},
	url = {https://doi.org/10.1145/3654961},
	doi = {10.1145/3654961},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Kim024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite decades of research into query optimization, optimizing queries with disjunctive predicate expressions remains a challenge. Solutions employed by existing systems (if any) are often simplistic and lead to much redundant work being performed by the execution engine. To address these problems, we propose a novel form of query execution called tagged execution. Tagged execution groups tuples into subrelations based on which predicates in the query they satisfy (or don't satisfy) and tags them with that information. These tags then provide additional context for query operators to take advantage of during runtime, allowing them to eliminate much of the redundant work performed by traditional engines and realize predicate pushdown optimizations for disjunctive predicates. However, tagged execution brings its own challenges, and the question of what tags to create is a nontrivial one. Careless creation of tags can lead to an exponential blowup in the tag space, with the overhead outweighing the benefits. To address this issue, we present a technique called tag generalization to minimize the space of tags. We implemented the tagged execution model with tag generalization in our system Basilisk, and our evaluation showed an average 2.7x speedup in runtime over the traditional execution model with up to a 19x speedup in certain situations.}
}


@article{DBLP:journals/pacmmod/KangS24,
	author = {Rui Kang and
                  Shaoxu Song},
	title = {Optimizing Time Series Queries with Versions},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {159},
	year = {2024},
	url = {https://doi.org/10.1145/3654962},
	doi = {10.1145/3654962},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KangS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We show that the time-series database for industrial IoT data management exhibits intrinsic demands for integrating an automatic version control system, which introduces advanced data semantics and query optimization. In deployed IoT database instances, IoT data managed by an LSM tree is multi-leveled and multi-versioned due to network issues and erroneous IoT readings. For data semantics, each query merges versioned data according to query expressions or data block levels. For query optimization, we find that existing time-series databases relying on write-ahead-logs suboptimally execute data queries, due to their performance bottlenecks in merging numerous versioned data. In this paper, an algebra consisting of version operators addresses the semantics for time-series applications to evaluate and optimize physical query plans. We propose version reducibility as a key feature of executing consistent plans and evaluate the benefits of putting off data merges. We also show the integration of version queries to existing relational databases by translating them to standard SQL based on relational reducibility. Finally, our extended experiments show the effectiveness of optimizing execution plans over versioned data.}
}


@article{DBLP:journals/pacmmod/PirhadiMCMS24,
	author = {Alireza Pirhadi and
                  Mohammad Hossein Moslemi and
                  Alexander Cloninger and
                  Mostafa Milani and
                  Babak Salimi},
	title = {OTClean: Data Cleaning for Conditional Independence Violations using
                  Optimal Transport},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {160},
	year = {2024},
	url = {https://doi.org/10.1145/3654963},
	doi = {10.1145/3654963},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/PirhadiMCMS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ensuring Conditional Independence (CI) constraints is pivotal for the development of fair and trustworthy machine learning models. In this paper, we introduce OTClean, a framework that harnesses optimal transport theory for data repair under CI constraints. Optimal transport theory provides a rigorous framework for measuring the discrepancy between probability distributions, thereby ensuring control over data utility. We formulate the data repair problem concerning CIs as a Quadratically Constrained Linear Program (QCLP) and propose an alternating method for its solution. However, this approach faces scalability issues due to the computational cost associated with computing optimal transport distances, such as the Wasserstein distance. To overcome these scalability challenges, we reframe our problem as a regularized optimization problem, enabling us to develop an iterative algorithm inspired by Sinkhorn's matrix scaling algorithm, which efficiently addresses high-dimensional and large-scale data. Through extensive experiments, we demonstrate the efficacy and efficiency of our proposed methods, showcasing their practical utility in real-world data cleaning and preprocessing tasks. Furthermore, we provide comparisons with traditional approaches, highlighting the superiority of our techniques in terms of preserving data utility while ensuring adherence to the desired CI constraints.}
}


@article{DBLP:journals/pacmmod/CaiTZ024,
	author = {Shuangyu Cai and
                  Boyu Tian and
                  Huanchen Zhang and
                  Mingyu Gao},
	title = {PimPam: Efficient Graph Pattern Matching on Real Processing-in-Memory
                  Hardware},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {161},
	year = {2024},
	url = {https://doi.org/10.1145/3654964},
	doi = {10.1145/3654964},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CaiTZ024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph pattern matching is powerful and widely applicable to many application domains. Despite the recent algorithm advances, matching patterns in large-scale real-world graphs still faces the memory access bottleneck on conventional computing systems. Processing-in-memory (PIM) is an emerging hardware architecture paradigm that puts computing cores into memory devices to alleviate the memory wall issues. Real PIM hardware has recently become commercially accessible to the public. In this work, we leverage the real PIM hardware platform to build a graph pattern matching framework, PimPam, to benefit from its abundant computation and memory bandwidth resources. We propose four key optimizations in PimPam to improve its efficiency, including (1) load-aware task assignment to ensure load balance, (2) space-efficient and parallel data partitioning to prepare input data for PIM cores, (3) adaptive multi-threading collaboration to automatically select the best parallelization strategy during processing, and (4) dynamic bitmap structures that accelerate the key operations of set intersection. When evaluated on five patterns and six real-world graphs, PimPam outperforms the state-of-the-art CPU baseline system by 22.5x on average and up to 71.7x, demonstrating significant performance improvements.}
}


@article{DBLP:journals/pacmmod/DingXWXZ24,
	author = {Zezhong Ding and
                  Yongan Xiang and
                  Shangyou Wang and
                  Xike Xie and
                  S. Kevin Zhou},
	title = {Play like a Vertex: {A} Stackelberg Game Approach for Streaming Graph
                  Partitioning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {162},
	year = {2024},
	url = {https://doi.org/10.1145/3654965},
	doi = {10.1145/3654965},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DingXWXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the realm of distributed systems tasked with managing and processing large-scale graph-structured data, optimizing graph partitioning stands as a pivotal challenge. The primary goal is to minimize communication overhead and runtime cost. However, alongside the computational complexity associated with optimal graph partitioning, a critical factor to consider is memory overhead. Real-world graphs often reach colossal sizes, making it impractical and economically unviable to load the entire graph into memory for partitioning. This is also a fundamental premise in distributed graph processing, where accommodating a graph with non-distributed systems is unattainable. Currently, existing streaming partitioning algorithms exhibit a skew-oblivious nature, yielding satisfactory partitioning results exclusively for specific graph types. In this paper, we propose a novel streaming partitioning algorithm, the Skewness-aware Vertex-cut Partitioner (S5P ), designed to leverage the skewness characteristics of real graphs for achieving high-quality partitioning. S5P offers high partitioning quality by segregating the graph's edge set into two subsets, head and tail sets. Following processing by a skewness-aware clustering algorithm, these two subsets subsequently undergo a Stackelberg graph game. Our extensive evaluations conducted on substantial real-world and synthetic graphs demonstrate that, in all instances, the partitioning quality of S5P surpasses that of existing streaming partitioning algorithms, operating within the same load balance constraints. For example, S5P can bring up to a 51% improvement in partitioning quality compared to the top partitioner among the baselines. Lastly, we showcase that the implementation of S5P results in up to an 81% reduction in communication cost and a 130% increase in runtime efficiency for distributed graph processing tasks on PowerGraph.}
}


@article{DBLP:journals/pacmmod/LeZ24,
	author = {Van{-}Hoang Le and
                  Hongyu Zhang},
	title = {PreLog: {A} Pre-trained Model for Log Analytics},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {163},
	year = {2024},
	url = {https://doi.org/10.1145/3654966},
	doi = {10.1145/3654966},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LeZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale software-intensive systems often produce a large volume of logs to record runtime status and events for troubleshooting purposes. The rich information in log data enables a variety of system management and diagnosis tasks. Over the years, many approaches have been proposed for automated log analytics. However, these approaches usually design separate models for each specific task, which cannot be generalized to other tasks. They are also not robust when dealing with logs from heterogeneous sources. In this paper, we propose PreLog, a novel pre-trained model for log analytics. PreLog is pre-trained on a large amount of unlabelled log data to capture the semantic meaning of logs. We design two log-specific pre-training objectives, including entry-level and sequence-level objectives, which enable PreLog to better understand the hidden structure and semantics of logs. To perform downstream log analytics tasks, we leverage a prompt tuning paradigm to convert downstream tasks' objectives into a similar form as the pre-training stage. We have conducted extensive experiments on two main log analytics tasks (i.e., log parsing and log-based anomaly detection). Experimental results show that PreLog achieves better or comparable results in comparison with the state-of-the-art, task-specific approaches. PreLog is cost-effective and can be uniformly applied to many log analytics tasks through the prompt tuning paradigm.}
}


@article{DBLP:journals/pacmmod/HuGSRY24,
	author = {Yihao Hu and
                  Amir Gilad and
                  Kristin Stephens{-}Martinez and
                  Sudeepa Roy and
                  Jun Yang},
	title = {Qr-Hint: Actionable Hints Towards Correcting Wrong {SQL} Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {164},
	year = {2024},
	url = {https://doi.org/10.1145/3654995},
	doi = {10.1145/3654995},
	timestamp = {Thu, 15 May 2025 16:54:50 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuGSRY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We describe a system called Qr-Hint that, given a (correct) target query Q* and a (wrong) working query Q, both expressed in SQL, provides actionable hints for the user to fix the working query so that it becomes semantically equivalent to the target. It is particularly useful in an educational setting, where novices can receive help from Qr-Hint without requiring extensive personal tutoring. Since there are many different ways to write a correct query, we do not want to base our hints completely on how Q* is written; instead, starting with the user\'s own working query, Qr-Hint purposefully guides the user through a sequence of steps that provably lead to a correct query, which will be equivalent to Q* but may still "look" quite different from it. Ideally, we would like Qr-Hint\'s hints to lead to the "smallest" possible corrections to Q. However, optimality is not always achievable in this case due to some foundational hurdles such as the undecidability of SQL query equivalence and the complexity of logic minimization. Nonetheless, by carefully decomposing and formulating the problems and developing principled solutions, we are able to provide provably correct and locally optimal hints through Qr-Hint. We show the effectiveness of Qr-Hint through quality and performance experiments as well as a user study in an educational setting.}
}


@article{DBLP:journals/pacmmod/GrulichLNPMZM24,
	author = {Philipp M. Grulich and
                  Aljoscha P. Lepping and
                  Dwi P. A. Nugroho and
                  Varun Pandey and
                  Bonaventura Del Monte and
                  Steffen Zeuch and
                  Volker Markl},
	title = {Query Compilation Without Regrets},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {165},
	year = {2024},
	url = {https://doi.org/10.1145/3654968},
	doi = {10.1145/3654968},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GrulichLNPMZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Engineering high-performance query execution engines is a challenging task. Query compilation provides excellent performance, but at the same time introduces significant system complexity, as it makes the engine hard to build, debug, and maintain. To overcome this complexity, we propose Nautilus, a framework that combines the ease of use of query interpretation and the performance of query compilation. On the one hand, Nautilus provides an interpretation-based operator interface that enables engineers to implement operators using imperative C++ code to ensure a familiar developer experience. On the other hand, Nautilus mitigates the performance drawbacks of interpretation by introducing a novel trace-based, multi-backend JIT compiler that translates operators into efficient code. As a result, Nautilus bridges the gap between compilation and interpretation and provides the best of both worlds, achieving high performance without sacrificing the productivity of engineers.}
}


@article{DBLP:journals/pacmmod/CampbellSSM24,
	author = {Felix S. Campbell and
                  Alon Silberstein and
                  Julia Stoyanovich and
                  Yuval Moskovitch},
	title = {Query Refinement for Diverse Top-k Selection},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {166},
	year = {2024},
	url = {https://doi.org/10.1145/3654969},
	doi = {10.1145/3654969},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/CampbellSSM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database queries are often used to select and rank items as decision support for many applications. As automated decision-making tools become more prevalent, there is a growing recognition of the need to diversify their outcomes. In this paper, we define and study the problem of modifying the selection conditions of an ORDER BY query so that the result of the modified query closely fits some user-defined notion of diversity while simultaneously maintaining the intent of the original query. We show the hardness of this problem and propose a mixed-integer linear programming (MILP) based solution. We further present optimizations designed to enhance the scalability and applicability of the solution in real-life scenarios. We investigate the performance characteristics of our algorithm and show its efficiency and the usefulness of our optimizations.}
}


@article{DBLP:journals/pacmmod/GaoL24,
	author = {Jianyang Gao and
                  Cheng Long},
	title = {RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error
                  Bound for Approximate Nearest Neighbor Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {167},
	year = {2024},
	url = {https://doi.org/10.1145/3654970},
	doi = {10.1145/3654970},
	timestamp = {Sun, 19 Jan 2025 15:06:03 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GaoL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Searching for approximate nearest neighbors (ANN) in the high-dimensional Euclidean space is a pivotal problem. Recently, with the help of fast SIMD-based implementations, Product Quantization (PQ) and its variants can often efficiently and accurately estimate the distances between the vectors and have achieved great success in the in-memory ANN search. Despite their empirical success, we note that these methods do not have a theoretical error bound and are observed to fail disastrously on some real-world datasets. Motivated by this, we propose a new randomized quantization method named RaBitQ, which quantizes D-dimensional vectors into D-bit strings. RaBitQ guarantees a sharp theoretical error bound and provides good empirical accuracy at the same time. In addition, we introduce efficient implementations of RaBitQ, supporting to estimate the distances with bitwise operations or SIMD-based operations. Extensive experiments on real-world datasets confirm that (1) our method outperforms PQ and its variants in terms of accuracy-efficiency trade-off by a clear margin and (2) its empirical performance is well-aligned with our theoretical analysis.}
}


@article{DBLP:journals/pacmmod/0001LW24,
	author = {Qichen Wang and
                  Qiyao Luo and
                  Yilei Wang},
	title = {Relational Algorithms for Top-k Query Evaluation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {168},
	year = {2024},
	url = {https://doi.org/10.1145/3654971},
	doi = {10.1145/3654971},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/0001LW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The evaluation of top-k conjunctive queries, a staple in business analysis, often requires evaluating the conjunctive query prior to filtering the top-k results, leading to a significant computational overhead within Database Management Systems (DBMSs). While efficient algorithms have been proposed, their integration into DBMSs remains arduous. We introduce relational algorithms, a paradigm where each algorithmic step is expressed by a relational operator. This allows the algorithm to be represented as a set of SQL queries, enabling easy deployment across different systems that support SQL. We introduce two novel relational algorithms, level-k and product-k, specifically designed for evaluating top-k conjunctive queries and demonstrate that level-k achieves optimal running time for top-k free-connex queries. Furthermore, these algorithms enable easy translation into an oblivious algorithm for secure query evaluations. The presented algorithms are not only theoretically optimal but also exhibit eminent efficiency in practice. The experiment results show significant improvements, with our rewritten SQL outperforming the baseline by up to 6 orders of magnitude. Moreover, our secure implementations not only achieve substantial speedup compared to the baseline with secure guarantees but even surpass those baselines that have no secure guarantees.}
}


@article{DBLP:journals/pacmmod/GaoB024,
	author = {Chuqing Gao and
                  Shreya Ballijepalli and
                  Jianguo Wang},
	title = {Revisiting B-tree Compression: An Experimental Study},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {169},
	year = {2024},
	url = {https://doi.org/10.1145/3654972},
	doi = {10.1145/3654972},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/GaoB024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {B-trees are widely recognized as one of the most important index structures in database systems, providing efficient query processing capabilities. Over the past few decades, many techniques have been developed to enhance the efficiency of B-trees from various perspectives. Among them, B-tree compression is an important technique introduced as early as the 1970s to improve both space efficiency and query performance. Since then, several B-tree compression techniques have been developed. However, to our surprise, we have found that these B-tree compression techniques were never compared against each other in prior works. Consequently, many important questions remain unanswered, such as whether B-tree compression is truly effective or not. If it is effective, under what scenarios and which B-tree compression methods should be employed? In this paper, we conduct the first experimental evaluation of seven widely used B-tree compression techniques using both synthetic and real datasets. Based on our evaluation, we present lessons and insights that can be leveraged to guide system design decisions in modern databases regarding the use of B-tree compression.}
}


@article{DBLP:journals/pacmmod/WeiT24,
	author = {Ziyun Wei and
                  Immanuel Trummer},
	title = {{ROME:} Robust Query Optimization via Parallel Multi-Plan Execution},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {170},
	year = {2024},
	url = {https://doi.org/10.1145/3654973},
	doi = {10.1145/3654973},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WeiT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a non-intrusive approach to robust query processing that can be used on top of any SQL execution engine. To reduce the risk of selecting highly sub-optimal query plans, we execute multiple plans in parallel. Query processing finishes once the first of these plans finishes execution. Plans are selected to be complementary in terms of the intermediate results they generate. This increases robustness to cardinality estimation errors, making cost prediction hard, that concern a subset of candidate results. We present multiple cost-based approaches to selecting plans for robust execution. The first approach uses a simple cost model, based on diversity of intermediate results. The second approach features a probabilistic model, approximating expected execution overheads, given uncertainty on true intermediate result sizes. We present greedy and exhaustive algorithms to select optimal plans according to those cost models. The experiments demonstrate that executing multiple plans in parallel is preferable over executing single plans that are occasionally sub-optimal, as well as over several baselines.}
}


@article{DBLP:journals/pacmmod/WidmoserKA24,
	author = {Manuel Widmoser and
                  Daniel Kocher and
                  Nikolaus Augsten},
	title = {Scalable Distributed Inverted List Indexes in Disaggregated Memory},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {171},
	year = {2024},
	url = {https://doi.org/10.1145/3654974},
	doi = {10.1145/3654974},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WidmoserKA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Memory disaggregation separates compute (CPU) and main memory resources into disjoint physical units to enable elastic and independent scaling. Connected via high-speed RDMA-enabled networks, compute nodes can directly access remote memory. This setting often requires complex protocols with many network roundtrips as memory nodes have near-zero compute power. In this paper, we design a scalable distributed inverted list index for disaggregated memory architectures. An inverted list index maps a set of terms to lists of documents that contain this term. Current solutions either partition the index horizontally or vertically with severe limitations in the disaggregated memory setting due to data and access skew, high network latency, or out-of-memory errors. Our method partitions lists into fixed-size blocks and spreads them across the memory nodes to balance skewed accesses. Block-based list processing keeps the memory footprint of compute nodes low and masks latency by interleaving remote accesses with expensive list operations. In addition, we propose efficient updates with optimistic concurrency control and read-write conflict detection. Our experiments confirm the efficiency and scalability of our method.}
}


@article{DBLP:journals/pacmmod/DohmenGHS24,
	author = {Till D{\"{o}}hmen and
                  Radu Geacu and
                  Madelon Hulsebos and
                  Sebastian Schelter},
	title = {SchemaPile: {A} Large Collection of Relational Database Schemas},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {172},
	year = {2024},
	url = {https://doi.org/10.1145/3654975},
	doi = {10.1145/3654975},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DohmenGHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Access to fine-grained schema information is crucial for understanding how relational databases are designed and used in practice, and for building systems that help users interact with them. Furthermore, such information is required as training data to leverage the potential of large language models (LLMs) for improving data preparation, data integration and natural language querying. Existing single-table corpora such as GitTables provide insights into how tables are structured in-the-wild, but lack detailed schema information about how tables relate to each other, as well as metadata like data types or integrity constraints. On the other hand, existing multi-table (or database schema) datasets are rather small and attribute-poor, leaving it unclear to what extent they actually represent typical real-world database schemas. In order to address these challenges, we present SchemaPile, a corpus of 221,171 database schemas, extracted from SQL files on GitHub. It contains 1.7 million tables with 10 million column definitions, 700 thousand foreign key relationships, seven million integrity constraints, and data content for more than 340 thousand tables. We conduct an in-depth analysis on the millions of schema metadata properties in our corpus, as well as its highly diverse language and topic distribution. In addition, we showcase the potential of \\corpus to improve a variety of data management applications, e.g., fine-tuning LLMs for schema-only foreign key detection, improving CSV header detection and evaluating multi-dialect SQL parsers. We publish the code and data for recreating SchemaPile and a permissively licensed subset SchemaPile-Perm.}
}


@article{DBLP:journals/pacmmod/DraganovSS24,
	author = {Andrew Draganov and
                  David Saulpic and
                  Chris Schwiegelshohn},
	title = {Settling Time vs. Accuracy Tradeoffs for Clustering Big Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {173},
	year = {2024},
	url = {https://doi.org/10.1145/3654976},
	doi = {10.1145/3654976},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/DraganovSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the theoretical and practical runtime limits of k-means and k-median clustering on large datasets. Since effectively all clustering methods are slower than the time it takes to read the dataset, the fastest approach is to quickly compress the data and perform the clustering on the compressed representation. Unfortunately, there is no universal best choice for compressing the number of points -- while random sampling runs in sublinear time and coresets provide theoretical guarantees, the former does not enforce accuracy while the latter is too slow as the numbers of points and clusters grow. Indeed, it has been conjectured that any sensitivity-based coreset construction requires super-linear time in the datase size. We examine this relationship by first showing that there does exist an algorithm that obtains coresets via sensitivity sampling in effectively linear time -- within log-factors of the time it takes to read the data. Any approach that significantly improves on this must then resort to practical heuristics, leading us to consider the spectrum of sampling strategies across both real and artificial datasets in the static and streaming settings. Through this, we show the conditions in which coresets are necessary for preserving cluster validity as well as the settings in which faster, cruder sampling strategies are sufficient. As a result, we provide a comprehensive theoretical and practical blueprint for effective clustering regardless of data size. Our code is publicly available at https://github.com/Andrew-Draganov/Fast-Coreset-Generation and has scripts to recreate the experiments.}
}


@article{DBLP:journals/pacmmod/GaoLZSS024,
	author = {Shihong Gao and
                  Yiming Li and
                  Xin Zhang and
                  Yanyan Shen and
                  Yingxia Shao and
                  Lei Chen},
	title = {{SIMPLE:} Efficient Temporal Graph Neural Network Training at Scale
                  with Dynamic Data Placement},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {174},
	year = {2024},
	url = {https://doi.org/10.1145/3654977},
	doi = {10.1145/3654977},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GaoLZSS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic graphs are essential in real-world scenarios like social media and e-commerce for tasks such as predicting links and classifying nodes. Temporal Graph Neural Networks (T-GNNs) stand out as a prime solution for managing dynamic graphs, employing temporal message passing to compute node embeddings at specific timestamps. Nonetheless, the high CPU-GPU data loading overhead has become the bottleneck for efficient training of T-GNNs over large-scale dynamic graphs. In this work, we present SIMPLE, a versatile system designed to address the major efficiency bottleneck in training existing T-GNNs on a large scale. It incorporates a dynamic data placement mechanism, which maintains a small buffer space in available GPU memory and dynamically manages its content during T-GNN training. SIMPLE is also empowered by systematic optimizations towards data processing flow. We compare SIMPLE to the state-of-the-art generic T-GNN training system TGL on four large-scale dynamic graphs with different underlying T-GNN models. Extensive experimental results show that SIMPLE effectively cuts down 80.5% ~ 96.8% data loading cost, and accelerates T-GNN training by 1.8× ~ 3.8× (2.6× on average) compared to TGL.}
}


@article{DBLP:journals/pacmmod/LiuWML24,
	author = {Junfeng Liu and
                  Fan Wang and
                  Dingheng Mo and
                  Siqiang Luo},
	title = {Structural Designs Meet Optimality: Exploring Optimized LSM-tree Structures
                  in a Colossal Configuration Space},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {175},
	year = {2024},
	url = {https://doi.org/10.1145/3654978},
	doi = {10.1145/3654978},
	timestamp = {Wed, 19 Mar 2025 21:16:37 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuWML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mainstream LSM-tree-based key-value stores face challenges in optimizing performance for point lookup, range lookup, and update operations concurrently due to their constrained configurations. They typically follow fixed patterns to specify the level capacity and the number of sorted runs per-level. This confines their designs to a restricted space, limiting opportunities for broader optimizations. To address this challenge, we consider a more flexible configuration that enables independent adjustments of the number of runs per-level, size ratio, and Bloom filter settings at each LSM-tree level. By carefully analyzing the cost of each operation based on the new design space, we unveil two critical insights for optimizing the tradeoff among the three operations. Firstly, achieving efficient point lookup requires a large last level. Secondly, there is a specific correlation between the number of runs per level and size ratio that is advantageous for overall update and range lookup performance. Based on these insights, we introduce Moose, a structure delivering an impressive overall performance for point lookup, range lookup, and update concurrently. Furthermore, we also introduce a new framework, Smoose, to navigate the design space for adapting specific workloads. We implemented Moose and Smoose on top of RocksDB and experimental results demonstrate that our proposed approach outperforms state-of-the-art LSM-tree structures across diverse workloads.}
}


@article{DBLP:journals/pacmmod/LiHYCGZF0C24,
	author = {Peng Li and
                  Yeye He and
                  Dror Yashar and
                  Weiwei Cui and
                  Song Ge and
                  Haidong Zhang and
                  Danielle Rifinski Fainman and
                  Dongmei Zhang and
                  Surajit Chaudhuri},
	title = {Table-GPT: Table Fine-tuned {GPT} for Diverse Table Tasks},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {176},
	year = {2024},
	url = {https://doi.org/10.1145/3654979},
	doi = {10.1145/3654979},
	timestamp = {Wed, 12 Feb 2025 19:17:36 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiHYCGZF0C24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Language models, such as GPT-3 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks, using instruction fine-tuning. However, when we test language models with a range of basic table-understanding tasks, we observe that today\'s language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on one-dimensional natural-language texts, whereas relational tables are two-dimensional objects. In this work, we propose a new "\\emphtable fine-tuning \'\' paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, which is analogous to "instruction fine-tuning\'\', but with the goal of enhancing language models\' ability to understand tables and perform table tasks. We show that our resulting \\sys models demonstrate: (1) better table-understanding capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide range of table tasks (data transformation, data cleaning, data profiling, data imputation, table-QA, etc.), including tasks that are completely holdout and unseen during training, and (2) strong generalizability, in its ability to respond to diverse human instructions to perform new and unseen table-tasks, in a manner similar to GPT-3.5 and ChatGPT. Our code and data have been released at https://github.com/microsoft/Table-GPT for future research.}
}


@article{DBLP:journals/pacmmod/DyresonSBS24,
	author = {Curtis E. Dyreson and
                  Amani M. Shatnawi and
                  Sourav S. Bhowmick and
                  Vishal Sharma},
	title = {Temporal {JSON} Keyword Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {177},
	year = {2024},
	url = {https://doi.org/10.1145/3654980},
	doi = {10.1145/3654980},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DyresonSBS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {JSON keyword search searches the current versions of documents in a collection. However, JSON documents change over time due to edits. Some applications, such as data forensics and auditing, need to search past versions of documents and for changes to documents. This paper introduces a system called Temporal JSON Keyword Search (TJKS) for search in a collection of JSON documents that vary over time. TJKS lets users control which temporal slice, or part of the history, can be searched using a temporal search semantics; we support both of the major temporal semantics: sequenced and nonsequenced search. This paper presents the semantics of temporal JSON keyword search, discusses an efficient implementation, and evaluates the implementation. Our extensions are largely orthogonal to specific keyword search techniques, so this research provides a blueprint for extending keyword search to include time and potentially other kinds of metadata.}
}


@article{DBLP:journals/pacmmod/MoSD24,
	author = {Guanlin Mo and
                  Shihong Song and
                  Hu Ding},
	title = {Towards Metric {DBSCAN:} Exact, Approximate, and Streaming Algorithms},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {178},
	year = {2024},
	url = {https://doi.org/10.1145/3654981},
	doi = {10.1145/3654981},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/MoSD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DBSCAN is a popular density-based clustering algorithm that has many different applications in practice. However, the running time of DBSCAN in high-dimensional space or general metric space (\\em e.g., clustering a set of texts by using edit distance) can be as large as quadratic in the input size. Moreover, most of existing accelerating techniques for DBSCAN are only available for low-dimensional Euclidean space. In this paper, we study the DBSCAN problem under the assumption that the inliers (the core points and border points) have a low intrinsic dimension (which is a realistic assumption for many high-dimensional applications), where the outliers can locate anywhere in the space without any assumption. First, we propose a k-center clustering based algorithm that can reduce the time-consuming labeling and merging tasks of DBSCAN to be linear. Further, we propose a linear time approximate DBSCAN algorithm, where the key idea is building a novel small-size summary for the core points. Also, our algorithm can be efficiently implemented for streaming data and the required memory is independent of the input size. Finally, we conduct our experiments and compare our algorithms with several popular DBSCAN algorithms. The experimental results suggest that our proposed approach can significantly reduce the computational complexity in practice.}
}


@article{DBLP:journals/pacmmod/Yao0SJH24,
	author = {Siyuan Yao and
                  Yuchen Li and
                  Shixuan Sun and
                  Jiaxin Jiang and
                  Bingsheng He},
	title = {uBlade: Efficient Batch Processing for Uncertainty Graph Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {179},
	year = {2024},
	url = {https://doi.org/10.1145/3654982},
	doi = {10.1145/3654982},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/Yao0SJH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The study of uncertain graphs is crucial in diverse fields, including but not limited to protein interaction analysis, viral marketing, and network reliability. Processing queries on uncertain graphs presents formidable challenges due to the vast probabilistic space they encapsulate. While existing systems employ batch processing to address these challenges, their performance is often compromised by the suboptimal selection of parallel graph traversal methods, the excessive costs in random number generation, and additional workloads intrinsic to batch processing. In this paper, we introduce uBlade, an efficient batch-processing framework for uncertain graph queries on multi-core CPUs. uBlade utilizes the work-efficient graph traversal, achieving superior parallelism in the batch processing model. Additionally, our Quasi-Sampling technique reduces the random number generation cost by a factor of B, with O(B) denoting the batch size. We further examine the extra workload resulting from batch processing and introduce an efficient strategy to reorder possible worlds, minimizing this associated overhead. Through comprehensive evaluations, we showcase that uBlade achieves up to two orders of magnitude speedups against the state-of-the-art CPU and GPU-based solutions.}
}


@article{DBLP:journals/pacmmod/Pang024,
	author = {Xi Pang and
                  Jianguo Wang},
	title = {Understanding the Performance Implications of the Design Principles
                  in Storage-Disaggregated Databases},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {180},
	year = {2024},
	url = {https://doi.org/10.1145/3654983},
	doi = {10.1145/3654983},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/Pang024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Storage-compute disaggregation has recently emerged as a novel architecture in modern data centers, particularly in the cloud. By decoupling compute from storage, this new architecture enables independent and elastic scaling of compute and storage resources, potentially increasing resource utilization and reducing overall costs. To best leverage the disaggregated architecture, a new breed of database systems termed storage-disaggregated databases has recently been developed, such as Amazon Aurora, Microsoft Socrates, Google AlloyDB, Alibaba PolarDB, and Huawei Taurus. However, little is known about the effectiveness of the design principles in these databases since they are typically developed by industry giants, and only the overall performance results are presented without detailing the impact of individual design principles. As a result, many critical research questions remain unclear, such as the performance impact of storage-disaggregation, the log-as-the-database design, shared-storage, and various log-replay methods. In this paper, we investigate the performance implications of the design principles that are widely adopted in storage-disaggregated databases for the first time. As these databases were usually not open-sourced, we have made a significant effort to implement a storage-disaggregated database prototype based on PostgreSQL v13.0. By fully controlling and instrumenting the codebase, we are able to selectively enable and disable individual optimizations and techniques to evaluate their impact on performance in various scenarios. Furthermore, we open-source our storage-disaggregated database prototype for use by the broader database research community, fostering collaboration and innovation in this field.}
}


@article{DBLP:journals/pacmmod/ChenK24,
	author = {Kaiwen Chen and
                  Nick Koudas},
	title = {Unstructured Data Fusion for Schema and Data Extraction},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {181},
	year = {2024},
	url = {https://doi.org/10.1145/3654984},
	doi = {10.1145/3654984},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/ChenK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, there has been significant interest in extracting actionable insights from the abundance of unstructured textual data. In this paper, we introduce a novel problem, which we term Semistructured Schema and Data Extraction (SDE). This task aims to enhance and complete tables using information discovered from textual repositories, given partial table specifications in the form of queries. To effectively solve SDE, several challenges must be overcome, which involve transforming the partial table specifications into effective queries, retrieving relevant documents, discerning values for partially specified attributes, inferring additional attributes, and constructing an enriched output table while mitigating the influence of false positives from the retrieval. We propose an end-to-end pipeline for SDE, which consists of a retrieval component and an augmentation component, to address each of the challenges. In the retrieval component, we serialize the partial table specifications into a query and employ a dense passage retrieval algorithm to extract the top-k relevant results from the text repository. Subsequently, the augmentation component ingests the output documents from the retrieval phase and generates an enriched table. We formulate this table enrichment task as a unique sequence-to-sequence task, distinct from traditional approaches, as it operates on multiple documents during generation. Utilizing an interpolation mechanism on the encoder output, our model maintains a nearly constant context length while automatically prioritizing the importance of documents during the generation. Due to the novelty of SDE, we establish a validation methodology, adapting and expanding existing benchmarks with the use of powerful large language models. Our extensive experiments show that our method achieves high accuracy in enriching query tables through multi-document fusion, while also surpassing baseline methods in both accuracy and computational efficiency.}
}


@article{DBLP:journals/pacmmod/WangW0NC24,
	author = {Xiaoying Wang and
                  Wentao Wu and
                  Chi Wang and
                  Vivek R. Narasayya and
                  Surajit Chaudhuri},
	title = {Wii: Dynamic Budget Reallocation In Index Tuning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {182},
	year = {2024},
	url = {https://doi.org/10.1145/3654985},
	doi = {10.1145/3654985},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangW0NC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Index tuning aims to find the optimal index configuration for an input workload. It is often a time-consuming and resource-intensive process, largely attributed to the huge amount of "what-if" calls made to the query optimizer during configuration enumeration. Therefore, in practice it is desirable to set a budget constraint that limits the number of what-if calls allowed. This yields a new problem of budget allocation, namely, deciding on which query-configuration pairs (QCP\'s) to issue what-if calls. Unfortunately, optimal budget allocation is NP-hard, and budget allocation decisions made by existing solutions can be inferior. In particular, many of the what-if calls allocated by using existing solutions are devoted to QCP\'s whose what-if costs can be approximated by using cost derivation, a well-known technique that is computationally much more efficient and has been adopted by commercial index tuning software. This results in considerable waste of the budget, as these what-if calls are unnecessary. In this paper, we propose "Wii," a lightweight mechanism that aims to avoid such spurious what-if calls. It can be seamlessly integrated with existing configuration enumeration algorithms. Experimental evaluation on top of both standard industrial benchmarks and real workloads demonstrates that Wii can eliminate significant number of spurious what-if calls. Moreover, by reallocating the saved budget to QCP\'s where cost derivation is less accurate, existing algorithms can be significantly improved in terms of the final configuration found.}
}


@article{DBLP:journals/pacmmod/ZhengJ0YZC24,
	author = {Chenguang Zheng and
                  Guanxian Jiang and
                  Xiao Yan and
                  Peiqi Yin and
                  Qihui Zhou and
                  James Cheng},
	title = {GE\({}^{\mbox{2}}\): {A} General and Efficient Knowledge Graph Embedding
                  Learning System},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {183},
	year = {2024},
	url = {https://doi.org/10.1145/3654986},
	doi = {10.1145/3654986},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhengJ0YZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph embedding learning computes an embedding vector for each node in a graph and finds many applications in areas such as social networks, e-commerce, and medicine. We observe that existing graph embedding systems (e.g., PBG, DGL-KE, and Marius) have long CPU time and high CPU-GPU communication overhead, especially when using multiple GPUs. Moreover, it is cumbersome to implement negative sampling algorithms on them, which have many variants and are crucial for model quality. We propose a new system called GE 2 , which achieves both <u>g</u>enerality and <u>e</u>fficiency for <u>g</u>raph <u>e</u>mbedding learning. In particular, we propose a general execution model that encompasses various negative sampling algorithms. Based on the execution model, we design a user-friendly API that allows users to easily express negative sampling algorithms. To support efficient training, we offload operations from CPU to GPU to enjoy high parallelism and reduce CPU time. We also design COVER, which, to our knowledge, is the first algorithm to manage data swap between CPU and multiple GPUs for small communication costs. Extensive experimental results show that, comparing with the state-of-the-art graph embedding systems, GE 2  trains consistently faster across different models and datasets, where the speedup is usually over 2x and can be up to 7.5x.}
}


@article{DBLP:journals/pacmmod/LisbonaBM24,
	author = {Carlos Bobed Lisbona and
                  Jordi Bernad and
                  Pierre Maillot},
	title = {Language-Model Based Informed Partition of Databases to Speed Up Pattern
                  Mining},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {184},
	year = {2024},
	url = {https://doi.org/10.1145/3654987},
	doi = {10.1145/3654987},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LisbonaBM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extracting interesting patterns from data is the main objective of Data Mining. In this context, Frequent Itemset Mining has shown its usefulness in providing insights from transactional databases, which, in turn, can be used to gain insights about the structure of Knowledge Graphs. While there have been a lot of advances in the field, due to the NP-hard nature of the problem, the main approaches still struggle when they are faced with large databases with large and sparse vocabularies, such as the ones obtained from graph propositionalizations. There have been efforts to propose parallel algorithms, but, so far, the goal has not been to tackle this source of complexity (i.e., vocabulary size), thus, in this paper, we propose to parallelize frequent itemset mining algorithms by partitioning the database horizontally (i.e., transaction-wise) while not neglecting all the possible vertical information (i.e., item-wise). Instead of relying on pure item co-appearance metrics, we advocate for the adoption of a different approach: modeling databases as documents, where each transaction is a sentence, and each item a word. In this way, we can apply recent language modeling techniques (i.e., word embeddings) to obtain a continuous representation of the database, clusterize it in different partitions, and apply any mining algorithm to them. We show how our proposal leads to informed partitions with a reduced vocabulary size and a reduced entropy (i.e., disorder). This enhances the scalability, allowing us to speed up mining even in very large databases with sparse vocabularies. We have carried out a thorough experimental evaluation over both synthetic and real datasets showing the benefits of our proposal.}
}


@article{DBLP:journals/pacmmod/AmaraHM24,
	author = {Ouael Ben Amara and
                  Sami Hadouaj and
                  Niccol{\`{o}} Meneghetti},
	title = {StarfishDB: {A} Query Execution Engine for Relational Probabilistic
                  Programming},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {185},
	year = {2024},
	url = {https://doi.org/10.1145/3654988},
	doi = {10.1145/3654988},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AmaraHM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce StarfishDB, a query execution engine optimized for relational probabilistic programming. Our engine adopts the model of Gamma Probabilistic Databases, representing probabilistic programs as a collection of relational constraints, imposed against a generative stochastic process. We extend the model with the support for recursion, factorization and the ability to leverage just-in-time compilation techniques to speed up inference. We test our engine against a state-of-the-art sampler for Latent Dirichlet Allocation.}
}


@article{DBLP:journals/pacmmod/JoT24,
	author = {Saehan Jo and
                  Immanuel Trummer},
	title = {ThalamusDB: Approximate Query Processing on Multi-Modal Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {186},
	year = {2024},
	url = {https://doi.org/10.1145/3654989},
	doi = {10.1145/3654989},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/JoT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce ThalamusDB, a novel approximate query processing system that processes complex SQL queries on multi-modal data. ThalamusDB supports SQL queries integrating natural language predicates on visual, audio, and text data. To answer such queries, ThalamusDB exploits a collection of zero-shot models in combination with relational processing. ThalamusDB utilizes deterministic approximate query processing, harnessing the relative efficiency of relational processing to mitigate the computational demands of machine learning inference. For evaluating a natural language predicate, ThalamusDB requests a small number of labels from users. User can specify their preferences on the performance objective regarding the three relevant metrics: approximation error, computation time, and labeling overheads. The ThalamusDB query optimizer chooses optimized plans according to user preferences, prioritizing data processing and requested labels to maximize impact. Experiments with several real-world data sets, taken from Craigslist, YouTube, and Netflix, show that ThalamusDB achieves an average speedup of 35.0x over MindsDB, an exact processing baseline, and outperforms ABAE, a sampling-based method, in 78.9% of cases.}
}


@article{DBLP:journals/pacmmod/SuSZ024,
	author = {Yongye Su and
                  Yinqi Sun and
                  Minjia Zhang and
                  Jianguo Wang},
	title = {Vexless: {A} Serverless Vector Data Management System Using Cloud
                  Functions},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {187},
	year = {2024},
	url = {https://doi.org/10.1145/3654990},
	doi = {10.1145/3654990},
	timestamp = {Fri, 19 Jul 2024 23:17:58 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/SuSZ024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud functions, exemplified by AWS Lambda and Azure Functions, are emerging as a new computing paradigm in the cloud. They provide elastic, serverless, and low-cost cloud computing, making them highly suitable for bursty and sparse workloads, which are quite common in practice. Thus, there is a new trend in designing data systems that leverage cloud functions. In this paper, we focus on vector databases, which have recently gained significant attention partly due to large language models. In particular, we investigate how to use cloud functions to build high-performance and cost-efficient vector databases. This presents significant challenges in terms of how to perform sharding, how to reduce communication overhead, and how to minimize cold-start times. In this paper, we introduce Vexless, the first vector database system optimized for cloud functions. We present three optimizations to address the challenges. To perform sharding, we propose a global coordinator (orchestrator) that assigns workloads to Cloud function instances based on their available hardware resources. To overcome communication overhead, we propose the use of stateful cloud functions, eliminating the need for costly communications during synchronization. To minimize cold-start overhead, we introduce a workload-aware Cloud function lifetime management strategy. Vexless has been implemented using Azure Functions. Experimental results demonstrate that Vexless can significantly reduce costs, especially on bursty and sparse workloads, compared to cloud VM instances, while achieving similar or higher query performance and accuracy.}
}


@article{DBLP:journals/pacmmod/BaR24,
	author = {Jinsheng Ba and
                  Manuel Rigger},
	title = {Keep It Simple: Testing Databases via Differential Query Plans},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {3},
	pages = {188},
	year = {2024},
	url = {https://doi.org/10.1145/3654991},
	doi = {10.1145/3654991},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BaR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query optimizers perform various optimizations, many of which have been proposed to optimize joins. It is pivotal that these optimizations are correct, meaning that they should be extensively tested. Besides manually written tests, automated testing approaches have gained broad adoption. Such approaches semi-randomly generate databases and queries. More importantly, they provide a so-called test oracle that can deduce whether the system's result is correct. Recently, researchers have proposed a novel testing approach called Transformed Query Synthesis (TQS) specifically designed to find logic bugs in join optimizations. TQS is a sophisticated approach that splits a given input table into several sub-tables and validates the results of the queries that join these sub-tables by retrieving the given table. We studied TQS's bug reports, and found that 14 of 15 unique bugs were reported by showing discrepancies in executing the same query with different query plans. Therefore, in this work, we propose a simple alternative approach to TQS. Our approach enforces different query plans for the same query and validates that the results are consistent. We refer to this approach as Differential Query Plan (DQP) testing. DQP can reproduce 14 of the 15 unique bugs found by TQS, and found 26 previously unknown and unique bugs. These results demonstrate that a simple approach with limited novelty can be as effective as a complex, conceptually appealing approach. Additionally, DQP is complementary to other testing approaches for finding logic bugs. 81% of the logic bugs found by DQP cannot be found by NoREC and TLP, whereas DQP overlooked 86% of the bugs found by NoREC and TLP. We hope that the practicality of our approach---we implemented in less than 100 lines of code per system---will lead to its wide adoption.}
}


@article{DBLP:journals/pacmmod/AgrawalAH24,
	author = {Divyakant Agrawal and
                  Azza Abouzied and
                  Joseph M. Hellerstein},
	title = {{PACMMOD} Volume 2 Issue 4: Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {189:1--189:2},
	year = {2024},
	url = {https://doi.org/10.1145/3677125},
	doi = {10.1145/3677125},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgrawalAH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We are pleased to present the 4th issue of Volume 2 of PACMMOD. This issue contains papers that were submitted to the SIGMOD research track in January 2024. Papers accepted in this issue have been invited for presentation in the research track of the ACM SIGMOD Conference on Management of Data 2025, to be held in Berlin, Germany. We look forward to seeing you in Berlin from the 22nd to the 27th June, 2025.}
}


@article{DBLP:journals/pacmmod/KamalB24,
	author = {Raj Kamal and
                  Amitabha Bagchi},
	title = {A Lov{\'{a}}sz-Simonovits Theorem for Hypergraphs with Application
                  to Local Clustering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {190:1--190:27},
	year = {2024},
	url = {https://doi.org/10.1145/3677126},
	doi = {10.1145/3677126},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KamalB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present the first analysis of diffusion on hypergraphs based on the Lovász-Simonovits theory. We demonstrate that an averaging-based diffusion operator is the appropriate generalization of the lazy random walk diffusion on 2-graphs because the diffusion rapidly converges to its stationary state from any initial state. By proving a Lovász-Simonovits-like theorem for this diffusion, we show that the diffusion rate depends on the hypergraph's conductance. To use averaging-based diffusion for clustering, we define a generalization of personalized page rank for hypergraphs, which we call ''Averaging-based Personalized Page Rank for Hypergraphs'' (APPRH). The fact that averaging-based diffusion is linear, unlike previous hypergraph diffusions used for clustering in the literature, allows us to use the Forward Push algorithm to compute APPRH efficiently. Using this method, we obtain theoretical bounds for the conductance of our clustering that are at least a constant times better than the best-known bounds in the literature. We compare our algorithm A- HyperCut  against baselines on million-scale hypergraphs and find that our method is an order of magnitude faster while being competitive regarding the conductance of the local clusters produced.}
}


@article{DBLP:journals/pacmmod/SunWWLLJ24,
	author = {Peng Sun and
                  Liantao Wu and
                  Zhibo Wang and
                  Jinfei Liu and
                  Juan Luo and
                  Wenqiang Jin},
	title = {A Profit-Maximizing Data Marketplace with Differentially Private Federated
                  Learning under Price Competition},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {191:1--191:27},
	year = {2024},
	url = {https://doi.org/10.1145/3677127},
	doi = {10.1145/3677127},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SunWWLLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of machine learning (ML) applications has given rise to a new and popular data marketplace paradigm. These marketplaces facilitate ML model requesters in obtaining data from data owners to train their desired models. To mitigate the privacy concerns of data owners, federated learning (FL) has been introduced, enabling collaborative model training without raw data trading. Furthermore, researchers have incorporated differential privacy (DP) techniques into FL, resulting in differentially private federated learning (DPFL) to enhance privacy preservation. However, existing designs of DPFL-based data marketplaces consider a  simplified but unrealistic  scenario where the model requester holds dominant market power, and data owners cannot set their own prices. In this work, we propose a novel DPFL-based data marketplace that accommodates both price-taking and price-setting data owners. We model the interactions among the model requester and these two types of data owners as a three-stage Stackelberg game, focusing on maximizing the model requester's profit. We rigorously establish that the formulated game is a convex game with a unique subgame perfect equilibrium. Moreover, we devise iterative algorithms to determine the equilibrium strategies for the model requester and price-setting data owners. Notably, our algorithms allow data owners to operate without requiring complete information about the model requester or other data owners. Numerical experiments demonstrate the superiority of our proposed three-stage framework in terms of the model requester's profitability compared to scenarios where only price-taking data owners are involved. Furthermore, we reveal that price competition among price-setting data owners reduces equilibrium market prices.}
}


@article{DBLP:journals/pacmmod/WenMTTBCFJP24,
	author = {Richard Wen and
                  Hunter McCoy and
                  David Tench and
                  Guido Tagliavini and
                  Michael A. Bender and
                  Alex Conway and
                  Martin Farach{-}Colton and
                  Rob Johnson and
                  Prashant Pandey},
	title = {Adaptive Quotient Filters},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {192:1--192:28},
	year = {2024},
	url = {https://doi.org/10.1145/3677128},
	doi = {10.1145/3677128},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WenMTTBCFJP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Filters trade off accuracy for space and occasionally return false positive matches with a bounded error. Numerous systems use filters in fast memory to avoid performing expensive I/Os to slow storage. A fundamental limitation in traditional filters is that they do not change their representation upon seeing a false positive match. Therefore, the maximum false positive rate is only guaranteed for a single query, not for an arbitrary set of queries. We can improve the filter's performance on a stream of queries, especially on a skewed distribution, if we can adapt after encountering false positives. Adaptive filters, such as telescoping quotient filters and adaptive cuckoo filters, update their representation upon detecting a false positive to avoid repeating the same error in the future. Adaptive filters require an auxiliary structure, typically much larger than the main filter and often residing on slow storage, to facilitate adaptation. However, existing adaptive filters are not practical and have not been adopted in real-world systems for two main reasons. First, they offer weak adaptivity guarantees, meaning that fixing a new false positive can cause a previously fixed false positive to come back. Secondly, the sub-optimal design of the auxiliary structure results in adaptivity overheads so substantial that they can actually diminish overall system performance compared to a traditional filter. In this paper, we design and implement the \\sysname, the first practical adaptive filter with minimal adaptivity overhead and strong adaptivity guarantees, which means that the performance and false-positive guarantees continue to hold even for adversarial workloads. The \\sysname is based on the state-of-the-art quotient filter design and preserves all the critical features of the quotient filter such as cache efficiency and mergeability. Furthermore, we employ a new auxiliary structure design which results in considerably low adaptivity overhead and makes the \\sysname practical in real systems. We evaluate the \\sysname by using it to filter queries to an on-disk B-tree database and find no negative impact on insert or query performance compared to traditional filters. Against adversarial workloads, the \\sysname preserves system performance, whereas traditional filters incur 2× slowdown from adversaries representing as low as 1% of the workload. Finally, we show that on skewed query workloads, the \\sysname can reduce the false-positive rate 100× using negligible (1/1000th of a bit per item) space overhead.}
}


@article{DBLP:journals/pacmmod/ZhouYYLJC24,
	author = {Qihui Zhou and
                  Peiqi Yin and
                  Xiao Yan and
                  Changji Li and
                  Guanxian Jiang and
                  James Cheng},
	title = {Atom: An Efficient Query Serving System for Embedding-based Knowledge
                  Graph Reasoning with Operator-level Batching},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {193:1--193:29},
	year = {2024},
	url = {https://doi.org/10.1145/3677129},
	doi = {10.1145/3677129},
	timestamp = {Sun, 19 Jan 2025 15:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhouYYLJC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graph reasoning (KGR) answers logical queries over a knowledge graph (KG), and  embedding-based KGR  (EKGR) becomes popular recently, which embeds both queries and KG entities such that the vector embeddings of a query and its answer entities are similar. Compared with traditional KGR methods based on subgraph matching, EKGR produces fewer intermediate results and is more robust to missing and noisy information in the KG. However, existing systems are inefficient for serving online EKGR queries because they can only batch queries of the same type for execution (i.e.,  query-level batching  ) and hence have limited batching opportunities due to the heterogeneity of queries. To serve EKGR queries efficiently, we propose the Atom system with  operator-level batching,  which decomposes queries into operators and batches operators of the same type from different queries for execution. The insight is that the types of operators are far fewer than the types of queries, and thus different queries typically share common operators, yielding more batching opportunities. To schedule the operators, Atom adopts a  hybrid policy,  which improves system throughput and avoids starving rare operators. For efficiency, Atom incorporates system optimizations including  two-level pipeline, opportunistic submission, pre-allocated memory buffer,  and  tailored GPU kernels.  Experiment results show that compared with existing systems, Atom can improve query throughput by over 20x and reduce query latency by over 5x. Micro experiments suggest that the designs and optimizations are effective in improving system performance.}
}


@article{DBLP:journals/pacmmod/GuFYCLZ24,
	author = {Tu Gu and
                  Kaiyu Feng and
                  Jingyi Yang and
                  Gao Cong and
                  Cheng Long and
                  Rui Zhang},
	title = {BT-Tree: {A} Reinforcement Learning Based Index for Big Trajectory
                  Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {194:1--194:27},
	year = {2024},
	url = {https://doi.org/10.1145/3677130},
	doi = {10.1145/3677130},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GuFYCLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing availability of trajectory data, it is important to have good indexes to facilitate query processing. In this work, we propose BT-Tree, which is built through a recursive bi-partitioning approach, for the processing of range and KNN queries for past trajectory data. We first propose a cost function based method (CFBM) to build the BT-Tree. Specifically, we design a novel cost function, which incorporates the characteristics of both the data and historical query workload, to decide how to partition a BT-Tree node. Then we propose a reinforcement learning (RL) based method to address CFBM's limitations, such as making locally optimal decisions that may lead to global suboptimality. Experiments on three real datasets with up to 800 million data points show that the CFBM generally outperforms the baselines in terms of query processing time and the RL based method consistently outperforms the baselines and has more significant advantages on larger datasets.}
}


@article{DBLP:journals/pacmmod/FanHXZ24,
	author = {Wenfei Fan and
                  Ziyan Han and
                  Min Xie and
                  Guangyi Zhang},
	title = {Discovering Top-k Relevant and Diversified Rules},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {195:1--195:28},
	year = {2024},
	url = {https://doi.org/10.1145/3677131},
	doi = {10.1145/3677131},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FanHXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies the problem of discovering top- k  relevant and diversified rules. Given a real-life dataset, it is to mine a set of  k  rules that are as close to users' interest as possible, and meanwhile, as diverse to each other as possible. It aims to reduce excessive irrelevant rules commonly returned by rule discovery. As a testbed, we consider Entity Enhancing Rules (REEs), which subsume popular data quality rules as special cases. We train a relevance model to learn users' prior knowledge, rank rules based on users' need, and propose four diversity measures to assess the diversity between rules. Based on these measures, we formulate a new discovery problem. We show that the bi-criteria discovery problem is NP-complete and hard to approximate. This said, we develop a practical algorithm for the problem, and prove its approximation bounds under certain conditions. Moreover, we develop optimization techniques to speed up the process, and parallelize the algorithm such that it guarantees to reduce runtime when given more processors. Using real-life data, we empirically verify that on average, the top-10 REEs discovered by our algorithm is able to catch 77.5% of errors detected by the entire set Σ all  of REEs and achieve F_1 = 0.74 for real error detection; moreover, discovering top-ranked REEs is 62.4X faster than mining Σ all  .}
}


@article{DBLP:journals/pacmmod/WuWQCL24,
	author = {Siyue Wu and
                  Dingming Wu and
                  Junyi Quan and
                  Tsz Nam Chan and
                  Kezhong Lu},
	title = {Efficient and Accurate PageRank Approximation on Large Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {196:1--196:26},
	year = {2024},
	url = {https://doi.org/10.1145/3677132},
	doi = {10.1145/3677132},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuWQCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {PageRank is a commonly used measurement in a wide range of applications, including search engines, recommendation systems, and social networks. However, this measurement suffers from huge computational overhead, which cannot be scaled to large graphs. Although many approximate algorithms have been proposed for computing PageRank values, these algorithms are either (i) not efficient or (ii) not accurate. Worse still, some of them cannot provide estimated PageRank values for all the vertices. In this paper, we first propose the CUR-Trans algorithm, which can reduce the time complexity for computing PageRank values and has lower error bound than existing matrix approximation-based PageRank algorithms. Then, we develop the  T 2 -Approx algorithm to further reduce the time complexity for computing this measurement. Experiment results on three large-scale graphs show that both the CUR-Trans algorithm and the  T 2 -Approx algorithm achieve the lowest response time for computing PageRank values with the best accuracy (for the CUR-Trans algorithm) or the competitive accuracy (for the  T 2 -Approx algorithm). Besides, the two proposed algorithms are able to provide estimated PageRank values for all the vertices.}
}


@article{DBLP:journals/pacmmod/FengCGZW24,
	author = {Chen Feng and
                  Xingguang Chen and
                  Qintian Guo and
                  Fangyuan Zhang and
                  Sibo Wang},
	title = {Efficient Approximation Algorithms for Minimum Cost Seed Selection
                  with Probabilistic Coverage Guarantee},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {197:1--197:26},
	year = {2024},
	url = {https://doi.org/10.1145/3677133},
	doi = {10.1145/3677133},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FengCGZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a social network  G , a cost associated with each user, and an influence threshold η, the minimum cost seed selection problem (MCSS) aims to find a set of seeds that minimizes the total cost to reach η users. Existing works are mainly devoted to providing an <u>e</u>xpected <u>c</u>overage <u>g</u>uarantee on reaching η, classified as MCSS-ECG, where their solutions either rely on an impractical influence oracle or cannot attain the expected influence threshold. More importantly, due to the expected coverage guarantee, the actual influence in a campaign may drift from the threshold evidently. Thus, the advertisers would like to request for a probability guarantee of reaching η. This motivates us to further solve the MCSS problem with a <u>p</u>robabilistic <u>c</u>overage <u>g</u>uarantee, termed MCSS-PCG. In this paper, we first propose our algorithm CLEAR to solve MCSS-ECG, which reaches the expected influence threshold without any influence oracle or influence shortfall but a practical approximation ratio. However, the ratio involves an unknown term (i.e., the optimal cost). Thus, we further devise the STAR method to derive a lower bound of the optimal cost and then obtain the first explicit approximation ratio for MCSS-ECG. In MCSS-PCG, it is necessary to estimate the probability that the current seeds reach η, to decide when to stop seed selection. To achieve this, we design a new technique named MRR, which provides efficient probability estimation with a theoretical guarantee. With MRR in hand, we propose our algorithm SCORE for MCSS-PCG, whose performance guarantee is derived by measuring the gap between MCSS-ECG and MCSS-PCG, and applying the theoretical results in MCSS-ECG. Finally, extensive experiments demonstrate that our algorithms achieve up to two orders of magnitude speed-up compared to alternatives while meeting the requirement of MCSS-PCG with the smallest cost.}
}


@article{DBLP:journals/pacmmod/TangZZLHD24,
	author = {Xilin Tang and
                  Feng Zhang and
                  Shuhao Zhang and
                  Yani Liu and
                  Bingsheng He and
                  Xiaoyong Du},
	title = {Enabling Adaptive Sampling for Intra-Window Join: Simultaneously Optimizing
                  Quantity and Quality},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {198:1--198:31},
	year = {2024},
	url = {https://doi.org/10.1145/3677134},
	doi = {10.1145/3677134},
	timestamp = {Mon, 03 Mar 2025 22:20:18 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/TangZZLHD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {>Sampling is one of the most widely employed approximations in big data processing. Among various challenges in sampling design, sampling for join is particularly intriguing yet complex. This perplexing problem starts with a classical case where the join of two Bernoulli samples shrinks its output size quadratically and exhibits a strong dependency on the input data, presenting a unique challenge that necessitates adaptive sampling to guarantee both the quantity and quality of the sampled data. The community has made strides in achieving this goal by constructing offline samples and integrating support from indexes or key frequencies. However, when dealing with stream data, due to the need for real-time processing and high-quality analysis, methods developed for processing static data become unavailable. Consequently, a fundamental question arises: Is it possible to achieve adaptive sampling in stream data without relying on offline techniques? To address this problem, we propose FreeSam, which couples hybrid sampling with intra-window join, a key stream join operator. Our focus lies on two widely used metrics: output size, ensuring quantity, and variance, ensuring quality. FreeSam enables adaptability in both the desired quantity and quality of data sampling by offering control on the two-dimensional space spanned by these metrics. Meanwhile, adjustable trade-offs between quality and performance make FreeSam practical for use. Our experiments show that, for every 1% increase in latency limitation, FreeSam can yield a 3.83% increase in the output size while maintaining the level of the estimator's variance. Additionally, we give FreeSam a multi-core implementation and ensure predictability of its latency through both an analytic model and a neural network model. The accuracy of these models is 88.05% and 96.75% respectively.}
}


@article{DBLP:journals/pacmmod/LiuZZJ24,
	author = {Wei Liu and
                  Wei Zhang and
                  Haiyan Zhao and
                  Zhi Jin},
	title = {GABoost: Graph Alignment Boosting via Local Optimum Escape},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {199:1--199:26},
	year = {2024},
	url = {https://doi.org/10.1145/3677135},
	doi = {10.1145/3677135},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuZZJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous graphs provide a universal data structure for representing various kinds of structured data in numerous domains. The  graph alignment  problem aims to find the correspondences of vertices in different graphs, playing a fundamental role in many downstream tasks of heterogeneous graph mining. In recent years, many graph alignment methods have been proposed, ranging from  classical optimization methods ,  spectral methods , to  embedding learning based-methods . Due to the problem's complexity, the result found by most existing methods is either a  heuristic solution  or a  critical point  in the solution space. In this paper, we propose GABoost, a graph alignment boosting algorithm that takes as input an initial alignment between two heterogeneous graphs and outputs a boosted alignment via an iterative  local-optimum-escape  process. One of the distinctive features of GABoost is that it can be sequentially composed with any graph alignment methods to improve the output of upstream methods. To examine the effectiveness of GABoost, we select 7 upstream methods of graph alignment as well as 6 real-world datasets, and quantitatively investigate the degree to which GABoost boosts these methods. The results show that GABoost improves the alignment accuracy of the 7 upstream methods by 25.25% on average with acceptable time overhead.}
}


@article{DBLP:journals/pacmmod/PengZD24,
	author = {Zhencan Peng and
                  Yuheng Zhang and
                  Dong Deng},
	title = {Near-Duplicate Text Alignment with One Permutation Hashing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {200:1--200:26},
	year = {2024},
	url = {https://doi.org/10.1145/3677136},
	doi = {10.1145/3677136},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/PengZD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies the near-duplicate text alignment problem under the constraint of Jaccard similarity. Specifically, given a collection of long texts and a short query text, this problem finds all the  subsequences  in each text whose Jaccard similarities to the query are no smaller than a given threshold. Near-duplicate text alignment is computationally intensive. This is because there are O(n 2 ) subsequences in a text with  n  tokens. To remedy this issue, a few recent studies propose to first generate the min-hash sketch of every subsequence in each text and then find all the subsequences whose min-hash sketches are similar to that of the query. They introduce the concept of "compact windows" and show that the O(n 2 k) min-hashes in a text with  n  tokens can be losslessly compressed in compact windows using O(nk) space, where  k  is the sketch size. However, the space cost O(nk) is still too high for long texts, especially when the sketch size  k  is large. To address this issue, we propose to use One Permutation Hashing (OPH) to generate the min-hash sketch and introduce the concept of "OPH compact windows". Although the size of each sketch remains the same, which is O(k), we prove that all the O(n 2 k) min-hashes generated by OPH in a text with  n  tokens can be losslessly compressed in OPH compact windows using only O(n+k) space. Note the generation of OPH compact windows does not necessitate the enumeration of the O(n 2 k) min-hashes. Moreover, we develop an algorithm to find all the sketches in a text similar to that of the query directly from OPH compact windows, along with three optimizations.We conduct extensive experiments on three real-world datasets. Empirical results show our proposed algorithms significantly outperformed existing methods in terms of index cost and query latency and scaled well.}
}


@article{DBLP:journals/pacmmod/HuiYQGLLRMLW24,
	author = {Yujie Hui and
                  Miao Yu and
                  Hao Qi and
                  Yifan Gan and
                  Tianxi Li and
                  Yuke Li and
                  Xueyuan Ren and
                  Sixiang Ma and
                  Xiaoyi Lu and
                  Yang Wang},
	title = {On the Feasibility and Benefits of Extensive Evaluation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {201:1--201:24},
	year = {2024},
	url = {https://doi.org/10.1145/3677137},
	doi = {10.1145/3677137},
	timestamp = {Sun, 19 Jan 2025 15:06:01 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HuiYQGLLRMLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benchmark and system parameters often have a significant impact on performance evaluation, which raises a long-lasting question about which settings we should use. This paper studies the feasibility and benefits of extensive evaluation. A full extensive evaluation, which tests all possible settings, is usually too expensive. This work investigates whether it is possible to sample a subset of the settings and, upon them, generate observations that match those from a full extensive evaluation. Towards this goal, we have explored the incremental sampling approach, which starts by measuring a small subset of random settings, builds a prediction model on these samples using the popular ANOVA approach, adds more samples if the model is not accurate enough, and terminates otherwise. To summarize our findings: 1) Enhancing a research prototype to support extensive evaluation mostly involves changing hard-coded configurations, which does not take much effort. 2) Some systems are highly predictable, which means that they can achieve accurate predictions with a low sampling rate, but some systems are less predictable. 3) We have not found a method that can consistently outperform random sampling + ANOVA. Based on these findings, we provide recommendations to improve artifact predictability and strategies for selecting parameter values during evaluation.}
}


@article{DBLP:journals/pacmmod/YuLYC24,
	author = {Weiping Yu and
                  Siqiang Luo and
                  Zihao Yu and
                  Gao Cong},
	title = {{CAMAL:} Optimizing LSM-trees via Active Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {202:1--202:26},
	year = {2024},
	url = {https://doi.org/10.1145/3677138},
	doi = {10.1145/3677138},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuLYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We use machine learning to optimize LSM-tree structure, aiming to reduce the cost of processing various read/write operations. We introduce a new approach CAMAL, which boasts the following features: (1)  ML-Aided : CAMAL is the first attempt to apply active learning to tune LSM-tree based key-value stores. The learning process is coupled with traditional cost models to improve the training process; (2)  Decoupled Active Learning : backed by rigorous analysis, CAMAL adopts active learning paradigm based on a decoupled tuning of each parameter, which further accelerates the learning process; (3)  Easy Extrapolation : CAMAL adopts an effective mechanism to incrementally update the model with the growth of the data size; (4)  Dynamic Mode : CAMAL is able to tune LSM-tree online under dynamically changing workloads; (5)  Significant System Improvement : By integrating CAMAL into a full system RocksDB, the system performance improves by 28% on average and up to 8x compared to a state-of-the-art RocksDB design.}
}


@article{DBLP:journals/pacmmod/MaCVHSR24,
	author = {Lei Ma and
                  Lei Cao and
                  Peter M. VanNostrand and
                  Dennis M. Hofmann and
                  Yao Su and
                  Elke A. Rundensteiner},
	title = {Pluto: Sample Selection for Robust Anomaly Detection on Polluted Log
                  Data},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {203:1--203:25},
	year = {2024},
	url = {https://doi.org/10.1145/3677139},
	doi = {10.1145/3677139},
	timestamp = {Sun, 19 Jan 2025 15:06:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/MaCVHSR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log anomaly detection, critical in identifying system failures and preempting security breaches, finds irregular patterns within large volumes of log data. Modern log anomaly detectors rely on training deep learning models on clean anomaly-free log data. However, such clean log data requires expensive and tedious human labeling. In this paper, we thus propose a robust log anomaly detection framework, PlutoNOSPACE, that automatically selects a clean representative sample subset of the polluted log sequence data to train a Transformer-based anomaly detection model. Pluto features three innovations. First, due to localized concentrations of anomalies inherent in the embedding space of log data, Pluto partitions the sequence embedding space generated by the model into regions that then allow it to identify and discard regions that are highly polluted by our pollution level estimation scheme, based on our pollution quantification via Gaussian mixture modeling. Second, for the remaining more slightly polluted regions, we select samples that maximally purify the eigenvector spectrum, which can be transformed into the NP-hard facility location problem; allowing us to leverage its greedy solution with a (1-(1/e)) approximation guarantee in optimality. Third, by iteratively alternating between the above subset selection, a model re-training on the latest subset, and a subset filtering using dynamic training artifacts generated by the latest model, the data selected is progressively refined. The final sample set is used to retrain the final anomaly detection model. Our experiments on four real-world log benchmark datasets demonstrate that by retaining 77.7% (BGL) to 96.6% (ThunderBird) of the normal sequences while effectively removing 90.3% (BGL) to 100.0% (ThunderBird, HDFS) of the anomalies, Pluto provides a significant absolute F-1 improvement up to 68.86% (2.16% → 71.02%) compared to the state-of-the-art sample selection methods. The implementation of this work is available at https://github.com/LeiMa0324/Pluto-SIGMOD25.}
}


@article{DBLP:journals/pacmmod/WuCPCAR24,
	author = {Renzhi Wu and
                  Pramod Chunduri and
                  Ali Payani and
                  Xu Chu and
                  Joy Arulraj and
                  Kexin Rong},
	title = {SketchQL: Video Moment Querying with a Visual Query Interface},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {204:1--204:27},
	year = {2024},
	url = {https://doi.org/10.1145/3677140},
	doi = {10.1145/3677140},
	timestamp = {Fri, 23 May 2025 21:09:09 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/WuCPCAR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Localizing video moments based on the movement patterns of objects is an important task in video analytics. Existing video analytics systems offer two types of querying interfaces based on natural language and SQL, respectively. However, both types of interfaces have major limitations. SQL-based systems require high query specification time, whereas natural language-based systems require large training datasets to achieve satisfactory retrieval accuracy. To address these limitations, we present SketchQL, a video database management system (VDBMS) for offline, exploratory video moment retrieval that is both easy to use and generalizes well across multiple video moment datasets. To improve ease-of-use, SketchQL features a  visual query interface  that enables users to sketch complex visual queries through intuitive drag-and-drop actions. To improve generalizability, SketchQL operates on object-tracking primitives that are reliably extracted across various datasets using pre-trained models. We present a learned similarity search algorithm for retrieving video moments closely matching the user's visual query based on object trajectories. SketchQL trains the model on a diverse dataset generated with a novel simulator, that enhances its accuracy across a wide array of datasets and queries. We evaluate SketchQL on four real-world datasets with nine queries, demonstrating its superior usability and retrieval accuracy over state-of-the-art VDBMSs.}
}


@article{DBLP:journals/pacmmod/LiuLZT24,
	author = {Haotian Liu and
                  Runzhong Li and
                  Ziyang Zhang and
                  Bo Tang},
	title = {Tao: Improving Resource Utilization while Guaranteeing {SLO} in Multi-tenant
                  Relational Database-as-a-Service},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {205:1--205:26},
	year = {2024},
	url = {https://doi.org/10.1145/3677141},
	doi = {10.1145/3677141},
	timestamp = {Sun, 19 Jan 2025 15:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuLZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is an open challenge for cloud database service providers to guarantee tenants' service-level objectives (SLOs) and enjoy high resource utilization simultaneously. In this work, we propose a novel system Tao to overcome it. Tao consists of three key components: (i) tasklet-based DAG generator, (ii) tasklet-based DAG executor, and (iii) SLO-guaranteed scheduler. The core concept in Tao is tasklet, a coroutine-based lightweight execution unit of the physical execution plan. In particular, we first convert each SQL operator in the traditional physical execution plan into a set of fine-grained tasklets by the tasklet-based DAG generator. Then, we abstract the tasklet-based DAG execution procedure and implement the tasklet-based DAG executor using C++20 coroutines. Finally, we introduce the SLO-guaranteed scheduler for scheduling tenants' tasklets across CPU cores. This scheduler guarantees tenants' SLOs with a token bucket model and improves resource utilization with an on-demand core adjustment strategy. We build Tao on an open-sourced relational database, Hyrise, and conduct extensive experimental studies to demonstrate its superiority over existing solutions.}
}


@article{DBLP:journals/pacmmod/DaiLCW24,
	author = {Qiangqiang Dai and
                  Ronghua Li and
                  Donghang Cui and
                  Guoren Wang},
	title = {Theoretically and Practically Efficient Maximum Defective Clique Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {4},
	pages = {206:1--206:27},
	year = {2024},
	url = {https://doi.org/10.1145/3677142},
	doi = {10.1145/3677142},
	timestamp = {Wed, 06 Nov 2024 22:18:39 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DaiLCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The study of  k -defective cliques, defined as induced subgraphs that differ from cliques by at most  k  missing edges, has attracted much attention in graph analysis due to their relevance in various applications, including social network analysis and implicit interaction predictions. However, determining the maximum  k -defective clique in graphs has been proven to be an NP-hard problem, presenting significant challenges in finding an efficient solution. To address this problem, we develop a theoretically and practically efficient algorithm that leverages newly-designed branch reduction rules and a pivot-based branching technique. Our analysis establishes that the time complexity of the proposed algorithm is bounded by O(mγ k n ), where γ k  is a real value strictly less than 2 (e.g., when k= 1, 2, and 3, γ k = 1.466, 1.755, and 1.889, respectively). To our knowledge, this algorithm achieves the best worst-case time complexity to date compared to state-of-the-art solutions. Moreover, to further reduce unnecessary branches, we propose a time-efficient upper bound-based pruning technique, which is obtained by manipulating information such as the number of distinct colors assigned to vertices and the presence of non-neighbors among them. Additionally, we employ an ordering-based heuristic approach as a preprocessing step to improve computational efficiency. Finally, we conduct extensive experiments on a diverse set of over 300 graphs to evaluate the efficiency of the proposed solutions. The results demonstrate that our algorithm achieves a speedup of 3 orders of magnitude over state-of-the-art solutions in processing most of real-world graphs.}
}


@article{DBLP:journals/pacmmod/CarmeliGK24,
	author = {Nofar Carmeli and
                  Floris Geerts and
                  Benny Kimelfeld},
	title = {PACMMOD, V2, {N5} (PODS), November 2024 Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {207:1--207:3},
	year = {2024},
	url = {https://doi.org/10.1145/3695825},
	doi = {10.1145/3695825},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CarmeliGK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This editorial accompanies the second issue dedicated to the PODS research track of the Proceedings of the ACM on Management of Data (PACMMOD) journal. The journal hosts a SIGMOD and a PODS research track. The PODS research track aims to provide a solid scientific basis for methods, techniques, and solutions for the data management challenges that continually arise in our data-driven society. Articles for the PODS track of PACMMOD present principled contributions to modeling, application, system building, and both theoretical and experimental validation in the context of data management. Such articles might be based, among others, on establishing theoretical results, developing new concepts and frameworks that deserve further exploration, providing experimental work that sheds light on the scientific foundations of the discipline, or conducting a rigorous analysis of industry artifacts that are widely used and/or recently developed. Articles published in this track will be invited for presentation to the ACM Symposium on Principles of Database Systems (PODS), which is held jointly with SIGMOD each year.}
}


@article{DBLP:journals/pacmmod/BiswasCKSZ24,
	author = {Ari Biswas and
                  Graham Cormode and
                  Yaron Kanza and
                  Divesh Srivastava and
                  Zhengyi Zhou},
	title = {Differentially Private Hierarchical Heavy Hitters},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {208:1--208:25},
	year = {2024},
	url = {https://doi.org/10.1145/3695826},
	doi = {10.1145/3695826},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BiswasCKSZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The task of finding  Hierarchical  Heavy Hitters (HHH) was introduced by Cormode et al. [12] as a generalisation of the heavy hitter problem. While finding HHH in data streams has been studied extensively, the question of releasing HHH when the underlying data is private remains unexplored. In this paper, we formalise and study the notion of differentially private HHH, in both the streaming and non-streaming setting. In the non-streaming setting, we show the surprising result that the relative error in estimating the count for any prefix is  independent  of the height of the hierarchy and the number of heavy hitters in the stream. Additionally, our algorithms also improve the error guarantees of Ghazi et al. [24] for the problem of counting over trees. Meanwhile, in the streaming setting, the main issue is that although the exact version of HHH has low global sensitivity (as counting queries are 1-sensitive), the approximation functions due to streaming have high global sensitivity, linear in the available space. Despite this obstacle, we show that the absolute error for estimating frequencies in the streaming setting is independent of the available space.}
}


@article{DBLP:journals/pacmmod/GanUWWZ24,
	author = {Junhao Gan and
                  Seeun William Umboh and
                  Hanzhi Wang and
                  Anthony Wirth and
                  Zhuo Zhang},
	title = {Optimal Dynamic Parameterized Subset Sampling},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {209:1--209:26},
	year = {2024},
	url = {https://doi.org/10.1145/3695827},
	doi = {10.1145/3695827},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GanUWWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the Dynamic Parameterized Subset Sampling (DPSS) problem in the Word RAM model. In DPSS, the input is a set,  S , of  n  items, where each item,  x , has a non-negative integer weight, w(x). Given a pair of query parameters, (α, β), each of which is a non-negative rational number, a parameterized subset sampling query on  S  seeks to return a subset T ⊆ S such that each item x∈ S is selected in  T , independently, with probability p_x(α, β) which is the minimum between 1 and w(x) / (α \\cdot W + β), where  W  is the total weight of the items in  S . More specifically, the DPSS problem is defined in a dynamic setting, where the item set,  S , can be updated with insertions of new items or deletions of existing items. Our first main result is an optimal algorithm for solving the DPSS problem, which achieves O(n) pre-processing time, O(1+μ_S(α,β)) expected time for each query parameterized by (α, β), given on-the-fly, and O(1) time for each update; here, μ_S(α,β) is the expected size of the query result. At all times, the worst-case space consumption of our algorithm is linear in the current number of items in  S . Our second main contribution is a hardness result for the DPSS problem when the item weights are O(1)-word float numbers, rather than integers. Specifically, we reduce Integer Sorting to the deletion-only DPSS problem with float item weights. Our reduction shows that an optimal algorithm for deletion-only DPSS with float item weights (achieving all the same bounds as aforementioned) implies an algorithm for sorting  N  integers in O(N) expected time. The latter remains an important open problem. Moreover, a deletion-only DPSS algorithm which supports float item weights, with complexities worse, by at most a factor of o(√łog łog N), than the optimal counterparts, would already improve the current-best integer sorting algorithm [FOCS 2002]. Last but not least, a key technical ingredient for our first main result is a set of exact and efficient algorithms for generating Bernoulli (of certain forms) and Truncated Geometric random variates in O(1) expected time with O(n) worst-case space in the Word RAM model. Generating Bernoulli and geometric random variates efficiently is of great importance not only to sampling problems but also to encryption in cybersecurity. We believe that our new algorithms may be of independent interests for related research.}
}


@article{DBLP:journals/pacmmod/WangT24,
	author = {Ru Wang and
                  Yufei Tao},
	title = {Optimal (Multiway) Spatial Joins},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {210:1--210:25},
	year = {2024},
	url = {https://doi.org/10.1145/3695828},
	doi = {10.1145/3695828},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/WangT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a  spatial join , we are given a constant number k ≥ 2 of sets - denoted as R1, R2, ..., Rk - containing axis-parallel rectangles in a 2D space. The objective is to report all k-tuples (r1, r2, ..., rk ) ∈ R1 × R2 × ... × Rk where the rectangles r1, r2, ..., rk have a non-empty intersection, i.e., r1 ∩ r2 ∩ ... ∩ rk ≠ ∅. The problem holds significant importance in spatial databases and has been extensively studied in the database community. In this paper, we show how to settle the problem in O(n logn + OUT) time - regardless of the constant k - where n = Ík i=1 |Ri | and OUT is the result size (i.e., the total number of k-tuples reported). The runtime is asymptotically optimal in the class of comparison-based algorithms, to which our solution belongs. Previously, the state of the art was an algorithm with running time O(n log 2k-1  n + OUT).}
}


@article{DBLP:journals/pacmmod/CalauttiLPS24b,
	author = {Marco Calautti and
                  Ester Livshits and
                  Andreas Pieris and
                  Markus Schneider},
	title = {Below and Above Why-Provenance for Datalog Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {211:1--211:21},
	year = {2024},
	url = {https://doi.org/10.1145/3695829},
	doi = {10.1145/3695829},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CalauttiLPS24b.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Datalog is a well-established rule-based language that allows us to express complex recursive queries. As for every other query language, explaining why a result to a Datalog query is obtained is an essential task towards explainable and transparent query evaluation. A standard way of explaining a query result is the so-called why-provenance, which provides information about the witnesses to a query result in the form of subsets of the input database that as a whole can be used to derive that result. The data complexity of why-provenance for Datalog queries has been recently studied and it was shown to be intractable, namely NP-complete even if the recursion is linear. An interesting question that comes up is whether we can ensure tractable data complexity by considering a slightly less informative notion of provenance, in particular, whyminimal-provenance that keeps only subset-minimal witnesses to a query result. Another interesting question is whether we can adopt a more informative notion of provenance than why-provenance, in particular, whymultiplicity-provenance that also keeps track of how many times a certain fact occurs in a witness to a query result, without paying a price in data complexity. This work provides definitive answers to the above questions: (i) whyminimal-provenance ensures tractable data complexity, and (ii) whymultiplicity-provenance can be adopted without paying a price in complexity, apart from one surprising case where the data complexity becomes PSPACE-complete.}
}


@article{DBLP:journals/pacmmod/GalhotraRS24,
	author = {Sainyam Galhotra and
                  Rahul Raychaudhury and
                  Stavros Sintos},
	title = {k-Clustering with Comparison and Distance Oracles},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {212:1--212:26},
	year = {2024},
	url = {https://doi.org/10.1145/3695830},
	doi = {10.1145/3695830},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GalhotraRS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we address clustering problems in scenarios where accurate direct access to the full dataset is impractical or impossible. Instead, we leverage oracle-based methods, which are particularly valuable in real-world applications where the data may be noisy, restricted due to privacy concerns or sheer volume. We utilize two oracles, the quadruplet and the distance oracle. The quadruplet oracle is a weaker oracle that only approximately compares the distances of two pairs of vertices. In practice, these oracles can be implemented using crowdsourcing or training classifiers or other predictive models. On the other hand, the distance oracle returns exactly the distance of two vertices, so it is a stronger and more expensive oracle to implement. We consider two noise models for the quadruplet oracle. In the adversarial noise model, if two pairs have similar distances, the response is chosen by an adversary. In the probabilistic noise model, the pair with the smaller distance is returned with a constant probability. We consider a set V of n vertices in a metric space that supports the quadruplet and the distance oracle. For each of the k-center, k-median, and k-means clustering problem on V, we design constant approximation algorithms that perform roughly O(nk) calls to the quadruplet oracle and O(k^2) calls to the distance oracle in both noise models. When the dataset has low intrinsic dimension, we significantly improve the approximation factors of our algorithms by performing a few additional calls to the distance oracle. We also show that for k-median and k-means clustering there is no hope to return any sublinear approximation using only the quadruplet oracle. Finally, we give constant approximation algorithms for estimating the clustering cost induced by any set of k vertices, performing roughly O(nk) calls to the quadruplet oracle and O(k^2) calls to the distance oracle.}
}


@article{DBLP:journals/pacmmod/EsmailpourS24,
	author = {Aryan Esmailpour and
                  Stavros Sintos},
	title = {Improved Approximation Algorithms for Relational Clustering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {213:1--213:27},
	year = {2024},
	url = {https://doi.org/10.1145/3695831},
	doi = {10.1145/3695831},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/EsmailpourS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustering plays a crucial role in computer science, facilitating data analysis and problem-solving across numerous fields. By partitioning large datasets into meaningful groups, clustering reveals hidden structures and relationships within the data, aiding tasks such as unsupervised learning, classification, anomaly detection, and recommendation systems. Particularly in relational databases, where data is distributed across multiple tables, efficient clustering is essential yet challenging due to the computational complexity of joining tables. This paper addresses this challenge by introducing efficient algorithms for k-median and k-means clustering on relational data without the need for pre-computing the join query results. For the relational k-median clustering, we propose the first efficient relative approximation algorithm. For the relational k-means clustering, our algorithm significantly improves both the approximation factor and the running time of the known relational k-means clustering algorithms, which suffer either from large constant approximation factors, or expensive running time. Given a join query q and a database instance D of O(N) tuples, for both k-median and k-means clustering on the results of q on D, we propose randomized (1+ε)γ-approximation algorithms that run in roughly O(k 2 N fhw  )+T_γ(k 2 ) time, where ε ∈ (0,1) is a constant parameter decided by the user, \\fhw is the fractional hyper-tree width of Q, while γ and T_γ(x) represent the approximation factor and the running time, respectively, of a traditional clustering algorithm in the standard computational setting over x points.}
}


@article{DBLP:journals/pacmmod/TaoWD24,
	author = {Yufei Tao and
                  Ru Wang and
                  Shiyuan Deng},
	title = {Parallel Communication Obliviousness: One Round and Beyond},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {214:1--214:24},
	year = {2024},
	url = {https://doi.org/10.1145/3695832},
	doi = {10.1145/3695832},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/TaoWD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies communication-oblivious algorithms under the massively parallel computation (MPC) model. The communication patterns of these algorithms follow a distribution dependent only on the definition of the underlying problem, the problem size N, and the number p of machines, but not on the specific input elements. Our objective is to understand when obliviousness necessitates --- or does not necessitate --- heavier communication compared to the traditional MPC model that does not enforce such a requirement. The first part of our investigation focuses on single-round algorithms. We prove that skew-free hashing, a fundamental problem solvable with load Õ(N/p) (with high probability or w.h.p.\\ for short) under the traditional model, demands a load of nearly Ω(N) under communication obliviousness. Intriguingly, we show that hashing can still be applied in an oblivious manner to process any natural join in one round with a load complexity matching that of the best traditional MPC algorithm. The second part of our investigation studies compilation methods that convert a traditional MPC algorithm A into a communication-oblivious counterpart. Given an A that operates within l = poly(p) rounds and entails a load at most L = Ω(p log p) w.h.p., we can produce w.h.p.\\ a communication-oblivious version running in 2l rounds with a load at most (1 + δ) L, where δ < 0 can be an arbitrarily small constant. Additionally, we establish hardness results indicating that the theoretical guarantees of our compilation can no longer be significantly improved.}
}


@article{DBLP:journals/pacmmod/ArenasMPR24,
	author = {Marcelo Arenas and
                  Timo Camillo Merkl and
                  Reinhard Pichler and
                  Cristian Riveros},
	title = {Towards Tractability of the Diversity of Query Answers: Ultrametrics
                  to the Rescue},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {215:1--215:26},
	year = {2024},
	url = {https://doi.org/10.1145/3695833},
	doi = {10.1145/3695833},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ArenasMPR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The set of answers to a query may be very large, potentially overwhelming users when presented with the entire set. In such cases, presenting only a small subset of the answers to the user may be preferable. A natural requirement for this subset is that it should be as diverse as possible to reflect the variety of the entire population. To achieve this, the diversity of a subset is measured using a metric that determines how different two solutions are and a diversity function that extends this metric from pairs to sets. In the past, several studies have shown that finding a diverse subset from an explicitly given set is intractable even for simple metrics (like Hamming distance) and simple diversity functions (like summing all pairwise distances). This complexity barrier becomes even more challenging when trying to output a diverse subset from a set that is only implicitly given (such as the query answers for a given query and a database). Until now, tractable cases have been found only for restricted problems and particular diversity functions. To overcome these limitations, we focus in this work on the notion of ultrametrics, which have been widely studied and used in many applications. Starting from any ultrametric  d  and a diversity function δ extending  d , we provide sufficient conditions over δ for having polynomial-time algorithms to construct diverse answers. To the best of our knowledge, these conditions are satisfied by all the diversity functions considered in the literature. Moreover, we complement these results with lower bounds that show specific cases when these conditions are not satisfied and finding diverse subsets becomes intractable. We conclude by applying these results to the evaluation of conjunctive queries, demonstrating efficient algorithms for finding a diverse subset of solutions for acyclic conjunctive queries when the attribute order is used to measure diversity.}
}


@article{DBLP:journals/pacmmod/PintoR24,
	author = {Dante Pinto and
                  Cristian Riveros},
	title = {Complex Event Recognition meets Hierarchical Conjunctive Queries},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {216:1--216:26},
	year = {2024},
	url = {https://doi.org/10.1145/3695834},
	doi = {10.1145/3695834},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/PintoR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hierarchical conjunctive queries (HCQ) are a subclass of conjunctive queries (CQ) with robust algorithmic properties. Among others, Berkholz, Keppeler, and Schweikardt have shown that HCQ is the subclass of CQ (without projection) that admits dynamic query evaluation with constant update time and constant delay enumeration. On a different but related setting stands Complex Event Recognition (CER), a prominent technology for evaluating sequence patterns over streams. Since one can interpret a data stream as an unbounded sequence of inserts in dynamic query evaluation, it is natural to ask to which extent CER can take advantage of HCQ to find a robust class of queries that can be evaluated efficiently. In this paper, we search to combine HCQ with sequence patterns to find a class of CER queries that can get the best of both worlds. To reach this goal, we propose a class of complex event automata model called Parallelized Complex Event Automata (PCEA) for evaluating CER queries with correlation (i.e., joins) over streams. This model allows us to express sequence patterns and compare values among tuples, but it also allows us to express conjunctions by incorporating a novel form of non-determinism that we call parallelization. We show that for every HCQ (under bag semantics), we can construct an equivalent PCEA. Further, we show that HCQ is the biggest class of full CQ that this automata model can define. Then, PCEA stands as a sweet spot that precisely expresses HCQ (i.e., among full CQ) and extends them with sequence patterns. Finally, we show that PCEA also inherits the good algorithmic properties of HCQ by presenting a streaming evaluation algorithm under sliding windows with logarithmic update time and output-linear delay for the class of PCEA with equality predicates.}
}


@article{DBLP:journals/pacmmod/AgarwalEHSY24,
	author = {Pankaj K. Agarwal and
                  Aryan Esmailpour and
                  Xiao Hu and
                  Stavros Sintos and
                  Jun Yang},
	title = {Computing {A} Well-Representative Summary of Conjunctive Query Results},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {217:1--217:27},
	year = {2024},
	url = {https://doi.org/10.1145/3695835},
	doi = {10.1145/3695835},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AgarwalEHSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data summarization is a powerful approach to deal with large-scale data analytics, which has wide applications in web search, recommendation systems, approximate query processing, etc. It computes a small, compact summary that preserves vital properties of the original data. In this paper, we study the data summarization problem of conjunctive query results, i.e., computing a k-size subset of a conjunctive query output, for any given k>0, that optimizes a certain objective. More specifically, we are interested in two commonly studied objectives: cohesion, which measures the maximum distance between a tuple in the query result tuples and its closest tuple in the summary (k-center clustering); and diversity, which measures the pairwise distances between the summary items. A simple approach that computes the entire query output and then applies existing algorithms on top of these materialized tuples suffers from high computational complexity because the query output can be large, e.g., for a relational database of N tuples, the number of result tuples can be N O(1).  We propose O(1)-approximation algorithms that compute well-representative summaries of size k in time O(N*k O(1)  ), or even O(N+ k O(1)  ) in some cases, without computing all result tuples. We also propose the first efficient (2+\\eps)-approximation algorithm for the k-center clustering problem over relational data. Our main idea is to formulate a few oracles that enable us to access specific query result tuples with certain properties, to show how these oracles can be implemented efficiently, and to compute desired summaries with few invocations of these oracles.}
}


@article{DBLP:journals/pacmmod/KhalfiouiW24,
	author = {Aziz Amezian El Khalfioui and
                  Jef Wijsen},
	title = {Computing Range Consistent Answers to Aggregation Queries via Rewriting},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {218:1--218:19},
	year = {2024},
	url = {https://doi.org/10.1145/3695836},
	doi = {10.1145/3695836},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KhalfiouiW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of answering conjunctive queries with aggregation on database instances that may violate primary key constraints. In SQL, these queries follow the SELECT-FROM-WHERE-GROUP BY format, where the WHERE-clause involves a conjunction of equalities, and the SELECT-clause can incorporate aggregate operators like MAX, MIN, SUM, AVG, or COUNT. Repairs of a database instance are defined as inclusion-maximal subsets that satisfy all primary keys. For a given query, our primary objective is to identify repairs that yield the lowest aggregated value among all possible repairs. We particularly investigate queries for which this lowest aggregated value can be determined through a rewriting in first-order logic with aggregate operators.}
}


@article{DBLP:journals/pacmmod/KhamisKOS24,
	author = {Mahmoud Abo Khamis and
                  Ahmet Kara and
                  Dan Olteanu and
                  Dan Suciu},
	title = {Insert-Only versus Insert-Delete in Dynamic Query Evaluation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {219:1--219:26},
	year = {2024},
	url = {https://doi.org/10.1145/3695837},
	doi = {10.1145/3695837},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KhamisKOS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the dynamic query evaluation problem: Given a full conjunctive query Q and a sequence of updates to the input database, we construct a data structure that supports constant-delay enumeration of the tuples in the query output after each update. We show that a sequence of N insert-only updates to an initially empty database can be executed in total time O(N w(Q) ), where w(Q) is the fractional hypertree width of Q. This matches the complexity of the static query evaluation problem for Q and a database of size N. One corollary is that the amortized time per single-tuple insert is constant for acyclic full conjunctive queries. In contrast, we show that a sequence of N inserts and deletes can be executed in total time Õ(N w(Q\') ), where Q\' is obtained from Q by extending every relational atom with extra variables that represent the "lifespans" of tuples in the database. We show that this reduction is optimal in the sense that the static evaluation runtime of Q\' provides a lower bound on the total update time for the output of Q. Our approach achieves amortized optimal update times for the hierarchical and Loomis-Whitney join queries.}
}


@article{DBLP:journals/pacmmod/DeepZFK24,
	author = {Shaleen Deep and
                  Hangdong Zhao and
                  Austen Z. Fan and
                  Paraschos Koutris},
	title = {Output-sensitive Conjunctive Query Evaluation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {220:1--220:24},
	year = {2024},
	url = {https://doi.org/10.1145/3695838},
	doi = {10.1145/3695838},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DeepZFK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Join evaluation is one of the most fundamental operations performed by database systems and arguably the most well-studied problem in the Database community. A staggering number of join algorithms have been developed, and commercial database engines use finely tuned join heuristics that take into account many factors including the selectivity of predicates, memory, IO, etc. However, most of the results have catered to either full join queries or non-full join queries but with degree constraints (such as PK-FK relationships) that makes join evaluation easier. Further, most of the algorithms are also not output-sensitive. In this paper, we present a novel, output-sensitive algorithm for the evaluation of acyclic Conjunctive Queries (CQs) that contain arbitrary free variables. Our result is based on a novel generalization of the Yannakakis algorithm and shows that it is possible to improve the running time guarantee of Yannakakis algorithm by a polynomial factor. Importantly, our algorithmic improvement does not depend on the use of fast matrix multiplication, as a recently proposed algorithm does. The application of our algorithm recovers known prior results and improves on known state-of-the-art results for common queries such as paths and stars. The upper bound is complemented with a matching lower bound for star queries, a restricted subclass of acyclic CQs, and a family of cyclic CQs conditioned on two variants of the  k -clique conjecture.}
}


@article{DBLP:journals/pacmmod/ImMNP24,
	author = {Sungjin Im and
                  Benjamin Moseley and
                  Hung Q. Ngo and
                  Kirk Pruhs},
	title = {Polynomial Time Convergence of the Iterative Evaluation of Datalogo
                  Programs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {221:1--221:19},
	year = {2024},
	url = {https://doi.org/10.1145/3695839},
	doi = {10.1145/3695839},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ImMNP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Datalog o  is an extension of Datalog that allows for aggregation and recursion over an arbitrary commutative semiring. Like Datalog, Datalogo programs can be evaluated via the natural iterative algorithm until a fixed point is reached. However unlike Datalog, the natural iterative evaluation of some Datalogo programs over some semirings may not converge. It is known that the commutative semirings for which the iterative evaluation of Datalogo programs is guaranteed to converge are exactly those semirings that are stable. Previously, the best known upper bound on the number of iterations until convergence over p-stable semirings is ∑i=1 ^n (p+2) i  = Θ(p n ) steps, where n is (essentially) the output size. We establish that, in fact, the natural iterative evaluation of a Datalogo program over a p-stable semiring converges within a polynomial number of iterations. In particular our upper bound is O(σ p n 2 ( n 2  lg Λ + lg σ)) where σ is the number of elements in the semiring present in either the input databases or the Datalogo program, and λ is the maximum number of terms in any product in the Datalogo program.}
}


@article{DBLP:journals/pacmmod/GawrychowskiMS24,
	author = {Pawel Gawrychowski and
                  Florin Manea and
                  Markus L. Schmid},
	title = {Revisiting Weighted Information Extraction: {A} Simpler and Faster
                  Algorithm for Ranked Enumeration},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {5},
	pages = {222:1--222:19},
	year = {2024},
	url = {https://doi.org/10.1145/3695840},
	doi = {10.1145/3695840},
	timestamp = {Sun, 22 Dec 2024 15:49:33 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GawrychowskiMS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information extraction from textual data, where the query is represented by a finite transducer and the task is to enumerate all results without repetition, and its extension to the weighted case, where each output element has a weight and the output elements are to be enumerated sorted by their weights, are important and well studied problems in database theory. On the one hand, the first framework already covers the well-known case of regular document spanners, while the latter setting covers several practically relevant tasks that cannot be described in the unweighted setting. It is known that in the unweighted case this problem can be solved with linear time preprocessing O(|D|) and output-linear delay O(|s|) in data complexity, where D is the input data and s is the current output element. For the weighted case, Bourhis, Grez, Jachiet, and Riveros [ICDT 2021] recently designed an algorithm with linear time preprocessing, but the delay of O(|s| · log|D|) depends on the size of the data. We first show how to leverage the existing results on enumerating shortest paths to obtain a simple alternative algorithm with linear preprocessing and a delay of O(|s i | + min\\ log i, log|D| ) for the i th  output element s i  (in data complexity); thus, substantially improving the previous algorithm. Next, we develop a technically involved rounding technique that allows us to devise an algorithm with linear time preprocessing and output-linear delay O(|s|) with high probability. To this end, we combine tools from algebra, high-dimensional geometry, and linear programming.}
}


@article{DBLP:journals/pacmmod/AbouziedHA24,
	author = {Azza Abouzied and
                  Joseph M. Hellerstein and
                  Divyakant Agrawal},
	title = {{PACMMOD} V2, {N6} (SIGMOD), December 2024: Editorial},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {223:1--223:2},
	year = {2024},
	url = {https://doi.org/10.1145/3698798},
	doi = {10.1145/3698798},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/AbouziedHA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Proceedings of the ACM on Management of Data (PACMMOD) is concerned with the principles, algorithms, techniques, systems, and applications of database management systems, data management technology, and science and engineering of data. It includes articles reporting cutting-edge data management, data engineering, and data science research. We are pleased to present the 6th issue of Volume 2 of PACMMOD. This issue contains papers that were submitted to the SIGMOD research track in April 2024.}
}


@article{DBLP:journals/pacmmod/ZhengXC24,
	author = {Liang Zheng and
                  Qingjun Xiao and
                  Xuyuan Cai},
	title = {A Universal Sketch for Estimating Heavy Hitters and Per-Element Frequency
                  Moments in Data Streams with Bounded Deletions},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {224:1--224:28},
	year = {2024},
	url = {https://doi.org/10.1145/3698799},
	doi = {10.1145/3698799},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhengXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the field of data stream processing, there are two prevalent models, i.e., insertion-only, and turnstile models. Most previous works were proposed for the insertion-only model, which assumes new elements arrive continuously as a stream, and neglects the possibilities of removing existing elements. In this paper, we make a  bounded deletion  assumption, putting a constraint on the number of deletions allowed. For such a turnstile stream, we focus on a new problem of  universal measurement  that estimates multiple kinds of statistical metrics simultaneously using limited memory and in an online fashion, including per-element frequency, heavy hitters, frequency moments, and frequency distribution. There are two key challenges for processing a turnstile stream with bounded deletions. Firstly, most previous methods for detecting heavy hitters cannot ensure a bounded detection error when there are deletion events. Secondly, there is still no prior work to estimate the per-element frequency moments under turnstile model, especially in an online fashion. In this paper, we address the former challenge by proposing a Removable Augmented Sketch, and address the latter by a Removable Universal Sketch, enhanced with an Online Moment Estimator. In addition, we improve the accuracy of frequency estimation by a compressed counter design, which can halve the memory cost of a frequency counter and support addition/minus operations. Our experiments show that our solution outperforms other algorithms by 16%~69% in F1 Score of heavy hitter detection, and improves the throughput of frequency moment estimation by 3.0x10 4  times.}
}


@article{DBLP:journals/pacmmod/XuLLWL24,
	author = {Xiaojia Xu and
                  Haoyu Liu and
                  Xiaowei Lv and
                  Yongcai Wang and
                  Deying Li},
	title = {An Efficient and Exact Algorithm for Locally \emph{h}-Clique Densest
                  Subgraph Discovery},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {225:1--225:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698800},
	doi = {10.1145/3698800},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuLLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting locally, non-overlapping, near-clique densest subgraphs is a crucial problem for community search in social networks. As a vertex may be involved in multiple overlapped local cliques, detecting locally densest sub-structures considering  h -clique density, i.e.,  locally h-clique densest subgraph (LhCDS)  attracts great interests. This paper investigates the L h CDS detection problem and proposes an efficient and exact algorithm to list the top- k  non-overlapping, locally  h -clique dense, and compact subgraphs. We in particular jointly consider  h -clique compact number and L h CDS and design a new ''Iterative Propose-Prune-and-Verify'' pipeline  (IPPV)  for top- k  L h CDS detection. (1) In the proposal part, we derive initial bounds for  h -clique compact numbers; prove the validity, and extend a convex programming method to tighten the bounds for proposing L h CDS candidates without missing any. (2) Then a tentative graph decomposition method is proposed to solve the challenging case where a clique spans multiple subgraphs in graph decomposition. (3) To deal with the verification difficulty, both a basic and a fast verification method are proposed, where the fast method constructs a smaller-scale flow network to improve efficiency while preserving the verification correctness. The verified L h CDSes are returned, while the candidates that remained unsure reenter the  IPPV  pipeline. (4) We further extend the proposed methods to locally more general pattern densest subgraph detection problems. We prove the exactness and low complexity of the proposed algorithm. Extensive experiments on real datasets show the effectiveness and high efficiency of  IPPV.  Codes are available at: https://github.com/Elssky/IPPV}
}


@article{DBLP:journals/pacmmod/DuS24,
	author = {Mingzhe Du and
                  Michael L. Scott},
	title = {Buffered Persistence in {B+} Trees},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {226:1--226:24},
	year = {2024},
	url = {https://doi.org/10.1145/3698801},
	doi = {10.1145/3698801},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DuS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-volatile Memory (NVM) offers the opportunity to build large, durable B+ trees with markedly higher performance and faster post-crash recovery than is possible with traditional disk- or flash-based persistence. Unfortunately, cache flush and fence instructions, required for crash consistency and failure atomicity on many machines, introduce substantial overhead not present in non-persistent trees, and force additional NVM reads and writes. The overhead is particularly pronounced in workloads that benefit from cache reuse due to good temporal locality or small working sets---traits commonly observed in real-world applications. In this paper, we propose a buffered durable B+ tree (BD+Tree) that improves performance and reduces NVM traffic via  relaxed  persistence. Execution of a BD+Tree is divided into  epochs  of a few milliseconds each; if a crash occurs in epoch  e,  the tree recovers to its state as of the end of epoch  e -2. (The persistence boundary can always be made current with an explicit sync operation, which quickly advances the epoch by 2.) NVM writes within an epoch are aggregated for delayed persistence, thereby increasing cache reuse and reducing traffic to NVM. In comparison to state-of-the-art persistent B+ trees, our micro-benchmark experiments show that BD+Tree can improve throughput by up to 2.4x and reduce NVM writes by up to 90% when working sets are small or workloads exhibit strong temporal locality. On real-world workloads that benefit from cache reuse, BD+Tree realizes throughput improvements of 1.1--2.4x and up to a 99% decrease in NVM writes. Even on uniform workloads, with working sets that significantly exceed cache capacity, BD+Tree still improves throughput by 1--1.3x. The performance advantage of BD+Tree increases with larger caches, suggesting ongoing benefits as CPUs evolve toward gigabyte cache capacities.}
}


@article{DBLP:journals/pacmmod/YaoCFGJL24,
	author = {Yuanyuan Yao and
                  Lu Chen and
                  Ziquan Fang and
                  Yunjun Gao and
                  Christian S. Jensen and
                  Tianyi Li},
	title = {\emph{Camel:} Efficient Compression of Floating-Point Time Series},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {227:1--227:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698802},
	doi = {10.1145/3698802},
	timestamp = {Tue, 28 Jan 2025 07:35:00 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YaoCFGJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series compression encodes the information in a time-ordered sequence of data points into fewer bits, thereby reducing storage costs and possibly other costs. Compression methods are either general or XOR-based. General compression methods are time-consuming and are not suitable in streaming scenarios, while XOR-based methods are unable to consistently maintain high compression ratios. Further, existing methods compress the integer and decimal parts of floating-point values as a whole, thus disregarding the different characteristics of the two parts. We propose  Camel , a new compression method for floating-point time series with the goal of advancing the compression ratios and efficiency achievable.  Camel  compresses the integer and decimal parts of the double-precision floating-point numbers in time series separately; and instead of performing XOR operations on values using their previous value,  Camel  identifies values that enable higher compression ratios.  Camel  also includes means of indexing compressed data, thereby making it possible to query compressed data efficiently. We report on an empirical study of  Camel  and 11 lossless and 6 lossy compression methods on 22 public datasets and three industrial datasets from AliCloud. The study offers evidence that  Camel  is capable of outperforming existing methods in terms of both compression ratio and efficiency and is capable of excellent compression performance on both time series and non-time series data.}
}


@article{DBLP:journals/pacmmod/HeWZLZ24,
	author = {Yizhang He and
                  Kai Wang and
                  Wenjie Zhang and
                  Xuemin Lin and
                  Ying Zhang},
	title = {Common Neighborhood Estimation over Bipartite Graphs under Local Differential
                  Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {228:1--228:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698803},
	doi = {10.1145/3698803},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/HeWZLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bipartite graphs, formed by two vertex layers, arise as a natural fit for modeling the relationships between two groups of entities. In bipartite graphs, common neighborhood computation between two vertices on the same vertex layer is a basic operator, which is easily solvable in general settings. However, it inevitably involves releasing the neighborhood information of vertices, posing a significant privacy risk for users in real-world applications. To protect edge privacy in bipartite graphs, in this paper, we study the problem of estimating the number of common neighbors of two vertices on the same layer under edge local differential privacy (edge LDP). The problem is challenging in the context of edge LDP since each vertex on the opposite layer of the query vertices can potentially be a common neighbor. To obtain efficient and accurate estimates, we propose a multiple-round framework that significantly reduces the candidate pool of common neighbors and enables the query vertices to construct unbiased estimators locally. Furthermore, we improve data utility by incorporating the estimators built from the neighbors of both query vertices and devise privacy budget allocation optimizations. These improve the estimator's robustness and consistency, particularly against query vertices with imbalanced degrees. Extensive experiments on 15 datasets validate the effectiveness and efficiency of our proposed techniques.}
}


@article{DBLP:journals/pacmmod/ShiPZLXL24,
	author = {Min Shi and
                  Peng Peng and
                  Xu Zhou and
                  Jiayu Liu and
                  Guoqing Xiao and
                  Kenli Li},
	title = {Connectivity-Oriented Property Graph Partitioning for Distributed
                  Graph Pattern Query Processing},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {229:1--229:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698804},
	doi = {10.1145/3698804},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ShiPZLXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph pattern query is a powerful tool for extracting crucial information from property graphs. With the exponential growth of sizes, property graphs are typically divided into multiple subgraphs (referred to as  partitions ) and stored across various sites in distributed environments. Existing graph partitioning methods have not been efficiently optimized for pattern queries, resulting in numerous query matches across multiple partitions, called  crossing matches.  Identifying these matches requires much inter-partition communication, which is the primary performance bottleneck in distributed query processing. To address this issue, this paper introduces a novel connectivity-oriented relationship-disjoint partitioning method, namely RCP (Relationship Connectivity Partitioning), aimed at enhancing the efficiency of graph pattern query processing by reducing crossing matches. By employing each weakly connected component of the subgraph, which is induced by different relationship labels, as a basic unit of partition, RCP ensures that matches for both  variable-length path  and  labeled graph pattern  queries are not crossing matches. Here,  variable-length path  and  labeled graph pattern  are two common components in graph pattern queries to identify paths meeting specific label constraints and retrieve subgraphs with consistent relationship types, respectively. Moreover, in the query processing phase, we further demonstrate that all graph pattern queries, belonging to these two basic queries or their extensions, can be executed independently under RCP, thereby avoiding crossing matches. In experiments, we implemented two prototype distributed property graph systems based on Neo4j and JanusGraph, which use declarative and functional query language, respectively. Experimental results on billion-scale datasets demonstrate that our approach brings a performance improvement of nearly two orders of magnitude over state-of-the-art partitioning methods.}
}


@article{DBLP:journals/pacmmod/XuWQLZL24,
	author = {Lantian Xu and
                  Dong Wen and
                  Lu Qin and
                  Ronghua Li and
                  Ying Zhang and
                  Xuemin Lin},
	title = {Constant-time Connectivity Querying in Dynamic Graphs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {230:1--230:23},
	year = {2024},
	url = {https://doi.org/10.1145/3698805},
	doi = {10.1145/3698805},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuWQLZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Connectivity query processing is a fundamental problem in graph processing. Given an undirected graph and two query vertices, the problem aims to identify whether they are connected via a path. Given frequent edge updates in real graph applications, in this paper, we study connectivity query processing in fully dynamic graphs, where edges are frequently inserted or deleted. A recent solution, called D-tree, maintains a spanning tree for each connected component and applies several heuristics to reduce the depth of the tree. To improve the efficiency, we propose a new spanning-tree-based solution by maintaining a disjoint-set tree simultaneously. By combining the advantages of two trees, we achieve the constant query time complexity and also significantly improve the theoretical running time in both edge insertion and edge deletion. Our performance studies on real large datasets show considerable improvement of our algorithms.}
}


@article{DBLP:journals/pacmmod/GaoCDHO24,
	author = {Haotian Gao and
                  Shaofeng Cai and
                  Tien Tuan Anh Dinh and
                  Zhiyong Huang and
                  Beng Chin Ooi},
	title = {CtxPipe: Context-aware Data Preparation Pipeline Construction for
                  Machine Learning},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {231:1--231:27},
	year = {2024},
	url = {https://doi.org/10.1145/3698831},
	doi = {10.1145/3698831},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/GaoCDHO24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning models are only as good as their training data. Simple models trained on well-chosen features extracted from the raw data often outperform complex models trained directly on the raw data. Data preparation pipelines, which clean and derive features from the data, are therefore important for machine learning applications. However, constructing such pipelines is a resource-intensive process that involves deep human expertise. Our goal is to design an efficient framework for automatically finding high-quality data preparation pipelines. The main challenge is how to explore a large search space of pipeline components with the objective of computing features that maximize the performance of the downstream models. Existing solutions are limited in terms of feature quality, which results in low accuracies of the downstream models, while incurring significant runtime overhead. We present CtxPipe, a novel framework that addresses the limitations of previous works by leveraging contextual information to improve the pipeline construction process. Specifically, it uses pre-trained embedding models to capture the data semantics, which are then used to guide the selection of pipeline components. We implement CtxPipe with deep reinforcement learning and evaluate it against state-of-the-art automated pipeline construction solutions. Our comprehensive experiments demonstrate that CtxPipe outperforms all of the baselines in both model performance and runtime cost.}
}


@article{DBLP:journals/pacmmod/CiacciaM24,
	author = {Paolo Ciaccia and
                  Davide Martinenghi},
	title = {Directional Queries: Making Top-k Queries More Effective in Discovering
                  Relevant Results},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {232:1--232:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698807},
	doi = {10.1145/3698807},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CiacciaM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Top- k  queries, in particular those based on a linear scoring function, are a common way to extract relevant results from large datasets. Their major advantage over alternative approaches, such as skyline queries (which return all the undominated objects in a dataset), is that the cardinality of the output can be easily controlled through the k parameter and user preferences can be accommodated by appropriately weighing the involved attributes. In this paper we concentrate on two so-far neglected aspects of top- k  queries: first, their general ability to return  all  the potentially interesting results, i.e., the tuples in the skyline; second, the difficulty that linear top- k  queries might encounter in returning tuples with  balanced  attribute values that match user preferences more closely than tuples that are extremely good in one dimension but (very) poor in others. In order to quantify these undesirable effects we introduce four novel indicators for skyline tuples, which measure their robustness as well as the difficulty incurred by top-k queries to retrieve them. After observing that real datasets usually contain many relevant results that are hardly retrievable by linear top- k  queries, and with the aim of favoring balanced results, we extend the queries with a term that accounts for the  distance  of a tuple from the preference direction established by the attributes' weights. This novel query, which we call  directional query,  adds the flexibility needed to allow  each  skyline tuple to be ranked first for a proper choice of weights, with no extra burden on the user and, in the most adverse scenarios, only a minor computational overhead, as measured through an extensive experimental analysis on real and synthetic data.}
}


@article{DBLP:journals/pacmmod/LemaitreBM24,
	author = {Rudi Poepsel Lemaitre and
                  Kaustubh Beedkar and
                  Volker Markl},
	title = {Disclosure-Compliant Query Answering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {233:1--233:28},
	year = {2024},
	url = {https://doi.org/10.1145/3698808},
	doi = {10.1145/3698808},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LemaitreBM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In today's data-driven world, organizations face increasing pressure to comply with data disclosure policies, which require data masking measures and robust access control mechanisms. This paper presents Mascara, a middleware for specifying and enforcing data disclosure policies. Mascara extends traditional access control mechanisms with data masking to support partial disclosure of sensitive data. We introduce data masks to specify disclosure policies flexibly and intuitively and propose a query modification approach to rewrite user queries into disclosure-compliant ones. We present a utility estimation framework to estimate the information loss of masked data based on relative entropy, which Mascara leverages to select the disclosure-compliant query that minimizes information loss. Our experimental evaluation shows that Mascara effectively chooses the best disclosure-compliant query with a success rate exceeding 90%, ensuring users get data with the lowest possible information loss. Additionally, Mascara's overhead compared to normal execution without data protection is negligible, staying lower than 300ms even for extreme scenarios with hundreds of possible disclosure-compliant queries.}
}


@article{DBLP:journals/pacmmod/StoianK24,
	author = {Mihail Stoian and
                  Andreas Kipf},
	title = {DPconv: Super-Polynomially Faster Join Ordering},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {234:1--234:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698809},
	doi = {10.1145/3698809},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/StoianK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We revisit the join ordering problem in query optimization. The standard exact algorithm, DPccp, has a worst-case running time of O(3 n ). This is prohibitively expensive for large queries, which are not that uncommon anymore. We develop a new algorithmic framework based on subset convolution. DPconv achieves a super-polynomial speedup over DPccp, breaking the O(3 n ) time-barrier for the first time. We show that the framework instantiation for the C max  cost function is up to 30x faster than DPccp for large clique queries.}
}


@article{DBLP:journals/pacmmod/DengMZR24,
	author = {Wenjing Deng and
                  Qiuyang Mang and
                  Chengyu Zhang and
                  Manuel Rigger},
	title = {Finding Logic Bugs in Spatial Database Engines via \emph{Affine Equivalent
                  Inputs}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {235:1--235:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698810},
	doi = {10.1145/3698810},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/DengMZR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial Database Management Systems (SDBMSs) aim to store, manipulate, and retrieve  spatial data . SDBMSs are employed in various modern applications, such as geographic information systems, computer-aided design tools, and location-based services. However, the presence of logic bugs in SDBMSs can lead to incorrect results, substantially undermining the reliability of these applications. Detecting logic bugs in SDBMSs is challenging due to the lack of ground truth for identifying incorrect results. In this paper, we propose an automated  geometry-aware generator  to generate high-quality SQL statements for SDBMSs and a novel concept named  Affine Equivalent Inputs (AEI)  to validate the results of SDBMSs. We implemented them as a tool named Spatter ( Spat ial DBMS Tes ter ) for finding logic bugs in four popular SDBMSs: PostGIS, DuckDB Spatial, MySQL, and SQL Server. Our testing campaign detected 34 previously unknown and unique bugs in these SDBMSs, of which 30 have been confirmed, and 18 have already been fixed. Our testing efforts have been well appreciated by the developers. Experimental results demonstrate that the  geometry-aware generator  significantly outperforms a naive random-shape generator in detecting unique bugs, and  AEI  can identify 14 logic bugs in SDBMSs that were totally overlooked by previous methodologies.}
}


@article{DBLP:journals/pacmmod/YanWWML24,
	author = {Mengyi Yan and
                  Yaoshu Wang and
                  Yue Wang and
                  Xiaoye Miao and
                  Jianxin Li},
	title = {{GIDCL:} {A} Graph-Enhanced Interpretable Data Cleaning Framework
                  with Large Language Models},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {236:1--236:29},
	year = {2024},
	url = {https://doi.org/10.1145/3698811},
	doi = {10.1145/3698811},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YanWWML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data quality is critical across many applications. The utility of data is undermined by various errors, making rigorous data cleaning a necessity. Traditional data cleaning systems depend heavily on predefined rules and constraints, which necessitate significant domain knowledge and manual effort. Moreover, while configuration-free approaches and deep learning methods have been explored, they struggle with complex error patterns, lacking interpretability, requiring extensive feature engineering or labeled data. This paper introduces GIDCL ( G raph-enhanced  I nterpretable  D ata  C leaning with  L arge language models), a pioneering framework that harnesses the capabilities of Large Language Models (LLMs) alongside Graph Neural Network (GNN) to address the challenges of traditional and machine learning-based data cleaning methods. By converting relational tables into graph structures, GIDCL utilizes GNN to effectively capture and leverage structural correlations among data, enhancing the model's ability to understand and rectify complex dependencies and errors. The framework's creator-critic workflow innovatively employs LLMs to automatically generate interpretable data cleaning rules and tailor feature engineering with minimal labeled data. This process includes the iterative refinement of error detection and correction models through few-shot learning, significantly reducing the need for extensive manual configuration. GIDCL not only improves the precision and efficiency of data cleaning but also enhances its interpretability, making it accessible and practical for non-expert users. Our extensive experiments demonstrate that GIDCL significantly outperforms existing methods, improving F1-scores by 10% on average while requiring only 20 labeled tuples.}
}


@article{DBLP:journals/pacmmod/BoeschenZB24,
	author = {Nils Boeschen and
                  Tobias Ziegler and
                  Carsten Binnig},
	title = {{GOLAP:} {A} GPU-in-Data-Path Architecture for High-Speed {OLAP}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {237:1--237:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698812},
	doi = {10.1145/3698812},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/BoeschenZB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we suggest a novel GPU-in-data-path architecture that leverages a GPU to accelerate the I/O path and thus can achieve almost in-memory bandwidth using SSDs. In this architecture, the main idea is to stream data in heavy-weight compressed blocks from SSDs directly into the GPU and decompress it on-the-fly as part of the table scan to inflate data before processing it by downstream query operators. Furthermore, we employ novel GPU-optimized pruning techniques that help us further inflate the perceived read bandwidth. In our evaluation, we show that the GPU-in-data-path architecture can achieve an effective bandwidth of up to 100 GiB/s, surpassing existing in-memory systems' capabilities.}
}


@article{DBLP:journals/pacmmod/KuschewskiGNL24,
	author = {Maximilian Kuschewski and
                  Jana Giceva and
                  Thomas Neumann and
                  Viktor Leis},
	title = {High-Performance Query Processing with NVMe Arrays: Spilling without
                  Killing Performance},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {238:1--238:27},
	year = {2024},
	url = {https://doi.org/10.1145/3698813},
	doi = {10.1145/3698813},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/KuschewskiGNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper aims to bridge the gap between fast in-memory query engines and slow but robust engines that can utilize external storage. We find that current systems have to choose between fast in-memory operators and slower out-of-memory operators. We present a solution that leverages two independent but complementary techniques: First, we propose adaptive materialization, which can turn any hash-based in-memory operator into an out-of-memory operator without reducing in-memory performance. Second, we introduce self-regulating compression, which optimizes the throughput of spilling operators based on the current workload and available hardware. We evaluate these techniques using the prototype query engine Spilly, which matches the performance of state-of-the-art in-memory systems, but also efficiently executes large out-of-memory workloads by spilling to NVMe arrays.}
}


@article{DBLP:journals/pacmmod/XuGGLJ24,
	author = {Yuexuan Xu and
                  Jianyang Gao and
                  Yutong Gou and
                  Cheng Long and
                  Christian S. Jensen},
	title = {iRangeGraph: Improvising Range-dedicated Graphs for Range-filtering
                  Nearest Neighbor Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {239:1--239:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698814},
	doi = {10.1145/3698814},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/XuGGLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Range-filtering approximate nearest neighbor (RFANN) search is attracting increasing attention in academia and industry. Given a set of data objects, each being a pair of a high-dimensional vector and a numeric value, an RFANN query with a vector and a numeric range as parameters returns the data object whose numeric value is in the query range and whose vector is nearest to the query vector. To process this query, a recent study proposes to build  O(n 2 ) dedicated graph-based indexes for all possible query ranges to enable efficient processing on a database of  n  objects. As storing all these indexes is prohibitively expensive, the study constructs compressed indexes instead, which reduces the memory consumption considerably. However, this incurs suboptimal performance because the compression is lossy. In this study, instead of materializing a compressed index for every possible query range in preparation for querying, we materialize graph-based indexes, called elemental graphs, for a moderate number of ranges. We then provide an effective and efficient algorithm that during querying can construct an index for any query range using the elemental graphs. We prove that the time needed to construct such an index is low. We also cover an experimental study on real-world datasets that provides evidence that the materialized elemental graphs only consume moderate space and that the proposed method is capable of superior and stable query performance across different query workloads.}
}


@article{DBLP:journals/pacmmod/FruthS24,
	author = {Michael Fruth and
                  Stefanie Scherzinger},
	title = {Live Patching for Distributed In-Memory Key-Value Stores},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {241:1--241:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698816},
	doi = {10.1145/3698816},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/FruthS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Providers of high-availability data stores need to roll out software updates without causing noticeable downtimes. For distributed data stores like Redis Cluster, the state-of-the-art is a rolling update, where the nodes are restarted in sequence. This requires preserving, restoring, and resynchronizing the database state, which can significantly prolong updates for larger memory states, and thus delay critical security fixes. In this article, we propose applying software updates directly in memory without restarting any nodes. We present the first fully operational live patching solution for Redis Cluster on Linux. We support both push- and pull-based distribution of patches, trading dissemination speed against cluster elasticity, the ability to allow nodes to dynamically join or leave the cluster. Our integration is very lightweight, as it piggybacks on the cluster-internal gossip protocol. Our experiments benchmark live patching against state-of-the-art rolling updates. In one scenario, live patching updates the entire cluster orders of magnitude faster, without unfavorable trade-offs regarding throughput, tail latencies, or network consumption. To showcase generalizability, we provide general guidelines on integrating live patching for distributed database systems and successfully apply them to a primary-replica PostgreSQL setup. Given our overall promising results, we discuss the opportunities of live patching in database DevOps.}
}


@article{DBLP:journals/pacmmod/RabbaniLBH24,
	author = {Kashif Rabbani and
                  Matteo Lissandrini and
                  Angela Bonifati and
                  Katja Hose},
	title = {Transforming {RDF} Graphs to Property Graphs using Standardized Schemas},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {242:1--242:25},
	year = {2024},
	url = {https://doi.org/10.1145/3698817},
	doi = {10.1145/3698817},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/RabbaniLBH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graphs can be encoded using different data models. They are especially abundant using RDF and recently also as property graphs. While knowledge graphs in RDF adhere to the subject-predicate-object structure, property graphs utilize multi-labeled nodes and edges, featuring properties as key/value pairs. Both models are employed in various contexts, thus applications often require transforming data from one model to another. To enhance the interoperability of the two models, we present a novel technique, S3PG, to convert RDF knowledge graphs into property graphs exploiting two popular standards to express schema constraints, i.e., SHACL for RDF and PG-Schema for property graphs. S3PG is the first approach capable of transforming large knowledge graphs to property graphs while fully preserving information and semantics. We have evaluated S3PG on real-world large-scale graphs, showing that, while existing methods exhibit lossy transformations (causing a loss of up to 70% of query answers), S3PG consistently achieves 100% accuracy. Moreover, when considering evolving graphs, S3PG exhibits fully monotonic behavior and requires only a fraction of the time to incorporate changes compared to existing methods.}
}


@article{DBLP:journals/pacmmod/YuGTSZYLZLLYZ24,
	author = {Song Yu and
                  Shufeng Gong and
                  Qian Tao and
                  Sijie Shen and
                  Yanfeng Zhang and
                  Wenyuan Yu and
                  Pengxi Liu and
                  Zhixin Zhang and
                  Hongfu Li and
                  Xiaojian Luo and
                  Ge Yu and
                  Jingren Zhou},
	title = {LSMGraph: {A} High-Performance Dynamic Graph Storage System with Multi-Level
                  {CSR}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {243:1--243:28},
	year = {2024},
	url = {https://doi.org/10.1145/3698818},
	doi = {10.1145/3698818},
	timestamp = {Wed, 19 Mar 2025 21:16:37 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/YuGTSZYLZLLYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing volume of graph data may exhaust the main memory. It is crucial to design a disk-based graph storage system to ingest updates and analyze graphs efficiently. However, existing dynamic graph storage systems suffer from read or write amplification and face the challenge of optimizing both read and write performance simultaneously. To address this challenge, we propose LSMGraph, a novel dynamic graph storage system that combines the write-friendly LSM-tree and the read-friendly CSR. It leverages the multi-level structure of LSM-trees to optimize write performance while utilizing the compact CSR structures embedded in the LSM-trees to boost read performance. LSMGraph uses a new memory structure, MemGraph, to efficiently cache graph updates and uses a multi-level index to speed up reads within the multi-level structure. Furthermore, LSMGraph incorporates a vertex-grained version control mechanism to mitigate the impact of LSM-tree compaction on read performance and ensure the correctness of concurrent read and write operations. Our evaluation shows that LSMGraph significantly outperforms state-of-the-art (graph) storage systems on both graph update and graph analytical workloads.}
}


@article{DBLP:journals/pacmmod/EslamiD24,
	author = {Navid Eslami and
                  Niv Dayan},
	title = {Memento Filter: {A} Fast, Dynamic, and Robust Range Filter},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {244:1--244:27},
	year = {2024},
	url = {https://doi.org/10.1145/3698820},
	doi = {10.1145/3698820},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/EslamiD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Range filters are probabilistic data structures that answer approximate range emptiness queries. They aid in avoiding processing empty range queries and have use cases in many application domains such as key-value stores and social web analytics. However, current range filters do not support dynamically changing and growing datasets. Moreover, several of these designs also exhibit impractically high false positive rates under correlated workloads, which are common in practice. These impediments restrict the applicability of range filters across a wide range of use cases. We introduce Memento filter, the first range filter to simultaneously offer dynamicity, fast operations, and a robust false positive rate for any workload. Memento filter partitions the key universe and clusters its keys according to this partitioning. For each cluster, it stores a fingerprint and a list of key suffixes contiguously. The encoding of these lists makes them amenable to existing dynamic filter structures. Due to the one-to-one mapping from keys to suffixes, Memento filter supports inserts and deletes and can even expand to accommodate a growing dataset. We implement Memento filter on top of a Rank-and-Select Quotient filter and InfiniFilter and demonstrate that it achieves a competitive false positive rate and performance with the state of the art while also providing dynamicity. Due to its dynamicity, Memento filter is the first range filter applicable to B-Trees. We showcase this by integrating Memento filter into WiredTiger, a B-Tree-based key-value store, significantly boosting its performance for mixed workloads.}
}


@article{DBLP:journals/pacmmod/ZhangWGYW24,
	author = {Aoqian Zhang and
                  Zexue Wu and
                  Yifeng Gong and
                  Ye Yuan and
                  Guoren Wang},
	title = {Multivariate Time Series Cleaning under Speed Constraints},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {245:1--245:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698821},
	doi = {10.1145/3698821},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangWGYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Errors are common in time series due to unreliable sensor measurements. Existing methods focus on univariate data but do not utilize the correlation between dimensions. Cleaning each dimension separately may lead to a less accurate result, as some errors can only be identified in the multivariate case. We also point out that the widely used minimum change principle is not always the best choice. Instead, we try to change the smallest number of data to avoid a significant change in the data distribution. In this paper, we propose MTCSC, the constraint-based method for cleaning multivariate time series. We formalize the repair problem, propose a linear-time method to employ online computing, and improve it by exploiting data trends. We also support adaptive speed constraint capturing. We analyze the properties of our proposals and compare them with SOTA methods in terms of effectiveness, efficiency versus error rates, data sizes, and applications such as classification. Experiments on real datasets show that MTCSC can have higher repair accuracy with less time consumption. Interestingly, it can be effective even when there are only weak or no correlations between the dimensions.}
}


@article{DBLP:journals/pacmmod/CaiSCZ24,
	author = {Yuzheng Cai and
                  Jiayang Shi and
                  Yizhuo Chen and
                  Weiguo Zheng},
	title = {Navigating Labels and Vectors: {A} Unified Approach to Filtered Approximate
                  Nearest Neighbor Search},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {246:1--246:27},
	year = {2024},
	url = {https://doi.org/10.1145/3698822},
	doi = {10.1145/3698822},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/CaiSCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a query vector, approximate nearest neighbor search (ANNS) aims to retrieve similar vectors from a set of high-dimensional base vectors. However, many real-world applications jointly query both vector data and structured data, imposing label constraints such as attributes and keywords on the search, known as filtered ANNS. Effectively incorporating filtering conditions with vector similarity presents significant challenges, including index for dynamically filtered search space, agnostic query labels, computational overhead for label-irrelevant vectors, and potential inadequacy in returning results. To tackle these challenges, we introduce a novel approach called the  Label Navigating Graph,  which encodes the containment relationships of label sets for all vectors. Built upon graph-based ANNS methods, we develop a general framework termed  Unified Navigating Graph (UNG)  to bridge the gap between label set containment and vector proximity relations. UNG offers several advantages, including  versatility  in supporting any query label size and specificity,  fidelity  in exclusively searching filtered vectors,  completeness  in providing sufficient answers, and  adaptability  in integration with most graph-based ANNS algorithms. Extensive experiments on real datasets demonstrate that the proposed framework outperforms all baselines, achieving 10x speedups at the same accuracy.}
}


@article{DBLP:journals/pacmmod/ZhangYS24,
	author = {Jiasheng Zhang and
                  Rex Ying and
                  Jie Shao},
	title = {Online Detection of Anomalies in Temporal Knowledge Graphs with Interpretability},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {247:1--247:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698823},
	doi = {10.1145/3698823},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangYS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal knowledge graphs (TKGs) are valuable resources for capturing evolving relationships among entities, yet they are often plagued by noise, necessitating robust anomaly detection mechanisms. Existing dynamic graph anomaly detection approaches struggle to capture the rich semantics introduced by node and edge categories within TKGs, while TKG embedding methods lack interpretability, undermining the credibility of anomaly detection. Moreover, these methods falter in adapting to pattern changes and semantic drifts resulting from knowledge updates. To tackle these challenges, we introduce  AnoT,  an efficient TKG summarization method tailored for interpretable online anomaly detection in TKGs.  AnoT  begins by summarizing a TKG into a novel rule graph, enabling flexible inference of complex patterns in TKGs. When new knowledge emerges,  AnoT  maps it onto a node in the rule graph and traverses the rule graph recursively to derive the anomaly score of the knowledge. The traversal yields reachable nodes that furnish interpretable evidence for the validity or the anomalous of the new knowledge. Overall,  AnoT  embodies a detector-updater-monitor architecture, encompassing a detector for offline TKG summarization and online scoring, an updater for real-time rule graph updates based on emerging knowledge, and a monitor for estimating the approximation error of the rule graph. Experimental results on four real-world datasets demonstrate that  AnoT  surpasses existing methods significantly in terms of accuracy and interoperability. All of the raw datasets and the implementation of  AnoT  are provided in https://github.com/zjs123/ANoT.}
}


@article{DBLP:journals/pacmmod/LiuHLKAL24,
	author = {Xiaozhen Liu and
                  Yicong Huang and
                  Xinyuan Lin and
                  Avinash Kumar and
                  Sadeem Alsudais and
                  Chen Li},
	title = {Pasta: {A} Cost-Based Optimizer for Generating Pipelining Schedules
                  for Dataflow DAGs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {248:1--248:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698832},
	doi = {10.1145/3698832},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LiuHLKAL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data analytics tasks are often formulated as data workflows represented as directed acyclic graphs (DAGs) of operators. The recent trend of adopting machine learning (ML) techniques in workflows results in increasingly complicated DAGs with many operators and edges. Compared to the operator-at-a-time execution paradigm, pipelined execution has benefits of reducing the materialization cost of intermediate results and allowing operators to produce results early, which are critical in iterative analysis on large data volumes. Correctly scheduling a workflow DAG for pipelined execution is non-trivial due to the richer semantics of operators and the increasing complexity of DAGs. Several existing data systems adopt simple heuristics to solve the problem without considering costs such as materialization sizes. In this paper, we systematically study the problem of scheduling a workflow DAG for pipelined execution, and develop a novel cost-based optimizer called Pasta for generating a high-quality schedule. The Pasta optimizer is not only general and applicable to a wide variety of cost functions, but also capable of utilizing properties inherent in a broad class of cost functions to improve its performance significantly. We conducted a thorough evaluation of developed techniques on real-world workflows and show the efficiency and efficacy of these solutions.}
}


@article{DBLP:journals/pacmmod/SunDQY24,
	author = {Dajun Sun and
                  Wei Dong and
                  Yuan Qiu and
                  Ke Yi},
	title = {Personalized Truncation for Personalized Privacy},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {249:1--249:25},
	year = {2024},
	url = {https://doi.org/10.1145/3698825},
	doi = {10.1145/3698825},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/SunDQY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the standard model of differential privacy (DP), every user's privacy is treated equally, which is captured by a single privacy parameter \\varepsilon. However, in many real-world situations, users may have diverse privacy concerns and requirements, some conservative while others liberal. This is formalized by the model of  personalized differential privacy (PDP),  where each user may have a different privacy parameter \\varepsilon. However, existing techniques for PDP cannot provide good utility for many fundamental problems such as basic counting and sum estimation. In this paper, we present the  personalized truncation mechanism  for these problems under PDP. We first show that, theoretically, it is never worse than previous mechanisms (up to polylogarithmic factors) on any instance, while can be much better in certain cases. Then we use extensive experiments on both real and synthetic data to demonstrate its empirical advantages. Our mechanism also works for user-level DP, thus supporting a large class of SJA queries over relational databases under foreign-key constraints.}
}


@article{DBLP:journals/pacmmod/ZhangZU24,
	author = {Jiachi Zhang and
                  Wenchao Zhou and
                  Benjamin E. Ujcich},
	title = {Provenance-Enabled Explainable {AI}},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {250:1--250:27},
	year = {2024},
	url = {https://doi.org/10.1145/3698826},
	doi = {10.1145/3698826},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhangZU24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) algorithms have advanced significantly in recent years, progressively evolving into artificial intelligence (AI) agents capable of solving complex, human-like intellectual challenges. Despite the advancements, the interpretability of these sophisticated models lags behind, with many ML architectures remaining "black boxes" that are too intricate and expansive for human interpretation. Recognizing this issue, there has been a revived interest in the field of explainable AI (XAI) aimed at explaining these opaque ML models. However, XAI tools often suffer from being tightly coupled with the underlying ML models and are inefficient due to redundant computations. We introduce provenance-enabled explainable AI (PXAI). PXAI decouples XAI computation from ML models through a provenance graph that tracks the creation and transformation of all data within the model. PXAI improves XAI computational efficiency by excluding irrelevant and insignificant variables and computation in the provenance graph. Through various case studies, we demonstrate how PXAI enhances computational efficiency when interpreting complex ML models, confirming its potential as a valuable tool in the field of XAI.}
}


@article{DBLP:journals/pacmmod/LeeLCCLPPKK24,
	author = {Suhyun Lee and
                  Chaemin Lim and
                  Jinwoo Choi and
                  Heelim Choi and
                  Chan Lee and
                  Yongjun Park and
                  Kwanghyun Park and
                  Hanjun Kim and
                  Youngsok Kim},
	title = {SPID-Join: {A} Skew-resistant Processing-in-DIMM Join Algorithm Exploiting
                  the Bank- and Rank-level Parallelisms of DIMMs},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {251:1--251:27},
	year = {2024},
	url = {https://doi.org/10.1145/3698827},
	doi = {10.1145/3698827},
	timestamp = {Wed, 21 May 2025 18:33:20 +0200},
	biburl = {https://dblp.org/rec/journals/pacmmod/LeeLCCLPPKK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in Dual In-line Memory Modules (DIMMs) allow DIMMs to support Processing-In-DIMM (PID) by placing In-DIMM Processors (IDPs) near their memory banks. Prior studies have shown that in-memory joins can benefit from PID by offloading their operations onto the IDPs and exploiting the high internal memory bandwidth of DIMMs. Aimed at evenly balancing the computational loads between the IDPs, the existing algorithms perform IDP-wise global partitioning on input tables and then make each IDP process a partition of the input tables. Unfortunately, we find that the existing PID join algorithms achieve low performance and scalability with skewed input tables. With skewed input tables, the IDP-wise global partitioning incurs imbalanced loads between the IDPs, making the IDPs remain idle until the heaviest-load IDP completes processing its partition. To fully exploit the IDPs for accelerating in-memory joins involving skewed input tables, therefore, we need a new PID join algorithm which achieves high skew resistance by mitigating the imbalanced inter-IDP loads. In this paper, we present SPID-Join, a skew-resistant PID join algorithm which exploits two parallelisms inherent in DIMM architectures, namely bank- and rank-level parallelisms. By replicating join keys across the banks within a rank and across ranks, SPID-Join significantly increases the internal memory bandwidth and computational throughput allocated to each join key, improving the load balance between the IDPs and accelerating join executions. SPID-Join exploits the bank- and the rank-level parallelisms to minimize join key replication overheads and support a wider range of join key replication ratios. Despite achieving high skew resistance, SPID-Join exhibits a trade-off between the join key replication ratio and the join execution latency, making the best-performing join key replication ratio depend on join and PID system configurations. We, therefore, augment SPID-Join with a cost model which identifies the best-performing join key replication ratio for given join and PID system configurations. By accurately modeling and scaling the IDPs' throughput and the inter-IDP communication bandwidth, the cost model accurately captures the impact of the join key replication ratio on SPID-Join. Our experimental results using eight UPMEM DIMMs, which collectively provide a total of 1,024 IDPs, show that SPID-Join achieves up to 10.38x faster join executions over PID-Join, the state-of-the-art PID join algorithm, with highly skewed input tables.}
}


@article{DBLP:journals/pacmmod/LouLLYZYZZ24,
	author = {Yunkai Lou and
                  Longbin Lai and
                  Bingqing Lyu and
                  Yufan Yang and
                  Xiaoli Zhou and
                  Wenyuan Yu and
                  Ying Zhang and
                  Jingren Zhou},
	title = {Towards a Converged Relational-Graph Optimization Framework},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {252:1--252:27},
	year = {2024},
	url = {https://doi.org/10.1145/3698828},
	doi = {10.1145/3698828},
	timestamp = {Wed, 19 Mar 2025 21:16:37 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/LouLLYZYZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent ISO SQL:2023 standard adopts SQL/PGQ (Property Graph Queries), facilitating graph-like querying within relational databases. This advancement, however, underscores a significant gap in how to effectively optimize SQL/PGQ queries within relational database systems. To address this gap, we extend the foundational SPJ (Select-Project-Join) queries to SPJM queries, which include an additional matching operator for representing graph pattern matching in SQL/PGQ. Although SPJM queries can be converted to SPJ queries and optimized using existing relational query optimizers, our analysis shows that such a graph-agnostic method fails to benefit from graph-specific optimization techniques found in the literature. To address this issue, we develop a converged relational-graph optimization framework called RelGo for optimizing SPJM queries, leveraging joint efforts from both relational and graph query optimizations. Using DuckDB as the underlying relational execution engine, our experiments show that RelGo can generate efficient execution plans for SPJM queries. On well-established benchmarks, these plans exhibit an average speedup of 21.90x compared to those produced by the graph-agnostic optimizer.}
}


@article{DBLP:journals/pacmmod/ZhongR24,
	author = {Suyang Zhong and
                  Manuel Rigger},
	title = {Understanding and Reusing Test Suites Across Database Systems},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {253:1--253:26},
	year = {2024},
	url = {https://doi.org/10.1145/3698829},
	doi = {10.1145/3698829},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhongR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database Management System (DBMS) developers have implemented extensive test suites to test their DBMSs. For example, the SQLite test suites contain over 92 million lines of code. Despite these extensive efforts, test suites are not systematically reused across DBMSs, leading to wasted effort. Integration is challenging, as test suites use various test case formats and rely on unstandardized test runner features. We present a unified test suite, SQuaLity, in which we integrated test cases from three widely-used DBMSs, SQLite, PostgreSQL, and DuckDB. In addition, we present an empirical study to determine the potential of reusing these systems' test suites. Our results indicate that reusing test suites is challenging: First, test formats and test runner commands vary widely; for example, SQLite has 4 test runner commands, while MySQL has 112 commands with additional features, to, for example, execute file operations or interact with a shell. Second, while some test suites contain mostly standard-compliant statements (e.g., 99% in SQLite), other test suites mostly test non-standardized functionality (e.g., 31% of statements in the PostgreSQL test suite are nonstandardized). Third, test reuse is complicated by various explicit and implicit dependencies, such as the need to set variables and configurations, certain test cases requiring extensions not present by default, and query results depending on specific clients. Despite the above findings, we have identified 3 crashes, 3 hangs, and multiple compatibility issues across four different DBMSs by executing test suites across DBMSs, indicating the benefits of reuse. Overall, this work represents the first step towards test-case reuse in the context of DBMSs, and we hope that it will inspire follow-up work on this important topic.}
}


@article{DBLP:journals/pacmmod/ZhaoCBLN24,
	author = {Wenyue Zhao and
                  Yang Cao and
                  Peter Buneman and
                  Jia Li and
                  Nikos Ntarmos},
	title = {Automating Vectorized Distributed Graph Computation},
	journal = {Proc. {ACM} Manag. Data},
	volume = {2},
	number = {6},
	pages = {254:1--254:27},
	year = {2024},
	url = {https://doi.org/10.1145/3698833},
	doi = {10.1145/3698833},
	timestamp = {Sat, 25 Jan 2025 23:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/pacmmod/ZhaoCBLN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-instance graph algorithms interleave the evaluation of multiple instances of the same algorithm with different inputs over the same graph. They have been shown to be significantly faster than traditional serial and batch evaluation, by sharing computation across instances. However, writing correct multi-instance algorithms is challenging; and in this work, we describe AutoMI, a framework for automatically converting vertex-centric graph algorithms into their vectorized multi-instance versions. We also develop an algebraic characterization of algorithms that can benefit best from multi-instance computation with simpler and faster streamlined vectorization. This allows users to decide when to use such optimization and instruct AutoMI to make the best use of SIMD vectorization. Using 6 real-life graphs, we show that AutoMI-converted multi-instance algorithms are 9.6 to 29.5 times faster than serial evaluation, 7.1 to 26.4 times faster than batch evaluation, and are even 2.6 to 4.6 times faster than existing highly optimized handcrafted multi-instance algorithms without vectorization.}
}
