@inproceedings{DBLP:conf/sigmod/Liskov22,
	author = {Barbara Liskov},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Reflections on a Career in Computer Science},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3523274},
	doi = {10.1145/3514221.3523274},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Liskov22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computer Science is a wonderful field with many interesting and important problems to work on. This is true today and was also true in the past; of course what is considered interesting changes as a field matures. In this talk I will discuss some of the problems that I found intriguing. In each case I will discuss the state of the field at the time I did this work to provide an historical context for my work and that of others doing related work. I will also discuss how I selected problems, what I brought to the table, and where the ideas for my solutions came from.}
}


@inproceedings{DBLP:conf/sigmod/Lakshmanan22,
	author = {Laks V. S. Lakshmanan},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {On a Quest for Combating Filter Bubbles and Misinformation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3523275},
	doi = {10.1145/3514221.3523275},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Lakshmanan22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of social networks and media has made it easier than ever for users to access up-to-date information as well as share news and views on matters of the world with many of their peers. Unfortunately, it has also led to increased societal polarization as well as deteriorating trust in institutions. Two of the problems that are blamed for this are filter bubbles and misinformation. Filter bubbles are the result of excessive personalization which has the benefit of enhancing relevance but comes at the price of limiting the exposure of users to a specific viewpoint. They are amplified by the so-called echo chambers that exist in social media, whereby members of a community mutually reinforce a fixed opinion or viewpoint on an issue. Misinformation, on the other hand, tends to propagate through the network, and studies show it does so faster and more virally than truth. Both problems manifest themselves in the form of groups of actors working in concert and providing mutual reinforcement. How can we recognize these groups? Having detected them, how can we counteract these problems? The first question can benefit from an examination of techniques developed to search for communities or more generally dense subgraphs from an underlying network. As for the second question, a natural approach for countering filter bubbles is to launch some kind of counter-campaign to try and enhance the balance in users' exposure to viewpoints. Countermeasures for misinformation propagating through a network, on the other hand, are manifold and can depend on who is planning the countermeasure. For example, the network host can intervene and take steps to limit the propagation of misinformation, but these actions come with a cost. Besides the political sensitivity and cost of limiting freedom of expression, what if the intervention was by mistake done on genuine information? As another example, a third party interested in countering the propagation of misinformation may launch a counter-campaign. Interestingly, some of the ideas behind designing such campaigns have strong connections to a classic problem called Influence Maximization, studied in a very different context, driven by different applications like viral marketing, infection containment, and revenue or welfare maximization. In this talk, we will examine research on detecting dense subgraphs as well as competitive influence maximization and discuss how that can inspire techniques for addressing the two problems above.}
}


@inproceedings{DBLP:conf/sigmod/Re22,
	author = {Christopher R{\'{e}}},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Is Data Management the Beating Heart of {AI} Systems?},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {3},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3523276},
	doi = {10.1145/3514221.3523276},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Re22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The era of artificially intelligent systems is here, and the breadth and speed of the change has been breathtaking. Less than a decade ago, Google did not use AI or machine learning in its products. Today, we interact with a wide range of AI-powered systems when we search the web, write an email, watch a video, or make a purchase. AI has spread beyond the web as well, we use it every time we touch phones, open our laptops, turn on our television, enter (or request) a car, and many more interactions. What's perhaps more remarkable than the ubiquity of these systems is how they operate: they optimize a small number of machine learning architectures on increasingly vast and diverse quantities of data. As a result, data management is at the heart of this new breed of systems. This talk explores the idea that the next phase of AI systems might have historic parallels with the growth of relational data management. In this talk, I survey some of the exciting recent developments that have affected my own work in industry and research building systems to support AI applications. My main message is that it is an incredibly exciting time for researchers and industrial practitioners from the data management community to bring their expertise to the next generation of AI systems.}
}


@inproceedings{DBLP:conf/sigmod/TangWZYZG022,
	author = {Chuzhe Tang and
                  Zhaoguo Wang and
                  Xiaodong Zhang and
                  Qianmian Yu and
                  Binyu Zang and
                  Haibing Guan and
                  Haibo Chen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Ad Hoc Transactions in Web Applications: The Good, the Bad, and the
                  Ugly},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {4--18},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526120},
	doi = {10.1145/3514221.3526120},
	timestamp = {Sun, 19 Jan 2025 13:27:24 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/TangWZYZG022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many transactions in web applications are constructed ad hoc in the application code. For example, developers might explicitly use locking primitives or validation procedures to coordinate critical code fragments. We refer to database operations coordinated by application code as ad hoc transactions. Until now, little is known about them. This paper presents the first comprehensive study on ad hoc transactions. By studying 91 ad hoc transactions among 8 popular open-source web applications, we find that (i) every studied application uses ad hoc transactions (up to 16 per application), 71 of which play critical roles; (ii) compared with database transactions, concurrency control of ad hoc transactions is much more flexible; (iii) ad hoc transactions are error-prone-53 of them have correctness issues, and 33 of them are confirmed by developers; and (iv) ad hoc transactions have the potential to improve performance in contentious workloads by utilizing application semantics such as access patterns. Based on the findings, we discuss the implications of ad hoc transactions to the database research community.}
}


@inproceedings{DBLP:conf/sigmod/ChenYKAAS22,
	author = {Youmin Chen and
                  Xiangyao Yu and
                  Paraschos Koutris and
                  Andrea C. Arpaci{-}Dusseau and
                  Remzi H. Arpaci{-}Dusseau and
                  Jiwu Shu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Plor: General Transactions with Predictable, Low Tail Latency},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {19--33},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517879},
	doi = {10.1145/3514221.3517879},
	timestamp = {Sun, 12 Nov 2023 02:07:19 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ChenYKAAS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present pessimistic locking and optimistic reading (PLOR), a hybrid concurrency control protocol for in-memory transaction systems that delivers high throughput and low tail latency. PLOR is especially designed for high-contention workloads: for high throughput, transactions are allowed to access records without being blocked by lock conflicts in the read phase; for low tail latency, conflict detection is delayed to the commit phase, where old transactions are always committed first using the timestamps in the lock. We demonstrate the efficacy of this approach under a variety of setups (e.g., stored-procedures, interactive mode, and persistent logging, etc.). Experiments show that PLOR delivers close or comparable throughput to that of Silo and TicToc in stored-procedures, while reducing 99.9th percentile latency by 8.8x to 14.5x. In the interactive processing mode, PLOR even achieves up to 2x higher throughput.}
}


@inproceedings{DBLP:conf/sigmod/ZhangH0L22,
	author = {Jianqiu Zhang and
                  Kaisong Huang and
                  Tianzheng Wang and
                  King Lv},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Skeena: Efficient and Consistent Cross-Engine Transactions},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {34--48},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526171},
	doi = {10.1145/3514221.3526171},
	timestamp = {Tue, 01 Apr 2025 19:09:25 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhangH0L22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database systems are becoming increasingly multi-engine. In particular, a main-memory database engine may coexist with a traditional storage-centric engine in a system to support various applications. It is desirable to allow applications to access data in both engines using cross-engine transactions. But existing systems are either only designed for single-engine accesses, or impose many restrictions by limiting cross-engine transactions to certain isolation levels and table operations. The result is inadequate cross-engine support in terms of correctness, performance and programmability. This paper describes Skeena, a holistic approach to cross-engine transactions. We propose a lightweight snapshot tracking structure and an atomic commit protocol to efficiently ensure correctness and support various isolation levels. Evaluation results show that Seena maintains high performance for single-engine transactions and enables cross-engine transactions which can improve throughput by up to 30x by judiciously placing tables in different engines.}
}


@inproceedings{DBLP:conf/sigmod/KimYAKJ22,
	author = {Jong{-}Bin Kim and
                  Jaeseon Yu and
                  Jaechan Ahn and
                  Sooyong Kang and
                  Hyungsoo Jung},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Diva: Making {MVCC} Systems HTAP-Friendly},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {49--64},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526135},
	doi = {10.1145/3514221.3526135},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/KimYAKJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multiversion concurrency control (MVCC) and design principles thereof are ingrained in modern database management systems, thus promoting remarkable progress in managing online transaction processing (OLTP) workloads for decades. However, MVCC systems would battle two vital concerns when facing hybrid transactional/analytical processing (HTAP). The first concern is to ensure rapid version searching for analytic queries with less I/O, and the second concern is to reclaim garbage data versions promptly for easing the strain on storage footprint. These are often tightly coupled since many MVCC systems rely on unified version storage that poses a space-time tradeoff in HTAP, giving rise to disappointing performance metrics that may negatively stereotype OLTP-friendly MVCC systems. This paper refutes the stereotype resulting from coupled design concerns and addresses the core problem by proposing Diva (Decoupling Index from Version dAta) that physically separates version index from version data; for decoupled concerns, we devise independent management policies: provisional version indexing and time interval-based version garbage collection. The separation of coupled concerns would render legacy disk-based MVCC systems more HTAP-friendly. We applied Diva to two full-fledged database systems---PostgreSQL and MySQL---and demonstrated that the systems with Diva escaped the space-time tradeoff under hybrid transactional/analytical workloads.}
}


@inproceedings{DBLP:conf/sigmod/Liu00ZS22,
	author = {Yijian Liu and
                  Li Su and
                  Vivek Shah and
                  Yongluan Zhou and
                  Marcos Antonio Vaz Salles},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Hybrid Deterministic and Nondeterministic Execution of Transactions
                  in Actor Systems},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {65--78},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526172},
	doi = {10.1145/3514221.3526172},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Liu00ZS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The actor model has been widely adopted in building stateful middle-tiers for large-scale interactive applications, where ACID transactions are useful to ensure application correctness. In this paper, we present Snapper, a new transaction library on top of Orleans, a popular actor system. Snapper exploits the characteristics of actor-oriented programming to improve the performance of multi-actor transactions by employing deterministic transaction execution, where pre-declared actor access information is used to generate deterministic execution schedules. The deterministic execution can potentially improve transaction throughput significantly, especially with a high contention level. Besides, Snapper can also execute actor transactions using conventional nondeterministic strategies, including S2PL, to account for scenarios where actor access information cannot be pre-declared. A salient feature of Snapper is the ability to execute concurrent hybrid workloads, where some transactions are executed deterministically while the others are executed nondeterministically. This novel hybrid execution is able to take advantage of the deterministic execution while being able to account for nondeterministic workloads. Our experimental results on two benchmarks show that deterministic execution can achieve up to 2x higher throughput than nondeterministic execution under a skewed workload. Additionally, the hybrid execution strategy can achieve a throughput that is close to deterministic execution when there is only a small percentage of nondeterministic transactions running in the system.}
}


@inproceedings{DBLP:conf/sigmod/WangK0PS22,
	author = {Yisu Remy Wang and
                  Mahmoud Abo Khamis and
                  Hung Q. Ngo and
                  Reinhard Pichler and
                  Dan Suciu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Optimizing Recursive Queries with Progam Synthesis},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {79--93},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517827},
	doi = {10.1145/3514221.3517827},
	timestamp = {Mon, 05 Feb 2024 20:26:57 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/WangK0PS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most work on query optimization has concentrated on loop-free queries. However, data science and machine learning workloads today typically involve recursive or iterative computation. In this work, we propose a novel framework for optimizing recursive queries using methods from program synthesis. In particular, we introduce a simple yet powerful optimization rule called the "FGH-rule" which aims to find a faster way to evaluate a recursive program. The solution is found by making use of powerful tools, such as a program synthesizer, an SMT-solver, and an equality saturation system. We demonstrate the strength of the optimization by showing that the FGH-rule can lead to speedups up to 4 orders of magnitude on three, already optimized Datalog systems.}
}


@inproceedings{DBLP:conf/sigmod/WangZYDHDT0022,
	author = {Zhaoguo Wang and
                  Zhou Zhou and
                  Yicun Yang and
                  Haoran Ding and
                  Gansen Hu and
                  Ding Ding and
                  Chuzhe Tang and
                  Haibo Chen and
                  Jinyang Li},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {WeTune: Automatic Discovery and Verification of Query Rewrite Rules},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {94--107},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526125},
	doi = {10.1145/3514221.3526125},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/WangZYDHDT0022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query rewriting transforms a relational database query into an equivalent but more efficient one, which is crucial for the performance of database-backed applications. Such rewriting relies on pre-specified rewrite rules. In existing systems, these rewrite rules are discovered through manual insights and accumulate slowly over the years. In this paper, we present WeTune, a rule generator that automatically discovers new rewrite rules. Inspired by compiler superoptimization, WeTune enumerates all valid logical query plans up to a certain size and tries to discover equivalent plans that could potentially lead to more efficient rewrites. The core challenge is to determine which set of conditions (aka constraints) allows one to prove the equivalence between a pair of query plans. We address this challenge by enumerating combinations of "interesting" constraints that relate tables and their attributes between each pair of queries. We also propose a new SMT-based verifier to verify the equivalence of a query pair under different enumerated constraints. To evaluate the usefulness of rewrite rules discovered by WeTune, we apply them on the SQL queries collected from the 20 most popular open-source web applications on GitHub. WeTune successfully optimizes 247 queries that existing databases cannot optimize, resulting in substantial performance improvements.}
}


@inproceedings{DBLP:conf/sigmod/0001022,
	author = {Qichen Wang and
                  Ke Yi},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Conjunctive Queries with Comparisons},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {108--121},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517830},
	doi = {10.1145/3514221.3517830},
	timestamp = {Sun, 19 Jan 2025 13:27:19 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0001022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conjunctive queries with predicates in the form of comparisons that span multiple relations have regained interest recently, due to their relevance in OLAP queries, spatiotemporal databases, and machine learning over relational data. The standard technique, predicate pushdown, has limited efficacy on such comparisons. A technique by Willard can be used to process short comparisons that are adjacent in the join tree in time linear in the input size plus output size. In this paper, we describe a new algorithm for evaluating conjunctive queries with both short and long comparisons, and identify an acyclic condition under which linear time can be achieved. We have also implemented the new algorithm on top of Spark, and our experimental results demonstrate order-of-magnitude speedups over SparkSQL on a variety of graph pattern and analytical queries.}
}


@inproceedings{DBLP:conf/sigmod/ManciniKCMA22,
	author = {Riccardo Mancini and
                  Srinivas Karthik and
                  Bikash Chandra and
                  Vasilis Mageirakos and
                  Anastasia Ailamaki},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Efficient Massively Parallel Join Optimization for Large Queries},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {122--135},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517871},
	doi = {10.1145/3514221.3517871},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ManciniKCMA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern data analytical workloads often need to run queries over a large number of tables. An optimal query plan for such queries is crucial for being able to run these queries within acceptable time bounds. However, with queries involving many tables, finding the optimal join order becomes a bottleneck in query optimization. Due to the exponential nature of join order optimization, optimizers resort to heuristic solutions after a threshold number of tables. Our objective is two fold: (a) reduce the optimization time for generating optimal plans; and (b) improve the quality of the heuristic solution. In this paper, we propose a new massively parallel algorithm, MPDP, that can efficiently prune the large search space (via a novel plan enumeration technique) while leveraging the massive parallelism offered by modern hardware (Eg: GPUs). When evaluated on real-world benchmark queries with PostgreSQL, MPDP is at least an order of magnitude faster compared to state-of-the-art techniques for large analytical queries. As a result, we are able to increase the heuristic-fall-back limit from 12 relations to 25 relations with same time budget in PostgreSQL. Also, to handle queries with even larger number of tables, we augment MPDP to a well-known heuristic, IDP2 (iterative DP version 2) and a novel heuristic, UnionDP. By systematically exploring a much larger search space, these heuristics provides query plans that are up to 7 times cheaper as compared to the state-of-the-art techniques while being faster to compute.}
}


@inproceedings{DBLP:conf/sigmod/AbeysingheHR22,
	author = {Supun Abeysinghe and
                  Qiyang He and
                  Tiark Rompf},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Efficient Incrementialization of Correlated Nested Aggregate Queries
                  using Relative Partial Aggregate Indexes {(RPAI)}},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {136--149},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517889},
	doi = {10.1145/3514221.3517889},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AbeysingheHR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incrementalization of queries is imperative in cases where data arrives as streams and output is latency-critical and/or desired before the full data has been received. Incremental execution computes the output at a given time by reusing the previously computed outputs or maintained views rather than re-evaluating the query from scratch. There are various approaches to perform this incrementalization ranging from query-specific algorithms and data structures (e.g., DYN, AJU) to general systems (e.g., DBToaster, Materialize). DBToaster is a state-of-the-art system that comes with an appealing theoretical background based on the idea of applying Incremental View Maintenance (IVM) recursively, maintaining a hierarchy of materialized views via delta queries. However, one key limitation of this approach is its inability to efficiently incrementalize correlated nested-aggregate queries due to an inefficient delta rule for such queries. Moreover, none of the other specialized approaches have shown efficient ways to optimize such queries either. Nonetheless, these types of queries can be found in many real-world application domains (e.g., finance), for which efficient incrementalization remains a crucial open problem. In this work, we propose an approach to incrementalize such queries based on a novel tree-based index structure called Relative Partial Aggregate Indexes (RPAI). Our approach is asymptotically faster than other systems and shows up to 1100× speedups in workloads of practical importance.}
}


@inproceedings{DBLP:conf/sigmod/KersbergenSS22,
	author = {Barrie Kersbergen and
                  Olivier Sprangers and
                  Sebastian Schelter},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Serenade - Low-Latency Session-Based Recommendation in e-Commerce
                  at Scale},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {150--159},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517901},
	doi = {10.1145/3514221.3517901},
	timestamp = {Tue, 21 Mar 2023 20:57:32 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/KersbergenSS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Session-based recommendation predicts the next item with which a user will interact, given a sequence of her past interactions with other items. This machine learning problem targets a core scenario in e-commerce platforms, which aim to recommend interesting items to buy to users browsing the site. Session-based recommenders are difficult to scale due to their exponentially large input space of potential sessions. This impedes offline precomputation of the recommendations, and implies the necessity to maintain state during the online computation of next-item recommendations. We propose VMIS-kNN, an adaptation of a state-of-the-art nearest neighbor approach to session-based recommendation, which leverages a prebuilt index to compute next-item recommendations with low latency in scenarios with hundreds of millions of clicks to search through. Based on this approach, we design and implement the scalable session-based recommender system Serenade, which is in production usage at bol.com, a large European e-commerce platform. We evaluate the predictive performance of VMIS-kNN, and show that Serenade can answer a thousand recommendation requests per second with a 90th percentile latency of less than seven milliseconds in scenarios with millions of items to recommend. Furthermore, we present results from a three week long online A/B test with up to 600 requests per second for 6.5 million distinct items on more than 45 million user sessions from our e-commerce platform. To the best of our knowledge, we provide the first empirical evidence that the superior predictive performance of nearest neighbor approaches to session-based recommendation in offline evaluations translates to superior performance in a real world e-commerce setting.}
}


@inproceedings{DBLP:conf/sigmod/WangH000022,
	author = {Hanchen Wang and
                  Rong Hu and
                  Ying Zhang and
                  Lu Qin and
                  Wei Wang and
                  Wenjie Zhang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Neural Subgraph Counting with Wasserstein Estimator},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {160--175},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526163},
	doi = {10.1145/3514221.3526163},
	timestamp = {Sun, 19 Jan 2025 13:27:29 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/WangH000022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph counting is a fundamental graph analysis task which has been widely used in many applications. As the problem of subgraph counting is NP-complete and hence intractable, approximate solutions have been widely studied, which fail to work with large and complex query graphs. Alternatively, Machine Learning techniques have been recently applied for this problem, yet the existing ML approaches either only support very small data graphs or cannot make full use of the data graph information, which inherently limits their scalability, estimation accuracies and robustness. In this paper, we propose a novel approximate subgraph counting algorithm, NeurSC, that can exploit and combine information from both the query graphs and the data graphs effectively and efficiently. It consists of two components: (1) an extraction module that adaptively generates simple yet representative substructures from data graph for each query graph and (2) an estimator WEst that first computes the representations from individual and joint distributions of query and data graphs and then estimates subgraph counts with the learned representations. Furthermore, we design a novel Wasserstein discriminator in WEst to minimize the Wasserstein distance between query and data graphs by updating the parameters in network with the vertex correspondence relationship between query and data graphs. By doing this, WEst can better capture the correlation between query and data graphs which is essential to the quality of the estimation. We conduct experimental studies on seven large real-life labeled graphs to demonstrate the superior performance of NeurSC in terms of estimation accuracy and robustness.}
}


@inproceedings{DBLP:conf/sigmod/TalbotT22,
	author = {Justin Talbot and
                  Daniel Ting},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Statistical Schema Learning with Occam's Razor},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {176--189},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526174},
	doi = {10.1145/3514221.3526174},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/TalbotT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A judiciously normalized database schema can increase data interpretability, reduce data size, and improve data integrity. However, real world data sets are often stored or shared in a denormalized state. We examine the problem of automatically creating a good schema for a denormalized table, approaching it as an unsupervised machine learning problem which must learn an optimal schema from the data. This differs from past rule-based approaches that focus on normalization into a canonical form. We define a principled schema optimization criterion, based on Occam's razor, that is robust to noise and extensible---allowing users to easily specify desirable properties of the resulting schema. We develop an efficient learning algorithm for this criterion and empirically demonstrate that it is 3 to 100 times faster than previous work and produces higher quality schemas with 1/5th the errors.}
}


@inproceedings{DBLP:conf/sigmod/Trummer22,
	author = {Immanuel Trummer},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{DB-BERT:} {A} Database Tuning Tool that "Reads the Manual"},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {190--203},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517843},
	doi = {10.1145/3514221.3517843},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Trummer22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DB-BERT is a database tuning tool that exploits information gained via natural language analysis of manuals and other relevant text documents. It uses text to identify database system parameters to tune as well as recommended parameter values. DB-BERT applies large, pre-trained language models (specifically, the BERT model) for text analysis. During an initial training phase, it fine-tunes model weights in order to translate natural language hints into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and prioritize hints to achieve optimal performance for a specific database system and benchmark. Both phases are iterative and use reinforcement learning to guide the selection of tuning settings to evaluate (penalizing settings that the database system rejects while rewarding settings that improve performance). In our experiments, we leverage hundreds of text documents about database tuning as input for DB-BERT. We compare DB-BERT against various baselines, considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT finds the best parameter settings among all compared methods. The code of DB-BERT is available online at https://itrummer.github.io/dbbert/.}
}


@inproceedings{DBLP:conf/sigmod/TangWSY0022,
	author = {Xiu Tang and
                  Sai Wu and
                  Mingli Song and
                  Shanshan Ying and
                  Feifei Li and
                  Gang Chen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {PreQR: Pre-training Representation for {SQL} Understanding},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {204--216},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517878},
	doi = {10.1145/3514221.3517878},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/TangWSY0022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the learning-based models are shown to outperform the conventional methods for many database tasks such as cardinality estimation, join order selection and performance tuning. However, most existing learning-based methods adopt the one-hot encoding for SQL query representation, unable to catch complicated semantic context, e.g. structure of query, database schema definition and distribution variance of columns. To address such above problem, we propose a novel pre-trained SQL representation model, called PreQR, which extends the language representation approach to SQL queries. We propose an automaton to encode the query structures, and apply a graph neural network to encode database schema information conditioned on the query. A new SQL encoder is then established by adopting the attention mechanism to support on-the-fly query-aware schema linking. Experimental results on real datasets show that replacing the one-hot encoding with our query representation can significantly improve the performances of existing learning-based models on several database tasks.}
}


@inproceedings{DBLP:conf/sigmod/GalhotraFLFMS22,
	author = {Sainyam Galhotra and
                  Anna Fariha and
                  Raoni Louren{\c{c}}o and
                  Juliana Freire and
                  Alexandra Meliou and
                  Divesh Srivastava},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {DataPrism: Exposing Disconnect between Data and Systems},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {217--231},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517864},
	doi = {10.1145/3514221.3517864},
	timestamp = {Sun, 12 Nov 2023 02:07:18 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/GalhotraFLFMS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As data is a central component of many modern systems, the cause of a system malfunction may reside in the data, and, specifically, particular properties of data. E.g., a health-monitoring system that is designed under the assumption that weight is reported in lbs will malfunction when encountering weight reported in kilograms. Like software debugging, which aims to find bugs in the source code or runtime conditions, our goal is to debug data to identify potential sources of disconnect between the assumptions about some data and systems that operate on that data. We propose DataPrism, a framework to identify data properties (profiles) that are the root causes of performance degradation or failure of a data-driven system. Such identification is necessary to repair data and resolve the disconnect between data and systems. Our technique is based on causal reasoning through interventions: when a system malfunctions for a dataset, DataPrism alters the data profiles and observes changes in the system's behavior due to the alteration. Unlike statistical observational analysis that reports mere correlations, DataPrism reports causally verified root causes -- in terms of data profiles -- of the system malfunction. We empirically evaluate DataPrism on seven real-world and several synthetic data-driven systems that fail on certain datasets due to a diverse set of reasons. In all cases, DataPrism identifies the root causes precisely while requiring orders of magnitude fewer interventions than prior techniques.}
}


@inproceedings{DBLP:conf/sigmod/IslamFMS22,
	author = {Maliha Tashfia Islam and
                  Anna Fariha and
                  Alexandra Meliou and
                  Babak Salimi},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Through the Data Management Lens: Experimental Analysis and Evaluation
                  of Fair Classification},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {232--246},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517841},
	doi = {10.1145/3514221.3517841},
	timestamp = {Sat, 30 Sep 2023 09:56:34 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/IslamFMS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Classification, a heavily-studied data-driven machine learning task, drives an increasing number of prediction systems involving critical human decisions such as loan approval and criminal risk assessment. However, classifiers often demonstrate discriminatory behavior, especially when presented with biased data. Consequently, fairness in classification has emerged as a high-priority research area. Data management research is showing an increasing presence and interest in topics related to data and algorithmic fairness, including the topic of fair classification. The interdisciplinary efforts in fair classification, with machine learning research having the largest presence, have resulted in a large number of fairness notions and a wide range of approaches that have not been systematically evaluated and compared. In this paper, we contribute a broad analysis of 13 fair classification approaches and additional variants, over their correctness, fairness, efficiency, scalability, robustness to data errors, sensitivity to underlying ML model, data efficiency, and stability using a variety of metrics and real-world datasets. Our analysis highlights novel insights on the impact of different metrics and high-level approach characteristics on different aspects of performance. We also discuss general principles for choosing approaches suitable for different practical settings, and identify areas where data-management-centric solutions are likely to have the most impact.}
}


@inproceedings{DBLP:conf/sigmod/PradhanZGS22,
	author = {Romila Pradhan and
                  Jiongli Zhu and
                  Boris Glavic and
                  Babak Salimi},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Interpretable Data-Based Explanations for Fairness Debugging},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {247--261},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517886},
	doi = {10.1145/3514221.3517886},
	timestamp = {Sun, 12 Nov 2023 02:07:19 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/PradhanZGS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A wide variety of fairness metrics and eXplainable Artificial Intelligence (XAI) approaches have been proposed in the literature to identify bias in machine learning models that are used in critical real-life contexts. However, merely reporting on a model's bias or generating explanations using existing XAI techniques is insufficient to locate and eventually mitigate sources of bias. We introduce Gopher, a system that produces compact, interpretable, and causal explanations for bias or unexpected model behavior by identifying coherent subsets of the training data that are root-causes for this behavior. Specifically, we introduce the concept of causal responsibility that quantifies the extent to which intervening on training data by removing or updating subsets of it can resolve the bias. Building on this concept, we develop an efficient approach for generating the top-k patterns that explain model bias by utilizing techniques from the machine learning (ML) community to approximate causal responsibility, and using pruning rules to manage the large search space for patterns. Our experimental evaluation demonstrates the effectiveness of Gopher in generating interpretable explanations for identifying and debugging sources of bias.}
}


@inproceedings{DBLP:conf/sigmod/0001ISR22,
	author = {Dong Wei and
                  Md Mouinul Islam and
                  Baruch Schieber and
                  Senjuti Basu Roy},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Rank Aggregation with Proportionate Fairness},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {262--275},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517865},
	doi = {10.1145/3514221.3517865},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0001ISR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given multiple individual rank orders over a set of candidates or items, where the candidates belong to multiple (non-binary) protected groups, we study the classical rank aggregation problem subject to proportionate fairness or p-fairness (RAPF in short), considering Kemeny distance. We first study the problem of producing the closest p-fair ranking to an individual ranked order IPF in short) considering Kendall-Tau distance, and present multiple solutions for IPF. We then present two computational frameworks(a randomized randpickperm and a deterministic algpickperm) to solve RAPF that leverages the solutions of IPF as a subroutine. We make several non-trivial algorithmic contributions: (i) we prove that when the group protected attribute is binary, IPF can be solved exactly using a greedy technique; (ii) we present two different solutions for IPF when the group protected attribute is multi-valued, algexact is optimal and algapprox admits a 2 approximation factor; (iii) we design a framework for RAPF solution with an approximation factor that is 2+ the approximation factor of the IPF solution. The resulting randpickperm and algpickperm solutions exhibit 3 and 4 approximation factors when designed using algexact and algapprox, respectively. We run extensive experiments using multiple real world and large scale synthetic datasets and compare our proposed solutions against multiple state-of-the-art related works to demonstrate the effectiveness and efficiency of our studied problem and proposed solution.}
}


@inproceedings{DBLP:conf/sigmod/GalhotraSSV22,
	author = {Sainyam Galhotra and
                  Karthikeyan Shanmugam and
                  Prasanna Sattigeri and
                  Kush R. Varshney},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Causal Feature Selection for Algorithmic Fairness},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {276--285},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517909},
	doi = {10.1145/3514221.3517909},
	timestamp = {Sat, 30 Sep 2023 09:56:33 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/GalhotraSSV22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The use of machine learning (ML) in high-stakes societal decisions has encouraged the consideration of fairness throughout the ML lifecycle. Although data integration is one of the primary steps to generate high-quality training data, most of the fairness literature ignores this stage. In this work, we consider fairness in the integration component of data management, aiming to identify features that improve prediction without adding any bias to the dataset. We work under the causal fairness paradigm. Without requiring the underlying structural causal model a priori, we propose an approach to identify a sub-collection of features that ensure fairness of the dataset by performing conditional independence tests between different subsets of features. We use group testing to improve the complexity of the approach. We theoretically prove the correctness of the proposed algorithm and show that sublinear conditional independence tests are sufficient to identify these variables. A detailed empirical evaluation is performed on real-world datasets to demonstrate the efficacy and efficiency of our technique.}
}


@inproceedings{DBLP:conf/sigmod/XuLN22,
	author = {Yunlong Xu and
                  Jinshu Liu and
                  Fatemeh Nargesian},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{TSUBASA:} Climate Network Construction on Historical and Real-Time
                  Data},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {286--295},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526177},
	doi = {10.1145/3514221.3526177},
	timestamp = {Sun, 04 Aug 2024 19:37:27 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/XuLN22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A climate network represents the global climate system by the interactions of a set of anomaly time-series. Network science has been applied to climate data to study the dynamics of a climate network. The core task to enable network dynamics analysis on climate data is the efficient computation and update of the correlation matrix for user-defined time-windows on historical and real-time data. We present TSUBASA, an algorithm for efficiently computing the exact pair-wise time-series correlation based on Pearson's correlation. By pre-computing simple and low-overhead sketches, TSUBASA can efficiently compute exact pairwise correlations on arbitrary time windows at query time. For real-time data, TSUBASA proposes a fast and incremental way of updating the correlation matrix. We provide a detailed time and space complexity analysis of TSUBASA. Our experiments show that with the same space overhead as a DFT-based approximate solution, TSUBASA has a lower sketching time and is on par with the approximate solution with respect to query time. TSUBASA is at least one order of magnitude faster than a baseline for both historical and real-time data.}
}


@inproceedings{DBLP:conf/sigmod/KimKEM22,
	author = {Bogyeong Kim and
                  Kyoseung Koo and
                  Undraa Enkhbat and
                  Bongki Moon},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {DenForest: Enabling Fast Deletion in Incremental Density-Based Clustering
                  over Sliding Windows},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {296--309},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517833},
	doi = {10.1145/3514221.3517833},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/KimKEM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The density-based clustering is utilized for various applications such as hot spot detection or segmentation. To serve those applications in real time, it is desired to update clusters incrementally by capturing only the recent data. The previous incremental density-based clustering algorithms often represent clusters as a graph and suffer serious performance degradation. This is because a costly graph traversal is required to check whether a cluster is still connected whenever a point is removed. In order to address the problem of slow deletion, this paper proposes a novel incremental density-based clustering algorithm called DenForest. By maintaining clusters as a group of spanning trees instead of a graph, DenForest can determine efficiently and accurately whether a cluster is to be split by a point removed from the window in logarithmic time. With extensive evaluations, it is demonstrated that DenForest outperforms the state-of-the-art density-based clustering algorithms significantly and achieves the clustering quality comparable with that of DBSCAN.}
}


@inproceedings{DBLP:conf/sigmod/SivanGS22,
	author = {Hadar Sivan and
                  Moshe Gabel and
                  Assaf Schuster},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {AutoMon: Automatic Distributed Monitoring for Arbitrary Multivariate
                  Functions},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {310--324},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517866},
	doi = {10.1145/3514221.3517866},
	timestamp = {Sun, 19 Jan 2025 13:27:31 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/SivanGS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approaches for evaluating functions over distributed data streams are increasingly important as data sources become more geographically distributed. However, existing methodologies are limited to small classes of functions, requiring non-trivial effort and substantial mathematical sophistication to tailor them to new functions. In this work we present AutoMon, the first general solution to this problem. AutoMon enables automatic, communication-efficient distributed monitoring of arbitrary functions. Given source code that computes a function from centralized data, the AutoMon algorithm approximates the function over the aggregate of distributed data streams, without centralizing data updates. Our evaluation shows that AutoMon sends the same number or fewer messages as state-of-the-art techniques when monitoring specific functions for which a distributed, hand-crafted solution is known. AutoMon, however, is a lot more powerful. It automatically generates a communication-efficient distributed monitoring solution for arbitrary functions, e.g., monitoring deep neural networks inference tasks for which no non-trivial solution is known.}
}


@inproceedings{DBLP:conf/sigmod/TenchWZBCDFSZ22,
	author = {David Tench and
                  Evan West and
                  Victor Zhang and
                  Michael A. Bender and
                  Abiyaz Chowdhury and
                  J. Ahmed Dellas and
                  Martin Farach{-}Colton and
                  Tyler Seip and
                  Kenny Zhang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {GraphZeppelin: Storage-Friendly Sketching for Connected Components
                  on Dynamic Graph Streams},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {325--339},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526146},
	doi = {10.1145/3514221.3526146},
	timestamp = {Sun, 06 Oct 2024 21:14:20 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/TenchWZBCDFSZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding the connected components of a graph is a fundamental problem with uses throughout computer science and engineering. The task of computing connected components becomes more difficult when graphs are very large, or when they are dynamic, meaning the edge set changes over time subject to a stream of edge insertions and deletions. A natural approach to computing the connected components on a large, dynamic graph stream is to buy enough RAM to store the entire graph. However, the requirement that the graph fit in RAM is prohibitive for very large graphs. Thus, there is an unmet need for systems that can process dense dynamic graphs, especially when those graphs are larger than available RAM. We present a new high-performance streaming graph-processing system for computing the connected components of a graph. This system, which we call GraphZeppelin, uses new linear sketching data structures (CubeSketch) to solve the streaming connected components problem and as a result requires space asymptotically smaller than the space required for a lossless representation of the graph. GraphZeppelin is optimized for massive dense graphs: GraphZeppelin can process millions of edge updates (both insertions and deletions) per second, even when the underlying graph is far too large to fit in available RAM. As a result GraphZeppelin vastly increases the scale of graphs that can be processed.}
}


@inproceedings{DBLP:conf/sigmod/AmirKS22,
	author = {Adar Amir and
                  Ilya Kolchinsky and
                  Assaf Schuster},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{DLACEP:} {A} Deep-Learning Based Framework for Approximate Complex
                  Event Processing},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {340--354},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526136},
	doi = {10.1145/3514221.3526136},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AmirKS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Complex event processing (CEP) is employed to detect user-specified patterns of events in data streams. CEP mechanisms operate by maintaining all sets of events that can potentially be composed into a pattern match. This approach can be wasteful when many of the sets do not participate in an actual match and are therefore discarded. We present DLACEP, a novel framework that fuses deep learning with CEP to efficiently extract complex pattern matches from streams. To the best of our knowledge, this is the first time deep learning is employed to detect events constituting a pattern match in the realm of CEP. To assess our approach, we performed extensive empirical testing on various scenarios with both real-world and synthetic data. We showcase examples in which our method achieves an increase in throughput of up to three orders of magnitude compared to solely employing CEP, while only suffering a minor loss in the number of detected matches.}
}


@inproceedings{DBLP:conf/sigmod/GiladMRY22,
	author = {Amir Gilad and
                  Zhengjie Miao and
                  Sudeepa Roy and
                  Jun Yang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Understanding Queries by Conditional Instances},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {355--368},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517898},
	doi = {10.1145/3514221.3517898},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/GiladMRY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A powerful way to understand a complex query is by observing how it operates on data instances. However, specific database instances are not ideal for such observations: they often include large amounts of superfluous details that are not only irrelevant to understanding the query but also cause cognitive overload; and one specific database may not be enough. Given a relational query, is it possible to provide a simple and generic "representative\'\' instance that (1) illustrates how the query can be satisfied, (2) summarizes all specific instances that would satisfy the query in the same way by abstracting away unnecessary details? Furthermore, is it possible to find a collection of such representative instances that together completely characterize all possible ways in which the query can be satisfied? This paper takes initial steps towards answering these questions. We design what these representative instances look like, define what they stand for, and formalize what it means for them to satisfy a query in "all possible ways." We argue that this problem is undecidable for general domain relational calculus queries, and develop practical algorithms for computing a minimum collection of such instances subject to other constraints. We evaluate the efficiency of our approach experimentally, and show its effectiveness in helping users debug relational queries through a user study.}
}


@inproceedings{DBLP:conf/sigmod/FlokasWLWV022,
	author = {Lampros Flokas and
                  Weiyuan Wu and
                  Yejia Liu and
                  Jiannan Wang and
                  Nakul Verma and
                  Eugene Wu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Complaint-Driven Training Data Debugging at Interactive Speeds},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {369--383},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517849},
	doi = {10.1145/3514221.3517849},
	timestamp = {Fri, 24 Nov 2023 11:53:31 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/FlokasWLWV022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern databases support queries that perform model inference (inference queries). Although powerful and widely used, inference queries are susceptible to incorrect results if the model is biased due to training data errors. Recently, prior work Rain proposed complaint-driven data debugging which uses user-specified errors in the output of inference queries (Complaints) to rank erroneous training examples that most likely caused the complaint. This can help users better interpret results and debug training sets. Rain combined influence analysis from the ML literature with relaxed query provenance polynomials from the DB literature to approximate the derivative of complaints w.r.t. training examples. Although effective, the runtime is O(|T|d), where T and d are the training set and model sizes, due to its reliance on the model's second order derivatives (the Hessian). On a Wide Resnet Network (WRN) model with 1.5 million parameters, it takes >1 minute to debug a complaint. We observe that most complaint debugging costs are independent of the complaint, and that modern models are overparameterized. In response, Rain++ uses precomputation techniques, based on non-trivial insights unique to data debugging, to reduce debugging latencies to a constant factor independent of model size. We also develop optimizations when the queried database is known apriori, and for standing queries over streaming databases. Combining these optimizations in Rain++ ensures interactive debugging latencies (~1ms) on models with millions of parameters.}
}


@inproceedings{DBLP:conf/sigmod/FanHWX22,
	author = {Wenfei Fan and
                  Ziyan Han and
                  Yaoshu Wang and
                  Min Xie},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Parallel Rule Discovery from Large Datasets by Sampling},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {384--398},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526165},
	doi = {10.1145/3514221.3526165},
	timestamp = {Mon, 26 Jun 2023 20:43:17 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/FanHWX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rule discovery from large datasets is often prohibitively costly. The problem becomes more staggering when the rules are collectively defined across multiple tables. To scale with large datasets, this paper proposes a multi-round sampling strategy for rule discovery. We consider entity enhancing rules (REEs) for collective entity resolution and conflict resolution, which may carry constant patterns and machine learning predicates. We sample large datasets with accuracy bounds a and B such that at least a% of rules discovered from samples are guaranteed to hold on the entire dataset (i.e., precision), and at least B% of rules on the entire dataset can be mined from the samples (i.e., recall). We also quantify the connection between support and confidence of the rules on samples and their counterparts on the entire dataset. To scale with the number of tuple variables in collective rules, we adopt deep Q-learning to select semantically relevant predicates. To improve the recall, we develop a tableau method to recover constant patterns from the dataset. We parallelize the algorithm such that it guarantees to reduce runtime when more processors are used. Using real-life and synthetic data, we empirically verify that the method speeds up REE discovery by 12.2 times with sample ratio 10% and recall 82%.}
}


@inproceedings{DBLP:conf/sigmod/Huang022,
	author = {Zezhou Huang and
                  Eugene Wu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Reptile: Aggregation-level Explanations for Hierarchical Data},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {399--413},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517854},
	doi = {10.1145/3514221.3517854},
	timestamp = {Sun, 06 Aug 2023 20:52:01 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/Huang022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Users often can see from overview-level statistics that some results look "off", but are rarely able to characterize even the type of error. Reptile is an iterative human-in-the-loop explanation and cleaning system for errors in hierarchical data. Users specify an anomalous distributive aggregation result (a complaint), and Reptile recommends drill-down operations to help the user "zoom-in" on the underlying errors. Unlike prior explanation systems that intervene on raw records, Reptile intervenes by learning a group\'s expected statistics, and ranks drill-down sub-groups by how much the intervention fixes the complaint. This group-level formulation supports a wide range of error types (missing, duplicates, value errors) and uniquely leverages the distributive properties of the user complaint. Further, the learning-based intervention lets users provide domain expertise that Reptile learns from. In each drill-down iteration, Reptile must train a large number of predictive models. We thus extend factorized learning from count-join queries to aggregation-join queries, and develop a suite of optimizations that leverage the data\'s hierarchical structure. These optimizations reduce runtimes by >6× compared to a Lapack-based implementation. When applied to real-world Covid-19 and African farmer survey data, Reptile correctly identifies 21/30 (vs 2 using existing explanation approaches) and 20/22 errors. Reptile has been deployed in Ethiopia and Zambia, and used to clean nation-wide farmer survey data; the clean data has been used to design national drought insurance policies.}
}


@inproceedings{DBLP:conf/sigmod/GalhotraFSS22,
	author = {Sainyam Galhotra and
                  Donatella Firmani and
                  Barna Saha and
                  Divesh Srivastava},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Hierarchical Entity Resolution using an Oracle},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {414--428},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526147},
	doi = {10.1145/3514221.3526147},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/GalhotraFSS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many applications, entity references (i.e., records) and entities need to be organized to capture diverse relationships like type-subtype, is-A (mapping entities to types), and duplicate (mapping records to entities) relationships. However, automatic identification of such relationships is often inaccurate due to noise and heterogeneous representation of records across sources. Similarly, manual maintenance of these relationships is infeasible and does not scale to large datasets. In this work, we circumvent these challenges by considering weak supervision in the form of an oracle to formulate a novel hierarchical ER task. In this setting, records are clustered in a tree-like structure containing records at leaf-level and capturing record-entity (duplicate), entity-type (is-A) and subtype-supertype relationships. For effective use of supervision, we leverage triplet comparison oracle queries that take three records as input and output the most similar pair(s). We develop HierER, a querying strategy that uses record pair similarities to minimize the number of oracle queries while maximizing the identified hierarchical structure. We show theoretically and empirically that HierER is effective under different similarity noise models and demonstrate empirically that HierER can scale up to million-size datasets.}
}


@inproceedings{DBLP:conf/sigmod/0002GC0L22,
	author = {Dezhong Yao and
                  Yuhong Gu and
                  Gao Cong and
                  Hai Jin and
                  Xinqiao Lv},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Entity Resolution with Hierarchical Graph Attention Networks},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {429--442},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517872},
	doi = {10.1145/3514221.3517872},
	timestamp = {Tue, 21 Mar 2023 20:57:32 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0002GC0L22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity Resolution (ER) links entities that refer to the same real-world entity from different sources. Existing work usually takes pairs of entities as input and judges those pairs independently. However, there is often interdependence between different pairs of ER decisions, e.g., the entities from the same data source are usually semantically related to each other. Furthermore, current ER approaches are mainly based on attribute similarity comparison, but ignore interdependence between attributes. To address the limits of existing methods, we propose HierGAT, a new method for ER based on a Hierarchical Graph Attention Transformer Network, which can model and exploit the interdependence between different ER decisions. The benefit of our method comes from: 1) The graph attention network model for joint ER decisions; 2) The graph-attention capability to identify the discriminative words from attributes and find the most discriminative attributes. Furthermore, we propose to learn contextual embeddings to enrich word embeddings for better performance. The experimental results on publicly available benchmark datasets show that HierGAT outperforms DeepMatcher by up to 32.5% of F1 score and up to 8.7% of F1 score compared with Ditto.}
}


@inproceedings{DBLP:conf/sigmod/TuF0WC0F022,
	author = {Jianhong Tu and
                  Ju Fan and
                  Nan Tang and
                  Peng Wang and
                  Chengliang Chai and
                  Guoliang Li and
                  Ruixue Fan and
                  Xiaoyong Du},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Domain Adaptation for Deep Entity Resolution},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {443--457},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517870},
	doi = {10.1145/3514221.3517870},
	timestamp = {Mon, 26 Jun 2023 20:43:17 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/TuF0WC0F022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity resolution (ER) is a core problem of data integration. The state-of-the-art (SOTA) results on ER are achieved by deep learning (DL) based methods, trained with a lot of labeled matching/non-matching entity pairs. This may not be a problem when using well-prepared benchmark datasets. Nevertheless, for many real-world ER applications, the situation changes dramatically, with a painful issue to collect large-scale labeled datasets. In this paper, we seek to answer: If we have a well-labeled source ER dataset, can we train a DL-based ER model for a target dataset, without any labels or with a few labels? This is known as domain adaptation (DA), which has achieved great successes in computer vision and natural language processing, but is not systematically studied for ER. Our goal is to systematically explore the benefits and limitations of a wide range of DA methods for ER. To this purpose, we develop a DADER (Domain Adaptation for Deep Entity Resolution) framework that significantly advances ER in applying DA. We define a space of design solutions for the three modules of DADER, namely Feature Extractor, Matcher, and Feature Aligner. We conduct so far the most comprehensive experimental study to explore the design space and compare different choices of DA for ER. We provide guidance for selecting appropriate design solutions based on extensive experiments.}
}


@inproceedings{DBLP:conf/sigmod/HouKMWTC22,
	author = {Pei{-}Yu Hou and
                  Daniel Robert Korn and
                  Cleber C. Melo{-}Filho and
                  David R. Wright and
                  Alexander Tropsha and
                  Rada Chirkova},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Compact Walks: Taming Knowledge-Graph Embeddings with Domain- and
                  Task-Specific Pathways},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {458--469},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517903},
	doi = {10.1145/3514221.3517903},
	timestamp = {Mon, 03 Mar 2025 21:21:49 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/HouKMWTC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge-graph (KG) embeddings have emerged as a promise in addressing challenges faced by modern biomedical research, including the growing gap between therapeutic needs and available treatments. The popularity of KG embeddings in graph analytics is on the rise, due at least partially to the presumed semanticity of the learned embeddings. Unfortunately, the ability of a node neighborhood picked up by an embedding to capture the node's semantics may depend on the characteristics of the data. One of the reasons for this problem is that KG nodes can be promiscuous, that is, associated with a number of different relationships that are not unique or indicative of the properties of the nodes. To address the promiscuity challenge and the documented runtime-performance challenge in real-life KG embedding tools, we propose to use domain- and task-specific information to specify regular-expression pathways that define neighborhoods of KG nodes of interest. Our proposed CompactWalks framework uses these semantic subgraphs to enable meaningful compact walks in random-walk based KG embedding methods. We report the results of case studies for the task of determining which pharmaceutical drugs could treat the same diseases. The findings suggest that our CompactWalks approach has the potential to address the promiscuity and runtime-performance challenges in applying embedding tools to large-scale KGs in real life, in the biomedical domain and possibly beyond.}
}


@inproceedings{DBLP:conf/sigmod/MiaoSZZNY022,
	author = {Xupeng Miao and
                  Yining Shi and
                  Hailin Zhang and
                  Xin Zhang and
                  Xiaonan Nie and
                  Zhi Yang and
                  Bin Cui},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{HET-GMP:} {A} Graph-based System Approach to Scaling Large Embedding
                  Model Training},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {470--480},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517902},
	doi = {10.1145/3514221.3517902},
	timestamp = {Sun, 19 Jan 2025 13:27:32 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/MiaoSZZNY022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Embedding models have been recognized as an effective learning paradigm for high-dimensional data. However, a major embedding model training obstacle is that updating and retrieving the shared large-scale embedding parameters usually dominates the distributed training cycle, leading to significant scalability issues. This paper presents HET-GMP, a distributed system on training embedding models. Uniquely, HET-GMP takes advantage of a graph-based approach to efficiently increase scalability. The key insight guiding our design is the "graph way of thinking". HET-GMP creates a bigraph abstraction to represent the access relationships between data samples and embedding vectors. This enables HET-GMP to embrace graph locality and skewness as new performance opportunities and to exploit graph-based replication/partitioning and bounded-asynchronous synchronization to reduce communication overhead. We evaluate the system on the embedding models for click-through rate (CTR) prediction, which presents the most significant challenge and communication bottleneck due to heavy access concurrency to a huge embedding table. The result shows that HET-GMP supports embedding model training with 1011 parameters, achieving a reduction in communication up to 87.5% and an up-to 27.5x speedup over the state-of-the-art baseline systems.}
}


@inproceedings{DBLP:conf/sigmod/Renz-WielandGKM22,
	author = {Alexander Renz{-}Wieland and
                  Rainer Gemulla and
                  Zoi Kaoudi and
                  Volker Markl},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {NuPS: {A} Parameter Server for Machine Learning with Non-Uniform Parameter
                  Access},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {481--495},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517860},
	doi = {10.1145/3514221.3517860},
	timestamp = {Sat, 30 Sep 2023 09:56:34 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/Renz-WielandGKM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parameter servers (PSs) facilitate the implementation of distributed training for large machine learning tasks. In this paper, we argue that existing PSs are inefficient for tasks that exhibit non-uniform parameter access; their performance may even fall behind that of single node baselines. We identify two major sources of such non-uniform access: skew and sampling. Existing PSs are ill-suited for managing skew because they uniformly apply the same parameter management technique to all parameters. They are inefficient for sampling because the PS is oblivious to the associated randomized accesses and cannot exploit locality. To overcome these performance limitations, we introduce NuPS, a novel PS architecture that (i) integrates multiple management techniques and employs a suitable technique for each parameter and (ii) supports sampling directly via suitable sampling primitives and sampling schemes that allow for a controlled quality-efficiency trade-off. In our experimental study, NuPS outperformed existing PSs by up to one order of magnitude and provided up to linear scalability across multiple machine learning tasks.}
}


@inproceedings{DBLP:conf/sigmod/KangAPBZ22,
	author = {Daniel Kang and
                  Nikos Ar{\'{e}}chiga and
                  Sudeep Pillai and
                  Peter D. Bailis and
                  Matei Zaharia},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Finding Label and Model Errors in Perception Data With Learned Observation
                  Assertions},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {496--505},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517907},
	doi = {10.1145/3514221.3517907},
	timestamp = {Sun, 19 Jan 2025 13:27:32 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/KangAPBZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {ML is being deployed in complex, real-world scenarios where errors have impactful consequences. In these systems, thorough testing of the ML pipelines is critical. A key component in ML deployment pipelines is the curation of labeled training data. Common practice in the ML literature assumes that labels are the ground truth. However, in our experience in a large autonomous vehicle development center, we have found that vendors can often provide erroneous labels, which can lead to downstream safety risks in trained models. To address these issues, we propose a new abstraction, learned observation assertions, and implement it in a system called Fixy. Fixy leverages existing organizational resources, such as existing (possibly noisy) labeled datasets or previously trained ML models, to learn a probabilistic model for finding errors in human- or model-generated labels. Given user-provided features and these existing resources, Fixy learns feature distributions that specify likely and unlikely values (e.g., that a speed of 30mph is likely but 300mph is unlikely). It then uses these feature distributions to score labels for potential errors. We show that Fixy can automatically rank potential errors in real datasets with up to 2x higher precision compared to recent work on model assertions and standard techniques such as uncertainty sampling. Furthermore, Fixy can uncover labeling errors in 70% of scenes in a popular autonomous vehicle dataset.}
}


@inproceedings{DBLP:conf/sigmod/Nakandala022,
	author = {Supun Nakandala and
                  Arun Kumar},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Nautilus: An Optimized System for Deep Transfer Learning over Evolving
                  Training Datasets},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {506--520},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517846},
	doi = {10.1145/3514221.3517846},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Nakandala022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL) has revolutionized unstructured data analytics. But in most cases, DL needs massive labeled datasets and large compute clusters, which hinders its adoption. These limitations can be overcome using a popular paradigm called deep transfer learning (DTL). With DTL, one adapts a pre-trained DL model instead of training a model from scratch. Thus, DTL reduces the massive training data and compute requirements to train a model. During adaptation, a common practice is to freeze most pre-trained model parts and adapt only the remaining. Since no single adaptation scheme is universally the best, one often evaluates several schemes, which is also called model selection. We also observed that data labeling for DTL is seldom a one-off process. One often updates their labeled data intermittently by adding new labeled data and performs model selection to evaluate the accuracy of the trained models. Today, one executes this workload by performing computations for the entire pre-trained model and repeats it for every model selection cycle. This approach results in redundant computations in frozen model parts and causes usability and system inefficiency issues. In this work, we reimagine DTL model selection in the presence of frozen layers as an instance of multi-query optimization and propose two optimizations that reduce redundant computations and training overheads. We implement our optimizations into a data system called Nautilus. Experiments with end-to-end workloads on benchmark datasets show that Nautilus reduces DTL model selection runtimes by up to 5X compared to the current practice.}
}


@inproceedings{DBLP:conf/sigmod/ZuoAD22,
	author = {Chaoji Zuo and
                  Sepehr Assadi and
                  Dong Deng},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Spine: Scaling up Programming-by-Negative-Example for String Filtering
                  and Transformation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {521--530},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517908},
	doi = {10.1145/3514221.3517908},
	timestamp = {Sun, 19 Jan 2025 13:27:31 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZuoAD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Program synthesis (a.k.a. programming-by-example, PBE) has been deployed in several widely-used commercial products, such as Microsoft Excel, Power BI, and Google Spreadsheet, due to its effectiveness and user-friendliness. It takes a few user-provided positive and negative examples as input and produces a program that is consistent with all the examples, which helps end-users wrangle messy texts without writing any code. In this paper, we focus on two text wrangling tasks, string filtering and transformation. Existing PBE systems for string filtering do not scale well with negative examples. This is because they first explicitly synthesize all the consistent programs and then greedily search a good one in them. However, when there are negative examples, it could take an exponential time and space to synthesize all the exponential number of consistent programs. In contrast, we propose to synthesize all the programs consistent with the positive examples first and then lazily determine whether a program is also consistent with all the negative examples on demand in the search step. For this purpose, we develop a dynamic programming algorithm to search the optimal consistent program. Many programs are never explored during dynamic programming as they are dominated by other better consistent programs. As for string transformation, existing PBE systems do not even support negative examples. Our approach naturally extends to string transformation. Experimental results show that our methods significantly outperformed the state-of-the-art string filtering and transformation approaches and achieved better scalability.}
}


@inproceedings{DBLP:conf/sigmod/PengDW0Z22,
	author = {Jinglin Peng and
                  Bolin Ding and
                  Jiannan Wang and
                  Kai Zeng and
                  Jingren Zhou},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {One Size Does Not Fit All: {A} Bandit-Based Sampler Combination Framework
                  with Theoretical Guarantees},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {531--544},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517900},
	doi = {10.1145/3514221.3517900},
	timestamp = {Wed, 19 Mar 2025 21:16:27 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/PengDW0Z22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sample-based estimation, which uses a sample to estimate population parameters (e.g., SUM, COUNT, and AVG), has various applications in database systems. A sampler defines how samples are drawn from a population. Various samplers have been proposed (e.g., uniform sampler, stratified sampler, and measure-biased sampler), since there is no single sampler that works well in all cases. To overcome the "one size does not fit all" challenge, we study how to combine multiple samplers to estimate population parameters, and propose SamComb, a novel bandit-based sampler combination framework. Given a set of samplers, a budget, and a population parameter, SamComb can automatically decide how much budget should be allocated to each sampler so that the combined estimation achieves the highest accuracy. We model this sampler combination problem as a multi-armed bandit (MAB) problem and propose effective approaches to balance the exploration and exploitation trade-off in a principled way. We provide theoretical guarantees for our approaches and conduct extensive experiments on both synthetic and real datasets. The results show that there is a strong need to combine multiple samplers, in order to obtain accurate estimations without the knowledge about population predicates and distributions, and SamComb is an effective framework to achieve this goal.}
}


@inproceedings{DBLP:conf/sigmod/ChunduriBLA22,
	author = {Pramod Chunduri and
                  Jaeho Bang and
                  Yao Lu and
                  Joy Arulraj},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Zeus: Efficiently Localizing Actions in Videos using Reinforcement
                  Learning},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {545--558},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526181},
	doi = {10.1145/3514221.3526181},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ChunduriBLA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detection and localization of actions in videos is an important problem in practice. State-of-the-art video analytics systems are unable to efficiently and effectively answer such action queries because actions often involve a complex interaction between objects and are spread across a sequence of frames; detecting and localizing them requires computationally expensive deep neural networks. It is also important to consider the entire sequence of frames to answer the query effectively. In this paper, we present ZEUS, a video analytics system tailored for answering action queries. We present a novel technique for efficiently answering these queries using deep reinforcement learning. ZEUS trains a reinforcement learning agent that learns to adaptively modify the input video segments that are subsequently sent to an action classification network. The agent alters the input segments along three dimensions - sampling rate, segment length, and resolution. To meet the user-specified accuracy target, ZEUS's query optimizer trains the agent based on an accuracy-aware, aggregate reward function. Evaluation on three diverse video datasets shows that ZEUS outperforms state-of-the-art frame- and window-based filtering techniques by up to 22.1x and 4.7x, respectively. It also consistently meets the user-specified accuracy target across all queries.}
}


@inproceedings{DBLP:conf/sigmod/CaoSHAK22,
	author = {Jiashen Cao and
                  Karan Sarkar and
                  Ramyad Hadidi and
                  Joy Arulraj and
                  Hyesoon Kim},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {FiGO: Fine-Grained Query Optimization in Video Analytics},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {559--572},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517857},
	doi = {10.1145/3514221.3517857},
	timestamp = {Mon, 26 Jun 2023 20:43:17 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/CaoSHAK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video database management systems (VDBMSs) enable automated analysis of videos at scale using computationally-intensive deep learning models. To reduce the computational overhead of these models, researchers have proposed two techniques: (1) leveraging a specialized, lightweight model to filter out irrelevant frames or to directly answer the query, and (2) using a cascade of models of increasing complexity to answer the query. For both techniques, the query optimizer generates a coarse-grained query plan for the entire video. These techniques suffer from four limitations: (1) lower query accuracy over hard-to-detect predicates, (2) lower filtering efficacy with frequently-occurring objects, (3) lower accuracy due to nontrivial model cascade configuration, and (4) missed optimization opportunities due to coarse-grained planning for the entire video. In this paper, we present FiGO to tackle these limitations. The design of FiGO is centered around three techniques. First, it uses an ensemble of models to support a range of throughput-accuracy tradeoffs. Second, it adopts a fine-grained approach to query optimization. It processes different chunks of the video using different models in the given ensemble to meet the user's accuracy requirement. Lastly, it uses a lightweight technique to prune the model ensemble to lower the query optimization time. We empirically show that these techniques enable FiGO to outperform the state-of-the-art systems for processing queries over videos by 3.3x on average across four video datasets.}
}


@inproceedings{DBLP:conf/sigmod/ChenH0QZ22,
	author = {Zihao Chen and
                  Baokun Han and
                  Chen Xu and
                  Weining Qian and
                  Aoying Zhou},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Redundancy Elimination in Distributed Matrix Computation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {573--586},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517877},
	doi = {10.1145/3514221.3517877},
	timestamp = {Sun, 19 Jan 2025 13:27:33 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ChenH0QZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As matrix computation becomes increasingly prevalent in large-scale data analysis, distributed matrix computation solutions have emerged. These solutions support query interfaces of linear algebra expressions, which often contain redundant subexpressions, i.e., common and loop-constant subexpressions. Hence, existing compilers rewrite queries to eliminate such redundancy. However, due to the large search space, they fail to find all redundant subexpressions, especially for matrix multiplication chains. Furthermore, redundancy elimination may change the original execution order of operators, and have negative impacts. To reduce the large search space and avoid the negative impacts, we propose automatic elimination and adaptive elimination, respectively. In particular, automatic elimination adopts a block-wise search that exploits the properties of matrix computation for speed-up. Adaptive elimination employs a cost model and a dynamic programming-based method to generate efficient plans for redundancy elimination. Finally, we implement ReMac atop SystemDS, eliminating redundancy in distributed matrix computation. In our experiments, ReMac is able to generate efficient execution plans at affordable overhead costs, and outperforms state-of-the-art solutions by an order of magnitude.}
}


@inproceedings{DBLP:conf/sigmod/ParkSBSIK22,
	author = {Kwanghyun Park and
                  Karla Saur and
                  Dalitso Banda and
                  Rathijit Sen and
                  Matteo Interlandi and
                  Konstantinos Karanasos},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {End-to-end Optimization of Machine Learning Prediction Queries},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {587--601},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526141},
	doi = {10.1145/3514221.3526141},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ParkSBSIK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Prediction queries are widely used across industries to perform advanced analytics and draw insights from data. They include a data processing part (e.g., for joining, filtering, cleaning, featurizing the datasets) and a machine learning (ML) part invoking one or more trained models to perform predictions. These parts have so far been optimized in isolation, leaving significant opportunities for optimization unexplored. We present Raven, a production-ready system for optimizing prediction queries. Raven follows the enterprise architectural trend of collocating data and ML runtimes. It relies on a unified intermediate representation that captures both data and ML operators in a single graph structure to unlock two families of optimizations. First, it employs logical optimizations that pass information between the data part (and the properties of the underlying data) and the ML part to optimize each other. Second, it introduces logical-to-physical transformations that allow operators to be executed on different run-times (relational, ML, and DNN) and hardware (CPU, GPU). Novel data-driven optimizations determine the runtime to be used for each part of the query to achieve optimal performance. Our evaluation shows that Raven is able to improve performance of prediction queries on Apache Spark and SQL Server by up to 13.1x and 330x, respectively. Finally, for complex models where GPU acceleration is beneficial, Raven provides up to 8× speedup compared to state-of-the-art systems.}
}


@inproceedings{DBLP:conf/sigmod/XuKAR22,
	author = {Zhuangdi Xu and
                  Gaurav Tarlok Kakkar and
                  Joy Arulraj and
                  Umakishore Ramachandran},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{EVA:} {A} Symbolic Approach to Accelerating Exploratory Video Analytics
                  with Materialized Views},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {602--616},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526142},
	doi = {10.1145/3514221.3526142},
	timestamp = {Sun, 19 Jan 2025 13:27:22 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/XuKAR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advances in deep learning have led to a resurgence of interest in video analytics. In an exploratory video analytics pipeline, a data scientist often starts by searching for a global trend and then iteratively refines the query until they identify the desired local trend. These queries tend to have overlapping computation and often differ in their predicates. However, these predicates are computationally expensive to evaluate since they contain user-defined functions (UDFs) that wrap around deep learning models. In this paper, we present EVA, a video database management system (VDBMS) that automatically materializes and reuses the results of expensive UDFs to facilitate faster exploratory data analysis. It differs from the state-of-the-art (SOTA) reuse algorithms in traditional DBMSs in three ways. First, it focuses on reusing the results of UDFs as opposed to those of sub-plans. Second, it takes a symbolic approach to analyze predicates and identify the degree of overlap between queries. Third, it factors reuse into UDF evaluation cost and uses the updated cost function in critical query optimization decisions like predicate reordering and model selection. Our empirical analysis of EVA demonstrates that it accelerates exploratory video analytics workloads by 4x with a negligible storage overhead (1.001x). We demonstrate that the reuse algorithm in EVA complements the specialized filters adopted in SOTA VDBMSs.}
}


@inproceedings{DBLP:conf/sigmod/ButrovichL0RZ0P22,
	author = {Matthew Butrovich and
                  Wan Shen Lim and
                  Lin Ma and
                  John Rollinson and
                  William Zhang and
                  Yu Xia and
                  Andrew Pavlo},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Tastes Great! Less Filling! High Performance and Accurate Training
                  Data Collection for Self-Driving Database Management Systems},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {617--630},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517845},
	doi = {10.1145/3514221.3517845},
	timestamp = {Tue, 01 Apr 2025 19:09:24 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/ButrovichL0RZ0P22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A self-driving database management system (DBMS) aims to configure, deploy, and optimize almost all aspects of itself automatically without human intervention or guidance. Achieving this high level of automation relies on machine learning (ML) models that predict how a DBMS will behave in different scenarios. This behavior encompasses all DBMS runtime operations, including query execution and maintenance tasks. These ML-based behavior models for a self-driving DBMS require low-level training data about a DBMS's internals. Such training data includes (1) features that describe the workload, environment, and DBMS configuration, and (2) both DBMS- and hardware-level metrics. But it is difficult to collect training data from a DBMS while it is running because it can introduce performance and measurement degradations that hinder the ML models' ability to predict the DBMS's behavior correctly. We present the TScout (TS) framework for collecting training data from self-driving DBMSs. Our framework is an internal approach where developers annotate a DBMS's source code with hooks to monitor the system's behavior. TS then extracts these hooks and generates a kernel-level program (via Linux's BPF) that efficiently captures metrics from multiple sources (e.g., CPU performance counters, memory allocators). TS combines these metrics with internal DBMS state observations, generating training data for behavior models. We integrated TS in a PostgreSQL-compatible DBMS and measured its ability to collect training data for both OLTP and OLAP workloads. Our results show that TS generates training data for a deployed DBMS to train more accurate models than previous methods with only a 7% performance reduction.}
}


@inproceedings{DBLP:conf/sigmod/ZhangW0T0022,
	author = {Xinyi Zhang and
                  Hong Wu and
                  Yang Li and
                  Jian Tan and
                  Feifei Li and
                  Bin Cui},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Towards Dynamic and Safe Configuration Tuning for Cloud Databases},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {631--645},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526176},
	doi = {10.1145/3514221.3526176},
	timestamp = {Thu, 20 Mar 2025 20:54:44 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhangW0T0022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Configuration knobs of database systems are essential to achieve high throughput and low latency. Recently, automatic tuning systems using machine learning methods (ML) have shown to find better configurations compared to experienced database administrators (DBAs). However, there are still gaps to apply the existing systems in production environments, especially in the cloud. First, they conduct tuning for a given workload within a limited time window and ignore the dynamicity of workloads and data. Second, they rely on a copied instance and do not consider the availability of the database when sampling configurations, making the tuning expensive, delayed, and unsafe. To fill these gaps, we propose OnlineTune, which tunes the online databases safely in changing cloud environments. To accommodate the dynamicity, OnlineTune embeds the environmental factors as context feature and adopts contextual Bayesian Optimization with context space partition to optimize the database adaptively and scalably. To pursue safety during tuning, we leverage the black-box and the white-box knowledge to evaluate the safety of configurations and propose a safe exploration strategy via subspace adaptation. We conduct evaluations on dynamic workloads from benchmarks and real-world workloads. Compared with the state-of-the-art methods, OnlineTune achieves 14.4% ~165.3% improvement on cumulative performance while reducing 91.0%~99.5% unsafe configuration recommendations.}
}


@inproceedings{DBLP:conf/sigmod/CaiLZZZLLCYX22,
	author = {Baoqing Cai and
                  Yu Liu and
                  Ce Zhang and
                  Guangyu Zhang and
                  Ke Zhou and
                  Li Liu and
                  Chunhua Li and
                  Bin Cheng and
                  Jie Yang and
                  Jiashu Xing},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{HUNTER:} An Online Cloud Database Hybrid Tuning System for Personalized
                  Requirements},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {646--659},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517882},
	doi = {10.1145/3514221.3517882},
	timestamp = {Sun, 19 Jan 2025 13:27:26 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/CaiLZZZLLCYX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, using machine learning for performance tuning of cloud database (CDB) service has shown great potentials. However, facing personalized requirements such as various restrictions for tuning with very different workloads, pre-trained models may mismatch or recommend suboptimal configurations given a new workload. On the other hand, if the system tunes configurations in an online fashion, the system will suffer from the cold start problem, resulting in long tuning time and performance fluctuation. To accommodate these problems, we propose an online CDB tuning system called HUNTER. The key feature of HUNTER is a hybrid architecture, which uses samples generated by Genetic Algorithm to warm-start the finer grained exploration of deep reinforcement learning. Meanwhile, we employ Principal Component Analysis, Random Forest, and Fast Exploration Strategy to reduce the search space and the update time of the learning model. In addition, we further propose a clone and parallelization scheme to stress-test workloads on multiple cloned CDB instances (CDBs), resulting in faster and safer configuration exploration. Extensive trials on CDB with public and real-world workloads demonstrate that, given the same time budget and resources, HUNTER improves performance and considerably decreases recommendation time compared to state-of-the-art tuning systems, with accelerations of up to 2.8× and 22.8× utilizing 1 and 20 cloned CDBs, respectively.}
}


@inproceedings{DBLP:conf/sigmod/SiddiquiJ00NC22,
	author = {Tarique Siddiqui and
                  Saehan Jo and
                  Wentao Wu and
                  Chi Wang and
                  Vivek R. Narasayya and
                  Surajit Chaudhuri},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{ISUM:} Efficiently Compressing Large and Complex Workloads for Scalable
                  Index Tuning},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {660--673},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526152},
	doi = {10.1145/3514221.3526152},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/SiddiquiJ00NC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today's database systems include index advisors that recommend an appropriate set of indexes for an input workload. Since index tuning on large and complex workloads can be resource-intensive and time-consuming, workload compression techniques have been proposed to improve the scalability of index tuning. Workload compression techniques aim to efficiently identify a small subset of queries in the workload to tune such that the indexes recommended when tuning the compressed workload give similar performance improvements as when tuning the input workload. In this paper, we propose ISUM, a new workload compression algorithm that is based on two key ideas: a low-overhead technique for estimating the improvement in performance of the input workload when a subset of queries is selected for index tuning, and a novel method for concisely representing information across queries in the workload that improves scalability by avoiding pairwise comparisons between queries when choosing the set of queries to tune. Our evaluation over industry benchmarks and real-world customer workloads shows that ISUM results in a 1.4x of median and 2x of maximum performance improvements for the input workload when compared to prior techniques over similar compressed workload sizes.}
}


@inproceedings{DBLP:conf/sigmod/Xin0022,
	author = {Jinhan Xin and
                  Kai Hwang and
                  Zhibin Yu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{LOCAT:} Low-Overhead Online Configuration Auto-Tuning of Spark {SQL}
                  Applications},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {674--684},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526157},
	doi = {10.1145/3514221.3526157},
	timestamp = {Sun, 19 Jan 2025 13:27:21 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Xin0022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spark SQL has been widely deployed in industry but it is challenging to tune its performance. Recent studies try to employ machine learning (ML) to solve this problem, but suffer from two drawbacks. First, it takes a long time (high overhead) to collect training samples. Second, the optimal configuration for one input data size of the same application might not be optimal for others. To address these issues, we propose a novel Bayesian Optimization (BO) based approach named LOCAT to automatically tune the configurations of Spark SQL applications online. LOCAT innovates three techniques. The first technique, named QCSA, eliminates the configuration-insensitive queries by Query Configuration Sensitivity Analysis (QCSA) when collecting training samples. The second technique, dubbed DAGP, is a Datasize-Aware Gaussian Process (DAGP) which models the performance of an application as a distribution of functions of configuration parameters as well as input data size. The third technique, called IICP, Identifies Important Configuration Parameters (IICP) with respect to performance and only tunes the important ones. As such, LOCAT can tune the configurations of a Spark SQL application with low overhead and adapt to different input data sizes We employ Spark SQL applications from benchmark suites TPC-DS, TPC-H, and HiBench running on two significantly different clusters, a four-node ARM cluster and an eight-node x86 cluster, to evaluate LOCAT. The experimental results on the ARM cluster show that LOCAT accelerates the optimization procedures of the state-of-the-art approaches by at least 4.1× and up to 9.7×; moreover, LOCAT improves the application performance by at least 1.9× and up to 2.4×. On the x86 cluster, LOCAT shows similar results to those on the ARM cluster.}
}


@inproceedings{DBLP:conf/sigmod/0001BL22,
	author = {Tobias Ziegler and
                  Carsten Binnig and
                  Viktor Leis},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {ScaleStore: {A} Fast and Cost-Efficient Storage Engine using DRAM,
                  NVMe, and {RDMA}},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {685--699},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526187},
	doi = {10.1145/3514221.3526187},
	timestamp = {Sun, 19 Jan 2025 13:27:21 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0001BL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose ScaleStore, a novel distributed storage engine that exploits DRAM caching, NVMe storage, and RDMA networking to achieve high performance, cost-efficiency, and scalability at the same time. Using low latency RDMA messages, ScaleStore implements a transparent memory abstraction that provides access to the aggregated DRAM memory and NVMe storage of all nodes. In contrast to existing distributed RDMA designs such as NAM-DB or FaRM, ScaleStore stores cold data on NVMe SSDs (flash), lowering the overall hardware cost significantly. The core of ScaleStore is a distributed caching strategy that dynamically decides which data to keep in memory (and which on SSDs) based on the workload. The caching protocol also provides strong consistency in the presence of concurrent data modifications. Our evaluation shows that ScaleStore achieves high performance for various types of workloads (read/write-dominated, uniform/skewed) even when the data size is larger than the aggregated memory of all nodes. We further show that ScaleStore can efficiently handle dynamic workload changes and supports elasticity.}
}


@inproceedings{DBLP:conf/sigmod/0001LD22,
	author = {Michael Abebe and
                  Horatiu Lazu and
                  Khuzaima Daudjee},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Proteus: Autonomous Adaptive Storage for Mixed Workloads},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {700--714},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517834},
	doi = {10.1145/3514221.3517834},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0001LD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enterprises use distributed database systems to meet the demands of mixed or hybrid transaction/analytical processing (HTAP) workloads that contain both transactional (OLTP) and analytical (OLAP) requests. Distributed HTAP systems typically maintain a complete copy of data in row-oriented storage format that is well-suited for OLTP workloads and a second complete copy in column-oriented storage format optimized for OLAP workloads. Maintaining these data copies consumes significant storage space and system resources. Conversely, if a system stores data in a single format, OLTP or OLAP workload performance suffers. This paper presents Proteus, a distributed HTAP database system that adaptively and autonomously selects and changes its storage layout to optimize for mixed workloads. Proteus generates physical execution plans that utilize storage-aware operators for efficient transaction execution. Using comprehensive HTAP workloads and state-of-the-art comparison systems, we demonstrate that Proteus delivers superior HTAP performance while providing OLTP and OLAP performance on par with designs specialized for either type of workload.}
}


@inproceedings{DBLP:conf/sigmod/YangY022,
	author = {Linguan Yang and
                  Xinan Yan and
                  Bernard Wong},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Natto: Providing Distributed Transaction Prioritization for High-Contention
                  Workloads},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {715--729},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526161},
	doi = {10.1145/3514221.3526161},
	timestamp = {Sun, 19 Jan 2025 13:27:24 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/YangY022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces Natto, a geo-distributed database system that supports transaction prioritization. Instead of having each shard process transactions in their arrival order, Natto leverages network measurements to estimate the transaction arrival time at each shard, and assigns a timestamp to the transaction based on its arrival time to the furthest shard. These timestamps establish a global ordering of transactions, and introduces opportunities to selectively abort pending low-priority transactions that conflict with a high-priority transaction, or even preempt transactions that are already partially prepared. Our experiments on both Microsoft Azure and a local cluster show that Natto's tail latency for high-priority transactions are significantly lower than the tail latencies of Carousel and TAPIR, which are the current state-of-the-art in geo-distributed transaction processing systems.}
}


@inproceedings{DBLP:conf/sigmod/SunZSC22,
	author = {Yu Sun and
                  Zheng Zheng and
                  Shaoxu Song and
                  Fei Chiang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Confidence Bounded Replica Currency Estimation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {730--743},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517852},
	doi = {10.1145/3514221.3517852},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/SunZSC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Replicas of the same data item often exhibit varying consistency levels when executing read and write requests due to system availability and network limitations. When one or more replicas respond to a query, estimating the currency (or staleness) of the returned data item (without accessing the other replicas) is essential for applications requiring timely data. Depending on how confident the estimation is, the query may dynamically decide to return the retrieved replicas, or wait for the remaining replicas to respond. The replica currency estimation is expected to be accurate and extremely time efficient without introducing large overhead during query processing. In this paper, we provide theoretical bounds on the confidence of replica currency estimation. Our system computes with a minimum probability p, whether the retrieved replicas are current or stale. Using this confidence-bounded replica currency estimation, we implement a novel DYNAMIC read consistency level in the open-source, NoSQL database, Cassandra. Experiments show that the proposed replica currency estimation is intuitive and efficient. In most tested scenarios, with various query loads and cluster configurations, we show our estimations with confidence levels of at least 0.99 while keeping query latency low (close to reading ONE replica). Moreover, the overheads introduced due to estimation scoring and training are low, incurring only 0.76% to 1.17% of the query processing and replica synchronization time costs, respectively.}
}


@inproceedings{DBLP:conf/sigmod/Zhao0LZ00C22,
	author = {Yikai Zhao and
                  Yinda Zhang and
                  Yuanpeng Li and
                  Yi Zhou and
                  Chunhui Chen and
                  Tong Yang and
                  Bin Cui},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {MinMax Sampling: {A} Near-optimal Global Summary for Aggregation in
                  the Wide Area},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {744--758},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526160},
	doi = {10.1145/3514221.3526160},
	timestamp = {Thu, 27 Mar 2025 18:54:47 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Zhao0LZ00C22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, wide-area data analyses are pervasive with emerging geo-distributed systems. These analyses often need to do the global aggregation in the wide area. Since scarce and variable WAN bandwidth may degrade the aggregation performance, it is highly desired to design a communication scheme for global aggregation in WAN. Unfortunately, no existing algorithm can meet the three design requirements of communication schemes: fast computation, adaptive transmission, and accurate aggregation. In this paper, we propose MinMax Sampling, a fast, adaptive, and accurate communication scheme for global aggregation in WAN. We first focus on the accuracy and design a scheme, namely MinMaxopt, to achieve optimal accuracy. However, MinMaxopt does not meet the other two requirements: fast computation and adaptive transmission. Based on MinMaxopt, we propose MinMaxadp, which trades little accuracy for the other two requirements. We evaluate MinMaxadp with three applications: federated learning, distributed state aggregation, and hierarchical aggregation. Our experimental results show that MinMaxadp is superior to existing algorithms (8.44× better accuracy on average) in all three applications. The source codes of MinMax Sampling are available at Github [1].}
}


@inproceedings{DBLP:conf/sigmod/DongF0TM22,
	author = {Wei Dong and
                  Juanru Fang and
                  Ke Yi and
                  Yuchao Tao and
                  Ashwin Machanavajjhala},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{R2T:} Instance-optimal Truncation for Differentially Private Query
                  Evaluation with Foreign Keys},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {759--772},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517844},
	doi = {10.1145/3514221.3517844},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/DongF0TM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Answering SPJA queries under differential privacy (DP), including graph pattern counting under node-DP as an important special case, has received considerable attention in recent years. The dual challenge of foreign-key constraints and self-joins is particularly tricky to deal with, and no existing DP mechanisms can correctly handle both. For the special case of graph pattern counting under node-DP, the existing mechanisms are correct (i.e., satisfy DP), but they do not offer nontrivial utility guarantees or are very complicated and costly. In this paper, we propose the first DP mechanism for answering arbitrary SPJA queries in a database with foreign-key constraints. Meanwhile, it achieves a fairly strong notion of optimality, which can be considered as a small and natural relaxation of instance optimality. Finally, our mechanism is simple enough that it can be easily implemented on top of any RDBMS and an LP solver. Experimental results show that it offers order-of-magnitude improvements in terms of utility over existing techniques, even those specifically designed for graph pattern counting.}
}


@inproceedings{DBLP:conf/sigmod/Liew0TK0Y22,
	author = {Seng Pei Liew and
                  Tsubasa Takahashi and
                  Shun Takagi and
                  Fumiyuki Kato and
                  Yang Cao and
                  Masatoshi Yoshikawa},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Network Shuffling: Privacy Amplification via Random Walks},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {773--787},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526162},
	doi = {10.1145/3514221.3526162},
	timestamp = {Tue, 21 Mar 2023 20:57:32 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Liew0TK0Y22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, it is shown that shuffling can amplify the central differential privacy guarantees of data randomized with local differential privacy. Within this setup, a centralized, trusted shuffler is responsible for shuffling by keeping the identities of data anonymous, which subsequently leads to stronger privacy guarantees for systems. However, introducing a centralized entity to the originally local privacy model loses some appeals of not having any centralized entity as in local differential privacy. Moreover, implementing a shuffler in a reliable way is not trivial due to known security issues and/or requirements of advanced hardware or secure computation technology. Motivated by these practical considerations, we rethink the shuffle model to relax the assumption of requiring a centralized, trusted shuffler. We introduce network shuffling, a decentralized mechanism where users exchange data in a random-walk fashion on a network/graph, as an alternative of achieving privacy amplification via anonymity. We analyze the threat model under such a setting, and propose distributed protocols of network shuffling that is straightforward to implement in practice. Furthermore, we show that the privacy amplification rate is similar to other privacy amplification techniques such as uniform shuffling. To our best knowledge, among the recently studied intermediate trust models that leverage privacy amplification techniques, our work is the first that is not relying on any centralized entity to achieve privacy amplification.}
}


@inproceedings{DBLP:conf/sigmod/LiYLLLZ22,
	author = {Sainan Li and
                  Qilei Yin and
                  Guoliang Li and
                  Qi Li and
                  Zhuotao Liu and
                  Jinwei Zhu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Unsupervised Contextual Anomaly Detection for Database Systems},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {788--802},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517861},
	doi = {10.1145/3514221.3517861},
	timestamp = {Sun, 19 Jan 2025 13:27:17 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/LiYLLLZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Abnormal data access operations in database systems always hap-pen, which are typically incurred by misoperations or attacks, though these systems are enforced with strict access control policies. However, prior arts only focus on detecting abnormal data accesses by utilizing known attack patterns or identifying behaviors significantly deviated from normal behaviors. They cannot capture stealthy abnormal data access operations that are similar to normal ones. In this paper, we propose a novel unsupervised anomaly detection system UCAD, which aims to detect abnormal data access operations, by comparing operation's semantics with their contextual intent. However, it is non-trivial to obtain accurate semantics of operations for intent analysis because (i) the same operation may exhibit diverse semantics under different operation contexts and (ii) different operation sequences could have identical semantics due to heterogeneous user access patterns. To address this issue, we develop a new transformer model called Trans-DAS for UCAD. Trans-DAS learns the semantics of individual operations by utilizing the attention mechanism that analyzes the relevance between any pair of operations in sequence, and captures the contextual intent of operations inferred from the contexts. Specifically, Trans-DAS utilizes a particular embedding layer to embed the semantics of individual operations without the operation order information and a masking mechanism that allows Trans-DAS to learn the semantics according to the bidirectional contexts. Also, we define a new training objective for Trans-DAS to enlarge the difference among the embedded semantics. Furthermore, in order to effectively utilize Trans-DAS for detection, we develop two modules in UCAD, i.e., a data preprocessing module that allows Trans-DAS to accurately learn the normal semantic information by removing noisy data, and an anomaly detection module that learns the semantic information for intent comparison. We evaluate the performance of UCAD on real-world data traces under different settings (e.g., varied parameters and hybrid datasets). The results demonstrate that UCAD achieves the average F1-score of 0.94 in two scenarios, which significantly outperform baselines, and shows robustness to hybrid data and good transferability to different tasks.}
}


@inproceedings{DBLP:conf/sigmod/Chang0W022,
	author = {Zhao Chang and
                  Dong Xie and
                  Sheng Wang and
                  Feifei Li},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Towards Practical Oblivious Join},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {803--817},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517868},
	doi = {10.1145/3514221.3517868},
	timestamp = {Sat, 30 Sep 2023 09:56:33 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/Chang0W022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many individuals and companies choose the public cloud as their data and IT infrastructure platform. But remote accesses over the data inevitably bring the issue of trust. Despite strong encryption schemes, adversaries can still learn sensitive information from encrypted data by observing data access patterns. Oblivious RAMs (ORAMs) are proposed to protect against access pattern attacks. However, directly deploying ORAM constructions in an encrypted database brings large computational overhead. In this work, we focus on oblivious joins over a cloud database. Existing studies in the literature are restricted to either primary-foreign key joins or binary equi-joins. Our major contribution is to support general binary and multiway equi-joins. We integrate B-tree indices into ORAMs for each input table and retrieve blocks through the indices in join processing. The key points are to address the security issue (i.e., leaking the number of accesses to any index) in the extended existing solutions and bound the total number of block accesses. Our index nested-loop join algorithm can also support some types of band joins obliviously. The effectiveness and efficiency of our algorithms are demonstrated through extensive evaluations over real-world datasets. Our method shows orders of magnitude speedup for oblivious multiway equi-joins in comparison with baseline algorithms.}
}


@inproceedings{DBLP:conf/sigmod/WangBNM22,
	author = {Chenghong Wang and
                  Johes Bater and
                  Kartik Nayak and
                  Ashwin Machanavajjhala},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {IncShrink: Architecting Efficient Outsourced Databases using Incremental
                  {MPC} and Differential Privacy},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {818--832},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526151},
	doi = {10.1145/3514221.3526151},
	timestamp = {Sun, 19 Jan 2025 13:27:22 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/WangBNM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider secure outsourced growing databases (SOGDB) that support view-based query answering. These databases allow untrusted servers to privately maintain a materialized view. This allows servers to use only the materialized view for query processing instead of accessing the original data from which the view was derived. To tackle this, we devise a novel view-based SOGDB framework, Incshrink. The key features of this solution are: (i) Incshrink maintains the view using incremental MPC operators which eliminates the need for a trusted third party upfront, and (ii) to ensure high performance, Incshrink guarantees that the leakage satisfies DP in the presence of updates. To the best of our knowledge, there are no existing systems that have these properties. We demonstrate Incshrink's practical feasibility in terms of efficiency and accuracy with extensive experiments on real-world datasets and the TPC-ds benchmark. The evaluation results show that Incshrink provides a 3-way trade-off in terms of privacy, accuracy and efficiency, and offers at least a 7,800x performance advantage over standard SOGDB that do not support view-based query paradigm.}
}


@inproceedings{DBLP:conf/sigmod/MaFCLH22,
	author = {Chenhao Ma and
                  Yixiang Fang and
                  Reynold Cheng and
                  Laks V. S. Lakshmanan and
                  Xiaolin Han},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {A Convex-Programming Approach for Efficient Directed Densest Subgraph
                  Discovery},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {845--859},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517837},
	doi = {10.1145/3514221.3517837},
	timestamp = {Tue, 11 Feb 2025 16:38:19 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/MaFCLH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a directed graph G, the directed densest subgraph (DDS) problem refers to finding a subgraph from G, whose density is the highest among all subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fake follower detection and community mining. Theoretically, the DDS problem closely connects to other essential graph problems, such as network flow and bipartite matching. However, existing DDS solutions suffer from efficiency and scalability issues. In this paper, we develop a convex-programming-based solution by transforming the DDS problem into a set of linear programs. Based on the duality of linear programs, we develop efficient exact and approximation algorithms. Especially, our approximation algorithm can support flexible parameterized approximation guarantees. We have performed an extensive empirical evaluation of our approaches on eight real large datasets. The results show that our proposed algorithms are up to five orders of magnitude faster than the state-of-the-art.}
}


@inproceedings{DBLP:conf/sigmod/YuLL022,
	author = {Kaiqiang Yu and
                  Cheng Long and
                  Shengxin Liu and
                  Da Yan},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Efficient Algorithms for Maximal k-Biplex Enumeration},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {860--873},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517847},
	doi = {10.1145/3514221.3517847},
	timestamp = {Tue, 22 Oct 2024 20:38:20 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/YuLL022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mining maximal subgraphs with cohesive structures from a bipartite graph has been widely studied. One important cohesive structure on bipartite graphs is k-biplex, where each vertex on one side disconnects at most k vertices on the other side. In this paper, we study the maximal k-biplex enumeration problem which enumerates all maximal k-biplexes. Existing methods suffer from efficiency and/or scalability issues and have the time of waiting for the next output exponential w.r.t. the size of the input bipartite graph (i.e., an exponential delay). In this paper, we adopt a reverse search framework called bTraversal, which corresponds to a depth-first search (DFS) procedure on an implicit solution graph on top of all maximal k-biplexes. We then develop a series of techniques for improving and implementing this framework including (1) carefully selecting an initial solution to start DFS, (2) pruning the vast majority of links from the solution graph of bTraversal, and (3) implementing abstract procedures of the framework. The resulting algorithm is called iTraversal, which has its underlying solution graph significantly sparser than (around 0.1% of) that of bTraversal. Besides, iTraversal provides a guarantee of polynomial delay. Our experimental results on real and synthetic graphs, where the largest one contains one billion edges, show that our algorithm is up to four orders of magnitude faster than existing algorithms.}
}


@inproceedings{DBLP:conf/sigmod/SunM022,
	author = {Yahui Sun and
                  Shuai Ma and
                  Bin Cui},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Hunting Temporal Bumps in Graphs with Dynamic Vertex Properties},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {874--888},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517859},
	doi = {10.1145/3514221.3517859},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/SunM022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a time interval and a graph where vertices exhibit a property of interest (PoI) dynamically, an interesting question is: where (i.e., which part of the graph) and when (i.e., which time sub-interval) does the PoI occur frequently? To our knowledge, no work has been done to answer this question to date. We address this issue in this paper. Specifically, given (i) a time interval composed of multiple time slots and (ii) a graph where each vertex either exhibits or does not exhibit the PoI in each time slot, our objective is to find a pair of a connected sub-graph and a time sub-interval (which we refer to as a temporal bump), such that the discrepancy between the numbers of times that vertices in this sub-graph exhibit and do not exhibit the PoI during this time sub-interval is maximized. Due to the NP-hardness of this problem, initially, we propose two approximation algorithms. The first one achieves a tight approximation guarantee, at the cost of a weak scalability to the number of time slots. The second one achieves a strong scalability to the number of time slots, at the price of a loose approximation guarantee. Then, we propose two heuristic algorithms that have no non-trivial approximation guarantee, but produce similar solutions with, and are considerably faster than, the two approximation algorithms. Experiments on real datasets show that, in comparison with baselines built using related existing techniques, our algorithms hunt bumps with significantly higher discrepancies, while scaling well to large graphs, and thus are more suitable for answering the aforementioned question.}
}


@inproceedings{DBLP:conf/sigmod/KimLCY22,
	author = {Junghoon Kim and
                  Siqiang Luo and
                  Gao Cong and
                  Wenyuan Yu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{DMCS} : Density Modularity based Community Search},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {889--903},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526137},
	doi = {10.1145/3514221.3526137},
	timestamp = {Sun, 06 Oct 2024 21:14:19 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/KimLCY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community Search, or finding a connected subgraph (known as a community) containing the given query nodes in a social network, is a fundamental problem. Most of the existing community search models only focus on the internal cohesiveness of a community. However, a high-quality community often has high modularity, which means dense connections inside communities and sparse connections to the nodes outside the community. In this paper, we conduct a pioneer study on searching a community with high modularity. We point out that while modularity has been popularly used in community detection (without query nodes), it has not been adopted for community search, surprisingly, and its application in community search (related to query nodes) brings in new challenges. We address these challenges by designing a new graph modularity function named Density Modularity. To the best of our knowledge, this is the first work on the community search problem using graph modularity. The community search based on the density modularity, termed as DMCS, is to find a community in a social network that contains all the query nodes and has high density-modularity. We prove that the DMCS problem is NP-hard. To efficiently address DMCS, we present new algorithms that run in log-linear time to the graph size. We conduct extensive experimental studies in real-world and synthetic networks, which offer insights into the efficiency and effectiveness of our algorithms. In particular, our algorithm achieves up to 8.5 times higher accuracy in terms of NMI than baseline algorithms.}
}


@inproceedings{DBLP:conf/sigmod/0001Q0CZ022,
	author = {Wentao Li and
                  Miao Qiao and
                  Lu Qin and
                  Lijun Chang and
                  Ying Zhang and
                  Xuemin Lin},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {On Scalable Computation of Graph Eccentricities},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {904--916},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517874},
	doi = {10.1145/3514221.3517874},
	timestamp = {Sun, 19 Jan 2025 13:27:34 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0001Q0CZ022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a graph, eccentricity measures the distance from each node to its farthest node. Eccentricity indicates the centrality of each node and collectively encodes fundamental graph properties: the radius and the diameter --- the minimum and maximum eccentricity, respectively, over all the nodes in the graph. Computing the eccentricities for all the graph nodes, however, is challenging in theory: any approach shall either complete in quadratic time or introduce a 1/3 relative error under certain hypotheses. In practice, the state-of-the-art approach PLLECC in computing exact eccentricities relies heavily on a precomputed all-pair-shortest-distance index whose expensive construction refrains PLLECC from scaling up. This paper provides insights to enable scalable exact eccentricity computation that does not rely on any index. The proposed algorithm IFECC handles billion-scale graphs that no existing approach can process and achieves up to two orders of magnitude speedup over PLLECC. As a by-product, IFECC can be terminated at any time during execution to produce approximate eccentricities, which is empirically more stable and reliable than KBFS, the state-of-the-art algorithm for approximately computing eccentricities.}
}


@inproceedings{DBLP:conf/sigmod/LiuS022,
	author = {Qiyu Liu and
                  Yanyan Shen and
                  Lei Chen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{HAP:} An Efficient Hamming Space Index Based on Augmented Pigeonhole
                  Principle},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {917--930},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517880},
	doi = {10.1145/3514221.3517880},
	timestamp = {Sun, 04 Aug 2024 19:37:28 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/LiuS022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging deep learning techniques prefer mapping complex data objects (e.g., images, documents) to compact binary vectors (i.e., hash codes) for efficient similarity search. In this paper, we study the problem of indexing large-scale binary databases to support fast Hamming distance-based similarity queries. Existing Hamming space indices usually divide long binary vectors into short disjoint pieces and apply the Pigeonhole Principle to prune unnecessary candidates. In our work, we relax the disjoint partition constraint by allowing dimension redundancy, which yields a tighter pruning bound named Augmented Pigeonhole Principle (APP). Intuitively, APP enables more optimization opportunities by capturing the correlation between database and query workloads. Based on APP, we propose HAP, an efficient Hamming space index framework to support both Hamming range queries and k-NN queries. To guide index construction and run-time query optimization, we introduce a novel DL-base query cardinality estimator named SimCardNet. To further reduce the index space cost, we propose a learned index compression scheme by combining the piece-wise linear approximation (PLA) and Elias-Fano encoding. In addition, we also study the problem of optimizing the execution time of a batch of queries using our index framework. The experimental results on large-scale binary databases reveal that our indexing scheme outperforms the state-of-the-art baselines in terms of both space and time efficiency.}
}


@inproceedings{DBLP:conf/sigmod/YangC0MLS22,
	author = {Zongheng Yang and
                  Wei{-}Lin Chiang and
                  Sifei Luan and
                  Gautam Mittal and
                  Michael Luo and
                  Ion Stoica},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Balsa: Learning a Query Optimizer Without Expert Demonstrations},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {931--944},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517885},
	doi = {10.1145/3514221.3517885},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/YangC0MLS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query optimizers are a performance-critical component in every database system. Due to their complexity, optimizers take experts months to write and years to refine. In this work, we demonstrate for the first time that learning to optimize queries without learning from an expert optimizer is both possible and efficient. We present Balsa, a query optimizer built by deep reinforcement learning. Balsa first learns basic knowledge from a simple, environment-agnostic simulator, followed by safe learning in real execution. On the Join Order Benchmark, Balsa matches the performance of two expert query optimizers, both open-source and commercial, with two hours of learning, and outperforms them by up to 2.8× in workload runtime after a few more hours. Balsa thus opens the possibility of automatically learning to optimize in future compute environments where expert-designed optimizers do not exist.}
}


@inproceedings{DBLP:conf/sigmod/ZhangCZ022,
	author = {Lixi Zhang and
                  Chengliang Chai and
                  Xuanhe Zhou and
                  Guoliang Li},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {LearnedSQLGen: Constraint-aware {SQL} Generation using Reinforcement
                  Learning},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {945--958},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526155},
	doi = {10.1145/3514221.3526155},
	timestamp = {Sun, 19 Jan 2025 13:27:18 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhangCZ022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many database optimization problems, e.g., slow SQL diagnosis, database testing, optimizer tuning, require a large volume of SQL queries. Due to privacy issues, it is hard to obtain real SQL queries, and thus SQL generation is a very important task in database optimization. Existing SQL generation methods either randomly generate SQL queries or rely on human-crafted SQL templates to generate SQL queries, but they cannot meet various user specific requirements, e.g., slow SQL queries, SQL queries with large result sizes. To address this problem, this paper studies the problem of constraint-aware SQL generation, which, given a constraint (e.g., cardinality within [1k,2k]), generates SQL queries satisfying the constraint. This problem is rather challenging, because it is rather hard to capture the relationship from query constraint (e.g., cardinality and cost) to SQL queries and thus it is hard to guide a generation method to explore the SQL generation direction towards meeting the constraint. To address this challenge, we propose a reinforcement learning (RL) based framework LearnedSQLGen, for generating queries satisfying the constraint. LearnedSQLGen adopts an exploration-exploitation strategy that exploits the generation direction following the query constraint, which is learned from query execution feedback. We judiciously design the reward function in RL to guide the generation process accurately. We integrate a finite-state machine to generate valid SQL queries. Experimental results on three benchmarks showed that LearnedSQLGen significantly outperformed the baselines in terms of both accuracy (30% better) and efficiency (10-35 times).}
}


@inproceedings{DBLP:conf/sigmod/HuLXAPRY22,
	author = {Xiao Hu and
                  Yuxi Liu and
                  Haibo Xiu and
                  Pankaj K. Agarwal and
                  Debmalya Panigrahi and
                  Sudeepa Roy and
                  Jun Yang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Selectivity Functions of Range Queries are Learnable},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {959--972},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517896},
	doi = {10.1145/3514221.3517896},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/HuLXAPRY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper explores the use of machine learning for estimating the selectivity of range queries in database systems. Using classic learning theory for real-valued functions based on shattering dimension, we show that the selectivity function of a range space with bounded VC-dimension is learnable. As many popular classes of queries (e.g., orthogonal range search, inequalities involving linear combination of attributes, distance-based search, etc.) represent range spaces with finite VC-dimension, our result immediately implies that their selectivity functions are also learnable. To the best of our knowledge, this is the first attempt at formally explaining the role of machine learning techniques in selectivity estimation, and complements the growing literature in empirical studies in this direction. Supplementing these theoretical results, our experimental results demonstrate that, empirically, even a basic learning algorithm with generic models is able to produce accurate predictions across settings, matching state-of-art methods designed for specific queries, and using training sample sizes commensurate with our theory.}
}


@inproceedings{DBLP:conf/sigmod/ZhaoYHLZ22,
	author = {Kangfei Zhao and
                  Jeffrey Xu Yu and
                  Zongyan He and
                  Rui Li and
                  Hao Zhang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Lightweight and Accurate Cardinality Estimation by Neural Network
                  Gaussian Process},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {973--987},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526156},
	doi = {10.1145/3514221.3526156},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhaoYHLZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep Learning (DL) has achieved great success in many real applications. Despite its success, there are some main problems when deploying advanced DL models in database systems, such as hyper-parameters tuning, the risk of overfitting, and lack of prediction uncertainty. In this paper, we study a lightweight and accurate cardinality estimation for SQL queries, which is also uncertainty-aware. By lightweight, we mean that we can train a DL model in a few seconds. With uncertainty ensured,it becomes possible to update the estimator to improve its prediction in areas with high uncertainty.The approach we explore is different from the direction of deploying sophisticated DL models as cardinality estimators in database systems. We employ Bayesian deep learning (BDL), which serves as a bridge between Bayesian inference and deep learning. The prediction distribution by BDL provides principled uncertainty calibration for the prediction. In addition, when the network width of a BDL model goes to infinity, the model performs equivalent to Gaussian Process (GP). This special class of BDL, known as Neural Network Gaussian Process (NNGP), inherits the advantages of Bayesian approach while keeping universal approximation of neural networks, and can utilize a much larger model space to model distribution-free data as a nonparametric model. We show our NNGP estimator achieves high accuracy, is built fast, and is robust to query workload shift, in our extensive performance studies by comparing with existing learned estimators. We also confirm the effectiveness of NNGP by integrating it into PostgreSQL.}
}


@inproceedings{DBLP:conf/sigmod/LeeLRPJPSC22,
	author = {Sangjin Lee and
                  Alberto Lerner and
                  Andr{\'{e}} Ryser and
                  Kibin Park and
                  Chanyoung Jeon and
                  Jinsub Park and
                  Yong Ho Song and
                  Philippe Cudr{\'{e}}{-}Mauroux},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{X-SSD:} {A} Storage System with Native Support for Database Logging
                  and Replication},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {988--1002},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526188},
	doi = {10.1145/3514221.3526188},
	timestamp = {Sun, 19 Jan 2025 13:27:20 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/LeeLRPJPSC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transaction logging and log shipping are standard techniques to provide recoverability and high availability in data management systems. They entail an update to a local log file at every transaction and sending such an update to a remote site in a coordinated fashion. Modern databases have leveraged technologies such as Persistent Memory (PM) and RDMA-enabled networking to perform these updates as fast as possible. This mix of technologies, however, presents several drawbacks: some technologies are not portable, restricting deployments to a single class of machines; they make the data path more contrived; and they force very low-level APIs to interoperate, posing severe correctness issues. In this paper, we introduce the X-SSD, an SSD architecture that mixes NAND Flash and PM and that offers similar top performance while avoiding these issues. X-SSD devices can take transaction log writes in a fast, PM-backed data path and offer Data Propagation Services that move these writes to remote sites and eventually to NAND Flash storage. To validate this new architecture, we design and implement an actual reference X-SSD device called Villars. Our experiments show that the Villars device can manage PM and remote transfers on behalf of the database in a faster, more straightforward, and more robust way. We believe that our new architecture and reference implementation can reignite the conversation between the database and storage communities about the potential of co-designed devices.}
}


@inproceedings{DBLP:conf/sigmod/BoeschenB22,
	author = {Nils Boeschen and
                  Carsten Binnig},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {GaccO - {A} GPU-accelerated {OLTP} {DBMS}},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1003--1016},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517876},
	doi = {10.1145/3514221.3517876},
	timestamp = {Sun, 04 Aug 2024 19:37:27 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/BoeschenB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present GaccO - a main memory DBMS for GPU-accelerated OLTP. For executing OLTP workloads, GaccO implements a novel scheme that splits the execution across the CPU and the GPU. Using such a co-execution scheme GaccO can thus not only efficiently make use of the vectorized execution of the GPU by grouping transactions of the same type into batches, but it can also support databases larger than device memory by leveraging CPU memory in addition to the GPU memory. In our evaluation with TPC-C, we show that GaccO can thus speed-up OLTP workloads by up to 6 times compared to a pure CPU-based OLTP execution.}
}


@inproceedings{DBLP:conf/sigmod/LutzBZRM22,
	author = {Clemens Lutz and
                  Sebastian Bre{\ss} and
                  Steffen Zeuch and
                  Tilmann Rabl and
                  Volker Markl},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Triton Join: Efficiently Scaling to a Large Join State on GPUs with
                  Fast Interconnects},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1017--1032},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517911},
	doi = {10.1145/3514221.3517911},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/LutzBZRM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database management systems are facing growing data volumes. Previous research suggests that GPUs are well-equipped to quickly process joins and similar stateful operators, as GPUs feature high-bandwidth on-board memory. However, GPUs cannot scale joins to large data volumes due to two limiting factors: (1)~large state does not fit into the on-board memory, and (2)~spilling state to main memory is constrained by the interconnect bandwidth. Thus, CPUs are often the better choice for scalable data processing. In this paper, we propose a new join algorithm that scales to large data volumes by taking advantage of fast interconnects. Fast interconnects such as NVLink~2.0 are a new technology that connect the GPU to main memory at a high bandwidth, and thus enable us to design our join to efficiently spill its state. Our evaluation shows that our Triton join outperforms a no-partitioning hash join by more than 100× on the same GPU, and a radix-partitioned join on the CPU by up to 2.5×. As a result, GPU-enabled DBMSs are able to scale beyond the GPU memory capacity.}
}


@inproceedings{DBLP:conf/sigmod/WangLS22,
	author = {Qing Wang and
                  Youyou Lu and
                  Jiwu Shu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Sherman: {A} Write-Optimized Distributed B+Tree Index on Disaggregated
                  Memory},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1033--1048},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517824},
	doi = {10.1145/3514221.3517824},
	timestamp = {Sun, 19 Jan 2025 13:27:26 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/WangLS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Memory disaggregation architecture physically separates CPU and memory into independent components, which are connected via high-speed RDMA networks, greatly improving resource utilization of databases. However, such an architecture poses unique challenges to data indexing due to limited RDMA semantics and near-zero computation power at memory-side. Existing indexes supporting disaggregated memory either suffer from low write performance, or require hardware modification. This paper presents Sherman, a write-optimized distributed B+Tree index on disaggregated memory that delivers high performance with commodity RDMA NICs. Sherman combines RDMA hardware features and RDMA-friendly software techniques to boost index write performance from three angles. First, to reduce round trips, Sherman coalesces dependent RDMA commands by leveraging in-order delivery property of RDMA. Second, to accelerate concurrent accesses, Sherman introduces a hierarchical lock that exploits on-chip memory of RDMA NICs. Finally, to mitigate write amplification, Sherman tailors the data structure layout of B+Tree with a two-level version mechanism. Our evaluation shows that, Sherman is one order of magnitude faster in terms of both throughput and 99th percentile latency on typical write-intensive workloads, compared with state-of-the-art designs.}
}


@inproceedings{DBLP:conf/sigmod/HuCC0022,
	author = {Daokun Hu and
                  Zhiwen Chen and
                  Wenkui Che and
                  Jianhua Sun and
                  Hao Chen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Halo: {A} Hybrid PMem-DRAM Persistent Hash Index with Fast Recovery},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1049--1063},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517884},
	doi = {10.1145/3514221.3517884},
	timestamp = {Sun, 06 Oct 2024 21:14:19 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/HuCC0022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hash index, a fundamental component in many data management systems, can benefit from the emerging persistent memory (PMem) to achieve high performance and instant recovery. However, existing persistent hash indexes are suboptimal in at least three aspects. First, their performance suffers from the mismatch between small random write and access granularity of PMem hardware. Second, none of them are aware of the significance of write amplification caused by memory allocators and synchronization primitives. Third, hybrid designs (PMem+DRAM) focus on improving throughput at the cost of extremely long recovery time. In this paper, we present the design and implementation of Halo, a hybrid hash index for PMem+DRAM environment, featuring a specifically designed volatile index and log-structured persistent storage layout. In order to suppress write amplification caused by memory allocators and to facilitate recovery, we propose Halloc, a highly-efficient memory manager for Halo. In addition, we propose mechanisms such as batched writes, prefetching for hybrid reads, and reactive snapshot to further optimize performance. We conduct extensive evaluations on a 32-core platform equipped with Intel Optane DC Persistent Memory Modules. The results show that Halo achieves up to 17.5x and 81.2x higher read and write throughput than state-of-the-art hash indexes under a wide range of workloads. Halo also outperforms current hybrid designs in recovery speed, which is 1 to 2 orders of magnitude faster.}
}


@inproceedings{DBLP:conf/sigmod/RenSYYZX22,
	author = {Xuebin Ren and
                  Liang Shi and
                  Weiren Yu and
                  Shusen Yang and
                  Cong Zhao and
                  Zongben Xu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{LDP-IDS:} Local Differential Privacy for Infinite Data Streams},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1064--1077},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526190},
	doi = {10.1145/3514221.3526190},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/RenSYYZX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Local differential privacy (LDP) is promising for private streaming data collection and analysis. However, existing few LDP studies over streams either apply to finite streams only or may suffer from insufficient protection. This paper investigates this problem by proposing LDP-IDS, a novel w-event LDP paradigm to provide practical privacy guarantee for infinite streams. By constructing a unified error analysis, we adapt the existing budget division framework in centralized differential privacy (CDP) for LDP-IDS, which however incurs prohibitive noise and expensive communication cost. To this end, we propose a novel and extensible framework of population division and recycling, as well as online adaptive population division algorithms for LDP-IDS. We provide theoretical guarantees and demonstrate, through extensive discussions, that our proposed framework not only achieves significant reduction in utility loss and communication overhead, but also enjoys great compatibility for varied analytic tasks and flexibility of incorporating ideas of many existing stream algorithms. Extensive experiments on synthetic and real-world datasets validate the high effectiveness, efficiency, and flexibility of our proposed framework and methods.}
}


@inproceedings{DBLP:conf/sigmod/MonteZRM22,
	author = {Bonaventura Del Monte and
                  Steffen Zeuch and
                  Tilmann Rabl and
                  Volker Markl},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Rethinking Stateful Stream Processing with {RDMA}},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1078--1092},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517826},
	doi = {10.1145/3514221.3517826},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/MonteZRM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Remote Direct Memory Access (RDMA) hardware has bridged the gap between network and main memory speed and thus invalidated the common assumption that network is often the bottleneck in distributed data processing systems. However, high-speed networks do not provide "plug-and-play" performance (e.g., using IP-over- InfiniBand) and require a careful co-design of system and application logic. As a result, system designers need to rethink the architecture of their data management systems to benefit from RDMA acceleration. In this paper, we focus on the acceleration of stream processing engines, which is challenged by real-time constraints and state consistency guarantees. To this end, we propose Slash, a novel stream processing engine that uses high-speed networks and RDMA to efficiently execute distributed streaming computations. Slash embraces a processing model suited for RDMA acceleration and scales out by omitting the expensive data re-partitioning demands of scale-out SPEs. While scale-out SPEs rely on data re-partitioning to execute a query over many nodes, Slash uses RDMA to share mutable state among nodes. Overall, Slash achieves a throughput improvement up to two orders of magnitude over existing systems deployed on an InfiniBand network. Furthermore, it is up to a factor of 22 faster than a self-developed solution that relies on RDMA-based data re-partitioning to scale out query processing.}
}


@inproceedings{DBLP:conf/sigmod/YankovitchKS22,
	author = {Maor Yankovitch and
                  Ilya Kolchinsky and
                  Assaf Schuster},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{HYPERSONIC:} {A} Hybrid Parallelization Approach for Scalable Complex
                  Event Processing},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1093--1107},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517829},
	doi = {10.1145/3514221.3517829},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/YankovitchKS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ability to promptly and efficiently detect arbitrarily complex patterns in massive real-time data streams is a crucial requirement in many modern applications. The ever-growing scale of these applications and the sophistication of the patterns involved make it imperative to employ advanced solutions that can optimize pattern detection. One of the most prominent and well-established ways to achieve the above goal is to apply complex event processing (CEP) in a parallel manner, using a multi-core machine and/or a distributed environment. However, the inherent tightly coupled nature of CEP severely limits the scalability of the parallelization methods currently available. In this paper, we introduce a novel parallelization mechanism for efficient complex event processing over data streams. This mechanism is based on a hybrid two-tier model combining multiple layers of parallelism. By employing a fine-grained load balancing model, this multi-layered approach leads to a substantial increase in event detection throughput, while at the same time reducing the latency and the memory consumption. An extensive experimental evaluation on multiple real-life datasets shows that our approach consistently outperforms state-of-the-art CEP parallelization methods by a factor of two to three orders of magnitude.}
}


@inproceedings{DBLP:conf/sigmod/ZhangGBKCZ22,
	author = {Zhuo Zhang and
                  Junhao Gan and
                  Zhifeng Bao and
                  Seyed Mohammad Hussein Kazemi and
                  Guangyong Chen and
                  Fengyuan Zhu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Approximate Range Thresholding},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1108--1121},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526123},
	doi = {10.1145/3514221.3526123},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhangGBKCZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the (approximate) Range Thresholding (RT) problem over streams. Each stream element is a d-dimensional point and with a positive integer weight. An RT query q specifies a d-dimensional axis-parallel rectangular range R(q) and a positive integer threshold τ(q). Once the query q is registered in the system, define s(q) as the total weight of the elements that satisfy: (i) they arrive after q's registration, and (ii) they fall in the range R(q). Given a real number 0 < ε < 1, the task of the system is to capture an arbitrary moment during the period between the first moment when s(q) ≥ (1 - ε)⋅ τ(q) and the first moment when s(q) ≥ τ(q). The challenge is to support a large number of RT queries simultaneously while achieving sub-quadratic overall running time and near-linear space consumption all the time. We propose a new algorithm called FastRTS, which can reduce the exponent in the poly-logarithmic factor of the state-of-the-art QGT algorithm from d+1 to d, yet slightly increasing the łog term itself. Besides, we propose two extremely effective optimization techniques which significantly improve the practical performance of FastRTS. Experimental results show that FastRTS outperforms the competitors by up to three orders of magnitude in both running time and peak memory usage.}
}


@inproceedings{DBLP:conf/sigmod/MaLPR22,
	author = {Lei Ma and
                  Chuan Lei and
                  Olga Poppe and
                  Elke A. Rundensteiner},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Gloria: Graph-based Sharing Optimizer for Event Trend Aggregation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1122--1135},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526145},
	doi = {10.1145/3514221.3526145},
	timestamp = {Mon, 05 Feb 2024 20:26:56 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/MaLPR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large workloads of event trend aggregation queries are widely deployed to derive high-level insights about current event trends in near real time. To speed-up the execution, we identify and leverage sharing opportunities from complex patterns with flat Kleene operators or even nested Kleene expressions. We propose Gloria, a graph-based sharing optimizer for event trend aggregation. First, we map the sharing optimization problem to a graph path search problem in the Gloria graph with execution costs encoded as weights. Second, we shrink the search space by applying cost-driven pruning principles that guarantee optimality of the reduced Gloria graph in most cases. Lastly, we propose a path search algorithm that identifies the sharing plan with minimum execution costs. Our experimental study on three real-world data sets demonstrates that our Gloria optimizer effectively reduces the search space, leading to 5-fold speed-up in optimization time. The optimized plan consistently reduces the query latency by 68%-93% compared to the plan generated by state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/sigmod/CiaperoniGKK22,
	author = {Martino Ciaperoni and
                  Aristides Gionis and
                  Athanasios Katsamanis and
                  Panagiotis Karras},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{SIEVE:} {A} Space-Efficient Algorithm for Viterbi Decoding},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1136--1145},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526170},
	doi = {10.1145/3514221.3526170},
	timestamp = {Sun, 19 Jan 2025 13:27:29 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/CiaperoniGKK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Can we get speech recognition tools to work on limited-memory devices? The Viterbi algorithm is a classic dynamic programming (DP) solution used to find the most likely sequence of hidden states in a Hidden Markov Model (HMM). While the algorithm finds universal application ranging from communication systems to speech recognition to bioinformatics, its scalability has been scarcely addressed, stranding it to a space complexity that grows with the number of observations. In this paper, we propose SIEVE (Space Efficient Viterbi), a reformulation of the Viterbi algorithm that eliminates its space-complexity dependence on the number of observations to be explained. SIEVE discards and recomputes parts of the DP solution for the sake of space efficiency, in divide-and-conquer fashion, without incurring a time-complexity overhead. Our thorough experimental evaluation shows that SIEVE is highly effective in reducing the memory usage compared to the classic Viterbi algorithm, while avoiding the runtime overhead of a naïve space-efficient solution.}
}


@inproceedings{DBLP:conf/sigmod/WangZD22,
	author = {Zhizhi Wang and
                  Chaoji Zuo and
                  Dong Deng},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {TxtAlign: Efficient Near-Duplicate Text Alignment Search via Bottom-k
                  Sketches for Plagiarism Detection},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1146--1159},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526178},
	doi = {10.1145/3514221.3526178},
	timestamp = {Sun, 04 Aug 2024 19:37:27 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/WangZD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the near-duplicate text alignment search problem, which, given a collection of source (data) documents and a suspicious (query) document, finds all the near-duplicate passage pairs between the suspicious document and every source document. It finds applications in plagiarism detection. Specifically, the first two steps in plagiarism detection are source retrieval and text alignment. Source retrieval finds candidate source documents in a corpus that share content with the suspicious document while text alignment finds all the similar passage pairs between the suspicious document and every candidate source document. This problem is computation-intensive, especially for long documents. This is because there are O(n2m2) passage pairs between a single source document with n words and a suspicious document with m words, not to mention the large number of source documents in a corpus. Due to the high computation cost, existing solutions primarily rely on heuristic rules, such as the "seeding-extension-filtering" pipeline, and involve many hard-to-tune hyper-parameters. To address these issues, a recent work ALLIGN leverages the min-wise hash sketch for the text alignment problem. However, ALLIGN only works for two documents and leaves the source retrieval problem unattended. In this paper, we propose to leverage the bottom-k sketch (a.k.a. conditional random sampling) to estimate the similarity of two passages. We observe that many nearby passages in a document would share the same bottom-k sketch. Thus we propose to group all the passages in a document by their sketches. We prove that all the O(n2) passages can be partitioned into O(nk) groups in a document with n words and develop an algorithm to generate these groups in O(nlogn+nk) time. Then, to address the source retrieval problem, we only need to find groups of passages with "similar" bottom-k sketches. Every passage pair in two groups with "similar" sketches are near-duplicates. Experimental results on real-world datasets show that our techniques are highly efficient.}
}


@inproceedings{DBLP:conf/sigmod/GershteinMNR22,
	author = {Shay Gershtein and
                  Tova Milo and
                  Slava Novgorodov and
                  Kathy Razmadze},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Classifier Construction Under Budget Constraints},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1160--1174},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517863},
	doi = {10.1145/3514221.3517863},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/GershteinMNR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Search mechanisms over large assortments of items are central to the operation of many platforms. As users commonly express filtering conditions based on item properties that are not initially stored, companies must derive the missing information by training and applying binary classifiers. Choosing which classifiers to construct is however not trivial, since classifiers differ in construction costs and range of applicability. Previous work has considered the problem of selecting a classifier set of minimum construction cost, but this has been done under the (often unrealistic) assumption that the available budget is unlimited and allows to support all search queries. In practice, budget constraints require prioritizing some queries over others. To capture this consideration, we study in this work a more general model that allows assigning to each search query a score that models how important it is to compute its result set and examine the optimization problem of selecting a classifier set, whose cost is within the budget, that maximizes the overall score of the queries it can answer. We show that this generalization is likely much harder to approximate complexity-wise, even assuming limited special cases. Nevertheless, we devise a heuristic algorithm, whose effectiveness is demonstrated in our experimental study over real-world data, consisting of a public dataset and datasets provided by a large e-commerce company that include costs and scores derived by business analysts. Finally, we show that our methods are applicable also for related problems in practical settings where there is some flexibility in determining the budget.}
}


@inproceedings{DBLP:conf/sigmod/BoniolMRP22,
	author = {Paul Boniol and
                  Mohammed Meftah and
                  Emmanuel Remy and
                  Themis Palpanas},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {dCAM: Dimension-wise Class Activation Map for Explaining Multivariate
                  Data Series Classification},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1175--1189},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526183},
	doi = {10.1145/3514221.3526183},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/BoniolMRP22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data series classification is an important and challenging problem in data science. Explaining the classification decisions by finding the discriminant parts of the input that led the algorithm to some decision is a real need in many applications. Convolutional neural networks perform well for the data series classification task; though, the explanations provided by this type of algorithms are poor for the specific case of multivariate data series. Addressing this important limitation is a significant challenge. In this paper, we propose a novel method that solves this problem by highlighting both the temporal and dimensional discriminant information. Our contribution is two-fold: we first describe a convolutional architecture that enables the comparison of dimensions; then, we propose a method that returns dCAM, a Dimension-wise Class Activation Map specifically designed for multivariate time series (and CNN-based models). Experiments with several synthetic and real datasets demonstrate that dCAM is not only more accurate than previous approaches, but the only viable solution for discriminant feature discovery and classification explanation in multivariate time series.}
}


@inproceedings{DBLP:conf/sigmod/BabaevOKIGNT22,
	author = {Dmitrii Babaev and
                  Nikita Ovsov and
                  Ivan Kireev and
                  Mariya Ivanova and
                  Gleb Gusev and
                  Ivan Nazarov and
                  Alexander Tuzhilin},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {CoLES: Contrastive Learning for Event Sequences with Self-Supervision},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1190--1199},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526129},
	doi = {10.1145/3514221.3526129},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/BabaevOKIGNT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We address the problem of self-supervised learning on discrete event sequences generated by real-world users. Self-supervised learning incorporates complex information from the raw data in low-dimensional fixed-length vector representations that could be easily applied in various downstream machine learning tasks. In this paper, we propose a new method "CoLES", which adapts contrastive learning, previously used for audio and computer vision domains, to the discrete event sequences domain in a self-supervised setting. We deployed CoLES embeddings based on sequences of transactions at the large European financial services company. Usage of CoLES embeddings significantly improves the performance of the pre-existing models on downstream tasks and produces significant financial gains, measured in hundreds of millions of dollars yearly. We also evaluated CoLES on several public event sequences datasets and showed that CoLES representations consistently outperform other methods on different downstream tasks.}
}


@inproceedings{DBLP:conf/sigmod/DaiQC22,
	author = {Yizhou Dai and
                  Miao Qiao and
                  Lijun Chang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Anchored Densest Subgraph},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1200--1213},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517890},
	doi = {10.1145/3514221.3517890},
	timestamp = {Sun, 04 Aug 2024 19:37:28 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/DaiQC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a graph, densest subgraph search reports a single subgraph that maximizes the density (i.e., average degree). To diversify the search results without imposing rigid constraints, this paper studies the problem of anchored densest subgraph search (ADS). Given a graph, a reference node set S and an anchored node set A with A-R, ADS reports a supergraph of A that maximizes the R-subgraph density ? a density that favors the nodes that are close to S and are not over-popular in comparison with nodes in R. The two levels of localities bring wide applications, as demonstrated by our use cases. For ADS, we propose an algorithm that is local since the complexity is only related to the nodes in S as opposed to the entire graph. Extensive experiments show that our local algorithm for ADS outperforms the global algorithm by up to three orders of magnitudes in time and space consumption; moreover, our local algorithm outperforms existing local community detection solutions in locality, result density, and query processing time and space.}
}


@inproceedings{DBLP:conf/sigmod/KimJSHCC22,
	author = {Kyoungmin Kim and
                  Jisung Jung and
                  In Seo and
                  Wook{-}Shin Han and
                  Kangwoo Choi and
                  Jaehyok Chong},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Learned Cardinality Estimation: An In-depth Study},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1214--1227},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526154},
	doi = {10.1145/3514221.3526154},
	timestamp = {Thu, 02 May 2024 17:06:40 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/KimJSHCC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learned cardinality estimation (CE) has recently gained significant attention for replacing long-studied traditional CE with machine learning, especially for deep learning. However, these estimators were developed independently and have not been fairly or comprehensively compared in common settings. Most studies use a subset of IMDB data which is too simple to measure their limits and determine whether they are ready for real, complex data. Furthermore, they are regarded as black boxes, without a deep understanding of why large errors occur. In this paper, we first provide a taxonomy and a unified workflow of learned estimators for a better understanding of estimators. We next comprehensively compare recent learned CE methods that support joins, from a subset of tables to full IMDB and TPC-DS datasets. Under the experimental results, we then demystify the black-box models and analyze critical components that cause large errors. We also measure their impact on query optimization. Finally, based on the findings, we suggest realizable research opportunities. We believe that a deeper understanding of the behavior of existing methods can provide a more comprehensive and substantial framework for developing better estimators.}
}


@inproceedings{DBLP:conf/sigmod/SabekUK22,
	author = {Ibrahim Sabek and
                  Tenzin Samten Ukyab and
                  Tim Kraska},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {LSched: {A} Workload-Aware Learned Query Scheduler for Analytical
                  Database Systems},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1228--1242},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526158},
	doi = {10.1145/3514221.3526158},
	timestamp = {Sun, 19 Jan 2025 13:27:24 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/SabekUK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query scheduling is a crucial task for analytical database systems that can greatly affect the query latency. However, existing scheduling approaches are based on heuristics and not optimal. A recent trial proposed to use reinforcement learning for automatically learning end-to-end scheduling policies. However, such trial was not capable of considering the database-specific characteristics (e.g., operator types, pipelining), and hence becomes not efficient for analytical database systems. In this paper, we try to fill this gap and introduce LSched (Learned Scheduler), a fully learned workload-aware query scheduler for in-memory analytical database systems. LSched provides an efficient inter-query and intra-query scheduling for dynamic analytical workloads (i.e., different queries can arrive/depart at any time). We integrated LSched with an efficient in-memory analytical database system, and evaluated it with TPCH, SSB, and JOB benchmarks. Our evaluation shows that LSched improves over the performance of existing state-of-the-art query schedulers and heuristic-based ones by at least 35% and 50% in both streaming and batching query workloads.}
}


@inproceedings{DBLP:conf/sigmod/Vogelsgesang0LK22,
	author = {Adrian Vogelsgesang and
                  Thomas Neumann and
                  Viktor Leis and
                  Alfons Kemper},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Efficient Evaluation of Arbitrarily-Framed Holistic {SQL} Aggregates
                  and Window Functions},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1243--1256},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526184},
	doi = {10.1145/3514221.3526184},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Vogelsgesang0LK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Window functions became part of the SQL standard in SQL:2003 and are widely used for data analytics: Percentiles, rankings, moving averages, running sums and local maxima are all expressed as window functions in SQL. Yet, the features offered by SQL's window functions lack composability: Framing is only available for distributive and algebraic aggregate functions, but not for holistic aggregates like percentiles and window functions like ranks. The SQL standard explicitly disallows holistic aggregates from being framed and thereby severely limits data analysts. This paper proposes to remove this restriction, thereby making window functions fully composable. The newly gained composability allows for more complex aggregates which are tricky to evaluate. The lack of subquadratic, parallel algorithms to evaluate framed holistic aggregates is probably the main objection against adding truly composable window functionality to the SQL standard. As such, this paper shows how to efficiently evaluate all window and aggregate functions from SQL:2011, except for DENSE_RANK, in combination with arbitrary window frames. This includes framed distinct aggregates, framed value functions, framed percentiles and framed ranks.}
}


@inproceedings{DBLP:conf/sigmod/0005BM22,
	author = {George Christodoulou and
                  Panagiotis Bouros and
                  Nikos Mamoulis},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{HINT:} {A} Hierarchical Index for Intervals in Main Memory},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1257--1270},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517873},
	doi = {10.1145/3514221.3517873},
	timestamp = {Sun, 19 Jan 2025 13:27:19 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0005BM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indexing intervals is a fundamental problem, finding a wide range of applications, most notably in temporal and uncertain databases. In this paper, we propose HINT, a novel and efficient in-memory index for intervals, with a focus on interval overlap queries, which are a basic component of many search and analysis tasks. HINT applies a hierarchical partitioning approach, which assigns each interval to at most two partitions per level and has controlled space requirements. We reduce the information stored at each partition to the absolutely necessary by dividing the intervals in it based on whether they begin inside or before the partition boundaries. In addition, our index includes storage optimization techniques for the effective handling of data sparsity and skewness. Experimental results on real and synthetic interval sets of different characteristics show that HINT is typically one order of magnitude faster than existing interval indexing methods.}
}


@inproceedings{DBLP:conf/sigmod/LiSC22,
	author = {Yiming Li and
                  Yanyan Shen and
                  Lei Chen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Camel: Managing Data for Efficient Stream Learning},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1271--1285},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517836},
	doi = {10.1145/3514221.3517836},
	timestamp = {Fri, 28 Apr 2023 13:34:02 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/LiSC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many real-world applications rely on predictive models that are incrementally learned online. Specifically, models are updated with a single pass over continuously arriving data batches in a typical stream learning framework. However, this framework has three shortcomings: high training cost, low data effectiveness, and catastrophic forgetting. We describe Camel, a system that addresses the above issues. Camel includes two independent data management components: coreset selection and buffer update. To accelerate model training, Camel selects a coreset from each streaming data batch for model update. Selecting a coreset with worst-case guarantees is NP-hard. To solve this problem, we reformulate coreset selection as a submodular maximization problem by deriving an upper bound on the objective function. To mitigate catastrophic forgetting, Camel maintains a buffer of past representative samples as new data arrive. Moreover, Camel quantizes numerical data in buffer via a quantile sketch to reduce the memory footprint. Finally, extensive experiments validate the effectiveness and efficiency of Camel. In particular, our coreset selection algorithm can achieve a linear speedup with a marginal accuracy loss on redundant datasets. Furthermore, our buffer update algorithms can outperform the state-of-the-art methods for anti-forgetting on various data distributions.}
}


@inproceedings{DBLP:conf/sigmod/XuQYJRGKLL0Y022,
	author = {Lijie Xu and
                  Shuang Qiu and
                  Binhang Yuan and
                  Jiawei Jiang and
                  C{\'{e}}dric Renggli and
                  Shaoduo Gan and
                  Kaan Kara and
                  Guoliang Li and
                  Ji Liu and
                  Wentao Wu and
                  Jieping Ye and
                  Ce Zhang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {In-Database Machine Learning with CorgiPile: Stochastic Gradient Descent
                  without Full Data Shuffle},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1286--1300},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526150},
	doi = {10.1145/3514221.3526150},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/XuQYJRGKLL0Y022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stochastic gradient descent (SGD) is the cornerstone of modern ML systems. Despite its computational efficiency, SGD requires random data access that is inherently inefficient when implemented in systems that rely on block-addressable secondary storage such as HDD and SSD, e.g., in-DB ML systems and TensorFlow/PyTorch over large files. To address this impedance mismatch, various data shuffling strategies have been proposed to balance the convergence rate of SGD (which favors randomness) and its I/O performance (which favors sequential access). In this paper, we first conduct a systematic empirical study on existing data shuffling strategies, which reveals that all existing strategies have room for improvement---they suffer in terms of I/O performance or convergence rate. With this in mind, we propose a simple but novel hierarchical data shuffling strategy, CorgiPile. Compared with existing strategies, CorgiPile avoids a full data shuffle while maintaining comparable convergence rate of SGD as if a full shuffle were performed. We provide a non-trivial theoretical analysis of CorgiPile on its convergence behavior. We further integrate CorgiPile into PostgreSQL by introducing three new physical operators with optimizations. Our experimental results show that CorgiPile can achieve comparable convergence rate to the full shuffle based SGD, and 1.6X-12.8X faster than two state-of-the-art in-DB ML systems, Apache MADlib and Bismarck, on both HDD and SSD.}
}


@inproceedings{DBLP:conf/sigmod/WangZWCZY22,
	author = {Qiange Wang and
                  Yanfeng Zhang and
                  Hao Wang and
                  Chaoyi Chen and
                  Xiaodong Zhang and
                  Ge Yu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {NeutronStar: Distributed {GNN} Training with Hybrid Dependency Management},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1301--1315},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526134},
	doi = {10.1145/3514221.3526134},
	timestamp = {Sun, 19 Jan 2025 13:27:18 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/WangZWCZY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {GNN's training needs to resolve issues of vertex dependencies, i.e., each vertex representation's update depends on its neighbors. Existing distributed GNN systems adopt either a dependencies-cached approach or a dependencies-communicated approach. Having made intensive experiments and analysis, we find that a decision to choose one or the other approach for the best performance is determined by a set of factors, including graph inputs, model configurations, and an underlying computing cluster environment. If various GNN trainings are supported solely by one approach, the performance results are often suboptimal. We study related factors for each GNN training before its execution to choose the best-fit approach accordingly. We propose a hybrid dependency-handling approach that adaptively takes the merits of the two approaches at runtime. Based on the hybrid approach, we further develop a distributed GNN training system called NeutronStar, which makes high performance GNN trainings in an automatic way. NeutronStar is also empowered by effective optimizations in CPU-GPU computation and data processing. Our experimental results on 16-node Aliyun cluster demonstrate that NeutronStar achieves 1.81X-14.25X speedup over existing GNN systems including DistDGL and ROC.}
}


@inproceedings{DBLP:conf/sigmod/FuXCT022,
	author = {Fangcheng Fu and
                  Huanran Xue and
                  Yong Cheng and
                  Yangyu Tao and
                  Bin Cui},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {BlindFL: Vertical Federated Machine Learning without Peeking into
                  Your Data},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1316--1330},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526127},
	doi = {10.1145/3514221.3526127},
	timestamp = {Sat, 30 Sep 2023 09:56:33 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/FuXCT022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the rising concerns on privacy protection, how to build machine learning (ML) models over different data sources with security guarantees is gaining more popularity. Vertical federated learning (VFL) describes such a case where ML models are built upon the private data of different participated parties that own disjoint features for the same set of instances, which fits many real-world collaborative tasks. Nevertheless, we find that existing solutions for VFL either support limited kinds of input features or suffer from potential data leakage during the federated execution. To this end, this paper aims to investigate both the functionality and security of ML modes in the VFL scenario. To be specific, we introduce BlindFL, a novel framework for VFL training and inference. First, to address the functionality of VFL models, we propose the federated source layers to unite the data from different parties. Various kinds of features can be supported efficiently by the federated source layers, including dense, sparse, numerical, and categorical features. Second, we carefully analyze the security during the federated execution and formalize the privacy requirements. Based on the analysis, we devise secure and accurate algorithm protocols, and further prove the security guarantees under the ideal-real simulation paradigm. Extensive experiments show that BlindFL supports diverse datasets and models efficiently whilst achieves robust privacy guarantees.}
}


@inproceedings{DBLP:conf/sigmod/KornaropoulosRT22,
	author = {Evgenios M. Kornaropoulos and
                  Silei Ren and
                  Roberto Tamassia},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {The Price of Tailoring the Index to Your Data: Poisoning Attacks on
                  Learned Index Structures},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1331--1344},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517867},
	doi = {10.1145/3514221.3517867},
	timestamp = {Tue, 24 Dec 2024 22:38:58 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/KornaropoulosRT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The concept of learned index structures relies on the idea that the input-output functionality of a database index can be viewed as a prediction task and, thus, implemented using a machine learning model instead of traditional algorithmic techniques. This novel angle for a decades-old problem has inspired exciting results at the intersection of machine learning and data structures. However, the advantage of learned index structures, i.e., the ability to adjust to the data at hand via the underlying ML-model, can become a disadvantage from a security perspective as it could be exploited. In this work, we present the first study of data poisoning attacks on learned index structures. Our poisoning approach is different from all previous works since the model under attack is trained on a cumulative distribution function (CDF) and, thus, every injection on the training set has a cascading impact on multiple data values. We formulate the first poisoning attacks on linear regression models trained on a CDF, which is a basic building block of the proposed learned index structures. We generalize our poisoning techniques to attack the advanced two-stage design of learned index structures called recursive model index (RMI), which has been shown to outperform traditional B-Trees. We evaluate our attacks under a variety of parameterizations of the model and show that the error of the RMI increases up to 300x and the error of its second-stage models increases up to 3000x.}
}


@inproceedings{DBLP:conf/sigmod/0001CSZZACLL22,
	author = {Qizhen Zhang and
                  Xinyi Chen and
                  Sidharth Sankhe and
                  Zhilei Zheng and
                  Ke Zhong and
                  Sebastian Angel and
                  Ang Chen and
                  Vincent Liu and
                  Boon Thau Loo},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Optimizing Data-intensive Systems in Disaggregated Data Centers with
                  {TELEPORT}},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1345--1359},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517856},
	doi = {10.1145/3514221.3517856},
	timestamp = {Mon, 07 Oct 2024 17:01:09 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/0001CSZZACLL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent proposals for the disaggregation of compute, memory, storage, and accelerators in data centers promise substantial operational benefits. Unfortunately, for resources like memory, this comes at the cost of performance overhead due to the potential insertion of network latency into every load and store operation. This effect is particularly felt by data-intensive systems due to the size of their working sets, the frequency at which they need to access memory, and the relatively low computation per access. This performance impairment offsets the elasticity benefit of disaggregated memory. This paper presents TELEPORT, a compute pushdown framework for data-intensive systems that run on disaggregated architectures; compared to prior work on compute pushdown, TELEPORT is unique in its efficiency and flexibility. We have developed optimization prin- ciples for several popular systems including a columnar in-memory DBMS, a graph processing system, and a MapReduce system. The evaluation results show that using TELEPORT to push down simple operators improves the performance of these systems on state-of-the-art disaggregated OSes by an order of magnitude, thus fully exploiting the elasticity of disaggregated data centers.}
}


@inproceedings{DBLP:conf/sigmod/HuL022,
	author = {Yu{-}Ching Hu and
                  Yuliang Li and
                  Hung{-}Wei Tseng},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{TCUDB:} Accelerating Database with Tensor Processors},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1360--1374},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517869},
	doi = {10.1145/3514221.3517869},
	timestamp = {Sun, 19 Jan 2025 13:27:26 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/HuL022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of novel hardware accelerators has powered the tremendous growth of machine learning in recent years. These accelerators deliver incomparable performance gains in processing high-volume matrix operators, particularly matrix multiplication, a core component of neural network training and inference. In this work, we explored opportunities of accelerating database systems using NVIDIA's Tensor Core Units (TCUs). We present TCUDB, a TCU-accelerated query engine processing a set of query operators including natural joins and group-by aggregates as matrix operators within TCUs. Matrix multiplication was considered inefficient in the past; however, this strategy has remained largely unexplored in conventional GPU-based databases, which primarily rely on vector or scalar processing. We demonstrate the significant performance gain of TCUDB in a range of real-world applications including entity matching, graph query processing, and matrix-based data analytics. TCUDB achieves up to 288x speedup compared to a baseline GPU-based query engine.}
}


@inproceedings{DBLP:conf/sigmod/JasnyT0B22,
	author = {Matthias Jasny and
                  Lasse Thostrup and
                  Tobias Ziegler and
                  Carsten Binnig},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{P4DB} - The Case for In-Network {OLTP}},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1375--1389},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517825},
	doi = {10.1145/3514221.3517825},
	timestamp = {Sun, 04 Aug 2024 19:37:27 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/JasnyT0B22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we present a new approach for distributed DBMSs called P4DB, that uses a programmable switch to accelerate OLTP workloads. The main idea of P4DB is that it implements a transaction processing engine on top of a P4-programmable switch. The switch can thus act as an accelerator in the network, especially when it is used to store and process hot (contended) tuples on the switch. In our experiments, we show that P4DB hence provides significant benefits compared to traditional DBMS architectures and can achieve a speedup of up to 8x.}
}


@inproceedings{DBLP:conf/sigmod/ShanbhagYYM22,
	author = {Anil Shanbhag and
                  Bobbi W. Yogatama and
                  Xiangyao Yu and
                  Samuel Madden},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Tile-based Lightweight Integer Compression in {GPU}},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1390--1403},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526132},
	doi = {10.1145/3514221.3526132},
	timestamp = {Tue, 08 Aug 2023 10:54:18 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/ShanbhagYYM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {GPUs are increasingly used for high-performance and interactive data analytics workloads due to their capability to accelerate computation using massive parallelism. A key constraint of GPU-based data analytics today is the limited memory capacity in GPU devices. Data compression is a powerful technique that can mitigate the capacity limitation in two ways: (1) fitting more data into GPU memory and (2) speeding up data transfer between CPU and GPU. However, compression schemes for GPU today are still limited in compression ratio and/or decompression speed. We identify two limiting factors of existing approaches. First, existing decompression solutions require multiple passes of scanning the global memory to decode layers of compression schemes, incurring significant memory traffic and hurting performance. We present the tile-based decompression model to decompress encoded data in a single pass over global memory and inline with query execution. Second, we develop an efficient implementation of bit-packing-based compression schemes and their optimization techniques in the context of GPU. Our evaluation shows that our schemes can achieve similar compression rates to the best state-of-the-art compression schemes in GPU (i.e., nvCOMP) while being 2.2× and 2.6× faster in decompression speed and query running time.}
}


@inproceedings{DBLP:conf/sigmod/AnSS022,
	author = {Mijin An and
                  In{-}Yeong Song and
                  Yong Ho Song and
                  Sang{-}Won Lee},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Avoiding Read Stalls on Flash Storage},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1404--1417},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526126},
	doi = {10.1145/3514221.3526126},
	timestamp = {Sun, 19 Jan 2025 13:27:32 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AnSS022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When a dirty victim page is selected for replacement upon page miss, the buffer manager has to first flush the dirty victim to the storage before reading the missing page. This conventional read-after-write (RAW) protocol, while working well on hard disks, causes the problem of read stall on flash storage with asymmetric read-write speed and parallelism; because of the resource conflict for a buffer frame between write and read operations, a page-missing process has to wait for the slow write to complete to secure a clean frame for the missing page. This strict write-then-read serialization under-utilizes CPU and storage, worsening transaction throughput and latency. To avoid the read stall problem on flash storage, this paper proposes write-after-read (WAR) protocol as a new I/O architecture between buffer manager and flash storage. In WAR, foreground processes make victim frames clean instantly by temporarily copying dirty pages at LRU tail into a separate DRAM space and read their missing pages into the cleaned frames with no stall. The dirty pages will be written to the storage asynchronously. By resolving resource conflict and thus avoiding read stalls, the database engine can issue more I/Os in parallel and better utilize CPU as well as storage, improving throughput and latency. We prototype WAR in two database storage engines, MySQL/InnoDB and Zero. Our comprehensive experimental results show that WAR improves transaction throughput by up to 2.9x compared to RAW.}
}


@inproceedings{DBLP:conf/sigmod/WangS22,
	author = {Zhiqi Wang and
                  Zili Shao},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {TimeUnion: An Efficient Architecture with Unified Data Model for Timeseries
                  Management Systems on Hybrid Cloud Storage},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1418--1432},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526175},
	doi = {10.1145/3514221.3526175},
	timestamp = {Mon, 26 Jun 2023 20:43:17 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/WangS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Timeseries management systems have attracted considerable attention during the last decade with the rise of IoT and performance monitoring. With the rapidly increasing data scale in the production environment, deploying timeseries management systems on cloud with cloud storage is a natural trend because of its high availability, reliability, and scalability. However, the state-of-the-art designs are not tailored for cloud environments; they suffer from the limited number of timeseries a single compute node can handle because of the imbalanced resource usage. In this paper, we present TimeUnion, an efficient timeseries management system tailored for hybrid cloud storage services (e.g. fast cloud block stores and slow cloud object stores). First, we propose our unified data model to represent both independent timeseries and groups of timeseries, which is capable of handling diverse use cases of timeseries. Second, since the main bottleneck of the current timeseries systems is the overuse of memory, we introduce our exploration on memory-efficient data structures for timeseries to maximize the number of timeseries a compute instance can maintain. Third, to absorb the memory data chunks quickly, we present our time-partitioned LSM-tree with tailored architecture, compaction mechanism, out-of-order data handling, and dynamic level size adjustment for timeseries data. We prototype TimeUnion with C++ from scratch and evaluate it on AWS EC2 with AWS EBS (cloud block store) and AWS S3 (cloud object store). Compared to the storage engine of Cortex, TimeUnion can handle at least 5x more timeseries, and achieve at least 24.8% higher insertion throughput and 49.8% lower query latency. We have released the open-source code of TimeUnion for public access.}
}


@inproceedings{DBLP:conf/sigmod/WuWZ22,
	author = {Jiacheng Wu and
                  Jin Wang and
                  Carlo Zaniolo},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Optimizing Parallel Recursive Datalog Evaluation on Multicore Machines},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1433--1446},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517853},
	doi = {10.1145/3514221.3517853},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/WuWZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past years, there has been a resurgence of interest in Datalog due to its superior ability of expressing applications that require recursive computations. However, in addition to expressive power, supporting analytical tasks with ever-increasing volume of data requires high performance and scalability. In this paper, we present DCDatalog, an in-memory Datalog engine specifically designed for modern shared-memory multicore architectures. Our key contribution is a novel system architecture that supports a wide scope of Datalog applications with a light-weight coordination scheme during parallel evaluation. To this end, we propose a dynamic scheduling strategy that can generate the parallel execution plan on-the-fly while reducing concurrent accesses to the shared memory. Experimental results on several large datasets show that our system significantly outperforms existing parallel Datalog engines and also scales well with increasing amount of data.}
}


@inproceedings{DBLP:conf/sigmod/ZhangY0Z22,
	author = {Hao Zhang and
                  Jeffrey Xu Yu and
                  Yikai Zhang and
                  Kangfei Zhao},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Parallel Query Processing: To Separate Communication from Computation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1447--1461},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526164},
	doi = {10.1145/3514221.3526164},
	timestamp = {Sun, 19 Jan 2025 13:27:21 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhangY0Z22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study parallel query processing with a focus on reducing the communication cost, which is the dominating factor in parallel query processing. The communication cost becomes large if the intermediate results between operators are large in intra-operator parallelism. In the existing approaches, it optimizes an SQL query by arranging relational algebra operators to reduce the total cost, where, for each operator, it involves (i) distribution of data partitioned to computing nodes by communication, and (ii)computation on computing nodes locally. The communication and computation are dealt with inside an operator and are not separable. In other words, it is difficult to avoid large intermediate results and hence reduce the communication cost. To reduce communication cost, we separate communication from computation using several new operators proposed in this paper. One is a pair operator () to pair the partitions of a relation R with the partitions of a relation S, where a partition is specified by a hash function. With the pair operator defined, we can explicitly deal with communication to deliver pairs of partitions to computing nodes. Together with, we can also explicitly treat the local computation on a computing node as op for any RA (relational algebra) operator op. We give a merge operator (U), to collect all partial results from computing nodes as they are. In short, with, op, and U, we are able to explicitly specify communication and computation for RA operators. Furthermore, we propose new techniques, namely, partitioning push-down and computation push-up to separate communication from computation for RA expressions. We prove that we can push-down/up for a wide range of relational expressions. We have developed a distributed system named Secco (Separate Communication from Computation) by revamping SparkSQL on Spark, and confirmed the efficiency of our approach in our performance studies using real datasets.}
}


@inproceedings{DBLP:conf/sigmod/UnnibhaviCB0B22,
	author = {Harshavardhan Unnibhavi and
                  David Cerdeira and
                  Antonio Barbalace and
                  Nuno Santos and
                  Pramod Bhatotia},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Secure and Policy-Compliant Query Processing on Heterogeneous Computational
                  Storage Architectures},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1462--1477},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517913},
	doi = {10.1145/3514221.3517913},
	timestamp = {Sun, 19 Jan 2025 13:27:24 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/UnnibhaviCB0B22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computation Storage Architectures (CSA) are increasingly adopted in the cloud for near data processing, where the underlying storage devices/servers are now equipped with heterogeneous cores which enable computation offloading near to the data. While CSA is a promising high-performance architecture for the cloud, in general data analytics also presents significant data security and policy compliance (e.g., GDPR) challenges in untrusted cloud environments. In this paper, we present IronSafe, a secure and policy-compliant query processing system for heterogeneous computational storage architectures, while preserving the performance advantages of CSA in untrusted cloud environments. To achieve these design properties in a computing environment with heterogeneous host (x86) and storage system (ARM), we design and implement the entire hardware and software system stack from the ground-up leveraging hardware-assisted Trusted Execution Environments (TEEs): namely, Intel SGX and ARM TrustZone. More specifically, IronSafe builds on three core contributions: (1) a heterogeneous confidential computing framework for shielded execution with x86 and ARM TEEs and associated secure storage system for the untrusted storage medium; (2) a policy compliance monitor to provide a unified service for attestation and policy compliance; and (3) a declarative policy language and associated interpreter for concisely specifying and efficiently evaluating a rich set of polices. Our evaluation using the TPC-H SQL benchmark queries and GDPR anti-pattern use-cases shows that IronSafe is faster, on average by 2.3x than a host-only secure system, while providing strong security and policy-compliance properties.}
}


@inproceedings{DBLP:conf/sigmod/0005YBPD22,
	author = {Yu Xia and
                  Xiangyao Yu and
                  Matthew Butrovich and
                  Andrew Pavlo and
                  Srinivas Devadas},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Litmus: Towards a Practical Database Management System with Verifiable
                  {ACID} Properties and Transaction Correctness},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1478--1492},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517851},
	doi = {10.1145/3514221.3517851},
	timestamp = {Sun, 19 Jan 2025 13:27:26 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0005YBPD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing secure database management systems (DBMSs) focus on security and privacy of data but overlook semantic properties, such as the correctness and ACID properties of transactions. Enforcing these properties is crucial to the functionality of applications. If these guarantees do not hold, catastrophic losses could result. To address this issue, we present Litmus, a DBMS that can provide verifiable proofs of transaction correctness and semantic properties including atomicity and serializability. Litmus features a co-design of both the database and the cryptographic parts. We evaluate a proof-of-concept prototype of Litmus on the YCSB and TPC-C benchmarks and show that under reasonable cryptographic assumptions it can process more than 15,000 transactions per second (txn/s) verifiably. Our result shows a promising practical direction considering that PayPal runs on average 115 txn/s and VISA 2000-4000 txn/s. The proof is about 30kB per verification batch and verifies with a constant time of 300 seconds. Litmus can extend to verify consistency as well.}
}


@inproceedings{DBLP:conf/sigmod/SuharaL0ZDCT22,
	author = {Yoshihiko Suhara and
                  Jinfeng Li and
                  Yuliang Li and
                  Dan Zhang and
                  {\c{C}}agatay Demiralp and
                  Chen Chen and
                  Wang{-}Chiew Tan},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Annotating Columns with Pre-trained Language Models},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1493--1503},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517906},
	doi = {10.1145/3514221.3517906},
	timestamp = {Mon, 21 Apr 2025 14:23:17 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/SuharaL0ZDCT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inferring meta information about tables, such as column headers or relationships between columns, is an active research topic in data management as we find many tables are missing some of this information. In this paper, we study the problem of annotating table columns (i.e., predicting column types and the relationships between columns) using only information from the table itself. We develop a multi-task learning framework (called Doduo) based on pre-trained language models, which takes the entire table as input and predicts column types/relations using a single model. Experimental results show that Doduo establishes new state-of-the-art performance on two benchmarks for the column type prediction and column relation prediction tasks with up to 4.0% and 11.9% improvements, respectively. We report that Doduo can already outperform the previous state-of-the-art performance with a minimal number of tokens, only 8 tokens per column. We release a toolbox (https://github.com/megagonlabs/doduo) and confirm the effectiveness of Doduo on a real-world data science problem through a case study.}
}


@inproceedings{DBLP:conf/sigmod/ZhaoF22,
	author = {Zixuan Zhao and
                  Raul Castro Fernandez},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Leva: Boosting Machine Learning Performance with Relational Embedding
                  Data Augmentation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1504--1517},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517891},
	doi = {10.1145/3514221.3517891},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhaoF22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present Leva, an end-to-end system that boosts the performance of machine learning tasks over relational data. Leva builds a relational embedding by representing relational data as a graph and then using embedding methods to represent the graph as vectors. The embedding represents information from the entire database, including useful information for the downstream machine learning task. At the same time, some information in the graph will be erroneous, for example, corresponding to incorrect inclusion dependencies. However, we show that the supervision signal from the downstream task filters out information that is not useful. The result is a boost in ML performance. This result means that it is possible for analysts to avoid the time-consuming effort of collecting features across multiple relations-which requires solving a data discovery and integration problem-and instead rely on these techniques to train better-performing models. We demonstrate Leva's performance on different classification and regression datasets and compare it with multiple other baselines.}
}


@inproceedings{DBLP:conf/sigmod/NikookarSSRBMPS22,
	author = {Sepideh Nikookar and
                  Paras Sakharkar and
                  Sathyanarayanan Somasunder and
                  Senjuti Basu Roy and
                  Adam Bienkowski and
                  Matthew Macesker and
                  Krishna R. Pattipati and
                  David Sidoti},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Cooperative Route Planning Framework for Multiple Distributed Assets
                  in Maritime Applications},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1518--1527},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526131},
	doi = {10.1145/3514221.3526131},
	timestamp = {Sun, 19 Jan 2025 13:27:31 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/NikookarSSRBMPS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work formalizes the Route Planning Problem (RPP), wherein a set of distributed assets (e.g., ships, submarines, unmanned systems) simultaneously plan routes to optimize a team goal (e.g., find the location of an unknown threat or object in minimum time and/or fuel consumption) while ensuring that the planned routes satisfy certain constraints (e.g., avoiding collisions and obstacles). This problem becomes overwhelmingly complex for multiple distributed assets as the search space grows exponentially to design such plans. The RPP is formalized as a Team Discrete Markov Decision Process (TDMDP) and we propose a Multi-agent Multi-objective Reinforcement Learning (MaMoRL) framework for solving it. We investigate challenges in deploying the solution in real-world settings and study approximation opportunities. We experimentally demonstrate MaMoRL's effectiveness on multiple real-world and synthetic grids, as well as for transfer learning. MaMoRL is deployed for use by the Naval Research Laboratory - Marine Meteorology Division (NRL-MMD), Monterey, CA.}
}


@inproceedings{DBLP:conf/sigmod/00010SWNCB22,
	author = {Wentao Wu and
                  Chi Wang and
                  Tarique Siddiqui and
                  Junxiong Wang and
                  Vivek R. Narasayya and
                  Surajit Chaudhuri and
                  Philip A. Bernstein},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Budget-aware Index Tuning with Reinforcement Learning},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1528--1541},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526128},
	doi = {10.1145/3514221.3526128},
	timestamp = {Mon, 03 Mar 2025 21:21:48 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/00010SWNCB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Index tuning aims to find the optimal index configuration for an input workload. It is a resource-intensive task since it requires making multiple expensive "what-if" calls to the query optimizer to estimate the cost of a query given an index configuration without actually building the indexes. In this paper, we study the problem of budget-aware index tuning where the number of what-if calls allowed when searching for the optimal configuration during tuning is constrained. This problem is challenging as it requires addressing the trade-off between investing what-if calls on exploring new configurations versus exploiting a known promising configuration. We formulate budget-aware index tuning as a Markov decision process, and propose a solution based on Monte Carlo tree search, a classic reinforcement learning technology. Experimental evaluation on both standard industry benchmarks and real workloads shows that our solution can significantly outperform alternative budget-aware solutions in terms of the quality of the index configuration.}
}


@inproceedings{DBLP:conf/sigmod/YangWCZH22,
	author = {Jingyi Yang and
                  Peizhi Wu and
                  Gao Cong and
                  Tieying Zhang and
                  Xiao He},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{SAM:} Database Generation from Query Workloads with Supervised Autoregressive
                  Models},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1542--1555},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526168},
	doi = {10.1145/3514221.3526168},
	timestamp = {Sun, 19 Jan 2025 13:27:24 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/YangWCZH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the prevalence of cloud databases, database users are increasingly reliant on the cloud database providers to manage their data. It becomes a challenge for cloud providers to benchmark different DBMS for a specific database instance without having access to the underlying data. One viable solution is to leverage a query workload, which contains a set of queries and the corresponding cardinalities, to generate a synthetic database with similar query performance. Existing methods for database generation with cardinality constraints, however, can only handle very small query workloads due to their high complexity and encounter challenges when handling join queries. In this work, we propose SAM, a supervised deep autoregressive model-based method for database generation from query workloads. First, SAM is able to process large-scale query workloads efficiently as its complexity is linear in the size of the query workload, the number of attributes and the attribute domain size. Second, we develop algorithms to obtain unbiased samples of base relations from the deep autoregressive model and assign join keys in a way that accurately recovers the full outer join of the target database. Comprehensive experiments on real-world datasets demonstrate that SAM is able to efficiently generate a high-fidelity database that not only satisfies the input cardinality constraints, but also is close to the target database.}
}


@inproceedings{DBLP:conf/sigmod/CampbellAG22,
	author = {Felix S. Campbell and
                  Bahareh Sadat Arab and
                  Boris Glavic},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Efficient Answering of Historical What-if Queries},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1556--1569},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526138},
	doi = {10.1145/3514221.3526138},
	timestamp = {Sun, 12 Nov 2023 02:07:18 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/CampbellAG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce historical what-if queries, a novel type of what-if analysis that determines the effect of a hypothetical change to the transactional history of a database. For example, "how would revenue be affected if we would have charged an additional $6 for shipping?" We develop efficient techniques for answering historical what-if queries, i.e., determining how a modified history affects the current database state. Our techniques are based on reenactment, a replay technique for transactional histories. We optimize this process using program and data slicing techniques that determine which updates and what data can be excluded from reenactment without affecting the result. Using an implementation of our techniques in Mahif (a Middleware for Answering Historical what-IF queries) we demonstrate their effectiveness experimentally.}
}


@inproceedings{DBLP:conf/sigmod/DeutchFKM22,
	author = {Daniel Deutch and
                  Nave Frost and
                  Benny Kimelfeld and
                  Mika{\"{e}}l Monet},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Computing the Shapley Value of Facts in Query Answering},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1570--1583},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517912},
	doi = {10.1145/3514221.3517912},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/DeutchFKM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Shapley value is a game-theoretic notion for wealth distribution that is nowadays extensively used to explain complex data-intensive computation, for instance, in network analysis or machine learning. Recent theoretical works show that query evaluation over relational databases fits well in this explanation paradigm. Yet, these works fall short of providing practical solutions to the computational challenge inherent to the Shapley computation. We present in this paper two practically effective solutions for computing Shapley values in query answering. We start by establishing a tight theoretical connection to the extensively studied problem of query evaluation over probabilistic databases, which allows us to obtain a polynomial-time algorithm for the class of queries for which probability computation is tractable. We then propose a first practical solution for computing Shapley values that adopts tools from probabilistic query evaluation. In particular, we capture the dependence of query answers on input database facts using Boolean expressions (data provenance), and then transform it, via Knowledge Compilation, into a particular circuit form for which we devise an algorithm for computing the Shapley values. Our second practical solution is a faster yet inexact approach that transforms the provenance to a Conjunctive Normal Form and uses a heuristic to compute the Shapley values. Our experiments on TPC-H and IMDB demonstrate the practical effectiveness of our solutions.}
}


@inproceedings{DBLP:conf/sigmod/HutterAK0L22,
	author = {Thomas H{\"{u}}tter and
                  Nikolaus Augsten and
                  Christoph M. Kirsch and
                  Michael J. Carey and
                  Chen Li},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{JEDI:} These aren't the {JSON} documents you're looking for?},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1584--1597},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517850},
	doi = {10.1145/3514221.3517850},
	timestamp = {Sun, 19 Jan 2025 13:27:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/HutterAK0L22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The JavaScript Object Notation (JSON) is a popular data format used in document stores to natively support semi-structured data. In this paper, we address the problem of JSON similarity lookup queries: given a query document and a distance threshold τ, retrieve all documents that are within τ from the query document. Different from other hierarchical formats such as XML, JSON supports both ordered and unordered sibling collections within a single document which poses a new challenge to the tree model and distance computation. We propose JSON tree, a lossless tree representation of JSON documents, and define the JSON Edit Distance (JEDI), the first edit-based distance measure for JSON. We develop QuickJEDI, an algorithm that computes JEDI by leveraging a new technique to prune expensive sibling matchings. It outperforms a baseline algorithm by an order of magnitude in runtime. To boost the performance of JSON similarity queries, we introduce an index called JSIM and an effective upper bound based on tree sorting. Our upper bound algorithm runs in O(nτ) time and O(n+τ log n) space, which substantially improves the previous best bound of O(n2) time and O(n log n) space (where n is the tree size). Our experimental evaluation shows that our solution scales to databases with millions of documents and JSON trees with tens of thousands of nodes.}
}


@inproceedings{DBLP:conf/sigmod/GalhotraGRS22,
	author = {Sainyam Galhotra and
                  Amir Gilad and
                  Sudeepa Roy and
                  Babak Salimi},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {HypeR: Hypothetical Reasoning With What-If and How-To Queries Using
                  a Probabilistic Causal Approach},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1598--1611},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526149},
	doi = {10.1145/3514221.3526149},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/GalhotraGRS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {What-if (provisioning for an update to a database) and how-to (how to modify the database to achieve a goal) analyses provide insights to users who wish to examine hypothetical scenarios without making actual changes to a database and thereby help plan strategies in their fields. Typically, such analyses are done by testing the effect of an update in the existing database on a specific view created by a query of interest. In real-world scenarios, however, an update to a particular part of the database may affect tuples and attributes in a completely different part due to implicit semantic dependencies. To allow for hypothetical reasoning while accommodating such dependencies, we develop HypeR, a framework that supports what-if and how-to queries accounting for probabilistic dependencies among attributes captured by a probabilistic causal model. We extend the SQL syntax to include the necessary operators for expressing these hypothetical queries, define their semantics, devise efficient algorithms and optimizations to compute their results using concepts from causality and probabilistic databases, and evaluate the effectiveness of our approach experimentally.}
}


@inproceedings{DBLP:conf/sigmod/Ting22,
	author = {Daniel Ting},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Adaptive Threshold Sampling},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1612--1625},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526122},
	doi = {10.1145/3514221.3526122},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Ting22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sampling is a fundamental problem in computer science and statistics. However, for a given task and stream, it is often not possible to choose good sampling probabilities in advance. We derive a general framework for adaptively changing the sampling probabilities via a collection of thresholds. In general, adaptive sampling procedures introduce dependence amongst the sampled points, making it difficult to compute expectations and ensure estimators are unbiased or consistent. Our framework address this issue and further shows when adaptive thresholds can be treated as if they were fixed thresholds which samples items independently. This makes our adaptive sampling schemes simple to apply as there is no need to create custom estimators for the sampling method. Using our framework, we derive new samplers that can address a broad range of new and existing problems including sampling with memory rather than sample size budgets, stratified samples, multiple objectives, distinct counting, and sliding windows. In particular, we design a sampling procedure for the top-K problem where, unlike in the heavy-hitter problem, the sketch size and sampling probabilities are adaptively chosen.}
}


@inproceedings{DBLP:conf/sigmod/AnneserKZ0K22,
	author = {Christoph Anneser and
                  Andreas Kipf and
                  Huanchen Zhang and
                  Thomas Neumann and
                  Alfons Kemper},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Adaptive Hybrid Indexes},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1626--1639},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526121},
	doi = {10.1145/3514221.3526121},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AnneserKZ0K22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While index structures are crucial components in high-performance query processing systems, they occupy a large fraction of the available memory. Recently-proposed compact indexes reduce this space overhead and thus speed up queries by allowing the database to keep larger working sets in memory. These compact indexes, however, are slower than performance-optimized in-memory indexes because they adopt encodings that trade performance for memory efficiency. Applying different encodings within a single index might allow optimizing both dimensions at the same time - however, it is not clear which encodings should be applied to which index parts at build-time. To take advantage of multiple encodings in one index structure, we present a new framework forming the basis of workload-adaptive hybrid indexes which moves encoding decisions to run-time instead. By sampling incoming queries adaptively, it tracks accesses to index parts and keeps fine-grained statistics which are used for space- and performance-optimized encoding migrations. We evaluated our framework using B+-trees and tries, and examine the adaptation process and space/performance trade-off for real-world and synthetic workloads. For skewed workloads, our framework can reduce the space by up to 82% while retaining more than 90% of the original performance.}
}


@inproceedings{DBLP:conf/sigmod/HentschelSI22,
	author = {Brian Hentschel and
                  Utku Sirin and
                  Stratos Idreos},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Entropy-Learned Hashing: Constant Time Hashing with Controllable Uniformity},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1640--1654},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517894},
	doi = {10.1145/3514221.3517894},
	timestamp = {Tue, 21 Mar 2023 20:57:32 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/HentschelSI22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hashing is a widely used technique for creating uniformly random numbers from arbitrary data. This is required in a large range of core data-driven operations including indexing, partitioning, filters, and sketches. As such, hashing is a core component in numerous systems including relational data systems, key-value stores, compilers, and networks. Due to both the computational and data heavy nature of hashing, it is a core systems bottleneck. For example, a typical database query in the standard TPC-H benchmark may spend 50% of its total cost in hash tables. Similarly, Google spends at least 2% of its total computational cost on C++ hash tables, resulting in a massive yearly cost footprint just from one hashing operation. We propose a new hashing method, called Entropy-Learned Hashing, which reduces the computational cost of hashing by up to an order of magnitude. We look at hashing from a pseudorandomness point of view and the key question we ask is \'\'how much randomness is needed?\'\' We show that state-of-the-art hash functions do too much work to perform their core task: extracting randomness from a data source to create random outputs. Entropy-Learned Hashing 1) models and estimates the randomness (entropy) of the input data, and then 2) creates data-specific hash functions that use only the parts of the data that are needed to differentiate the outputs. The resulting hash functions dramatically reduce the amount of computation needed while we prove their output is similarly uniform to that of traditional hash functions. We test Entropy-Learned Hashing across diverse and core hashing operations such as hash tables, Bloom filters, and partitioning and we demonstrate an increase in throughput in the order of 3.7x, 4.0x, and 14x respectively compared to the best in-class hash functions and implementations used at scale by Google and Meta. In this paper we propose a new method, called Entropy-Learned Hashing, which reduces the computational cost of hashing by up to an order of magnitude. The key question we ask is "how much randomness is needed?\'\': We look at hashing from a pseudorandom point of view, wherein hashing is viewed as extracting randomness from a data source to create random outputs and we show that state-of-the-art hash functions do too much work. Entropy-Learned Hashing 1) models and estimates the randomness (entropy) of the input data, and then 2) creates data-specific hash functions that use only the parts of the data that are needed to differentiate the outputs. Thus the resulting hash functions can minimize the amount of computation needed while we prove that they act similarly to traditional hash functions in terms of the uniformity of their outputs. We test Entropy-Learned Hashing across diverse and core hashing operations such as hash tables, Bloom filters, and partitioning and we observe an increase in throughput in the order of 3.7X, 4.0X, and 14X respectively compared to the best in-class hash functions and implementations used at scale by Google and Meta.}
}


@inproceedings{DBLP:conf/sigmod/0007WZZCL022,
	author = {Feng Zhang and
                  Weitao Wan and
                  Chenyang Zhang and
                  Jidong Zhai and
                  Yunpeng Chai and
                  Haixiang Li and
                  Xiaoyong Du},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {CompressDB: Enabling Efficient Compressed Data Direct Processing for
                  Various Databases},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1655--1669},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526130},
	doi = {10.1145/3514221.3526130},
	timestamp = {Tue, 28 Jan 2025 12:51:52 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0007WZZCL022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In modern data management systems, directly performing operations on compressed data has been proven to be a big success facing big data problems. These systems have demonstrated significant compression benefits and performance improvement for data analytics applications. However, current systems only focus on data queries, while a complete big data system must support both data query and data manipulation. We develop a new storage engine, called CompressDB, which can support data processing for databases without decompression. CompressDB has the following advantages. First, CompressDB utilizes context-free grammar to compress data, and supports both data query and data manipulation. Second, for adaptability, we integrate CompressDB to file systems so that a wide range of databases can directly use CompressDB without any change. Third, we enable operation pushdown to storage so that we can perform data query and manipulation in storage systems without bringing large data to memory for high efficiency. We validate the efficacy of CompressDB supporting various kinds of database systems, including SQLite, LevelDB, MongoDB, and ClickHouse. We evaluate our method using six real-world datasets with various lengths, structures, and content in both single node and cluster environments. Experiments show that CompressDB achieves 40% throughput improvement and 44% latency reduction, along with 1.81 compression ratio on average.}
}


@inproceedings{DBLP:conf/sigmod/KnorrLLLZIM22,
	author = {Eric R. Knorr and
                  Baptiste Lemaire and
                  Andrew Lim and
                  Siqiang Luo and
                  Huanchen Zhang and
                  Stratos Idreos and
                  Michael Mitzenmacher},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Proteus: {A} Self-Designing Range Filter},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1670--1684},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526167},
	doi = {10.1145/3514221.3526167},
	timestamp = {Sun, 06 Oct 2024 21:14:19 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/KnorrLLLZIM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce Proteus, a novel self-designing approximate range filter, which configures itself based on sampled data in order to optimize its false positive rate (FPR) for a given space requirement. Proteus unifies the probabilistic and deterministic design spaces of state-of-the-art range filters to achieve robust performance across a larger variety of use cases. At the core of Proteus lies our Contextual Prefix FPR (CPFPR) model - a formal framework for the FPR of prefix-based filters across their design spaces. We empirically demonstrate the accuracy of our model and Proteus' ability to optimize over both synthetic workloads and real-world datasets. We further evaluate Proteus in RocksDB and show that it is able to improve end-to-end performance by as much as 5.3x over more brittle state-of-the-art methods such as SuRF and Rosetta. Our experiments also indicate that the cost of modeling is not significant compared to the end-to-end performance gains and that Proteus is robust to workload shifts.}
}


@inproceedings{DBLP:conf/sigmod/AlghamdiZRE22,
	author = {Noura S. Alghamdi and
                  Liang Zhang and
                  Elke A. Rundensteiner and
                  Mohamed Y. Eltabakh},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Scalable Time Series Compound Infrastructure},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1685--1698},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517888},
	doi = {10.1145/3514221.3517888},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AlghamdiZRE22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Objects ranging from a patient's history of medical tests to an IoT device's series of sensor maintenance records leave digital traces in the form of big time series. These time series objects do not only span exceedingly long time periods (sometimes years), but are also characterized by intermittent yet interrelated time series measurements punctuated by long gaps of silence. This prevalent data type, which we refer to as Time Series Compound objects (or, TSC), has been largely overlooked in the literature. Unique challenges arise when managing, querying and analyzing repositories of these big TSC objects. These include appropriate similarity semantics with time misalignment resiliency, efficient storage of excessively long and complex objects, and TSC-holistic indexing. We demonstrate that state-of-the-art time series systems, although effective at indexing and searching regular time series data, fail to support such big TSC data. In this work, we introduce the first comprehensive solution for managing TSC objects as first class citizen. We introduce new similarity-match semantics as well as a compact misalignment-resilient representation for TSCs. Upon this foundation, we then design a TSC-aware distributed indexing infrastructure Sloth that supports scalable storage, indexing and querying of TB-scale TSC datasets. Our experimental study demonstrates that for TB-scale datasets, the query response time of Sloth is up to one order of magnitude faster than that of existing systems, while the mean average precision (mAP) for approximate kNN similarity match query results by Sloth is 70% more accurate than existing solutions.}
}


@inproceedings{DBLP:conf/sigmod/Chen022,
	author = {Yiru Chen and
                  Eugene Wu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{PI2:} End-to-end Interactive Visualization Interface Generation from
                  Queries},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1711--1725},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526166},
	doi = {10.1145/3514221.3526166},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Chen022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interactive visualization interfaces are critical in data analysis. Yet creating new interfaces is challenging, as the developer must understand the queries needed for the desired analysis task, and then design the appropriate interface. Existing task models are too abstract to be used to automatically generate interfaces, and visualization recommenders do not take the queries nor interactions into account. PI2 is the first system to generate fully functional interactive visualization interfaces from a representative sequence of task queries. PI2 analyzes queries syntactically and proposes a novel Difftree representation that encodes the systematic variations between query abstract syntax trees. PI2 then poses interface generation as a schema mapping problem from each Difftree to a visualization that renders its results, and the variations encoded in each Difftree to interactions in the interface. Interface generation further takes the layout and screen size into account. Our user studies show that PI2 interfaces are comparable to or better than those designed by developers, and that PI2 can generate exploration interfaces that are easier to use than the state-of-the-art SQL notebook products. What's more, PI2 generates high-quality interfaces within a few seconds.}
}


@inproceedings{DBLP:conf/sigmod/FanLLL22,
	author = {Wenfei Fan and
                  Yuanhao Li and
                  Muyang Liu and
                  Can Lu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {A Hierarchical Contraction Scheme for Querying Big Graphs},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1726--1740},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517862},
	doi = {10.1145/3514221.3517862},
	timestamp = {Tue, 28 Mar 2023 16:27:38 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/FanLLL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a scheme for querying big graphs with a single machine. The scheme iteratively contracts regular structures into supernodes and builds a hierarchy of contracted graphs, until the one at the top fits into the memory. For each query class Q in use, supernodes carry synopses SQ such that queries of Q are answered by using SQ if possible, and otherwise by drilling down to the next level with decontraction of a bounded size. Moreover, we show how to adapt a variety of existing sequential (single-machine) algorithms to the hierarchy by reusing their logic and data structures. We also provide a bounded incremental algorithm to maintain the contracted graphs in response to updates, such that its cost is determined by the sizes of changes to the input and output only. Using real-life and synthetic graphs, we experimentally verify that with a single machine, the hierarchy is able to compute exact query answers when memory is as small as 7.6% of graphs, speeds up various applications by 9.8 times on average, and is even 120.1 times faster than some parallel graph systems that use 6 machines.}
}


@inproceedings{DBLP:conf/sigmod/BeharC22,
	author = {Rachel Behar and
                  Sara Cohen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Representative Query Results by Voting},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1741--1754},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517858},
	doi = {10.1145/3514221.3517858},
	timestamp = {Tue, 21 Mar 2023 20:57:32 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/BeharC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional query answering returns all answers  T  to a given query. When  T  is large, the user may be interested in viewing only a smaller subset  S  ⊆  T . Previous work has focused on finding subsets  S  that are  diverse , i.e., such that all items s,s' ∈  S  are very different one from another. This paper focuses on a complementary problem, namely finding subsets that are highly representative of the entire set of query results. Intuitively, a representative subset  S  is similar, in values and proportionality, to the entire set  T . Finding such a representative set is challenging, both conceptually, and in practice. This paper proposes a novel method of choosing a representative subset, called SimSTV, which draws inspiration from the field of voting theory. An efficient algorithm is presented, which overcomes and leverages the many differences between choosing answers in a database, and voting in a real-life election. We also provide extensions to our algorithm, e.g., to accommodate affirmative action. Experimental results show the effectiveness of our algorithm.}
}


@inproceedings{DBLP:conf/sigmod/Fernandez22,
	author = {Raul Castro Fernandez},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Protecting Data Markets from Strategic Buyers},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1755--1769},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517855},
	doi = {10.1145/3514221.3517855},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Fernandez22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing adoption of data analytics platforms and machine learning-based solutions for decision-makers creates a significant demand for datasets, which explains the appearance of data markets. In a well-functioning data market, sellers share data in exchange for money, and buyers pay for datasets that help them solve problems. The market raises sufficient money to compensate sellers and incentivize them to keep sharing datasets. This low-friction matching of sellers and buyers distributes the value of data among participants. But designing online data markets is challenging because they must account for the strategic behavior of participants. In this paper, we introduce techniques to protect data markets from strategic participants, even when the asset traded is data. We combine those techniques into a pricing algorithm specifically designed to trade data. The evaluation includes a user study and extensive simulations. Together, the evaluation demonstrates how participants strategize and the effectiveness of our techniques.}
}


@inproceedings{DBLP:conf/sigmod/AvronGGMN22,
	author = {Uri Avron and
                  Shay Gershtein and
                  Ido Guy and
                  Tova Milo and
                  Slava Novgorodov},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Automated Category Tree Construction in E-Commerce},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1770--1783},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526124},
	doi = {10.1145/3514221.3526124},
	timestamp = {Sun, 19 Jan 2025 13:27:34 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AvronGGMN22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Category trees play a central role in many web applications, enabling browsing-style information access. Building trees that reflect users' dynamic interests is, however, a challenging task, carried out by taxonomists. This manual construction leads to outdated trees as it is hard to keep track of market trends. While taxonomists can identify candidate categories, i.e. sets of items with a shared label, most such categories cannot simultaneously exist in the tree, as platforms set a bound on the number of categories an item may belong to. To address this setting, we formalize the problem of constructing a tree where the categories are maximally similar to desirable candidate categories while satisfying combinatorial requirements and provide a model that captures practical considerations. In previous work, we proved inapproximability bounds for this model. Nevertheless, in this work we provide two heuristic algorithms, and demonstrate their effectiveness over datasets from real-life e-commerce platforms, far exceeding the worst-case bounds. We also identify a natural special case, for which we devise a solution with tight approximation guarantees. Moreover, we explain how our approach facilitates continual updates, maintaining consistency with an existing tree. Finally, we propose to include in the input candidate categories derived from result sets to recent search queries to reflect dynamic user interests and trends.}
}


@inproceedings{DBLP:conf/sigmod/GuanK22,
	author = {Naiqing Guan and
                  Nick Koudas},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{FILA:} Online Auditing of Machine Learning Model Accuracy under Finite
                  Labelling Budget},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1784--1794},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517904},
	doi = {10.1145/3514221.3517904},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/GuanK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) is increasingly adopted in industrial applications. Typically, a ML pipeline is instantiated to automate the process of collecting training data, training a model, auditing the model accuracy and generating predictions. In this paper we present a sampling based approach to audit the model accuracy in ML pipelines in an online manner, with general applicability, including entity resolution. We target an online setting in which a deployed model makes predictions, which can be selectively assessed for accuracy by humans in the loop. We present a consistent adaptive stratified sampling estimator for model accuracy and propose the Finite Labels (FILA) method to allocate samples under a finite label budget. We demonstrate that under mild statistical assumptions FILA is asymptotically optimal. We analyze the variance of the estimator under a finite labelling budget and compare our approach to other applicable techniques and analytically establish the conditions under which our proposed FILA is the method of choice. We also present an algorithm based on Thompson Sampling, named FILA-Thompson, utilizing explore-exploit trade-offs to estimate model accuracy. Finally we present the results of a thorough experimental evaluation using real benchmark data sets demonstrating the practical utility (in terms of estimation accuracy and variance minimization under finite samples) of our proposals compared to other applicable approaches.}
}


@inproceedings{DBLP:conf/sigmod/MaltenbergerITR22,
	author = {Tobias Maltenberger and
                  Ivan Ilic and
                  Ilin Tolovski and
                  Tilmann Rabl},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Evaluating Multi-GPU Sorting with Modern Interconnects},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1795--1809},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517842},
	doi = {10.1145/3514221.3517842},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/MaltenbergerITR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {GPUs have become a mainstream accelerator for database operations such as sorting. Most GPU sorting algorithms are single-GPU approaches. They neither harness the full computational power nor exploit the high-bandwidth P2P interconnects of modern multi-GPU platforms. The latest NVLink 2.0 and NVLink 3.0-based NVSwitch interconnects promise unparalleled multi-GPU acceleration. So far, multi-GPU sorting has only been evaluated on systems with PCIe 3.0. In this paper, we analyze serial, parallel, and bidirectional data transfer rates to, from, and between multiple GPUs on systems with PCIe 3.0/4.0, NVLink 2.0/3.0, and NVSwitch. We measure up to 35x higher parallel P2P throughput with NVLink 3.0-based NVSwitch over PCIe 3.0. To study GPU-accelerated sorting on today's hardware, we implement a P2P-based GPU-only (P2P sort) and a heterogeneous (HET sort) multi-GPU sorting algorithm and evaluate them on three modern platforms. We observe speedups over state-of-the-art parallel CPU radix sort of up to 14x for P2P sort and 9x for HET sort. On systems with fast P2P interconnects, P2P sort outperforms HET sort up to 1.65x. Finally, we show that overlapping GPU copy/compute operations does not mitigate the transfer bottleneck when sorting large out-of-core data.}
}


@inproceedings{DBLP:conf/sigmod/MilkaiCGGPY22,
	author = {Elena Milkai and
                  Yannis Chronis and
                  Kevin P. Gaffney and
                  Zhihan Guo and
                  Jignesh M. Patel and
                  Xiangyao Yu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {How Good is My {HTAP} System?},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1810--1824},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526148},
	doi = {10.1145/3514221.3526148},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/MilkaiCGGPY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hybrid Transactional and Analytical Processing (HTAP) systems have recently gained popularity as they combine OLAP and OLTP processing to reduce administrative and synchronization costs between dedicated systems. However, there is no precise characterization of the features that distinguish a good HTAP system from a poor one. In this paper, we seek to solve this problem from the perspectives of both performance and freshness. To simultaneously capture the performance of both transactional and analytical processing, we introduce a new concept called throughput frontier, which visualizes both transactional and analytical throughput in a single 2D graph. The throughput frontier can capture information regarding the performance of each engine, the interference between the two engines, and various system design decisions. To capture how well an HTAP system supports real-time analytics, we define a freshness metric which quantifies how recent is the snapshot of the data seen by each analytical query. We also develop a practical way to measure freshness in a real system. We design a new hybrid benchmark called HATtrick which incorporates both throughput frontier and freshness as metrics. Using the benchmark, we evaluate three representative HTAP systems under various data size and system configurations and demonstrate how the metrics reveal important system characteristics and performance information.}
}


@inproceedings{DBLP:conf/sigmod/IsenkoMJJ22,
	author = {Alexander Isenko and
                  Ruben Mayer and
                  Jeffrey Jedele and
                  Hans{-}Arno Jacobsen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning
                  Preprocessing Pipelines},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1825--1839},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517848},
	doi = {10.1145/3514221.3517848},
	timestamp = {Tue, 07 May 2024 20:05:52 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/IsenkoMJJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Preprocessing pipelines in deep learning aim to provide sufficient data throughput to keep the training processes busy. Maximizing resource utilization is becoming more challenging as the throughput of training processes increases with hardware innovations (e.g., faster GPUs, TPUs, and inter-connects) and advanced parallelization techniques that yield better scalability. At the same time, the amount of training data needed in order to train increasingly complex models is growing. As a consequence of this development, data preprocessing and provisioning are becoming a severe bottleneck in end-to-end deep learning pipelines. In this paper, we provide an in-depth analysis of data preprocessing pipelines from four different machine learning domains. We introduce a new perspective on efficiently preparing datasets for end-to-end deep learning pipelines and extract individual trade-offs to optimize throughput, preprocessing time, and storage consumption. Additionally, we provide an open-source profiling library that can automatically decide on a suitable preprocessing strategy to maximize throughput. By applying our generated insights to real-world use-cases, we obtain an increased throughput of 3x to 13x compared to an untuned system while keeping the pipeline functionally identical. These findings show the enormous potential of data pipeline tuning.}
}


@inproceedings{DBLP:conf/sigmod/Al-SayehMJPS22,
	author = {Hani Al{-}Sayeh and
                  Bunjamin Memishi and
                  Muhammad Attahir Jibril and
                  Marcus Paradies and
                  Kai{-}Uwe Sattler},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Juggler: Autonomous Cost Optimization and Performance Prediction of
                  Big Data Applications},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1840--1854},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517892},
	doi = {10.1145/3514221.3517892},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Al-SayehMJPS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed in-memory processing frameworks accelerate iterative workloads by caching suitable datasets in memory rather than recomputing them in each iteration. Selecting appropriate datasets to cache as well as allocating a suitable cluster configuration for caching these datasets play a crucial role in achieving optimal performance. In practice, both are tedious, time-consuming tasks and are often neglected by end users, who are typically not aware of workload semantics, sizes of intermediate data, and cluster specification. To address these problems, we present Juggler, an end-to-end framework, which autonomously selects appropriate datasets for caching and recommends a correspondingly suitable cluster configuration to end users, with the aim of achieving optimal execution time and cost. We evaluate Juggler on various iterative, real-world, machine learning applications. Compared with our baseline, Juggler reduces execution time to 25.1% and cost to 58.1%, on average, as a result of selecting suitable datasets for caching. It recommends optimal cluster configuration in 50% of cases and near-to-optimal configuration in the remaining cases. Moreover, Juggler achieves an average performance prediction accuracy of 90%.}
}


@inproceedings{DBLP:conf/sigmod/AlnegheimishLSB22,
	author = {Sarah Alnegheimish and
                  Dongyu Liu and
                  Carles Sala and
                  Laure Berti{-}{\'{E}}quille and
                  Kalyan Veeramachaneni},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Sintel: {A} Machine Learning Framework to Extract Insights from Signals},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1855--1865},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517910},
	doi = {10.1145/3514221.3517910},
	timestamp = {Mon, 03 Mar 2025 21:21:48 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AlnegheimishLSB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The detection of anomalies in time series data is a critical task with many monitoring applications. Existing systems often fail to encompass an end-to-end detection process, to facilitate comparative analysis of various anomaly detection methods, or to incorporate human knowledge to refine output. This precludes current methods from being used in real-world settings by practitioners who are not ML experts. In this paper, we introduce Sintel, a machine learning framework for end-to-end time series tasks such as anomaly detection. The framework uses state-of-the-art approaches to support all steps of the anomaly detection process. Sintel logs the entire anomaly detection journey, providing detailed documentation of anomalies over time. It enables users to analyze signals, compare methods, and investigate anomalies through an interactive visualization tool, where they can annotate, modify, create, and remove events. Using these annotations, the framework leverages human knowledge to improve the anomaly detection pipeline. We demonstrate the usability, efficiency, and effectiveness of Sintel through a series of experiments on three public time series datasets, and through a real-world use case with spacecraft experts. Sintel's framework, code, and datasets are open-sourced at https://github.com/sintel-dev/}
}


@inproceedings{DBLP:conf/sigmod/WuDH0CO22,
	author = {Yuncheng Wu and
                  Tien Tuan Anh Dinh and
                  Guoyu Hu and
                  Meihui Zhang and
                  Yeow Meng Chee and
                  Beng Chin Ooi},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Serverless Data Science - Are We There Yet? {A} Case Study of Model
                  Serving},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1866--1875},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517905},
	doi = {10.1145/3514221.3517905},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/WuDH0CO22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) is an important part of modern data science applications. Data scientists today have to manage the end-to-end ML life cycle that includes both model training and model serving, the latter of which is essential, as it makes their works available to end-users. Systems of model serving require high performance, low cost, and ease of management. Cloud providers are already offering model serving choices, including managed services and self-rented servers. Recently, serverless computing, whose advantages include high elasticity and a fine-grained cost model, brings another option for model serving. Our goal in this paper is to examine the viability of serverless as a mainstream model serving platform. To this end, we first conduct a comprehensive evaluation of the performance and cost of serverless against other model serving systems on Amazon Web Service and Google Cloud Platform. We find that serverless outperforms many cloud-based alternatives. Further, there are settings under which it even achieves better performance than GPU-based systems. Next, we present the design space of serverless model serving, which comprises multiple dimensions, including cloud platforms, serving runtimes, and other function-specific parameters. For each dimension, we analyze the impact of different choices and provide suggestions for data scientists to better utilize serverless model serving. Finally, we discuss challenges and opportunities in building a more practical serverless model serving system.}
}


@inproceedings{DBLP:conf/sigmod/GuoHH22,
	author = {Peizhen Guo and
                  Bo Hu and
                  Wenjun Hu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Sommelier: Curating {DNN} Models for the Masses},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1876--1890},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526173},
	doi = {10.1145/3514221.3526173},
	timestamp = {Sat, 30 Sep 2023 09:56:33 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/GuoHH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning model repositories are indispensable in machine learning ecosystems today to facilitate model reuse. However, existing model repositories provide a bare-bone interface for model retrieval. The onus is on the user to profile and select from potentially hundreds of choices, barely relieving an average user of the expertise required to design the model in the first place. In this paper, we present Sommelier, an indexing and query system above typical DNN model repositories to interface directly with inference serving or other use cases. Given a desirable accuracy target and resource budget for an inference task category, Sommelier automatically searches through the repository for the most suitable model, without requiring manual profiling from the user. Motivated by manual iterative model search processes and typical model design strategies that generate model variants or models with common segments, Sommelier organizes DNN models based on their semantic correlation, defined as the probability of models producing the same results. This is further combined with a resource index based on relative resource consumption. Sommelier is implemented as a standalone query engine that can interface with an existing repository such as TF-Hub. A case study of 163 models in TF-Hub highlights the extent of model correlation across different model series, suggesting the best candidate model can easily evade manual profiling. Extensive evaluation shows that Sommelier returns the ideal model for over 95% of the queries; When interfaced with an inference server, Sommelier can reduce the 90th percentile tail latency of inference tasks by a factor of 6 via automatic model switching, far more than typical scale-out system optimizations.}
}


@inproceedings{DBLP:conf/sigmod/HanL022,
	author = {Donghyoung Han and
                  Jongwuk Lee and
                  Min{-}Soo Kim},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {FuseME: Distributed Matrix Computation Engine based on Cuboid-based
                  Fused Operator and Plan Generation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1891--1904},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517895},
	doi = {10.1145/3514221.3517895},
	timestamp = {Sun, 19 Jan 2025 13:27:20 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/HanL022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Operator fusion is essentially and widely used in a large number of matrix computation systems in science and industry. The existing distributed operator fusion methods focus on only either low communication cost with the risk of out of memory or large-scale processing with high communication cost. We propose a distributed elastic fused operator called Cuboid-based Fused Operator (CFO) that achieves both low communication cost and large-scale processing. We also propose a novel fusion plan generator called Cuboid-based Fusion plan Generator (CFG) that finds a fusion plan to fuse more operators including large-scale matrix multiplication. We implement a fast distributed matrix computation engine called FuseME by integrating both CFO and CFG seamlessly. FuseME outperforms the state-of-the-art systems including SystemDS by orders of magnitude.}
}


@inproceedings{DBLP:conf/sigmod/HuGH22,
	author = {Bo Hu and
                  Peizhen Guo and
                  Wenjun Hu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Video-zilla: An Indexing Layer for Large-Scale Video Analytics},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1905--1919},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517840},
	doi = {10.1145/3514221.3517840},
	timestamp = {Sat, 30 Sep 2023 09:56:33 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/HuGH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pervasive deployment of surveillance cameras today poses enormous scalability challenges to video analytics systems operating over many camera feeds. Currently, there are few indexing tools to organize video feeds beyond what is provided by a standard file system. Recent video analytic systems implement application-specific frame profiling and sampling techniques to reduce the number of raw videos processed, leveraging frame-level redundancy or manually labeled spatial-temporal correlation between cameras. This paper presents Video-zilla, a standalone indexing layer between video query systems and a video store to organize video data. We propose a video data unit abstraction, semantic video stream (SVS), based on a notion of distance between objects in the video. SVS implicitly captures scenes, which is missing from current video content characterization and a middle ground between individual frames and an entire camera feed. We then build a hierarchical index that exposes the semantic similarity both within and across camera feeds, such that Video-zilla can quickly cluster video feeds based on their content semantics without manual labeling. We implement and evaluate Video-zilla in three use cases: object identification queries, clustering for training specialized DNNs, and archival services. In all three cases, Video-zilla reduces the time complexity of inter-camera video analytics from linear with the number of cameras to sublinear, and reduces query resource usage by up to 14× compared to using frame-level or spatial-temporal similarity built into existing query systems.}
}


@inproceedings{DBLP:conf/sigmod/LiLK22,
	author = {Beibin Li and
                  Yao Lu and
                  Srikanth Kandula},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Warper: Efficiently Adapting Learned Cardinality Estimators to Data
                  and Workload Drifts},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1920--1933},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526179},
	doi = {10.1145/3514221.3526179},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/LiLK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent learned cardinality estimation (CE) models are vulnerable when query predicates or the underlying datasets drift from what the models were trained upon. We propose a system Warper that accelerates model adaptation to drifts; Warper generates additional queries when limited examples are available from the new workload and carefully picks which queries to use to update the CE model. We show that Warper can be used to adapt different CE models including ones that support queries over single tables and join expressions. Experiments with different drifts suggest that Warper has a small computational cost and adapts much faster compared to state-of-the-art solutions. We also show that faster model adaptation improves query performance by shortening the period for which imperfect query plans are picked by a query optimizer due to incorrect cardinality estimates.}
}


@inproceedings{DBLP:conf/sigmod/KangGBHZ22,
	author = {Daniel Kang and
                  John Guibas and
                  Peter D. Bailis and
                  Tatsunori Hashimoto and
                  Matei Zaharia},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{TASTI:} Semantic Indexes for Machine Learning-based Queries over
                  Unstructured Data},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1934--1947},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517897},
	doi = {10.1145/3514221.3517897},
	timestamp = {Mon, 05 Feb 2024 20:26:57 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/KangGBHZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unstructured data (e.g., video or text) is now commonly queried by using computationally expensive deep neural networks or human labelers to produce structured information, e.g., object types and positions in video. To accelerate queries, many recent systems (e.g., BlazeIt, NoScope, Tahoma, SUPG, etc.) train a query-specific proxy model to approximate a large target labelers (i.e., these expensive neural networks or human labelers). These models return proxy scores that are then used in query processing algorithms. Unfortunately, proxy models usually have to be trained per query and require large amounts of annotations from the target labelers. In this work, we develop an index (trainable semantic index, TASTI) that simultaneously removes the need for per-query proxies and is more efficient to construct than prior indexes. TASTI accomplishes this by leveraging semantic similarity across records in a given dataset. Specifically, it produces embeddings for each record such that records with close embeddings have similar target labeler outputs. TASTI then generates high-quality proxy scores via embeddings without needing to train a per-query proxy. These scores can be used in existing proxy-based query processing algorithms (e.g., for aggregation, selection, etc.). We theoretically analyze TASTI and show that a low embedding training error guarantees downstream query accuracy for a natural class of queries. We evaluate TASTI on five video, text, and speech datasets, and three query types. We show that TASTI's indexes can be 10x less expensive to construct than generating annotations for current proxy-based methods, and accelerate queries by up to 24x.}
}


@inproceedings{DBLP:conf/sigmod/OlteanuVZ22,
	author = {Dan Olteanu and
                  Nils Vortmeier and
                  Dorde Zivanovic},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Givens {QR} Decomposition over Relational Databases},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1948--1961},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526144},
	doi = {10.1145/3514221.3526144},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/OlteanuVZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces Figaro, an algorithm for computing the upper-triangular matrix in the QR decomposition of the matrix defined by the natural join over a relational database. The QR decomposition lies at the core of many linear algebra techniques and their machine learning applications, including: the matrix inverse; the least squares; the singular value decomposition; eigenvalue problems; and the principal component analysis. Figaro's main novelty is that it pushes the QR decomposition past the join. This leads to several desirable properties. For acyclic joins, it takes time linear in the database size and independent of the join size. Its execution is equivalent to the application of a sequence of Givens rotations proportional to the join size. Its number of rounding errors relative to the classical QR decomposition algorithms is on par with the input size relative to the join size. In experiments with real-world and synthetic databases, Figaro outperforms both in runtime performance and accuracy the LAPACK libraries openblas and Intel MKL by a factor proportional to the gap between the join output and input sizes.}
}


@inproceedings{DBLP:conf/sigmod/DerakhshanMKRM22,
	author = {Behrouz Derakhshan and
                  Alireza Rezaei Mahdiraji and
                  Zoi Kaoudi and
                  Tilmann Rabl and
                  Volker Markl},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Materialization and Reuse Optimizations for Production Data Science
                  Pipelines},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1962--1976},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526186},
	doi = {10.1145/3514221.3526186},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/DerakhshanMKRM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many companies and businesses train and deploy machine learning (ML) pipelines to answer prediction queries. In many applications, new training data continuously becomes available. A typical approach to ensure that ML models are up-to-date is to retrain the ML pipelines following a schedule, e.g., every day on the last seven days of data. Several use cases, such as A/B testing and ensemble learning, require many pipelines to be deployed in parallel. Existing solutions train each pipeline separately, which generates redundant data processing. Our goal is to eliminate redundant data processing in such scenarios using materialization and reuse optimizations. Our solution comprises of two main parts. First, we propose a materialization algorithm that given a storage budget, materializes the subset of the artifacts to minimize the run time of the subsequent executions. Second, we design a reuse algorithm to generate an execution plan by combining the pipelines into a directed acyclic graph (DAG) and reusing the materialized artifacts when appropriate. Our experiments show that our solution can reduce the training time by up to an order of magnitude for different deployment scenarios.}
}


@inproceedings{DBLP:conf/sigmod/YangSHX22,
	author = {Renchi Yang and
                  Jieming Shi and
                  Keke Huang and
                  Xiaokui Xiao},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Scalable and Effective Bipartite Network Embedding},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1977--1991},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517838},
	doi = {10.1145/3514221.3517838},
	timestamp = {Sun, 09 Feb 2025 10:52:30 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/YangSHX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a bipartite graph G consisting of inter-set weighted edges connecting the nodes in two disjoint sets U and V, bipartite network embedding (BNE) maps each node ui in U and vj in V to compact embedding vectors that capture the hidden topological features surrounding the nodes, to facilitate downstream tasks. Effective BNE should preserve not only the direct connections between nodes but also the multi-hop relationships formed alternately by the two types of nodes in G, which can incur prohibitive overheads, especially on massive bipartite graphs with millions of nodes and billions of edges. Existing solutions are hardly scalable to massive bipartite graphs, and often produce low-quality results. This paper proposes GEBE, a generic BNE framework achieving state-of-the-art performance on massive bipartite graphs, via four main algorithmic designs. First, we present two generic measures to capture the multi-hop similarity/proximity between homogeneous/heterogeneous nodes respectively, and the measures can be instantiated with three popular probability distributions, including Poisson, Geometric, and Uniform distributions. Second, GEBE formulates a novel and unified BNE objective to preserve the two measures of all possible node pairs. Third, GEBE includes several efficiency designs to get high-quality embeddings on massive graphs. Finally, we observe that GEBE achieves the best performance when instantiating MHS and MHP using a Poisson distribution, and thus, we further develop GEBEp based on Poisson-instantiated MHS and MHP, with non-trivial efficiency optimizations. Extensive experiments, comparing 15 competitors on 10 real datasets, demonstrate that our solutions, especially GEBEp, obtain superior result utility than all competitors for top-N recommendation and link prediction, while being up to orders of magnitude faster.}
}


@inproceedings{DBLP:conf/sigmod/0001Y22,
	author = {Yikai Zhang and
                  Jeffrey Xu Yu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Relative Subboundedness of Contraction Hierarchy and Hierarchical
                  2-Hop Index in Dynamic Road Networks},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {1992--2005},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517875},
	doi = {10.1145/3514221.3517875},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0001Y22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computing the shortest path for any two given vertices is an important problem in road networks. Since real road networks are dynamically updated due to real-time traffic conditions and it is costly to recompute the oracle O in use from scratch, O needs to be updated to reflect the changes in the network using incremental algorithms. An incremental algorithm is said to be bounded if its cost is polynomial in |CHANGED|, where CHANGED comprises both the changes to the graph and the resulting changes to O. An incremental problem is bounded if it has a bounded algorithm and is unbounded otherwise. We study the boundedness of the incremental counterparts of two state-of-the-art oracles, namely contraction hierarchy (CH) and hierarchical 2-hop index (H2H). We prove that under specific computational models, both CH and H2H are unbounded to maintain. Despite this fact, we introduce relative subboundedness as an alternative to boundedness. We prove that the state-of-the-art incremental algorithm for CH is relatively subbounded, and moreover, we propose a relatively subbounded algorithm for H2H. Our experimental study on real road networks shows that the algorithms studied are faster than recomputing from scratch even when 10% of the index needs to be updated, thereby verifying the effectiveness of relative subboundedness.}
}


@inproceedings{DBLP:conf/sigmod/LiZCLHY22,
	author = {Xiaofan Li and
                  Rui Zhou and
                  Lu Chen and
                  Chengfei Liu and
                  Qiang He and
                  Yun Yang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {One Set to Cover All Maximal Cliques Approximately},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2006--2019},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517881},
	doi = {10.1145/3514221.3517881},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/LiZCLHY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Maximal clique, the most cohesive structure in a graph, has a broad range of applications, e.g., community detection, bioinformatics, anomaly detection, and graph visualization. However, the sheer number of maximal cliques brings the challenge to fully examine them all. In addition, the omnipresent overlaps between cliques imply that it may not be necessary to process every maximal clique, since many vertices are shared in multiple cliques. A real example is that, in commercial advertising, a small group of individuals who participate in different communities can help spread an advertisement across all the communities. Inspired by this observation, we study the problem of finding a τ-cover, which is a subset of vertices in a graph. This subset overlaps with each maximal clique by no less than τ, where τ is a threshold reflecting the user's requirement. We prove the NP-hardness and the non-submodularity of finding a minimum τ-cover. As a result, to find a small τ-cover as best effort, we propose three methods: MCCb, MCC, and EMCC. MCCb is a baseline that adds vertices into the cover while doing clique enumeration until the coverage requirement is satisfied. MCC decides whether to add a vertex with more caution by evaluating the increment of coverage lower bound with O(1) time complexity. EMCC is a randomized algorithm built on an elegant adaptive sampling, which further achieves cover conciseness by relaxing the coverage requirement in a statistical manner. Extensive experiments show that MCC (1.3 ∽ 2.5 × faster) produces a cover whose size is 1/2 of MCCb, and EMCC (2 ∽ 5 × faster) averagely produces a cover whose size is one order of magnitude smaller vs. MCCb.}
}


@inproceedings{DBLP:conf/sigmod/FarhanWK22,
	author = {Muhammad Farhan and
                  Qing Wang and
                  Henning Koehler},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {BatchHL: Answering Distance Queries on Batch-Dynamic Networks at Scale},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2020--2033},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517883},
	doi = {10.1145/3514221.3517883},
	timestamp = {Tue, 21 Mar 2023 20:57:32 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/FarhanWK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many real-world applications operate on dynamic graphs that undergo rapid changes in their topological structure over time. However, it is challenging to design dynamic algorithms that are capable of supporting such graph changes efficiently. To circumvent the challenge, we propose a batch-dynamic framework for answering distance queries, which combines offline labelling and online searching to leverage the advantages from both sides - accelerating query processing through a partial distance labelling that is of limited size but provides a good approximation to bound online searches. We devise batch-dynamic algorithms to dynamize a distance labelling efficiently in order to reflect batch updates on the underlying graph. In addition to providing theoretical analysis for the correctness, labelling minimality, and computational complexity, we have conducted experiments on 14 real-world networks to empirically verify the efficiency and scalability of the proposed algorithms.}
}


@inproceedings{DBLP:conf/sigmod/DaiLLCW22,
	author = {Qiangqiang Dai and
                  Rong{-}Hua Li and
                  Meihao Liao and
                  Hongzhi Chen and
                  Guoren Wang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Fast Maximal Clique Enumeration on Uncertain Graphs: {A} Pivot-based
                  Approach},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2034--2047},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526143},
	doi = {10.1145/3514221.3526143},
	timestamp = {Sun, 19 Jan 2025 13:27:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/DaiLLCW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Maximal clique enumeration on uncertain graphs is a fundamental problem in uncertain graph analysis. In this paper, we study a problem of enumerating all maximal (k,n)-cliques on an uncertain graph G, where a vertex set H of G is a maximal (k,n)-clique if (1) H (|H| ≥ k) is a clique with probability no less than n, and (2) H is a maximal vertex set satisfying (1). The state-of-the-art algorithms for enumerating all maximal (k,n)-cliques are based on a set enumeration technique which are often very costly. This is because the set enumeration based techniques may explore all subsets of a maximal (k,n)-clique, thus resulting in many unnecessary computations. To overcome this issue, we propose several novel and efficient pivot-based algorithms to enumerate all maximal (k,n)-cliques based on a newly-developed pivot-based pruning principle. Our pivot-based pruning principle is very general which can be applied to speed up the enumeration of any maximal subgraph that satisfies a hereditary property. Here the hereditary property means that if a maximal subgraph H satisfies a property P, any subgraph of H also meets P. To the best of our knowledge, our work is the first to systematically explore the idea of pivot for maximal clique enumeration on uncertain graphs. In addition, we also develop a nontrivial size-constraint based pruning technique and a new graph reduction technique to further improve the efficiency. Extensive experiments on nine real-world graphs demonstrate the efficiency, effectiveness, and scalability of the proposed algorithms.}
}


@inproceedings{DBLP:conf/sigmod/LiaoLDW22,
	author = {Meihao Liao and
                  Rong{-}Hua Li and
                  Qiangqiang Dai and
                  Guoren Wang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Efficient Personalized PageRank Computation: {A} Spanning Forests
                  Sampling Based Approach},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2048--2061},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526140},
	doi = {10.1145/3514221.3526140},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/LiaoLDW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computing the personalized PageRank vector is a fundamental problem in graph analysis. In this paper, we propose several novel algorithms to efficiently compute the personalized PageRank vector with a decay factor α based on an interesting connection between the personalized PageRank values and the weights of random spanning forests of the graph. Such a connection is derived based on a newly-developed matrix forest theorem on graphs. Based on this, we present an efficient spanning forest sampling algorithm via simulating loop-erased α-random walks to estimate the personalized PageRank vector. Compared to all existing methods, a striking feature of our approach is that its performance is insensitive w.r.t. (with respect to) the parameter α. As a consequence, our algorithm is often much faster than the state-of-the-art algorithms when α is small, which is the demanding case for many graph analysis tasks. We show that our technique can significantly improve the efficiency of the state-of-the-art algorithms for answering two well-studied personalized PageRank queries, including single source query and single target query. Extensive experiments on seven large real-world graphs demonstrate the efficiency of the proposed method.}
}


@inproceedings{DBLP:conf/sigmod/0002FMT22,
	author = {Andrea Rossi and
                  Donatella Firmani and
                  Paolo Merialdo and
                  Tommaso Teofili},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Explaining Link Prediction Systems based on Knowledge Graph Embeddings},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2062--2075},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517887},
	doi = {10.1145/3514221.3517887},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0002FMT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Link Prediction (LP) aims at tackling Knowledge Graph incompleteness by inferring new, missing facts from the already known ones. The rise of novel Machine Learning techniques has led researchers to develop LP models that represent Knowledge Graph elements as vectors in an embedding space. These models can outperform traditional approaches and they can be employed in multiple downstream tasks; nonetheless, they tend to be opaque, and are mostly regarded as black boxes. Their lack of interpretability limits our understanding of their inner mechanisms, and undermines the trust that users can place in them. In this paper, we propose the novel Kelpie explainability framework. Kelpie can be applied to any embedding-based LP models independently from their architecture, and it explains predictions by identifying the combinations of training facts that have enabled them. Kelpie can extract two complementary types of explanations, that we dub necessary and sufficient. We describe in detail both the structure and the implementation details of Kelpie, and thoroughly analyze its performance through extensive experiments. Our results show that Kelpie significantly outperforms baselines across almost all scenarios.}
}


@inproceedings{DBLP:conf/sigmod/HuSGAY22,
	author = {Xiao Hu and
                  Stavros Sintos and
                  Junyang Gao and
                  Pankaj K. Agarwal and
                  Jun Yang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Computing Complex Temporal Join Queries Efficiently},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2076--2090},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517893},
	doi = {10.1145/3514221.3517893},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/HuSGAY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies multi-way join queries over temporal data, where each tuple is associated with a valid time interval indicating when the tuple is valid. A temporal join requires that joining tuples' valid intervals intersect. Previous work on temporal joins has focused on joining two relations, but pairwise processing is often inefficient because it may generate unnecessarily large intermediate results. This paper investigates how to efficiently process complex temporal joins involving multiple relations. We also consider a useful extension, durable temporal joins, which further selects results with long enough valid intervals so they are not merely transient patterns. We classify temporal join queries into different classes based on their computational complexity. We identify the class of r-hierarchical joins and show that a linear-time algorithm exists for a temporal join if and only it is r-hierarchical (assuming the 3SUM conjecture holds). We further propose output-sensitive algorithms for non-r-hierarchical joins. We implement our algorithms and evaluate them on both synthetic and real datasets.}
}


@inproceedings{DBLP:conf/sigmod/BastaniM22,
	author = {Favyen Bastani and
                  Samuel Madden},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{OTIF:} Efficient Tracker Pre-processing over Large Video Datasets},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2091--2104},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517835},
	doi = {10.1145/3514221.3517835},
	timestamp = {Tue, 08 Aug 2023 10:54:18 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/BastaniM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Performing analytics tasks over large-scale video datasets is increasingly common in a wide range of applications, from traffic planning to sports analytics. These tasks generally involve object detection and tracking operations that require pre-processing the video through expensive machine learning models. To address this cost, several video query optimizers have recently been proposed. Broadly, these methods trade large reductions in pre-processing cost for increases in query execution cost: during query execution, they apply query-specific machine learning operations over portions of the video dataset. Although video query optimizers reduce the overall cost of executing a single query over large video datasets compared to naive object tracking methods, executing several queries over the same video remains cost-prohibitive; moreover, the high per-query latency makes these systems unsuitable for exploratory analytics where fast response times are crucial. In this paper, we present OTIF, a video pre-processor that efficiently extracts all object tracks from large-scale video datasets. By integrating several optimizations under a joint parameter tuning framework, OTIF is able to extract all object tracks from video as fast as existing video query optimizers can execute just one single query. In contrast to the outputs of video query optimizers, OTIF's outputs are general-purpose object tracks that can be used to execute many queries with sub-second latencies. We compare OTIF against three recent video query optimizers, as well as several general-purpose object detection and tracking techniques, and find that, across multiple datasets, OTIF provides a 6x to 25x average reduction in the overall cost to execute five queries over the same video.}
}


@inproceedings{DBLP:conf/sigmod/HeC22,
	author = {Wenjia He and
                  Michael J. Cafarella},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Controlled Intentional Degradation in Analytical Video Systems},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2105--2119},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517899},
	doi = {10.1145/3514221.3517899},
	timestamp = {Mon, 17 Apr 2023 15:08:44 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/HeC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is increasingly affordable for governments to collect video data of public locations. This video can be used for a range of broadly valuable analytical tasks, such as counting traffic, measuring commerce, or detecting accidents. Governments also have a range of policy goals --- preserving privacy, reducing bandwidth use, and legal compliance --- that may be obtained by degrading the video at some potential cost to analytical accuracy. Ideally, public administrators could employ controlled intentional video degradation to achieve policy goals while still obtaining the required analytical accuracy. Unfortunately, the optimal amount of induced degradation is data- and query-dependent, and so is difficult to determine even when public policy preferences are well-known. We propose a video degradation-accuracy profiling model for the problem of controlling the appropriate amount of degradation. It offers administrators a profile that illustrates the tradeoff between increased analytical accuracy and increased amounts of degradation. Computing the true tradeoff curves requires full access to the non-degraded video stream, so a primary technical contribution of this work lies in methods for accurately approximating the curves with only limited information. In addition, we propose a profile repair policy to further improve tradeoff curves' accuracy. We describe our prototype system, Smokescreen, plus experiments on two video datasets, two detection models and four aggregate query types. Compared with competing methods, we show our upper bound estimation of analytical error is up to 155% tighter, and Smokescreen enables 88% more accurate tradeoffs.}
}


@inproceedings{DBLP:conf/sigmod/ChanUCX22,
	author = {Tsz Nam Chan and
                  Leong Hou U and
                  Byron Choi and
                  Jianliang Xu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{SLAM:} Efficient Sweep Line Algorithms for Kernel Density Visualization},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2120--2134},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517823},
	doi = {10.1145/3514221.3517823},
	timestamp = {Sat, 30 Sep 2023 09:56:33 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/ChanUCX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Kernel Density Visualization (KDV) has been extensively used in a wide range of applications, including traffic accident hotspot detection, crime hotspot detection, disease outbreak detection, and ecological modeling. However, KDV is a computationally expensive operation, which is not scalable to large datasets (e.g., million-scale data points) and high resolution sizes (e.g., 1920 x 1080). To significantly improve the efficiency for generating KDV, we develop two efficient Sweep Line AlgorithMs (SLAM), which can theoretically reduce the time complexity for generating KDV. By incorporating the resolution-aware optimization (RAO) into SLAM, we can further achieve the lowest time complexity for generating KDV. Our extensive experiments on four large-scale real datasets (up to 4.33 million data points) show that all our methods can achieve one to two-order-of-magnitude speedup in many test cases and efficiently support KDV with exploratory operations (e.g., zooming and panning) compared with the state-of-the-art solutions.}
}


@inproceedings{DBLP:conf/sigmod/ZengT022,
	author = {Yuxiang Zeng and
                  Yongxin Tong and
                  Lei Chen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Faster and Better Solution to Embed Lp Metrics by Tree Metrics},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2135--2148},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3517831},
	doi = {10.1145/3514221.3517831},
	timestamp = {Tue, 07 May 2024 20:05:52 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/ZengT022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hierarchically Separated Tree (HST) is the most popular solution to embed a metric space into a tree metric. By using HSTs, many optimization problems, which are hard on defined metrics, become easier to get good approximation bounds with respect to the effectiveness, e.g., task assignment, trip planning, and facility location planning. Existing work focuses on constructing HSTs for arbitrary metric spaces, which makes a general-purpose algorithm take at least O(n2)-time to get tight distortion guarantees O(log(n)). Here, distortion is a prevalent measurement of HSTs' effectiveness and usability. However, we observe that (1) in many applications that HSTs are applied, only Lp metrics are used (e.g., Euclidean space), (2) the state-of-the-art solution is still time-consuming to construct HSTs for large-scale data, and (3) distortions of existing algorithms are only satisfactory for high-dimensional data. Thus, in this paper, we are motivated to study the Embedding Lp metrics through Tree metrics (ELT) problem. We aim to design a faster algorithm than O(n2) time to construct HSTs with not only O(log(n)) distortion guarantees but also good and robust empirical results. Specifically, we first present a divide-and-conquer based general framework and prove that it has a distortion guarantee of O(log(n)). To achieve a better time complexity than O(n2), we next design two optimization techniques: reducing to nearest neighbor search (by indexing) and sampling. Finally, extensive experiments demonstrate that our algorithm DCsam outperforms the state-of-the-art algorithms by a large margin in terms of both distortion and running time.}
}


@inproceedings{DBLP:conf/sigmod/Zhang0YYL22,
	author = {Jiahao Zhang and
                  Bo Tang and
                  Man Lung Yiu and
                  Xiao Yan and
                  Keming Li},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {T-LevelIndex: Towards Efficient Query Processing in Continuous Preference
                  Space},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2149--2162},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526182},
	doi = {10.1145/3514221.3526182},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Zhang0YYL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Top-k related queries in continuous preference space (e.g., k-shortlist preference query kSPR, uncertain top-k query UTK, output-size specified utility-based query ORU) have numerous applications but are expensive to process. Existing algorithms process each query via specialized optimizations, which are difficult to generalize. In this work, we propose a novel and general index structure T-LevelIndex, which can be used to process various queries in continuous preference space efficiently. We devise efficient approaches to build the T-LevelIndex by fully exploiting the properties of continuous preference space. We conduct extensive experimental studies on both real- and synthetic- benchmarks. The results show that (i) our proposed index building approaches have low costs in terms of both space and time, and (ii) T-LevelIndex significantly outperforms specialized solutions for processing a spectrum of queries in continuous preference space, and the speedup can be two to three orders of magnitude.}
}


@inproceedings{DBLP:conf/sigmod/000122,
	author = {Ahmed Metwally},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Scaling Equi-Joins},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2163--2176},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526042},
	doi = {10.1145/3514221.3526042},
	timestamp = {Sun, 19 Jan 2025 13:27:28 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/000122.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes Adaptive-Multistage-Join (AM-Join) for scalable and fast equi-joins in distributed shared-nothing architectures. AM-Join utilizes (a) Tree-Join, a novel algorithm that scales well when the joined tables share hot keys, and (b) Broadcast-Join, the fastest-known when joining keys that are hot in only one table. Unlike the state-of-the-art algorithms, AM-Join (a) holistically solves the join-key skew problem by achieving load balancing throughout the join execution, and (b) supports all outer-join variants without record deduplication or custom table partitioning. For the best AM-Join outer-join performance, we propose Index-Broadcast-Join (IB-Join) for Small-Large outer-joins, where one table fits in memory and the other is orders of magnitude larger. IB-Join improves on the state-of-the-art outer-join algorithms. The proposed algorithms can be adopted in any shared-nothing architecture. We implemented a MapReduce version using Spark. Our evaluation shows the proposed algorithms execute significantly faster and scale to more skewed and orders-of-magnitude bigger tables when compared to the state-of-the-art algorithms.}
}


@inproceedings{DBLP:conf/sigmod/MaXZLL22,
	author = {Yunus Ma and
                  Siphrey Xie and
                  Henry Zhong and
                  Leon Lee and
                  King Lv},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {HiEngine: How to Architect a Cloud-Native Memory-Optimized Database
                  Engine},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2177--2190},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526043},
	doi = {10.1145/3514221.3526043},
	timestamp = {Sun, 19 Jan 2025 13:27:30 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/MaXZLL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fast database engines have become an essential building block in many systems and applications. Yet most of them are designed based on on-premise solutions and do not directly work in the cloud. Existing cloud-native database systems are mostly disk resident databases that follow a storage-centric design and exploit the potential of modern cloud infrastructure, such as manycore processors, large main memory and persistent memory. However, in-memory databases are infrequent and untapped. This paper presents HiEngine, Huawei's cloud-native memory-optimized in-memory database engine that endows hierarchical database architecture and fills this gap. HiEngine simultaneously (1) leverages the cloud infrastructure with reliable storage services on the compute-side (in addition to the storage tier) for fast persistence and reliability, (2) achieves main-memory database engines' high performance, and (3) retains backward compatibility with existing cloud-native database systems. HiEngine is integrated with Huawei GaussDB(for MySQL), it brings the benefits of main-memory database engines to the cloud and co-exists with disk-based engines. Compared to conventional systems, HiEngine outperforms prior storage-centric solutions by up to 7.5X and provides comparable performance to on-premise memory-optimized database engines.}
}


@inproceedings{DBLP:conf/sigmod/TaranovBMH22,
	author = {Konstantin Taranov and
                  Steve Byan and
                  Virendra J. Marathe and
                  Torsten Hoefler},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {KafkaDirect: Zero-copy Data Access for Apache Kafka over {RDMA} Networks},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2191--2204},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526056},
	doi = {10.1145/3514221.3526056},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/TaranovBMH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Apache Kafka is an open-source distributed publish-subscribe system, which is widely used in data centers for messaging between applications, log aggregation, and stream processing. The existing Kafka implementation uses TCP/IP for communication, which has various inefficiencies such as a high message dispatch cost due to OS involvement and excessive memory copies. Recently, the availability of cost-effective RDMA-capable network controllers within data centers and cloud infrastructures have encouraged many modern applications to adopt RDMA networking, which offers the potential to outperform classical TCP/IP. We introduce KafkaDirect, an extension to Apache Kafka, that uses RDMA to accelerate the three most network intensive datapaths: record production, record replication, and record consumption. In this work, we explore the design choices including which RDMA operations to use to take full advantage of offloaded communication. Our RDMA design relies on one-sided RDMA requests to attain true zero-copy communication completely avoiding the need for using intermediate buffers in Kafka servers, thereby ensuring low latency and high throughput communication. KafkaDirect can offer up to 9x increase in throughput for both Kafka producers and Kafka consumers, and can provide 4x and 50x reduction in latency for Kafka producers and Kafka consumers, respectively.}
}


@inproceedings{DBLP:conf/sigmod/ArmenatzoglouBB22,
	author = {Nikos Armenatzoglou and
                  Sanuj Basu and
                  Naga Bhanoori and
                  Mengchu Cai and
                  Naresh Chainani and
                  Kiran Chinta and
                  Venkatraman Govindaraju and
                  Todd J. Green and
                  Monish Gupta and
                  Sebastian Hillig and
                  Eric Hotinger and
                  Yan Leshinksy and
                  Jintian Liang and
                  Michael McCreedy and
                  Fabian Nagel and
                  Ippokratis Pandis and
                  Panos Parchas and
                  Rahul Pathak and
                  Orestis Polychroniou and
                  Foyzur Rahman and
                  Gaurav Saxena and
                  Gokul Soundararajan and
                  Sriram Subramanian and
                  Doug Terry},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Amazon Redshift Re-invented},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2205--2217},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526045},
	doi = {10.1145/3514221.3526045},
	timestamp = {Sun, 19 Jan 2025 13:27:30 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ArmenatzoglouBB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In 2013, AmazonWeb Services revolutionized the data warehousing industry by launching Amazon Redshift, the first fully-managed, petabyte-scale, enterprise-grade cloud data warehouse. Amazon Redshift made it simple and cost-effective to efficiently analyze large volumes of data using existing business intelligence tools. This cloud service was a significant leap from the traditional on-premise data warehousing solutions, which were expensive, not elastic, and required significant expertise to tune and operate. Customers embraced Amazon Redshift and it became the fastest growing service in AWS. Today, tens of thousands of customers use Redshift in AWS's global infrastructure to process exabytes of data daily. In the last few years, the use cases for Amazon Redshift have evolved and in response, the service has delivered and continues to deliver a series of innovations that delight customers. Through architectural enhancements, Amazon Redshift has maintained its industry-leading performance. Redshift improved storage and compute scalability with innovations such as tiered storage, multicluster auto-scaling, cross-cluster data sharing and the AQUA query acceleration layer. Autonomics have made Amazon Redshift easier to use. Amazon Redshift Serverless is the culmination of autonomics effort, which allows customers to run and scale analytics without the need to set up and manage data warehouse infrastructure. Finally, Amazon Redshift extends beyond traditional data warehousing workloads, by integrating with the broad AWS ecosystem with features such as querying the data lake with Spectrum, semistructured data ingestion and querying with PartiQL, streaming ingestion from Kinesis and MSK, Redshift ML, federated queries to Aurora and RDS operational databases, and federated materialized views.}
}


@inproceedings{DBLP:conf/sigmod/RuanKOS22,
	author = {Pingcheng Ruan and
                  Yaron Kanza and
                  Beng Chin Ooi and
                  Divesh Srivastava},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {LedgerView: Access-Control Views on Hyperledger Fabric},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2218--2231},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526046},
	doi = {10.1145/3514221.3526046},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/RuanKOS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present LedgerView -- a system that adds access control views to permissioned blockchains. The approach is motivated by an AT&T application of tracking refurbished devices. A blockchain is a decentralized tamper-resistant ledger managed by a group of peers. It is used in many applications for storing and sharing sensitive information, e.g., monetary transactions, health records, personal documents, etc. But in blockchain, all the peers see all the stored transactions, while in some applications, access to sensitive information should be limited, that is, concealed from peers and users who do not have proper access permissions. In database management systems, sets of records that are visible to some users and concealed from others are defined by views, but existing blockchain systems lack such access-control capabilities. Thus, in this paper, we introduce access-control views for Hyperledger Fabric. We present two types of views -- irrevocable and revocable, according to whether access to sensitive information can or cannot be revoked. We explain how to implement the two types of view by using cryptographic hash functions and encryption keys, and we show how to support Role-Based Access Control (RBAC). Experiments with supply chain transactions illustrate the incurred costs of the views in LedgerView, including latency, transaction rate and storage overhead.}
}


@inproceedings{DBLP:conf/sigmod/KangC0Z0CS22,
	author = {Junbin Kang and
                  Le Cai and
                  Feifei Li and
                  Xingxuan Zhou and
                  Wei Cao and
                  Songlu Cai and
                  Daming Shao},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Remus: Efficient Live Migration for Distributed Databases with Snapshot
                  Isolation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2232--2245},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526047},
	doi = {10.1145/3514221.3526047},
	timestamp = {Sun, 19 Jan 2025 13:27:33 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/KangC0Z0CS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shared-nothing, distributed databases scale transactional and analytical processing over a large data volume by spreading data across servers. However, static sharding of data across nodes makes such systems fail to timely adapt to changing workloads and struggle to obey the cloud pay-as-you-go model. Migrating shards between nodes online is a key technique to react to dynamic changes of workloads for cloud elasticity. Existing approaches introduce severely degraded performance and service interruption, resulting in SLA violation on the cloud; or they are tailor-made to deterministic databases. In this paper, we propose Remus, a new live migration approach for shared-nothing, distributed databases with snapshot isolation. Remus migrates shards between nodes with zero service interruption and minimal performance impact. This is achieved by an efficient unidirectional dual execution during migration. We implement Remus on a shared-nothing, distributed version of PolarDB-PG and evaluate it against state-of-the-art approaches using standard OLTP workloads TPC-C and YCSB, and hybrid workloads consisting of long-lived and short transactions. The results demonstrate Remus is the only effective approach to achieve the goal of zero transaction interruption, zero downtime and marginal performance impact, paving the way for applying the shared-nothing architecture to a cloud database which needs to provide elasticity while guaranteeing strict SLAs.}
}


@inproceedings{DBLP:conf/sigmod/DeutschFGHLLLMM22,
	author = {Alin Deutsch and
                  Nadime Francis and
                  Alastair Green and
                  Keith W. Hare and
                  Bei Li and
                  Leonid Libkin and
                  Tobias Lindaaker and
                  Victor Marsault and
                  Wim Martens and
                  Jan Michels and
                  Filip Murlak and
                  Stefan Plantikow and
                  Petra Selmer and
                  Oskar van Rest and
                  Hannes Voigt and
                  Domagoj Vrgoc and
                  Mingxi Wu and
                  Fred Zemke},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Graph Pattern Matching in {GQL} and {SQL/PGQ}},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2246--2258},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526057},
	doi = {10.1145/3514221.3526057},
	timestamp = {Fri, 14 Mar 2025 10:57:21 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/DeutschFGHLLLMM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As graph databases become widespread, the International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC) have approved a project to create GQL, a standard property graph query language. This complements the SQL/PGQ project, which specifies how to define graph views over a SQL tabular schema, and to run read-only queries against them. Both projects have been assigned to the ISO/IEC JTC1 SC32 working group for Database Languages, WG3, which continues to maintain and enhance SQL as a whole. This common responsibility helps enforce a policy that the identical core of both PGQ and GQL is a graph pattern matching sub-language, here termed GPML. The WG3 design process is also analyzed by an academic working group, part of the Linked Data Benchmark Council (LDBC), whose task is to produce a formal semantics of these graph data languages, which complements their standard specifications. This paper, written by members of WG3 and LDBC, presents the key elements of the GPML of SQL/PGQ and GQL in advance of the publication of these new standards.}
}


@inproceedings{DBLP:conf/sigmod/IlyasRKPQS22,
	author = {Ihab F. Ilyas and
                  Theodoros Rekatsinas and
                  Vishnu Konda and
                  Jeffrey Pound and
                  Xiaoguang Qi and
                  Mohamed A. Soliman},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Saga: {A} Platform for Continuous Construction and Serving of Knowledge
                  at Scale},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2259--2272},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526049},
	doi = {10.1145/3514221.3526049},
	timestamp = {Sun, 19 Jan 2025 13:27:33 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/IlyasRKPQS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce Saga, a next-generation knowledge construction and serving platform for powering knowledge-based applications at industrial scale. Saga follows a hybrid batch-incremental design to continuously integrate billions of facts about real-world entities and construct a central knowledge graph that supports multiple production use cases with diverse requirements around data freshness, accuracy, and availability. In this paper, we discuss the unique challenges associated with knowledge graph construction at industrial scale, and review the main components of Saga and how they address these challenges. Finally, we share lessons-learned from a wide array of production use cases powered by Saga.}
}


@inproceedings{DBLP:conf/sigmod/AleyasenMASKPMD22,
	author = {Amirhossein Aleyasen and
                  Mark Morcos and
                  Lyublena Antova and
                  Marc Sugiyama and
                  Dmitri Korablev and
                  Jozsef Patvarczki and
                  Rima Mutreja and
                  Michael Duller and
                  Florian M. Waas and
                  Marianne Winslett},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Intelligent Automated Workload Analysis for Database Replatforming},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2273--2285},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526050},
	doi = {10.1145/3514221.3526050},
	timestamp = {Sun, 19 Jan 2025 13:27:20 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AleyasenMASKPMD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Performing a detailed workload analysis is a crucial step in determining the feasibility, timeline and cost of a major data warehouse replatforming project, i.e., migration from one platform to another. A large company's data warehouse applications may include millions of queries, some of which will use features that are unsupported or have different semantics in the new warehouse, or may have poor performance there. In this paper we present qInsight, a workload analyzer that Datometry has used in data warehouse replatforming efforts for dozens of major clients. qInsight leverages Datometry's Hyper-Q to obtain insights from a workload, including SQL features and workload structural information that could not be obtained without deep query analysis. qInsight uses the identified features and a weighting scheme based on human expert judgments to assess the difficulty of rewriting each application in the workload via traditional migration methods. Datometry's clients find this information useful in planning their projects, including the order in which to migrate applications. We present a qInsight-based data warehouse usage analysis of over 1.7 billion queries from real-world workloads.}
}


@inproceedings{DBLP:conf/sigmod/ZhangCXDFZWC022,
	author = {Jiachi Zhang and
                  Shi Cheng and
                  Zhihui Xue and
                  Jianjun Deng and
                  Cuiyun Fu and
                  Wenchao Zhou and
                  Sheng Wang and
                  Changcheng Chen and
                  Feifei Li},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{ESDB:} Processing Extremely Skewed Workloads in Real-time},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2286--2298},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526051},
	doi = {10.1145/3514221.3526051},
	timestamp = {Sun, 19 Jan 2025 13:27:34 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhangCXDFZWC022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of cloud computing, efficient management of multi-tenant databases has become a vital challenge for cloud service providers. It is particularly important for Alibaba, which hosts a distributed multi-tenant database supporting one of the world's largest e-commerce platforms. It serves tens of millions of sellers as tenants, and supports transactions from hundreds of millions of buyers. The inherent imbalance of shopping preferences from the buyers essentially generates a drastically skewed workload on the database, which could create unpredictable hotspots and consequently large throughput decline and latency increase. In this paper, we present the architecture and implementation of ESDB (ElasticSearch Database), a cloud-native document-oriented database which has been running on Alibaba Cloud for 5 years as the main transaction database behind Alibaba's e-commerce platform. ESDB provides strong full-text search and retrieval capability, and proposes dynamic secondary hashing as the solution for processing extremely skewed workloads. We evaluate ESDB with both simulated workloads and real-world workloads, and demonstrate that ESDB significantly enhances write throughput and reduces the completion time of writes without sacrificing query throughput.}
}


@inproceedings{DBLP:conf/sigmod/ZhangIM0GLFHPJ22,
	author = {Wangda Zhang and
                  Matteo Interlandi and
                  Paul Mineiro and
                  Shi Qiao and
                  Nasim Ghazanfari and
                  Karlen Lie and
                  Marc T. Friedman and
                  Rafah Hosn and
                  Hiren Patel and
                  Alekh Jindal},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Deploying a Steered Query Optimizer in Production at Microsoft},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2299--2311},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526052},
	doi = {10.1145/3514221.3526052},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhangIM0GLFHPJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern analytical workloads are highly heterogeneous and massively complex, making generic out of the box query optimizers untenable for many customers and scenarios. As a result, it is important to specialize these optimizers to instances of the workloads. In this paper, we continue a recent line of work in steering a query optimizer towards better plans for a given workload, and make major strides in pushing previous research ideas to production deployment. Along the way we solve several operational challenges including, making steering actions more manageable, keeping the costs of steering within budget, and avoiding unexpected performance regressions in production. Our resulting system, QO-Advisor, essentially externalizes the query planner to a massive offline pipeline for better exploration and specialization. We discuss various aspects of our design and show detailed results over production SCOPE workloads at Microsoft, where the system is currently enabled by default.}
}


@inproceedings{DBLP:conf/sigmod/VanBenschotenAG22,
	author = {Nathan VanBenschoten and
                  Arul Ajmani and
                  Marcus Gartner and
                  Andrei Matei and
                  Aayush Shah and
                  Irfan Sharif and
                  Alexander Shraer and
                  Adam Storm and
                  Rebecca Taft and
                  Oliver Tan and
                  Andy Woods and
                  Peyton Walters},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Enabling the Next Generation of Multi-Region Applications with CockroachDB},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2312--2325},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526053},
	doi = {10.1145/3514221.3526053},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/VanBenschotenAG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A database service is required to meet the consistency, performance, and availability goals of modern applications serving a global user-base. Configuring a database deployed across multiple regions such that it fulfils these goals requires significant expertise. In this paper, we describe how CockroachDB makes this easy for developers by providing a high-level declarative syntax that allows expressing data access locality and availability goals through SQL statements. These high-level goals are then mapped to database configuration, replica placement, and data partitioning decisions. We show how all layers of the database, from the SQL Optimizer to Replication, were enhanced to support multi-region workloads. We also describe a new Transaction Management protocol that enables local, strongly consistent reads from any database replica. Finally, the paper includes an extensive evaluation demonstrating that CockroachDB's new declarative SQL syntax for multi-region clusters is easy to use and supports a variety of configuration options with different performance tradeoffs to benefit a variety of workloads. We also show that throughput scales linearly with the number of regions, and the new Transaction Management protocol reduces tail latency by over 10x compared to prior approaches.}
}


@inproceedings{DBLP:conf/sigmod/BehmPAACDGHJKLL22,
	author = {Alexander Behm and
                  Shoumik Palkar and
                  Utkarsh Agarwal and
                  Timothy Armstrong and
                  David Cashman and
                  Ankur Dave and
                  Todd Greenstein and
                  Shant Hovsepian and
                  Ryan Johnson and
                  Arvind Sai Krishnan and
                  Paul Leventis and
                  Ala Luszczak and
                  Prashanth Menon and
                  Mostafa Mokhtar and
                  Gene Pang and
                  Sameer Paranjpye and
                  Greg Rahn and
                  Bart Samwel and
                  Tom van Bussel and
                  Herman Van Hovell and
                  Maryann Xue and
                  Reynold Xin and
                  Matei Zaharia},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Photon: {A} Fast Query Engine for Lakehouse Systems},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2326--2339},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526054},
	doi = {10.1145/3514221.3526054},
	timestamp = {Sun, 19 Jan 2025 13:27:22 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/BehmPAACDGHJKLL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many organizations are shifting to a data management paradigm called the "Lakehouse," which implements the functionality of structured data warehouses on top of unstructured data lakes. This presents new challenges for query execution engines. The engine needs to provide good performance on the raw uncurated datasets that are ubiquitous in data lakes, and excellent performance on structured data stored in popular columnar file formats like Apache Parquet. Toward these goals, we present Photon, a vectorized query engine for Lakehouse environments that we developed at Databricks. Photon can outperform existing warehouses on SQL workloads and also supports the Apache Spark API. We discuss the design choices we made in Photon (e.g., vectorization vs. code generation) and describe its integration with our existing SQL and Apache Spark runtimes, its task model, and its memory manager. Photon has accelerated some customer workloads by over 10x and has recently allowed Databricks to set a new audited performance record for the official 100TB TPC-DS benchmark.}
}


@inproceedings{DBLP:conf/sigmod/ProutWVSLCBHWGS22,
	author = {Adam Prout and
                  Szu{-}Po Wang and
                  Joseph Victor and
                  Zhou Sun and
                  Yongzhu Li and
                  Jack Chen and
                  Evan Bergeron and
                  Eric N. Hanson and
                  Robert Walzer and
                  Rodrigo Gomes and
                  Nikita Shamgunov},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Cloud-Native Transactions and Analytics in SingleStore},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2340--2352},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3526055},
	doi = {10.1145/3514221.3526055},
	timestamp = {Sun, 19 Jan 2025 13:27:33 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ProutWVSLCBHWGS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The last decade has seen a remarkable rise in specialized database systems. Systems for transaction processing, data warehousing, time series analysis, full-text search, data lakes, in-memory caching, document storage, queuing, graph processing, and geo-replicated operational workloads are now available to developers. A belief has taken hold that a single general-purpose database is not capable of running varied workloads at a reasonable cost with strong performance, at the level of scale and concurrency people demand today. There is value in specialization, but the complexity and cost of using multiple specialized systems in a single application environment is becoming apparent. This realization is driving developers and IT decision makers to seek databases capable of powering a broader set of use cases when looking to adopt a new database. Hybrid transaction and analytical (HTAP) databases have been developed to try to tame some of this chaos. In this paper we introduce SinglestoreDB (S2DB), formerly called MemSQL, a distributed general-purpose SQL database designed to have the versatility to run both operational and analytical workloads with good performance. It was one of the earliest distributed HTAP databases on the market. It can scale out to efficiently utilize 100s of hosts, 1000s of cores and 10s of TBs of RAM while still providing a user experience similar to a single-host SQL database such as Oracle or SQL Server. S2DB's unified table storage runs both transactional and analytical workloads efficiently with operations like fast scans, seeks, filters, aggregations, and updates. This is accomplished through a combination of rowstore, columnstore and vectorization techniques, ability to seek efficiently into a columnstore using secondary indexes, and using in-memory rowstore buffers for recently modified data. It avoids design simplifications (i.e., only supporting batch loading, or limiting the query surface area to particular patterns of queries) that sacrifice the ability to run a broad set of workloads. Today, after 10 years of development, S2DB runs demanding production workloads for some of the world's largest financial, telecom, high-tech, and energy companies. These customers drove the product towards a database capable of running a breadth of workloads across their organizations, often replacing two or three different databases with S2DB. The design of S2DB's storage, transaction processing, and query processing were developed to maintain this versatility.}
}


@inproceedings{DBLP:conf/sigmod/TangLOLC22,
	author = {Jiawei Tang and
                  Yuyu Luo and
                  Mourad Ouzzani and
                  Guoliang Li and
                  Hongyang Chen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Sevi: Speech-to-Visualization through Neural Machine Translation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2353--2356},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520150},
	doi = {10.1145/3514221.3520150},
	timestamp = {Sun, 19 Jan 2025 13:27:21 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/TangLOLC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data visualization is a powerful tool for understating information through visual cues. However, allowing novices to create visualization artifacts for what they want to see is not easy, just as not everyone can write SQL queries. Arguably, the most natural way to specify what to visualize is through natural language or speech, similar to our daily search on Google or Apple Siri, leaving to the system the task of reasoning about what to visualize and how. In this demo, we present Sevi an end-to-end data visualization system that acts as a virtual assistant to allow novices to create visualizations through either natural language or speech. Sevi is powered by two main components: Speech2Text which is based on Google Cloud Speech-to-Text Rest API, and Text2VIS, which uses an end-to-end neural machine translation model called ncNet trained using a cross-domain benchmark called nvBench. Both ncNet and nvBench have been developed by us. We will walk the audience through two general domain datasets, one related to COVID-19 and the other on NBA player statistics, to highlight how Sevi enables novices to easily create data visualizations. Because nvBench contains Text2VIS training samples from 105 domains (e.g., sport, college, hospital, etc.), the audience can play with speech or text input with any of these domains.}
}


@inproceedings{DBLP:conf/sigmod/LaiLHZ0K22,
	author = {Ziliang Lai and
                  Chris Liu and
                  Chenxia Han and
                  Pengfei Zhang and
                  Eric Lo and
                  Ben Kao},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Everest: {A} Top-K Deep Video Analytics System},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2357--2360},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520151},
	doi = {10.1145/3514221.3520151},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/LaiLHZ0K22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The impressive accuracy of deep neural networks (DNNs) has created great demands on practical analytics over video data. Although efficient and accurate, the latest video analytic systems have not supported analytics beyond selection and aggregation queries. In data analytics, Top-K is a very important analytical operation that enables analysts to focus on the most important entities. In this demonstration, we present Everest, the first system that supports efficient and accurate Top-K video analytics. Everest ranks and identifies the most interesting frames/clips from videos with probabilistic guarantees. Furthermore, it supports user-defined functions to rank frames/clips based on different semantics using different deep vision models. Everest leverages techniques from computer vision, uncertain databases, and Top-K query processing to return results quickly.}
}


@inproceedings{DBLP:conf/sigmod/VitaglianoR00N22,
	author = {Gerardo Vitagliano and
                  Lucas Reisener and
                  Lan Jiang and
                  Mazhar Hameed and
                  Felix Naumann},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Mondrian: Spreadsheet Layout Detection},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2361--2364},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520152},
	doi = {10.1145/3514221.3520152},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/VitaglianoR00N22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spreadsheet datasets are valuable sources of data, but often ill-suited for machine consumption. Their unstructured nature allows users to arrange data and metadata freely in a human-readable format, often in canvas-like layouts. To extract their content, data practitioners need to resort to manual inspection and run cumbersome preparation pipelines. The Mondrian system assists users in identifying and handling multiregion layout templates: spreadsheet layouts composed of independent regions that appear repeatedly across different files. Mondrian comprises an automated approach to detect multiple regions within a single file and an algorithm that leverages mapping region layouts to graphs to compute layout similarity and identify templates. Users interact with Mondrian through a web-based visual interface, that serves as a practical toolkit to handle collections of multiregion spreadsheets and enables their automated preparation.}
}


@inproceedings{DBLP:conf/sigmod/TaoC022,
	author = {Jeffrey Tao and
                  Yiru Chen and
                  Eugene Wu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Demonstration of {PI2:} Interactive Visualization Interface Generation
                  for {SQL} Analysis in Notebook},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2365--2368},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520153},
	doi = {10.1145/3514221.3520153},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/TaoC022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We demonstrate PI2, the first notebook extension that can automatically generate interactive visualization interfaces during SQL-based analyses.}
}


@inproceedings{DBLP:conf/sigmod/RazmadzeASDM22,
	author = {Kathy Razmadze and
                  Yael Amsterdamer and
                  Amit Somech and
                  Susan B. Davidson and
                  Tova Milo},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {SubTab: Data Exploration with Informative Sub-Tables},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2369--2372},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520154},
	doi = {10.1145/3514221.3520154},
	timestamp = {Sun, 06 Oct 2024 21:14:20 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/RazmadzeASDM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We demonstrate SubTab, a framework for creating small, informative sub-tables of large data tables to speed up data exploration. Given a table with n rows and m columns where n and m are large, SubTab creates a sub-table T_sub with k<n rows and l<m columns, i.e. a subset of k rows of the table projected over a subset of l columns. The rows and columns are chosen as representatives of prominent data patterns within and across columns in the input table. SubTab can also be used for query results, enabling the user to quickly understand the results and determine subsequent queries.}
}


@inproceedings{DBLP:conf/sigmod/DavidsonDFKKM22,
	author = {Susan B. Davidson and
                  Daniel Deutch and
                  Nave Frost and
                  Benny Kimelfeld and
                  Omer Koren and
                  Mika{\"{e}}l Monet},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {ShapGraph: An Holistic View of Explanations through Provenance Graphs
                  and Shapley Values},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2373--2376},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520172},
	doi = {10.1145/3514221.3520172},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/DavidsonDFKKM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Explaining query results is an essential tool for enhancing the transparency and quality of data processing, and has been extensively studied in recent years. In particular, Data Provenance -- the tracking of transformations that data undergoes in query evaluation -- has been shown to be a key component of explanations. A hurdle that remains is that data provenance itself is often too large and complex to be presented in its entirety. To that end, we propose to leverage novel advancements on quantifying and computing the contributions of individual input tuples to query answers, based on the game-theoretic notion of the Shapley value. Our proposed prototype solution, called ShapGraph, combines the global view of explanations through provenance graphs with a local quantification of contributions through Shapley values. The graphical interface allows users to switch between and combine these two views to obtain a deeper understanding of the most influential parts of the database and how they interact to yield query answers.}
}


@inproceedings{DBLP:conf/sigmod/PaviaKPG22,
	author = {Sophie Pavia and
                  Rituparna Khan and
                  Anna Pyayt and
                  Michael N. Gubanov},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Simplifying Access to Large-scale Structured Datasets by Meta-Profiling
                  with Scalable Training Set Enrichment},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2377--2380},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520156},
	doi = {10.1145/3514221.3520156},
	timestamp = {Sun, 19 Jan 2025 13:27:26 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/PaviaKPG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accessing large-scale structured datasets such as WDC [21], having millions of tables coming from hundreds of thousands of sources is very challenging [11, 13, 14, 30, 31]. Even if one topic (e.g. Job postings) is of interest, Jobs tables in different sources have hundreds of different schemas, which significantly complicates both finding and querying them. Here we demonstrate our scalable Meta-data profiler, capable of constructing a standardized interface to a topic of interest in large-scale structured datasets using Deep-Learning and our new unsupervised, scalable training set enrichment algorithm. This interface, called Meta-profile represents a meta-data summary per each topic, representative of the entire dataset. It helps data scientists and end users get access to all relevant topical tables, even in ultra large-scale datasets such as WDC, which would be very difficult or impossible otherwise [22, 31].}
}


@inproceedings{DBLP:conf/sigmod/YuanCBYCH22,
	author = {Zifeng Yuan and
                  Huey{-}Eng Chua and
                  Sourav S. Bhowmick and
                  Zekun Ye and
                  Byron Choi and
                  Wook{-}Shin Han},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{PLAYPEN:} Plug-and-Play Visual Graph Query Interfaces for Top-down
                  and Bottom-Up Search on Large Networks},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2381--2384},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520157},
	doi = {10.1145/3514221.3520157},
	timestamp = {Sun, 19 Jan 2025 13:27:23 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/YuanCBYCH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Visual graph query interfaces (VQI) facilitate non-programmers to query graph data effortlessly. The construction of these interfaces for large networks is typically not data-driven. That is, they do not exploit the underlying networks to automatically generate the contents of various panels of a VQI. Such data-driven construction has several benefits such as facilitating efficient top-down and bottom-up query formulation and portability of an interface across different application domains and sources. In this demonstration, we present a novel plug-and-play visual subgraph query interface construction engine called PLAYPEN that can be plugged on any large network G with a plug specification b to automatically generate the VQI for G that satisfies b by populating various components of the interface.}
}


@inproceedings{DBLP:conf/sigmod/SongWZJ22,
	author = {Yuanfeng Song and
                  Raymond Chi{-}Wing Wong and
                  Xuefang Zhao and
                  Di Jiang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {VoiceQuerySystem: {A} Voice-driven Database Querying System Using
                  Natural Language Questions},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2385--2388},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520158},
	doi = {10.1145/3514221.3520158},
	timestamp = {Sun, 19 Jan 2025 13:27:28 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/SongWZJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With recent development in natural language processing (NLP) and automatic speech recognition (ASR), voice-based interfaces have become a necessity for applications such as chatbots, search engines, and databases. In this demonstration, we introduce VoiceQuerySystem, a voice-based database querying system that enables users to conduct data operations with natural language questions (NLQs). Different from existing voice-based interfaces such as SpeakQL or EchoQuery, which restricts the voice input to be an exact SQL or follow a pre-defined template, VoiceQuerySystem attempts to achieve data manipulation via common NLQs, and thus does not require the user's technical background in SQL language. The underlying techniques in VoiceQuerySystem is a new task named Speech-to-SQL, which aims to understand the semantic in speech and then translate it into SQL queries. We explore two proposed approaches - the cascaded one and the end-to-end (E2E) one towards speech-to-SQL translation. The cascaded method first converts the user's voice-based NLQs into text by a self-developed ASR module, and then conducts downstream SQL generation via a text-to-SQL model (i.e., IRNet). In contrast, the E2E method is a novel neural architecture named SpeechSQLNet designed by us, which converts the speech signals into SQL queries directly without the middle medium as text. Extensive experiments and demonstrations validate the rationale of the speech-to-SQL task and the effectiveness of the proposed SpeechSQLNet model. To the best of our knowledge, this is the first system that provides a voice-based querying functionality on DBMS from common NLQs.}
}


@inproceedings{DBLP:conf/sigmod/FischerHG22,
	author = {Tim Fischer and
                  Denis Hirn and
                  Torsten Grust},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Snakes on a Plan: Compiling Python Functions into Plain {SQL} Queries},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2389--2392},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520175},
	doi = {10.1145/3514221.3520175},
	timestamp = {Sun, 19 Jan 2025 13:27:24 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/FischerHG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {"Move your computation close to the data" is decades-old advice that is hard to follow if your code exhibits complex control flow. The runtime of such applications suffers from a continual back and forth between database-external code execution and plan-based SQL evaluation. We demonstrate the ByePy compiler which translates entire Python functions with arbitrary control flow-including deeply nested iteration-into plain recursive SQL:1999 queries. The invocation of a ByePy-compiled function enters the database engine once to execute the plan of a single query. Computation does not get much closer to the data than this. The system rewards this translation effort from Python to SQL with runtime improvements of up to an order of magnitude.}
}


@inproceedings{DBLP:conf/sigmod/HattaschBB22,
	author = {Benjamin H{\"{a}}ttasch and
                  Jan{-}Micha Bodensohn and
                  Carsten Binnig},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Demonstrating {ASET:} Ad-hoc Structured Exploration of Text Collections},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2393--2396},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520174},
	doi = {10.1145/3514221.3520174},
	timestamp = {Sun, 04 Aug 2024 19:37:27 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/HattaschBB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this demo, we present ASET, a novel tool to explore the contents of unstructured data (text) by automatically transforming relevant parts into tabular form. ASET works in an ad-hoc manner without the need to curate extraction pipelines for the (unseen) text collection or to annotate large amounts of training data. The main idea is to use a new two-phased approach that first extracts a superset of information nuggets from the texts using existing extractors such as named entity recognizers. In a second step, it leverages embeddings and a novel matching strategy to match the extractions to a structured table definition as requested by the user. This demo features the ASET system with a graphical user interface that allows people without machine learning or programming expertise to explore text collections efficiently. This can be done in a self-directed and flexible manner, and ASET provides an intuitive impression of the result quality.}
}


@inproceedings{DBLP:conf/sigmod/PorwalMDASRMKNA22,
	author = {Vibhor Porwal and
                  Subrata Mitra and
                  Fan Du and
                  John Anderson and
                  Nikhil Sheoran and
                  Anup B. Rao and
                  Tung Mai and
                  Gautam Kowshik and
                  Sapthotharan Nair and
                  Sameeksha Arora and
                  Saurabh Mahapatra},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Efficient Insights Discovery through Conditional Generative Model
                  based Query Approximation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2397--2400},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520161},
	doi = {10.1145/3514221.3520161},
	timestamp = {Sun, 19 Jan 2025 13:27:35 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/PorwalMDASRMKNA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There are various scenarios where very quick insights from a massive amount of data need to be extracted in a time-critical manner. These might be fresh insights or re-looking at why previous insights did not work and how to fix those. A marketing campaign is one real-world scenario where a non-programmer needs to dig such huge data in a very short period of time (a few hours) in order to hit a target revenue. In this demo paper, we will describe Electra - a system that integrates an automated data-insight discovery mechanism with a novel machine-learning (ML) driven approximate query processing (AQP) engine that can answer complex queries with a large number of predicates or conditions with high accuracy. This AQP engine uses a conditional generative model to generate a very small sample (~1000 rows) corresponding to the actual query to be answered and computes the highly accurate approximate answer from those instead of running the query against the original data. The insight discovery workflow bootstraps insights using ML algorithms based on the statistical characteristics of the data and further offers a no-code based interface to drill down for deeper insights. The queries from this interface are answered by the AQP engine that runs locally at the client-side itself to offer low latency interactions.}
}


@inproceedings{DBLP:conf/sigmod/MeyuhasBHD22,
	author = {Idan Meyuhas and
                  Aviv Ben{-}Arie and
                  Yair Horesh and
                  Daniel Deutch},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{CFDB:} Machine Learning Model Analysis via Databases of CounterFactuals},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2401--2404},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520162},
	doi = {10.1145/3514221.3520162},
	timestamp = {Sun, 19 Jan 2025 13:27:33 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/MeyuhasBHD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data Scientists often design and train complex Machine Learning models that evolve over time due to re-training on new data, a revised architecture, or both. To assist Data Scientists in this process, many methods for analyzing models have been recently developed. A prominent approach for model analysis is based on the notion of Counterfactuals. A counterfactual (CF) intuitively explains the label assigned by the model to a particular instance by identifying perturbations to the instance that lead to a different predicted label. A large body of recent literature has demonstrated the usefulness of CFs for deriving insights on the model at large. The analyzed CFs come in various flavors and are applied to instances chosen based on various criteria, in the context of different analysis goals. In this work we propose to demonstrate CFDB (Counterfactuals Database), a unified framework for querying Counterfactuals. CFDB allows to consolidate common approaches in CF-based analysis and to provide multiple levels of abstractions in a relational framework. We will demonstrate CFDB in the context of the Lending Club Loan Data, showing its usefulness by formulating and executing multiple analyses over evolving classifiers for Loan Approval.}
}


@inproceedings{DBLP:conf/sigmod/GuFZ0F022,
	author = {Zihui Gu and
                  Ruixue Fan and
                  Xiaoman Zhao and
                  Meihui Zhang and
                  Ju Fan and
                  Xiaoyong Du},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {OpenTFV: An Open Domain Table-Based Fact Verification System},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2405--2408},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520163},
	doi = {10.1145/3514221.3520163},
	timestamp = {Mon, 03 Mar 2025 21:21:49 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/GuFZ0F022.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prevalence of misinformation, both online and offline, has prompted a great demand of fact verification. Table-based fact verification aims to check whether a textual claim is supported or refuted based on relational tables. However, most of the existing approaches are in a closed-domain setting, which may not be realistic in practice. To address this problem, in this paper, we introduce OpenTFV, a user-friendly system that supports open domain table-based fact verification. Given a claim input by an end-user, OpenTFV retrieves the relevant tables, and provides a verification result for each table with an intuitive interpretation in natural language. We have implemented OpenTFV and demonstrated OpenTFV in two representative scenarios, COVID-19 claims fact verification based on academic tables and general fact verification on Wiki-tables.}
}


@inproceedings{DBLP:conf/sigmod/VeltriSB0P22,
	author = {Enzo Veltri and
                  Donatello Santoro and
                  Gilbert Badaro and
                  Mohammed Saeed and
                  Paolo Papotti},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Pythia: Unsupervised Generation of Ambiguous Textual Claims from Relational
                  Data},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2409--2412},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520164},
	doi = {10.1145/3514221.3520164},
	timestamp = {Sat, 30 Sep 2023 09:56:35 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/VeltriSB0P22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Applications such as computational fact checking and data-to-text generation exploit the relationship between relational data and natural language text. Despite promising results in these areas, state of the art solutions simply fail in managing "data-ambiguity", i.e., the case when there are multiple interpretations of the relationship between the textual sentence and the relational data. To tackle this problem, we introduce Pythia, a system that, given a relational table D, generates textual sentences that contain factual ambiguities w.r.t. the data in D. Such sentences can then be used to train target applications in handling data-ambiguity. In this demonstration, we first show how our system generates data ambiguous sentences for a given table in an unsupervised fashion by data profiling and query generation. We then demonstrate how two existing applications benefit from Pythia\'s generated sentences, improving the state-of-the-art results. The audience will interact with Pythia by changing input parameters in an interactive fashion, including the upload of their own dataset to see what data ambiguous sentences are generated for it.}
}


@inproceedings{DBLP:conf/sigmod/ChenLBJW22,
	author = {Peng Chen and
                  Hui Li and
                  Sourav S. Bhowmick and
                  Shafiq R. Joty and
                  Weiguo Wang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{LANTERN:} Boredom-conscious Natural Language Description Generation
                  of Query Execution Plans for Database Education},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2413--2416},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520165},
	doi = {10.1145/3514221.3520165},
	timestamp = {Sun, 19 Jan 2025 13:27:20 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ChenLBJW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The database systems course in an undergraduate computer science degree program is gaining increasing importance due to the continuous supply of database-related jobs as well as the rise of Data Science. A key learning goal of learners taking such a course is to understand how SQL queries are executed in an RDBMS in practice. An RDBMS typically exposes a query execution plan (QEP) in a visual or textual format, which describes the execution steps for a given query. However, it is often daunting for a learner to comprehend these QEPs containing vendor-specific implementation details. In this demonstration, we present a novel, generic, and portable system called LANTERN that generates a natural language (NL)-based description of the execution strategy chosen by the underlying RDBMS to process a query. It provides a declarative framework called POOL for subject matter experts (SME) to efficiently create and manipulate the NL descriptions of physical operators of any RDBMS. It then exploits POOL to generate the NL descriptions of QEPs by integrating a rule-based and a deep learning-based techniques to infuse language variability in the descriptions. Such an NL generation strategy mitigates the impact of boredom on learners caused by repeated exposure of similar text generated by a rule-based system.}
}


@inproceedings{DBLP:conf/sigmod/LiuTZDZSYZMZYWJ22,
	author = {Haotian Liu and
                  Bo Tang and
                  Jiashu Zhang and
                  Yangshen Deng and
                  Xinying Zheng and
                  Qiaomu Shen and
                  Xiao Yan and
                  Dan Zeng and
                  Zunyao Mao and
                  Chaozu Zhang and
                  Zhengxin You and
                  Zhihao Wang and
                  Runzhe Jiang and
                  Fang Wang and
                  Man Lung Yiu and
                  Huan Li and
                  Mingji Han and
                  Qian Li and
                  Zhenghai Luo},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {GHive: {A} Demonstration of GPU-Accelerated Query Processing in Apache
                  Hive},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2417--2420},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520166},
	doi = {10.1145/3514221.3520166},
	timestamp = {Sun, 19 Jan 2025 13:27:34 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/LiuTZDZSYZMZYWJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a distributed, fault-tolerant data warehouse system for large-scale data analytics, Apache Hive has been used for various applications in many organizations (e.g., Facebook, Amazon, and Huawei). Exploiting the large degrees of parallelism of GPU to improve the performance of online analytical processing (OLAP) in database system is a common practice in the industry. Meanwhile, it is a common practice to exploit the large degrees of parallelism of GPU to improve the performance of online analytical processing (OLAP) in database systems. This demo presents GHive, which enables Apache Hive to accelerate OLAP queries by jointly utilizing CPU and GPU in intelligent and efficient ways. The takeaways for SIGMOD attendees include: (1) the superior performance of GHive compared with vanilla Hive that only uses CPU; (2) intuitive visualizations of execution statistics for Hive and GHive to understand where the acceleration of GHive comes from; (3) detailed profiling of the time taken by each operator on CPU and GPU to show the advantages of GPU execution.}
}


@inproceedings{DBLP:conf/sigmod/SunJ0C22,
	author = {Luming Sun and
                  Tao Ji and
                  Cuiping Li and
                  Hong Chen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {DeepO: {A} Learned Query Optimizer},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2421--2424},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520167},
	doi = {10.1145/3514221.3520167},
	timestamp = {Tue, 01 Apr 2025 19:09:25 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/SunJ0C22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query optimization is crucial for the query performance of database systems. Despite decades of efforts from both research and industrial communities, query optimization remains one of the most challenging problems. Thanks to the advances in artificial intelligence, data-driven and learning-based techniques are seeing traction in database research recently. However, most former learning-based works perform less practical because they are evasive about the interaction between learning components and database systems. In this demonstration, we introduce DeepO, a novel deep-learning-based query optimizer that offers high-quality and fine-grained query optimization efficiently and practically. We implement DeepO and incorporate it into PostgreSQL, and we also provide a web user interface, where users can carry out the optimization operations interactively and evaluate the optimization performance. Preliminary results show that DeepO outperforms the baseline PostgreSQL optimizer.}
}


@inproceedings{DBLP:conf/sigmod/YangJYLMB22,
	author = {Junran Yang and
                  Hyekang Kevin Joo and
                  Sai S. Yerramreddy and
                  Siyao Li and
                  Dominik Moritz and
                  Leilani Battle},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Demonstration of VegaPlus: Optimizing Declarative Visualization Languages},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2425--2428},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520168},
	doi = {10.1145/3514221.3520168},
	timestamp = {Mon, 03 Mar 2025 21:21:51 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/YangJYLMB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While many visualization specification languages are user-friendly, they tend to have one critical drawback: they are designed for small data on the client-side and, as a result, perform poorly at scale. We propose a system that takes declarative visualization specifications as input and automatically optimizes the resulting visualization execution plans by offloading computational-intensive operations to a separate database management system (DBMS). Our demo emphasizes live programming of visualizations over big data, enabling users to write or import Vega specifications, view the optimized plans from our system, and even modify these plans and compare their performance via a dedicated performance dashboard.}
}


@inproceedings{DBLP:conf/sigmod/SarkarCZA22,
	author = {Subhadeep Sarkar and
                  Kaijie Chen and
                  Zichen Zhu and
                  Manos Athanassoulis},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Compactionary: {A} Dictionary for {LSM} Compactions},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2429--2432},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520169},
	doi = {10.1145/3514221.3520169},
	timestamp = {Sat, 30 Sep 2023 09:56:35 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/SarkarCZA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log-structured merge (LSM) trees are widely used as the storage layer of modern NoSQL data stores, as they offer efficient ingestion performance. To enable competitive read performance and reduce space amplification, LSM-trees re-organize data layout on disk iteratively, through compactions. Compactions are at the heart of every LSM-based storage engine, fundamentally influencing their performance. However, the process of compaction in LSM-engines is often treated as a black-box that is rarely exposed as a tuning knob. In this paper, we demonstrate Compactionary, a dictionary for LSM compactions, that helps to visualize the implications of compactions on performance for different workloads and LSM tunings. Compactionary breaks down the LSM compaction black-box, expressing compactions as an ensemble of four first-order design choices: (i) when to compact, (ii) how to organize the data after compaction, (iii) how much data to compact, and (iv) which data to compact. We configure Compactionary to demonstrate the operational flow of several state-of-the-art LSM compaction strategies and how each strategy affects performance. The participants can (i) customize the workload, (ii) configure the LSM tuning, and (iii) switch between advanced compaction options, to understand individually the impact of the different factors on performance. Further, to engage the interested participants, we extend the demonstration by allowing the participants to create custom hybrid compaction strategies, as well as to configure the settings separately for each strategy in an individual analysis phase. The demo is available at https://disc-projects.bu.edu/compactionary/#interactiveDemo.}
}


@inproceedings{DBLP:conf/sigmod/ZhuPGS22,
	author = {Jiongli Zhu and
                  Romila Pradhan and
                  Boris Glavic and
                  Babak Salimi},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Generating Interpretable Data-Based Explanations for Fairness Debugging
                  using Gopher},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2433--2436},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520170},
	doi = {10.1145/3514221.3520170},
	timestamp = {Sun, 12 Nov 2023 02:07:18 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/ZhuPGS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) models, while increasingly being used to make life-altering decisions, are known to reinforce systemic bias and discrimination. Consequently, practitioners and model developers need tools to facilitate debugging for bias in ML models. We introduce Gopher, a system that generates compact, interpretable and causal explanations for ML model bias. Gopher identifies the top-k coherent subsets of the training data that are root causes for model bias by quantifying the extent to which removing or updating a subset can resolve the bias. We describe the architecture of Gopher and will walk the audience through real-world use cases to highlight how Gopher generates explanations that enable data scientists to understand how subsets of the training data contribute to the bias of a machine learning (ML) model. Gopher is available as open-source software; The code and the demonstration video are available at https://gopher-sys.github.io/.}
}


@inproceedings{DBLP:conf/sigmod/Trummer22a,
	author = {Immanuel Trummer},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Demonstrating {DB-BERT:} {A} Database Tuning Tool that "Reads" the
                  Manual},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2437--2440},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520171},
	doi = {10.1145/3514221.3520171},
	timestamp = {Mon, 03 Mar 2025 21:21:50 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Trummer22a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DB-BERT is a database tuning tool that mines tuning hints from text documents, including the database manual. DB-BERT uses mined hints as a starting point for an iterative tuning approach, guided via reinforcement learning. This demonstration enables visitors to try out DB-BERT for tuning different database management systems, including Postgres and MySQL, on benchmarks such as TPC-C and TPC-H. Visitors can vary the input text to observe how mined hints influence DB-BERT's behavior. The demonstration interface allows tracing back configurations selected for trial runs to the text passages that motivated them. Finally, visitors may try to beat configurations proposed by DB-BERT with their own parameter settings. The code for this demo is publicly available at https://itrummer.github.io/dbbert/.}
}


@inproceedings{DBLP:conf/sigmod/BhowmickC22,
	author = {Sourav S. Bhowmick and
                  Byron Choi},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Data-driven Visual Query Interfaces for Graphs: Past, Present, and
                  (Near) Future},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2441--2447},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3522562},
	doi = {10.1145/3514221.3522562},
	timestamp = {Sun, 19 Jan 2025 13:27:23 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/BhowmickC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Visual graph query interfaces (VQI) widen the reach of graph querying frameworks across a variety of end users by enabling non-programmers to use them. Several industrial and academic frameworks for querying graphs expose such visual interfaces. In this tutorial, we survey recent developments in the emerging area of data-driven visual query interface that is grounded on the principles of human-computer interaction (HCI) and cognitive psychology to enhance usability of graph querying frameworks. A data-driven VQI has many benefits such as reducing the cost in constructing and maintaining an interface, superior support for query formulation, and increased portability of the interface. We discuss the notion of making VQIs data-driven and compare it with its classical manual counterpart, and review techniques for automatic construction and maintenance of these interfaces. In addition, the tutorial suggests open problems and new research directions. In summary, in this tutorial, we review and summarize the research thus far into data-driven visual graph query interface management, giving researchers a snapshot of the current state of the art in this topic, and future research directions.}
}


@inproceedings{DBLP:conf/sigmod/BharadwajC22,
	author = {Akash Bharadwaj and
                  Graham Cormode},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {An Introduction to Federated Computation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2448--2451},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3522561},
	doi = {10.1145/3514221.3522561},
	timestamp = {Sun, 06 Oct 2024 21:14:18 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/BharadwajC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Computation is an emerging area that seeks to provide stronger privacy for user data, by performing large scale, distributed computations where the data remains in the hands of users. Only the necessary summary information is shared, and additional security and privacy tools can be employed to provide strong guarantees of secrecy. The most prominent application of federated computation is in training machine learning models (federated learning), but many additional applications are emerging, more broadly relevant to data management and querying data. This tutorial gives an overview of federated computation models and algorithms. It includes an introduction to security and privacy techniques and guarantees, and shows how they can be applied to solve a variety of distributed computations providing statistics and insights to distributed data. It also discusses the issues that arise when implementing systems to support federated computation, and open problems for future research.}
}


@inproceedings{DBLP:conf/sigmod/PradhanLGS22,
	author = {Romila Pradhan and
                  Aditya Lahiri and
                  Sainyam Galhotra and
                  Babak Salimi},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Explainable {AI:} Foundations, Applications, Opportunities for Data
                  Management Research},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2452--2457},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3522564},
	doi = {10.1145/3514221.3522564},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/PradhanLGS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Algorithmic decision-making systems are successfully being adopted in a wide range of domains for diverse tasks. While the potential benefits of algorithmic decision-making are many, the importance of trusting these systems has only recently attracted attention. There is growing concern that these systems are complex, opaque and non-intuitive, and hence are difficult to trust. There has been a recent resurgence of interest in explainable artificial intelligence (XAI) that aims to reduce the opacity of a model by explaining its behavior, its predictions or both, thus allowing humans to scrutinize and trust the model. A host of technical advances have been made and several explanation methods have been proposed in recent years that address the problem of model explainability and transparency. In this tutorial, we will present these novel explanation approaches, characterize their strengths and limitations, position existing work with respect to the database (DB) community, and enumerate opportunities for data management research in the context of XAI.}
}


@inproceedings{DBLP:conf/sigmod/NargesianAJ22,
	author = {Fatemeh Nargesian and
                  Abolfazl Asudeh and
                  H. V. Jagadish},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Responsible Data Integration: Next-generation Challenges},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2458--2464},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3522567},
	doi = {10.1145/3514221.3522567},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/NargesianAJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data integration has been extensively studied by the data management community and is a core task in the data pre-processing step of ML pipelines. When the integrated data is used for analysis and model training, responsible data science requires addressing concerns about data quality and bias. We present a tutorial on data integration and responsibility, highlighting the existing efforts in responsible data integration along with research opportunities and challenges. In this tutorial, we encourage the community to audit data integration tasks with responsibility measures and develop integration techniques that optimize the requirements of responsible data science. We focus on three critical aspects: (1) the requirements to be considered for evaluating and auditing data integration tasks for quality and bias; (2) the data integration tasks that elicit attention to data responsibility measures and methods to satisfy these requirements; and, (3) techniques, tasks, and open problems in data integration that help achieve data responsibility.}
}


@inproceedings{DBLP:conf/sigmod/NarasayyaC22,
	author = {Vivek R. Narasayya and
                  Surajit Chaudhuri},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Multi-Tenant Cloud Data Services: State-of-the-Art, Challenges and
                  Opportunities},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2465--2473},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3522566},
	doi = {10.1145/3514221.3522566},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/NarasayyaC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enterprises are moving their business-critical workloads to public clouds at an accelerating pace. Multi-tenancy is a crucial tenet for cloud data service providers allowing them to provide services in cost-effective manner by sharing of resources among tenants of the service. In this tutorial we review architectures of today's cloud data services and identify trends and challenges that arise in multi-tenant cloud data services. We discuss techniques that have been developed for enabling elasticity, providing SLAs, ensuring performance isolation, and reducing cost. We conclude with open research problems in cloud data services.}
}


@inproceedings{DBLP:conf/sigmod/LiT0CJ22,
	author = {Huan Li and
                  Bo Tang and
                  Hua Lu and
                  Muhammad Aamir Cheema and
                  Christian S. Jensen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Spatial Data Quality in the IoT Era: Management and Exploitation},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2474--2482},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3522568},
	doi = {10.1145/3514221.3522568},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/LiT0CJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Within the rapidly expanding Internet of Things (IoT), growing amounts of spatially referenced data are being generated. Due to the dynamic, decentralized, and heterogeneous nature of the IoT, spatial IoT data (SID) quality has attracted considerable attention in academia and industry. How to invent and use technologies for managing spatial data quality and exploiting low-quality spatial data are key challenges in the IoT. In this tutorial, we highlight the SID consumption requirements in applications and offer an overview of spatial data quality in the IoT setting. In addition, we review pertinent technologies for quality management and low-quality data exploitation, and we identify trends and future directions for quality-aware SID management and utilization. The tutorial aims to not only help researchers and practitioners to better comprehend SID quality challenges and solutions, but also offer insights that may enable innovative research and applications.}
}


@inproceedings{DBLP:conf/sigmod/0001Z22,
	author = {Guoliang Li and
                  Chao Zhang},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {{HTAP} Databases: What is New and What is Next},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2483--2488},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3522565},
	doi = {10.1145/3514221.3522565},
	timestamp = {Sun, 19 Jan 2025 13:27:26 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0001Z22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Processing the mixed workloads of transactions and analytical queries in a single database system can eliminate the ETL process and enable real-time data analysis on the transaction data. However, there is no free lunch. Such systems must balance the trade-off between workload isolation and data freshness due to interweaving workloads of OLTP and OLAP. Since Gartner coined the term, Hybrid Transactional/Analytical Processing (HTAP), we have witnessed the emergence of various database systems to support HTAP. One common feature is that they leverage the best of row store and column store to achieve high quality of HTAP. As they have disparate storage strategies and processing techniques to satisfy the requirements of various HTAP applications, it is essential to understand, compare, and evaluate their key techniques. In this tutorial, we offer a comprehensive survey of HTAP databases. We introduce a taxonomy of state-of-the-art HTAP databases according to their storage strategies and architectures. We then take a deep dive into their key techniques regarding transaction processing, analytical processing, data synchronization, query optimization, and resource scheduling. We also introduce existing HTAP benchmarks. Finally, we discuss the research challenges and open problems for HTAP.}
}


@inproceedings{DBLP:conf/sigmod/SarkarA22,
	author = {Subhadeep Sarkar and
                  Manos Athanassoulis},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Dissecting, Designing, and Optimizing LSM-based Data Stores},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2489--2497},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3522563},
	doi = {10.1145/3514221.3522563},
	timestamp = {Sat, 30 Sep 2023 09:56:35 +0200},
	biburl = {https://dblp.org/rec/conf/sigmod/SarkarA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log-structured merge (LSM) trees have emerged as one of the most commonly used disk-based data structures in modern data systems. LSM-trees employ out-of-place ingestion to support high throughput for writes, while their immutable file structure allows for good utilization of disk space. Thus, the log-structured paradigm has been widely adopted in state-of-the-art NoSQL, relational, spatial, and time-series data systems. However, despite their popularity, there is a lack of pedagogical textbook-like material on LSM designs. The goal of this tutorial is to present the fundamental principles of the LSM paradigm along with a digest of optimizations and new designs proposed in recent research and adopted by modern LSM engines. This will serve as introductory material for non-experts, and as a roadmap to cutting-edge LSM results for the LSM-aware researchers and practitioners. Toward this, we first discuss in detail the basic operations (inserts, updates, deletes, point and range queries), their access patterns, and their paths through the LSM data structure. We then dive into the details of recent research on optimizing each of those operations. We first discuss techniques and designs that optimize data ingestion in LSM-trees and the performance tradeoff constructed by writes and reads for the LSM engines. Finally, we present the rich design space of the log-structured paradigm and outline how to navigate it and tune LSM-based systems. We conclude with a discussion on open challenges on LSM systems. This will be a 1.5-hour tutorial.}
}


@inproceedings{DBLP:conf/sigmod/AilamakiBGK0R0B22,
	author = {Anastasia Ailamaki and
                  Leilani Battle and
                  Johannes Gehrke and
                  Masaru Kitsuregawa and
                  David Maier and
                  Christopher R{\'{e}} and
                  Meihui Zhang and
                  Magdalena Balazinska},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {The {DB} Community vis-{\`{a}}-vis Environmental, Health, and Societal
                  Grand Challenges: Innovation Engine, Plumber, or Bystander?},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2498--2500},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3528414},
	doi = {10.1145/3514221.3528414},
	timestamp = {Mon, 03 Mar 2025 21:21:48 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AilamakiBGK0R0B22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This panel considers the role of the database research community in addressing humanity's greatest challenges. Are we an innovation engine, tool providers, or are we standing on the side while other research communities take the lead?}
}


@inproceedings{DBLP:conf/sigmod/Amer-YahiaBDILS22,
	author = {Sihem Amer{-}Yahia and
                  Sourav S. Bhowmick and
                  Xin Luna Dong and
                  Stratos Idreos and
                  Wolfgang Lehner and
                  Divesh Srivastava},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Publication Culture and Review Processes in the Data Management Community:
                  An Open Discussion},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2501--2502},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3528413},
	doi = {10.1145/3514221.3528413},
	timestamp = {Mon, 03 Mar 2025 21:21:48 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Amer-YahiaBDILS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Data Management community has explored many options in recent years to improve our publication culture and review processes, ranging from innovative journal-conference hybrids that decouple publication from presentation, incorporating journal-style reviewing for conference-style papers, requesting code reproducibility and code/data availability, multiple submission deadlines in a year, new categories of papers, informal shepherding processes, guidelines for diversity and inclusion, automated COI check, and so on. This panel seeks to examine our many experiments, comparing them with other CS disciplines, and help determine (i) have our experiments worked? (ii) what has their impact been? and (iii) can we do better?}
}


@inproceedings{DBLP:conf/sigmod/Stoian22,
	author = {Mihail Stoian},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Concurrent Link-Cut Trees},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2503--2505},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520247},
	doi = {10.1145/3514221.3520247},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Stoian22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/sigmod/Justen22,
	author = {David Justen},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Cost-efficiency and Performance Robustness in Serverless Data Exchange},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2506--2508},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520248},
	doi = {10.1145/3514221.3520248},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Justen22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enterprises increasingly store their sporadically used data on cheap, elastic cloud storage to save costs. Analytical workloads on that data often appear in infrequent bursts presenting a high disparity in input data sizes. Using conservatively over-provisioned compute resources for this class of workloads is not cost-efficient as a fixed amount of resources only matches steady demand. Elastic query processors with a serverless architecture resolve this issue by running workers in cloud functions [3][9][10]. These systems can start thousands of functions within seconds, enabling elasticity down to query pipeline granularity. Moreover, serverless query processors come at no cost for idle times.}
}


@inproceedings{DBLP:conf/sigmod/Yaroslav22,
	author = {Yaroslav Plaksin},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {An Approach for Unlabeled Tasks Prioritization},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2509--2511},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520245},
	doi = {10.1145/3514221.3520245},
	timestamp = {Sun, 16 Mar 2025 21:49:59 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Yaroslav22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Managing a task backlog is a vital and challenging activity, particularly in a large firm with thousands of developers and a massive backlog. It is beneficial to software managers to have some tools at their disposal to prioritize activities from the current backlog so that only important tasks could be accessible for tracking. This study presents a novel approach to the prioritization of software development tasks with the use of a neural network developed in the presence of unlabeled data}
}


@inproceedings{DBLP:conf/sigmod/Schonberger22,
	author = {Manuel Sch{\"{o}}nberger},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Applicability of Quantum Computing on Database Query Optimization},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2512--2514},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520257},
	doi = {10.1145/3514221.3520257},
	timestamp = {Sun, 19 Jan 2025 13:27:30 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Schonberger22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We evaluate the applicability of quantum computing on two fundamental query optimization problems, join order optimization and multi query optimization (MQO). We analyze the problem dimensions that can be solved on current gate-based quantum systems and quantum annealers, the two currently commercially available architectures. First, we evaluate the use of gate-based systems on MQO, previously solved with quantum annealing. We show that, contrary to classical computing, a different architecture requires involved adaptations. We moreover propose a multi-step reformulation for join ordering problems to make them solvable on current quantum systems. Finally, we systematically evaluate our contributions for gate-based quantum systems and quantum annealers. Doing so, we identify the scope of current limitations, as well as the future potential of quantum computing technologies for database systems.}
}


@inproceedings{DBLP:conf/sigmod/Chockchowwat22,
	author = {Supawit Chockchowwat},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Tuning Hierarchical Learned Indexes on Disk and Beyond},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2515--2517},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520255},
	doi = {10.1145/3514221.3520255},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Chockchowwat22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entry retrieval-a process to retrieve rows whose field(s) associates with the given key(s)-is one of the core operations in databases. Classical indexes such as B-tree [2, 4] and skip list recursively partition the key space into a hierarchical structure. Such a structure retrieves an entry by traversing the path of partitions that enclose the given key, effectively reducing uncertainty of key's position as the traversal proceeds. Recently, index designers have developed interests in learned indexes, a concept introduced by [13]. Using patterns in key-position pairs, a learned model can provide a significantly higher information gain about the key's position than a pessimistic partitioning index can [8]. Many works have successfully outperformed classical indexes by multiple factors in latency and memory usage. Overall, they incorporate various combinations of models, partitioning, error corrections (a.k.a. last mile search), mutability, and tunable parameters [6, 9, 10, 12, 16] among many other works.}
}


@inproceedings{DBLP:conf/sigmod/Xie22,
	author = {Jiadong Xie},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Hindering Influence Diffusion of Community},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2518--2520},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520250},
	doi = {10.1145/3514221.3520250},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Xie22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Considering the rapid spread of incidents like rumours or epidemics, it is important to hinder their influence diffusion. However, none of the existing works can well control the influence diffusion of a community. Based on a novel metric named interaction frequency that can measure the influence diffusion of a community, we aim to remove b nodes from a given community such that the interaction frequency of the remaining nodes is minimized. We also design a polynomial-time algorithm for the problem. The experiments show our algorithm can efficiently hinder the influence diffusion.}
}


@inproceedings{DBLP:conf/sigmod/Wickman22,
	author = {Ryan Wickman},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {SparRL: Graph Sparsification via Deep Reinforcement Learning},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2521--2523},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520254},
	doi = {10.1145/3514221.3520254},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Wickman22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/sigmod/Fruth22,
	author = {Michael Fruth},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Live Patching Database Management Systems},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2524--2526},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520253},
	doi = {10.1145/3514221.3520253},
	timestamp = {Sun, 19 Jan 2025 13:27:30 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Fruth22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/sigmod/Sheoran22,
	author = {Nikhil Sheoran},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {DeepOLA: Online Aggregation for Deeply Nested Queries},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2527--2529},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520249},
	doi = {10.1145/3514221.3520249},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Sheoran22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advent of data-driven operating model, deriving useful insights from big data analysis has become very important. But the ever-increasing volume of data has made obtaining insights at "rates resonant with the pace of human thought" [4] more challenging. Online Aggregation (OLA) [3] is a technique that tries to counter this by incrementally improving the query result estimates and allowing the user to observe the query progress as well as control its execution on the fly. OLA provides the user with an approximate estimate of the query result as soon as it has processed a small portion (hereafter referred to as a partition) of the data. The user based on their latency-error trade-off requirements can choose to stop the execution of the query.}
}


@inproceedings{DBLP:conf/sigmod/Kaushik22,
	author = {Sughosh V. Kaushik},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Lineage Resource Manager},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2530--2532},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520252},
	doi = {10.1145/3514221.3520252},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Kaushik22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Main memory columnar database systems such as HyPer [1], Hana [4], MemSQL [9] have rapidly grown in use for both OLTP and OLAP applications in enterprise software [5]. Lineage capture in such in-memory columnar database systems incurs memory and performance overhead[2]. Hence we want to control the lineage capture overhead in such systems. We propose a lineage resource manager that employs specific mechanisms to control the lineage capture overhead. For implementing the resource manager, we consider an in-memory columnar query execution engine that supports concurrent query execution, where operators have been instrumented to capture lineage. Controlling lineage capture overhead is more imminent in embedded analytical systems (database systems running alongside another process, not as a standalone process) like DuckDB [11] and MonetDBLite [10].}
}


@inproceedings{DBLP:conf/sigmod/Pan22,
	author = {Joshua Pan},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Workload-Adaptive Filtering in Storage Engines},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2533--2535},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520256},
	doi = {10.1145/3514221.3520256},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Pan22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log-Structured Merge-Tree (LSM-tree) based storage engines power numerous applications from social media to machine learning, networks, and Blockchain. Today, storage engine performance deteriorates as data increases especially for skewed workloads due in a large part to inefficient filters. LSM-tree based storage engines rely on the effectiveness of filters to prune unneeded disk accesses. We present an adaptive filter that remembers frequent false positives to turn them into true negatives for future queries. The filter is tailored for integration in state-of-the-art storage engines, and we compare it against traditional workload agnostic filters. We show that our adaptive filter can provide up to 2x end-to-end throughput improvement.}
}


@inproceedings{DBLP:conf/sigmod/Yao22,
	author = {Alexander Yao},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Interactive Query Explanations Using Fine Grained Provenance},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2536--2538},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520251},
	doi = {10.1145/3514221.3520251},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Yao22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database users, enabled by business intelligence and data visualization platforms, use relational queries to better understand their data. When surprising aggregation results or unexpected rows appear, they want to know which input table rows caused them. While state-of-the-art query explanation systems advocate for re-running queries under counterfactual interventions on the input tables, running many of them interactively remains challenging. Prior work relies on incremental view maintenance (IVM) to refresh query results, incurring significant materialization overhead for query intermediates and scaling poorly when interventions result in many insertions/deletions. Inspired by recent advances in fast fine grained lineage capture we propose a compilation engine that leverages lineage metadata to interactively evaluate counterfactual interventions. By forgoing the conventional approach of representing lineage metadata as symbolic expressions and executing them, we decrease latencies by multiple orders of magnitude against IVM and lineage based approaches.}
}


@inproceedings{DBLP:conf/sigmod/Gorb22,
	author = {Anna Gorb},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {A Recommender Algorithm to Automatically Generate Metrics for {GQM}
                  Models in Software Development},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2539--2541},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3520244},
	doi = {10.1145/3514221.3520244},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Gorb22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software development is a wicked problem that has no formal criteria of solution correctness. For this reason, developers use goal-focused software metrics to estimate and improve the quality of the result. However, it is not an easy task to determine the minimal set of metrics that fits the goals. That is why this research presents the recommender system that automatically generates metrics for each question from the GQM model using multi-label classification.}
}


@inproceedings{DBLP:conf/sigmod/GroppeGH22,
	author = {Sven Groppe and
                  Le Gruenwald and
                  Ching{-}Hsien Hsu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {BiDEDE'22: Second International Workshop on Big Data in Emergent Distributed
                  Environments},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2542--2543},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3524072},
	doi = {10.1145/3514221.3524072},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/GroppeGH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Second International Workshop on Big Data in Emergent Distributed Environments (BiDEDE) focuses on scalable data management issues in emergent computing environments like (post) cloud and fog/edge/dew computing. All these computing environments aim to smoothly integrate scalable data management and processing into distributed environments, such that communication and computational costs are reduced for higher throughput, lower latencies of applications and extending battery lifetimes of nodes in companion with robust approaches to overcome failures and crashes. While there has been research in these areas for already over one decade, still many open challenges exist because of technology triggers like lightweight virtualization, increasing capabilities of nodes and increasing massive parallelization. This workshop supports lively discussions in these and related areas.}
}


@inproceedings{DBLP:conf/sigmod/DeutchMC22,
	author = {Daniel Deutch and
                  Tanu Malik and
                  Adriane Chapman},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {Theory and Practice of Provenance},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2544--2545},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3524073},
	doi = {10.1145/3514221.3524073},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/DeutchMC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Provenance is metadata about the origin, history, or derivation of something; in computer science, provenance usually describes some informational artifact, such as a dataset, an executable program, a news article, or a chart or graph in a scientific publication. Notably, provenance is closely related to issues of explanation, accountability, transparency and ethics. Indeed, these and related issues are the subject of extensive investigation in multiple areas of research such as Scientific Workflows, Databases, Machine Learning and Artificial Intelligence. TaPP, the international workshop on Theory and Practice of Provenance, is widely considered to be the premier venue dedicated to provenance. In 2022, it is held for the first time in conjunction with ACM SIGMOD.}
}


@inproceedings{DBLP:conf/sigmod/KalavriS22,
	author = {Vasiliki Kalavri and
                  Semih Salihoglu},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {GRADES-NDA'22: 5th International Workshop on Graph Data management
                  Experiences and Systems {(GRADES)} and Network Data Analytics {(NDA)}},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2546--2547},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3524074},
	doi = {10.1145/3514221.3524074},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/KalavriS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {GRADES-NDA is the premier workshop on graph data management and analytics that aims to bring together researchers from academia, industry, and government. GRADES-NDA'22 is a forum for discussing recent advances in (large-scale) graph data management and analytics systems, as well as propose and discuss novel methods and techniques towards addressing domain specific challenges or handling noise in real-world graphs. In 2022, GRADES-NDA is in its fifth edition.}
}


@inproceedings{DBLP:conf/sigmod/0001VX22,
	author = {Matthias Boehm and
                  Paroma Varma and
                  Doris Xin},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {DEEM'22: Data Management for End-to-End Machine Learning},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2548--2549},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3524075},
	doi = {10.1145/3514221.3524075},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/0001VX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The DEEM'22 workshop (Data Management for End-to-End Machine Learning) is held on Sunday June 12th, in conjunction with SIGMOD/PODS 2022. DEEM brings together researchers and practitioners at the intersection of applied machine learning, data management and systems research, with the goal to discuss the arising data management issues in ML application scenarios. The workshop solicits regular research papers (10 pages) describing preliminary and ongoing research results, including industrial experience reports of end-to-end ML deployments, related to DEEM topics. In addition, DEEM 2022 establishes a new paper category for reports on applications and tools (4 pages) as a forum for sharing interesting use cases, problems, datasets, benchmarks, visionary ideas, system designs, and descriptions of system components and tools related to end-to-end ML pipelines. DEEM 2022 received 13 high-quality submissions from Africa, Asia, Europe, and North America, with 5 regular research papers, and 8 reports on applications and tools.}
}


@inproceedings{DBLP:conf/sigmod/BordawekarAFMS22,
	author = {Rajesh Bordawekar and
                  Yael Amsterdamer and
                  Donatella Firmani and
                  Ryan Marcus and
                  Oded Shmueli},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {aiDM'22: Fifth International Workshop on Exploiting Artificial Intelligence
                  Techniques for Data Management},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2550--2551},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3524076},
	doi = {10.1145/3514221.3524076},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/BordawekarAFMS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in AI techniques, as well as enabling hardware and infrastructure, has led to the integration of AI in wide-ranging domains and tasks. In particular, AI has been used to handle various types of data (including numerical, textual and graphical data) and has been adopted in large-scale distributed systems. From a data management perspective, this calls for the harnessing of state-of- the-art AI solutions for data management tasks and systems. aiDM is a full-day workshop that offers a stage for innovative interdisciplinary research that studies the interaction between AI and data management and develops new AI technologies for data-related tasks. This year, aiDM'22 particularly focuses on the transparent exploitation of AI techniques in existing enterprise-level data management workloads.}
}


@inproceedings{DBLP:conf/sigmod/AbouziedMC22,
	author = {Azza Abouzied and
                  Dominik Moritz and
                  Michael J. Cafarella},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {HILDA'22: The {SIGMOD} 2022 Workshop on Human-in-the-Loop Data Analytics},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2552--2553},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3524077},
	doi = {10.1145/3514221.3524077},
	timestamp = {Mon, 03 Mar 2025 21:21:48 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/AbouziedMC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {HILDA brings together researchers and practitioners to exchange ideas and results on human-data interaction. It explores how data management and analysis can be made more effective when taking into account the people who design and build these processes as well as those who are impacted by their results. We are trying to change things up a bit for HILDA 2022. There are now several good venues for the kind of work that HILDA has traditionally attracted. We plan to focus this year's workshop on early-stage research that is promising and exciting. A core part of this plan is that every paper gets a mentor. In this summary, we describe the workshop, its main areas of focus and our review and mentorship plan.}
}


@inproceedings{DBLP:conf/sigmod/RiggerT22,
	author = {Manuel Rigger and
                  Pinar T{\"{o}}z{\"{u}}n},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {DBTest '22: 9th International Workshop on Testing Database Systems},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2554--2555},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3524078},
	doi = {10.1145/3514221.3524078},
	timestamp = {Sun, 19 Jan 2025 13:27:26 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/RiggerT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the ever-increasing amount of data stored and processed and ever-evolving hardware technology, there is not only an ongoing need for testing database management systems but also data-intensive systems in general. Reviving the previous success of the eight previous workshops, the goal of DBTest 2022 is to bring researchers and practitioners from academia and industry together to discuss key problems and ideas related to testing database systems and applications. The long-term objective is to reduce the cost and time required to test and tune data management and processing products so that users and vendors can spend more time and energy on actual innovations.}
}


@inproceedings{DBLP:conf/sigmod/Aivaloglou0M22,
	author = {Efthimia Aivaloglou and
                  George Fletcher and
                  Daphne Miedema},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {DataEd'22 - 1st International Workshop on Data Systems Education:
                  Bridging Education Practice with Education Research},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2556--2557},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3524079},
	doi = {10.1145/3514221.3524079},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/Aivaloglou0M22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interest in data systems education is increasing, especially with the rise in demand for well trained and re-trained data scientists. The database and the computing education research communities have complementary perspectives and experiences to share with each other. The DataEd workshop is organized as a dedicated venue for these communities to come together to share findings, to cross-pollinate perspectives and methods, and to shed light on opportunities for mutual progress in data systems education. In the DataEd workshop, we will present and discuss data management systems education experiences and research via keynotes, an industry panel discussion, and paper and poster presentations.}
}


@inproceedings{DBLP:conf/sigmod/BlanasM22,
	author = {Spyros Blanas and
                  Norman May},
	editor = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
	title = {International Workshop on Data Management on New Hardware (DaMoN)},
	booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
	pages = {2558--2559},
	publisher = {{ACM}},
	year = {2022},
	url = {https://doi.org/10.1145/3514221.3524080},
	doi = {10.1145/3514221.3524080},
	timestamp = {Thu, 16 Mar 2023 09:51:25 +0100},
	biburl = {https://dblp.org/rec/conf/sigmod/BlanasM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {New hardware, like multi-core CPUs, GPUs, FPGAs, new memory and storage technologies, and low-power hardware impose a great challenge to optimizing database performance. Consequently, exploiting the characteristics of modern hardware has become an important topic of database systems research. In the past decade the DaMoN workshop has established itself as the primary database venue to present ideas how to exploit new hardware for data management, in particular how to improve performance or scalability of databases, how new hardware unlocks new database application scenarios, and how data management could benefit from future hardware.}
}
