@inproceedings{DBLP:conf/icde/Vassiliadis21,
	author = {Panos Vassiliadis},
	title = {Profiles of Schema Evolution in Free Open Source Software Projects},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1--12},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00008},
	doi = {10.1109/ICDE51399.2021.00008},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Vassiliadis21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present the findings of a large study of the evolution of the schema of 195 Free Open Source Software projects. We identify families of evolutionary behaviors, or taxa, in FOSS projects. A large percentage of the projects demonstrate very few, if any, actions of schema evolution. Two other taxa involve the evolution via focused actions, with either a single focused maintenance action, or a large percentage of evolution activity grouped in no more than a couple interventions. Schema evolution also involves moderate, and active evolution, with very different volumes of updates to the schema. To the best of our knowledge, this is the first study of this kind in the area of schema evolution, both in terms of presenting profiles of how schemata evolve, and, in terms of the dataset magnitude and the generalizability of the findings.}
}


@inproceedings{DBLP:conf/icde/LiRBZCZ21,
	author = {Peng Li and
                  Xi Rao and
                  Jennifer Blase and
                  Yue Zhang and
                  Xu Chu and
                  Ce Zhang},
	title = {CleanML: {A} Study for Evaluating the Impact of Data Cleaning on {ML}
                  Classification Tasks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {13--24},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00009},
	doi = {10.1109/ICDE51399.2021.00009},
	timestamp = {Sat, 30 Sep 2023 09:44:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiRBZCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data quality affects machine learning (ML) model performances, and data scientists spend considerable amount of time on data cleaning before model training. However, to date, there does not exist a rigorous study on how exactly cleaning affects ML — ML community usually focuses on developing ML algorithms that are robust to some particular noise types of certain distributions, while database (DB) community has been mostly studying the problem of data cleaning alone without considering how data is consumed by downstream ML analytics.We propose a CleanML study that systematically investigates the impact of data cleaning on ML classification tasks. The open-source and extensible CleanML study currently includes 14 real-world datasets with real errors, five common error types, seven different ML models, and multiple cleaning algorithms for each error type (including both commonly used algorithms in practice as well as state-of-the-art solutions in academic literature). We control the randomness in ML experiments using statistical hypothesis testing, and we also control false discovery rate in our experiments using the Benjamini-Yekutieli (BY) procedure. We analyze the results in a systematic way to derive many interesting and nontrivial observations. We also put forward multiple research directions for researchers.}
}


@inproceedings{DBLP:conf/icde/JinTZ021,
	author = {Yifeng Jin and
                  Zijing Tan and
                  Weijun Zeng and
                  Shuai Ma},
	title = {Approximate Order Dependency Discovery},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {25--36},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00010},
	doi = {10.1109/ICDE51399.2021.00010},
	timestamp = {Tue, 21 Mar 2023 20:50:55 +0100},
	biburl = {https://dblp.org/rec/conf/icde/JinTZ021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Lexicographical order dependencies (ODs) specify orders between list of attributes, and are proven useful in optimizing SQL queries with order by clauses. To find hidden ODs from dirty data in practice, in this paper we make a first effort to study the approximate OD discovery problem, aiming at automatically discovering ODs that hold on the data with some exceptions. (1) We adapt two error measures to ODs, prove their desirable properties, and present efficient algorithms for computing the measures and related lower and upper bounds. (2) We present an efficient approximate OD discovery algorithm that is well suited to the two error measures, with a set of pruning rules and optimization techniques. (3) We conduct extensive experiments to verify the effectiveness and scalability of our methods, using real-life and synthetic data.}
}


@inproceedings{DBLP:conf/icde/CorainGA21,
	author = {Matteo Corain and
                  Paolo Garza and
                  Abolfazl Asudeh},
	title = {{DBSCOUT:} {A} Density-based Method for Scalable Outlier Detection
                  in Very Large Datasets},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {37--48},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00011},
	doi = {10.1109/ICDE51399.2021.00011},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CorainGA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent technological advancements have enabled generating and collecting huge amounts of data in a daily manner. This data is used for different purposes that may impact us on an unprecedented scale. Understanding the data, including detecting its outliers, is a critical step before utilizing it.Outlier detection has been studied well in the literature but the existing approaches fail to scale to these very large settings. In this paper, we propose DBSCOUT, an efficient exact algorithm for outlier detection with a linear complexity that can run in parallel over multiple independent machines, making it a fit for the settings with billions of tuples. Besides the theoretical analysis, our experiment results confirm orders of magnitude improvement over the existing work, proving the efficiency, scalability, and effectiveness of our approach.}
}


@inproceedings{DBLP:conf/icde/LiangF0XCH21,
	author = {Jiaqing Liang and
                  Suo Feng and
                  Chenhao Xie and
                  Yanghua Xiao and
                  Jindong Chen and
                  Seung{-}won Hwang},
	title = {Bootstrapping Information Extraction via Conceptualization},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {49--60},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00012},
	doi = {10.1109/ICDE51399.2021.00012},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiangF0XCH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bootstrapping enables us to use existing knowledge to find patterns and extract new knowledge from free texts, from which more patterns can be found. Due to its minimally supervised, domain-independent, and language-independent nature, it has been widely adopted in real-world applications. However, as iterations go on, semantic drift may happen. The extraction may shift from the target class to other classes and result in errors, which propagate in the succeeding iterations and hurt the performance significantly. Existing solutions simply throw away bad patterns, sacrificing recall to ensure high precision. However, we argue that most of these patterns and instances can be kept as long as being applied selectively, guided by prior knowledge. In this paper, we propose a pattern-based extraction framework with three distinguished features: (1) it uses conceptual taxonomies to guide the extraction to reduce semantic drift; (2) it uses the knowledge of existing triples to improve the precision; (3) it integrates all patterns to form a generalized pattern set with quantified confidence measurement. The proposed solution is applied on enriching two real-world knowledge bases and achieves higher precision and recall compared to existing solutions.}
}


@inproceedings{DBLP:conf/icde/MeiSFYFL21,
	author = {Yinan Mei and
                  Shaoxu Song and
                  Chenguang Fang and
                  Haifeng Yang and
                  Jingyun Fang and
                  Jiang Long},
	title = {Capturing Semantics for Imputation with Pre-trained Language Models},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {61--72},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00013},
	doi = {10.1109/ICDE51399.2021.00013},
	timestamp = {Mon, 03 Jan 2022 22:33:29 +0100},
	biburl = {https://dblp.org/rec/conf/icde/MeiSFYFL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing imputation methods generally generate several possible fillings as candidates and determine the value from the candidates for imputing. However, semantics are ignored in these methods. Recently, pre-trained language models achieve good performances in various language understanding tasks. Motivated by this, we propose IPM that captures semantics for Imputation with Pre-trained language Models. A straightforward idea is to model the imputation task as a multiclass classfication task, named IPM-Multi. IPM-Multi predicts the missing values by fine-tuning the pre-trained model. Due to the low redundancy of databases and large domain sizes, IPM-Multi may suffer the over-fitting problem. In this case, we develop another approach named IPM-Binary. IPM-Binary first generates a set of uncertain candidates and fine-tunes a pre-trained language model to select candidates. Specifically, IPM-Binary models the candidate selection task as a binary classification problem. Unlike IPM-Multi, IPM-Binary computes the probability for each candidate filling respectively, by accepting both complete attributes and a candidate filling as input. The attention mechanism enhances the ability of IPM-Binary to capture semantic information. Moreover, negative sampling from neighbors rather than domains is employed to accelerate the training process and makes the training more targeted and effective. As a result, IPM-Binary requires fewer data to converge. We compare our proposal IPM to the state-of-the-art baselines on multiple datasets. And the extensive experimental results show that IPM outperforms existing solutions. The evaluation of IPM validates our intuitions and demonstrates the effectiveness of the proposed optimizations.}
}


@inproceedings{DBLP:conf/icde/00010WRWQ21,
	author = {Wentao Li and
                  Min Gao and
                  Fan Wu and
                  Wenge Rong and
                  Junhao Wen and
                  Lu Qin},
	title = {Manipulating Black-Box Networks for Centrality Promotion},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {73--84},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00014},
	doi = {10.1109/ICDE51399.2021.00014},
	timestamp = {Tue, 21 Mar 2023 20:50:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/00010WRWQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Centrality measures are widely used to map each node to its importance in a network. For many practical applications, vital nodes bearing high centrality scores have superior positions over other nodes. To benefit from the positive impact of becoming a vital node, the problem of improving the centrality of the target node has attracted increasing attention. Many existing studies attack this problem by directly increasing the centrality score of the target node on the premise of knowing the network structure. However, these methods suffer from privacy issues due to their dependence on the network structure and may lose their effectiveness because other nodes can simultaneously increase the scores. Therefore, in this paper, we explore the following question: given a black-box network whose structure is unknown, is it possible to improve the centrality ranking (rather than the score) of a target node by implementing certain strategies? We provide an affirmative answer to this question. First, to avoid relying on the network structure for promotion, we propose strategies that freeze the original graph while appending nodes and edges just around the target node. Second, to guide strategies for effectively boosting centrality, we devise two principles that provide the target node with either the maximum gain or the minimum loss of centrality scores over other nodes. We prove that a strategy meeting the proposed principles is guaranteed to upgrade the target node’s ranking. Extensive experiments were conducted to verify the effectiveness of the proposed strategies on black-box networks.}
}


@inproceedings{DBLP:conf/icde/Wang00ZQZ21,
	author = {Kai Wang and
                  Wenjie Zhang and
                  Xuemin Lin and
                  Ying Zhang and
                  Lu Qin and
                  Yuting Zhang},
	title = {Efficient and Effective Community Search on Large-scale Bipartite
                  Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {85--96},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00015},
	doi = {10.1109/ICDE51399.2021.00015},
	timestamp = {Tue, 04 Jan 2022 17:01:26 +0100},
	biburl = {https://dblp.org/rec/conf/icde/Wang00ZQZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bipartite graphs are widely used to model relation-ships between two types of entities. Community search retrieves densely connected subgraphs containing a query vertex, which has been extensively studied on unipartite graphs. However, com-munity search on bipartite graphs remains largely unexplored. Moreover, all existing cohesive subgraph models on bipartite graphs can only be applied to measure the structure cohesiveness between two sets of vertices while overlooking the edge weight in forming the community. In this paper, we study the significant (α, β)-community search problem on weighted bipartite graphs. Given a query vertex q, we aim to find the significant (α, β)-community ℛ of q which adopts (α, β)-core to characterize the engagement level of vertices, and maximizes the minimum edge weight (significance) within ℛ.To support fast retrieval of ℛ, we first retrieve the maximal connected subgraph of (α, β)-core containing the query vertex (the (α, β)-community), and the search space is limited to this subgraph with a much smaller size than the original graph. A novel index structure is presented which can be built in O(δ•m) time and takes O(δ•m) space where m is the number of edges in G, δ is bounded by \\sqrt m\nand is much smaller in practice. Utilizing the index, the (α, β)-community can be retrieved in optimal time. To further obtain ℛ, we develop peeling and expansion algorithms to conduct searches by shrinking from the (α, β)-community and expanding from the query vertex, respectively. The experimental results on real graphs not only demonstrate the effectiveness of the significant (α, β)-community model but also validate the efficiency of our query processing and indexing techniques.}
}


@inproceedings{DBLP:conf/icde/LiuZZ0Z21,
	author = {Boge Liu and
                  Fan Zhang and
                  Wenjie Zhang and
                  Xuemin Lin and
                  Ying Zhang},
	title = {Efficient Community Search with Size Constraint},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {97--108},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00016},
	doi = {10.1109/ICDE51399.2021.00016},
	timestamp = {Tue, 21 Mar 2023 20:50:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiuZZ0Z21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The studies of k-truss based community search demonstrated that it can find high-quality personalized com-munities with good properties such as high connectivity and bounded diameter. Motivated by natural restrictions from real applications, in this paper, we investigate the search of triangle-connected k-truss with size constraint (denoted by SCkT) in a graph G: given a size constraint s, an integer k, and query set Q, SCkT search aims to find a triangle-connected k-truss H containing the vertices in Q and with size (i.e., total number of vertices in H) not exceeding s. We prove that the SCkT search problem is NP-hard. To tame the hardness, we fully exploit the properties of triangle-connected k-truss subgraphs s.t. a practically-efficient exact solution for SCkT search is developed. A novel and effective lower bound is proposed to early terminate unpromising search branches and narrow down the search space. Two search strategies, expansion and shrinking, are investigated to tailor for efficient support of SCkT search. A hybrid search method is proposed combining the expansion and shrinking strategies, where a score function is used to guide the search order. Our extensive experiments on real-life and synthetic graphs demonstrate the effectiveness of the SCkT model and the efficiency of the proposed techniques.}
}


@inproceedings{DBLP:conf/icde/Guo0WZS21,
	author = {Fangda Guo and
                  Ye Yuan and
                  Guoren Wang and
                  Xiangguo Zhao and
                  Hao Sun},
	title = {Multi-attributed Community Search in Road-social Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {109--120},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00017},
	doi = {10.1109/ICDE51399.2021.00017},
	timestamp = {Tue, 07 May 2024 20:05:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Guo0WZS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a location-based social network, how to find the communities that are highly relevant to query users and have top overall scores in multiple attributes according to user preferences? Typically, in the face of such a problem setting, we can model the network as a multi-attributed road-social network, in which each user is linked with location information and d (≥1) numerical attributes. In practice, user preferences (i.e., weights) are usually inherently uncertain and can only be estimated with bounded accuracy, because a human user is not able to designate exact values with absolute precision. Inspired by this, we introduce a normative community model suitable for multi-criteria decision making, called multi-attributed community (MAC), based on the concepts of k-core and a novel dominance relationship specific to preferences. Given uncertain user preferences, namely, an approximate representation of weights, the MAC search reports the exact communities for each of the possible weight settings. We devise an elegant index structure to maintain the dominance relationships, based on which two algorithms are developed to efficiently compute the top-j MACs. The efficiency and scalability of our algorithms and the effectiveness of MAC model are demonstrated by extensive experiments on both real-world and synthetic road-social networks.}
}


@inproceedings{DBLP:conf/icde/0001KR21,
	author = {Dong Wei and
                  Ioannis Koutis and
                  Senjuti Basu Roy},
	title = {Peer Learning Through Targeted Dynamic Groups Formation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {121--132},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00018},
	doi = {10.1109/ICDE51399.2021.00018},
	timestamp = {Sun, 02 Oct 2022 16:04:34 +0200},
	biburl = {https://dblp.org/rec/conf/icde/0001KR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Peer groups leverage the presence of knowledgeable individuals in order to increase the knowledge level of other participants. The ‘smart’ formation of peer groups can thus play a crucial role in educational settings, including online social networks and learning platforms. Indeed, the targeted groups formation problem, where the objective is to maximize a measure of aggregate knowledge, has received considerable attention in recent literature. In this paper we initiate a dynamic variant of the problem that, unlike previous works, allows the change of group composition over time while still targeting to maximize the aggregated knowledge level. The problem is studied in a principled way, using a realistic learning gain function and for two different interaction modes among the group members. On the algorithmic side, we present DyGroups, a generic algorithmic framework that is greedy in nature and highly scalable. We present non-trivial proofs to demonstrate theoretical guarantees for DyGroups in a special case. We also present real peer learning experiments with humans, and perform synthetic data experiments to demonstrate the effectiveness of our proposed solutions by comparing against multiple appropriately selected baseline algorithms.}
}


@inproceedings{DBLP:conf/icde/ZhangLH021,
	author = {Mengxuan Zhang and
                  Lei Li and
                  Wen Hua and
                  Xiaofang Zhou},
	title = {Efficient 2-Hop Labeling Maintenance in Dynamic Small-World Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {133--144},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00019},
	doi = {10.1109/ICDE51399.2021.00019},
	timestamp = {Tue, 21 Mar 2023 20:50:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhangLH021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shortest path computation is a fundamental operation in small-world networks and index-based methods, especially 2-hop labeling, are commonly applied which have achieved high query efficiency. However, small-world networks keep evolving in real life, making it indispensable to study the maintenance of shortest path index. In this work, we adopt the state-of-the-art Parallel Shortest-distance Labeling (PSL) as the underlying 2-hop labeling construction method, and design algorithms to support efficient update of the index given edge weight change (increase and decrease) in the network. Specifically, we focus on weighted PSL (WPSL) and propose the update propagation mechanism for both synchronous propagation and asynchronous propagation. We then identify the curse of pruning power generated for the propagation under edge weight increase, and solve this problem with a balance between index size and effectiveness. Finally, we extend the proposed asynchronous propagation method to Pruned Landmark Labeling (PLL) for faster index maintenance and query processing with smaller index size. Our experimental results on real-life and synthetic networks demonstrate the superiority of our algorithms on index maintenance.}
}


@inproceedings{DBLP:conf/icde/TangCSGJL21,
	author = {Peng Tang and
                  Rui Chen and
                  Sen Su and
                  Shanqing Guo and
                  Lei Ju and
                  Gaoyuan Liu},
	title = {Differentially Private Publication of Multi-Party Sequential Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {145--156},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00020},
	doi = {10.1109/ICDE51399.2021.00020},
	timestamp = {Tue, 07 Nov 2023 15:17:55 +0100},
	biburl = {https://dblp.org/rec/conf/icde/TangCSGJL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a set of local sequential datasets held by multiple parties, we study the problem of publishing a synthetic dataset that preserves approximate sequentiality information of the integrated dataset while satisfying differential privacy for each local dataset. The existing solutions for publishing differentially private sequential data in the centralized setting mostly adopt tree-based approaches. Such approaches rely on different tree structures that encode sequential data’s statistical information. The construction of a tree structure is normally done by recursively splitting nodes whose noisy scores (e.g., entropy or count) are larger than a given threshold. However, extending similar ideas to the multi-party setting is challenging. First, the comparison between noisy scores and a given threshold needs to be done in a distributed manner without letting the parties know the noisy scores, while satisfying differential privacy for each local dataset. Second, in the multi-party setting the large number of node splitting decisions incurs prohibitive computation costs. In addressing the above challenges, we present DPST, a distributed prediction suffix tree construction solution. In DPST, we first introduce a novel node splitting decision method that calculates the comparison result under encryption with substantially improved efficiency. Then we present a novel batch-based tree construction approach to reduce the computation costs. In order to achieve high parallel performance without incurring any extra communication cost, we introduce the conjunction and slide methods to ensure that each batch contains a stable number of carefully arranged decision tasks. Extensive experiments on real datasets demonstrate that our DPST solution offers desirable data utility with low computation and communication costs.}
}


@inproceedings{DBLP:conf/icde/ZeighamiGS21,
	author = {Sepanta Zeighami and
                  Gabriel Ghinita and
                  Cyrus Shahabi},
	title = {Secure Dynamic Skyline Queries Using Result Materialization},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {157--168},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00021},
	doi = {10.1109/ICDE51399.2021.00021},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZeighamiGS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Skyline computation is an increasingly popular query, with broad applicability to many domains. Given the trend to outsource databases, and due to the sensitive nature of the data (e.g., in healthcare), it is essential to evaluate skylines on encrypted datasets. Research efforts acknowledged the importance of secure skyline computation, but existing solutions suffer from several shortcomings: (i) they only provide ad-hoc security; (ii) they are prohibitively expensive; or (iii) they rely on assumptions such as the presence of multiple non-colluding parties in the protocol. Inspired by solutions for secure nearest-neighbors, we conjecture that a secure and efficient way to compute skylines is through result materialization. However, materialization is much more challenging for skylines queries due to large space requirements. We show that pre-computing skyline results while minimizing storage overhead is NP-hard, and we provide heuristics that solve the problem more efficiently, while maintaining storage at reasonable levels. Our algorithms are novel and also applicable to regular skyline computation, but we focus on the encrypted setting where materialization reduces the response time of skyline queries from hours to seconds. Extensive experiments show that we clearly outperform existing work in terms of performance, and our security analysis proves that we obtain a small (and quantifiable) data leakage.}
}


@inproceedings{DBLP:conf/icde/Takagi00Y21,
	author = {Shun Takagi and
                  Tsubasa Takahashi and
                  Yang Cao and
                  Masatoshi Yoshikawa},
	title = {{P3GM:} Private High-Dimensional Data Release via Privacy Preserving
                  Phased Generative Model},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {169--180},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00022},
	doi = {10.1109/ICDE51399.2021.00022},
	timestamp = {Tue, 21 Mar 2023 20:50:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/Takagi00Y21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How can we release a massive volume of sensitive data while mitigating privacy risks? Privacy-preserving data synthesis enables the data holder to outsource analytical tasks to an untrusted third party. The state-of-the-art approach for this problem is to build a generative model under differential privacy, which offers a rigorous privacy guarantee. However, the existing method cannot adequately handle high dimensional data. In particular, when the input dataset contains a large number of features, the existing techniques require injecting a prohibitive amount of noise to satisfy differential privacy, which results in the outsourced data analysis meaningless. To address the above issue, this paper proposes privacy-preserving phased generative model (P3GM), which is a differentially private generative model for releasing such sensitive data. P3GM employs the two-phase learning process to make it robust against the noise, and to increase learning efficiency (e.g., easy to converge). We give theoretical analyses about the learning complexity and privacy loss in P3GM. We further experimentally evaluate our proposed method and demonstrate that P3GM significantly outperforms existing solutions. Compared with the state-of-the-art methods, our generated samples look fewer noises and closer to the original data in terms of data diversity. Besides, in several data mining tasks with synthesized data, our model outperforms the competitors in terms of accuracy.}
}


@inproceedings{DBLP:conf/icde/LuoWXO21,
	author = {Xinjian Luo and
                  Yuncheng Wu and
                  Xiaokui Xiao and
                  Beng Chin Ooi},
	title = {Feature Inference Attack on Model Predictions in Vertical Federated
                  Learning},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {181--192},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00023},
	doi = {10.1109/ICDE51399.2021.00023},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuoWXO21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an emerging paradigm for facilitating multiple organizations’ data collaboration without revealing their private data to each other. Recently, vertical FL, where the participating organizations hold the same set of samples but with disjoint features and only one organization owns the labels, has received increased attention. This paper presents several feature inference attack methods to investigate the potential privacy leakages in the model prediction stage of vertical FL. The attack methods consider the most stringent setting that the adversary controls only the trained vertical FL model and the model predictions, relying on no background information of the attack target’s data distribution. We first propose two specific attacks on the logistic regression (LR) and decision tree (DT) models, according to individual prediction output. We further design a general attack method based on multiple prediction outputs accumulated by the adversary to handle complex models, such as neural networks (NN) and random forest (RF) models. Experimental evaluations demonstrate the effectiveness of the proposed attacks and highlight the need for designing private mechanisms to protect the prediction outputs in vertical FL.}
}


@inproceedings{DBLP:conf/icde/GaoSLXQXMKS21,
	author = {Peng Gao and
                  Fei Shao and
                  Xiaoyuan Liu and
                  Xusheng Xiao and
                  Zheng Qin and
                  Fengyuan Xu and
                  Prateek Mittal and
                  Sanjeev R. Kulkarni and
                  Dawn Song},
	title = {Enabling Efficient Cyber Threat Hunting With Cyber Threat Intelligence},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {193--204},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00024},
	doi = {10.1109/ICDE51399.2021.00024},
	timestamp = {Wed, 08 Sep 2021 16:17:48 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GaoSLXQXMKS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log-based cyber threat hunting has emerged as an important solution to counter sophisticated attacks. However, existing approaches require non-trivial efforts of manual query construction and have overlooked the rich external threat knowledge provided by open-source Cyber Threat Intelligence (OSCTI). To bridge the gap, we propose ThreatRaptor, a system that facilitates threat hunting in computer systems using OSCTI. Built upon system auditing frameworks, ThreatRaptor provides (1) an unsupervised, light-weight, and accurate NLP pipeline that extracts structured threat behaviors from unstructured OSCTI text, (2) a concise and expressive domain-specific query language, TBQL, to hunt for malicious system activities, (3) a query synthesis mechanism that automatically synthesizes a TBQL query for hunting, and (4) an efficient query execution engine to search the big audit logging data. Evaluations on a broad set of attack cases demonstrate the accuracy and efficiency of ThreatRaptor in practical threat hunting.}
}


@inproceedings{DBLP:conf/icde/MenetreyPFS21,
	author = {J{\"{a}}mes M{\'{e}}n{\'{e}}trey and
                  Marcelo Pasin and
                  Pascal Felber and
                  Valerio Schiavoni},
	title = {Twine: An Embedded Trusted Runtime for WebAssembly},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {205--216},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00025},
	doi = {10.1109/ICDE51399.2021.00025},
	timestamp = {Tue, 07 May 2024 20:05:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MenetreyPFS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WebAssembly is an Increasingly popular lightweight binary instruction format, which can be efficiently embedded and sandboxed. Languages like C, C++, Rust, Go, and many others can be compiled into WebAssembly. This paper describes Twine, a WebAssembly trusted runtime designed to execute unmodified, language-independent applications. We leverage Intel SGX to build the runtime environment without dealing with language-specific, complex APIs. While SGX hardware provides secure execution within the processor, Twine provides a secure, sandboxed software runtime nested within an SGX enclave, featuring a WebAssembly system interface (WASI) for compatibility with unmodified WebAssembly applications. We evaluate Twine with a large set of general-purpose benchmarks and real-world applications. In particular, we used Twine to implement a secure, trusted version of SQLite, a well-known full-fledged embeddable database. We believe that such a trusted database would be a reasonable component to build many larger application services. Our evaluation shows that SQLite can be fully executed inside an SGX enclave via WebAssembly and existing system interface, with similar average performance overheads. We estimate that the performance penalties measured are largely compensated by the additional security guarantees and its full compatibility with standard WebAssembly. An indepth analysis of our results indicates that performance can be greatly improved by modifying some of the underlying libraries. We describe and implement one such modification in the paper, showing up to 4.1 × speedup. Twine is open-source, available at GitHub along with instructions to reproduce our experiments.}
}


@inproceedings{DBLP:conf/icde/LiuPMY0WL21,
	author = {Chi Harold Liu and
                  Chengzhe Piao and
                  Xiaoxin Ma and
                  Ye Yuan and
                  Jian Tang and
                  Guoren Wang and
                  Kin K. Leung},
	title = {Modeling Citywide Crowd Flows using Attentive Convolutional {LSTM}},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {217--228},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00026},
	doi = {10.1109/ICDE51399.2021.00026},
	timestamp = {Mon, 03 Jan 2022 22:33:29 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiuPMY0WL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Understanding the movement patterns of humans and vehicles traveling in a city is important for many applications like emergency evacuation and rescue, as well as city planning and management. In this paper, we aim to predict citywide crowd flows within a period in the future to give aid to urban management, through modeling spatiotemporal patterns of recent crowd flows. We present a novel deep model for this task, called "AttConvLSTM", which leverages a convolutional LSTM (ConvLSTM), Convolutional Neural Networks (CNNs) along with an attention mechanism, where ConvLSTM keeps spatial information as intact as possible during sequential analysis, and the attention mechanism can focus important crowd flow variations which cannot be identified by the recurrent module. We conducted extensive experiments for performance evaluation using three large datasets, including Beijing Taxi dataset, Rome Taxi dataset, and Chengdu Didi chauffeuring trace. The experimental results show that AttConvLSTM significantly outperforms several widely-used baselines in terms of Root Mean Squared Error (RMSE), and Mean Average Percentage Error (MAPE), indicating that our approach can deal with crowd flows with different dynamics in both spatial and temporal domains, and make valid predictions several steps ahead.}
}


@inproceedings{DBLP:conf/icde/IslamHS21,
	author = {Fariha Tabassum Islam and
                  Tanzima Hashem and
                  Rifat Shahriyar},
	title = {A Privacy-Enhanced and Personalized Safe Route Planner with Crowdsourced
                  Data and Computation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {229--240},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00027},
	doi = {10.1109/ICDE51399.2021.00027},
	timestamp = {Mon, 05 Feb 2024 20:31:12 +0100},
	biburl = {https://dblp.org/rec/conf/icde/IslamHS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce a novel safe route planning problem and develop an efficient solution to ensure the travelers’ safety on roads. Though few research attempts have been made in this regard, all of them assume that people share their sensitive travel experiences with a centralized entity for finding the safest routes, which is not ideal in practice for privacy reasons. Furthermore, existing works formulate the safe route planning query in ways that do not meet a traveler’s need for safe travel on roads. Our approach finds the safest routes within a user-specified distance threshold based on the personalized travel experience of the knowledgeable crowd without involving any centralized computation. We develop a privacy preserving model to quantify the travel experience of a user into personalized safety scores. Our algorithms for finding the safest route further enhance user privacy by minimizing the exposure of personalized safety scores with others. We implement a working prototype of our solution on the Android platform. Extensive experiments using real datasets show that our approach finds the safest route in seconds with 50% less exposure of personalized safety scores.}
}


@inproceedings{DBLP:conf/icde/ZhaoGCH0Z21,
	author = {Yan Zhao and
                  Jiannan Guo and
                  Xuanhao Chen and
                  Jianye Hao and
                  Xiaofang Zhou and
                  Kai Zheng},
	title = {Coalition-based Task Assignment in Spatial Crowdsourcing},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {241--252},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00028},
	doi = {10.1109/ICDE51399.2021.00028},
	timestamp = {Thu, 14 Oct 2021 10:29:17 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoGCH0Z21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the fast-paced development of mobile networks and the widespread usage of mobile devices, Spatial Crowdsourcing (SC), which refers to assigning location-based tasks to moving workers, has drawn increasing attention in recent years. One of the critical issues in SC is task assignment that allocates tasks to appropriate workers. In this paper, we propose a novel SC problem, namely Coalition-based Task Assignment (CTA), where the spatial tasks (e.g., house removals, furniture installation) may require more than one workers (forming a coalition) to cooperate in order to maximize the overall rewards of workers. To tackle the CTA problem, we design both greedy method and equilibrium-based method. In particular, the greedy method aims to form a set of worker coalitions greedily to perform the tasks, in which we introduce an acceptance possibility to find the high-value task assignments. In the equilibrium-based algorithm, workers form coalitions in sequence and update their strategy (i.e., selecting a best-response task) at their turn, in order to maximize their own utility (i.e., reward of the coalition they stay in) until Nash equilibrium is reached. Since the equilibrium point obtained by the best-response approach is not unique and optimal in terms of total rewards, we further propose a simulated annealing scheme to find a better Nash equilibrium. The extensive experiments demonstrate the efficiency and effectiveness of the proposed methods on both real and synthetic datasets.}
}


@inproceedings{DBLP:conf/icde/AnX0X021,
	author = {Baoyi An and
                  Mingjun Xiao and
                  An Liu and
                  Xike Xie and
                  Xiaofang Zhou},
	title = {Crowdsensing Data Trading based on Combinatorial Multi-Armed Bandit
                  and Stackelberg Game},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {253--264},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00029},
	doi = {10.1109/ICDE51399.2021.00029},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/AnX0X021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowdsensing Data Trading (CDT), through which a platform can aggregate some data collected by a group of mobile users with sensing devices (a.k.a., data sellers) and sell the corresponding statistics to data consumers, has been recognized as a promising paradigm for large-scale data trading in recent years. It is critical to select sellers with high sensing qualities and maximize all trading participants’ profits simultaneously. However, most existing CDT systems either assume that sellers’ sensing qualities are known in advance or cannot realize concurrent profit maximization. In this paper, we propose a data trading mechanism based on Combinatorial Multi-Armed Bandit and three-stage Hierarchical Stackelberg game, called CMAB-HS, to tackle the problem of quality unknown seller selection and incentive strategy design. Our objective is to select a group of sellers to maximize the total sensing quality within time budget, and determine the optimal incentive strategy for each participant to maximize individual profit simultaneously. We theoretically prove that CMAB-HS achieves Stackelberg Equilibrium and a tight bound on regret. Additionally, we demonstrate its significant performances through extensive simulations on real data traces.}
}


@inproceedings{DBLP:conf/icde/ZhaoZGYPJ21,
	author = {Yan Zhao and
                  Kai Zheng and
                  Jiannan Guo and
                  Bin Yang and
                  Torben Bach Pedersen and
                  Christian S. Jensen},
	title = {Fairness-aware Task Assignment in Spatial Crowdsourcing: Game-Theoretic
                  Approaches},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {265--276},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00030},
	doi = {10.1109/ICDE51399.2021.00030},
	timestamp = {Wed, 07 Dec 2022 23:09:59 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoZGYPJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread diffusion of smartphones offers a capable foundation for the deployment of Spatial Crowdsourcing (SC), where mobile users, called workers, perform location- dependent tasks assigned to them. A key issue in SC is how best to assign tasks, e.g., the delivery of food and packages, to appropriate workers. Specifically, we study the problem of Fairness-aware Task Assignment (FTA) in SC, where tasks are to be assigned in a manner that achieves some notion of fairness across workers. In particular, we aim to minimize the payoff difference among workers while maximizing the average worker payoff. To solve the problem, we first generate so-called Valid Delivery Point Sets (VDPSs) for each worker according to an approach that exploits dynamic programming and distance- constrained pruning. Next, we show that FTA is NP-hard and proceed to propose two heuristic algorithms, a Fairness-aware Game-Theoretic (FGT) algorithm and an Improved Evolutionary Game-Theoretic (IEGT) algorithm. More specifically, we formulate FTA as a multi-player game. In this setting, the FGT approach represents a best-response method with sequential and asynchronous updates of workers' strategies, given by the VDPSs, that achieves a satisfying task assignment when a pure Nash equilibrium is reached. Next, the IEGT approach considers a setting with a large population of workers that repeatedly engage in strategic interactions. The IEGT approach exploits replicator dynamics that cause the whole population to evolve and choose better resources, i.e., VDPSs. Using the property of evolutionary equilibrium, a satisfying task assignment is obtained that corresponds to a stable state with similar payoffs among workers and good average worker payoff. Extensive experiments offer insight into the effectiveness and efficiency of the proposed solutions.}
}


@inproceedings{DBLP:conf/icde/YangZFCPY021,
	author = {Jingru Yang and
                  Xiaoman Zhao and
                  Ju Fan and
                  Gong Chen and
                  Chong Peng and
                  Sheng Yao and
                  Xiaoyong Du},
	title = {A Human-in-the-loop Approach to Social Behavioral Targeting},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {277--288},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00031},
	doi = {10.1109/ICDE51399.2021.00031},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YangZFCPY021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Behavioral targeting plays an important role in social media advertising for capturing users’ preferences of ads. While existing studies of behavioral targeting mainly focus on the user behaviors that have explicit correlations with ads, such as ad clicking and web search, many implicit relationships between users and ads, which reside in a variety of heterogeneous sources in social media platforms, are not utilized to enhance the prediction of users’ preferences of ads.In this paper, we propose a two-pronged approach to behavioral targeting that effectively addresses the above difficulties. First, we model the implicit relationships between users and ads as a heterogeneous information network (HIN), and propose a method that first performs representation learning in the HIN and then uses the learned representations to train a prediction model for boosting the performance of behavioral targeting. Second, we develop a human-in-the-loop framework to address the incompleteness challenge in HIN construction that may result in inferior performance of model prediction. The framework judiciously selects the most "beneficial" tasks to ask human for completing the HIN and utilizes the results from human to update the representation learning of HIN. We validate the effectiveness of our approach through extensive experiments on real datasets collected from WeChat, the largest social media platform in China. The experimental results show that our approach is effective at constructing a high-quality HIN at a low cost of human involvement, and the HIN can significantly improve the performance of social behavioral targeting.}
}


@inproceedings{DBLP:conf/icde/Li0WHLW21,
	author = {Kaiyu Li and
                  Guoliang Li and
                  Yong Wang and
                  Yan Huang and
                  Zitao Liu and
                  Zhongqin Wu},
	title = {CrowdRL: An End-to-End Reinforcement Learning Framework for Data Labelling},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {289--300},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00032},
	doi = {10.1109/ICDE51399.2021.00032},
	timestamp = {Sat, 11 Jun 2022 17:14:16 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Li0WHLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data labelling is very important in many database and machine learning applications. Traditional methods rely on humans (workers or experts) to acquire labels. However, the human cost is rather expensive for a large dataset. Active learning based methods only label a small set of data with large uncertainty, train a model on these labelled data, and use the trained model to label the remainder unlabelled data. However they have two limitations. First, they cannot judiciously select appropriate data (task selection) and assign the tasks to proper humans (task assignment). Moreover, they independently process task selection and task assignment, which cannot capture the correlation between them. Second, they simply infer the truth of a task based on the answers from humans and the trained model (truth inference) by independently modeling humans and models. In other words, they ignore the correlation between them (the labelled data may have noise caused by humans with biases, and the model trained by the noisy labels may bring additional biases), and thus lead to poor inference results.To address these limitations, in this paper, we propose CrowdRL, an end-to-end reinforcement learning (RL) based framework for data labelling. To the best of our knowledge, CrowdRL is the first RL framework designed for the data labelling workflow by seamlessly integrating task selection, task assignment and truth inference together. CrowdRL fully utilizes the power of heterogeneous annotators (experts and crowdsourcing workers) and machine learning models together to infer the truth, which highly improves the quality of data labelling. CrowdRL uses RL to model task assignment and task selection, and designs an agent to judiciously assign tasks to appropriate workers. CrowdRL jointly models the answers of workers, experts and models, and designs a joint inference model to infer the truths. Experimental results on real datasets show that CrowdRL outperforms state-of-the-art approaches with the same (even fewer) monetary cost while achieving 5%-20% higher accuracy.}
}


@inproceedings{DBLP:conf/icde/ZhengL0CL21,
	author = {Guanjie Zheng and
                  Chang Liu and
                  Hua Wei and
                  Chacha Chen and
                  Zhenhui Li},
	title = {Rebuilding City-Wide Traffic Origin Destination from Road Speed Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {301--312},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00033},
	doi = {10.1109/ICDE51399.2021.00033},
	timestamp = {Mon, 05 Feb 2024 20:31:12 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhengL0CL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Understanding city-wide traffic problems may benefit many downstream applications, such as city planning and public transportation development. One key step to understand traffic is to reveal how many people travel from one location to another during one period (we call TOD, short for temporal origin-destination). With TOD, we can rebuild the city-wide traffic by simulating the volume and speed on each road segment.Frequently used mobility data, e.g., GPS trajectories, surveillance cameras, can only cover a subset of vehicles or selected regions of the city. Hence, we propose to use pervasive speed data to recover TOD, and use other mobility data as auxiliary data. To the best of our knowledge, we are the first to work on this challenging problem. It is highly challenging because the speed is generated from a complex process from TOD, and there exists multiple TOD distributions that may generate similar city-wide road speed observations. We propose a new method that models the complex process via separate modules and takes auxiliary data to eliminate infeasible solutions. Extensive experiments on synthetic and real datasets have shown the superior performance of our model over baselines.}
}


@inproceedings{DBLP:conf/icde/WangYWZMW21,
	author = {Yishu Wang and
                  Ye Yuan and
                  Hao Wang and
                  Xiangmin Zhou and
                  Congcong Mu and
                  Guoren Wang},
	title = {Constrained Route Planning over Large Multi-Modal Time-Dependent Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {313--324},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00034},
	doi = {10.1109/ICDE51399.2021.00034},
	timestamp = {Wed, 17 Jul 2024 16:21:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangYWZMW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Constrained route planning (CRP) on transportation networks has been extensively studied because of its broad applications, such as route recommendation. However, the existing works on CRP neglect the time-dependent and multi-modal properties of transportation networks. This paper proposes an approach for CRP over multi-modal time-dependent networks. Specifically, we design two novel constrained route planning algorithms, function-dependent routing and labeling-index-based routing. While function-dependent routing generates an accurate route to CRP by traversing the network, labeling-index-based one ensures the fast response with the support of an efficient index and the compression scheme of networks. In order to demonstrate the efficiency and effectiveness of our proposed algorithms, experiments are performed over real datasets.}
}


@inproceedings{DBLP:conf/icde/ChenYDCW21,
	author = {Di Chen and
                  Ye Yuan and
                  Wenjin Du and
                  Yurong Cheng and
                  Guoren Wang},
	title = {Online Route Planning over Time-Dependent Road Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {325--335},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00035},
	doi = {10.1109/ICDE51399.2021.00035},
	timestamp = {Thu, 22 Jul 2021 07:58:24 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenYDCW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Route planning problem has been well studied in static road networks, since it has wide applications in transportation networks. However, recently there have been more actual requirements that current path planning algorithms cannot solve, such as food delivery, ride-sharing and crowdsourced parcel delivery. These requirements are in a dynamic scenario, but the existing algorithms are offline. These requirements need to find the least total travel time path from the source through the nodes that appear dynamically over time to the destination, which referred to as the online route planning. On the other hand, the costs of edges in road networks always change over time, since real road networks are dynamic. Such road networks can be modelled as time-dependent road networks. Therefore, in this paper, we study the online route planning over time-dependent road networks (ORPTD). We formally proof that the ORPTD problem is NP-complete and its competitive ratio cannot be guaranteed. To attack the hard problem, we first propose two efficient heuristic algorithms. To adapt to large-scale time-dependent road networks, we further speed up the two heuristic algorithms by incorporating indexing techniques into them. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on real datasets.}
}


@inproceedings{DBLP:conf/icde/ZhangLHMCZ21,
	author = {Mengxuan Zhang and
                  Lei Li and
                  Wen Hua and
                  Rui Mao and
                  Pingfu Chao and
                  Xiaofang Zhou},
	title = {Dynamic Hub Labeling for Road Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {336--347},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00036},
	doi = {10.1109/ICDE51399.2021.00036},
	timestamp = {Tue, 21 Mar 2023 20:50:58 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhangLHMCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shortest path finding is the building block of various applications in road networks and the index-based algorithms, especially hub labeling, can boost the query performance dramatically. However, the traffic condition keeps changing in real life, making the pre-computed index unable to answer the query correctly. In this work, we adopt the state-of-the-art tree decomposition-based hub labeling as the underlying index, and design efficient algorithms to incrementally maintain the index. Specifically, we first analyze the structural stability of the index in dynamic road networks which enables us to concentrate on label value maintenance. We then introduce the minimum weight property and minimum distance property to guarantee the index correctness without graph traversal. Moreover, we propose the star-centric paradigm for tracing index change and design various pruning techniques to further accelerate the index maintenance. Finally, we extend our algorithms to batch mode for shared computation, extend to structural maintenance for full types of update, and generalize to all kinds of TDHL. Our experimental results validate the superiority of our proposals over existing solutions on both index maintenance and query processing.}
}


@inproceedings{DBLP:conf/icde/00020BF21,
	author = {Haitao Yuan and
                  Guoliang Li and
                  Zhifeng Bao and
                  Ling Feng},
	title = {An Effective Joint Prediction Model for Travel Demands and Traffic
                  Flows},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {348--359},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00037},
	doi = {10.1109/ICDE51399.2021.00037},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/00020BF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study how to jointly predict travel demands and traffic flows for all regions of a city at a future time interval. From an empirical analysis of traffic data, we outline three desired properties, namely region-level correlations, temporal periodicity and inter-traffic correlations. Then, we propose a comprehensive neural network based traffic prediction model, where various effective embeddings or encodings are designed to capture the aforementioned properties. First, we design effective region embeddings to capture two forms of region-level correlations: spatially close regions have similar embeddings, and regions with similar properties (e.g., the number of POIs and the number of roads in a region) other than locations have similar embeddings. Second, we extract the "day-in-week" and "time-in-day" and utilize the temporal periodicity in designing the embeddings for time intervals. Third, we propose an effective encoding for past traffic data which captures two forms of inter-traffic correlations - the correlation between past and future traffic, and the correlation between travel demands and traffic flows within past traffic data. Extensive experiments on two real datasets verify the high effectiveness of our model.}
}


@inproceedings{DBLP:conf/icde/HuangWZ021,
	author = {Shuai Huang and
                  Yong Wang and
                  Tianyu Zhao and
                  Guoliang Li},
	title = {A Learning-based Method for Computing Shortest Path Distances on Road
                  Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {360--371},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00038},
	doi = {10.1109/ICDE51399.2021.00038},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangWZ021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computing the shortest path distances between two vertices on road networks is a core operation in many real-world applications, e.g., finding the closest taxi/hotel. However existing techniques have several limitations. First, traditional Dijkstra-based methods have long latency and cannot meet the high-performance requirement. Second, existing indexing-based methods either involve huge index sizes or have poor performance. To address these limitations, in this paper we propose a learning-based method which can efficiently compute an approximate shortest-path distance such that (1) the performance is super fast, e.g., taking 60-150 nanoseconds; (2) the error ratio of the approximate results is super small, e.g., below 0.7%; (3) scales well to large road networks, e.g., millions of nodes. The key idea is to first embed the road networks into a low dimensional space for capturing the distance relations between vertices, get an embedded vector for each vertex, and then perform a distance metric (L 1 metric) on the embedded vectors to approximate shortest-path distances. We propose a hierarchical model to represent the embedding, and design an effective method to train the model. We also design a fine-tuning method to judiciously select high-quality training data. Extensive experiments on real-world datasets show that our embedding based approach significantly outperforms the state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/Li0WTHQF021,
	author = {Anran Li and
                  Lan Zhang and
                  Junhao Wang and
                  Juntao Tan and
                  Feng Han and
                  Yaxuan Qin and
                  Nikolaos M. Freris and
                  Xiang{-}Yang Li},
	title = {Efficient Federated-Learning Model Debugging},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {372--383},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00039},
	doi = {10.1109/ICDE51399.2021.00039},
	timestamp = {Thu, 23 Jun 2022 19:56:00 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Li0WTHQF021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables large amounts of participants to construct a global learning model, while storing training data privately at each client device. A fundamental issue in this framework is the susceptibility to the erroneous training data. This problem is especially challenging due to the invisibility of clients’ local training data and training process, as well as the resource constraints of a large number of mobile and edge devices. In this paper, we try to tackle this challenging issue by introducing the first FL debugging framework, FLDebugger, for mitigating test error caused by erroneous training data. The pro-posed solution traces the global model’s bugs (test errors), jointly through the training log and the underlying learning algorithm, back to first identify the clients and subsequently their training samples that are most responsible for the errors. In addition, we devise an influence-based participant selection strategy to fix bugs as well as to accelerate the convergence of model retraining. The performance of the identification algorithm is evaluated via extensive experiments on a real AIoT system (50 clients, including 20 edge computers, 20 laptops and 10 desktops) and in larger-scale simulated environments. The evaluation results attest to that our framework achieves accurate and efficient identification of negatively influential clients and samples, and significantly improves the model performance by fixing bugs.}
}


@inproceedings{DBLP:conf/icde/ZhouLLOWY21,
	author = {Pan Zhou and
                  Qian Lin and
                  Dumitrel Loghin and
                  Beng Chin Ooi and
                  Yuncheng Wu and
                  Hongfang Yu},
	title = {Communication-efficient Decentralized Machine Learning over Heterogeneous
                  Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {384--395},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00040},
	doi = {10.1109/ICDE51399.2021.00040},
	timestamp = {Thu, 20 Jun 2024 15:06:42 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhouLLOWY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the last few years, distributed machine learning has been usually executed over heterogeneous networks such as a local area network within a multi-tenant cluster or a wide area network connecting data centers and edge clusters. In these heterogeneous networks, the link speeds among worker nodes vary significantly, making it challenging for state-of-the-art machine learning approaches to perform efficient training. Both centralized and decentralized training approaches suffer from low-speed links. In this paper, we propose a decentralized approach, namely NetMax, that enables worker nodes to communicate via high-speed links and, thus, significantly speed up the training process. NetMax possesses the following novel features. First, it consists of a novel consensus algorithm that allows worker nodes to train model copies on their local dataset asynchronously and exchange information via peer-to-peer communication to synchronize their local copies, instead of a central master node (i.e., parameter server). Second, each worker node selects one peer randomly with a fine-tuned probability to exchange information per iteration. In particular, peers with high-speed links are selected with high probability. Third, the probabilities of selecting peers are designed to minimize the total convergence time. Moreover, we mathematically prove the convergence of NetMax. We evaluate NetMax on heterogeneous cluster networks and show that it achieves speedups of 3.7×, 3.4×, and 1.9× in comparison with the state-of-the-art decentralized training approaches Prague, Allreduce-SGD, and AD-PSGD, respectively.}
}


@inproceedings{DBLP:conf/icde/SongZLSFDS21,
	author = {Fei Song and
                  Khaled Zaouk and
                  Chenghao Lyu and
                  Arnab Sinha and
                  Qi Fan and
                  Yanlei Diao and
                  Prashant J. Shenoy},
	title = {Spark-based Cloud Data Analytics using Multi-Objective Optimization},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {396--407},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00041},
	doi = {10.1109/ICDE51399.2021.00041},
	timestamp = {Tue, 21 Mar 2023 20:50:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/SongZLSFDS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data analytics in the cloud has become an integral part of enterprise businesses. Big data analytics systems, however, still lack the ability to take task objectives such as user performance goals and budgetary constraints and automatically configure an analytic job to achieve these objectives. This paper presents UDAO, a Spark-based Unified Data Analytics Optimizer that can automatically determine a cluster configuration with a suitable number of cores as well as other system parameters that best meet the task objectives. At a core of our work is a principled multi-objective optimization (MOO) approach that computes a Pareto optimal set of configurations to reveal tradeoffs between different objectives, recommends a new Spark configuration that best explores such tradeoffs, and employs novel optimizations to enable such recommendations within a few seconds. Detailed experiments using benchmark workloads show that our MOO techniques provide a 2-50x speedup over existing MOO methods, while offering good coverage of the Pareto frontier. Compared to Ottertune, a state-of-the-art performance tuning system, UDAO recommends Spark configurations that yield 26%-49% reduction of running time of the TPCx-BB benchmark while adapting to different user preferences on multiple objectives.}
}


@inproceedings{DBLP:conf/icde/Nawab21,
	author = {Faisal Nawab},
	title = {WedgeChain: {A} Trusted Edge-Cloud Store With Asynchronous (Lazy)
                  Trust},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {408--419},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00042},
	doi = {10.1109/ICDE51399.2021.00042},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Nawab21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose WedgeChain, a data store that spans both edge and cloud nodes (an edge-cloud system). WedgeChain consists of a logging layer and a data indexing layer. In this study, we encounter two challenges: (1) edge nodes are untrusted and potentially malicious, and (2) edge-cloud coordination is expensive. WedgeChain tackles these challenges by the following proposals: (1) Lazy (asynchronous) certification: where data is committed at the untrusted edge and then lazily certified at the cloud node. This lazy certification method takes advantage of the observation that an untrusted edge node is unlikely to act maliciously if it knows it will be detected (and punished) eventually. Our lazy certification method guarantees that malicious acts (i.e., lying) are eventually detected. (2) Data-free certification: our lazy certification method only needs to send digests of data to the cloud—instead of sending all data to the cloud—which enables saving network and cloud resources and reduce costs. (3) LSMerkle: we extend a trusted index (mLSM [32]) to enable indexing data at the edge while utilizing lazy and data-free certification.}
}


@inproceedings{DBLP:conf/icde/MittalN21,
	author = {Natasha Mittal and
                  Faisal Nawab},
	title = {CooLSM: Distributed and Cooperative Indexing Across Edge and Cloud
                  Machines},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {420--431},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00043},
	doi = {10.1109/ICDE51399.2021.00043},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MittalN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We tackle one of the fundamental data management challenges in edge-cloud computing, the problem of data indexing. We propose Cooperative LSM (CooLSM), a distributed Log-Structured Merge Tree that is designed to overcome the unique challenges of edge-cloud indexing such as machine and workload heterogeneity and the communication latency asymmetry between the edge and the cloud. To tackle these challenges, CooLSM deconstructs the LSM tree [23] into its basic parts. This deconstruction allows a better distribution and placement of resources across edge and cloud devices. For example, append-specific functionality is managed at the edge to ensure appending and serving data in real-time, whereas resource-intensive operations such as compaction and querying is managed at the cloud where more compute resources are available.}
}


@inproceedings{DBLP:conf/icde/PedreiraDPLSTLG21,
	author = {Pedro Pedreira and
                  Amit Dutta and
                  Sergey Pershin and
                  Lin Liu and
                  Sushant Shringarpure and
                  Jialiang Tan and
                  Brian Landers and
                  Ge Gao and
                  Karen Pieper},
	title = {Interactive Analytic DBMSs: Breaching the Scalability Wall},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {432--443},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00044},
	doi = {10.1109/ICDE51399.2021.00044},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PedreiraDPLSTLG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analytic DBMSs optimized for query interactivity commonly push the computation down to storage nodes, thus avoiding large network transfers and keeping query execution wall-time to a minimum. In these systems, data is sharded and stored locally by cluster nodes, which must all participate in query execution. As the system scales-out, hardware failures and other non-deterministic sources of tail latency start to dominate, to a point where query latency and success ratio increasingly violate the system’s SLA. We refer to this tipping point as the system’s scalability wall, when sharding data between more nodes only worsens the problem.This paper describes how an analytic DBMS optimized for low-latency queries can breach the scalability wall by sharding different tables to different subsets of cluster nodes — a strategy we call partial sharding — and reduce the query fan-out. Because partial sharding requires the DBMS to implement many tedious and complex shard management tasks, such as shard mapping, load balancing and fault tolerance, this paper describes how a database system can leverage an external general-purpose shard management service for such tasks. We present a case study based on Cubrick, an in-memory analytic DBMS developed at Facebook, highlighting the integration points with a shard management framework called Shard Manager. Finally, we describe the many design decisions, pitfalls and lessons learned during this process, which eventually allowed Cubrick to scale to thousands of nodes.}
}


@inproceedings{DBLP:conf/icde/HarmouchPN21,
	author = {Hazar Harmouch and
                  Thorsten Papenbrock and
                  Felix Naumann},
	title = {Relational Header Discovery using Similarity Search in a Table Corpus},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {444--455},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00045},
	doi = {10.1109/ICDE51399.2021.00045},
	timestamp = {Mon, 05 Feb 2024 20:31:12 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HarmouchPN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Column headers are among the most relevant types of meta-data for relational tables, because they provide meaning and context in which the data is to be interpreted. Headers play an important role in many data integration, exploration, and cleaning scenarios, such as schema matching, knowledge base augmentation, and similarity search. Unfortunately, in many cases column headers are missing, because they were never defined properly, are meaningless, or have been lost during data extraction, transmission, or storage. For example, around one third of the tables on the Web have missing headers. Missing headers leave abundant tabular data shrouded and inaccessible to many data-driven applications.We introduce a fully automated, multi-phase system that discovers table column headers for cases where headers are missing, meaningless, or unrepresentative for the column values. It leverages existing table headers from web tables to suggest human-understandable, representative, and consistent headers for any target table. We evaluate our system on tables extracted from Wikipedia. Overall, 60% of the automatically discovered table headers are exact and complete. Considering more header candidates, top-5 for example, increases this percentage to 72%.}
}


@inproceedings{DBLP:conf/icde/DongT0O21,
	author = {Yuyang Dong and
                  Kunihiro Takeoka and
                  Chuan Xiao and
                  Masafumi Oyamada},
	title = {Efficient Joinable Table Discovery in Data Lakes: {A} High-Dimensional
                  Similarity-Based Approach},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {456--467},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00046},
	doi = {10.1109/ICDE51399.2021.00046},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DongT0O21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding joinable tables in data lakes is key procedure in many applications such as data integration, data augmentation, data analysis, and data market. Traditional approaches that find equi-joinable tables are unable to deal with misspellings and different formats, nor do they capture any semantic joins. In this paper, we propose PEXESO, a framework for joinable table discovery in data lakes. We target the case when textual values are embedded as high-dimensional vectors and columns are joined upon similarity predicates on high-dimensional vectors, hence to address the limitations of equi-join approaches and identify more meaningful results. To efficiently find joinable tables with similarity, we propose a block-and-verify method that utilizes pivot-based filtering. A partitioning technique is developed to cope with the case when the data lake is large and cannot fit in main memory. An experimental evaluation on real datasets shows that our solution identifies substantially more tables than equi-joins and outperforms other similarity-based options, and the join results are useful in data enrichment for machine learning tasks. The experiments also demonstrate the efficiency of the proposed method.}
}


@inproceedings{DBLP:conf/icde/KoutrasSIPBFLBK21,
	author = {Christos Koutras and
                  George Siachamis and
                  Andra Ionescu and
                  Kyriakos Psarakis and
                  Jerry Brons and
                  Marios Fragkoulis and
                  Christoph Lofi and
                  Angela Bonifati and
                  Asterios Katsifodimos},
	title = {Valentine: Evaluating Matching Techniques for Dataset Discovery},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {468--479},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00047},
	doi = {10.1109/ICDE51399.2021.00047},
	timestamp = {Sun, 06 Oct 2024 21:04:57 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KoutrasSIPBFLBK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data scientists today search large data lakes to discover and integrate datasets. In order to bring together disparate data sources, dataset discovery methods rely on some form of schema matching: the process of establishing correspondences between datasets. Traditionally, schema matching has been used to find matching pairs of columns between a source and a target schema. However, the use of schema matching in dataset discovery methods differs from its original use. Nowadays schema matching serves as a building block for indicating and ranking inter-dataset relationships. Surprisingly, although a discovery method’s success relies highly on the quality of the underlying matching algorithms, the latest discovery methods employ existing schema matching algorithms in an ad-hoc fashion due to the lack of openly-available datasets with ground truth, reference method implementations, and evaluation metrics.In this paper, we aim to rectify the problem of evaluating the effectiveness and efficiency of schema matching methods for the specific needs of dataset discovery. To this end, we propose Valentine, an extensible open-source experiment suite to execute and organize large-scale automated matching experiments on tabular data. Valentine includes implementations of seminal schema matching methods that we either implemented from scratch (due to absence of open source code) or imported from open repositories. The contributions of Valentine are: i) the definition of four schema matching scenarios as encountered in dataset discovery methods, ii) a principled dataset fabrication process tailored to the scope of dataset discovery methods and iii) the most comprehensive evaluation of schema matching techniques to date, offering insight on the strengths and weaknesses of existing techniques, that can serve as a guide for employing schema matching in future dataset discovery methods.}
}


@inproceedings{DBLP:conf/icde/ZouDXST0W21,
	author = {Xiangyu Zou and
                  Cai Deng and
                  Wen Xia and
                  Philip Shilane and
                  Haoliang Tan and
                  Haijun Zhang and
                  Xuan Wang},
	title = {Odess: Speeding up Resemblance Detection for Redundancy Elimination
                  by Fast Content-Defined Sampling},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {480--491},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00048},
	doi = {10.1109/ICDE51399.2021.00048},
	timestamp = {Sat, 30 Sep 2023 09:44:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZouDXST0W21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multiple data reduction techniques have been investigated to lower storage costs for a wide variety of customers. In this work, we focus on similarity-based delta compression, which calculates and stores the difference of very similar, but non-duplicate, chunks in storage systems. Delta compression is often implemented along with deduplication and has been shown to achieve a much higher compression ratio.Currently, the N-Transform method is the most popular and widely-used approach to generate features for data content (e.g. chunks) to detect similar candidates (and then apply delta compression). For delta compression systems, though, the throughput of N-Transform is often the bottleneck. Finesse is a high throughput variant of N-Transform, but it suffers from lower detection accuracy and compression ratio. The computation overhead of N-Transform consists of two parts: calculating the rolling hash across data and applying time-consuming transforms on each hash. In this work, we propose Odess, a fast resemblance detection approach, that uses a novel Content-Defined Sampling method to generate a much smaller proxy hash set and then applies transforms on this small hash set. This reduces the calculations in the transform step from being the bottleneck. Meanwhile, Odess also leverages the faster Gear hash to generate rolling hashes. Thus, Odess greatly reduces the computational overhead for resemblance detection while achieving high detection accuracy and high compression ratio.Our evaluation results show that Odess is ~ 5.4× (Finesse) and ~ 26.9× (N-Transform) faster (on average) at generating features for resemblance detection. When considering an end-to-end data reduction storage system, Odess increases throughput by ~ 1.36× (Finesse) and ~ 2.76× (N-Transform) while maintaining the compression ratio of N-Transform and increasing the compression ratio ~ 1.22× over Finesse.}
}


@inproceedings{DBLP:conf/icde/ZhongP21,
	author = {Guo Zhong and
                  Chi{-}Man Pun},
	title = {Latent Low-rank Graph Learning for Multimodal Clustering},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {492--503},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00049},
	doi = {10.1109/ICDE51399.2021.00049},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhongP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multimodal clustering has become a fundamental and important problem in the data mining community since the development of multimedia technology over the last two decades has led to a tremendous increase in unlabeled multimodal data. Although a panoply of multimodal subspace clustering methods shows promising performance via fusing information from different views of multimodal data, most of them consist of two sequential steps, i.e., learning a consensus affinity matrix from the original data and then feeding the resulting affinity matrix into the framework of spectral clustering. However, this leads to the suboptimal clustering performance due to the following limitations: 1) the two steps of learning the affinity matrix and clustering are carried out independently; 2) the affinity matrix may be unreliable; 3) the post-processing requirement, such as K-means. To address these issues, we propose a novel multimodal subspace clustering method via adaptively learning a similarity graph on a latent low-rank representation space. In particular, the number of connected components of the learned graph is precisely equal to the number of clusters, i.e., the optimal solution of the associated problem directly reveals the clustering structure of data. Extensive evaluations on several benchmark multimodal datasets demonstrate that the proposed approach outperforms state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/MasudDMJGD021,
	author = {Sarah Masud and
                  Subhabrata Dutta and
                  Sakshi Makkar and
                  Chhavi Jain and
                  Vikram Goyal and
                  Amitava Das and
                  Tanmoy Chakraborty},
	title = {Hate is the New Infodemic: {A} Topic-aware Modeling of Hate Speech
                  Diffusion on Twitter},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {504--515},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00050},
	doi = {10.1109/ICDE51399.2021.00050},
	timestamp = {Wed, 07 Dec 2022 23:09:59 +0100},
	biburl = {https://dblp.org/rec/conf/icde/MasudDMJGD021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online hate speech, particularly over microblogging platforms like Twitter, has emerged as arguably the most severe issue of the past decade. Several countries have reported a steep rise in hate crimes infuriated by malicious hate campaigns. While the detection of hate speech is one of the emerging research areas, the generation and spread of topic-dependent hate in the information network remain under-explored. In this work, we focus on exploring user behavior, which triggers the genesis of hate speech on Twitter and how it diffuses via retweets. We crawl a large-scale dataset of tweets, retweets, user activity history, and follower networks, comprising over 161 million tweets from more than 41 million unique users. We also collect over 600k contemporary news articles published online. We characterize different signals of information that govern these dynamics. Our analyses differentiate the diffusion dynamics in the presence of hate from usual information diffusion. This motivates us to formulate the modeling problem in a topic-aware setting with real-world knowledge. For predicting the initiation of hate speech for any given hashtag, we propose multiple feature-rich models, with the best performing one achieving a macro F1 score of 0.65. Meanwhile, to predict the retweet dynamics on Twitter, we propose RETINA, a novel neural architecture that incorporates exogenous influence using scaled dot-product attention. RETINA achieves a macro F1-score of 0.85, outperforming multiple state-of-the-art models. Our analysis reveals the superlative power of RETINA to predict the retweet dynamics of hateful content compared to the existing diffusion models.}
}


@inproceedings{DBLP:conf/icde/YaoS0021,
	author = {Xingyu Yao and
                  Yingxia Shao and
                  Bin Cui and
                  Lei Chen},
	title = {UniNet: Scalable Network Representation Learning with Metropolis-Hastings
                  Sampling},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {516--527},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00051},
	doi = {10.1109/ICDE51399.2021.00051},
	timestamp = {Sat, 09 Apr 2022 12:45:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YaoS0021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network representation learning (NRL) has been successfully adopted in various data mining and machine learning applications. Random walk based NRL is one popular paradigm, which uses a set of random walks to capture the network structural information, and then employs word2vec models to learn the low-dimensional representations. However, until now there is lack of a framework, which unifies existing random walk based NRL models and efficiently learns from large networks. The main obstacle comes from the diverse random walk models and the inefficient sampling method for the random walk generation. In this paper, we first introduce a new and efficient edge sampler based on Metropolis-Hastings sampling technique, and theoretically show the convergence property of the edge sampler to arbitrary discrete probability distributions. Then we propose a random walk model abstraction, in which users can easily define different transition probability by specifying dynamic edge weights and random walk states. The abstraction is efficiently supported by our edge sampler, since our sampler can draw samples from unnormalized probability distribution in constant time complexity. Finally, with the new edge sampler and random walk model abstraction, we carefully implement a scalable NRL framework called UniNet. We conduct extensive experiments with five random walk based NRL models over eleven real-world datasets, and the results verify the efficiency of UniNet over billion-edge networks.}
}


@inproceedings{DBLP:conf/icde/HuangLBL21,
	author = {Shixun Huang and
                  Yuchen Li and
                  Zhifeng Bao and
                  Zhao Li},
	title = {Towards Efficient Motif-based Graph Partitioning: An Adaptive Sampling
                  Approach},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {528--539},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00052},
	doi = {10.1109/ICDE51399.2021.00052},
	timestamp = {Sat, 30 Sep 2023 09:44:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangLBL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the problem of efficient motif-based graph partitioning (MGP). We observe that existing methods require to enumerate all motif instances to compute the exact edge weights for partitioning. However, the enumeration is prohibitively expensive against large graphs. We thus propose a sampling-based MGP (SMGP) framework that employs an unbiased sampling mechanism to efficiently estimate the edge weights while trying to preserve the partitioning quality. To further improve the effectiveness, we propose a novel adaptive sampling framework called SMGP+. SMGP+ iteratively partitions the input graph based on up-to-date estimated edge weights, and adaptively adjusts the sampling distribution so that edges that are more likely to affect the partitioning outcome will be prioritized for weight estimation. To our best knowledge, this is the first attempt to solve the MGP problem without employing exact edge weight computations, which gives hope for existing MGP methods to perform on complicated motifs in a scalable yet effective manner. Extensive experiments on seven real-world datasets have validated that our framework delivers competitive partitioning quality compared to existing workflows based on exact edge weights, while achieving orders of magnitude speedup.}
}


@inproceedings{DBLP:conf/icde/Park021,
	author = {Himchan Park and
                  Min{-}Soo Kim},
	title = {LineageBA: {A} Fast, Exact and Scalable Graph Generation for the Barab{\'{a}}si-Albert
                  Model},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {540--551},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00053},
	doi = {10.1109/ICDE51399.2021.00053},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Park021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Barabási-Albert(BA) model plays an important role in many domains since it can generate a scale-free graph having the degree exponents that real graphs have. However, due to the dependency among the edges generated at different time steps, the exact generation methods support only a single thread, and the parallel generation methods generate a graph only approximately. There is no method that can generate a large-scale graph following the BA model strictly using multiple threads. We propose a fast, exact, and scalable graph generation method called LineageBA that solves the above issue. We propose the concept of lineage relationship for reducing memory usage significantly and the detection of hash collisions for parallelizing the graph generation. Through extensive experiments, we have shown that LineageBA significantly outperforms the state-of-the-art BA graph generation methods and easily generates 2.5 trillion edges within four hours using a small cluster of PCs.}
}


@inproceedings{DBLP:conf/icde/ZhaoYT21,
	author = {Huan Zhao and
                  Quanming Yao and
                  Weiwei Tu},
	title = {Search to aggregate neighborhood for graph neural network},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {552--563},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00054},
	doi = {10.1109/ICDE51399.2021.00054},
	timestamp = {Mon, 09 Sep 2024 19:07:29 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoYT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the popularity and success of graph neural networks (GNN) in various scenarios. To obtain data-specific GNN architectures, researchers turn to neural architecture search (NAS), which have made impressive success in discovering effective architectures in convolutional neural networks. However, it is non-trivial to apply NAS approaches to GNN due to challenges in search space design and expensive searching cost of existing NAS methods. In this work, to obtain the data-specific GNN architectures and address the computational challenges facing by NAS approaches, we propose a framework, which tries to Search to Aggregate NEighborhood (SANE), to automatically design data-specific GNN architectures. By designing a novel and expressive search space, we propose a differentiable search algorithm, which is more efficient than previous reinforcement learning based methods. Experimental results on four tasks and seven real-world datasets demonstrate the superiority of SANE compared to existing GNN models and NAS approaches in terms of effectiveness and efficiency. 1}
}


@inproceedings{DBLP:conf/icde/WangWHSL21,
	author = {Chaokun Wang and
                  Binbin Wang and
                  Bingyang Huang and
                  Shaoxu Song and
                  Zai Li},
	title = {FastSGG: Efficient Social Graph Generation Using a Degree Distribution
                  Generation Model},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {564--575},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00055},
	doi = {10.1109/ICDE51399.2021.00055},
	timestamp = {Mon, 03 Jan 2022 22:33:27 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WangWHSL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularity of social networks, large-scale social graphs are necessary to evaluate the algorithms for various social network analysis tasks, especially in the era of big data. An efficient and configurable social graph generator has become more important than ever before because it is difficult to obtain billion-scale real-world social graphs for various scenarios.In this paper, we present an efficient and widely-applicable social graph generator called FastSGG. FastSGG generates social graphs according to a user-defined configuration depicting the features of the target social graph, which is a flexible way to generate graphs in a variety of applications. The generation method consists of two main steps: the determination of out-degree for a source vertex and the determination of a target vertex to construct an edge. In order to accelerate the graph generation process, a degree distribution generation (D 2 G) model is proposed. The D 2 G model is a universal model for generating graphs following different degree distributions as long as the probability density functions or probability mass functions are given. The extensive experimental results demonstrate that FastSGG can generate high-quality social graphs with small world properties, power-law degree distributions, and community structures. Moreover, FastSGG generates graphs at least four times faster than the state-of-the-art graph generators. In addition, the peak memory usage of FastSGG is less than one seventh of that of the state-of-the-art method.}
}


@inproceedings{DBLP:conf/icde/Yang021,
	author = {Lei Yang and
                  Lei Zou},
	title = {Noah: Neural-optimized A* Search Algorithm for Graph Edit Distance
                  Computation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {576--587},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00056},
	doi = {10.1109/ICDE51399.2021.00056},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Yang021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Edit Distance (GED) is a classical graph similarity metric that can be tailored to a wide range of applications. However, the exact GED computation is NP-complete, which means it is only feasible for small graphs only. And therefore, approximate GED computation methods are used in most real-world applications. However, traditional practices and end-to-end learning-based methods have their shortcomings when applied for approximate GED computation. The former relies on experience and usually performs not well. The latter is only capable of computing similarity scores between graphs without an actual edit path, which is crucial in specific problems (e.g., Graph Alignment, Semantic Role Labeling). This paper proposes a novel approach Noah, which combines A* search algorithm and graph neural networks to compute approximate GED in a more effective and intelligent way. The combination is mainly reflected in two aspects. First, we learn the estimated cost function h(•) by Graph Path Networks. Pre-training GEDs and corresponding edit paths are also incorporated for training the model, therefore helping optimize the search direction of A* search algorithm. Second, we learn an elastic beam size that can help reduce search size and satisfy various user settings. Experimental results demonstrate the practical effectiveness of our approach on several tasks and suggest that our approach significantly outperforms the state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/HaoQCLSTZD21,
	author = {Yuanzhe Hao and
                  Xiongpai Qin and
                  Yueguo Chen and
                  Yaru Li and
                  Xiaoguang Sun and
                  Yu Tao and
                  Xiao Zhang and
                  Xiaoyong Du},
	title = {TS-Benchmark: {A} Benchmark for Time Series Databases},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {588--599},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00057},
	doi = {10.1109/ICDE51399.2021.00057},
	timestamp = {Tue, 30 Nov 2021 17:09:36 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HaoQCLSTZD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series data is widely used in scenarios such as supply chain, stock data analysis, and smart manufacturing. A number of time series database systems have been invented to manage and query large volumes of time series data. We observe that the existing benchmarks of time series databases are focused on workloads of complex analysis such as pattern matching and trend prediction whose performance may be highly affected by the data analysis algorithms, instead of the back-end databases. However, in many real applications of time series databases, people are more interested in the performance metrics such as data injection throughput and query processing time. A benchmark is still required to extensively compare the performance of time series databases in such metrics. We introduce such a benchmark called TS-Benchmark which majorly applies a scenario of device monitoring for wind turbines. A DCGAN-based data generation model is proposed to generate large volumes of time series data from some real time series data. The workloads are categorized into three folds: data loading (in batch), streaming data injection, and historical data access (for typical queries). We implement the benchmark and compare four representative time series databases: InfluxDB, TimescaleDB, Druid and OpenTSDB. The results are reported and analyzed.}
}


@inproceedings{DBLP:conf/icde/PereraORB21,
	author = {R. Malinga Perera and
                  Bastian Oetomo and
                  Benjamin I. P. Rubinstein and
                  Renata Borovica{-}Gajic},
	title = {{DBA} bandits: Self-driving index tuning under ad-hoc, analytical
                  workloads with safety guarantees},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {600--611},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00058},
	doi = {10.1109/ICDE51399.2021.00058},
	timestamp = {Sun, 06 Oct 2024 21:04:58 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PereraORB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automating physical database design has remained a long-term interest in database research due to substantial performance gains afforded by optimised structures. Despite significant progress, a majority of today’s commercial solutions are highly manual, requiring offline invocation by database administrators (DBAs) who are expected to identify and supply representative training workloads. Even the latest advancements like query stores provide only limited support for dynamic environments. This status quo is untenable: identifying representative static workloads is no longer realistic; and physical design tools remain susceptible to the query optimiser’s cost misestimates.We propose a self-driving approach to online index selection that eschews the DBA and query optimiser, and instead learns the benefits of viable structures through strategic exploration and direct performance observation. We view the problem as one of sequential decision making under uncertainty, specifically within the bandit learning setting. Multi-armed bandits balance exploration and exploitation to provably guarantee average performance that converges to policies that are optimal with perfect hindsight. Our simplified bandit framework outperforms deep reinforcement learning (RL) in terms of convergence speed and performance volatility. Comprehensive empirical results demonstrate up to 75% speed-up on shifting and ad-hoc workloads and 28% speed-up on static workloads compared against a state-of-the-art commercial tuning tool and up to 58% speed-up against the deep RL alternatives.}
}


@inproceedings{DBLP:conf/icde/HuangJSS021,
	author = {Kecheng Huang and
                  Zhiping Jia and
                  Zhaoyan Shen and
                  Zili Shao and
                  Feng Chen},
	title = {Less is More: De-amplifying I/Os for Key-value Stores with a Log-assisted
                  LSM-tree},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {612--623},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00059},
	doi = {10.1109/ICDE51399.2021.00059},
	timestamp = {Sun, 06 Oct 2024 21:04:57 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangJSS021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, Log-Structured Merge Tree (LSMtree) based key-value stores, such as LevelDB and RocksDB, have been widely adopted in data center systems. Though optimized for high-speed write processing, the severe I/O amplification remains a critical constraint that hinders them from reaching their maximum performance potential. Unfortunately, this problem is deeply rooted in the fundamental design of the LSMtree structure. A small number of frequently updated key-value items could quickly pollute the entire tree structure, causing repeated changes in the structure and quickly amplifying the amount of disk IOs across the levels in the tree. In this paper, we present a novel scheme, called Log-assisted LSM-tree (L2SM), to fundamentally address the long-existing I/O amplification problem. L2SM adopts a small-size, multi-level log structure to isolate selected key-value items that have a disruptive effect on the tree structure, accumulates and absorbs the repeated updates in a highly efficient manner, and removes obsolete and deleted key-value items at an early stage. We have prototyped the L2SM structure based on LevelDB. Our evaluation with the YCSB benchmark shows promising results by reducing the amount of disk IOs by up to 40.2%, increasing the throughput by up to 67.4%, and decreasing the average latency by up to 40.1%.}
}


@inproceedings{DBLP:conf/icde/NeroneHAM21,
	author = {Matheus Agio Nerone and
                  Pedro Holanda and
                  Eduardo C. de Almeida and
                  Stefan Manegold},
	title = {Multidimensional Adaptive {\&} Progressive Indexes},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {624--635},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00060},
	doi = {10.1109/ICDE51399.2021.00060},
	timestamp = {Mon, 07 Mar 2022 09:31:03 +0100},
	biburl = {https://dblp.org/rec/conf/icde/NeroneHAM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Exploratory data analysis is the primary technique used by data scientists to extract knowledge from new data sets. This type of workload is composed of trial-and-error hypothesis-driven queries with a human in the loop. To keep up with the data scientist’s productivity, the system must be capable of answering queries in interactive times. Given that these queries are highly selective multidimensional queries, multidimensional indexes are necessary to ensure low latency. However, creating the appropriate indexes is not a given due to the highly exploratory and interactive nature of such human-in-the-loop scenarios.In this paper, we identify four main objectives that are desirable for exploratory data analysis workloads: (1) low overhead over the initial queries, (2) low query variance (i.e., high robustness), (3) predictable index convergence, and (4) low total workload time. Given that not all of them can be achieved at the same time, we present three novel incremental multidimensional indexing techniques that represent three sample points on a Pareto front for this multi-objective optimization problem. (a) The Adaptive KD-Tree is designed to achieve the lowest total workload time at the expense of a higher indexing penalty for the initial queries, lack of robustness, and unpredictable convergence. (b) The Progressive KD-Tree has predictable convergence and a user-defined indexing cost for the initial queries. However, total workload time can be higher than with Adaptive KD-Trees, and per-query time still varies. (c) The Greedy Progressive KD-Tree aims at full robustness at the expense of only improving the per-query cost after full index convergence.Our extensive experimental evaluation using both synthetic and real-life data sets and workloads shows that (a) the Adaptive KD-Tree reduces total workload time by up to a factor 2 compared to the state-of-the-art, (b) the Progressive KD-Tree achieves predictable convergence with up to one order of magnitude lower initial query cost, and (c) the Greedy Progressive KDTree exhibits the lowest query variance up to three orders of magnitude lower than the state-of-the-art.}
}


@inproceedings{DBLP:conf/icde/XieLMGHDC21,
	author = {Rongbiao Xie and
                  Meng Li and
                  Zheyu Miao and
                  Rong Gu and
                  He Huang and
                  Haipeng Dai and
                  Guihai Chen},
	title = {Hash Adaptive Bloom Filter},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {636--647},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00061},
	doi = {10.1109/ICDE51399.2021.00061},
	timestamp = {Sat, 09 Apr 2022 12:45:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XieLMGHDC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bloom filter is a compact memory-efficient probabilistic data structure supporting membership testing, i.e., to check whether an element is in a given set. However, as Bloom filter maps each element with uniformly random hash functions, few flexibilities are provided even if the information of negative keys (elements are not in the set) are available. The problem gets worse when the misidentification of negative keys brings different costs. To address the above problems, we propose a new Hash Adaptive Bloom Filter (HABF) that supports the customization of hash functions for keys. The key idea of HABF is to customize the hash functions for positive keys (elements are in the set) to avoid negative keys with high cost, and pack customized hash functions into a lightweight data structure named HashExpressor. Then, given an element at query time, HABF follows a two-round pattern to check whether the element is in the set. Further, we theoretically analyze the performance of HABF and bound the expected false positive rate. We conduct extensive experiments on representative datasets, and the results show that HABF outperforms the standard Bloom filter and its cutting-edge variants on the whole in terms of accuracy, construction time, query time, and memory space consumption (Note that source codes are available in [1]).}
}


@inproceedings{DBLP:conf/icde/ZengTC21,
	author = {Yuxiang Zeng and
                  Yongxin Tong and
                  Lei Chen},
	title = {{HST+:} An Efficient Index for Embedding Arbitrary Metric Spaces},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {648--659},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00062},
	doi = {10.1109/ICDE51399.2021.00062},
	timestamp = {Tue, 07 May 2024 20:05:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZengTC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Metric embeddings have been widely used in approximate algorithms to guarantee the effectiveness of geometric problems. Among the metric embedding techniques, Hierarchically Separated Tree (HST) is one of the most prevalent data structures, which maps the points of the original metric space into a tree-based metric space. A few selected applications of the HST include clustering, task assignment, trip planning, privacy preservation, information routing in wireless sensor networks, etc. Despite the popularity in ensuring the effectiveness, the HST-based solutions can be inefficient in large-scale datasets, since the state-of-the-art construction method has high time and space complexity (O(n 3 ) and O(n 2 ) in the worst-case). Moreover, existing studies overlook the insertion of new points in real applications (deletions can be trivially supported), which can cause the reconstruction of the whole HST. To address these limitations, we focus on designing an efficient index for embedding arbitrary metric spaces by tree metric spaces. Specifically, for construction, we design a dynamic programming-based method, which significantly reduces the time and space complexity to O(n 2 ) and O(n) respectively. For insertion of new points, we propose a new data structure, called Hierarchically Separated Forest (HSF), i.e., a collection of HSTs. An HSF can efficiently support insertion of new points with a tight theoretical guarantee (O(log n)). Finally, extensive experiments demonstrate the superior performance of our proposed algorithms with respect to the effectiveness and the running time. For instance, compared with the state-of-the-art algorithms, our construction method is up to 29.8× faster and our insertion method is up to 491× faster.}
}


@inproceedings{DBLP:conf/icde/KosyfakiMPT21,
	author = {Chrysanthi Kosyfaki and
                  Nikos Mamoulis and
                  Evaggelia Pitoura and
                  Panayiotis Tsaparas},
	title = {Flow Computation in Temporal Interaction Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {660--671},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00063},
	doi = {10.1109/ICDE51399.2021.00063},
	timestamp = {Tue, 21 Mar 2023 20:50:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/KosyfakiMPT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal interaction networks capture the history of activities between entities along a timeline. At each interaction, some quantity of data (money, information, traffic) flows from one vertex of the network to another. Flow-based analysis can reveal important information, such as unusually large money transfers in a part of a financial transaction network. In this paper, we introduce the flow computation problem between two vertrices in an interaction network. We propose and study two models of flow computation, one based on a greedy flow transfer assumption and one that finds the maximum possible flow. We show that the greedy flow computation problem can be easily solved by a single scan of the interactions in time order. For the harder maximum flow problem, we propose precomputation and simplification approaches that can greatly reduce its complexity in practice. We also approach the problem of flow pattern enumeration in interaction networks and propose an effective path indexing technique. We evaluate our algorithms using real datasets. The results demonstrate the efficiency and scalability of our algorithms.}
}


@inproceedings{DBLP:conf/icde/ZhuFY21,
	author = {Kaijie Zhu and
                  George Fletcher and
                  Nikolay Yakovets},
	title = {Leveraging Temporal and Topological Selectivities in Temporal-clique
                  Subgraph Query Processing},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {672--683},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00064},
	doi = {10.1109/ICDE51399.2021.00064},
	timestamp = {Tue, 07 May 2024 20:05:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhuFY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of temporal-clique subgraph pattern matching. In such patterns, edges are required to jointly overlap in time within a given temporal window in addition to forming a topological sub-structure. This problem arises in many application domains, e.g., in social networks, life sciences, smart cities, telecommunications, and others. State-of-the-art subgraph matching techniques, however, are shown to be limited and inefficient in processing queries with both temporal and topological constraints. We propose an approach that takes full advantage of both topological and temporal selectivities during the processing of temporal-clique subgraph queries. Additionally, we investigate a number of optimizations that can be introduced into our approach to improve its efficiency. Our experimental results demonstrate that our approach outperforms the existing methods by a wide margin at a small additional storage cost.}
}


@inproceedings{DBLP:conf/icde/WangLC21,
	author = {Zheng Wang and
                  Cheng Long and
                  Gao Cong},
	title = {Trajectory Simplification with Reinforcement Learning},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {684--695},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00065},
	doi = {10.1109/ICDE51399.2021.00065},
	timestamp = {Wed, 23 Oct 2024 08:55:28 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory data is used in various applications including traffic analysis, logistics, and mobility services. It is usually collected continuously by sensors and accumulated at a server resulting in big volume. A common practice is to conduct trajectory simplification which is to drop some points of a trajectory when they are being collected (online mode) and/or after they are accumulated (batch mode). Existing algorithms usually involve some decision making tasks (e.g., deciding which point to drop), for which, some human-crafted rules are used. In this paper, we propose to learn a policy for the decision making tasks via reinforcement learning (RL) and develop trajectory simplification methods based on the learned policy. Compared with existing algorithms, our RL-based methods are data-driven and can adapt to different dynamics underlying the problem. We conduct extensive experiments to verify that our RL-based methods compute simplified trajectories with smaller errors while running comparably fast (and faster in the batch mode) compared with existing methods.}
}


@inproceedings{DBLP:conf/icde/FangDCHGC21,
	author = {Ziquan Fang and
                  Yuntao Du and
                  Lu Chen and
                  Yujia Hu and
                  Yunjun Gao and
                  Gang Chen},
	title = {E\({}^{\mbox{2}}\)DTC: An End to End Deep Trajectory Clustering Framework
                  via Self-Training},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {696--707},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00066},
	doi = {10.1109/ICDE51399.2021.00066},
	timestamp = {Sun, 06 Aug 2023 20:52:07 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FangDCHGC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory clustering has played an essential role in trajectory mining tasks. It serves in a wide range of real-life applications, including transportation, location-based services, behavioral study, and so on. To support trajectory clustering analytics, a plethora of trajectory clustering methods have been proposed, which mainly extend traditional clustering algorithms by using spatio-temporal characteristics of trajectories. However, existing traditional trajectory clustering approaches based on raw trajectory representation highly rely on hand-craft similarity metrics, and can not capture hidden spatial dependencies in trajectory data, which is inefficient and inflexible for clustering analysis. To this end, we propose an end-to-end deep trajectory clustering framework via self-training, termed as E 2 DTC, inspired by the data-driven capabilities of deep neural networks. E 2 DTC does not require any additional manual feature extraction operations, and can be easily adapted for trajectory clustering analytics on any trajectory dataset. Extensive experimental evaluations on three real-life datasets show that our framework E2DTC achieves superior accuracy and efficiency, compared with classical clustering methods (i.e., K-Medoids) and state-of-the-art neural-network based approaches (i.e., t2vec).}
}


@inproceedings{DBLP:conf/icde/ZhengWZZ0J21,
	author = {Bolong Zheng and
                  Lianggui Weng and
                  Xi Zhao and
                  Kai Zeng and
                  Xiaofang Zhou and
                  Christian S. Jensen},
	title = {{REPOSE:} Distributed Top-k Trajectory Similarity Search with Local
                  Reference Point Tries},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {708--719},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00067},
	doi = {10.1109/ICDE51399.2021.00067},
	timestamp = {Fri, 01 Sep 2023 11:19:57 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhengWZZ0J21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory similarity computation is a fundamental component in a variety of real-world applications, such as ridesharing, road planning, and transportation optimization. Recent advances in mobile devices have enabled an unprecedented increase in the amount of available trajectory data such that efficient query processing can no longer be supported by a single machine. As a result, means of performing distributed in-memory trajectory similarity search are called for. However, existing distributed proposals either suffer from computing resource waste or are unable to support the range of similarity measures that are being used. We propose a distributed in-memory management framework called REPOSE for processing top-k trajectory similarity queries on Spark. We develop a reference point trie (RP-Trie) index to organize trajectory data for local search. In addition, we design a novel heterogeneous global partitioning strategy to eliminate load imbalance in distributed settings. We report on extensive experiments with real-world data that offer insight into the performance of the solution, and show that the solution is capable of outperforming the state-of-the-art proposals.}
}


@inproceedings{DBLP:conf/icde/GaoSAY21,
	author = {Junyang Gao and
                  Stavros Sintos and
                  Pankaj K. Agarwal and
                  Jun Yang},
	title = {Durable Top-K Instant-Stamped Temporal Records with User-Specified
                  Scoring Functions},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {720--731},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00068},
	doi = {10.1109/ICDE51399.2021.00068},
	timestamp = {Mon, 05 Jul 2021 13:48:35 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GaoSAY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A way of finding interesting or exceptional records from instant-stamped temporal data is to consider their "durability, " or, intuitively speaking, how well they compare with other records that arrived earlier or later, and how long they retain their supremacy. For example, people are naturally fascinated by claims with long durability, such as: "On January 22, 2006, Kobe Bryant dropped 81 points against Toronto Raptors. Since then, this scoring record has yet to be broken." In general, given a sequence of instant-stamped records, suppose that we can rank them by a user-specified scoring function f, which may consider multiple attributes of a record to compute a single score for ranking. This paper studies durable top-k queries, which find records whose scores were within top-k among those records within a "durability window" of given length, e.g., a 10-year window starting/ending at the timestamp of the record. The parameter k, the length of the durability window, and parameters of the scoring function (which capture user preference) can all be given at the query time. We illustrate why this problem formulation yields more meaningful answers in some practical situations than other similar types of queries considered previously. We propose new algorithms for solving this problem, and provide a comprehensive theoretical analysis on the complexities of the problem itself and of our algorithms. Our algorithms vastly outperform various baselines (by up to two orders of magnitude on real and synthetic datasets).}
}


@inproceedings{DBLP:conf/icde/CrottyGLC21,
	author = {Andrew Crotty and
                  Alex Galakatos and
                  Connor Luckett and
                  Ugur {\c{C}}etintemel},
	title = {The Case for In-Memory {OLAP} on "Wimpy" Nodes},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {732--743},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00069},
	doi = {10.1109/ICDE51399.2021.00069},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CrottyGLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Research projects will often use the latest hardware to achieve orders-of-magnitude performance improvements while ignoring the (usually hefty) associated price tag. Real-world deployments typically follow suit, requiring expensive computing infrastructures that cost even more to power and cool.In this paper, we challenge the conventional wisdom that high-end hardware is absolutely necessary for state-of-the-art performance and instead advocate for a radically different approach based on cheap single-board computers (SBCs). While others have previously explored similar ideas for computationally simple and easily partitionable use cases (e.g., key-value stores), so-called "wimpy" nodes have traditionally been rejected as unsuitable for more complex workloads. We believe, however, that recent hardware advancements driven by the mobile computing market call this orthodoxy into question. For example, our microbenchmarks show that one popular SBC, the Raspberry Pi 3B+, offers single-core compute performance that is surprisingly competitive with many server-grade Intel Xeon and ARM-based CPUs at a fraction of the cost and energy consumption.To make our case, we conducted an extensive experimental study, beginning with a series of microbenchmarks to identify the strengths and weaknesses of SBCs relative to server-grade CPUs. Then, to evaluate the ability of SBCs to handle more complex use cases, we analyzed the performance of an in-memory OLAP workload in both single-node and distributed settings. Overall, our results demonstrate up to several orders of magnitude in cost reductions coupled with substantial energy savings when compared to traditional on-premises and cloud deployments, all without a significant increase in absolute runtimes.}
}


@inproceedings{DBLP:conf/icde/LiZLHS21,
	author = {Yuchen Li and
                  Qiwei Zhu and
                  Zheng Lyu and
                  Zhongdong Huang and
                  Jianling Sun},
	title = {DyCuckoo: Dynamic Hash Tables on GPUs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {744--755},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00070},
	doi = {10.1109/ICDE51399.2021.00070},
	timestamp = {Wed, 05 Jan 2022 16:54:21 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiZLHS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The hash table is a fundamental structure that has been implemented on graphics processing units (GPUs) to accelerate a wide range of analytics workloads. Most existing works have focused on static scenarios and occupy large GPU memory to maximize the insertion efficiency. In many cases, data stored in hash tables get updated dynamically, and existing approaches use unnecessarily large memory resources. One naïve solution is to rebuild a hash table (known as rehashing) whenever it is either filled or mostly empty. However, this approach renders significant overheads for rehashing. In this paper, we propose a novel dynamic cuckoo hash table technique on GPUs, known as DyCuckoo. We devise a resizing strategy for dynamic scenarios without rehashing the entire table that ensures a guaranteed filled factor. The strategy trades search performance with resizing efficiency, and this tradeoff can be configured by users. To further improve efficiency, we propose a 2-in-d cuckoo hashing scheme that ensures a maximum of two lookups for find and delete operations, while retaining similar performance for insertions as a general cuckoo hash. Extensive experiments have validated the proposed design’s effectiveness over several state-of-the-art hash table implementations on GPUs. DyCuckoo achieves superior efficiency while enables fine-grained memory control, which is not available in existing GPU hash table approaches.}
}


@inproceedings{DBLP:conf/icde/DoLL21,
	author = {Jaeyoung Do and
                  Chen Luo and
                  David B. Lomet},
	title = {Programming an {SSD} Controller to Support Batched Writes for Variable-Size
                  Pages},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {756--767},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00071},
	doi = {10.1109/ICDE51399.2021.00071},
	timestamp = {Mon, 19 Dec 2022 20:39:08 +0100},
	biburl = {https://dblp.org/rec/conf/icde/DoLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Exploiting a storage hierarchy is critical to cost-effective data management. However, most systems are challenged when data is not in cache because of the additional I/O to move data between SSD and main memory. To improve both cost and performance, some systems use a log structured store to write a batch of pages instead of a "block-at-a-time". However, host-based log structuring incurs the additional cost and complexity of garbage collection and recovery, duplicating similar SSD FTL functionality. In prior work, we presented a customized SSD controller implementation for an Open-Channel SSD to enable host computers to write batches of fixed size pages. This current work is a major redesign to support a batched write interface with variable size pages. Variable size pages can enable easy support of data compression and encryption, as well as reducing internal page storage fragmentation, e.g, within a B-tree. Thus it further improves I/O performance while making it easier and more efficient to support these capabilities.}
}


@inproceedings{DBLP:conf/icde/KargarLN21,
	author = {Saeed Kargar and
                  Heiner Litz and
                  Faisal Nawab},
	title = {Predict and Write: Using K-Means Clustering to Extend the Lifetime
                  of {NVM} Storage},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {768--779},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00072},
	doi = {10.1109/ICDE51399.2021.00072},
	timestamp = {Mon, 26 Jun 2023 20:41:56 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KargarLN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-volatile memory (NVM) technologies suffer from limited write endurance. To address this challenge, we propose Predict and Write (PNW), a K/V-store that uses a clustering-based machine learning approach to extend the lifetime of NVMs. PNW decreases the number of bit flips for PUT/UPDATE operations by determining the best memory location an updated value should be written to. PNW leverages the indirection level of K/V-stores to freely choose the target memory location for any given write based on its value. PNW organizes NVM addresses in a dynamic address pool clustered by the similarity of the data values they refer to. We show that, by choosing the right target memory location for a given PUT/UPDATE operation, the number of total bit flips and cache lines can be reduced by up to 85% and 56% over the state of the art.}
}


@inproceedings{DBLP:conf/icde/WangCQZ21,
	author = {Donghui Wang and
                  Peng Cai and
                  Weining Qian and
                  Aoying Zhou},
	title = {Discriminative Admission Control for Shared-everything Database under
                  Mixed {OLTP} Workloads},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {780--791},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00073},
	doi = {10.1109/ICDE51399.2021.00073},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangCQZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the variability of IT applications, the back-end databases usually run the mixed OLTP workload, which comprises a variety of transactions. Some of these transactions are high-conflict and others are low-conflict. Furthermore, high-conflict transactions may contend on different groups of data stored in the database. Without precise admission control, too many transactions with conflict on the same group of records are simultaneously executed by the OLTP engine, and this will lead to the well-known problem of data-contention thrashing. Under mixed OLTP workloads, conflicting transactions would be blocked for a long time or rolled back finally, and other transactions have not enough opportunity to be processed.To achieve the optimal performance for each kind of transaction, we design a discriminative admission control mechanism for shared-everything database, referred to as DAC. DAC can quickly identify and classify high-conflict transactions according to the set of records they try to access, which is defined as a conflict zone. DAC makes admission control over OLTP transactions with the conflict zone as the granularity. By adaptively adjusting the transaction concurrency level for each zone, transaction blocking and waiting among the same kind of high-conflict transactions can be alleviated. Furthermore, thread resources are released to make the execution of low-conflict transactions less affected. We evaluate DAC using a main-memory database prototype and a classical disk-based database system. Experimental results demonstrate that DAC can help OLTP engine significantly improve the performance under mixed OLTP workloads.}
}


@inproceedings{DBLP:conf/icde/LometL21,
	author = {David B. Lomet and
                  Chen Luo},
	title = {Efficiently Reclaiming Space in a Log Structured Store},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {792--803},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00074},
	doi = {10.1109/ICDE51399.2021.00074},
	timestamp = {Mon, 19 Dec 2022 20:39:08 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LometL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern storage devices do not support update-in-place. Rather, flash and shingled disks, are forms of log structured stores. Such a store writes a number of diverse and non-contiguous logical pages into a unit of contiguous storage we call a segment instead of using a write I/O to update each page in place. The result is that pages need to be relocated and remapped on every write. Log structuring was invented for and used initially to improve performance in file systems. Segments need to be garbage collected, but can be only when they no longer house any current pages. A process of "cleaning" produces an empty segment by, when necessary, moving (re-writing) still current pages of the segment to another location. Cleaning effectiveness has a major impact on the performance of modern storage devices, and for flash, impacts the rate of wear and hence the lifetime of the device. We analyze cleaning performance and introduce a cleaning strategy that uses a new way to prioritize the order in which segments are cleaned. Our cleaning strategy approximates an "optimal cleaning strategy". Simulation studies confirm the results of the analysis. This strategy is a significant improvement over previous cleaning strategies.}
}


@inproceedings{DBLP:conf/icde/0004WZYTG21,
	author = {Peng Jia and
                  Pinghui Wang and
                  Junzhou Zhao and
                  Ye Yuan and
                  Jing Tao and
                  Xiaohong Guan},
	title = {LogLog Filter: Filtering Cold Items within a Large Range over High
                  Speed Data Streams},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {804--815},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00075},
	doi = {10.1109/ICDE51399.2021.00075},
	timestamp = {Sun, 30 Apr 2023 12:18:05 +0200},
	biburl = {https://dblp.org/rec/conf/icde/0004WZYTG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many real-world datasets are given in the format of data streams, and processing these data streams is fundamental for many applications such as anomaly detection. In this paper, we study the problem of computing item frequencies, finding top-k hot items, and detecting heavy changes. However, the widely-used sketches cost large memory usage and their performance is easily affected by the unbalanced distribution of data streams. To solve this issue, a novel method Cold Filter (CF) is proposed to split cold items and hot items, and use a separate structure to record the frequencies of hot items. Typically, CF has a small filter range and is only effective for filtering cold items with small frequencies. For some real-world applications, however, the cold items’ frequencies may also be greater than hundreds or even tens of thousands. To solve the above challenges, we exploit the "LogLog" structure and develop a memory-efficient method LogLog Filter (LLF) to accurately estimate the above three metrics. LLF builds a register array where each register approximately counts the sum of item frequencies hashed into it. Our method remarkably enlarges the filter range of CF with fewer bits and only requires 4 bits to filter cold items with frequencies up to\n2\n2\n4\n. We conduct extensive experiments on real-world and synthetic datasets, and the experimental results demonstrate the efficiency and effectiveness of our method.}
}


@inproceedings{DBLP:conf/icde/Kwon0LS21,
	author = {Taehyung Kwon and
                  Inkyu Park and
                  Dongjin Lee and
                  Kijung Shin},
	title = {SliceNStitch: Continuous {CP} Decomposition of Sparse Tensor Streams},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {816--827},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00076},
	doi = {10.1109/ICDE51399.2021.00076},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Kwon0LS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consider traffic data (i.e., triplets in the form of source-destination-timestamp) that grow over time. Tensors (i.e., multi-dimensional arrays) with a time mode are widely used for modeling and analyzing such multi-aspect data streams. In such tensors, however, new entries are added only once per period, which is often an hour, a day, or even a year. This discreteness of tensors has limited their usage for real-time applications, where new data should be analyzed instantly as it arrives.How can we analyze time-evolving multi-aspect sparse data ‘continuously’ using tensors where time is ‘discrete’? We propose SLICENSTITCH for continuous CANDECOMP/PARAFAC (CP) decomposition, which has numerous time-critical applications, including anomaly detection, recommender systems, and stock market prediction. SLICENSTITCH changes the starting point of each period adaptively, based on the current time, and updates factor matrices (i.e., outputs of CP decomposition) instantly as new data arrives. We show, theoretically and experimentally, that SLICENSTITCH is (1) ‘Any time’: updating factor matrices immediately without having to wait until the current time period ends, (2) Fast: with constant-time updates up to 464× faster than online methods, and (3) Accurate: with fitness comparable (specifically, 72 − 100%) to offline methods.}
}


@inproceedings{DBLP:conf/icde/KimKKM21,
	author = {Bogyeong Kim and
                  Kyoseung Koo and
                  Juhun Kim and
                  Bongki Moon},
	title = {{DISC:} Density-Based Incremental Clustering by Striding over Streaming
                  Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {828--839},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00077},
	doi = {10.1109/ICDE51399.2021.00077},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KimKKM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given the prevalence of mobile and IoT devices, continuous clustering against streaming data has become an essential tool of increasing importance for data analytics. Among many clustering approaches, the density-based clustering has garnered much attention due to its unique advantages. The main drawback is, however, the limited scalability attributed to its relatively high computational cost, which is further aggravated when it has to update clusters continuously along with evolving data. In this paper, we present a new incremental density-based clustering algorithm called DISC optimized for the sliding window model. DISC is capable of producing exactly the same clustering results as existing methods such as Incremental DBSCAN for streaming data much more quickly and efficiently.}
}


@inproceedings{DBLP:conf/icde/LeeS21,
	author = {Dongjin Lee and
                  Kijung Shin},
	title = {Robust Factorization of Real-world Tensor Streams with Patterns, Missing
                  Values, and Outliers},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {840--851},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00078},
	doi = {10.1109/ICDE51399.2021.00078},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LeeS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consider multiple seasonal time series being collected in real-time, in the form of a tensor stream. Real-world tensor streams often include missing entries (e.g., due to network disconnection) and at the same time unexpected outliers (e.g., due to system errors). Given such a real-world tensor stream, how can we estimate missing entries and predict future evolution accurately in real-time?In this work, we answer this question by introducing SOFIA, a robust factorization method for real-world tensor streams. In a nutshell, SOFIA smoothly and tightly integrates tensor factorization, outlier removal, and temporal-pattern detection, which naturally reinforce each other. Moreover, SOFIA integrates them in linear time, in an online manner, despite the presence of missing entries. We experimentally show that SOFIA is (a) robust and accurate: yielding up to 76% lower imputation error and 71% lower forecasting error; (b) fast: up to 935× faster than the second-most accurate competitor; and (c) scalable: scaling linearly with the number of new entries per time step.}
}


@inproceedings{DBLP:conf/icde/SaadBBD21,
	author = {Muhammad Saad and
                  Abraham Bernstein and
                  Michael H. B{\"{o}}hlen and
                  Daniele Dell'Aglio},
	title = {Single Point Incremental Fourier Transform on 2D Data Streams},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {852--863},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00079},
	doi = {10.1109/ICDE51399.2021.00079},
	timestamp = {Mon, 26 Jun 2023 20:41:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SaadBBD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In radio astronomy, antennas monitor portions of the sky to collect radio signals. The antennas produce data streams that are of high volume and velocity (~2.5 GB/s) and the inverse Fourier transform is used to convert the collected signals into sky images that astrophysicists use to conduct their research. Applying the inverse Fourier transform in a streaming setting, however, is not ideal since its computational complexity is quadratic in the size of the image.In this article, we propose the Single Point Incremental Fourier Transform (SPIFT), a novel incremental algorithm to produce sequences of sky images. SPIFT computes the Fourier transform for a new signal in a linear number of complex multiplications by exploiting twiddle factors, multiplicative constant coefficients. We prove that twiddle factors are periodic and show how circular shifts can be exploited to reuse multiplication results. The cost of the additive operations can be curbed by exploiting the embarrassingly parallel nature of the additions, which modern big data streaming frameworks can leverage to compute slices of the image in parallel. Our experiments suggest that SPIFT can efficiently generate sequences of sky images: it computes the complex multiplications 4 to 12x faster than the Discrete Fourier Transform, and its parallelisation of the additive operations shows linear speedup.}
}


@inproceedings{DBLP:conf/icde/BasatEMV21,
	author = {Ran Ben Basat and
                  Gil Einziger and
                  Michael Mitzenmacher and
                  Shay Vargaftik},
	title = {{SALSA:} Self-Adjusting Lean Streaming Analytics},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {864--875},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00080},
	doi = {10.1109/ICDE51399.2021.00080},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BasatEMV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Counters are the fundamental building block of many data sketching schemes, which hash items to a small number of counters and account for collisions to provide good approximations for frequencies and other measures. Most existing methods rely on fixed-size counters, which may be wasteful in terms of space, as counters must be large enough to eliminate any risk of overflow. Instead, some solutions use small, fixed-size counters that may overflow into secondary structures.This paper takes a different approach. We propose a simple and general method called SALSA for dynamic re-sizing of counters, and show its effectiveness. SALSA starts with small counters, and overflowing counters simply merge with their neighbors. SALSA can thereby allow more counters for a given space, expanding them as necessary to represent large numbers. Our evaluation demonstrates that, at the cost of a small overhead for its merging logic, SALSA significantly improves the accuracy of popular schemes (such as Count-Min Sketch and Count Sketch) over a variety of tasks. Our code is released as open source [1].}
}


@inproceedings{DBLP:conf/icde/YangLT21,
	author = {Yueji Yang and
                  Yuchen Li and
                  Anthony K. H. Tung},
	title = {NewsLink: Empowering Intuitive News Search with Knowledge Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {876--887},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00081},
	doi = {10.1109/ICDE51399.2021.00081},
	timestamp = {Wed, 05 Jan 2022 16:54:21 +0100},
	biburl = {https://dblp.org/rec/conf/icde/YangLT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {News search tools help end users to identify relevant news stories. However, existing search approaches often carry out in a "black-box" process. There is little intuition that helps users understand how the results are related to the query. In this paper, we propose a novel news search framework, called NEWSLINK, to empower intuitive news search by using relationship paths discovered from open Knowledge Graphs (KGs). Specifically, NEWSLINK embeds both a query and news documents to subgraphs, called subgraph embeddings, in the KG. Their embeddings’ overlap induces relationship paths between the involving entities. Two major advantages are obtained by incorporating subgraph embeddings into search. First, they enrich the search context, leading to robust results. Second, the relationship paths linking entities inter and intra news documents can help users better understand and digest the results for the given query. Through both human and automatic evaluations, we verify that NEWSLINK can help users understand the result-to-query relatedness, while its search quality is robust and outperforms many established search approaches, including Apache Lucene and a KG-powered query expansion approach, as well as popular deep learning models, Sentence-BERT (SBERT) and DOC2VEC.}
}


@inproceedings{DBLP:conf/icde/LiZZ0C0Z21,
	author = {Na Li and
                  Renyu Zhu and
                  Xiaoxu Zhou and
                  Xiangnan He and
                  Wenyuan Cai and
                  Ming Gao and
                  Aoying Zhou},
	title = {On Disambiguating Authors: Collaboration Network Reconstruction in
                  a Bottom-up Manner},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {888--899},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00082},
	doi = {10.1109/ICDE51399.2021.00082},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiZZ0C0Z21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Author disambiguation arises when different authors share the same name, which is a critical task in digital libraries, such as DBLP, CiteULike, CiteSeerX, etc. While the state-of-the-art methods have developed various paper embedding-based methods performing in a top-down manner, they primarily focus on the ego-network of a target name and overlook the low-quality collaborative relations existed in the ego-network. Thus, these methods can be suboptimal for disambiguating authors.In this paper, we model the author disambiguation as a collaboration network reconstruction problem, and propose an incremental and unsupervised author disambiguation method, namely IUAD, which performs in a bottom-up manner. Initially, we build a stable collaboration network based on stable collaborative relations. To further improve the recall, we build a probabilistic generative model to reconstruct the complete collaboration network. In addition, for newly published papers, we can incrementally judge who publish them via only computing the posterior probabilities. We have conducted extensive experiments on a large-scale DBLP dataset to evaluate IUAD. The experimental results demonstrate that IUAD not only achieves the promising performance, but also outperforms comparable baselines significantly. Codes are available at https://github.com/papergitgit/IUAD.}
}


@inproceedings{DBLP:conf/icde/Yi0LL21,
	author = {Pei Yi and
                  Hong Xie and
                  Yongkun Li and
                  John C. S. Lui},
	title = {A Bootstrapping Approach to Optimize Random Walk Based Statistical
                  Estimation over Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {900--911},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00083},
	doi = {10.1109/ICDE51399.2021.00083},
	timestamp = {Mon, 16 Jan 2023 09:00:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/Yi0LL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs are commonly used in various applications such as online social networks (OSNs), E-commerce systems and social recommender systems. Random walk sampling is often used to conduct statistical estimation over such graphs. This paper develops an algorithmic framework to reduce the mean square error of such statistical estimation. Our algorithmic framework is inspired by that the mean square error can be decomposed into a sum of the bias and variance of the estimator. More specifically, we apply the bootstrapping technique to design a bias reduction algorithm. A new feature of this bias reduction algorithm is that it allows the variance to increase whenever the bias can be further reduced. The increased variance may lead to a large mean square error of the estimator. We use multiple parallel random walks to reduce this variance such that it can be reduced to arbitrarily small by deploying a sufficient number of random walks. Our algorithmic framework enables one to attain different trade-offs between the sample complexity (i.e., number of parallel random walks) and the mean square error of the statistical estimation. Also, the proposed bias reduction algorithm is generic and can be applied to optimize a large class of random walk sampling algorithms. To demonstrate the versatility of the framework, we apply it to optimize the Metropolis random walk and simple random walk sampling. Extensive experiments confirm the effectiveness and efficiency of our proposed algorithmic framework.}
}


@inproceedings{DBLP:conf/icde/LiDKSM21,
	author = {Xiang Li and
                  Danhao Ding and
                  Ben Kao and
                  Yizhou Sun and
                  Nikos Mamoulis},
	title = {Leveraging Meta-path Contexts for Classification in Heterogeneous
                  Information Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {912--923},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00084},
	doi = {10.1109/ICDE51399.2021.00084},
	timestamp = {Mon, 16 Aug 2021 08:40:49 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiDKSM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A heterogeneous information network (HIN) has as vertices objects of different types and as edges the relations between objects, which are also of various types. We study the problem of classifying objects in HINs. Most existing methods perform poorly when given scarce labeled objects as training sets, and methods that improve classification accuracy under such scenarios are often computationally expensive. To address these problems, we propose ConCH, a graph neural network model. ConCH formulates the classification problem as a multitask learning problem that combines semi-supervised learning with self-supervised learning to learn from both labeled and unlabeled data. ConCH employs meta-paths, which are sequences of object types that capture semantic relationships between objects. ConCH co-derives object embeddings and context embeddings via graph convolution. It also uses the attention mechanism to fuse such embeddings. We conduct extensive experiments to evaluate the performance of ConCH against other 15 classification methods. Our results show that ConCH is an effective and efficient method for HIN classification.}
}


@inproceedings{DBLP:conf/icde/AlotaibiLQEO21,
	author = {Rana Alotaibi and
                  Chuan Lei and
                  Abdul Quamar and
                  Vasilis Efthymiou and
                  Fatma {\"{O}}zcan},
	title = {Property Graph Schema Optimization for Domain-Specific Knowledge Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {924--935},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00085},
	doi = {10.1109/ICDE51399.2021.00085},
	timestamp = {Sun, 02 Oct 2022 16:04:34 +0200},
	biburl = {https://dblp.org/rec/conf/icde/AlotaibiLQEO21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enterprises are creating domain-specific knowledge graphs by curating and integrating their business data from multiple sources. Ontologies provide a semantic abstraction for such knowledge graphs to describe their data in terms of the entities involved and their relationships. There has been a lot of effort to build systems that enable efficient querying over knowledge graphs, represented as property graphs. However the problem of schema optimization in the property graph setting has been largely ignored. In this work, we show that graph schema design has significant impact on query performance, and propose two algorithms to generate an optimized property graph schema from the domain ontology. To the best of our knowledge, we are the first to present an ontology-driven approach for property graph schema optimization. The rich semantic relationships in an ontology contain a variety of opportunities to reduce edge traversals and consequently improve the graph query performance. Our experimental study with two real-world knowledge graphs shows that our algorithms produce high-quality schemas, achieving up to 2 orders of magnitude speed-up compared to alternative schema designs.}
}


@inproceedings{DBLP:conf/icde/ZengU0HT21,
	author = {Jian Zeng and
                  Leong Hou U and
                  Xiao Yan and
                  Mingji Han and
                  Bo Tang},
	title = {Fast Core-based Top-k Frequent Pattern Discovery in Knowledge Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {936--947},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00086},
	doi = {10.1109/ICDE51399.2021.00086},
	timestamp = {Sat, 30 Sep 2023 09:44:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZengU0HT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graph is a way of structuring information in graph form, by representing entities as nodes and relationships between entities as edges. A knowledge graph often consists of large amount of facts in real-world which can be used in supporting many analytical tasks, e.g., exceptional facts discovery and fact check of claims. In this work, we study a core-based top-k frequent pattern discovery problem which is frequently used as a subroutine in analyzing knowledge graphs. The main challenge of the problem is search space of the candidate patterns is exponential to the combinations of the nodes and edges in the knowledge graph.To reduce the search space, we devise a novel computation framework FastPat with a suite of optimizations. First, we devise a meta-index, which can be used to avoid generating invalid candidate patterns. Second, we propose an upper bound of the frequency score (i.e., MNI) of the candidate pattern that prunes unqualified candidates earlier and prioritize the enumeration order of the patterns. Lastly, we design a join-based approach to compute the MNI of candidate pattern efficiently. We conduct extensive experimental studies in real-world datasets to verify the superiority of our proposed method over the baselines. We also demonstrate the utility of the discovered frequent patterns by a case study in COVID-19 knowledge graph.}
}


@inproceedings{DBLP:conf/icde/ZhangC0R21,
	author = {Fan Zhang and
                  Hanhua Chen and
                  Hai Jin and
                  Pedro Reviriego},
	title = {The Logarithmic Dynamic Cuckoo Filter},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {948--959},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00087},
	doi = {10.1109/ICDE51399.2021.00087},
	timestamp = {Mon, 28 Jun 2021 09:22:10 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangC0R21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of big data applications makes efficient representation for large-scale dynamic data sets a challenge. The state-of-the-art design, i.e., the dynamic cuckoo filter (DCF), provides extensible approximate set representation by employing a novel chain based data structure which allows appending new building cuckoo filter blocks. However, such a design needs linearly increasing computation costs and memory space when a set scales. This makes it inefficient for big data sets. In this paper, we propose a novel data structure for dynamic big data sets, called logarithmic dynamic cuckoo filter (LDCF). LDCF uses a novel multi-level tree structure and reduces the worst insertion and membership testing times from O(N) to O(1), where N is the size of the set. At the same time, LDCF reduces the memory cost of DCF as the cardinality of the set increases. Comprehensive experiment results show that LDCF significantly reduces the membership checking time and the memory space cost for large-scale datasets compared to state-of-the-art designs.}
}


@inproceedings{DBLP:conf/icde/HeC0Z21,
	author = {Xiaolong He and
                  Peng Cai and
                  Xuan Zhou and
                  Aoying Zhou},
	title = {Continuously Bulk Loading over Range Partitioned Tables for Large
                  Scale Historical Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {960--971},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00088},
	doi = {10.1109/ICDE51399.2021.00088},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HeC0Z21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To support efficiently and continuously loading large scale historical data into a distributed data management system (DDMS), it needs to balance the bulk workload across machines. The fundamental problem is to estimate the time used to merge currently loaded data (defined as incremental data) into previously loaded data (defined as baseline data) for each partition, referred to as partition merge. In this work, we present a learning-based framework, referred to as LeaBalancer, to balance the merge loads across cluster nodes. In the situation where the system is scheduled to have regular bulk loading tasks, LeaBalancer can learn to predict the partition merge time from the merge logs generated by previous bulk loadings. Nevertheless, it is still difficult to balance the bulk workload only using a single plan phase because of inaccurate merge time prediction or other in-progress heavy workloads during the bulk loading. To resolve this problem, we design a multi-round balancing strategy, and at the beginning of each round LeaBalancer carefully chooses partitions for migration according to the remaining merge loads in each node. Experimental results show that LeaBalancer can adaptively perform load balance under various settings.}
}


@inproceedings{DBLP:conf/icde/Liu0ZP021,
	author = {Jinfei Liu and
                  Li Xiong and
                  Qiuchen Zhang and
                  Jian Pei and
                  Jun Luo},
	title = {Eclipse: Generalizing kNN and Skyline},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {972--983},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00089},
	doi = {10.1109/ICDE51399.2021.00089},
	timestamp = {Tue, 21 Mar 2023 20:50:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/Liu0ZP021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {k nearest neighbor (kNN) queries and skyline queries are important operators on multi-dimensional data points. Given a query point, kNN returns the k nearest neighbors based on a scoring function such as a weighted sum of the attributes, which requires predefined attribute weights (or preferences). Skyline returns all possible nearest neighbors for any monotonic scoring functions without requiring attribute weights but the number of returned points can be prohibitively large.In this paper, we propose an eclipse operator that generalizes the classic 1NN and skyline queries and provides a more customizable query solution for users. In eclipse, users can specify rough and customizable attribute preferences and control the number of returned points. We show that both 1NN and skyline are instantiations of eclipse. To process eclipse queries, we propose a baseline algorithm with time complexity O(n 2 2 d−1 ), and an improved O(nlog d−1 n) time transformationbased algorithm, where n is the number of points and d is the number of dimensions. Furthermore, we propose a novel index-based algorithm utilizing duality transform with much better efficiency. The experimental results on the real NBA dataset and the synthetic datasets demonstrate the effectiveness of the eclipse operator and the efficiency of our eclipse algorithms.}
}


@inproceedings{DBLP:conf/icde/MullerFM21,
	author = {Magnus M{\"{u}}ller and
                  Daniel Flachs and
                  Guido Moerkotte},
	title = {Memory-Efficient Key/Foreign-Key Join Size Estimation via Multiplicity
                  and Intersection Size},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {984--995},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00090},
	doi = {10.1109/ICDE51399.2021.00090},
	timestamp = {Sun, 06 Oct 2024 21:04:58 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MullerFM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Join size estimation plays a crucial role in query optimization. In this paper, we present a technique to estimate the size of a key/foreign-key join of two filtered relations. We build on a model by Allen Van Gelder, in which there is no notion of join selectivity. Instead, the size of a join is estimated as a multiple of the intersection size of the join attributes. We present both a data structure to approximate the number of distinct values in a join attribute after a filter operation, and formulas to estimate the factor by which a join size exceeds the intersection size. In addition, we evaluate three existing intersection size estimation methods that are based on HyperLogLog sketches, to which our approach is closely linked. For both real-world and generated data sets, our estimator competes well, in terms of accuracy and memory footprint, against several industry-strength and state-of-the-art join size estimation methods. In particular, our experiments indicate that our approach is less prone to heavy underestimates.}
}


@inproceedings{DBLP:conf/icde/ZhangXWXC21,
	author = {Ce Zhang and
                  Cheng Xu and
                  Haixin Wang and
                  Jianliang Xu and
                  Byron Choi},
	title = {Authenticated Keyword Search in Scalable Hybrid-Storage Blockchains},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {996--1007},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00091},
	doi = {10.1109/ICDE51399.2021.00091},
	timestamp = {Mon, 06 Feb 2023 10:37:51 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhangXWXC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain has emerged as a promising solution for secure data storage and retrieval for decentralized applications. To scale blockchain systems, a prevailing approach is to employ a hybrid storage model, where only small meta-data are stored on-chain while the raw data are outsourced to an off-chain storage service provider. The key issue for query processing in such a system is the design of gas-efficient authenticated data structure (ADS) to authenticate the query results. In this paper, we study novel ADS schemes for authenticated keyword search in hybrid-storage blockchains. We first propose the Suppressed Merkle inverted (Merkle inv ) index, which maintains only a partial ADS structure on-chain that can be securely updated with a logarithm-sized cryptographic proof. Moreover, we propose a Chameleon inverted (Chameleon inv ) index that leverages the chameleon vector commitment to achieve a constant maintenance cost. It is further optimized with Bloom filters to enhance the query and verification performance. We prove the security of the proposed ADS schemes and evaluate their performance using real datasets on the Ethereum platform. Experimental results show that, compared to a baseline solution, the proposed Merkle inv and Chameleon inv indexes reduce the average on-chain maintenance cost from US\n10.39downtoUS\n2.50 and US$0.24, respectively, without sacrificing much the query performance.}
}


@inproceedings{DBLP:conf/icde/FloratosXWGYLZ21,
	author = {Sofoklis Floratos and
                  Mengbai Xiao and
                  Hao Wang and
                  Chengxin Guo and
                  Yuan Yuan and
                  Rubao Lee and
                  Xiaodong Zhang},
	title = {NestGPU: Nested Query Processing on {GPU}},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1008--1019},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00092},
	doi = {10.1109/ICDE51399.2021.00092},
	timestamp = {Thu, 21 Oct 2021 07:17:47 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FloratosXWGYLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nested queries are commonly used to express complex use-cases by connecting the output of a subquery as an input to the outer query block. However, their execution is highly time-consuming. Researchers have proposed various algorithms and techniques that unnest subqueries to improve performance. Since this is a customized approach that needs high algorithmic and engineering efforts, it is largely not an open feature in most existing database systems.Our approach is general-purpose and GPU-acceleration based, aiming for high performance at a minimum development cost. We look into the major differences between nested and unnested query structures to identify their merits and limits for GPU processing. Furthermore, we focus on the nested approach that is algorithmically simple and rich in parallels, in relatively low space complexity, and generic in program structure. We create a new code generation framework that best fits GPU for the nested method. We also make several critical system optimizations including massive parallel scanning with indexing, effective vectorization to optimize join operations, exploiting cache locality for loops and efficient GPU memory management. We have implemented the proposed solutions in NestGPU, a GPU-based column-store database system that is GPU device independent. We have extensively evaluated and tested the system to show the effectiveness of our proposed methods.}
}


@inproceedings{DBLP:conf/icde/YangCLWS21,
	author = {Fan Yang and
                  Youmin Chen and
                  Youyou Lu and
                  Qing Wang and
                  Jiwu Shu},
	title = {Aria: Tolerating Skewed Workloads in Secure In-memory Key-value Stores},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1020--1031},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00093},
	doi = {10.1109/ICDE51399.2021.00093},
	timestamp = {Tue, 15 Nov 2022 13:06:01 +0100},
	biburl = {https://dblp.org/rec/conf/icde/YangCLWS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent advent of the hardware trusted execution environment (TEE), e.g., Intel SGX, enables encrypted and integrity-verified in-memory key-value (KV) stores. However, due to the architectural limitations of the hardware, it is non-trivial to build a secure in-memory KV store with SGX without compromising the performance. The reason comes from (i) the limited memory capacity the SGX TEE provides, and (ii) being unaware of the access patterns of skewed workloads, which are commonly seen in the real world.In this paper, we present Aria, a secure in-memory KV store based on SGX. Our goal is to utilize the limited resource while still achieving high performance. Aria places KV pairs and index structures directly in the untrusted memory and introduces the security metadata in the TEE to conduct protection. The core component of Aria is Secure Cache, a software-based cache layer, which uses the limited memory resource to guarantee the confidentiality and integrity (including freshness) of Aria. Secure Cache keeps the frequently accessed security metadata in the TEE memory at fine-granularity and evicts rarely-used ones to the untrusted memory. With Secure Cache, we have the opportunities to explore strategies that are impossible in SGX implementation. By decoupling the security metadata management from the index structure, Aria supports various index schemes. We implement Aria with the indexes of both a hash table and a B-tree. Experiments show that Aria improves throughput by up to 104% compared to the state-of-the-art system.}
}


@inproceedings{DBLP:conf/icde/LiangC21,
	author = {Junkai Liang and
                  Yunpeng Chai},
	title = {CruiseDB: An LSM-Tree Key-Value Store with Both Better Tail Throughput
                  and Tail Latency},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1032--1043},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00094},
	doi = {10.1109/ICDE51399.2021.00094},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiangC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to excellent performance, LSM-tree key-value stores have been widely used in various applications in recent years. However, LSM-tree’s inherent batched data processing approach makes it suffer from poor SLA behaviors, such as a very unstable throughput and high tail latency. Unlike the I/O isolation or prioritization methods that cannot solve the SLA problem thoroughly, we have designed and implemented a new SLA-oriented LSM-tree KV store, i.e., CruiseDB, to solve both the essential and the direct SLA problems of LSM-tree KV stores by introducing an adaptive admission mechanism and improving the LSM-tree structure. According to reliable estimation of the service capacity of the LSM-tree, CruiseDB adaptively admits only an appropriate number of user requests to enter the LSM-tree memory buffer in unit time and removes the internal roadblocks of the request processing, with the advantages of preventing the write stall phenomenon, which leads to SLA declines. CruiseDB can promote the guaranteed throughput by 2.08 times on average compared with the state-of-the-art LSM-tree or B-tree KV stores.}
}


@inproceedings{DBLP:conf/icde/EryilmazKPSP21,
	author = {Zubeyr F. Eryilmaz and
                  Aarati Kakaraparthy and
                  Jignesh M. Patel and
                  Rathijit Sen and
                  Kwanghyun Park},
	title = {{FPGA} for Aggregate Processing: The Good, The Bad, and The Ugly},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1044--1055},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00095},
	doi = {10.1109/ICDE51399.2021.00095},
	timestamp = {Sun, 02 Oct 2022 16:04:35 +0200},
	biburl = {https://dblp.org/rec/conf/icde/EryilmazKPSP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we focus on current CPU-FPGA architectures and study their usability for database management systems. To focus our scope, we choose aggregation as the query processing primitive for this investigation. We implement a fully pipelined stall-free module that performs aggregation on the FPGA, and also describe a performance model that predicts the runtime of this module with 99% accuracy. We study the performance of this module on two different CPU-FPGA architectures, namely remote-main-memory and bump-in-the-wire. Compared to an implementation of aggregation on CPU, we find that the former is 1.7× slower whereas the latter is 2.2× faster. This significant performance gap suggests two important architectural considerations when designing CPU-FPGA systems, namely the bandwidth ceiling and the resource ceiling, while also highlighting issues of switching times and programmer efficiency. We consider broader hardware trends to study the suitability of the two FPGA architectures for accelerating the aggregation operation, and find that the performance gap is likely to stay in the coming future. Based on these observations, we discuss some challenges and opportunities for CPU-FPGA architectures.}
}


@inproceedings{DBLP:conf/icde/HalsteadKRPBP21,
	author = {Ben Halstead and
                  Yun Sing Koh and
                  Patricia Riddle and
                  Mykola Pechenizkiy and
                  Albert Bifet and
                  Russel Pears},
	title = {Fingerprinting Concepts in Data Streams with Supervised and Unsupervised
                  Meta-Information},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1056--1067},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00096},
	doi = {10.1109/ICDE51399.2021.00096},
	timestamp = {Mon, 05 Feb 2024 20:31:12 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HalsteadKRPBP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Streaming sources of data are becoming more common as the ability to collect data in real-time grows. A major concern in dealing with data streams is concept drift, a change in the distribution of data over time, for example, due to changes in environmental conditions. Representing concepts (stationary periods featuring similar behaviour) is a key idea in adapting to concept drift. By testing the similarity of a concept representation to a window of observations, we can detect concept drift to a new or previously seen recurring concept. Concept representations are constructed using meta-information features, values describing aspects of concept behaviour. We find that previously proposed concept representations rely on small numbers of meta-information features. These representations often cannot distinguish concepts, leaving systems vulnerable to concept drift. We propose FiCSUM, a general framework to represent both supervised and unsupervised behaviours of a concept in a fingerprint, a vector of many distinct meta-information features able to uniquely identify more concepts. Our dynamic weighting strategy learns which meta-information features describe concept drift in a given dataset, allowing a diverse set of meta-information features to be used at once. FiCSUM outperforms state-of-the-art methods over a range of 11 real world and synthetic datasets in both accuracy and modeling underlying concept drift.}
}


@inproceedings{DBLP:conf/icde/KoryckiK21,
	author = {Lukasz Korycki and
                  Bartosz Krawczyk},
	title = {Concept Drift Detection from Multi-Class Imbalanced Data Streams},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1068--1079},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00097},
	doi = {10.1109/ICDE51399.2021.00097},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KoryckiK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continual learning from data streams is among the most important topics in contemporary machine learning. One of the biggest challenges in this domain lies in creating algorithms that can continuously adapt to arriving data. However, previously learned knowledge may become outdated, as streams evolve over time. This phenomenon is known as concept drift and must be detected to facilitate efficient adaptation of the learning model. While there exists a plethora of drift detectors, all of them assume that we are dealing with roughly balanced classes. In the case of imbalanced data streams, those detectors will be biased towards the majority classes, ignoring changes happening in the minority ones. Furthermore, class imbalance may evolve over time and classes may change their roles (majority becoming minority and vice versa). This is especially challenging in the multi-class setting, where relationships among classes become complex. In this paper, we propose a detailed taxonomy of challenges posed by concept drift in multi-class imbalanced data streams, as well as a novel trainable concept drift detector based on Restricted Boltzmann Machine. It is capable of monitoring multiple classes at once and using reconstruction error to detect changes in each of them independently. Our detector utilizes a skew-insensitive loss function that allows it to handle multiple imbalanced distributions. Due to its trainable nature, it is capable of following changes in a stream and evolving class roles, as well as it can deal with local concept drift occurring in minority classes. Extensive experimental study on multi-class drifting data streams, enriched with a detailed analysis of the impact of local drifts and changing imbalance ratios, confirms the high efficacy of our approach.}
}


@inproceedings{DBLP:conf/icde/YangGSZC21,
	author = {Keyu Yang and
                  Yunjun Gao and
                  Yifeng Shen and
                  Baihua Zheng and
                  Lu Chen},
	title = {DisMASTD: An Efficient Distributed Multi-Aspect Streaming Tensor Decomposition},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1080--1091},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00098},
	doi = {10.1109/ICDE51399.2021.00098},
	timestamp = {Sat, 30 Sep 2023 09:44:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YangGSZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tensor decomposition is a fundamental multidimensional data analysis tool for many data-driven applications, such as social computing, computer vision, and bioinformatics, to name but a few. However, the rapidly increasing streaming data nowadays introduces new challenges to traditional static tensor decomposition. It requires an efficient distributed dynamic tensor decomposition without re-computing the whole tensor from scratch. In this paper, we propose DisMASTD, an efficient distributed multi-aspect streaming tensor decomposition. First, we prove the optimal tensor partitioning problem is NP-hard. Second, we present two heuristic tensor partitioning approaches to ensure the load balancing. Third, we develop a distributed multi-aspect streaming tensor decomposition computation method, which avoids repetitive computation and reduces network communication by maintaining and reusing the intermediate results. Last but not least, we perform extensive experiments with both real and synthetic datasets to demonstrate the efficiency and scalability of DisMASTD.}
}


@inproceedings{DBLP:conf/icde/HuiC0K21,
	author = {Bo Hui and
                  Haiquan Chen and
                  Da Yan and
                  Wei{-}Shinn Ku},
	title = {{EDGE:} Entity-Diffusion Gaussian Ensemble for Interpretable Tweet
                  Geolocation Prediction},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1092--1103},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00099},
	doi = {10.1109/ICDE51399.2021.00099},
	timestamp = {Fri, 06 Oct 2023 08:56:41 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuiC0K21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowing the locations of tweets can benefit a wide variety of applications such as venue recommendation, event detection, and monitoring disaster outbreaks. However, the problem of fine-grained tweet geolocation prediction is challenging since tweets are short and therefore may not contain any geo-indicative words or may contain ambiguous, noisy information. Existing solutions either yield an unsatisfactory accuracy in practical applications or make predictions that even experts struggle to interpret, failing to engender sufficient trust and actionability for real-world deployment. Our paper presents a tweet geolocation prediction framework, EDGE (Entity-Diffusion Gaussian Ensemble), which delivers predictions that are both accurate and highly interpretable without requiring any additional contextual information such as user profile and location history. In EDGE, we cast the geolocation problem as a neutral network optimization problem by learning probabilistic generative models. Compared with existing works, EDGE has two distinctive features: (1) the inference builds on mining the correlation between non geo-indicative entities and geo-indicative entities by diffusing their semantic embeddings over the constructed graph neural network (Entity Diffusion) and (2) each prediction result is represented as a Gaussian mixture instead of specific geographical coordinates (Gaussian Ensemble). Extensive experiments using real-world tweet datasets validate the superiority of EDGE over the state of the art in terms of all distance-based and POI-based metrics.}
}


@inproceedings{DBLP:conf/icde/DiYZC21,
	author = {Shimin Di and
                  Quanming Yao and
                  Yongqi Zhang and
                  Lei Chen},
	title = {Efficient Relation-aware Scoring Function Search for Knowledge Graph
                  Embedding},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1104--1115},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00100},
	doi = {10.1109/ICDE51399.2021.00100},
	timestamp = {Tue, 07 May 2024 20:05:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DiYZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The scoring function, which measures the plausibility of triplets in knowledge graphs (KGs), is the key to ensure the excellent performance of KG embedding, and its design is also an important problem in the literature. Automated machine learning (AutoML) techniques have recently been introduced into KG to design task-aware scoring functions, which achieve the state-of-the-art performance in KG embedding. However, the effectiveness of searched scoring functions is still not as good as desired. In this paper, observing that existing scoring functions can exhibit distinct performance on different semantic patterns, we are motivated to explore such semantics by searching relationa-ware scoring functions. But the relation-aware search requires a much larger search space than the previous one. Hence, we propose to encode the space as a supernet and propose an efficient alternative minimization algorithm to search through the supernet in a one-shot manner. Finally, experimental results on benchmark datasets demonstrate that the proposed method can efficiently search relation-aware scoring functions, and achieve better embedding performance than state-of-the-art methods. 1}
}


@inproceedings{DBLP:conf/icde/LeeVKLPJRF21,
	author = {Meng{-}Chieh Lee and
                  Catalina Vajiac and
                  Aayushi Kulshrestha and
                  Sacha Levy and
                  Namyong Park and
                  Cara Jones and
                  Reihaneh Rabbany and
                  Christos Faloutsos},
	title = {{INFOSHIELD:} Generalizable Information-Theoretic Human-Trafficking
                  Detection},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1116--1127},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00101},
	doi = {10.1109/ICDE51399.2021.00101},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LeeVKLPJRF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a million escort advertisements, how can we spot near-duplicates? Such micro-clusters of ads are usually signals of human trafficking. How can we summarize them, visually, to convince law enforcement to act? Can we build a general tool that works for different languages? Spotting micro-clusters of near-duplicate documents is useful in multiple, additional settings, including spam-bot detection in Twitter ads, plagiarism, and more.We present INFOSHIELD, which makes the following contributions: (a) Practical, being scalable and effective on real data, (b) Parameter-free and Principled, requiring no user-defined parameters, (c) Interpretable, finding a document to be the cluster representative, highlighting all the common phrases, and automatically detecting "slots", i.e. phrases that differ in every document; and (d) Generalizable, beating or matching domain-specific methods in Twitter bot detection and human trafficking detection respectively, as well as being language-independent finding clusters in Spanish, Italian, and Japanese. Interpretability is particularly important for the anti human-trafficking domain, where law enforcement must visually inspect ads.Our experiments on real data show that INFOSHIELD correctly identifies Twitter bots with an F1 score over 90% and detects human-trafficking ads with 84% precision. Moreover, it is scalable, requiring about 8 hours for 4 million documents on a stock laptop.}
}


@inproceedings{DBLP:conf/icde/WangTS021,
	author = {Yansheng Wang and
                  Yongxin Tong and
                  Dingyuan Shi and
                  Ke Xu},
	title = {An Efficient Approach for Cross-Silo Federated Learning to Rank},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1128--1139},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00102},
	doi = {10.1109/ICDE51399.2021.00102},
	timestamp = {Sun, 06 Oct 2024 21:04:59 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangTS021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional learning-to-rank (LTR) models are usually trained in a centralized approach based upon a large amount of data. However, with the increasing awareness of data privacy, it is harder to collect data from multiple owners as before, and the resultant data isolation problem makes the performance of learned LTR models severely compromised. Inspired by the recent progress in federated learning, we propose a novel framework named Cross-Silo Federated Learning-to-Rank (CS-F-LTR), where the efficiency issue becomes the major bottleneck. To deal with the challenge, we first devise a privacy-preserving cross-party term frequency querying scheme based on sketching algorithms and differential privacy. To further improve the overall efficiency, we propose a new structure named reverse top-K sketch (RTK-Sketch) which significantly accelerates the feature generation process while holding theoretical guarantees on accuracy loss. Extensive experiments conducted on public datasets verify the effectiveness and efficiency of the proposed approach.}
}


@inproceedings{DBLP:conf/icde/ChengKZ021,
	author = {Zhaoyue Cheng and
                  Nick Koudas and
                  Zhe Zhang and
                  Xiaohui Yu},
	title = {Efficient Construction of Nonlinear Models over Normalized Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1140--1151},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00103},
	doi = {10.1109/ICDE51399.2021.00103},
	timestamp = {Mon, 05 Feb 2024 20:31:12 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ChengKZ021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine Learning (ML) applications are proliferating in the enterprise. Relational data which are prevalent in enterprise applications are typically normalized; as a result, data has to be denormalized via primary/foreign-key joins to be provided as input to ML algorithms. In this paper, we study the implementation of popular nonlinear ML models, Gaussian Mixture Models (GMM) and Neural Networks (NN), over normalized data addressing both cases of binary and multiway joins over normalized relations.For the case of GMM, we show how it is possible to decompose computation in a systematic way both for binary joins and for multi-way joins to construct mixture models. We demonstrate that by factoring the computation, one can conduct the training of the models much faster compared to other applicable approaches, without any loss in accuracy.For the case of NN, we propose algorithms to train the network taking normalized data as the input. Similarly, we present algorithms that can conduct the training of the network in a factorized way and offer performance advantages. The redundancy introduced by denormalization can be exploited for certain types of activation functions. However, we demonstrate that attempting to explore this redundancy is helpful up to a certain point; exploring redundancy at higher layers of the network will always result in increased costs and is not recommended.We present the results of a thorough experimental evaluation, varying several parameters of the input relations involved and demonstrate that our proposals for the training of GMM and NN yield drastic performance improvements typically starting at 100%, which become increasingly higher as parameters of the underlying data vary, without any loss in accuracy.}
}


@inproceedings{DBLP:conf/icde/AslayCGM21,
	author = {{\c{C}}igdem Aslay and
                  Martino Ciaperoni and
                  Aristides Gionis and
                  Michael Mathioudakis},
	title = {Workload-aware Materialization for Efficient Variable Elimination
                  on Bayesian Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1152--1163},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00104},
	doi = {10.1109/ICDE51399.2021.00104},
	timestamp = {Mon, 03 Jan 2022 22:33:29 +0100},
	biburl = {https://dblp.org/rec/conf/icde/AslayCGM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bayesian networks are general, well-studied probabilistic models that capture dependencies among a set of variables. Variable Elimination is a fundamental algorithm for probabilistic inference over Bayesian networks. In this paper, we propose a novel materialization method, which can lead to significant efficiency gains when processing inference queries using the Variable Elimination algorithm. In particular, we address the problem of choosing a set of intermediate results to precompute and materialize, so as to maximize the expected efficiency gain over a given query workload. For the problem we consider, we provide an optimal polynomial-time algorithm and discuss alternative methods. We validate our technique using real-world Bayesian networks. Our experimental results confirm that a modest amount of materialization can lead to significant improvements in the running time of queries, with an average gain of 70%, and reaching up to a gain of 99%, for a uniform workload of queries. Moreover, in comparison with existing junction tree methods that also rely on materialization, our approach achieves competitive efficiency during inference using significantly lighter materialization.}
}


@inproceedings{DBLP:conf/icde/AlfassiGYK21,
	author = {Yuval Alfassi and
                  Moshe Gabel and
                  Gal Yehuda and
                  Daniel Keren},
	title = {A Distance-Based Scheme for Reducing Bandwidth in Distributed Geometric
                  Monitoring},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1164--1175},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00105},
	doi = {10.1109/ICDE51399.2021.00105},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/AlfassiGYK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tracking the value of a function computed from a dynamic, distributed data stream is a challenging problem with many real-world applications. Continuously forwarding data updates can be costly, yet complex functions are difficult to evaluate when data is not centralized. One general approach to continuous distributed monitoring is the Geometric Monitoring (GM) family of techniques. GM reduces the functional monitoring problem to a set of local constraints that each node checks locally, and uses a simple protocol to update those constraints as needed.While most work on GM focuses on reducing the number of messages exchanged by the common GM protocol, with one recent notable exception, there has been little attention to reducing the size of those messages, which impacts bandwidth.We propose the Distance Scheme: a novel bandwidth-efficient variation of the GM protocol that reduces the size of most monitoring messages in GM to a single scalar, and is compatible with the large body of prior work on GM. We apply it to monitor three different functions using three real-world datasets, and show it substantially reduces bandwidth while requiring fewer messages to be transmitted than the current state-of-the-art approach. We further describe a value-based scheme that, while typically outperformed by the Distance Scheme, is simpler to apply, matches state-of-the-art bandwidth performance with fewer messages, and is also compatible with existing work.}
}


@inproceedings{DBLP:conf/icde/LiuSC21,
	author = {Qiyu Liu and
                  Yanyan Shen and
                  Lei Chen},
	title = {LHist: Towards Learning Multi-dimensional Histogram for Massive Spatial
                  Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1188--1199},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00107},
	doi = {10.1109/ICDE51399.2021.00107},
	timestamp = {Sun, 04 Aug 2024 19:37:45 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuSC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data synopsis is widely adopted to speed-up query processing over large spatial databases. As one of the most popular spatial data synopses, multi-dimensional histograms (MH) have been studied and adopted by modern DBMS and analytical systems for decades. However, existing MH construction techniques highly rely on expert knowledge and statistical assumptions, making them hard to achieve consistently satisfactory performance across different datasets. Inspired by the emerging learned index techniques where the widely used index structures like B-tree can be further improved by integrating simple machine learning models, in this paper, we propose a learned data synopsis technique named Learned Multi-dimensional Histogram (LHist). Compared with the traditional data synopsis techniques, LHist is fully data-driven, easy-to-implement, and has the potential to achieve better storage-accuracy trade-off. On the typical task of range COUNT query estimation, the extensive experimental studies on large-scale real-world datasets and synthetic benchmarks reveal that LHist can outperform the existing synopsis structures in terms of storage cost, query processing efficiency, and estimation accuracy.}
}


@inproceedings{DBLP:conf/icde/WangZWM0Z21,
	author = {Guang Wang and
                  Shuxin Zhong and
                  Shuai Wang and
                  Fei Miao and
                  Zheng Dong and
                  Desheng Zhang},
	title = {Data-Driven Fairness-Aware Vehicle Displacement for Large-Scale Electric
                  Taxi Fleets},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1200--1211},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00108},
	doi = {10.1109/ICDE51399.2021.00108},
	timestamp = {Tue, 07 May 2024 20:05:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangZWM0Z21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We are witnessing a rapid taxi electrification process due to the ever-increasing concern about urban air quality and energy security. A key difference between conventional gas taxis and electric taxis is their energy replenishment mechanisms, i.e., refueling or charging, which is reflected in two aspects: (i) much longer charging processes vs. short refueling processes and (ii) time-varying electricity prices vs. time-invariant gasoline prices during a day. The complicated charging issues (e.g., long charging time and dynamic charging pricing) potentially reduce electric taxis’ daily operation time and profits, and also cause overcrowded charging stations during some off-peak charging pricing periods. Motivated by a set of findings obtained from a data-driven investigation, in this paper, we design a fairness-aware vehicle displacement system called FairMove to improve the overall profit efficiency and profit fairness of electric taxi fleets by considering both the passenger travel demand and taxi charging demand. We first formulate the electric taxi displacement problem as multi-agent deep reinforcement learning, and then we propose a centralized multi-agent actor-critic approach to tackle this problem. More importantly, we implement and evaluate FairMove with real-world streaming data from the Chinese city Shenzhen, including GPS data and transaction data from more than 20,100 electric taxis, coupled with the data of 123 charging stations, which constitute, to our knowledge, the largest all-electric taxi network in the world. The extensive experimental results show that our fairness-aware FairMove effectively improves the profit efficiency and profit fairness of the Shenzhen electric taxi fleet by 25.2% and 54.7%, respectively.}
}


@inproceedings{DBLP:conf/icde/WangX0PWX21,
	author = {Ting Wang and
                  Xike Xie and
                  Xin Cao and
                  Torben Bach Pedersen and
                  Yang Wang and
                  Mingjun Xiao},
	title = {On Efficient and Scalable Time-Continuous Spatial Crowdsourcing},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1212--1223},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00109},
	doi = {10.1109/ICDE51399.2021.00109},
	timestamp = {Fri, 14 Apr 2023 17:08:27 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangX0PWX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of advanced mobile terminals opened up a new crowdsourcing avenue, spatial crowdsourcing, to utilize the crowd potential to perform real-world tasks. In this work, we study a new type of spatial crowdsourcing, called time-continuous spatial crowdsourcing (TCSC in short). It supports broad applications for long-term continuous spatial data acquisition, ranging from environmental monitoring to traffic surveillance in citizen science and crowdsourcing projects. However, due to limited budgets and limited availability of workers in practice, the data collected is often incomplete, incurring data deficiency problem. To tackle that, in this work, we first propose an entropy-based quality metric, which captures the joint effects of incompletion in data acquisition and the imprecision in data interpolation. Based on that, we investigate quality-aware task assignment methods for both single- and multi-task scenarios. We show the NP-hardness of the single-task case, and design polynomial-time algorithms with guaranteed approximation ratios. We study novel indexing and pruning techniques for further enhancing the performance in practice. Then, we extend the solution to multi-task scenarios and devise a parallel framework for speeding up the process of optimization. We conduct extensive experiments on both real and synthetic datasets to show the effectiveness of our proposals.}
}


@inproceedings{DBLP:conf/icde/LiHLPPC21,
	author = {Guanyao Li and
                  Chih{-}Chieh Hung and
                  Mengyun Liu and
                  Linfei Pan and
                  Wen{-}Chih Peng and
                  S.{-}H. Gary Chan},
	title = {Spatial-Temporal Similarity for Trajectories with Location Noise and
                  Sporadic Sampling},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1224--1235},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00110},
	doi = {10.1109/ICDE51399.2021.00110},
	timestamp = {Mon, 05 Feb 2024 20:31:12 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiHLPPC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid advances and the penetration of the Internet of Things and sensors, a massive amount of trajectory data, given by discrete locations at certain timestamps, have been extracted or collected. Knowing the similarity between trajectories is fundamental to understanding their spatial-temporal correlation, with direct and far-reaching applications in contact tracing, companion detection, personalized marketing, etc. In this work, we consider the general and realistic sensing scenario that the locations of the trajectories may be noisy, and that these trajectories are sporadically sampled with randomness and asynchrony from the underlying continuous paths. Most of the prior work on trajectory similarity has not sufficiently considered the temporal dimension, or the issues of location noise and sporadic sampling, while others have limitations of strong assumptions such as a fixed known speed of users or the availability of a large amount of training data.We propose a novel and effective spatial-temporal measure termed STS (Spatial-Temporal Similarity) to evaluate the spatial-temporal overlap between any two trajectories. In order to account for the location noise and sporadic sampling, STS models each location in a trajectory as an observable outcome drawn from a probability distribution. With that, it efficiently reduces the need for training data by estimating a personalized spatial-temporal probability distribution of the object position from its own trajectory. Based on that, it subsequently computes the co-location probability and hence derives the similarity of any two trajectories. We have conducted extensive experiments to evaluate STS using real large-scale indoor (mall) and outdoor (taxi) datasets. Our results show that STS is substantially more accurate and robust than the state-of-the-art approaches, with an improvement of 63% on precision and 85% on mean rank.}
}


@inproceedings{DBLP:conf/icde/ShragaAG21,
	author = {Roee Shraga and
                  Ofra Amir and
                  Avigdor Gal},
	title = {Learning to Characterize Matching Experts},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1236--1247},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00111},
	doi = {10.1109/ICDE51399.2021.00111},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ShragaAG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Matching is a task at the heart of any data integration process, aimed at identifying correspondences among data elements. Matching problems were traditionally solved in a semi-automatic manner, with correspondences being generated by matching algorithms and outcomes subsequently validated by human experts. Human-in-the-loop data integration has been recently challenged by the introduction of big data and recent studies have analyzed obstacles to effective human matching and validation. In this work we characterize human matching experts, those humans whose proposed correspondences can mostly be trusted to be valid. We provide a novel framework for characterizing matching experts that, accompanied with a novel set of features, can be used to identify reliable and valuable human experts. We demonstrate the usefulness of our approach using an extensive empirical evaluation. In particular, we show that our approach can improve matching results by filtering out inexpert matchers.}
}


@inproceedings{DBLP:conf/icde/GazzarriH21,
	author = {Leonardo Gazzarri and
                  Melanie Herschel},
	title = {End-to-end Task Based Parallelization for Entity Resolution on Dynamic
                  Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1248--1259},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00112},
	doi = {10.1109/ICDE51399.2021.00112},
	timestamp = {Sun, 06 Oct 2024 21:04:57 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GazzarriH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity resolution (ER) is the problem of finding which digital representations of entities correspond to the same real-world entity. In many Big Data scenarios, in addition to the problems of volume and variety that are commonly addressed in ER, data is continuously generated, which requires novel solutions to address the velocity problem.This paper presents a framework for end-to-end ER that incrementally and efficiently produces results as heterogeneous data streams in. These characteristics are achieved by proposing a novel functional model for ER on incremental or streaming data, and adopting task-based parallelization. Our evaluation demonstrates that even without parallelization, our framework outperforms state-of-the-art (batch) ER in terms of runtime and quality. We also validate that it can achieve high throughput and low latency on streaming data, paving the way to real-time ER.}
}


@inproceedings{DBLP:conf/icde/LiWLD0Z21,
	author = {Youfu Li and
                  Jin Wang and
                  Mingda Li and
                  Ariyam Das and
                  Jiaqi Gu and
                  Carlo Zaniolo},
	title = {KDDLog: Performance and Scalability in Knowledge Discovery by Declarative
                  Queries with Aggregates},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1260--1271},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00113},
	doi = {10.1109/ICDE51399.2021.00113},
	timestamp = {Fri, 06 Sep 2024 16:14:02 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiWLD0Z21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Demand for powerful, high-performance analytics on large Data Bases has been ever growing. Database Management Systems have long shown that descriptive analytics can be supported quite effectively by enriching traditional aggregates with constructs such as Data Cubes and other ROLAPs — thus extending the optimizability and parallelizability of RDBMS. In this paper, we show that these benefits can now be extended to predictive analytics, e.g. clustering, classification and association, by using aggregates in declarative recursive queries. Therefore, we introduce KDDLog, a scalable framework which leverages recursive queries with aggregates and our newly-proposed chain aggregates to enable users to build or customize knowledge discovery models with concise and expressive queries. We further propose specialized compilation techniques for semi-naive fix-point computation in the presence of aggregates, and optimizations for complex recursive queries on distributed data platforms. We provide KDDLib to build knowledge discovery tasks and advanced interfaces to ease users of porting new models. Extensive evaluations on large-scale datasets demonstrate that our approach achieves promising performance gain while offering both increased generality and ease of programming knowledge discovery applications.}
}


@inproceedings{DBLP:conf/icde/BogatuPDDF21,
	author = {Alex Bogatu and
                  Norman W. Paton and
                  Mark Douthwaite and
                  Stuart Davie and
                  Andr{\'{e}} Freitas},
	title = {Cost-effective Variational Active Entity Resolution},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1272--1283},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00114},
	doi = {10.1109/ICDE51399.2021.00114},
	timestamp = {Mon, 26 Jun 2023 20:41:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BogatuPDDF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurately identifying different representations of the same real–world entity is an integral part of data cleaning and many methods have been proposed to accomplish it. The challenges of this entity resolution task that demand so much research attention are often rooted in the task–specificity and user–dependence of the process. Adopting deep learning techniques has the potential to lessen these challenges. In this paper, we set out to devise an entity resolution method that builds on the robustness conferred by deep autoencoders to reduce human–involvement costs. Specifically, we reduce the cost of training deep entity resolution models by performing unsupervised representation learning. This unveils a transferability property of the resulting model that can further reduce the cost of applying the approach to new datasets by means of transfer learning. Finally, we reduce the cost of labeling training data through an active learning approach that builds on the properties conferred by the use of deep autoencoders. Empirical evaluation confirms the accomplishment of our cost–reduction desideratum, while achieving comparable effectiveness with state–of–the–art alternatives.}
}


@inproceedings{DBLP:conf/icde/BleifussBKNS21,
	author = {Tobias Bleifu{\ss} and
                  Leon Bornemann and
                  Dmitri V. Kalashnikov and
                  Felix Naumann and
                  Divesh Srivastava},
	title = {Structured Object Matching across Web Page Revisions},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1284--1295},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00115},
	doi = {10.1109/ICDE51399.2021.00115},
	timestamp = {Sat, 31 Jul 2021 17:22:27 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BleifussBKNS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A considerable amount of useful information on the web is (semi-)structured, such as tables and lists. An extensive corpus of prior work addresses the problem of making these human-readable representations interpretable by algorithms. Most of these works focus only on the most recent snapshot of these web objects. However, their evolution over time represents valuable information that has barely been tapped, enabling various applications, including visual change exploration and trust assessment. To realize the full potential of this information, it is critical to match such objects across page revisions.In this work, we present novel techniques that match tables, infoboxes and lists within a page across page revisions. We are, thus, able to extract the evolution of structured information in various forms from a long series of web page revisions. We evaluate our approach on a representative sample of pages and measure the number of correct matches. Our approach achieves a significant improvement in object matching over baselines and over related work.}
}


@inproceedings{DBLP:conf/icde/WangZWP21,
	author = {Pei Wang and
                  Weiling Zheng and
                  Jiannan Wang and
                  Jian Pei},
	title = {Automating Entity Matching Model Development},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1296--1307},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00116},
	doi = {10.1109/ICDE51399.2021.00116},
	timestamp = {Fri, 24 Nov 2023 11:49:36 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WangZWP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper seeks to answer one important but unexplored question for Entity Matching (EM): can we develop a good machine learning pipeline automatically for the EM task? If yes, to what extent the process can be automated? To answer this question, we find that a general-purpose AutoML tool cannot be directly applied to solve an EM problem, thus propose AutoML-EM, an automated model pipeline development solution tailored for EM. In reality, however, another bottleneck of EM problem is the insufficient labeled data. To mitigate this issue, active learning based solutions are widely adopted. Under this setting, we propose AutoML-EM-Active, investigating how to maximize the benefit of AutoML-EM with automatic data labeling. We provide fundamental insights into our solutions and conduct extensive experiments to examine their performance on benchmark datasets. The results suggest that AutoML-EM not only avoids human involvement in model development process but also reaches or exceeds the state-of-the-art EM performance, and AutoML-EM-Active improves the model performance under the active learning setting effectively.}
}


@inproceedings{DBLP:conf/icde/ChenLQ0L21,
	author = {Xiaoshuang Chen and
                  Longbin Lai and
                  Lu Qin and
                  Xuemin Lin and
                  Boge Liu},
	title = {A Framework to Quantify Approximate Simulation on Graph Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1308--1319},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00117},
	doi = {10.1109/ICDE51399.2021.00117},
	timestamp = {Sun, 04 Aug 2024 19:37:46 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenLQ0L21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Simulation and its variants (e.g., bisimulation and degree-preserving simulation) are useful in a wide spectrum of applications. However, all simulation variants are coarse "yes-or-no" indicators that simply confirm or refute whether one node simulates another, which limits the scope and power of their utility. Therefore, it is meaningful to develop a fractional χ-simulation measure to quantify the degree to which one node simulates another by the simulation variant χ. To this end, we first present several properties necessary for a fractional χ-simulation measure. Then, we present FSim χ , a general fractional χ-simulation computation framework that can be configured to quantify the extent of all χ-simulations. Comprehensive experiments and real-world case studies show the measure to be effective and the computation framework to be efficient.}
}


@inproceedings{DBLP:conf/icde/LaiPY0021,
	author = {Zhengmin Lai and
                  You Peng and
                  Shiyu Yang and
                  Xuemin Lin and
                  Wenjie Zhang},
	title = {{PEFP:} Efficient k-hop Constrained s-t Simple Path Enumeration on
                  {FPGA}},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1320--1331},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00118},
	doi = {10.1109/ICDE51399.2021.00118},
	timestamp = {Tue, 21 Mar 2023 20:50:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LaiPY0021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph plays a vital role in representing entities and their relationships in a variety of fields, such as e-commerce networks, social networks and biological networks. Given two vertices s and t, one of the fundamental problems in graph databases is to investigate the relationships between s and t. A well-studied problem in such area is k-hop constrained s-t simple path enumeration. Nevertheless, all existing algorithms targeting this problem follow the DFS-based paradigm, which cannot scale up well. Moreover, using hardware devices like FPGA to accelerate graph computation has become popular. Motivated by this, in this paper, we propose the first FPGA-based algorithm PEFP to solve the problem of k-hop constrained s-t simple path enumeration efficiently. On the host side, we propose a preprocessing algorithm Pre-BFS to reduce the graph size and search space. On the FPGA side in PEFP, we propose a novel DFS-based batching technique to save on-chip memory efficiently. In addition, we also propose caching techniques to cache necessary data in BRAM, which overcome the latency bottleneck brought by the read/write operations from/to FPGA DRAM. Finally, we propose a data separation technique to enable dataflow optimization for the path verification module; hence the sub-stages in that module can be executed in parallel. Comprehensive experiments show that PEFP outperforms the state-of-the-art algorithm JOIN by more than 1 order of magnitude by average, and up to 2 orders of magnitude in terms of preprocessing time, query processing time and total time, respectively.}
}


@inproceedings{DBLP:conf/icde/YuQZZ021,
	author = {Michael Yu and
                  Lu Qin and
                  Ying Zhang and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {{DPTL+:} Efficient Parallel Triangle Listing on Batch-Dynamic Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1332--1343},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00119},
	doi = {10.1109/ICDE51399.2021.00119},
	timestamp = {Tue, 21 Mar 2023 20:50:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/YuQZZ021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Triangle listing is an important topic in many practical applications. We have observed that this problem has not yet been studied systematically in the context of batch-dynamic graphs. In this paper, we aim to fill this gap by developing novel and efficient parallel solutions. Specifically, given a graph G and a batch-update of edges B, we report the updated triangles (deleted triangles and new triangles) resulting from the batch of updates. We notice that it is cost expensive to directly apply state-of-the-art triangle listing algorithms because they are designed to enumerate the complete set of triangles from a given graph, whereas only the updated ones are the relevant output for our problem setting. In this paper, we developed an efficient algorithm, namely DPTL, based on a newly designed orientation technique, which only outputs the updated triangles while ensuring that each triangle solution is identified without any duplicate solutions. We follow up by taking advantage of a graph’s degree distributions and designed a more sophisticated algorithm, namely DPTL+. We show that DPTL+ can achieve the best performance in terms of both practical performance and theoretical time complexity. Our comprehensive experiments over 28 real-life large graphs show the superior performance of the DPTL+ algorithm when compared against DPTL and two baseline solutions. Theoretically, we also show that DPTL+ has a time complexity of Θ(∑ 〈u,v〉∈B min{deg(u), deg(v)}+m) where deg(x) is the degree of a vertex x, and m is the number of edges adjacent to the vertices in the batch-update. This time complexity is more promising than that of other solutions.}
}


@inproceedings{DBLP:conf/icde/Li0CZL0021,
	author = {Xiaofan Li and
                  Rui Zhou and
                  Lu Chen and
                  Yong Zhang and
                  Chengfei Liu and
                  Qiang He and
                  Yun Yang},
	title = {Finding a Summary for All Maximal Cliques},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1344--1355},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00120},
	doi = {10.1109/ICDE51399.2021.00120},
	timestamp = {Thu, 25 Jul 2024 07:48:20 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Li0CZL0021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The number of maximal cliques could be exponentially large with respect to the number of vertices. A clique summary is a subset of all the maximal cliques and can somehow represent all the maximal cliques. Finding such a summary is deemed important in information distribution, influence estimation, cost-effective marketing, etc. The existing approach that finds a maximal clique summary suffers from long running time due to the excessive number of costly bound calculations that are used to estimate the size of to-be-found cliques during the enumeration process. Furthermore, we found that, sometimes, the bound calculation is not necessary at all. As a result, in order to provide the best study of the problem, we propose four strategies in two directions to speed up the process of finding a maximal clique summary by (1) restricting the bound calculation operation to a particular subset of all search branches and (2) making the best use of the bounds that have been previously calculated. Extensive experiments are conducted on eight real-world datasets to validate our strategies. Results demonstrate that the proposed method can reduce the number of bound calculations by 3 ∼ 5 orders of magnitude, and each run of our algorithm can be up to 2.x times faster than the state-of-the-art algorithm while still keeping the summary concise. Our method can potentially benefit other large-output enumeration based problems, such as frequent itemset mining, when a summary of results is needed.}
}


@inproceedings{DBLP:conf/icde/Liu0ZX21,
	author = {Kaixin Liu and
                  Sibo Wang and
                  Yong Zhang and
                  Chunxiao Xing},
	title = {An Efficient Algorithm for the Anchored k-Core Budget Minimization
                  Problem},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1356--1367},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00121},
	doi = {10.1109/ICDE51399.2021.00121},
	timestamp = {Sun, 12 Nov 2023 02:08:08 +0100},
	biburl = {https://dblp.org/rec/conf/icde/Liu0ZX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online social networking companies devote much effort to maximize the engagement of users. Most existing methods assume that the budget is already known and focus on heuristic solutions to maximize the number of users that keep engaged. However, many real-life applications aim to keep the engagement of the majority of users in the social network. In such scenarios, the budget is flexible, and the goal is to minimize the budget used to keep these users active. In this paper, we define and study the anchored k-core budget minimization problem. Given an undirected graph G, a degree constraint k and a quota q for engaged users, this problem aims to minimize the budget that assures at least q users not in k-core keeping engaged.We propose CLOCK, an efficient greedy algorithm to solve the anchored k-core budget minimization problem. The main idea is to delete the anchored vertex with the lowest score in every iteration while maintaining the required number of engaged users. By considering the influence among the anchored vertices, a scoring function is well designed to identify the anchored vertices with high quality. Then we propose two merging strategies to get better anchored vertices. In addition, we dynamically maintain all anchored vertices by a local update strategy which avoids many redundant calculations. Finally, we conduct extensive experiments on real world datasets, demonstrating the effectiveness and efficiency of CLOCK.}
}


@inproceedings{DBLP:conf/icde/GuNPGIH21,
	author = {Geonmo Gu and
                  Yehyun Nam and
                  Kunsoo Park and
                  Zvi Galil and
                  Giuseppe F. Italiano and
                  Wook{-}Shin Han},
	title = {Scalable Graph Isomorphism: Combining Pairwise Color Refinement and
                  Backtracking via Compressed Candidate Space},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1368--1379},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00122},
	doi = {10.1109/ICDE51399.2021.00122},
	timestamp = {Sun, 06 Oct 2024 21:04:57 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GuNPGIH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph isomorphism is a core problem in graph analysis of various application domains. Given two graphs, the graph isomorphism problem is to determine whether there exists an isomorphism between them. As real-world graphs are getting bigger and bigger, applications demand practically fast algorithms that can run on large-scale graphs. However, existing approaches such as graph canonization and subgraph isomorphism show limited performances on large-scale graphs either in time or space. In this paper, we propose a new approach to graph isomorphism, which is the framework of pairwise color refinement and efficient backtracking. The main features of our approach are: (1) pairwise color refinement and binary cell mapping (2) compressed CS (candidate space), and (3) partial failing set, which together lead to a much faster and scalable algorithm for graph isomorphism. Extensive experiments with real-world datasets show that our approach outperforms state-of-the-art algorithms by up to orders of magnitude in terms of running time.}
}


@inproceedings{DBLP:conf/icde/JensenP021,
	author = {S{\o}ren Kejser Jensen and
                  Torben Bach Pedersen and
                  Christian Thomsen},
	title = {Scalable Model-Based Management of Correlated Dimensional Time Series
                  in ModelarDB+},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1380--1391},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00123},
	doi = {10.1109/ICDE51399.2021.00123},
	timestamp = {Thu, 14 Oct 2021 10:29:21 +0200},
	biburl = {https://dblp.org/rec/conf/icde/JensenP021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To monitor critical infrastructure, high quality sensors sampled at a high frequency are increasingly used. However, as they produce huge amounts of data, only simple aggregates are stored. This removes outliers and fluctuations that could indicate problems. As a remedy, we present a model-based approach for managing time series with dimensions that exploits correlation in and among time series. Specifically, we propose compressing groups of correlated time series using an extensible set of model types within a user-defined error bound (possibly zero). We name this new category of model-based compression methods for time series Multi-Model Group Compression (MMGC). We present the first MMGC method GOLEMM and extend model types to compress time series groups. We propose primitives for users to effectively define groups for differently sized data sets, and based on these, an automated grouping method using only the time series dimensions. We propose algorithms for executing simple and multi-dimensional aggregate queries on models. Last, we implement our methods in the Time Series Management System (TSMS) ModelarDB (ModelarDB + ). Our evaluation shows that compared to widely used formats, ModelarDB + provides up to 13.7x faster ingestion due to high compression, 113x better compression due to the adaptivity of GOLEMM, 573x faster aggregates by using models, and close to linear scalability. It is also extensible and supports online query processing.}
}


@inproceedings{DBLP:conf/icde/GuptaHS21,
	author = {Suyash Gupta and
                  Jelle Hellings and
                  Mohammad Sadoghi},
	title = {{RCC:} Resilient Concurrent Consensus for High-Throughput Secure Transaction
                  Processing},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1392--1403},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00124},
	doi = {10.1109/ICDE51399.2021.00124},
	timestamp = {Tue, 01 Oct 2024 16:56:08 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GuptaHS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, we saw the emergence of consensus-based database systems that promise resilience against failures, strong data provenance, and federated data management. Typically, these fully-replicated systems are operated on top of a primary-backup consensus protocol, which limits the throughput of these systems to the capabilities of a single replica (the primary).To push throughput beyond this single-replica limit, we propose concurrent consensus. In concurrent consensus, replicas independently propose transactions, thereby reducing the influence of any single replica on performance. To put this idea in practice, we propose our RCC paradigm that can turn any primary-backup consensus protocol into a concurrent consensus protocol by running many consensus instances concurrently. RCC is designed with performance in mind and requires minimal coordination between instances. Furthermore, RCC also promises increased resilience against failures. We put the design of RCC to the test by implementing it in ResilientDB, our high-performance resilient blockchain fabric, and comparing it with state-of-the-art primary-backup consensus protocols. Our experiments show that RCC achieves up to 2.75× higher throughput than other consensus protocols and can be scaled to 91 replicas.}
}


@inproceedings{DBLP:conf/icde/ZhaoJW21,
	author = {Xingsheng Zhao and
                  Song Jiang and
                  Xingbo Wu},
	title = {WipDB: {A} Write-in-place Key-value Store that Mimics Bucket Sort},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1404--1415},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00125},
	doi = {10.1109/ICDE51399.2021.00125},
	timestamp = {Wed, 24 Aug 2022 13:35:40 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoJW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Key-value (KV) stores have become a major storage infrastructure on which databases, file systems, and other data management systems are built. To support efficient indexing and range search, the key-value items must be sorted. However, this sorting process can be excessively expensive. In the KV systems adopting the popular Log-Structured Merge Tree (LSM) structure or its variants, the write volume can be amplified by tens of times due to its repeated internal merge-sorting operation.In this paper we propose a KV store design that leverages relatively stable key distributions to bound the write amplification by a number as low as 4.15 in practice. The key idea is, instead of incrementally sorting KV items in the LSM’s hierarchical structure, it writes KV items right in place in an approximately sorted list, much like a bucket sort algorithm does. The design also makes it possible to keep most internal data reorganization operations off the critical path of read service. The so-called Write-in-place (Wip) scheme has been implemented with its source code publicly available. Experiment results show that WipDB improves write throughput by 3 to 8× (to around 1Mops/s on one Intel PCIe SSD) over state-of-the-art KV stores.}
}


@inproceedings{DBLP:conf/icde/GuoZC21,
	author = {Hua Guo and
                  Xuan Zhou and
                  Le Cai},
	title = {Lock Violation for Fault-tolerant Distributed Database System\({}^{\mbox{*}}\)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1416--1427},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00126},
	doi = {10.1109/ICDE51399.2021.00126},
	timestamp = {Sun, 06 Oct 2024 21:04:57 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GuoZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern distributed database systems scale horizontally by partitioning their data across a large number of nodes. Most such systems build their transactional layers on a replication layer, employing a consensus protocol to ensure data consistency to achieve fault tolerance. Synchronization among replicated state machines thus becomes a significant overhead of transaction processing. Without careful design, synchronization could amplify transactions’ lock duration and impair the system’s scalability. Speculative techniques, such as Controlled Lock Violation (CLV) and Early Lock Release (ELR), prove useful in shortening lock’s critical path and boosting transaction processing performance. To use these techniques to optimize geo-replicated distributed databases(GDDB) is an intuitive idea. This paper shows that a naive application of speculation is often unhelpful in a distributed environment. Instead, we introduce Distributed Lock Violation (DLV), a specialized speculative technique for geo-replicated distributed databases. DLV can achieve good performance without incurring severe side effects.}
}


@inproceedings{DBLP:conf/icde/GevayRBMQM21,
	author = {G{\'{a}}bor E. G{\'{e}}vay and
                  Tilmann Rabl and
                  Sebastian Bre{\ss} and
                  Lorand Madai{-}Tahy and
                  Jorge{-}Arnulfo Quian{\'{e}}{-}Ruiz and
                  Volker Markl},
	title = {Efficient Control Flow in Dataflow Systems: When Ease-of-Use Meets
                  High Performance},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1428--1439},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00127},
	doi = {10.1109/ICDE51399.2021.00127},
	timestamp = {Mon, 26 Jun 2023 20:41:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GevayRBMQM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern data analysis tasks often involve control flow statements, such as iterations. Common examples are PageRank and K-means. To achieve scalability, developers usually implement data analysis tasks in distributed dataflow systems, such as Spark and Flink. However, for tasks with control flow statements, these systems still either suffer from poor performance or are hard to use. For example, while Flink supports iterations and Spark provides ease-of-use, Flink is hard to use and Spark has poor performance for iterative tasks. As a result, developers typically have to implement different workarounds to run their jobs with control flow statements in an easy and efficient way.We propose Mitos, a system that achieves the best of both worlds: it achieves both high performance and ease-of-use. Mitos uses an intermediate representation that abstracts away specific control flow statements and is able to represent any imperative control flow. This facilitates building the dataflow graph and coordinating the distributed execution of control flow in a way that is not tied to specific control flow constructs. Our experimental evaluation shows that the performance of Mitos is more than one order of magnitude better than systems that launch new dataflow jobs for every iteration step. Remarkably, it is also up to 10.5 times faster than Flink, which has native iteration support, while matching the ease-of-use of Spark.}
}


@inproceedings{DBLP:conf/icde/MaiyyaAAA21,
	author = {Sujaya Maiyya and
                  Ishtiyaque Ahmad and
                  Divyakant Agrawal and
                  Amr El Abbadi},
	title = {Samya: {A} Geo-Distributed Data System for High Contention Aggregate
                  Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1440--1451},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00128},
	doi = {10.1109/ICDE51399.2021.00128},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MaiyyaAAA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geo-distributed databases are the state of the art tools for managing cloud-based data. But maintaining hot records in geo-distributed databases such as Google’s Spanner can be expensive, as it synchronizes each update across a majority of replicas. Frequent synchronization poses an obstacle to achieve high throughput for contentious updateheavy workloads. While such synchronizations are inevitable for complex data types, simple data types such as aggregate data can benefit from reduced synchronizations. To this end, we propose an alternate data management system, Samya, to manage aggregate cloud resource usage data. Samya disaggregates available resources and stores fractions of these resources across geo-distributed sites. Dis-aggregation allows sites to serve client requests independently without synchronization for each update. Samya incorporates a learning mechanism to predict future resource demands. If the predicted demand is not satisfied locally, a synchronization protocol, Avantan, is executed to redistribute available resources in the system. Avantan is a novel fault-tolerant consensus protocol where sites agree on the global availability of resources prior to redistribution. Experiments conducted on Google Cloud Platform highlight that dis-aggregating data and reducing synchronizations allows Samya to commit 16x to 18x more transactions than state of the art cloud geo-distributed systems such as Spanner and CockroachDB.}
}


@inproceedings{DBLP:conf/icde/JinY0YQP21,
	author = {Xin Jin and
                  Zhengyi Yang and
                  Xuemin Lin and
                  Shiyu Yang and
                  Lu Qin and
                  You Peng},
	title = {{FAST:} FPGA-based Subgraph Matching on Massive Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1452--1463},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00129},
	doi = {10.1109/ICDE51399.2021.00129},
	timestamp = {Mon, 26 Jun 2023 20:41:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/JinY0YQP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph matching is a basic operation widely used in many applications. However, due to its NP-hardness and the explosive growth of graph data, it is challenging to compute subgraph matching, especially in large graphs. In this paper, we aim at scaling up subgraph matching on a single machine using FPGAs. Specifically, we propose a CPU-FPGA co-designed framework. On the CPU side, we first develop a novel auxiliary data structure called candidate search tree (CST) which serves as a complete search space of subgraph matching. CST can be partitioned and fully loaded into FPGAs’ on-chip memory. Then, a workload estimation technique is proposed to balance the load between the CPU and FPGA. On the FPGA side, we design and implement the first FPGA-based subgraph matching algorithm, called FAST. To take full advantage of the pipeline mechanism on FPGAs, task parallelism optimization and task generator separation strategy are proposed for FAST, achieving massive parallelism. Moreover, we carefully develop a BRAM-only matching process to fully utilize FPGA’s on-chip memory, which avoids the expensive intermediate data transfer between FPGA’s BRAM and DRAM. Comprehensive experiments show that FAST achieves up to 462.0x and 150.0x speedup compared with the state-of-the-art algorithm DAF and CECI, respectively. In addition, FAST is the only algorithm that can handle the billion-scale graph using one machine in our experiments.}
}


@inproceedings{DBLP:conf/icde/MhedhbiGKS21,
	author = {Amine Mhedhbi and
                  Pranjal Gupta and
                  Shahid Khaliq and
                  Semih Salihoglu},
	title = {{A+} Indexes: Tunable and Space-Efficient Adjacency Lists in Graph
                  Database Management Systems},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1464--1475},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00130},
	doi = {10.1109/ICDE51399.2021.00130},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MhedhbiGKS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph database management systems (GDBMSs) are highly optimized to perform fast traversals, i.e., joins of vertices with their neighbours, by indexing the neighbourhoods of vertices in adjacency lists. However, existing GDBMSs have system-specific and fixed adjacency list structures, which makes each system efficient on only a fixed set of workloads. We describe a new tunable indexing subsystem for GDBMSs, we call A+ indexes, with materialized view support. The subsystem consists of two types of indexes: (i) vertex-partitioned indexes that partition 1-hop materialized views into adjacency lists on either the source or destination vertex IDs; and (ii) edge-partitioned indexes that partition 2-hop views into adjacency lists on one of the edge IDs. As in existing GDBMSs, a system by default requires one forward and one backward vertex-partitioned index, which we call the primary A+ index. Users can tune the primary index or secondary indexes by adding nested partitioning and sorting criteria. Our secondary indexes are space-efficient and use a technique we call offset lists. Our indexing subsystem allows a wider range of applications to benefit from GDBMSs’ fast join capabilities. We demonstrate the tunability and space efficiency of A+ indexes through extensive experiments on three workloads.}
}


@inproceedings{DBLP:conf/icde/SongLMW21,
	author = {Qi Song and
                  Peng Lin and
                  Hanchao Ma and
                  Yinghui Wu},
	title = {Explaining Missing Data in Graphs: {A} Constraint-based Approach},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1476--1487},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00131},
	doi = {10.1109/ICDE51399.2021.00131},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SongLMW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces a constraint-based approach to clarify missing values in graphs. Our method capitalizes on a set Σ of graph data constraints. An explanation is a sequence of operational enforcement of Σ towards the recovery of interested yet missing data (e.g., attribute values, edges). We show that constraint-based approach helps us to understand not only why a value is missing, but also how to recover the missing value. We study Σ-explanation problem, which is to compute the optimal explanations with guarantees on the informativeness and conciseness. We show the problem is in\nΔ\nP\n2\nfor established graph data constraints such as graph keys and graph association rules. We develop an efficient bidirectional algorithm to compute optimal explanations, without enforcing Σ on the entire graph. We also show our algorithm can be easily extended to support graph refinement within limited time, and to explain missing answers. Using real-world graphs, we experimentally verify the effectiveness and efficiency of our algorithms.}
}


@inproceedings{DBLP:conf/icde/TengSTYLC21,
	author = {Ya{-}Wen Teng and
                  Yishuo Shi and
                  Chih{-}Hua Tai and
                  De{-}Nian Yang and
                  Wang{-}Chien Lee and
                  Ming{-}Syan Chen},
	title = {Influence Maximization Based on Dynamic Personal Perception in Knowledge
                  Graph},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1488--1499},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00132},
	doi = {10.1109/ICDE51399.2021.00132},
	timestamp = {Sat, 09 Apr 2022 12:45:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/TengSTYLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Viral marketing on social networks, also known as Influence Maximization (IM), aims to select k users for the promotion of a target item by maximizing the total spread of their influence. However, most previous works on IM do not explore the dynamic user perception of promoted items in the process. In this paper, by exploiting the knowledge graph (KG) to capture dynamic user perception, we formulate the problem of Influence Maximization based on Dynamic Personal Perception (IMDPP) that considers user preferences and social influence reflecting the impact of relevant item adoptions. We prove the hardness of IMDPP and design an approximation algorithm, named Dynamic perception for seeding in target markets (Dysim), by exploring the concepts of dynamic reachability, target markets, and substantial influence to select and promote a sequence of relevant items. We evaluate the performance of Dysim in comparison with the state-of-the-art approaches using real social networks with real KGs. The experimental results show that Dysim effectively achieves at least 6 times of influence spread in large datasets over the state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/icde/XuJCXB21,
	author = {Lyu Xu and
                  Jiaxin Jiang and
                  Byron Choi and
                  Jianliang Xu and
                  Sourav S. Bhowmick},
	title = {Privacy Preserving Strong Simulation Queries on Large Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1500--1511},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00133},
	doi = {10.1109/ICDE51399.2021.00133},
	timestamp = {Tue, 07 May 2024 20:05:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XuJCXB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies privacy preserving query services for strong simulation queries in the database outsourcing paradigm. In such a paradigm, clients send their queries to a third-party service provider (SP), who has the outsourced large graph data, and the SP computes the query answers. However, as SP may not always be trusted, the sensitive information of the clients’ queries, importantly, the query structures, should be protected. Moreover, graph pattern queries often have high complexities, whereas data graphs can be large. This paper adopts strong simulation as a practical query semantic for this paradigm. Under this semantic, queries are matched with a notion of balls, which are subgraphs related to the query diameter. We transform the core of the existing strong simulation algorithm using data-oblivious operations (ObSSA) and propose its secure version. We show that the algorithm may encounter an overflow problem even partially homomorphic encryption (PHE) has been used. We then propose an efficient inexact algorithm EncSSA, which is secure under chosen plaintext attack (CPA). The results of privacy analysis are presented. We have conducted experiments on Twitter and Citeseer datasets, and the results show that EncSSA is both efficient and effective.}
}


@inproceedings{DBLP:conf/icde/ParkX021,
	author = {Himchan Park and
                  Jinjun Xiong and
                  Min{-}Soo Kim},
	title = {Trillion-scale Graph Processing Simulation based on Top-Down Graph
                  Upscaling},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1512--1523},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00134},
	doi = {10.1109/ICDE51399.2021.00134},
	timestamp = {Tue, 21 Mar 2023 20:50:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ParkX021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the number of graph applications increases rapidly in many domains, new graph algorithms (or queries) have become more important than ever before. The current two-step approach to develop and test a graph algorithm is very expensive for trillion-scale graphs required in many industrial applications. In this paper, we propose a concept of graph processing simulation, a single-step approach that generates a graph and processes a graph algorithm simultaneously. It consists of a top-down graph upscaling method called V-Upscaler and a graph processing simulation method following the vertex-centric GAS model called T-GPS. Users can develop a graph algorithm and check its correctness and performance conveniently and cost-efficiently even for trillion-scale graphs. Through extensive experiments, we have demonstrated that our single-step approach of V-Upscaler and T-GPS significantly outperforms the conventional two-step approach, although ours uses only a single machine, while the conventional one uses a cluster of eleven machines.}
}


@inproceedings{DBLP:conf/icde/TanYW0Z21,
	author = {Yanchao Tan and
                  Carl Yang and
                  Xiangyu Wei and
                  Yun Ma and
                  Xiaolin Zheng},
	title = {Multi-Facet Recommender Networks with Spherical Optimization},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1524--1535},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00135},
	doi = {10.1109/ICDE51399.2021.00135},
	timestamp = {Thu, 15 Feb 2024 16:22:21 +0100},
	biburl = {https://dblp.org/rec/conf/icde/TanYW0Z21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Implicit feedback is widely explored by modern recommender systems. Since the feedback is often sparse and imbalanced, it poses great challenges to the learning of complex interactions among users and items. Metric learning has been proposed to capture user-item interactions from implicit feedback, but existing methods only represent users and items in a single metric space, ignoring the fact that users can have multiple preferences and items can have multiple properties, which leads to potential conflicts limiting their performance in recommendation. To capture the multiple facets of user preferences and item properties while resolving their potential conflicts, we propose the novel framework of Multi-fAcet Recommender networks with Spherical optimization (MARS). By designing a cross-facet similarity measurement, we project users and items into multiple metric spaces for fine-grained representation learning, and compare them only in the proper spaces. Furthermore, we devise a spherical optimization strategy to enhance the effectiveness and robustness of the multi-facet recommendation framework. Extensive experiments on six real-world benchmark datasets show drastic performance gains brought by MARS, which constantly achieves up to 40% improvements over the state-of-the-art baselines regarding both HR and nDCG metrics. 1}
}


@inproceedings{DBLP:conf/icde/ZhangGJ021,
	author = {Jun Zhang and
                  Chen Gao and
                  Depeng Jin and
                  Yong Li},
	title = {Group-Buying Recommendation for Social E-Commerce},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1536--1547},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00136},
	doi = {10.1109/ICDE51399.2021.00136},
	timestamp = {Sat, 03 Aug 2024 16:25:26 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangGJ021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Group buying, as an emerging form of purchase in social e-commerce websites, such as Pinduoduo 1 , has recently achieved great success. In this new business model, users, initiator, can launch a group and share products to their social networks, and when there are enough friends, participants, join it, the deal is clinched. Group-buying recommendation for social e-commerce, which recommends an item list when users want to launch a group, plays an important role in the group success ratio and sales. However, designing a personalized recommendation model for group buying is an entirely new problem that is seldom explored. In this work, we take the first step to approach the problem of group-buying recommendation for social e-commerce and develop a GBGCN method (short for Group-Buying Graph Convolutional Network). Considering there are multiple types of behaviors (launch and join) and structured social network data, we first propose to construct directed heterogeneous graphs to represent behavioral data and social networks. We then develop a graph convolutional network model with multi-view embedding propagation, which can extract the complicated high-order graph structure to learn the embeddings. Last, since a failed group-buying implies rich preferences of the initiator and participants, we design a double-pairwise loss function to distill such preference signals. We collect a real-world dataset of group-buying and conduct experiments to evaluate the performance. Empirical results demonstrate that our proposed GBGCN can significantly outperform baseline methods by 2.69%-7.36%. The codes and the dataset are released at https://github.com/Sweetnow/group-buying-recommendation.}
}


@inproceedings{DBLP:conf/icde/LyuY0LLD21,
	author = {Yanzhang Lyu and
                  Hongzhi Yin and
                  Jun Liu and
                  Mengyue Liu and
                  Huan Liu and
                  Shizhuo Deng},
	title = {Reliable Recommendation with Review-level Explanations},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1548--1558},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00137},
	doi = {10.1109/ICDE51399.2021.00137},
	timestamp = {Mon, 05 Dec 2022 16:43:34 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LyuY0LLD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The quality of user-generated reviews is significant for users to understand recommendation results and make online purchasing decisions correctly. However, the reliability of a review, which captures the likelihood that a review is benign, is ignored by many studies. The low reliability reviews cause a recommendation system’s unsatisfying performance. Especially the fake reviews written by fraudulent users mislead the system into generating error recommendation results and explanations, which confuse customers and deprive customers of confidence in the system. In this paper, we propose a model, Reliable Recommendation with Review-level Explanations (RRRE), which detects reliable reviews and improves the performance of the explainable recommendation system as well. Recognizing the textual content of reviews, user-item interactions are valuable features for both rating prediction and reliability prediction. RRRE builds a uniform framework to predict rating scores and reliability scores simultaneously. Firstly, RRRE embeds user preferences and item profiles, which are extracted from textual and interactive features, into the representation of the review. Secondly, the supervised information of two subtasks is jointly combined. It makes the optimization of RRRE faster and better. Finally, the reviews with both high reliability scores and rating scores are given to customers as reliable explanations. To the best of our knowledge, we are the first to consider the reliability of reviews for improving explainable recommender system. And the experimental results confirm this idea and show that our model outperforms other baseline methods on Yelp and Amazon datasets.}
}


@inproceedings{DBLP:conf/icde/ZhaoZ0LS021,
	author = {Jing Zhao and
                  Pengpeng Zhao and
                  Lei Zhao and
                  Yanchi Liu and
                  Victor S. Sheng and
                  Xiaofang Zhou},
	title = {Variational Self-attention Network for Sequential Recommendation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1559--1570},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00138},
	doi = {10.1109/ICDE51399.2021.00138},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoZ0LS021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation has become an attractive topic in recommender systems. Existing sequential recommendation methods, including the methods based on the state-of-the-art self-attention mechanism, usually employ deterministic neural networks to represent user preferences as fixed-points in the latent feature spaces. However, the fixed-point vector lacks the ability to capture the uncertainty and dynamics of user preferences that are prevalent in recommender systems. In this paper, we propose a new Variational Self-Attention Network (VSAN), which introduces a variational autoencoder (VAE) into the self-attention network to capture latent user preferences. Specifically, we represent the obtained self-attention vector as density via variational inference, whose variance well characterizes the uncertainty of user preferences. Furthermore, we employ self-attention networks to learn the inference process and generative process of VAE, which well captures long-range and local dependencies. Finally, we evaluate our proposed method VSAN with two public real-world datasets. Our experimental results show the effectiveness of our model compared to the state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/icde/DengLLAS21,
	author = {Zhiyi Deng and
                  Changyu Li and
                  Shujin Liu and
                  Waqar Ali and
                  Jie Shao},
	title = {Knowledge-Aware Group Representation Learning for Group Recommendation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1571--1582},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00139},
	doi = {10.1109/ICDE51399.2021.00139},
	timestamp = {Tue, 28 Mar 2023 16:27:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DengLLAS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, going out and participating in group activities is an indispensable part of human life, and group recommendation systems are needed to provide suggestions. In practice, group recommendation faces serious sparsity issues due to the lack of group-item interaction data, and the key challenge is to aggregate group member preference for group decision making. Conventional group recommendations applied a predefined strategy to aggregate the preferences of group members, which cannot model the group decision making process and do not address the data sparsity problem well. In this paper, we introduce knowledge graph into group recommendation as side information, and propose a novel end-to-end method named knowledge graph-based attentive group recommendation (KGAG) to solve the data sparsity and preference aggregation problems. Specifically, a graph convolution network (GCN) is employed to capture abundant structure information of items and users in knowledge graph to overcome the sparsity problem. Besides, to learn knowledge-aware group representation for inferring the group decision better, we capture the user-item connectivity and user-user connectivity in knowledge graph, and then adopt attention mechanism to learn the influence of each member according to user-user interaction in group and the candidate item, for member preference aggregation. Additionally, the attention mechanism can provide interpretability to group recommendation. Moreover, we extend the margin loss to our KGAG which forces the prediction score of positive item to be a distance larger than that of negative item. Experimental results show the superiority of the proposed KGAG and verify the efficacy of each component of KGAG.}
}


@inproceedings{DBLP:conf/icde/FanDZ0LWT021,
	author = {Wenqi Fan and
                  Tyler Derr and
                  Xiangyu Zhao and
                  Yao Ma and
                  Hui Liu and
                  Jianping Wang and
                  Jiliang Tang and
                  Qing Li},
	title = {Attacking Black-box Recommendations via Copying Cross-domain User
                  Profiles},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1583--1594},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00140},
	doi = {10.1109/ICDE51399.2021.00140},
	timestamp = {Sun, 04 Aug 2024 19:37:46 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FanDZ0LWT021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems, which aim to suggest personalized lists of items for users, have drawn a lot of attention. In fact, many of these state-of-the-art recommender systems have been built on deep neural networks (DNNs). Recent studies have shown that these deep neural networks are vulnerable to attacks, such as data poisoning, which generate fake users to promote a selected set of items. Correspondingly, effective defense strategies have been developed to detect these generated users with fake profiles. Thus, new strategies of creating more ‘realistic’ user profiles to promote a set of items should be investigated to further understand the vulnerability of DNNs based recommender systems. In this work, we present a novel framework CopyAttack. It is a reinforcement learning based black-box attacking method that harnesses real users from a source domain by copying their profiles into the target domain with the goal of promoting a subset of items. CopyAttack is constructed to both efficiently and effectively learn policy gradient networks that first select, then further refine/craft user profiles from the source domain, and ultimately copy them into the target domain. CopyAttack’s goal is to maximize the hit ratio of the targeted items in the Top-k recommendation list of the users in the target domain. We conducted experiments on two real-world datasets and empirically verified the effectiveness of the proposed framework. The implementation of CopyAttack is available at https://github.com/wenqifan03/CopyAttack.}
}


@inproceedings{DBLP:conf/icde/ShekelyanDGG21,
	author = {Michael Shekelyan and
                  Anton Dign{\"{o}}s and
                  Johann Gamper and
                  Minos N. Garofalakis},
	title = {Approximating Multidimensional Range Counts with Maximum Error Guarantees},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1595--1606},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00141},
	doi = {10.1109/ICDE51399.2021.00141},
	timestamp = {Mon, 03 Jan 2022 22:33:30 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ShekelyanDGG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We address the problem of compactly approximating multidimensional range counts with a guaranteed maximum error and propose a novel histogram-based summary structure, termed SliceHist. The key idea is to operate a grid histogram in an approximately rank-transformed space, where the data points are more uniformly distributed and each grid slice contains only a small number of points. Then, the points of each slice are summarised again using the same technique. As each query box partially intersects only few slices and each grid slice has few data points, the summary is able to achieve tight error guarantees. In experiments and through analysis of non-asymptotic formulas we show that SliceHist is not only competitive with existing heuristics in terms of performance, but additionally offers tight error guarantees.}
}


@inproceedings{DBLP:conf/icde/Patil021,
	author = {Mayur Patil and
                  Amr Magdy},
	title = {{LATEST:} Learning-Assisted Selectivity Estimation Over Spatio-Textual
                  Streams},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1607--1618},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00142},
	doi = {10.1109/ICDE51399.2021.00142},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Patil021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Selectivity and cardinality estimation are main driving factors for developing cheap query plans and ultimately faster query processing. Traditionally, database systems use estimation data structures, e.g., histograms, to maintain data summaries. Machine learning models have recently been employed, acting as black boxes, in several database tasks, including cardinality estimation. In the dynamic streaming environments, both estimation data structures and machine learning models struggle with adaptation for dynamic changes in data and query workloads. This paper proposes LATEST; a system module that uses machine learning to enable dynamic adaptation of estimation data structures. For spatial-keyword queries in a streaming environment, it shows on par or better performance than the state-of-the-art estimators. LATEST builds an incremental supervised learning model over a moving time window that helps the underlying system to switch among several estimation structures to keep estimation accuracy high at all times. As an incremental learner, LATEST effectively adapts to dynamic changes of both data and queries in streaming environments. Our extensive experiments on three real datasets and various query workloads verify the effectiveness of LATEST with higher accuracy and lower response times over the state-of-the-art estimators.}
}


@inproceedings{DBLP:conf/icde/SongGZ021,
	author = {Yang Song and
                  Yu Gu and
                  Rui Zhang and
                  Ge Yu},
	title = {ProMIPS: Efficient High-Dimensional c-Approximate Maximum Inner Product
                  Search with a Lightweight Index},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1619--1630},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00143},
	doi = {10.1109/ICDE51399.2021.00143},
	timestamp = {Tue, 08 Aug 2023 15:56:12 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SongGZ021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the wide applications in recommendation systems, multi-class label prediction and deep learning, the Maximum Inner Product (MIP) search problem has received extensive attention in recent years. Faced with large-scale datasets containing high-dimensional feature vectors, the state-of-the-art LSH-based methods usually require a large number of hash tables or long hash codes to ensure the searching quality, which takes up lots of index space and causes excessive disk page accesses. In this paper, we relax the guarantee of accuracy for efficiency and propose an efficient method for c-Approximate Maximum Inner Product (c-AMIP) search with a lightweight iDistance index. We project high-dimensional points to low-dimensional ones via 2-stable random projections and derive probability-guaranteed searching conditions, by which the c-AMIP results can be guaranteed in accuracy with arbitrary probabilities. To further improve the efficiency, we propose Quick-Probe for quickly determining the searching bound satisfying the derived condition in advance, avoiding the inefficient incremental searching process. Extensive experimental evaluations on four real datasets demonstrate that our method requires less pre-processing cost including index size and pre-processing time. In addition, compared to the state-of-the-art benchmark methods, it provides superior results on searching quality in terms of overall ratio and recall, and efficiency in terms of page access and running time.}
}


@inproceedings{DBLP:conf/icde/00010WT21,
	author = {Yanhao Wang and
                  Yuchen Li and
                  Raymond Chi{-}Wing Wong and
                  Kian{-}Lee Tan},
	title = {A Fully Dynamic Algorithm for k-Regret Minimizing Sets},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1631--1642},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00144},
	doi = {10.1109/ICDE51399.2021.00144},
	timestamp = {Mon, 03 Jan 2022 22:33:30 +0100},
	biburl = {https://dblp.org/rec/conf/icde/00010WT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Selecting a small set of representatives from a large database is important in many applications such as multi-criteria decision making, web search, and recommendation. The k-regret minimizing set (k-RMS) problem was recently proposed for representative tuple discovery. Specifically, for a large database P of tuples with multiple numerical attributes, the k-RMS problem returns a size-r subset Q of P such that, for any possible ranking function, the score of the top-ranked tuple in Q is not much worse than the score of the k th -ranked tuple in P. Although the k-RMS problem has been extensively studied in the literature, existing methods are designed for the static setting and cannot maintain the result efficiently when the database is updated. To address this issue, we propose the first fully-dynamic algorithm for the k-RMS problem that can efficiently provide the up-to-date result w.r.t. any tuple insertion and deletion in the database with a provable guarantee. Experimental results on several real-world and synthetic datasets demonstrate that our algorithm runs up to four orders of magnitude faster than existing k-RMS algorithms while providing results of nearly equal quality.}
}


@inproceedings{DBLP:conf/icde/ZhaoDDTCC21,
	author = {Kai Zhao and
                  Sheng Di and
                  Maxim Dmitriev and
                  Thierry{-}Laurent D. Tonellot and
                  Zizhong Chen and
                  Franck Cappello},
	title = {Optimizing Error-Bounded Lossy Compression for Scientific Data by
                  Dynamic Spline Interpolation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1643--1654},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00145},
	doi = {10.1109/ICDE51399.2021.00145},
	timestamp = {Wed, 07 Dec 2022 23:09:59 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoDDTCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today’s scientific simulations are producing vast volumes of data that cannot be stored and transferred efficiently because of limited storage capacity, parallel I/O bandwidth, and network bandwidth. The situation is getting worse over time because of the ever-increasing gap between relatively slow data transfer speed and fast-growing computation power in modern supercomputers. Error-bounded lossy compression is becoming one of the most critical techniques for resolving the big scientific data issue, in that it can significantly reduce the scientific data volume while guaranteeing that the reconstructed data is valid for users because of its compression-error-bounding feature. In this paper, we present a novel error-bounded lossy compressor based on a state-of-the-art prediction-based compression framework. Our solution exhibits substantially better compression quality than all of the existing error-bounded lossy compressors, with comparable compression speed. Specifically, our contribution is threefold. (1) We provide an in-depth analysis of why the best-existing prediction-based lossy compressor can only minimally improve the compression quality. (2) We propose a dynamic spline interpolation approach with a series of optimization strategies that can significantly improve the data prediction accuracy, substantially improving the compression quality in turn. (3) We perform a thorough evaluation using six real-world scientific simulation datasets across different science domains to evaluate our solution vs. all other related works. Experiments show that the compression ratio of our solution is higher than that of the second-best lossy compressor by 20% 460% with the same error bound in most of the cases. ∼}
}


@inproceedings{DBLP:conf/icde/LuoYZZZCFLNO21,
	author = {Zhaojing Luo and
                  Sai Ho Yeung and
                  Meihui Zhang and
                  Kaiping Zheng and
                  Lei Zhu and
                  Gang Chen and
                  Feiyi Fan and
                  Qian Lin and
                  Kee Yuan Ngiam and
                  Beng Chin Ooi},
	title = {MLCask: Efficient Management of Component Evolution in Collaborative
                  Data Analytics Pipelines},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1655--1666},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00146},
	doi = {10.1109/ICDE51399.2021.00146},
	timestamp = {Wed, 10 Apr 2024 16:20:14 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuoYZZZCFLNO21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the ever-increasing adoption of machine learning for data analytics, maintaining a machine learning pipeline is becoming more complex as both the datasets and trained models evolve with time. In a collaborative environment, the changes and updates due to pipeline evolution often cause cumbersome coordination and maintenance work, raising the costs and making it hard to use. Existing solutions, unfortunately, do not address the version evolution problem, especially in a collaborative environment where non-linear version control semantics are necessary to isolate operations made by different user roles. The lack of version control semantics also incurs unnecessary storage consumption and lowers efficiency due to data duplication and repeated data pre-processing, which are avoidable.In this paper, we identify two main challenges that arise during the deployment of machine learning pipelines, and address them with the design of versioning for an end-to-end analytics system MLCask. The system supports multiple user roles with the ability to perform Git-like branching and merging operations in the context of the machine learning pipelines. We define and accelerate the metric-driven merge operation by pruning the pipeline search tree using reusable history records and pipeline compatibility information. Further, we design and implement the prioritized pipeline search, which gives preference to the pipelines that probably yield better performance. The effectiveness of MLCask is evaluated through an extensive study over several real-world deployment cases. The performance evaluation shows that the proposed merge operation is up to 7.8x faster and saves up to 11.9x storage space than the baseline method that does not utilize history records.}
}


@inproceedings{DBLP:conf/icde/GuyMNY21,
	author = {Ido Guy and
                  Tova Milo and
                  Slava Novgorodov and
                  Brit Youngmann},
	title = {Improving Constrained Search Results By Data Melioration},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1667--1678},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00147},
	doi = {10.1109/ICDE51399.2021.00147},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GuyMNY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of finding an item-set of maximal aggregated utility that satisfies a set of constraints is at the cornerstone of many search applications. Its classical definition assumes that all the information needed to verify the constraints is explicitly given. However, in real-world databases, the data available on items is often partial. Hence, adequately answering constrained search queries requires the completion of this missing information. A common approach to complete missing data is to employ Machine Learning (ML)-based inference. However, such methods are naturally error-prone. More accurate data can be obtained by asking humans to complete missing information. But, as the number of items in the repository is vast, limiting human effort is crucial. To this end, we introduce the Probabilistic Constrained Search (PCS) problem, which identifies a bounded-size item-set whose data completion is likely to be highly beneficial, as these items are expected to belong to the result set of the constrained search queries in question. We prove PCS to be hard to approximate, and consequently propose a best-effort PTIME heuristic to solve it. We demonstrate the effectiveness and efficiency of our algorithm over real-world datasets and scenarios, showing that our algorithm significantly improves the result sets of constrained search queries, in terms of both utility and constraints satisfaction probability.}
}


@inproceedings{DBLP:conf/icde/ZhangPZZSM021,
	author = {Feng Zhang and
                  Zaifeng Pan and
                  Yanliang Zhou and
                  Jidong Zhai and
                  Xipeng Shen and
                  Onur Mutlu and
                  Xiaoyong Du},
	title = {{G-TADOC:} Enabling Efficient GPU-Based Text Analytics without Decompression},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1679--1690},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00148},
	doi = {10.1109/ICDE51399.2021.00148},
	timestamp = {Mon, 03 Jan 2022 22:33:29 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhangPZZSM021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Text analytics directly on compression (TADOC) has proven to be a promising technology for big data analytics. GPUs are extremely popular accelerators for data analytics systems. Unfortunately, no work so far shows how to utilize GPUs to accelerate TADOC. We describe G-TADOC, the first framework that provides GPU-based text analytics directly on compression, effectively enabling efficient text analytics on GPUs without decompressing the input data.G-TADOC solves three major challenges. First, TADOC involves a large amount of dependencies, which makes it difficult to exploit massive parallelism on a GPU. We develop a novel fine-grained thread-level workload scheduling strategy for GPU threads, which partitions heavily-dependent loads adaptively in a fine-grained manner. Second, in developing G-TADOC, thousands of GPU threads writing to the same result buffer leads to inconsistency while directly using locks and atomic operations lead to large synchronization overheads. We develop a memory pool with thread-safe data structures on GPUs to handle such difficulties. Third, maintaining the sequence information among words is essential for lossless compression. We design a sequence-support strategy, which maintains high GPU parallelism while ensuring sequence information.Our experimental evaluations show that G-TADOC provides 31.1× average speedup compared to state-of-the-art TADOC.}
}


@inproceedings{DBLP:conf/icde/FujiwaraIKKU21,
	author = {Yasuhiro Fujiwara and
                  Yasutoshi Ida and
                  Sekitoshi Kanai and
                  Atsutoshi Kumagai and
                  Naonori Ueda},
	title = {Fast Similarity Computation for t-SNE},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1691--1702},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00149},
	doi = {10.1109/ICDE51399.2021.00149},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FujiwaraIKKU21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data visualization has become a fundamental process of data engineering. t-SNE is one of the most popular data visualization approaches. However, its computation cost is quadratic to the number of data points because it needs to compute similarities for all pairs of data points. One practical way of using t-SNE is random walk-based t-SNE. This approach visualizes user-specified landmark points from the similarities between them based on random walks in a neighborhood graph of data points. It offers two approaches to computing similarities: the direct and analytical approaches. The direct approach approximately computes similarities by explicitly computing random walks in the graph. Unfortunately, it needs to perform numerous random walks for adequate computation accuracy. The analytical approach performs Cholesky factorization on the graph Laplacian and computes exact similarities using the decomposed graph Laplacian. This, however, incurs high computation cost in performing Cholesky factorization. Our proposal, F-tSNE, reduces the computation cost of random walk-based t-SNE by computing the LDL decomposition for the graph Laplacian based on two ideas: (1) reducing non-zero elements in the LDL decomposition by using a reordering matrix and (2) exploiting the sparse structure of the graph when computing the similarities. Theoretically, our approach is guaranteed to yield exact similarities. Experiments show that it is up to 88.4 times faster than the existing alternatives.}
}


@inproceedings{DBLP:conf/icde/MackeADPR21,
	author = {Stephen Macke and
                  Maryam Aliakbarpour and
                  Ilias Diakonikolas and
                  Aditya G. Parameswaran and
                  Ronitt Rubinfeld},
	title = {Rapid Approximate Aggregation with Distribution-Sensitive Interval
                  Guarantees},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1703--1714},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00150},
	doi = {10.1109/ICDE51399.2021.00150},
	timestamp = {Sat, 30 Sep 2023 09:44:51 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MackeADPR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aggregating data is fundamental to data analytics, data exploration, and OLAP. Approximate query processing (AQP) techniques are often used to accelerate computation of aggregates using samples, for which confidence intervals (CIs) are widely used to quantify the associated error. CIs used in practice fall into two categories: techniques that are tight but not correct, i.e., they yield tight intervals but only offer asymptoticguarantees,makingthem unreliable, or techniques that are correct but not tight, i.e., they offer rigorous guarantees, but are overly conservative, leading to confidence intervals that are too loose to be useful. In this paper, we develop a CI technique that is both correct and tighter than traditional approaches. Starting from conservative CIs, we identify two issues they often face: pessimistic mass allocation (PMA) and phantom outlier sensitivity (PHOS). By developing a novel range-trimming technique for eliminating PHOS and pairing it with known CI techniques without PMA, we develop a technique for computing CIs with strong guarantees that requires fewer samples for the same width. We implement our techniques underneath a sampling-optimized in-memory column store and show how they accelerate queries involving aggregates on real datasets with typical speedups on the order of 10× over both traditional AQP-with-guarantees and exact methods, all while obeying accuracy constraints.}
}


@inproceedings{DBLP:conf/icde/TrummerA21,
	author = {Immanuel Trummer and
                  Connor Anderson},
	title = {Optimally Summarizing Data by Small Fact Sets for Concise Answers
                  to Voice Queries},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1715--1726},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00151},
	doi = {10.1109/ICDE51399.2021.00151},
	timestamp = {Wed, 30 Nov 2022 07:34:27 +0100},
	biburl = {https://dblp.org/rec/conf/icde/TrummerA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Our goal is to find combinations of facts that optimally summarize data sets. We consider this problem in the context of voice query interfaces for simple, exploratory data analysis. Here, the system answers voice queries with a short summary of relevant data. Finding optimal voice data summaries is computationally expensive. Prior work in this domain has exploited sampling and incremental processing. Instead, we rely on a pre-processing stage generating summaries of data subsets in a batch operation. This step reduces run time overheads by orders of magnitude.We present multiple algorithms for the pre-processing stage, realizing different tradeoffs between optimality and data processing overheads. We analyze our algorithms formally and compare them experimentally with prior methods for generating voice data summaries. We report on multiple user studies with a prototype system implementing our approach. Furthermore, we report on insights gained from a public deployment of our system on the Google Assistant Platform.}
}


@inproceedings{DBLP:conf/icde/DaiZ021,
	author = {Yimeng Dai and
                  Rui Zhang and
                  Jianzhong Qi},
	title = {Automatic Webpage Briefing},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1727--1738},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00152},
	doi = {10.1109/ICDE51399.2021.00152},
	timestamp = {Thu, 23 Jun 2022 19:55:59 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DaiZ021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce the task of webpage briefing (WB) to provide a summary of a webpage in a hierarchical manner, from the broad topic of the webpage, to finer level key attributes. A straightforward approach for this task is to train a machine learning model for generating topics and extracting key attributes. However, such a model may not perform well on webpages that are from domains not seen in the training data. An ideal model should be able to adapt to unseen domains while preserving knowledge learned from the seen domains. Knowledge distillation (KD) offers a potential solution, in which a teacher pre-trained with specific domains can pass the knowledge to a student, while unseen domains can also be added to increase the robustness of the models. However, existing works usually assume the models have no access to seen domains during distillation and the knowledge on seen domains may be lost. In our setting, we have access to the generated topics, which contain representative knowledge of seen domains and can help preserve that knowledge during distillation. Moreover, a vanilla KD does not pass on the knowledge about the location patterns of the informative contents in webpages, which are essential for identifying the topics to be generated or the key attributes to be extracted. To preserve more knowledge of seen domains and to better utilize the location patterns, we propose a Dual Distillation model which consists of identification distillation (ID) and understanding distillation (UD); ID distills knowledge on the identification of informative contents under the guidance of the learned topics of seen domains, while UD distills knowledge on topic generation or key attribute extraction. Since topics and key attributes are distilled separately in two students in Dual Distillation, the inherent correlations between them are not utilized. To better exploit such correlations, we propose a Triple Distillation model which consists of a shared ID and two UDs, one for topic generation and the other for key attribute extraction. We further propose a joint model for WB with signal enhancement and exchange among a key attribute extractor, a topic generator, and an informative section predictor. Experiments on real-world webpages show that our models achieve high performances for WB, and validate the superiority of Dual Distillation and Triple Distillation in their target settings. Experiments also show that the proposed joint model outperforms single-task baselines and other joint models.}
}


@inproceedings{DBLP:conf/icde/CirsteaKG0P21,
	author = {Razvan{-}Gabriel Cirstea and
                  Tung Kieu and
                  Chenjuan Guo and
                  Bin Yang and
                  Sinno Jialin Pan},
	title = {EnhanceNet: Plugin Neural Networks for Enhancing Correlated Time Series
                  Forecasting},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1739--1750},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00153},
	doi = {10.1109/ICDE51399.2021.00153},
	timestamp = {Thu, 23 Jun 2022 19:56:00 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CirsteaKG0P21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Correlated time series forecasting plays an essential role in many cyber-physical systems, where entities interact with each other over time. To enable accurate forecasting, it is essential to capture both the temporal dynamics and the correlations among different entities. To capture the former, two popular types of models, recurrent neural networks (RNNs) and temporal convolution networks (TCNs), are employed. To capture the latter, a graph is constructed to reflect certain relationships among entities and then graph convolution (GC) is applied upon the graph to capture the correlations among the entities. The state-of-the-art forecasting accuracy is achieved by models that combine RNNs or TCNs with GC. However, they neither capture distinct temporal dynamics that exist among different entities nor consider the entity correlations that evolve across time.In this paper, rather than proposing yet another new end-to-end forecasting model, we aim at providing a framework to enhance existing forecasting models, where we propose generic plugins that can be easily integrated into existing solutions to solve the two challenges and thus further enhance their accuracy. Specifically, we propose two plugin neural networks that are able to better capture distinct temporal dynamics for different entities and dynamic entity correlations across time, so that forecasting accuracy is improved while model parameters to be learned are reduced. Experimental results on three real-world correlated time series data sets demonstrate that the proposed framework with the two plugin networks is able to achieve the above goals.}
}


@inproceedings{DBLP:conf/icde/WangXJ0KSS21,
	author = {Zhaonan Wang and
                  Tianqi Xia and
                  Renhe Jiang and
                  Xin Liu and
                  Kyoung{-}Sook Kim and
                  Xuan Song and
                  Ryosuke Shibasaki},
	title = {Forecasting Ambulance Demand with Profiled Human Mobility via Heterogeneous
                  Multi-Graph Neural Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1751--1762},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00154},
	doi = {10.1109/ICDE51399.2021.00154},
	timestamp = {Sat, 30 Sep 2023 09:44:51 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangXJ0KSS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Forecasting regional ambulance demand plays a fundamental part in dynamic fleet allocation and redeployment. This topic has been gaining increasing significance, as virtually every country is experiencing an aging population, with generally higher level of vulnerability and demand for the emergency medical service (EMS). Although exploring the spatial and temporal correlations in EMS historical records, the existing methods principally consider the former time-invariant, which does not necessarily hold in reality. Moreover, this assumption ignores the fact that the behind-the-scenes dynamics are people, whose demographic profiles and activity patterns could be determinants of regional EMS demands. In this paper, we are therefore motivated to mine the collective daily routines in human mobility, to further represent the evolving spatial correlations. Particularly, we model profiled mobility groups as multiple random walkers and propose a novel bicomponent neural network, including a heterogeneous multi-graph convolution layer and spatio-temporal interlacing attention module, to perform the prediction task. Experimental results on the real-world data verify the effectiveness of introducing dynamic human mobility and the advantage of our approach over the state-of-the-art models.}
}


@inproceedings{DBLP:conf/icde/LiuLZHCZ21,
	author = {Ziyi Liu and
                  Lei Li and
                  Mengxuan Zhang and
                  Wen Hua and
                  Pingfu Chao and
                  Xiaofang Zhou},
	title = {Efficient Constrained Shortest Path Query Answering with Forest Hop
                  Labeling},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1763--1774},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00155},
	doi = {10.1109/ICDE51399.2021.00155},
	timestamp = {Fri, 18 Oct 2024 15:26:38 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuLZHCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Constrained Shortest Path (CSP) problem aims to find the shortest path between two nodes in a road network subject to a given constraint on another attribute. It is typically processed as a skyline path problem on the two attributes, resulting in very high computational cost which can be prohibitive for large road networks. The main bottleneck is to deal with a large amount of partial skyline paths, which further makes the existing index-based methods incapable to obtain the complete exact skyline paths. In this paper, we propose a novel skyline path concatenation approach to avoid the expensive skyline path search, which is then used to efficiently construct a 2-hop labeling index for the CSP queries. Specifically, a rectangle-based technique is designed to prune the concatenation space from multiple hops, and a constraint pruning method is used to further speed up the CSP query processing. To further scale up to larger networks, we propose a novel forest hop labeling that constructs labels from different partitions in parallel. Our approach is the first method that can achieve both accuracy and efficiency for CSP query answering. Extensive experiments on real-life road networks demonstrate that our method outperforms the state-of-the-art CSP solutions by several orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/DaumHHMB21,
	author = {Maureen Daum and
                  Brandon Haynes and
                  Dong He and
                  Amrita Mazumdar and
                  Magdalena Balazinska},
	title = {{TASM:} {A} Tile-Based Storage Manager for Video Analytics},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1775--1786},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00156},
	doi = {10.1109/ICDE51399.2021.00156},
	timestamp = {Tue, 28 Mar 2023 08:56:16 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DaumHHMB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern video data management systems store videos as a single encoded file, which significantly limits possible storage level optimizations. We design, implement, and evaluate TASM, a new tile-based storage manager for video data. TASM uses a feature in modern video codecs called "tiles" that enables spatial random access into encoded videos. TASM physically tunes stored videos by optimizing their tile layouts given the video content and a query workload. Additionally, TASM dynamically tunes that layout in response to changes in the query workload or if the query workload and video contents are incrementally discovered. Finally, TASM also produces efficient initial tile layouts for newly ingested videos. We demonstrate that TASM can speed up subframe selection queries by an average of over 50% and up to 94%. TASM can also improve the throughput of the full scan phase of object detection queries by up to 2×.}
}


@inproceedings{DBLP:conf/icde/TsitsigkosLBMT21,
	author = {Dimitrios Tsitsigkos and
                  Konstantinos Lampropoulos and
                  Panagiotis Bouros and
                  Nikos Mamoulis and
                  Manolis Terrovitis},
	title = {A Two-layer Partitioning for Non-point Spatial Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1787--1798},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00157},
	doi = {10.1109/ICDE51399.2021.00157},
	timestamp = {Sat, 30 Sep 2023 09:44:51 +0200},
	biburl = {https://dblp.org/rec/conf/icde/TsitsigkosLBMT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-point spatial objects (e.g., polygons, linestrings, etc.) are ubiquitous and their effective management is always timely. We study the problem of indexing non-point objects in memory. We propose a secondary partitioning technique for space-oriented partitioning indices (e.g., grids), which improves their performance significantly, by avoiding the generation and elimination of duplicate results. Our approach is novel and of a high impact, as (i) it is extremely easy to implement and (ii) it can be used by any space-partitioning index. We show how our approach can be used to boost the performance of spatial range queries. We also show how we can avoid performing the expensive refinement step of a range query for the majority of objects and study the efficient processing of numerous queries in batch and in parallel. Extensive experiments on real datasets confirm the superiority of space-oriented partitioning over data-oriented partitioning and the advantage of our approach against alternative duplicate elimination techniques.}
}


@inproceedings{DBLP:conf/icde/KimKM21,
	author = {Sangchul Kim and
                  Bogyeong Kim and
                  Bongki Moon},
	title = {Spangle: {A} Distributed In-Memory Processing System for Large-Scale
                  Arrays},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1799--1810},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00158},
	doi = {10.1109/ICDE51399.2021.00158},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KimKM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With increasing volumes of scientific data, a scalable and parallel computing framework is required for scientific analysis in computer simulations and experiments. Scientific data are commonly generated in multi-dimensional arrays, and the array data model is appropriate to store them for analysis, including for data mining and arithmetic computation. In this paper, we introduce an array processing system called Spangle. It is implemented on top of Apache Spark, a popular map-reduce framework for complex computation workloads. To support array data computation, we extended Resilient Distributed Dataset (RDD) based on the array data model named ArrayRDD. ArrayRDD is an inherently parallel data structure that provides fault-tolerance. In addition, by adopting the array data model, Spangle provides an interface for expressing machine learning algorithms, which heavily rely on linear algebra. We tailored two popular algorithms, PageRank and Stochastic Gradient Descent, for large-scale datasets in Spangle.}
}


@inproceedings{DBLP:conf/icde/HalfpapS21,
	author = {Stefan Halfpap and
                  Rainer Schlosser},
	title = {Memory-Efficient Database Fragment Allocation for Robust Load Balancing
                  when Nodes Fail},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1811--1816},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00159},
	doi = {10.1109/ICDE51399.2021.00159},
	timestamp = {Wed, 07 Dec 2022 23:09:59 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HalfpapS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Load balancing queries that access the same data fragments to the same node improves caching for a memory-efficient scale-out. However, to suitably allocate fragments to multiple nodes is a highly challenging problem, particularly when nodes might fail. The problem is to find a good balance between memory efficiency and allocating enough fragments to nodes to obtain robustness through load balancing flexibility. Existing allocation approaches are either not memory-efficient or result in load imbalances, both degrading cost/performance. In this paper, we present an optimal approach and a scalable heuristic, based on three mutually supportive linear programming models, to calculate memory-efficient fragment allocations that guarantee to distribute the workload evenly - even in the case of node failures. We demonstrate the applicability and the effectiveness of our three-step approach using numerical as well as end-to-end evaluations for TPC-H and TPC-DS workloads. We find that our robust solutions clearly outperform state-of-the-art heuristics by achieving a better workload distribution with even less memory.}
}


@inproceedings{DBLP:conf/icde/LeePJCCK21,
	author = {Hyunwook Lee and
                  Cheonbok Park and
                  Seungmin Jin and
                  Hyeshin Chu and
                  Jaegul Choo and
                  Sungahn Ko},
	title = {An Empirical Experiment on Deep Learning Models for Predicting Traffic
                  Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1817--1822},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00160},
	doi = {10.1109/ICDE51399.2021.00160},
	timestamp = {Sun, 02 Oct 2022 16:04:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LeePJCCK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To tackle ever-increasing city traffic congestion problems, researchers have proposed deep learning models to aid decision-makers in the traffic control domain. Although the proposed models have been remarkably improved in recent years, there are still questions that need to be answered before deploying models. For example, it is difficult to figure out which models provide state-of-the-art performance, as recently proposed models have often been evaluated with different datasets and experiment environments. It is also difficult to determine which models would work when traffic conditions change abruptly (e.g., rush hour). In this work, we conduct two experiments to answer the two questions. In the first experiment, we conduct an experiment with the state-of-the-art models and the identical public datasets to compare model performance under a consistent experiment environment. We then extract a set of temporal regions in the datasets, whose speeds change abruptly and use these regions to explore model performance with difficult intervals. The experiment results indicate that Graph-WaveNet and GMAN show better performance in general. We also find that prediction models tend to have varying performances with data and intervals, which calls for in-depth analysis of models on difficult intervals for real-world deployment.}
}


@inproceedings{DBLP:conf/icde/0001LPS21,
	author = {Jianguo Wang and
                  Chunbin Lin and
                  Yannis Papakonstantinou and
                  Steven Swanson},
	title = {Evaluating List Intersection on SSDs for Parallel {I/O} Skipping},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1823--1828},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00161},
	doi = {10.1109/ICDE51399.2021.00161},
	timestamp = {Mon, 05 Feb 2024 20:31:12 +0100},
	biburl = {https://dblp.org/rec/conf/icde/0001LPS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {List intersection is at the core of information retrieval systems. Existing disk-based intersection algorithms were optimized for hard disk drives (HDDs) since HDDs have dominated the storage market for decades. In particular, those HDD-centric algorithms read every relevant list entirely to memory to minimize expensive random reads by performing sequential reads, although many entries in the list may be useless. Such a tradeoff makes perfect sense on HDDs, because random reads are one to two orders of magnitude slower than sequential reads. However, fast solid state drives (SSDs) have changed this landscape by improving random I/O performance dramatically. More importantly, they are manufactured with multiple flash channels to support parallel I/Os. As a result, the performance gap between random and sequential reads becomes very small on SSDs. This means that HDD-optimized intersection algorithms might not be suitable on SSDs because the total amount of data accessed is unnecessarily high.To understand the impact of SSDs to list intersection, in this work, we tune existing in-memory intersection algorithms to be SSD-aware with the idea of parallel I/O skipping, and experimentally evaluate them on synthetic and real datasets. The results provide insights on how to design efficient SSD-optimized intersection algorithms.}
}


@inproceedings{DBLP:conf/icde/SirinDA21,
	author = {Utku Sirin and
                  Sandhya Dwarkadas and
                  Anastasia Ailamaki},
	title = {Performance Characterization of {HTAP} Workloads},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1829--1834},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00162},
	doi = {10.1109/ICDE51399.2021.00162},
	timestamp = {Tue, 21 Mar 2023 20:50:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/SirinDA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hybrid Transactional and Analytical Processing (HTAP) systems have become popular in the past decade. HTAP systems allow running transactional and analytical processing workloads on the same data and hardware. As a result, they suffer from workload interference. Despite the large body of existing work in HTAP systems and architectures, none of the existing work has systematically analyzed workload interference for HTAP systems.In this work, we characterize workload interference for HTAP systems. We show that the OLTP throughput drops by up to 42% due to sharing the hardware resources. Partitioning the last-level cache (LLC) among the OLTP and OLAP workloads can significantly improve the OLTP throughput without hurting the OLAP throughput. The OLAP throughput is significantly reduced due to sharing the data. The OLAP execution time is exponentially increased if the OLTP workload generates fresh tuples faster than the HTAP system propagates them. Therefore, in order to minimize the workload interference, HTAP systems should isolate the OLTP and OLAP workloads in the shared hardware resources and should allocate enough resources to fresh tuple propagation to propagate the fresh tuples faster than they are generated.}
}


@inproceedings{DBLP:conf/icde/TaylorG21,
	author = {Colin Taylor and
                  Michael Gowanlock},
	title = {Accelerating the Yinyang K-Means Algorithm Using the {GPU}},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1835--1840},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00163},
	doi = {10.1109/ICDE51399.2021.00163},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/TaylorG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The k-means clustering algorithm is widely employed for unsupervised learning. The algorithm takes as input a multidimensional dataset of points and number of clusters/centroids, k, where each point is assigned to one of the clusters. For exact k-means clustering, the algorithm must compute the same result as Lloyd’s algorithm, which is well-known to be computationally expensive due to the large number of distance comparisons between each point and the k centroids. Several algorithms have been proposed for k-means clustering that avoid distance calculations but produce an exact result. However, these algorithms have all been designed for execution using the CPU, and no published works have examined using the GPU to accelerate k-means while simultaneously avoiding distance calculations. This paper examines the state-of-the-art Yinyang algorithm that avoids distance calculations as executed on the GPU. Since Lloyd’s algorithm is well-suited to a GPU execution, it is not clear whether the Yinyang algorithm will obtain significant performance gains on GPU hardware. In this context, this paper: (i) proposes the first GPU-accelerated Yinyang algorithm in the literature; (ii) advances several optimizations to GPU kernels; (iii) contrasts and evaluates different degrees of distance calculation pruning; and, (iv) compares the performance of our GPU-accelerated Yinyang algorithm to four reference implementations. Our GPU algorithm achieves a speedup over the multi-core CPU Yinyang algorithm of up to 8× on real-world datasets.}
}


@inproceedings{DBLP:conf/icde/ZhangHXCYFZ021,
	author = {Zihao Zhang and
                  Huiqi Hu and
                  Zhihui Xue and
                  Changcheng Chen and
                  Yang Yu and
                  Cuiyun Fu and
                  Xuan Zhou and
                  Feifei Li},
	title = {{SLIMSTORE:} {A} Cloud-based Deduplication System for Multi-version
                  Backups},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1841--1846},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00164},
	doi = {10.1109/ICDE51399.2021.00164},
	timestamp = {Mon, 28 Jun 2021 08:46:39 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangHXCYFZ021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud backup is becoming the preferred way for users to support disaster recovery. In addition to its convenience, users are deeply concerned about reducing storage costs in the face of large-scale backup data. Data deduplication is an effective method for backup storage. However, current deduplicate methods lack the utilization of cloud resources to provide scalable backup service for cloud backup users, and cannot meet the biased preference for different backup versions. For new backup versions, users want higher deduplicate and restore speed to reduce the waiting time. Conversely, reducing storage costs is more necessary for old backup versions.In this paper, we present SLIMSTORE, with a cloud-based deduplication architecture that disassembles the system into a storage layer and a computing layer to support elastic utilization of cloud resources. We propose two types of processing nodes with different design focuses to meet the needs of cloud-based backup. The L-node exploits locality and similarity, and adopts a history-aware strategy to provide fast online deduplication service. L-node also optimizes online restoration to realize high restore efficiency. Meanwhile, the G-node provides exact deduplication offline for the old versions, and helps the restore performance of the new versions by optimizing their physical storage. We compare SLIMSTORE with some state-of-art deduplicate and restore methods. Experimental results show that SLIMSTORE can achieve fast deduplication, efficient restoration, and effective space reduction. Furthermore, SLIMSTORE attains scalable deduplication and restoration.}
}


@inproceedings{DBLP:conf/icde/ZhengXZZYZ21,
	author = {Peilin Zheng and
                  Quanqing Xu and
                  Zibin Zheng and
                  Zhiyuan Zhou and
                  Ying Yan and
                  Hui Zhang},
	title = {Meepo: Sharded Consortium Blockchain},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1847--1852},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00165},
	doi = {10.1109/ICDE51399.2021.00165},
	timestamp = {Mon, 19 Dec 2022 20:39:08 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhengXZZYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain performance cannot meet the requirement nowadays. One of the crucial ways to improve performance is sharding. However, most blockchain sharding research focuses on public blockchain. As for consortium blockchain, previous studies cannot support high cross-shard efficiency, cross-contract flexibility, shard availability, and strict transaction atomicity, which are the essential requirements but also the challenges in consortium blockchain systems. Facing these challenges, we propose Meepo, a systematic study on sharded consortium blockchain. Meepo enhances cross-shard efficiency via the cross-epoch and cross-call. Moreover, a partial cross-call merging strategy is designed to handle the multi-state dependency in contract calls, achieving cross-contract flexibility. Meepo employs a replay-epoch to ensure strict transaction atomicity, and it also uses a backup algorithm called shadow shard based recovery to improve the shard robustness. We implement Meepo on the AliCloud, using 32 shards in maximum, achieving more than 120,000 cross-shard TPS under the workload of 100,000,000 asset transactions.}
}


@inproceedings{DBLP:conf/icde/Al-MamunYZ21,
	author = {Abdullah Al{-}Mamun and
                  Feng Yan and
                  Dongfang Zhao},
	title = {SciChain: Blockchain-enabled Lightweight and Efficient Data Provenance
                  for Reproducible Scientific Computing},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1853--1858},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00166},
	doi = {10.1109/ICDE51399.2021.00166},
	timestamp = {Tue, 06 Jul 2021 13:52:51 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Al-MamunYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The state-of-the-art for auditing and reproducing scientific applications on high-performance computing (HPC) systems is through a data provenance subsystem. While recent advances in data provenance lie in reducing the performance overhead and improving the user’s query flexibility, the fidelity of data provenance is often overlooked: there is no such way to ensure that the provenance data itself has not been fabricated or falsified. This paper advocates leveraging blockchains to deliver immutable and autonomous data provenance services such that scientific discoveries are trustworthy. The challenges for adopting blockchains to HPC include designing a new blockchain architecture compatible with the HPC platforms and, more importantly, a set of new consensus protocols for scientific applications atop blockchains. To this end, we have designed the proof-of-scalable-traceability (POST) protocol and implemented it in a blockchain prototype, namely SciChain, the very first practical blockchain system for provenance services on HPC. We evaluated SciChain by comparing it with multiple state-of-the-art systems; experimental results showed that SciChain guaranteed trustworthy data provenance while incurring orders of magnitude lower overhead than existing solutions.}
}


@inproceedings{DBLP:conf/icde/WangYS21,
	author = {Fang Wang and
                  Man Lung Yiu and
                  Zili Shao},
	title = {Accelerating Similarity-based Mining Tasks on High-dimensional Data
                  by Processing-in-memory},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1859--1864},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00167},
	doi = {10.1109/ICDE51399.2021.00167},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangYS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Similarity computation is a core subroutine of many mining tasks on multi-dimensional data, which are often massive datasets at high dimensionality. In these mining tasks, the performance bottleneck is caused by the ‘memory wall’ problem as substantial amount of data needs to be transferred from memory to processors. Recent advances in non-volatile memory (NVM) enable processing-in-memory (PIM), which reduces data transfer and thus alleviates the performance bottleneck. Nevertheless, NVM PIM supports specific operations only (e.g., dot-product on non-negative integer vectors) but not arbitrary operations. In this paper, we tackle the above challenge and carefully exploit NVM PIM to accelerate similarity-based mining tasks on multi-dimensional data without compromising the accuracy of results. Experimental results on real datasets show that our proposed method achieves up to 10.5x and 8.5x speedup for state-of-art kNN classification and k-means clustering algorithms, respectively.}
}


@inproceedings{DBLP:conf/icde/HeLPW21,
	author = {Zeyu He and
                  Zhifang Li and
                  Xiaoshuang Peng and
                  Chuliang Weng},
	title = {DS\({}^{\mbox{2}}\): Handling Data Skew Using Data Stealings over
                  High-Speed Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1865--1870},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00168},
	doi = {10.1109/ICDE51399.2021.00168},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HeLPW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed in-memory computing systems have dramatic performance improvement over traditional disk-based systems, which makes them widely used in large-scale data processing applications. Unfortunately, uneven and unpredictable data distributions caused by data skew have a significant impact on the performance. In Spark, when data skew happens, some tasks will process much more data than other tasks and become the performance bottleneck. The traditional approaches to handling data skew are based on sampling and repartitioning, which incur additional overhead. In this paper, we divide data skew in distributed data processing systems into intra-node and inter-node skew. Based on data stealing, we proposed DS 2 to handle both intra-node and inter-node data skew. It aims to improve the performance under data skew, without involving additional overhead. DS 2 first balances the skewed data distribution in the local and then handles the inter-node skew by RDMA during execution. It achieves up to 2.96× speedup on the aggregation operator and 2.81× speedup on the join operator.}
}


@inproceedings{DBLP:conf/icde/YuW000021,
	author = {Yuanhang Yu and
                  Dong Wen and
                  Ying Zhang and
                  Xiaoyang Wang and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {Efficient Matrix Factorization on Heterogeneous {CPU-GPU} Systems},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1871--1876},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00169},
	doi = {10.1109/ICDE51399.2021.00169},
	timestamp = {Sun, 06 Oct 2024 21:04:59 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YuW000021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Matrix Factorization (MF) has been widely applied in machine learning and data mining. Due to the large computational cost of MF, we aim to improve the efficiency of SGD-based MF computation by utilizing the massive parallel processing power of heterogeneous multiprocessors. The main challenge in parallel SGD algorithms on heterogeneous CPU-GPU systems lies in the strategy to assign tasks. We design a novel strategy to divide the matrix into a set of blocks by considering two aspects. First, we observe that the matrix should be divided nonuniformly, and relatively large blocks should be assigned to GPUs to saturate the computing power of GPUs. In addition to exploiting the characteristics of hardware, the workloads assigned to two types of hardware should be balanced. We design a cost model tailored for our problem to accurately estimate the performance of hardware on different data sizes. Extensive experiments show that our proposed algorithm achieves high efficiency with a high quality of training quality.}
}


@inproceedings{DBLP:conf/icde/WangWC021,
	author = {Yangyang Wang and
                  Zikai Wang and
                  Yunpeng Chai and
                  Xin Wang},
	title = {Rethink the Linearizability Constraints of Raft for Distributed Key-Value
                  Stores},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1877--1882},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00170},
	doi = {10.1109/ICDE51399.2021.00170},
	timestamp = {Sun, 06 Oct 2024 21:04:59 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangWC021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed key-value stores have been widely used as NoSQL systems or the storage layer of distributed relational databases for various big data applications (e.g., social networking, graph processing, machine learning, etc.) due to their excellent scalability and adaptability. Although modern hardware such as Flash-based SSDs and the high-speed network is commonly deployed in key-value stores to promote performance, the distributed consensus and consistency module (e.g., Raft) is typically the most time-consuming part in distributed systems. The reason lies in that Raft introduces some very strict constraints to ensure the linearizability. Therefore, in this paper, we rethink these constraints in-depth and find that some of them are not necessary, and can be broken to accelerate the performance significantly without breaking the linear consistency for distributed key-value storage systems. An improved distributed consensus algorithm called KV-Raft is proposed in this paper and implemented in an industry-level distributed key-value system, i.e., TiKV. The experimental results suggest that both the write and the read performance can be accelerated significantly by KV-Raft. For example, in the typical read/write-balanced case, KV-Raft promotes the system throughput by 53.6%, and reduce the average write and read latency by 37.8% and 29.4%, respectively.}
}


@inproceedings{DBLP:conf/icde/PengFP21,
	author = {Botao Peng and
                  Panagiota Fatourou and
                  Themis Palpanas},
	title = {{SING:} Sequence Indexing Using GPUs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1883--1888},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00171},
	doi = {10.1109/ICDE51399.2021.00171},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PengFP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data series similarity search is a core operation for several data series analysis applications across many domains. This has attracted lots of interest that led to the development of several indexing techniques. Nevertheless, these techniques fail to deliver the similarity search time performance that is needed for interactive exploration, or analysis of large data series collections. We propose SING, the first data series index designed to take advantage of Graphics Processing Units (GPUs). SING is an in-memory index that uses CPU+GPU co-processing (as well as SIMD, multi-core and multi-socket architectures), in order to accelerate similarity search. Our experimental evaluation with synthetic and real datasets shows that SING is up to 5.1x faster than the state-of-the-art parallel in-memory approach, and up to 62x faster than the state-of-the-art parallel serial scan algorithm. SING achieves exact similarity search query times as low as 32msec on 100GB datasets, which enables interactive data exploration on very large data series collections.}
}


@inproceedings{DBLP:conf/icde/LuoJZC21,
	author = {Yongping Luo and
                  Peiquan Jin and
                  Qinglin Zhang and
                  Bin Cheng},
	title = {TLBtree: {A} Read/Write-Optimized Tree Index for Non-Volatile Memory},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1889--1894},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00172},
	doi = {10.1109/ICDE51399.2021.00172},
	timestamp = {Thu, 14 Oct 2021 10:29:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuoJZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid advance of Non-Volatile Memory (NVM), it has been a hot topic to improve traditional tree indices like B+-tree for NVM. However, due to the high cost of the writing operations on NVM, few existing tree indices can offer high performance for both read and write operations. For example, the WB-tree with unsorted leaf nodes is write-optimized but has poor search performance. To address this problem, in this paper, we propose a read/write-optimized tree index called TLBtree (Two-Layer B+-tree) for NVM. TLBtree consists of a read-optimized top layer and a write-optimized bottom layer. We notice that the top levels of a B+-tree are read frequently, while the bottom levels are written frequently. Motivated by such an observation, we propose to design a read-optimized top layer and a write-optimized layer for the TLBtree index. We offer several read optimizations to implement the top layer and employ write-optimized structures to organize the bottom layer. With this mechanism, we can alleviate the read and write tradeoff of the index on NVM. We conduct extensive experiments on a server with Intel Optane DC Persistent Memory and compare TLBtree with state-of-the-art NVM-based tree indices, including WB-tree, Fast&fair, and FPtree. The results show that TLBtree outperforms other indices in write-intensive workloads by up to 1.7x throughput and achieves comparable read-only performance with read-optimized indices.}
}


@inproceedings{DBLP:conf/icde/Schafer021,
	author = {Nico Sch{\"{a}}fer and
                  Sebastian Michel},
	title = {Utilizing Delta Trees for Efficient, Iterative Exploration and Transformation
                  of Semi-Structured Contents},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1895--1900},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00173},
	doi = {10.1109/ICDE51399.2021.00173},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Schafer021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The keywords data exploration or data wrangling summarize various different query workload scenarios in which users aim to explore or tailor data to their needs. For semi-structured data, next to commonly used SQL-style select-from-where and aggregation queries, also the structure of the possibly-nested schema-free data can be altered, schema attributes renamed, and so on. This typically involves various rounds of refining or discarding queries—imposing that intermediate results as well as the original sources cannot be eliminated. In this work, we extend our prior work on JODA, a vertically scalable, versatile JSON data processor, to make use of so-called delta trees for the succinct representation of incrementally created query results.}
}


@inproceedings{DBLP:conf/icde/RichlyS021,
	author = {Keven Richly and
                  Rainer Schlosser and
                  Martin Boissier},
	title = {Joint Index, Sorting, and Compression Optimization for Memory-Efficient
                  Spatio-Temporal Data Management},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1901--1906},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00174},
	doi = {10.1109/ICDE51399.2021.00174},
	timestamp = {Wed, 07 Dec 2022 23:09:58 +0100},
	biburl = {https://dblp.org/rec/conf/icde/RichlyS021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The wide distribution of location-acquisition technologies has led to large volumes of spatio-temporal data, which are the foundation for a broad spectrum of applications. Based on these applications’ performance requirements, in-memory databases are used to store and process the data. As DRAM capacities are limited and expensive, modern database systems apply various configuration optimizations (e.g., compression) to reduce the memory footprint. The selection of cost and performance balancing configurations is challenging due to the vast amount of possible setups consisting of mutually dependent individual decisions. In this paper, we present a linear programming approach to determine fine-grained configuration decisions for spatio-temporal workloads. By dividing the data into partitions of fixed size, we can apply the compression, sorting, and index selections on a fine-grained level to reflect spatiotemporal access patterns. Our approach jointly optimizes these configurations to maximize performance under a given memory budget. We demonstrate on a real-world dataset that models specifically optimized for spatio-temporal data characteristics allow us to reduce the memory footprint (up to 60% by equal performance) and increase the performance (up to 80% by equal memory size) compared to established rule-based heuristics.}
}


@inproceedings{DBLP:conf/icde/FangZJZ21,
	author = {Min Fang and
                  Zhao Zhang and
                  Cheqing Jin and
                  Aoying Zhou},
	title = {High-Performance Smart Contracts Concurrent Execution for Permissioned
                  Blockchain Using {SGX}},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1907--1912},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00175},
	doi = {10.1109/ICDE51399.2021.00175},
	timestamp = {Mon, 02 Aug 2021 16:24:33 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FangZJZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since there are no security concerns such as Sybil attacks, selfish mining, etc., the higher the system throughput, the better for the permissioned blockchain. And with the emergence of consensus algorithms, the throughput rates of permissioned blockchain can be up to thousands of transactions per second. The existing serial execution method for smart contracts becomes a new bottleneck for the system. Due to the lack of mutual trust between nodes, for a batch of smart contracts contained in a block, the traditional two-phase smart contract concurrency approach can only achieve concurrency within a single node, but not the parallel execution of contracts between nodes. In this paper, we propose a new two-phase framework based on trusted hardware Intel SGX, which can avoid the re-execution of all smart contracts on all nodes and improve parallelism between nodes. And consistency between nodes is achieved directly with state replication, rather than by re-executing transactions. We design a pre-execution mechanism for smart contracts in untrusted memory to batch fetch all the state data that a smart contract needs to access to reduce frequent enclave transitions during smart contract execution. Besides, we propose a method that generates a compact read-write set and a data structure named Merkle Forest which can generate the compact Merkle multiproofs for the initial data in untrusted memory in parallel and can quickly verify the correctness of the data passed in the enclave. Finally, we integrate all the techniques proposed in this paper into an open-source system BFT-SMaRt to evaluate our approach in a distributed setting. Experimental results show the efficiency of the proposed methods.}
}


@inproceedings{DBLP:conf/icde/ForoniLV21,
	author = {Daniele Foroni and
                  Matteo Lissandrini and
                  Yannis Velegrakis},
	title = {Estimating the extent of the effects of Data Quality through Observations},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1913--1918},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00176},
	doi = {10.1109/ICDE51399.2021.00176},
	timestamp = {Tue, 07 May 2024 20:05:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ForoniLV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing data quality works have so far focused on the computation of many data characteristics as a mean of quantifying different quality dimensions, like freshness, consistency, accuracy, or completeness, that are all defined about some ideal (clean) dataset. We claim that this approach falls short in providing a full specification of the quality of the data since it does not take into consideration the task for which the data is to be used, neither any future instances of the dataset. We argue that apart from the difference from the clean dataset, it is equally important to know the degree to which such difference affects the results of the task at hand. Thus, we extend the existing data quality definition to include that degree. Our approach, not only allows data quality to be considered in the context of the intended task, but can also provide useful information even in the absence of the clean dataset, and proffer an understanding of the effect of data quality in future dataset instances. We describe a system and its implementation that computes this extended form of data quality through a principled approach of systematic noise generation and task result evaluation. We perform numerous experiments illustrating the effectiveness of the approach and how this allows contextualizing traditional data quality measures.}
}


@inproceedings{DBLP:conf/icde/SongM21,
	author = {Jihyeon Song and
                  Bongki Moon},
	title = {Decoupled Instance-label Extreme Multi-label Classification with Skew
                  Coordinate Feature Space},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1919--1924},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00177},
	doi = {10.1109/ICDE51399.2021.00177},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SongM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extreme multi-label classification predicts labels that instances can have for a dataset with a massive number of labels. State-of-the-art approaches consider it more critical to accurately predict related labels than non-relevant labels, so they evaluate only the top k label candidates. However, the top k results are not suitable for applications, such as a drug repositioning, which aims to assign additional labels to instances. In this work, we propose DilXML, an extreme multi-label classifier that suggests a reasonable number of labels for each instance. DilXML overcomes the absence of negative data and the poverty of instances by decoupling instances and labels. Also, we propose three criteria for conceptual distance formulas considering a hierarchical structure between features. Through this, a skew coordinate feature space better reflects the relatedness between points. DilXML is the first extreme multi-label classification that conducts example-based evaluations. We compare over five state-of-the-art approaches: AnnexML, Bonsai, DiSMEC, FastXML, and ProXML. DilXML is the only one that achieves the best performance for all metrics and outperforms by 10% except for one data. For the targeted medical data, DilXML is 58% better on all four evaluation metrics than other methods. Besides, we conduct a literature review on drug repositioning candidates and confirm that newly obtained labels are significantly related to the instance.}
}


@inproceedings{DBLP:conf/icde/ZhaoYL21,
	author = {Xinyu Zhao and
                  Hao Yan and
                  Yongming Liu},
	title = {Hierarchical Tree-based Sequential Event Prediction with Application
                  in the Aviation Accident Report},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1925--1930},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00178},
	doi = {10.1109/ICDE51399.2021.00178},
	timestamp = {Sun, 02 Oct 2022 16:04:38 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential event prediction is a well-studied area and has been widely used in proactive management, recommender systems and healthcare. One major assumption of the existing sequential event prediction methods is that similar event sequence patterns in the historical record will repeat themselves, enabling us to predict future events. However, in reality, the assumption becomes less convincing when we are trying to predict rare or unique sequences. Furthermore, the representation of the event may be complex with hierarchical structures. In this paper, we aim to solve this issue by taking advantage of the multi-level or hierarchical representation of these rare events. We proposed to build a sequential Encoder-Decoder framework to predict the event sequences. More specifically, in the encoding layer, we built a hierarchical embedding representation for the events. In the decoding layer, we first predict the high-level events and the low-level events are generated according to a hierarchical graphical structure. We propose to link the encoding decoding layers with the temporal models for future event prediction. In this article, we further discussed applying the proposed model into the failure event prediction according to the aviation accident reports and have shown improved accuracy and model interpretability.}
}


@inproceedings{DBLP:conf/icde/XiaHXDLB21,
	author = {Lianghao Xia and
                  Chao Huang and
                  Yong Xu and
                  Peng Dai and
                  Mengyin Lu and
                  Liefeng Bo},
	title = {Multi-Behavior Enhanced Recommendation with Cross-Interaction Collaborative
                  Relation Modeling},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1931--1936},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00179},
	doi = {10.1109/ICDE51399.2021.00179},
	timestamp = {Thu, 01 Aug 2024 19:56:56 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XiaHXDLB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many previous studies aim to augment collaborative filtering with deep neural network techniques, so as to achieve better recommendation performance. However, most existing deep learning-based recommender systems are designed for modeling singular type of user-item interaction behavior, which can hardly distill the heterogeneous relations between user and item. In practical recommendation scenarios, there exist multi-typed user behaviors, such as browse and purchase. Due to the overlook of user’s multi-behavioral patterns over different items, existing recommendation methods are insufficient to capture heterogeneous collaborative signals from user multi-behavior data. Inspired by the strength of graph neural networks for structured data modeling, this work proposes a Graph Neural Multi-Behavior Enhanced Recommendation (GNMR) framework which explicitly models the dependencies between different types of user-item interactions under a graph-based message passing architecture. GNMR devises a relation aggregation network to model interaction heterogeneity, and recursively performs embedding propagation between neighboring nodes over the user-item interaction graph. Experiments on real-world recommendation datasets show that our GNMR consistently outperforms state-of-the-art methods. The source code is available at https://github.com/akaxlh/GNMR.}
}


@inproceedings{DBLP:conf/icde/FarchiNN21,
	author = {Eitan Farchi and
                  Ramasuri Narayanam and
                  Lokesh Nagalapatti},
	title = {Ranking Data Slices for {ML} Model Validation: {A} Shapley Value Approach},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1937--1942},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00180},
	doi = {10.1109/ICDE51399.2021.00180},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FarchiNN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To make ML systems deployment ready, one of the prominent challenges is to debug the performance issues of the trained ML models. This can be done by associating the issues with a set of Data Slices - aggregates of validation data records - to help the developers investigate these technical issues at a deeper level of granularity. Since the possible number of data slices are exponential in number, there is a need to prioritize the order (i.e. ranking) of slices before presenting to the users. However, there does not exist any work that deals with ranking these automatically generated slices and we refer to this problem as the data slice ranking problem (DSRP). This problem is challenging to address as the top ranked slices should contain significant error concentration (i.e. number of mis-classified data points), be statistically significant (i.e. having large size), and be non-redundant (i.e. contain unique mis-classified data points).In this paper, we tackle this challenging problem by proposing a novel game theoretic framework building upon Shapley value concept to derive a rank order for a given collection of data slices. In particular, we formally present a scheme that explicitly accounts only for the error concentration and we refer to this as Shapley Slice Ranking with Error concentration (SSR-E). We then prove a few useful properties of this scheme. Using thorough experimentation on 7 open source data sets, we demonstrate the superior performance of SSR mechanism vis-à-vis two baseline methods.}
}


@inproceedings{DBLP:conf/icde/SunS21,
	author = {Yu Sun and
                  Shaoxu Song},
	title = {From Minimum Change to Maximum Density: On S-Repair under Integrity
                  Constraints},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1943--1948},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00181},
	doi = {10.1109/ICDE51399.2021.00181},
	timestamp = {Thu, 07 Dec 2023 07:48:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/SunS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To clean dirty data, integrity constraints are often employed. A typical S-repair model removes a minimal set of tuples (to avoid excessive removal and information loss) such that the integrity constraints are no longer violated in the remaining tuples. However, multiple candidates of minimal removal sets exist and are difficult to determine. We intuitively notice that a clean tuple often has more close neighbors (i.e., higher density) than dirty tuples. In this sense, our study proposes to return the S-repair under integrity constraints with the highest density, among various minimal removal sets. We explicitly analyze the hardness of maximizing S-repair density under integrity constraints, together with efficient approximation. Extensive experiments over real datasets collected from industry with real-world errors show that our proposal can achieve higher accuracy in cleaning dirty tuples, compared to the state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/DrienAA21,
	author = {Osnat Drien and
                  Antoine Amarilli and
                  Yael Amsterdamer},
	title = {Managing Consent for Data Access in Shared Databases},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1949--1954},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00182},
	doi = {10.1109/ICDE51399.2021.00182},
	timestamp = {Sun, 02 Oct 2022 16:04:35 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DrienAA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data sharing is commonplace on the cloud, in social networks and other platforms. When a peer shares data and the platform owners (or other peers) wish to use it, they need the consent of the data contributor (as per regulations such as GDPR). The standard solution is to require this consent in advance, when the data is provided to the system. However, platforms cannot always know ahead of time how they will use the data, so they often require coarse-grained and excessively broad consent. The problem is exacerbated because the data is transformed and queried internally in the platform, which makes it harder to identify whose consent is needed to use or share the query results. Motivated by this, we propose a novel framework for actively procuring consent in shared databases, focusing on the relational model and SPJU queries. The solution includes a consent model that is reminiscent of existing Access Control models, with the important distinction that the basic building blocks – consent for individual input tuples – are unknown. This yields the following problem: how to probe peers to ask for their consent regarding input tuples, in a way that determines whether there is sufficient consent to share the query output, while making as few probes as possible in expectation. We formalize the problem and analyze it for different query classes, both theoretically and experimentally.}
}


@inproceedings{DBLP:conf/icde/AlOmeirLMP21,
	author = {Omar AlOmeir and
                  Eugenie Yujing Lai and
                  Mostafa Milani and
                  Rachel Pottinger},
	title = {Summarizing Provenance of Aggregate Query Results in Relational Databases},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1955--1960},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00183},
	doi = {10.1109/ICDE51399.2021.00183},
	timestamp = {Sun, 04 Aug 2024 19:37:46 +0200},
	biburl = {https://dblp.org/rec/conf/icde/AlOmeirLMP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data provenance is any information about the origin of a piece of data and the process that led to its creation. Most database provenance work has focused on creating models and semantics to query and generate this information. While comprehensive, provenance information remains large and overwhelming, which can make it hard for provenance systems to support data exploration. We present a new approach to provenance exploration that builds on data summarization techniques. We contribute two novel summarization schemes for the provenance of aggregation queries: Impact summaries, and comparative summaries. We show with experiments that our techniques incur little overhead compared to basic summaries. We conduct a survey to show that our approaches are useful to users.}
}


@inproceedings{DBLP:conf/icde/MoskovitchJ21,
	author = {Yuval Moskovitch and
                  H. V. Jagadish},
	title = {Patterns Count-Based Labels for Datasets},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1961--1966},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00184},
	doi = {10.1109/ICDE51399.2021.00184},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MoskovitchJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Counts of attribute-value combinations are central to the profiling of a data set, particularly in determining fitness for use and in eliminating bias and unfairness. While counts of individual attribute values may be stored in some data set profiles, there are too many combinations of attributes for it to be practical to store counts for each combination. In this paper, we develop the notion of storing a "label" of limited size that can be used to obtain good estimates for these counts. A label, in this paper, contains information regarding the count of selected attribute-value combinations (which we call "patterns") in the data. We define an estimation function, that uses this label to estimate the count of every pattern. We present the problem of finding the optimal label given a bound on its size and propose a heuristic algorithm for generating optimal labels. We experimentally show the accuracy of count estimates derived from the resulting labels and the efficiency of our algorithm.}
}


@inproceedings{DBLP:conf/icde/MyrtakisTC21,
	author = {Nikolaos Myrtakis and
                  Ioannis Tsamardinos and
                  Vassilis Christophides},
	title = {{PROTEUS:} Predictive Explanation of Anomalies},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1967--1972},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00185},
	doi = {10.1109/ICDE51399.2021.00185},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MyrtakisTC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Numerous algorithms have been proposed for detecting anomalies (outliers, novelties) in an unsupervised manner. Unfortunately, it is not trivial, in general, to understand why a given sample (record) is labelled as an anomaly and thus diagnose its root causes. We propose the following reduced-dimensionality, surrogate model approach to explain detector decisions: approximate the detection model with another one that employs only a small subset of features. Subsequently, samples can be visualized in this low-dimensionality space for human understanding. To this end, we develop PROTEUS, an AutoML pipeline to produce the surrogate model, specifically designed for feature selection on imbalanced datasets. The PROTEUS surrogate model can not only explain the training data, but also the out-of-sample (unseen) data. In other words, PROTEUS produces predictive explanations by approximating the decision surface of an unsupervised detector. PROTEUS is designed to return an accurate estimate of out-of-sample predictive performance to serve as a metric of the quality of the approximation. Computational experiments confirm the efficacy of PROTEUS to produce predictive explanations for different families of detectors and to reliably estimate their predictive performance in unseen data. Unlike several ad-hoc feature importance methods, PROTEUS is robust to high-dimensional data.}
}


@inproceedings{DBLP:conf/icde/QinCLZ00FYO21,
	author = {Xuedi Qin and
                  Chengliang Chai and
                  Yuyu Luo and
                  Tianyu Zhao and
                  Nan Tang and
                  Guoliang Li and
                  Jianhua Feng and
                  Xiang Yu and
                  Mourad Ouzzani},
	title = {Ranking Desired Tuples by Database Exploration},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1973--1978},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00186},
	doi = {10.1109/ICDE51399.2021.00186},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/QinCLZ00FYO21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database exploration – the problem of finding and ranking desired tuples – is important for data discovery and analysis. Precisely specifying SQL queries is not always feasible in practice, such as "finding and ranking off-road cars based on a combination of Price, Make, Model, Age, and Mileage." – not only due to the query complexity (e.g., which may have many if-then-else, and, or and not logic), but also because the user typically does not have the knowledge of all data instances.We propose DExPlorer, a system for interactive database exploration. DExPlorer offers a simple and user-friendly interface which allows to: (1) confirm whether a tuple is desired or not, and (2) decide whether a tuple is more preferred than another. Behind the scenes, we jointly use multiple ML models to learn from the above two types of user feedback. Moreover, in order to effectively involve users, we carefully select the set of tuples for which we need to solicit feedback. Therefore, we devise question selection algorithms that consider not only the estimated benefit of each tuple, but also the possible partial orders between any two suggested tuples. Experiments on real-world datasets show that DExPlorer is more effective than existing approaches.}
}


@inproceedings{DBLP:conf/icde/0002T0EK21,
	author = {Cong Ding and
                  Dixin Tang and
                  Xi Liang and
                  Aaron J. Elmore and
                  Sanjay Krishnan},
	title = {{CIAO:} An Optimization Framework for Client-Assisted Data Loading},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1979--1984},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00187},
	doi = {10.1109/ICDE51399.2021.00187},
	timestamp = {Sat, 30 Sep 2023 09:44:49 +0200},
	biburl = {https://dblp.org/rec/conf/icde/0002T0EK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data loading has been one of the most common performance bottlenecks for many big data applications, especially when they are running on inefficient human-readable formats, such as JSON or CSV. Parsing, validating, integrity checking and data structure maintenance are all computationally expensive steps in loading these formats. Regardless of these costs, many records may be filtered later during query evaluation due to highly selective predicates – resulting in wasted computation. Meanwhile, the computing power of client ends is typically not exploited. Here, we explore investing limited cycles of clients on prefiltering to accelerate data loading and enable data skipping for query execution. In this paper, we present CIAO, a tunable system to enable client cooperation with the server to enable efficient partial loading and data skipping for a given workload. We proposed an efficient algorithm that would select a near-optimal predicate set to push down within a given budget. Our experiments show that CIAO can significantly accelerate the data loading processing and query execution.}
}


@inproceedings{DBLP:conf/icde/Dossinger021,
	author = {Manuel Dossinger and
                  Sebastian Michel},
	title = {Optimizing Multiple Multi-Way Stream Joins},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1985--1990},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00188},
	doi = {10.1109/ICDE51399.2021.00188},
	timestamp = {Sat, 30 Sep 2023 09:44:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Dossinger021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We address the joint optimization of multiple stream joins in a scale-out architecture by tailoring prior work on multi-way stream joins to predicate-driven data partitioning schemes. We present an integer linear programming (ILP) formulation for selecting the partitioning and tuple routing with minimal probe load. The presented algorithms and optimization schemes are implemented in CLASH, a data stream processor developed in our group that translates queries to deployable Apache Storm topologies after optimization. The experiments conducted on TPC-H data exhibit the potential of multi-query optimization of multi-way stream joins and the effectiveness and feasibility of the ILP optimization problem.}
}


@inproceedings{DBLP:conf/icde/KlabeS021,
	author = {Steffen Kl{\"{a}}be and
                  Kai{-}Uwe Sattler and
                  Stephan Baumann},
	title = {Updatable Materialization of Approximate Constraints},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1991--1996},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00189},
	doi = {10.1109/ICDE51399.2021.00189},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KlabeS021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern big data applications integrate data from various sources. As a result, these datasets may not satisfy perfect constraints, leading to sparse schema information and non-optimal query performance. The existing approach of PatchIndexes enable the definition of approximate constraints and improve query performance by exploiting the materialized constraint information. As real world data warehouse workloads are often not limited to read-only queries, we enhance the PatchIndex structure towards an update-conscious design in this paper. Therefore, we present a sharded bitmap as the underlying data structure which offers efficient update operations, and describe approaches to maintain approximate constraints under updates, avoiding index recomputations and full table scans. In our evaluation, we prove that PatchIndexes provide more lightweight update support than traditional materialization approaches.}
}


@inproceedings{DBLP:conf/icde/KanellosVSDV21,
	author = {Ilias Kanellos and
                  Thanasis Vergoulis and
                  Dimitris Sacharidis and
                  Theodore Dalamagas and
                  Yannis Vassiliou},
	title = {Ranking Papers by their Short-Term Scientific Impact},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {1997--2002},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00190},
	doi = {10.1109/ICDE51399.2021.00190},
	timestamp = {Sun, 06 Oct 2024 21:04:57 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KanellosVSDV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The constantly increasing rate at which scientific papers are published makes it difficult for researchers to identify papers that currently impact the research field of their interest. In this work, we present a method that ranks papers based on their estimated short-term impact, as measured by the number of citations received in the near future. Our method models a researcher exploring the paper citation network, and introduces an attention-based mechanism, akin to a time-restricted version of preferential attachment, that explicitly captures the researcher’s preference to read papers which received a lot of attention recently. A detailed experimental evaluation on real citation datasets across disciplines, shows that our approach is more effective than previous work.}
}


@inproceedings{DBLP:conf/icde/SongSL21,
	author = {Gwangho Song and
                  Kyuseok Shim and
                  Hongrae Lee},
	title = {Substring Similarity Search with Synonyms},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2003--2008},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00191},
	doi = {10.1109/ICDE51399.2021.00191},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SongSL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To allow deeper semantic understanding of strings, string matching with synonyms has recently received increasing attention. However, all the works focus on string semantics, which requires matching of entire strings, and cannot handle partial matching with substring semantics, resulting in its limited applications. To remedy this issue, we first propose a novel similarity measure between two strings allowing substring matching with synonyms, and develop an efficient algorithm to find the strings that have a substring semantically similar to the query string. Since considering synonyms on substrings enlarges the search space significantly, we devise efficient filtering methods to reduce the number of expensive similarity computations. Experiments with real-life datasets show the efficiency of our algorithm.}
}


@inproceedings{DBLP:conf/icde/XinLC21,
	author = {Hao Xin and
                  Xueling Lin and
                  Lei Chen},
	title = {CaSIE: Canonicalize and Informative Selection of the OpenIE system},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2009--2014},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00192},
	doi = {10.1109/ICDE51399.2021.00192},
	timestamp = {Fri, 27 Aug 2021 14:40:05 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XinLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge extraction has become a hot topic recently with the increasing number of applications needed for large-scale knowledge bases (KBs), such as semantic search and QA systems. The goal of knowledge extraction is to extract relations and their arguments from natural language text. Recent research proposes two kinds of solutions. The first one, called Closed IE, tries to construct KB through predefined features or rules with respect to a specific domain. It requires specifying the interested predicates in advance, which restricts its application to the domains where prior knowledge about the interested predicates must be given. The second one, called Open IE, tries to extract facts by using the parsing structure from the unstructured text. However, they cannot avoid extracting redundant facts. Such extractions can hardly be directly used to populate the existing KB. Moreover, many correct extractions are not relevant to the document, which limits the applications to understand the essential information that the document conveys. In this paper, we propose an end-to-end system which takes a target incomplete KB and documents as input. It first performs joint entity and relation linking to the existing KB based on both contexts of document and background KB information. Then it summarizes the extracted facts by considering the relevance to the document and the diversity between them. Extensive experiments over real datasets demonstrate the effectiveness and efficiency of the proposed methods.}
}


@inproceedings{DBLP:conf/icde/FengC00LS21,
	author = {Shanshan Feng and
                  Lisi Chen and
                  Kaiqi Zhao and
                  Wei Wei and
                  Fan Li and
                  Shuo Shang},
	title = {Node2LV: Squared Lorentzian Representations for Node Proximity},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2015--2020},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00193},
	doi = {10.1109/ICDE51399.2021.00193},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FengC00LS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, network embedding has attracted extensive research interest. Most existing network embedding models are based on Euclidean spaces. However, Euclidean embedding models cannot effectively capture complex patterns, especially latent hierarchical structures underlying in real-world graphs. Consequently, hyperbolic representation models have been developed to preserve the hierarchical information. Nevertheless, existing hyperbolic models only capture the first-order proximity between nodes. To this end, we propose a new embedding model, named Node2LV, that learns the hyperbolic representations of nodes using squared Lorentzian distances. This yields three advantages. First, our model can effectively capture hierarchical structures that come from the network topology. Second, compared with the conventional hyperbolic embedding methods that use computationally expensive Riemannian gradients, it can be optimized in a more efficient way. Lastly, different from existing hyperbolic embedding models, Node2LV captures higher-order proximities. Specifically, we represent each node with two hyperbolic embeddings, and make the embeddings of related nodes close to each other. To preserve higher-order node proximity, we use a random walk strategy to generate local neighborhood context. We conduct extensive experiments on four different types of real-world networks. Empirical results demonstrate that Node2LV significantly outperforms various graph embedding baselines.}
}


@inproceedings{DBLP:conf/icde/HoangCF21,
	author = {Anh{-}Tu Hoang and
                  Barbara Carminati and
                  Elena Ferrari},
	title = {Privacy-Preserving Sequential Publishing of Knowledge Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2021--2026},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00194},
	doi = {10.1109/ICDE51399.2021.00194},
	timestamp = {Tue, 12 Nov 2024 10:12:41 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HoangCF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graphs (KGs) are widely shared because they can model both users’ attributes as well as their relationships. Unfortunately, adversaries can re-identify their victims in these KGs by using a rich background knowledge about not only the victims’ attributes but also their relationships. A preliminary work to deal with this issue has been proposed in [1] which anonymizes both user attributes and relationships, but this is not enough. Indeed, adversaries can still re-identify target users if data providers publish new versions of their anonymized KGs. We remedy this problem by presenting the k w -Time-Varying Attribute Degree (k w -tad) principle that prevents adversaries from re-identifying any user appearing in w continuous anonymized KGs with a confidence higher than\n1\nk\n. Moreover, we introduce the Cluster-based Time-Varying Knowledge Graph Anonymization Algorithm to generate anonymized KGs satisfying k w -tad. Finally, we prove that even if data providers insert/re-insert/update/delete their users, the users are protected by k w -tad.}
}


@inproceedings{DBLP:conf/icde/GiakkoupisKRT21,
	author = {George Giakkoupis and
                  Anne{-}Marie Kermarrec and
                  Olivier Ruas and
                  Fran{\c{c}}ois Ta{\"{\i}}ani},
	title = {Cluster-and-Conquer: When Randomness Meets Graph Locality},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2027--2032},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00195},
	doi = {10.1109/ICDE51399.2021.00195},
	timestamp = {Mon, 26 Jun 2023 20:41:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GiakkoupisKRT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining and machine-learning applications. Some of the most efficient KNN graph algorithms are incremental and local: they start from a random graph, which they incrementally improve by traversing neighbors-of-neighbors links. Unfortunately, the initial random graph exhibits a poor graph locality, leading to many unnecessary similarity computations. In this paper, we remove this drawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts the starting configuration of greedy algorithms thanks to a novel lightweight clustering mechanism, dubbed FastRandomHash. FastRandomHash leverages randomness and recursion to pre-cluster similar nodes at a very low cost. Our extensive evaluation on real datasets shows that Cluster-and-Conquer significantly outperforms existing approaches, including LSH, yielding speed-ups of up to ×4.42 and even improving the KNN quality.}
}


@inproceedings{DBLP:conf/icde/ImranY0H0021,
	author = {Mubashir Imran and
                  Hongzhi Yin and
                  Tong Chen and
                  Zi Huang and
                  Xiangliang Zhang and
                  Kai Zheng},
	title = {{DDHH:} {A} Decentralized Deep Learning Framework for Large-scale
                  Heterogeneous Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2033--2038},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00196},
	doi = {10.1109/ICDE51399.2021.00196},
	timestamp = {Tue, 21 Mar 2023 20:50:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ImranY0H0021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning vector representations (i.e., embeddings) of nodes for graph-structured information network has attracted vast interest from both industry and academia. Most real-world networks exhibit a complex and heterogeneous format, enclosing high-order relationships and rich semantic information among nodes. However, existing heterogeneous network embedding (HNE) frameworks are commonly designed in a centralized fashion, i.e., all the data storage and learning process take place on a single machine. Hence, those HNE methods show severe performance bottlenecks when handling large-scale networks due to high consumption on memory, storage, and running time. In light of this, to cope with large-scale HNE tasks with strong efficiency and effectiveness guarantee, we propose Decentralized Deep Heterogeneous Hypergraph (DDHH) embedding framework in this paper. In DDHH, we innovatively formulate a large heterogeneous network as a hypergraph, where its hyperedges can connect a set of semantically similar nodes. Our framework then intelligently partitions the heterogeneous network using the identified hyperedges. Then, each resulted subnetwork is assigned to a distributed worker, which employs the deep information maximization theorem to locally learn node embeddings from the partition received. We further devise a novel embedding alignment scheme to precisely project independently learned node embeddings from all subnetworks onto a public vector space, thus allowing for downstream tasks. As shown from our experimental results, DDHH significantly improves the efficiency and accuracy of existing HNE models, and can easily scale up to large-scale heterogeneous networks.}
}


@inproceedings{DBLP:conf/icde/RenZZDB21,
	author = {Yuxiang Ren and
                  Hao Zhu and
                  Jiawei Zhang and
                  Peng Dai and
                  Liefeng Bo},
	title = {EnsemFDet: An Ensemble Approach to Fraud Detection based on Bipartite
                  Graph},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2039--2044},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00197},
	doi = {10.1109/ICDE51399.2021.00197},
	timestamp = {Thu, 01 Aug 2024 19:56:56 +0200},
	biburl = {https://dblp.org/rec/conf/icde/RenZZDB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fraud detection is extremely critical for e-commerce business platforms. Utilizing graph structure data and identifying unexpected dense subgraphs as suspicious is a category of commonly used fraud detection methods. Among them, spectral methods solve the problem efficiently but hurt the performance due to the relaxed constraints. Heuristic methods cannot be accelerated with parallel computation and fail to control the scope of returned suspicious nodes. These drawbacks affect the real-world applications of existing graph-based methods. In this paper, we propose an Ensemble based Fraud DETection (ENSEMFDET) method to scale up fraud detection in bipartite graphs. By oversampling the graph and solving the subproblems, the ensemble approach further votes suspicious nodes without sacrificing the prediction accuracy. Extensive experiments have been done on real transaction data from JD.com and demonstrate the effectiveness, practicability, and scalability of ENSEMFDET.}
}


@inproceedings{DBLP:conf/icde/FangW0J0Y21,
	author = {Peng Fang and
                  Fang Wang and
                  Zhan Shi and
                  Hong Jiang and
                  Dan Feng and
                  Lei Yang},
	title = {HuGE: An Entropy-driven Approach to Efficient and Scalable Graph Embeddings},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2045--2050},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00198},
	doi = {10.1109/ICDE51399.2021.00198},
	timestamp = {Fri, 02 Aug 2024 08:20:04 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FangW0J0Y21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph embedding is becoming widely adopted as an efficient way to learn graph representations required to solve graph analytics problems. However, most existing graph embedding methods, owing to computation-efficiency challenges for large-scale graphs, generally employ a one-size-fits-all strategy to extract information, resulting in a large amount of redundant or inaccurate representations. In this work, we propose HuGE, an efficient and scalable graph embedding method enabled by an entropy-driven mechanism. Specifically, HuGE leverages hybrid-property heuristic random walk to capture node features, which considers both node degree and the number of common neighbors in each walking step. More importantly, to guarantee information effectiveness of sampling, HuGE adopts two heuristic methods to decide the random walk length and the number of walks per node, respectively. Extensive experiments on real-world graphs demonstrate that HuGE achieves both efficiency and performance advantages over recent popular graph embedding approaches. For link prediction and multi-label classification, our approach not only offers >10% average gains, but also exhibits 22×–126× speedup compared with existing sampling-based methods.}
}


@inproceedings{DBLP:conf/icde/LuoY00C21,
	author = {Qi Luo and
                  Dongxiao Yu and
                  Zhipeng Cai and
                  Xuemin Lin and
                  Xiuzhen Cheng},
	title = {Hypercore Maintenance in Dynamic Hypergraphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2051--2056},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00199},
	doi = {10.1109/ICDE51399.2021.00199},
	timestamp = {Sat, 09 Apr 2022 12:45:21 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuoY00C21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study exact hypercore maintenance in large-scale dynamic hypergraphs. A hypergraph, whose hyperedges may contain a set of vertices rather than two vertices in pairwise graphs, can represent complex interactions in more sophisticated applications. However, the exponential number of hyperedges incurs unaffordable costs to recompute the hypercore number of vertices and hyperedges when updating a hypergraph. This motivates us to propose an efficient approach for exact hypercore maintenance with the intention of significantly reducing the hypercore updating time comparing with recomputation approaches. The proposed algorithms can pinpoint the vertices and hyperedges whose hypercore numbers have to be updated by only traversing a small sub-hypergraph. Extensive experiments on real-world and temporal hypergraphs demonstrate the superiority of our algorithms in terms of efficiency.}
}


@inproceedings{DBLP:conf/icde/ZengSG21,
	author = {Yiling Zeng and
                  Chunyao Song and
                  Tingjian Ge},
	title = {Selective Edge Shedding in Large Graphs Under Resource Constraints},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2057--2062},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00200},
	doi = {10.1109/ICDE51399.2021.00200},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZengSG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of the information age, many complex systems can be modeled as graphs. However, the unprecedented growth of data makes it extremely difficult for everyday users to process and mine very large graphs, given their limited computing resources such as personal computers and laptops. To address this challenge, we propose selective edge shedding. By estimating the original graph information from the reduced graph, it can accelerate graph algorithms and queries.In this paper, we propose two vertex-degree preserving edge shedding methods, the core of which are to maintain the expected vertex degree, so as to capture the basic characteristics of the network. Both methods allow users to control the size of the reduced graph based on the computing resource constraint. The experimental results show that the methods proposed in this paper can achieve up to 65% higher accuracy on graph analysis tasks compared to the competitive method, while consuming only 26%-57% running time, which fully demonstrates the advantages of the methods proposed in this work.}
}


@inproceedings{DBLP:conf/icde/Zheng0L21,
	author = {Shiyuan Zheng and
                  Hong Xie and
                  John C. S. Lui},
	title = {Social Visibility Optimization in OSNs with Anonymity Guarantees:
                  Modeling, Algorithms and Applications},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2063--2068},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00201},
	doi = {10.1109/ICDE51399.2021.00201},
	timestamp = {Thu, 14 Oct 2021 10:29:19 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Zheng0L21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online social network (OSN) is an ideal venue to enhance one’s visibility. This paper considers how a user (called requester) in an OSN selects a small number of available users and invites them as new friends/followers so as to maximize his "social visibility". More importantly, the requester has to do this under the anonymity setting, which means he is not allowed to know the neighborhood information of these available users in the OSN. In this paper, we first develop a mathematical model to quantify the social visibility and formulate the problem of visibility maximization with anonymity guarantee, abbreviated as "VisMAX-A". Then we design an algorithmic framework named as "AdaExp", which adaptively expands the requester’s visibility in multiple rounds. In each round of the expansion, AdaExp uses a query oracle with anonymity guarantee to select only one available user. By using probabilistic data structures like the k-minimum values (KMV) sketch, we design an efficient query oracle with anonymity guarantees. We also conduct experiments on real-world social networks and validate the effectiveness of our algorithms.}
}


@inproceedings{DBLP:conf/icde/BanerjeeCZLW21,
	author = {Prithu Banerjee and
                  Lingyang Chu and
                  Yong Zhang and
                  Laks V. S. Lakshmanan and
                  Lanjun Wang},
	title = {Stealthy Targeted Data Poisoning Attack on Knowledge Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2069--2074},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00202},
	doi = {10.1109/ICDE51399.2021.00202},
	timestamp = {Sat, 19 Aug 2023 18:10:20 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BanerjeeCZLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A host of different KG embedding techniques have emerged recently and have been empirically shown to be very effective in accurately predicting missing facts in a KG, thus improving its coverage and quality. Unfortunately, embedding techniques can fall prey to adversarial data poisoning attack. In this form of attack, facts may be added to or deleted from a KG, called performing perturbations, that results in the manipulation of the plausibility of target facts in a KG. While recent works confirm this intuition, the attacks considered there ignore the risk of exposure. Intuitively, an attack is of limited value if it is highly likely to be caught, i.e., exposed. To address this, we introduce a notion of the exposure risk and propose a novel problem of attacking a KG by means of perturbations where the goal is to maximize the manipulation of the target fact’s plausibility while keeping the risk of exposure under a given budget. We design a deep reinforcement learning-based framework, called RATA, that learns to use low-risk perturbations without compromising on the performance, i.e., manipulation of target fact plausibility. We test the performance of RATA against recently proposed strategies for KG attacks, on two different benchmark datasets and on different kinds of target facts. Our experiments show that RATA achieves state-of-the-art performance even while using a fraction of the risk.}
}


@inproceedings{DBLP:conf/icde/ChenSYLSYC21,
	author = {Hsi{-}Wen Chen and
                  Hong{-}Han Shuai and
                  De{-}Nian Yang and
                  Wang{-}Chien Lee and
                  Chuan Shi and
                  Philip S. Yu and
                  Ming{-}Syan Chen},
	title = {Structure-Aware Parameter-Free Group Query via Heterogeneous Information
                  Network Transformer},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2075--2080},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00203},
	doi = {10.1109/ICDE51399.2021.00203},
	timestamp = {Sat, 09 Apr 2022 12:45:21 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenSYLSYC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Owing to a wide range of important applications, such as team formation, dense subgraph discovery, and activity attendee suggestions on online social networks, Group Query attracts a lot of attention from the research community. However, most existing works are constrained by a unified social tightness k (e.g., for k-core, or k-plex), without considering the diverse preferences of social cohesiveness in individuals. In this paper, we introduce a new group query, namely Parameter-free Group Query (PGQ), and propose a learning-based model, called PGQN, to find a group that accommodates personalized requirements on social contexts and activity topics. First, PGQN extracts node features by a GNN-based method on Heterogeneous Activity Information Network (HAIN). Then, we transform the PGQ into a graph-to-set (Graph2Set) problem to learn the diverse user preference on topics and members, and find new attendees to the group. Experimental results manifest that our proposed model outperforms nine state-of-the-art methods by at least 51% in terms of F1-score on three public datasets.}
}


@inproceedings{DBLP:conf/icde/AyallDLGAD21,
	author = {Tewodros Ayall and
                  Hancong Duan and
                  Changhong Liu and
                  Fantahun Gereme and
                  Mohammed Abegaz and
                  Mesay Deleli},
	title = {Taking Heuristic Based Graph Edge Partitioning One Step Ahead via
                  OffStream Partitioning Approach},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2081--2086},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00204},
	doi = {10.1109/ICDE51399.2021.00204},
	timestamp = {Sun, 06 Oct 2024 21:04:56 +0200},
	biburl = {https://dblp.org/rec/conf/icde/AyallDLGAD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the modern era of big data, large-scale graph computing has become challenging because of the dramatic rise in graph data size. Graph edge partitioning (GEP) is a crucial preprocessing step to distributed graph platforms, yet it is challenging to partition the large-scale graphs. GEP has shown better partition quality than the graph vertex partitioning for the graph’s skewed degree distribution. Existing GEP approaches are classified into two as stream and offline. The former category assigns edges to the partitions based on the previously received edge information. It has less partitioning quality and is affected by stream order compared to the latter while supporting big graph partitioning. The latter uses complete knowledge of a graph during partitioning and hence has a better partitioning quality than the former; however, it does not support large-scale graphs. In this study, we propose a novel OffStream partitioning approach (OSPA) and hybrid graph edge partitioner OffStreamNH. OSPA leverages both the offline and stream graph partitioning approaches through stateful partitioning by introducing a state layer. This stateful partition state is recorded while offline is partitioning its input graph. It contains partial knowledge of previously partitioned data and is used by the stream partitioner. The OffStreamNH uses Neighborhood Expansion (NE) and Higher Degree Replicated First (HDRF) algorithms for the offline and online; respectively, with minor modifications of both algorithms. Experimental results show that OffStreamNH outperforms the state of the art stream partitioners in terms of replication factor, load balance and tolerates the effect of stream orders.}
}


@inproceedings{DBLP:conf/icde/ZhangQY021,
	author = {Hao Zhang and
                  Miao Qiao and
                  Jeffrey Xu Yu and
                  Hong Cheng},
	title = {Fast Distributed Complex Join Processing},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2087--2092},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00205},
	doi = {10.1109/ICDE51399.2021.00205},
	timestamp = {Sat, 20 Aug 2022 01:06:47 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangQY021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Big data analytics often requires processing complex join queries in parallel in distributed systems such as Hadoop, Spark, Flink. The previous works consider that the main bottleneck of processing complex join queries is the communication cost incurred by shuffling of intermediate results, and propose a way to cut down such shuffling cost to zero by a one-round multiway join algorithm. The one-round multi-way join algorithm is built on a one-round communication optimal algorithm for data shuffling over servers and a worst-case optimal computation algorithm for sequential join evaluation on each server. The previous works focus on optimizing the communication bottleneck, while neglecting the fact that the query could be computationally intensive. With the communication cost being well optimized, the computation cost may become a bottleneck. To reduce the computation bottleneck, a way is to trade computation with communication via pre-computing some partial results, but it can make communication or pre-computing becomes the bottleneck. With one of the three costs being considered at a time, the combined lowest cost may not be achieved. Thus the question left unanswered is how much should be traded such that the combined cost of computation, communication, and pre-computing is minimal.In this work, we study the problem of co-optimize communication, pre-computing, and computation cost in one-round multiway join evaluation. We propose a multi-way join approach ADJ (Adaptive Distributed Join) for complex join which finds one optimal query plan to process by exploring cost-effective partial results in terms of the trade-off between pre-computing, communication, and computation.We analyze the input relations for a given join query and find one optimal over a set of query plans in some specific form, with high-quality cost estimation by sampling. Our extensive experiments confirm that ADJ outperforms the existing multi-way join methods by up to orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/RaiL21,
	author = {Niranjan Rai and
                  Xiang Lian},
	title = {Top-k Community Similarity Search Over Large Road-Network Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2093--2098},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00206},
	doi = {10.1109/ICDE51399.2021.00206},
	timestamp = {Mon, 26 Jun 2023 20:41:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/RaiL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the urbanization and development of infrastructure, the community search over road networks has become increasingly important in many real applications such as urban/city planning, social study on local communities, and community recommendations by real estate agencies. In this paper, we propose a novel problem, namely top-k community similarity search (Top-kCS 2 ), which efficiently and effectively obtains spatial communities that are the most similar to a given query community over road-network graphs. In order to efficiently and effectively tackle the Top-kCS 2 problem, in this paper, we will design an effective similarity measure between communities, and propose a framework for retrieving Top-kCS 2 query answers. Extensive experiments have been conducted on real and synthetic data sets to confirm the efficiency and effectiveness of our proposed Top-kCS 2 approach under various parameter settings.}
}


@inproceedings{DBLP:conf/icde/JoshiSRBKK21,
	author = {Manas Joshi and
                  Arshdeep Singh and
                  Sayan Ranu and
                  Amitabha Bagchi and
                  Priyank Karia and
                  Puneet Kala},
	title = {Batching and Matching for Food Delivery in Dynamic Road Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2099--2104},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00207},
	doi = {10.1109/ICDE51399.2021.00207},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/JoshiSRBKK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a stream of food orders and available delivery vehicles, how should orders be assigned to vehicles so that the delivery time is minimized? For a successful assignment strategy, two key decisions need to be made: (1) assignment of orders to vehicles, (2) grouping orders into batches to cope with limited vehicle availability. We show that the minimization problem is not only NP-hard but inapproximable in polynomial time. To mitigate this computational bottleneck, we develop an algorithm called FOODMATCH, which maps the vehicle assignment problem to that of minimum weight perfect matching on a bipartite graph. The solution quality is further enhanced by reducing batching to a graph clustering problem. Extensive experiments on food-delivery data from large metropolitan cities establish that FOODMATCH is substantially better than baseline strategies on a number of metrics.}
}


@inproceedings{DBLP:conf/icde/XieXCLZ21,
	author = {Tao Xie and
                  Yangjun Xu and
                  Liang Chen and
                  Yang Liu and
                  Zibin Zheng},
	title = {Sequential Recommendation on Dynamic Heterogeneous Information Network},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2105--2110},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00208},
	doi = {10.1109/ICDE51399.2021.00208},
	timestamp = {Sun, 05 Mar 2023 01:38:19 +0100},
	biburl = {https://dblp.org/rec/conf/icde/XieXCLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The sequential recommendation has been widely used to predict users’ preferences in the near future by utilizing their dynamic interactions with items. However, existing methods only consider single-typed interactions (e.g., purchase), ignoring the rich heterogeneous information such as multi-typed interactions (e.g., click, purchase) and item attributes (e.g, category), which leads to a suboptimal model. We can integrate this rich information by introducing Dynamic Heterogeneous Information Networks (DHINs). Our solution contains three special designs: 1) Static Initialization; 2) Heterogeneous User Memory Network; 3) Two-level attention mechanism. Extensive experiments conducted on two real-world datasets show that our model outperforms other state-of-the-art solutions. Furthermore, we provide some insights into parameter settings and model interpretability.}
}


@inproceedings{DBLP:conf/icde/HernandezH0R21,
	author = {Daniel Ayala Hern{\'{a}}ndez and
                  Inma Hern{\'{a}}ndez and
                  David Ruiz and
                  Erhard Rahm},
	title = {Towards the smart use of embedding and instance features for property
                  matching},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2111--2116},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00209},
	doi = {10.1109/ICDE51399.2021.00209},
	timestamp = {Mon, 03 Jan 2022 22:33:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HernandezH0R21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data integration tasks such as the creation and extension of knowledge graphs involve the fusion of heterogeneous entities from many sources. Matching and fusion of such entities require to also match and combine their properties (attributes) . However, previous schema matching approaches mostly focus on two sources only and often rely on simple similarity measurements. They thus face problems in challenging use cases such as the integration of heterogeneous product entities from many sources.We therefore present a new machine learning-based property matching approach called LEAPME (LEArning-based Property Matching with Embeddings) that utilizes numerous features of both property names and instance values. The approach heavily makes use of word embeddings to better utilize the domain-specific semantics of both property names and instance values. The use of supervised machine learning helps exploit the predictive power of word embeddings.Our comparative evaluation against five baselines for several multi-source datasets with real-world data shows the high effectiveness of LEAPME.}
}


@inproceedings{DBLP:conf/icde/LiCZZJCH21,
	author = {Yuening Li and
                  Zhengzhang Chen and
                  Daochen Zha and
                  Kaixiong Zhou and
                  Haifeng Jin and
                  Haifeng Chen and
                  Xia Hu},
	title = {AutoOD: Neural Architecture Search for Outlier Detection},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2117--2122},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00210},
	doi = {10.1109/ICDE51399.2021.00210},
	timestamp = {Sun, 28 Apr 2024 17:30:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiCZZJCH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Outlier detection is an important data mining task with numerous applications such as intrusion detection, credit card fraud detection, and video surveillance. However, given a specific task with complex data, the process of building an effective deep learning based system for outlier detection still highly relies on human expertise and laboring trials. Moreover, while Neural Architecture Search (NAS) has shown its promise in discovering effective deep architectures in various domains, such as image classification, object detection and semantic segmentation, contemporary NAS methods are not suitable for outlier detection due to the lack of intrinsic search space and low sample efficiency. To bridge the gap, in this paper, we propose AutoOD, an automated outlier detection framework, which aims to search for an optimal neural network model within a predefined search space. Specifically, we introduce an experience replay mechanism based on self-imitation learning to improve the sample efficiency. Experimental results on various real-world benchmark datasets demonstrate that the deep model identified by AutoOD achieves the best performance, comparing with existing handcrafted models and traditional search methods.}
}


@inproceedings{DBLP:conf/icde/HerodotouCK21,
	author = {Herodotos Herodotou and
                  Despoina Chatzakou and
                  Nicolas Kourtellis},
	title = {Catching them red-handed: Real-time Aggression Detection on Social
                  Media},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2123--2128},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00211},
	doi = {10.1109/ICDE51399.2021.00211},
	timestamp = {Thu, 14 Oct 2021 10:29:20 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HerodotouCK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aggression on social media has evolved into a major point of concern. However, recently proposed machine learning (ML) approaches to detect various types of aggressive behavior fall short, due to the fast and increasing pace of content generation as well as evolution of such behavior over time. This work introduces the first, practical, real-time framework for detecting aggression on Twitter via embracing the streaming ML paradigm. This method adapts its ML binary classifiers in an incremental fashion, while receiving new annotated examples, and achieves similar performance as batch-based ML models, with 82–93% accuracy, precision, and recall. Experimental analysis on real Twitter data reveals how this framework, implemented in Spark Streaming, easily scales to process millions of tweets in minutes.}
}


@inproceedings{DBLP:conf/icde/WangY0LWW021,
	author = {Yuandong Wang and
                  Hongzhi Yin and
                  Tong Chen and
                  Chunyang Liu and
                  Ben Wang and
                  Tianyu Wo and
                  Jie Xu},
	title = {Gallat: {A} Spatiotemporal Graph Attention Network for Passenger Demand
                  Prediction},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2129--2134},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00212},
	doi = {10.1109/ICDE51399.2021.00212},
	timestamp = {Wed, 07 Dec 2022 23:09:59 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WangY0LWW021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online ride-hailing services have become an important component of urban transportation in recent years. As a fundamental research problem for such services, the timely prediction of passenger demands in different regions is vital for effective traffic flow control. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modelling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges. Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. However, existing graph-based solutions fail to simultaneously consider those three crucial aspects of dynamic, directed and weighted (DDW) graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose a novel spatiotemporal graph attention network, namely Gallat (Graph prediction with all attention) as a solution. In Gallat, by comprehensively incorporating those three intrinsic properties of DDW graphs, we build three attention layers to fully capture the spatiotemporal dependencies among different regions across all historical time slots. Our experimental results on real-world datasets demonstrate that Gallat outperforms the state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/icde/Zhang0L21,
	author = {Xiaoying Zhang and
                  Hong Xie and
                  John C. S. Lui},
	title = {Heterogeneous Information Assisted Bandit Learning: Theory and Application},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2135--2140},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00213},
	doi = {10.1109/ICDE51399.2021.00213},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Zhang0L21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contextual bandit serves as an invaluable tool to balance the exploration vs. exploitation trade-off in various applications like online recommendation. In many applications, heterogeneous information network (HIN) can be derived to provide rich side information for contextual bandits, such as different types of attributes and relationships among users and items. In this paper, we propose the first HIN-assisted contextual bandit framework, which utilizes a given HIN to assist contextual bandit learning. The proposed framework uses meta-paths in HIN to extract rich relations among users and items for the contextual bandit. The main challenge is how to leverage these relations, since users’ preference over items, the target of our online learning, are closely related to users’ preference over meta-paths, however it is unknown which meta-path a user prefers more. We propose the HUCB algorithm to address such a challenge. For each meta-path, the HUCB algorithm employs an independent base bandit algorithm to handle online item recommendation by leveraging the relationship captured in this meta-path. The bandit master is then employed to learn users’ preference over meta-paths to dynamically combine base bandit algorithms with a balance of exploration-exploitation trade-off. Experimental results on real datasets from LastFM and Yelp demonstrate the efficacy of the HUCB algorithm.}
}


@inproceedings{DBLP:conf/icde/WenLWW0WSX21,
	author = {Haomin Wen and
                  Youfang Lin and
                  Fan Wu and
                  Huaiyu Wan and
                  Shengnan Guo and
                  Lixia Wu and
                  Chao Song and
                  Yinghui Xu},
	title = {Package Pick-up Route Prediction via Modeling Couriers' Spatial-Temporal
                  Behaviors},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2141--2146},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00214},
	doi = {10.1109/ICDE51399.2021.00214},
	timestamp = {Sat, 30 Sep 2023 09:44:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WenLWW0WSX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over 10 billion packages are picked up every day in China. Accurate prediction of couriers’ pick-up routes can help the dispatch system to assign packages to couriers more intelligently, which is able to further increase the pick-up efficiency and reduce the overdue rate. In the package pick-up scene, the decision-making of a courier is quite complex since it’s affected by strict spatial-temporal constraints (e.g., package location, promised pick-up time, current time and courier’s current location). In this paper, we propose a novel model, named DeepRoute, to predict couriers’ future package pick-up routes according to the couriers’ decision experience learnt from their historical spatial-temporal behaviors. Specifically, DeepRoute consists of three layers: 1) The representation layer produces experience-aware representations for unpicked-up packages. 2) The transformer encoder layer encodes the representations of packages while considering the spatial-temporal correlations among them. 3) The attention-based decoder layer uses the attention mechanism to generate the whole pick-up route recurrently. Experiments on a real-world logistics dataset demonstrate the state-of-the-art performance of our DeepRoute model.}
}


@inproceedings{DBLP:conf/icde/LiSC21,
	author = {Yiming Li and
                  Yanyan Shen and
                  Lei Chen},
	title = {Palette: Towards Multi-source Model Selection and Ensemble for Reuse},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2147--2152},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00215},
	doi = {10.1109/ICDE51399.2021.00215},
	timestamp = {Sun, 02 Oct 2022 16:04:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiSC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The success of deep learning presents exciting opportunities for reusing pre-trained models from source domains to solve a target task with possibly limited training data. While various pre-trained models have been developed and become available, there is no principled way to select appropriate models for reuse. Although it is suggested that source and target tasks should be sufficiently similar, the calculation of task relevance usually requires extra storage of source training data and great efforts from domain experts, which is impractical in many applications. In this paper, we study the Multi-source Model Selection and Ensemble (MSMSE) problem. Given a collection of source models, we aim to select a subset of source models and develop an ensemble model that achieves the best performance for a target task. An ensemble of multiple models enables a boarder utilization of various underlying source knowledge and leads to better generalization ability. To this end, we present Palette, a generic framework that first selects potentially well-performed models from a source model pool, and then builds an ensemble with refinement. We introduce different model selection strategies that combine multi-armed bandits with adaptive resource allocation and Bayesian optimization techniques to accelerate the selection process. Extensive experimental results validate the effectiveness and efficiency of Palette and our model selection strategies.}
}


@inproceedings{DBLP:conf/icde/XarchakosK21,
	author = {Yannis Xarchakos and
                  Nick Koudas},
	title = {Querying for Interactions},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2153--2158},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00216},
	doi = {10.1109/ICDE51399.2021.00216},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XarchakosK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advances in Deep Learning and Computer Vision enabled sophisticated information extraction out of images and video frames. Recent research aims to make objects, their types and relative locations as the video evolves, first class citizens for query processing purposes.In this paper, we initiate research to explore declarative style of querying for real time video streams involving objects and their interactions. We seek to efficiently identify frames in a streaming video in which an object is interacting with another in a specific way, such as for example a human kicking a ball. We first propose an algorithm called progressive filters (PF) that deploys a sequence of inexpensive and less accurate models (filters) to detect the presence of the query specified objects on frames. We demonstrate that PF derives a least cost sequence of filters given the current selectivities of query objects. Since selectivities may vary as the video evolves, we present a dynamic statistical test to determine when to trigger re-optimization of the filters. Finally, we present a filtering approach called Interaction Sheave (IS) that utilizes learned spatial information about objects and interactions to effectively prune frames that are unlikely to involve the query specified action between them, thus improving the frame processing rate further.We present the results of a thorough experimental evaluation involving real data sets, demonstrating the performance benefits of each of our proposals. In particular we experimentally demonstrate that our techniques can improve query performance substantially (up to an order of magnitude in our experiments) while maintaining essentially the same F1-score as alternatives.}
}


@inproceedings{DBLP:conf/icde/Han00S21,
	author = {Yue Han and
                  Guoliang Li and
                  Haitao Yuan and
                  Ji Sun},
	title = {An Autonomous Materialized View Management System with Deep Reinforcement
                  Learning},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2159--2164},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00217},
	doi = {10.1109/ICDE51399.2021.00217},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Han00S21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Materialized views (MVs) can significantly optimize the query processing in databases. However, it is hard to generate MVs for ordinary users because it relies on background knowledge, and existing methods rely on DBAs to generate and maintain MVs. However, DBAs cannot handle large-scale databases, especially cloud databases that have millions of database instances and support millions of users. Thus it calls for an autonomous MV management system. In this paper, we propose an autonomous materialized view management system, AutoView. It analyzes query workloads, estimates the costs and benefits of materializing queries as views, and selects MVs to maximize the benefit within a space budget. We propose a deep reinforcement learning model to select high-quality MVs, which enriches the state representation with query and MVs’ embedding. Experimental results show that our method outperforms existing studies in terms of MV selection quality.}
}


@inproceedings{DBLP:conf/icde/ChenZWWX21,
	author = {Yu Chen and
                  Yong Zhang and
                  Jiacheng Wu and
                  Jin Wang and
                  Chunxiao Xing},
	title = {Revisiting Data Prefetching for Database Systems with Machine Learning
                  Techniques},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2165--2170},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00218},
	doi = {10.1109/ICDE51399.2021.00218},
	timestamp = {Thu, 11 Aug 2022 16:27:58 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenZWWX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Among diverse parts in database systems, database prefetching, which aims at predicting future page access patterns and fetching pages to be accessed ahead of time to mitigate blocked I/O operations, plays a crucial role in the overall performance tuning. Existing approaches just use simple heuristic-based methods and suffer from the low hit rate and extra I/O overhead. Recently, with the emerging success of machine learning in different applications, attempts using learning-based models to augment or improve components for database systems have shed some light on this tough problem. Impressed by the enormous potential of machine learning in data management, we present an end-to-end deep learning-based framework to predict page access patterns. We model the prediction of page access as a classification problem and evaluate several variants of neural networks on the accuracy of prediction. On the basis of it, we propose a new Multi-Model framework to construct an accurate model for prefetching. On a suite of real-world database benchmarks, our neural network based prefetching model consistently outperforms existing widely used solutions in real-world database systems.}
}


@inproceedings{DBLP:conf/icde/ArsomngernLSN21,
	author = {Pattaramanee Arsomngern and
                  Cheng Long and
                  Supasorn Suwajanakorn and
                  Sarana Nutanong},
	title = {Self-Supervised Deep Metric Learning for Pointsets},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2171--2176},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00219},
	doi = {10.1109/ICDE51399.2021.00219},
	timestamp = {Tue, 22 Oct 2024 20:38:20 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ArsomngernLSN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep metric learning is a supervised learning paradigm to construct a meaningful vector space to represent complex objects. A successful application of deep metric learning to pointsets means that we can avoid expensive retrieval operations on objects such as documents and can significantly facilitate many machine learning and data mining tasks involving pointsets. We propose a self-supervised deep metric learning solution for pointsets. The novelty of our proposed solution lies in a self-supervision mechanism that makes use of a distribution distance for set ranking called the Earth’s Mover Distance (EMD) to generate pseudo labels. Our experimental studies on four documents datasets show that our proposed solutions outperform baselines and state-of-the-art approaches on unsupervised deep metric learning in most settings.}
}


@inproceedings{DBLP:conf/icde/BrunnerS21,
	author = {Ursin Brunner and
                  Kurt Stockinger},
	title = {ValueNet: {A} Natural Language-to-SQL System that Learns from Database
                  Information},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2177--2182},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00220},
	doi = {10.1109/ICDE51399.2021.00220},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BrunnerS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we propose ValueNet light and ValueNet – two end-to-end Natural Language-to-SQL systems that incor-porate values using the challenging Spider dataset. The main idea of our approach is to use not only metadata information from the underlying database but also information on the base data as input for our neural network architecture. In particular, we propose a novel architecture sketch to extract values from a user question and come up with possible value candidates which are not explicitly mentioned in the question. We then use a neural model based on an encoder-decoder architecture to synthesize the SQL query. Finally, we evaluate our model on the Spider challenge using the Execution Accuracy metric, a more difficult metric than used by most participants of the challenge. Our experimental evaluation demonstrates that ValueNet light and ValueNet reach state-of-the-art results of 67% and 62% accuracy, respectively, for translating from NL to SQL whilst incorporating values.}
}


@inproceedings{DBLP:conf/icde/YangW0Q0021,
	author = {Peilun Yang and
                  Hanchen Wang and
                  Ying Zhang and
                  Lu Qin and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {{T3S:} Effective Representation Learning for Trajectory Similarity
                  Computation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2183--2188},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00221},
	doi = {10.1109/ICDE51399.2021.00221},
	timestamp = {Mon, 29 Jul 2024 16:18:15 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YangW0Q0021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advances of the sensor and GPS techniques have motivated the proliferation of trajectory data in a wide spectrum of applications. Trajectory similarity computation is one of the most fundamental problems in trajectory analytics. Considering that the high complexity of similarity computation is usually a bottleneck for large-scale trajectory data analysis, there are many research efforts for reducing the complexity such as the approximate algorithms. However, most of them are proposed for only one or two specific similarity measures, and thus cannot support different similarity measures well. In this paper, we propose a deep learning based model, namely T3S, which embeds each trajectory (i.e., a sequence of points) into a vector (point) in a d-dimensional space, and hence can significantly accelerate the similarity computation between the trajectories. By applying recurrent and attention neural networks, T3S can capture various unique characteristics of the trajectories such as the ordering of the points, spatial and structural information. Furthermore, our learning based T3S can easily handle any trajectory similarity measures by adjusting its parameters through the training. Extensive experiments on two real-life datasets demonstrate the effectiveness and efficiency of T3S. T3S outperforms state-of-the-art deep learning based methods under four popular trajectories similarity measures.}
}


@inproceedings{DBLP:conf/icde/ShaoWZXW21,
	author = {Kangjia Shao and
                  Yang Wang and
                  Zhengyang Zhou and
                  Xike Xie and
                  Guang Wang},
	title = {TrajForesee: How limited detailed trajectories enhance large-scale
                  sparse information to predict vehicle trajectories?},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2189--2194},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00222},
	doi = {10.1109/ICDE51399.2021.00222},
	timestamp = {Mon, 12 Feb 2024 16:07:13 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ShaoWZXW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Foreseeing detailed vehicle future trajectories collectively enables a large scope of urban applications such as route planning and commercial advertising. Existing methods focused on predicting future trajectories of urban vehicles with their own fine-grained historical trajectories. Unfortunately, in real-world scenarios, fine-grained trajectories provided by GPS are limited to obtain due to privacy issues and business competitions. In this paper, our solution enables the ubiquitous but coarse-grained location-based surveillance information to predict the fine-grained trajectories of all vehicles with limited number of fine-grained trajectories. We first capture the vectorized semantic representation of trajectories by training the spatiotemporal embedding in large coarse trajectory set. Then, we propose a new measurement to calculate the trajectory similarity, which combines the vehicles’ historical behavior similarity and short-term trajectory similarity. The obtained trajectory similarity is then seamlessly embedded into the dynamic graph convolution network in the manner of spatial attention. The dynamic graph convolution sequence-to-sequence module and the fully-connected layer are devised to generate final sequential trajectory predictions. The whole process is to train in a multi-task framework. Extensive experiments on real-world datasets show the excellent performance of our method.}
}


@inproceedings{DBLP:conf/icde/Su0Z21,
	author = {Xuebin Su and
                  Hongzhi Wang and
                  Yan Zhang},
	title = {Concurrency Control Based on Transaction Clustering},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2195--2200},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00223},
	doi = {10.1109/ICDE51399.2021.00223},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Su0Z21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Concurrency control is a mechanism that database systems provide to allow multiple transactions to be executed at the same time while enforcing isolation. The concurrency control algorithm is key to performance of a database system. However, different concurrency control algorithms have different strengths and weaknesses, making each of them fits only for some types of workloads, while performs unsatisfactorily for others. As a result, the user will have to make assumptions about the workloads before choosing the concurrency control algorithm to achieve the best performance. To overcome this limitation, we propose a scheme, called transaction clustering, to decide the best isolation mechanism for any given pair of transactions automatically. Based on transaction clustering, we further develop the Clustering-based Concurrency Control algorithm, or C3 for short, which combines the pessimistic and the optimistic concurrency control algorithms to get the best of both worlds while mitigating their performance bottlenecks at the same time. Both theoretical and experimental studies show that, for high-conflict workloads, the performance of the C3 algorithm can be significantly better than the performance of both the pessimistic and the optimistic algorithms that C3 is based on.}
}


@inproceedings{DBLP:conf/icde/TangWCGCP21,
	author = {Xiu Tang and
                  Sai Wu and
                  Gang Chen and
                  Jinyang Gao and
                  Wei Cao and
                  Zhifei Pang},
	title = {A Learning to Tune Framework for {LSH}},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2201--2206},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00224},
	doi = {10.1109/ICDE51399.2021.00224},
	timestamp = {Tue, 08 Feb 2022 08:24:39 +0100},
	biburl = {https://dblp.org/rec/conf/icde/TangWCGCP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nearest neighbor (NN) search in high-dimensional spaces is inherently computationally expensive due to the curse of dimensionality. As a well-known solution to approximate NN search, locality-sensitive hashing (LSH) is able to answer c-approximate NN (c-ANN) queries in sublinear time with a well-defined performance bound. The success of LSH family mainly depends on the design of randomly projected hash functions. However, instead of randomly drawing hash functions from a conventional hashing family such as Gaussian projection for Euclidean space, we argue that whether there could be a set of data sensitive hashing functions with higher capacity to distinguish nearby points and far away points, which could have rigorous performance guarantee like conventional LSH. To this end, we propose a learning to tune framework, called LSH-tuning, which consists of a pruning model and a learning to rank model. The pruning model reduces the total number of hash tables to maximize the separating capacity on the given data distribution and minimize the storage overhead. The learning to rank model ranks hash tables based on their effectiveness on NN retrieval. We also have a theoretic model that guides us to gradually search more hash tables and probe nearby buckets. Extensive experiments with real-world data demonstrate that LSH-tuning is capable of outperforming existing proposals with respect to both efficiency and storage overhead.}
}


@inproceedings{DBLP:conf/icde/YuYJZZL21,
	author = {Tan Yu and
                  Xuemeng Yang and
                  Yan Jiang and
                  Hongfang Zhang and
                  Weijie Zhao and
                  Ping Li},
	title = {{TIRA} in Baidu Image Advertising},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2207--2212},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00225},
	doi = {10.1109/ICDE51399.2021.00225},
	timestamp = {Sun, 02 Oct 2022 16:04:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YuYJZZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since an image can be perceived by customers in few seconds, it is an effective medium for advertising and adored by advertisers. Baidu, as one of the lead search companies in the world, receives billions of text queries per day. How to feed attractive images to capture the customers’ attentions is the core task of Baidu image advertising. Traditionally, the query-to-image search is tackled by matching the text query with the image title. Nevertheless, title-based image search relies on high-quality image titles, which are not easy to be obtained or unavailable in some cases. A more reliable solution is to understand the image content and conduct content-based query-to-image retrieval. In this paper, we introduce a text-image cross-modal retrieval for advertising (TIRA) model, which has been launched in Baidu image advertising. The proposed TIRA is built upon the popularly used image classification model, ResNet and the recent state-of-the-art NLP model, BERT. It targets to bridge the modal gap by mapping the images and texts into the same feature space. Meanwhile, we propose to use contrast loss to train the TIRA model, which consistently outperforms existing methods based on pairwise loss or triplet loss. Since the proposed TIRA model directly conducts the content-based query-to-image and image-to-query retrieval, and does not rely on high-quality labeled titles, it significantly enhances the search flexibility. The TIRA model has been deployed in image2X and query2X frameworks of Baidu image advertising. After the launch of TIRA, it has achieved considerable improvement in click-through-rate (CTR) and cost per mille (CPM), which brings considerable revenue increase for advertisers.}
}


@inproceedings{DBLP:conf/icde/ZhouZFRW021,
	author = {Meng Zhou and
                  Jingbo Zhou and
                  Yanjie Fu and
                  Zhaochun Ren and
                  Xiaoli Wang and
                  Hui Xiong},
	title = {Description Generation for Points of Interest},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2213--2218},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00226},
	doi = {10.1109/ICDE51399.2021.00226},
	timestamp = {Wed, 13 Nov 2024 14:22:37 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhouZFRW021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Description of Points of Interest (POIs) plays an important role to enhance the quality of many location-based services, such as displaying concentrated information of POIs for user-friendly experience and leading to successful POI recommendation. However, only a few popular POIs have enough description on the web. Collecting or writing high-quality descriptions for many unpopular or long-tail POIs remains a huge challenge for online map services, especially considering there are numerous new appeared POIs every day. Unlike existing studies about automatic product description generation, the POI description is quite diverse across different locations over a country, and requires high expert knowledge. To address this issue, we first study the POI description generation problem by proposing a novel model, named as Multi Mode Description Generator (MMDG), to automatically generate description based on POIs’ reviews and other features. To extract key information for POI description generation, MMDG is equipped with a multi-mode encoder and a transformer-based decoder. Besides user reviews, the multi-mode encoder also considers the category and spatial context information of target POIs, and integrate them with a fusion function. We have conducted an extensive experimental evaluation on a large-scale real-world dataset to demonstrate its effectiveness and superiority over state-of-the-art baselines in terms of various metrics.}
}


@inproceedings{DBLP:conf/icde/E0H21,
	author = {Jinlong E and
                  Mo Li and
                  Jianqiang Huang},
	title = {CrowdAtlas: Estimating Crowd Distribution within the Urban Rail Transit
                  System},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2219--2224},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00227},
	doi = {10.1109/ICDE51399.2021.00227},
	timestamp = {Sat, 09 Apr 2022 12:45:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/E0H21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While the urban rail transit systems are playing an increasingly important role in meeting the transportation demands of people, the precise awareness of how the human crowd is distributed within the urban rail transit system is highly necessary, which serves to a range of important applications including emergency response, transit recommendation, commercial valuation, etc. Most urban rail transit systems are closed systems where once entered the travelers are free to move around all stations that are connected into the system and are difficult to track. In this paper, we attempt to estimate the crowd distribution within the urban rail transit system based only on the entrance and exit records of all the rail riders. Specifically, we study Singapore MRT (Mass Rapid Transit) as a vehicle and leverage the tap-in and tap-out records of the EZ-Link transit cards to estimate the crowd distribution. Guided by a key observation that the passenger inflows and arrival flows at various MRT stations are spatio-temporally correlated due to behavioral consistence of MRT riders, we design and implement a machine learning based solution, CrowdAtlas, that accurately estimates the crowd distribution within the MRT system. Our trace-driven performance evaluation demonstrates the effectiveness of CrowdAtlas.}
}


@inproceedings{DBLP:conf/icde/ChenDHZZZZ21,
	author = {Xuanhao Chen and
                  Liwei Deng and
                  Feiteng Huang and
                  Chengwei Zhang and
                  Zongquan Zhang and
                  Yan Zhao and
                  Kai Zheng},
	title = {{DAEMON:} Unsupervised Anomaly Detection and Interpretation for Multivariate
                  Time Series},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2225--2230},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00228},
	doi = {10.1109/ICDE51399.2021.00228},
	timestamp = {Tue, 08 Feb 2022 10:19:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ChenDHZZZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many complex systems, devices are typically monitored and generating massive multivariate time series. However, due to the complex patterns and little useful labeled data, it is a great challenge to detect anomalies from these time series data. Existing methods either rely on less regularizations, or require a large number of labeled data, leading to poor accuracy in anomaly detection. To overcome those limitations, in this paper, we propose an unsupervised anomaly detection framework, called DAEMON (Adversarial Autoencoder Anomaly Detection Interpretation), which performs robustly for various datasets. The key idea is to use two discriminators to adversarially train an autoencoder to learn the normal pattern of multivariate time series, and thereafter use the reconstruction error to detect anomalies. The robustness of DAEMON is guaranteed by the regularization of hidden variables and reconstructed data using the adversarial generation method. Moreover, in order to help operators better diagnose anomalies, DAEMON provides anomaly interpretation based on the reconstruction error of the constituent univariate time series. Experiment results on four real datasets show that DAEMON can achieve an overall F1-score of 0.94, outperforming state-of-the-art methods. In addition, the anomaly interpretation accuracy of DAEMON can achieve 97%.}
}


@inproceedings{DBLP:conf/icde/Park0HMK21,
	author = {Namyong Park and
                  Minhyeok Kim and
                  Nguyen Xuan Hoai and
                  Robert I. McKay and
                  Dong{-}Kyun Kim},
	title = {Knowledge-Based Dynamic Systems Modeling: {A} Case Study on Modeling
                  River Water Quality},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2231--2236},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00229},
	doi = {10.1109/ICDE51399.2021.00229},
	timestamp = {Thu, 23 Jun 2022 19:56:00 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Park0HMK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modeling real-world phenomena is a focus of many science and engineering efforts, from ecological modeling to financial forecasting. Building an accurate model for complex and dynamic systems improves understanding of underlying processes and leads to resource efficiency. Knowledge-driven modeling builds a model based on human expertise, yet is often suboptimal. At the opposite extreme, data-driven modeling learns a model directly from data, requiring extensive data and potentially generating overfitting. We focus on an intermediate approach, model revision, in which prior knowledge and data are combined to achieve the best of both worlds. We propose a genetic model revision framework based on tree-adjoining grammar (TAG) guided genetic programming (GP), using the TAG formalism and GP operators in an effective mechanism making data-driven revisions while incorporating prior knowledge. Our framework is designed to address the high computational cost of evolutionary modeling of complex systems. Via a case study on the challenging problem of river water quality modeling, we show that the framework efficiently learns an interpretable model, with higher modeling accuracy than existing methods.}
}


@inproceedings{DBLP:conf/icde/HongJS21,
	author = {Daeyoung Hong and
                  Woohwan Jung and
                  Kyuseok Shim},
	title = {Collecting Geospatial Data with Local Differential Privacy for Personalized
                  Services},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2237--2242},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00230},
	doi = {10.1109/ICDE51399.2021.00230},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HongJS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geospatial data provides a lot of benefits for personalized services. However, since the geospatial data contains sensitive information about personal activities, collecting the raw data has a potential risk of leaking private information from the collectors. Recently, local differential privacy (LDP), which protects the privacy of users without trusting the collector, has been adopted to preserve privacy in many real applications. However, most of existing LDP algorithms focus on obtaining aggregated values such as mean and histogram from the collected data. In this paper, we investigate the problem of collecting the locations of individual users under LDP, and propose a perturbation mechanism designed carefully to reduce the error of each perturbed location according to the privacy budget and the domain size. In addition, we show the effectiveness of the proposed algorithm through experiments on various real datasets.}
}


@inproceedings{DBLP:conf/icde/SinglaEDMS21,
	author = {Samriddhi Singla and
                  Ahmed Eldawy and
                  Tina Diao and
                  Ayan Mukhopadhyay and
                  Elia Scudiero},
	title = {Experimental Study of Big Raster and Vector Database Systems},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2243--2248},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00231},
	doi = {10.1109/ICDE51399.2021.00231},
	timestamp = {Tue, 21 Mar 2023 20:50:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/SinglaEDMS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial data is traditionally represented using two data models, raster and vector. Raster data refers to satellite imagery while vector data includes GPS data, Tweets, and regional boundaries. While there are many real-world applications that need to process both raster and vector data concurrently, state-of-the-art systems are limited to processing one of these two representations while converting the other one which limits their scalability. This paper draws the attention of the research community to the research problems that emerge from the concurrent processing of raster and vector data. It describes three real-world applications and explains their computation and access patterns for raster and vector data. Additionally, it runs an extensive experimental evaluation using state-of-the-art big spatial data systems with raster data of up-to a trillion pixels, and vector data with up-to hundreds of millions of edges. The results show that while most systems can analyze raster and vector concurrently, but they have limited scalability for large-scale data.}
}


@inproceedings{DBLP:conf/icde/MoC0W21,
	author = {Xiaoyun Mo and
                  Chu Cao and
                  Mo Li and
                  David Z. W. Wang},
	title = {Predicting the Impact of Disruptions to Urban Rail Transit Systems},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2249--2254},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00232},
	doi = {10.1109/ICDE51399.2021.00232},
	timestamp = {Sat, 09 Apr 2022 12:45:21 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MoC0W21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Service disruptions of rail transit systems become more frequent in the past decades in urban cities like Singapore, due to various reasons such as power failures, signal errors, etc. We study and predict the impact of disruptions to transit systems and commuters. This benefits service providers in making both short and long term plans to improve their services. Specifically, we define two metrics, stay ratio and travel delay, to quantify the impact. To tackle the main challenge of abnormal data scarcity, i.e., only 6 observed disruptions in our one-year data records, we propose to format the problem into a training problem on a feature space relevant to alternative route choices of the commuters. We demonstrate the new feature space corresponds to more similar data distribution among different disruptions, which is beneficial for training more generalisable predictors for future disruptions. We implement and evaluate our approach with a real-world transit card dataset. The result clearly shows that our method outperforms a range of baseline methods.}
}


@inproceedings{DBLP:conf/icde/SaadallahTM21,
	author = {Amal Saadallah and
                  Maryam Tavakol and
                  Katharina Morik},
	title = {An Actor-Critic Ensemble Aggregation Model for Time-Series Forecasting},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2255--2260},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00233},
	doi = {10.1109/ICDE51399.2021.00233},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SaadallahTM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ensemble models are widely used as an effective technique in time-series forecasting, and recently, are inclined toward leveraging meta-learning methods due to their proven predictive advantages in combining individual models in an ensemble. However, finding the optimal strategy for ensemble aggregation is an open research question, particularly, when the ensemble needs to be adapted in real-time. In this paper, we pro-pose a novel meta-learning approach for aggregation of linearly weighted ensembles for the task of time-series forecasting. We outline a deep reinforcement learning framework with a coherent design of the components of the environment and the objective function as an aggregation method in our task. In this framework, the combination policy in ensembles is modeled as a sequential decision making process which is able to capture the temporal behavior in time-series, and an actor-critic model aims at learning the optimal weights in a continuous action space. An extensive empirical study on various real-world datasets demonstrates that our method achieves excellent or on par results in comparison to the state-of-the-art approaches as well as several baselines.}
}


@inproceedings{DBLP:conf/icde/NiCCC021,
	author = {Wangze Ni and
                  Nian Chen and
                  Peng Cheng and
                  Lei Chen and
                  Xuemin Lin},
	title = {Crowdrebate: An Effective Platform to Get more Rebate for Customers},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2261--2266},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00234},
	doi = {10.1109/ICDE51399.2021.00234},
	timestamp = {Mon, 26 Jun 2023 20:41:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/NiCCC021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To encourage users to buy more products, many online stores offer coupons. When a customer finds that the price of the products she/he wants to order is below the threshold of a coupon, she/he might want to place the order together with others to meet this threshold and enjoy more instant rebates. However, to conduct these orders and deliver products to receivers, users may need to pay extra delivery costs. When an order comprises several receivers’ requests, the products in the order should first be delivered from stores to an assigned warehouse, packed into different packages, and delivered to the different receivers. It may be costly than directly delivering products from stores to receivers. For the benefits of buyers, we propose a platform, the Crowdrebate platform, which collects requests from users, groups requests into a set of orders to get more rebates, and relays products to different receivers in an order. The platform will make a profit by getting a proportion from the benefit of the receiver (defined as the rebate minus the extra cost) of orders as its revenue. In this paper, we define the Crowdrebate problem, which aims to maximize the benefit of receivers. We prove the NP-hardness of the Crowdrebate problem. Therefore, we propose a heuristic solution to address the problem. Moreover, we evaluate the effectiveness and efficiency of our algorithm via comprehensive experiments.}
}


@inproceedings{DBLP:conf/icde/LuWTLWWW21,
	author = {Yi Lu and
                  Peng Wang and
                  Bo Tang and
                  Shen Liang and
                  Chen Wang and
                  Wei Wang and
                  Jianmin Wang},
	title = {{GRAB:} Finding Time Series Natural Structures via {A} Novel Graph-based
                  Scheme},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2267--2272},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00235},
	doi = {10.1109/ICDE51399.2021.00235},
	timestamp = {Mon, 26 Jun 2023 20:41:56 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuWTLWWW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the widespread use of sensors has substantially stimulated researchers’ interest in time series data mining. Real-world time series often include natural structures. For example, a time series captured from a patient rehabilitation app can be divided into a series of movements, e.g., sitting, standing, and walking. Finding time series natural structures (i.e., latent semantic states) is one of the core subroutines in time series mining applications. However, this task is not trivial as it has two challenges: (1) how to determine the correct change points between consecutive segments, and (2) how to cluster segments into different states.In this paper, we propose a novel graph-based approach, GRAB, to discover time series natural structures. In particular, GRAB first partitions the time series into a set of non-overlapping fragments via the similarity between subsequences. Then, it constructs a fragment-based graph and employs a graph partition method to cluster the fragments into states. Extensive experiments on real-world datasets demonstrate the effectiveness and efficiency of our GRAB method. Specifically, GRAB finds high-quality latent states, and it outperforms state-of-the-art solutions by orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/Li0UK21,
	author = {Hanlin Li and
                  Xiaowei Wu and
                  Leong Hou U and
                  Kun Pang Kou},
	title = {Near-Optimal Fixed-Route Scheduling for Crowdsourced Transit System},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2273--2278},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00236},
	doi = {10.1109/ICDE51399.2021.00236},
	timestamp = {Mon, 05 Feb 2024 20:31:13 +0100},
	biburl = {https://dblp.org/rec/conf/icde/Li0UK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bus scheduling is a crucial component for public transport service. Inefficient shift arrangement leads to poor vehicle loading rate or crowd inboard. In this paper, we consider a crowdsourced bus service system (on a fixed route) that receives user requests as input and computes a scheduling of buses with flexible departure time and skip-stop to minimize the travel time of users. We first show that the general problem of computing the optimal scheduling is NP-hard. Then we propose the Optimized Departure Time (ODT) algorithm that computes an optimal scheduling, which is built on an innovative reduction of the problem to a variant of the k-clustering problem, and an efficient application of dynamic programming. On top of ODT, we propose the Optimized Departure Time with Skip-Stop (ODTS) algorithm, which further improves the effectiveness of the solution by utilizing skip-stop. Our experimental results demonstrate that ODT and ODTS dramatically improve the baseline solution and outperform existing algorithms for the bus scheduling problem, which are very close to the optimum.}
}


@inproceedings{DBLP:conf/icde/BaigTK021,
	author = {Furqan Baig and
                  Dejun Teng and
                  Jun Kong and
                  Fusheng Wang},
	title = {{SPEAR:} Dynamic Spatio-Temporal Query Processing over High Velocity
                  Data Streams},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2279--2284},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00237},
	doi = {10.1109/ICDE51399.2021.00237},
	timestamp = {Tue, 21 Mar 2023 20:50:56 +0100},
	biburl = {https://dblp.org/rec/conf/icde/BaigTK021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advent of IoT and emerging 5G technology, real-time streaming data are being generated at unprecedented speed and volume having both temporal and spatial dimensions. Effective analysis at such scale and speed require support for dynamically adjusting querying capabilities in real-time. In the spatio-temporal domain, this warrants data as well as query optimization strategies especially for objects with changing motion states. Contemporary spatio-temporal data stream management systems in the distributed domain are mostly dominated by specify-once-apply-continuously query model. Any modification in query state requires query restart limiting system responsiveness and producing outdated or in worst case erroneous results. In this paper, we propose adaptations of principles from streaming databases, spatial data management, and distributed computing to support dynamic spatio-temporal query processing over high-velocity big data streams. We first formulate a set of spatio-temporal data types and functions to seamlessly handle changes in distributed query states. We develop a comprehensive set of streaming spatio-temporal querying methods and propose geohash based dynamic spatial partitioning for effective parallel processing. We implement a prototype on top of Apache Flink, where the in-memory stream processing fits nicely with our spatio-temporal models. Comparative evaluation of our prototype demonstrates the effectiveness of our strategy by maintaining high consistent processing rates for both stationary as well as moving queries over high velocity spatio-temporal big data streams.}
}


@inproceedings{DBLP:conf/icde/Shin0A21,
	author = {Jaewoo Shin and
                  Jianguo Wang and
                  Walid G. Aref},
	title = {The {LSM} RUM-Tree: {A} Log Structured Merge R-Tree for Update-intensive
                  Spatial Workloads},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2285--2290},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00238},
	doi = {10.1109/ICDE51399.2021.00238},
	timestamp = {Sun, 12 Nov 2023 02:08:10 +0100},
	biburl = {https://dblp.org/rec/conf/icde/Shin0A21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many applications require update-intensive work-loads on spatial objects, e.g., social-network services and shared-riding services that track moving objects (devices). By buffering insert and delete operations in memory, the Log Structured Merge Tree (LSM) has been used widely in various systems because of its ability to handle insert-intensive workloads. While the focus on LSM has been on key-value stores and their optimizations, there is a need to study how to efficiently support LSM-based secondary indexes. We investigate the augmentation of a main-memory-based memo structure into an LSM secondary index structure to handle update-intensive workloads efficiently. We conduct this study in the context of an R-tree-based secondary index. In particular, we introduce the LSM RUM-tree that demonstrates the use of an Update Memo in an LSM-based R-tree to enhance the performance of the R-tree’s insert, delete, update, and search operations. The LSM RUM-tree introduces novel strategies to reduce the size of the Update Memo to be a light-weight in-memory structure that is suitable for handling update-intensive workloads without introducing significant over-head. Experimental results using real spatial data demonstrate that the LSM RUM-tree achieves up to 9.6x speedup on update operations and up to 2400x speedup on query processing over the existing LSM R-tree implementations.}
}


@inproceedings{DBLP:conf/icde/LiGCXX21,
	author = {Yafei Li and
                  Hongyan Gu and
                  Rui Chen and
                  Jianliang Xu and
                  Mingliang Xu},
	title = {Top-k Publish/Subscribe for Ride Hitching},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2291--2296},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00239},
	doi = {10.1109/ICDE51399.2021.00239},
	timestamp = {Mon, 30 Sep 2024 07:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiGCXX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continued proliferation of mobile Internet and geo-locating technologies, carpooling as a green transport mode is widely accepted and becoming tremendously popular worldwide. In this paper, we focus on a popular carpooling service called ride hitching, which is typically implemented using a publish/subscribe approach. In a ride hitching service, drivers subscribe the ride orders published by riders and continuously receive the matching ride orders until one is picked. The current systems (e.g., Didi Hitch) adopt a threshold-based approach to filter ride orders. That is, a new ride order will be sent to all subscribing drivers whose planned trips can match the ride order within a pre-defined detour threshold. A limitation of this approach is that it is difficult for drivers to specify a reasonable detour threshold in practice. In addressing this problem, we propose a novel type of top-k subscription queries called Top-k Ride Subscription (TkRS) query, which continuously returns to drivers the best k ride orders that match their trip plans. We propose two efficient algorithms to enable the top-k result maintenance. Finally, extensive experiments on real-life datasets suggest that our proposed algorithms are capable of achieving desirable performance in practical settings.}
}


@inproceedings{DBLP:conf/icde/Ning0T21,
	author = {Wentao Ning and
                  Xiao Yan and
                  Bo Tang},
	title = {Towards Efficient MaxBRNN Computation for Streaming Updates},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2297--2302},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00240},
	doi = {10.1109/ICDE51399.2021.00240},
	timestamp = {Sun, 02 Oct 2022 16:04:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Ning0T21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose the streamingMaxBRNNquery, which finds the optimal region to deploy a new service point when both the service points and client points are under continuous updates. The streaming MaxBRNNquery has many applications such as taxi scheduling, shared bike placements, etc. Existing MaxBRNNsolutions are insufficient for streaming updates as they need to re-run from scratch even for a small amount of updates, resulting in long query processing time. To tackle this problem, we devise an efficient slot partitioning-based algorithm (SlotP), which divides the space into equal-sized slots and processes each slot independently. The superiorities of our proposal for streaming MaxBRNNquery are: (i) an update affects only a smaller number of slots and works done on the unaffected slots can be reused directly; (ii) the influence value upper bound of each slot can be derived efficiently and accurately, which facilitate pruning many slots from expensive computation. We conducted extensive experiments to validate the performance of the SlotPalgorithm. The results show that SlotPis 2-3 orders of magnitude faster than state-of-the-art baselines.}
}


@inproceedings{DBLP:conf/icde/ShenL0TP21,
	author = {Jiaxing Shen and
                  Oren Lederman and
                  Jiannong Cao and
                  Shaojie Tang and
                  Alex 'Sandy' Pentland},
	title = {User Profiling based on Nonlinguistic Audio Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2303--2308},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00241},
	doi = {10.1109/ICDE51399.2021.00241},
	timestamp = {Tue, 02 Jan 2024 17:14:17 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ShenL0TP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User profiling refers to inferring people’s attributes of interest (AoIs) like gender and occupation, which enables various applications ranging from personalized services to collective analyses. Massive nonlinguistic audio data brings a novel opportunity for user profiling due to the prevalence of studying spontaneous face-to-face communication. In this poster, we are the first to build a user profiling system to infer gender and personality based on nonlinguistic audio. Instead of linguistic or acoustic features which are unable to extract, we focus on conversational features that could reflect AoIs. We firstly develop an adaptive voice activity detection algorithm that could address individual differences in voice and false-positive voice activities caused by people nearby. Secondly, we propose a gender-assisted multi-task learning method to combat dynamics in human behavior by integrating gender differences and the correlation of personality traits. The experimental evaluation of 100 people in 273 meetings indicates the superiority of the proposed method in gender identification and personality recognition respectively.}
}


@inproceedings{DBLP:conf/icde/SoperHBG21,
	author = {Elizabeth Soper and
                  Jordan Hosier and
                  Dustin Bales and
                  Vijay K. Gurbani},
	title = {Semantic Search Pipeline: From Query Expansion to Concept Forging},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2309--2314},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00242},
	doi = {10.1109/ICDE51399.2021.00242},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SoperHBG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When searching a database for a topic (e.g. Covid-19), there may not exist a precise match, especially if the topic is novel. Furthermore, the topic may surface in the data under different guises (‘Covid-19,’ ‘coronavirus,’ ‘pandemic’, etc.). The results of a keyword search are limited by the querier’s imagination and familiarity with the data. Such searches have high precision, but low recall. In order to increase the recall of searches, we present the Semantic Search Pipeline, a novel approach to document retrieval that uses distributional semantic models and locality sensitive hashing to expand queries and efficiently identify other relevant documents that may not contain the obvious query terms. We evaluate the pipeline using a dataset curated from financial customer service call centers, resulting in an increase in recall of 32% over a simple keyword baseline, with a negligible drop in precision. Furthermore, we present the notion of concept forging, a process of tracing a topic or concept through time and through its various surface realizations. Applied to Covid-19, the search pipeline retrieves a set of documents that allow us to uncover the short- and long-term effects of Covid-19 on the lives of the people and businesses impacted by it. Although Covid-19 is a timely test case, our search pipeline is general in nature and can be easily applied to any range of topics.}
}


@inproceedings{DBLP:conf/icde/Ding0SW0021,
	author = {Xiaoou Ding and
                  Hongzhi Wang and
                  Jiaxuan Su and
                  Muxian Wang and
                  Jianzhong Li and
                  Hong Gao},
	title = {Leveraging Currency for Repairing Inconsistent and Incomplete Data
                  (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2315--2316},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00243},
	doi = {10.1109/ICDE51399.2021.00243},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Ding0SW0021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the growth of data from various sources, data quality is faced with multiple problems. In this paper, we study the multiple data cleaning on incompleteness and inconsistency with currency reasoning and determination. We introduce a 4-step method, named Imp3C, for error detection and repair in incomplete and inconsistent data without timestamps. We propose an integrated currency determining approach to compute currency order among tuples, thus, the dirty data can be repaired effectively considering the temporal impact. Experiments on three real-life datasets verify that Imp3C improves data repairing performance with multiple quality problems, especially in datasets with complex currency orders.}
}


@inproceedings{DBLP:conf/icde/Luo0YHZH21,
	author = {Dongsheng Luo and
                  Shuai Ma and
                  Yaowei Yan and
                  Chunming Hu and
                  Xiang Zhang and
                  Jinpeng Huai},
	title = {A Collective Approach to Scholar Name Disambiguation (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2317--2318},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00244},
	doi = {10.1109/ICDE51399.2021.00244},
	timestamp = {Fri, 08 Apr 2022 11:01:34 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Luo0YHZH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study investigates name disambiguation for scholarly data. We propose a collective approach, which considers the connections of different ambiguous names, such that it initially treats each author reference as a unique author entity and reformulates the bibliography data as a heterogeneous multipartite network. Disambiguation results of one author name propagate to the others in the network. To further deal with the sparsity problem caused by limited available information, we also introduce word-word and venue-venue similarities and measure author similarities by assembling similarities from multiple perspectives. Using three real-life datasets, we experimentally show that our approach is both effective and efficient.}
}


@inproceedings{DBLP:conf/icde/HemmatpourMRS21,
	author = {Masoud Hemmatpour and
                  Bartolomeo Montrucchio and
                  Maurizio Rebaudengo and
                  Mohammad Sadoghi},
	title = {Analyzing In-Memory NoSQL Landscape (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2319--2320},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00245},
	doi = {10.1109/ICDE51399.2021.00245},
	timestamp = {Sun, 25 Jul 2021 11:50:57 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HemmatpourMRS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-memory key-value stores have quickly become a key enabling technology to build high-performance applications that must cope with massively distributed workloads. In-memory key-value stores (also referred to as NoSQL) primarly aim to offer low-latency and high-throughput data access which motivates the rapid adoption of modern network cards such as Remote Direct Memory Access (RDMA). In this paper, we present the fundamental design principles for exploiting RDMAs in modern NoSQL systems. Moreover, we describe a break-down analysis of the state-of-the-art of the RDMA-based in-memory NoSQL systems. In addition, we compare traditional in-memory NoSQL with their RDMA-enabled counterparts.}
}


@inproceedings{DBLP:conf/icde/MenonQRSJ21,
	author = {Prashanth Menon and
                  Thamir M. Qadah and
                  Tilmann Rabl and
                  Mohammad Sadoghi and
                  Hans{-}Arno Jacobsen},
	title = {LogStore: {A} Workload-aware, Adaptable Key-Value Store on Hybrid
                  Storage Systems (Extended abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2321--2322},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00246},
	doi = {10.1109/ICDE51399.2021.00246},
	timestamp = {Mon, 03 Jan 2022 22:33:27 +0100},
	biburl = {https://dblp.org/rec/conf/icde/MenonQRSJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the recent explosion of data volume and velocity, a new array of lightweight key-value stores have emerged to serve as alternatives to traditional databases. The majority of these storage engines, however, sacrifice their read performance in order to cope with write throughput by avoiding random disk access when writing a record in favor of fast sequential accesses. But, the boundary between sequential vs. random access is becoming blurred with the advent of solid-state drives (SSDs).In this work, we propose our new key-value store, Log-Store, optimized for hybrid storage architectures. Additionally, introduce a novel cost-based data staging model based on log-structured storage, in which recent changes are first stored on SSDs, and pushed to HDD as it ages while minimizing the read/write amplification for merging data from SSDs and HDDs. Furthermore, we take a holistic approach in improving both the read and write performance by dynamically optimizing the data layout, such as deferring and reversing the compaction process and developing an access strategy to leverage the strengths of each available medium in our storage hierarchy. Lastly, in our extensive evaluation, we demonstrate that LogStore achieves up to 6x improvement in throughput/latency over LevelDB, a state-of-the-art key-value store.}
}


@inproceedings{DBLP:conf/icde/TamTYVSZH21,
	author = {Thanh Tam Nguyen and
                  Thanh Trung Huynh and
                  Hongzhi Yin and
                  Tong Van Vinh and
                  Darnbi Sakong and
                  Bolong Zheng and
                  Nguyen Quoc Viet Hung},
	title = {Entity Alignment for Knowledge Graphs with Multi-order Convolutional
                  Networks (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2323--2324},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00247},
	doi = {10.1109/ICDE51399.2021.00247},
	timestamp = {Sun, 02 Oct 2022 16:04:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/TamTYVSZH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graph (KG) entity alignment is the task of identifying corresponding entities across different KGs. Existing alignment techniques often require large amounts of labelled data, are unable to encode multi-modal data simultaneously, and enforce only a few consistency constraints. In this paper, we propose an end-to-end, unsupervised entity alignment framework for cross-lingual KGs using multi-order graph convolutional networks. An evaluation of our method using real-world datasets reveals that it consistently outperforms the state-of-the-art in terms of accuracy, efficiency, and label saving.}
}


@inproceedings{DBLP:conf/icde/0001PV21,
	author = {Raffaele Perego and
                  Giulio Ermanno Pibiri and
                  Rossano Venturini},
	title = {Compressed Indexes for Fast Search of Semantic Data (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2325--2326},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00248},
	doi = {10.1109/ICDE51399.2021.00248},
	timestamp = {Tue, 07 May 2024 20:05:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/0001PV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The sheer increase in volume of RDF data demands efficient solutions for the triple indexing problem, that is devising a compressed data structure to compactly represent RDF triples by guaranteeing, at the same time, fast pattern matching operations. This problem lies at the heart of delivering good practical performance for the resolution of complex SPARQL queries on large RDF datasets. We propose a trie-based index layout to solve the problem and introduce two novel techniques to reduce its space of representation for improved effectiveness. The extensive experimental analysis reveals that our best space/time trade-off configuration substantially outperforms existing solutions at the state-of-the-art, by taking 30–60% less space and speeding up query execution by a factor of 2–81 times.}
}


@inproceedings{DBLP:conf/icde/WuK21,
	author = {Renjie Wu and
                  Eamonn J. Keogh},
	title = {FastDTW is approximate and Generally Slower than the Algorithm it
                  Approximates (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2327--2328},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00249},
	doi = {10.1109/ICDE51399.2021.00249},
	timestamp = {Sat, 09 Apr 2022 12:45:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WuK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many time series data mining problems can be solved with repeated use of distance measure. Examples of such tasks include similarity search, clustering, classification, anomaly detection and segmentation. For over two decades it has been known that the Dynamic Time Warping (DTW) distance measure is the best measure to use for most tasks, in most domains. Because the classic DTW algorithm has quadratic time complexity, many ideas have been introduced to reduce its amortized time, or to quickly approximate it. One of the most cited approximate approaches is FastDTW. The FastDTW algorithm has well over a thousand citations and has been explicitly used in several hundred research efforts. In this work, we make a surprising claim. In any realistic data mining application, the approximate FastDTW is much slower than the exact DTW. This fact clearly has implications for the community that uses this algorithm: allowing it to address much larger datasets, get exact results, and do so in less time.}
}


@inproceedings{DBLP:conf/icde/Li0WLCF21,
	author = {Yanni Li and
                  Hui Li and
                  Zhi Wang and
                  Bing Liu and
                  Jiangtao Cui and
                  Hang Fei},
	title = {ESA-Stream: Efficient Self-Adaptive Online Data Stream Clustering
                  (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2329--2330},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00250},
	doi = {10.1109/ICDE51399.2021.00250},
	timestamp = {Wed, 07 Aug 2024 07:51:02 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Li0WLCF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With ever-increasing data streams from various applications such as smart phones, network monitoring, Internet of Things (IoT), etc., unsupervised clustering of data streams has become an important problem for machine learning and big data analysis. As data streams are data-intensive, temporally ordered, and rapidly evolving, efficiently and effectively online clustering of data streams presents a challenging problem [1] .}
}


@inproceedings{DBLP:conf/icde/MiaoMYS0YJ21,
	author = {Xupeng Miao and
                  Lingxiao Ma and
                  Zhi Yang and
                  Yingxia Shao and
                  Bin Cui and
                  Lele Yu and
                  Jiawei Jiang},
	title = {CuWide: Towards Efficient Flow-based Training for Sparse Wide Models
                  on GPUs (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2330--2331},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00251},
	doi = {10.1109/ICDE51399.2021.00251},
	timestamp = {Tue, 27 Aug 2024 17:30:57 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MiaoMYS0YJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose an efficient GPU-training framework for the large-scale wide models, named cuWide. To fully benefit from the memory hierarchy of GPU, cuWide applies a new flow-based schema for training, which leverages the spatial and temporal locality of wide models to drastically reduce the amount of communication with GPU global memory. Comprehensive experiments show that cuWide can be up to more than 20× faster than the state-of-the-art GPU solutions and multi-core CPU solutions.}
}


@inproceedings{DBLP:conf/icde/KeKHR21,
	author = {Xiangyu Ke and
                  Arijit Khan and
                  Mohammad Al Hasan and
                  Rojin Rezvansangsari},
	title = {Reliability Maximization in Uncertain Graphs (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2332--2333},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00252},
	doi = {10.1109/ICDE51399.2021.00252},
	timestamp = {Tue, 21 Mar 2023 20:50:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/KeKHR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network reliability measures the probability that a target node is reachable from a source node in an uncertain graph, i.e., a graph where every edge is associated with a probability of existence. In this paper, we investigate the novel and fundamental problem of adding a small number of edges in the uncertain network for maximizing the reliability between a given pair of nodes. We study the NP-hardness and the approximation hardness of our problem, and design effective, scalable solutions. Furthermore, we consider extended versions of our problem (e.g., multiple source and target nodes can be provided as input) to support and demonstrate a wider family of queries and applications, including sensor network reliability maximization and social influence maximization.}
}


@inproceedings{DBLP:conf/icde/LiuZXG21,
	author = {Qing Liu and
                  Ziyuan Zhu and
                  Jianliang Xu and
                  Yunjun Gao},
	title = {MaxiZone: Maximizing Influence Zone over Geo-Textual Data (Extended
                  Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2334--2335},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00253},
	doi = {10.1109/ICDE51399.2021.00253},
	timestamp = {Tue, 14 Mar 2023 18:13:10 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiuZXG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A reverse top-k keyword-based location query returns the influence zone for the query object. Given a specified query object q, the influence zone of q varies for different key-word sets. Users may be interested in identifying the maximum influence zone of the query object. To this end, we study the problem called MaxiZone that finds the keyword set maximizing the influence zone of a specified query object. The MaxiZone problem has many real-life applications, e.g., a business owner would like to identify the maximum influence zone so as to attract as many customers as possible. To address the MaxiZone problem, we propose three algorithms, including a basic algorithm, an index-centric algorithm together with a series of optimizations and a sampling-based algorithm. Extensive empirical study using real-world datasets demonstrates the effectiveness and efficiency of proposed algorithms.}
}


@inproceedings{DBLP:conf/icde/LiCXBCW21,
	author = {Guozhong Li and
                  Byron Choi and
                  Jianliang Xu and
                  Sourav S. Bhowmick and
                  Kwok{-}Pan Chun and
                  Grace Lai{-}Hung Wong},
	title = {Efficient Shapelet Discovery for Time Series Classification (Extended
                  Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2336--2337},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00254},
	doi = {10.1109/ICDE51399.2021.00254},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiCXBCW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-series shapelets are discriminative subsequences, recently found effective for time series classification (TSC). It is evident that the quality of shapelets is crucial to the accuracy of TSC. However, major research has focused on building accurate models from some shapelet candidates. To determine such candidates, existing studies are surprisingly simple, e.g., enumerating subsequences of some fixed lengths, or randomly selecting some subsequences as shapelet candidates. The major bulk of computation is then on building the model from the candidates. In this paper, we propose a novel efficient shapelet discovery method, called BSPCOVER, to discover a set of high-quality shapelet candidates for model building. We have conducted extensive experiments with well-known UCR time-series datasets and representative state-of-the-art methods. Results show that BSPCOVER speeds up the state-of-the-art methods by more than 70 times, and the accuracy is often comparable to or higher than existing works.}
}


@inproceedings{DBLP:conf/icde/JiangCXB21,
	author = {Jiaxin Jiang and
                  Byron Choi and
                  Jianliang Xu and
                  Sourav S. Bhowmick},
	title = {A Generic Ontology Framework for Indexing Keyword Search on Massive
                  Graphs (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2338--2339},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00255},
	doi = {10.1109/ICDE51399.2021.00255},
	timestamp = {Tue, 07 May 2024 20:05:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/JiangCXB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the unstructuredness and the lack of schema information of knowledge graphs, social networks and RDF graphs, keyword search has been proposed for querying such graphs/networks. Recently, various keyword search semantics have been designed. In this work, we propose a generic ontology-based indexing framework for keyword search, called Bisimulation of Generalized Graph Index (BiG-index), to enhance the search performance. Novelties of BiG-index reside in using an ontology graph G Ont to summarize and index a data graph G iteratively, to form a hierarchical index structure {\\mathbb{G}}\n. BiG-index is generic since it is applicable to keyword search algorithms that have two properties. BiG-index reduced the runtimes of popular keyword search work Blinks by 50.5% and r-clique by 29.5%.}
}


@inproceedings{DBLP:conf/icde/WijayantoWKC21,
	author = {Heri Wijayanto and
                  Wenlu Wang and
                  Wei{-}Shinn Ku and
                  Arbee L. P. Chen},
	title = {LShape Partitioning: Parallel Skyline Query Processing using MapReduce
                  (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2340--2341},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00256},
	doi = {10.1109/ICDE51399.2021.00256},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WijayantoWKC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this study, we propose two parallel skyline processing algorithms using a novel LShape partitioning strategy and an effective Propagation Filtering method. By extensive experiments, we verify that our algorithms outperformed the state-of-the-art approaches, especially for high-dimensional large scale datasets.}
}


@inproceedings{DBLP:conf/icde/Rasool0CL021,
	author = {Zafaryab Rasool and
                  Rui Zhou and
                  Lu Chen and
                  Chengfei Liu and
                  Jiajie Xu},
	title = {Index-based Solutions for Efficient Density Peak Clustering (Extended
                  Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2342--2343},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00257},
	doi = {10.1109/ICDE51399.2021.00257},
	timestamp = {Tue, 16 Aug 2022 09:19:44 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Rasool0CL021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clusters reflect a potential relationship among different entities of data. This data can be sourced from a wide range of domains like market research, spatial data analysis, etc. Many clustering algorithms have been developed in the last few decades in response to the proliferating demands across industries and organizations, which help them make operational and strategic decisions. Among them, density-based clustering algorithms are popular, which find subsets of objects in "dense regions" separated by not-so-dense regions, where each subset represents a cluster. In this paper, our focal point will be Density Peak Clustering (DPC) [1] , a popular approach towards obtaining density-based clusters.}
}


@inproceedings{DBLP:conf/icde/GeGM0W21,
	author = {Congcong Ge and
                  Yunjun Gao and
                  Xiaoye Miao and
                  Bin Yao and
                  Haobo Wang},
	title = {A Hybrid Data Cleaning Framework Using Markov Logic Networks (Extended
                  Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2344--2345},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00258},
	doi = {10.1109/ICDE51399.2021.00258},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GeGM0W21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the growth of dirty data, data cleaning turns into a crux of data analysis. In this paper, we propose a novel hybrid data cleaning framework, termed as MLNClean, which is capable of learning instantiated rules to supplement the insufficient integrity constraints. MLNClean consists of two steps, i.e., pre-processing and two-stage data cleaning. In the pre-processing step, MLNClean first infers a set of probable instantiated rules according to Markov logic network (MLN) and then builds a two-layer MLN index to generate multiple data versions and facilitate the cleaning process. In the two-stage data cleaning step, MLNClean first presents a concept of reliability score to clean errors within each data version separately, and then, it eliminates the conflict values among different data versions using a novel concept of fusion score. Considerable experimental results on both real and synthetic scenarios demonstrate the effectiveness of MLNClean.}
}


@inproceedings{DBLP:conf/icde/HuangHX21,
	author = {Jinbin Huang and
                  Xin Huang and
                  Jianliang Xu},
	title = {Truss-based Structural Diversity Search in Large Graphs (Extended
                  Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2346--2347},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00259},
	doi = {10.1109/ICDE51399.2021.00259},
	timestamp = {Sun, 02 Oct 2022 16:04:35 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangHX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Structural diversity, the multiplicity of social contexts inside an individual’s contact neighborhood, is shown to play an important role in the social contagion process. Existing models have limited decomposability for analyzing large-scale networks, which may suffer from the inaccurate reflection of social context diversity. In this paper, we propose a truss-based structural diversity model to address the limitations. We study the problem of top-r structural diversity search to find r vertices with the largest truss-based structural diversity scores in a graph. We propose two novel index structures of TSD-index and GCT-index, and efficient index-based query processing algorithms to solve the problem. Extensive experiments demonstrate the effectiveness and efficiency of our proposed model and algorithms, against state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/MiaoGCPYL21,
	author = {Xiaoye Miao and
                  Yunjun Gao and
                  Lu Chen and
                  Huanhuan Peng and
                  Jianwei Yin and
                  Qing Li},
	title = {Towards Query Pricing on Incomplete Data (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2348--2349},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00260},
	doi = {10.1109/ICDE51399.2021.00260},
	timestamp = {Mon, 05 Feb 2024 20:31:13 +0100},
	biburl = {https://dblp.org/rec/conf/icde/MiaoGCPYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As data markets have started to receive much attention from both industry and academia, how to price the tradable data is an indispensable problem. Pricing incomplete data is more practical and challenging, due to the pervasiveness of incomplete data. In this paper, we explore the pricing problem for queries over incomplete data. We propose a sophisticated pricing mechanism, termed as iDBPricer, which considers a series of essential factors, including the data contribution/usage, data completeness, and query quality. We present two novel price functions, namely, the usage and completeness-aware price function (UCA price for short) and the quality, usage, and completeness-aware price function (QUCA price for short). Moreover, we develop efficient algorithms for deriving the query prices. Extensive experiments using both real and benchmark datasets confirm the superiority of iDBPricer to the state-of-the-art price functions.}
}


@inproceedings{DBLP:conf/icde/KargarGSSZ21,
	author = {Mehdi Kargar and
                  Lukasz Golab and
                  Divesh Srivastava and
                  Jaroslaw Szlichta and
                  Morteza Zihayat},
	title = {Effective Keyword Search in Weighted Graphs (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2350--2351},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00261},
	doi = {10.1109/ICDE51399.2021.00261},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KargarGSSZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real graphs contain edge and node weights, representing penalty, distance or cost. We study the problem of keyword search in weighted node-labeled graphs, in which a query consists of a set of keywords and an answer is a subgraph. We consider three ranking strategies for answer subgraphs: edge weights, node weights, and a bi-objective combination of both node and edge weights. We propose and experimentally evaluate algorithms that optimize these objectives with an approximation ratio of two.}
}


@inproceedings{DBLP:conf/icde/LuZTW21,
	author = {Jing Lu and
                  Yuhai Zhao and
                  Kian{-}Lee Tan and
                  Zhengkui Wang},
	title = {Distributed Density Peaks Clustering Revisited (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2352--2353},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00262},
	doi = {10.1109/ICDE51399.2021.00262},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuZTW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Density Peaks (DP) Clustering organizes data into clusters by finding peaks in dense regions. This involves computing density (ρ) and distance (δ) of every point, and the time complexity is O(N 2 ) where N is the number of data points. In this paper, we propose a fast distributed density peaks clustering algorithm, FDDP, based on the z-value index. We also propose FC, an efficient algorithm that employs a forward computing strategy to calculate ρ linearly, and CB, which uses a caching and efficient searching strategy to compute δ. Our experimental results show that FDDP outperforms the state-of-the-art algorithms significantly.}
}


@inproceedings{DBLP:conf/icde/CaruccioDNP21,
	author = {Loredana Caruccio and
                  Vincenzo Deufemia and
                  Felix Naumann and
                  Giuseppe Polese},
	title = {Discovering Relaxed Functional Dependencies based on Multi-attribute
                  Dominance [Extended Abstract]},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2354--2355},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00263},
	doi = {10.1109/ICDE51399.2021.00263},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CaruccioDNP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To assess the quality of data it is useful to extract properties and relationships among them. However, exceptions and approximations need be considered in real-world settings. To this end, relaxed FDs (RFDs) are data dependencies accounting for both exceptions and similarities on data, but their discovery is an extremely complex problem, also due to the necessity of specifying similarity and validity thresholds. The RFD discovery algorithm presented in this paper exploits the concept of dominance to automatically derive similarity thresholds. The discovery performances and the effectiveness of the proposed algorithm are assessed through a comparative evaluation with state-of-art approaches.}
}


@inproceedings{DBLP:conf/icde/YeWZKZ0021,
	author = {Chen Ye and
                  Hongzhi Wang and
                  Kangjie Zheng and
                  YouKang Kong and
                  Rong Zhu and
                  Jing Gao and
                  Jianzhong Li},
	title = {Constrained Truth Discovery (Extended Abstract)},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2356--2357},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00264},
	doi = {10.1109/ICDE51399.2021.00264},
	timestamp = {Sun, 06 Oct 2024 21:04:59 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YeWZKZ0021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aggregating the information provided by multiple data sources, which is also known as information integration , plays an important role in data analytics. Since there often exists recording errors, intentional errors, conflicts and outdated data across different data sources, finding the true attribute values of each entity is a fundamental task of crucial importance [3] . The process to fulfill this task is called truth discovery , which has been extensively studied in the literature.}
}


@inproceedings{DBLP:conf/icde/PitouraSK21,
	author = {Evaggelia Pitoura and
                  Kostas Stefanidis and
                  Georgia Koutrika},
	title = {Fairness in Rankings and Recommenders: Models, Methods and Research
                  Directions},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2358--2361},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00265},
	doi = {10.1109/ICDE51399.2021.00265},
	timestamp = {Mon, 03 Jan 2022 22:33:27 +0100},
	biburl = {https://dblp.org/rec/conf/icde/PitouraSK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We increasingly depend on a variety of data-driven algorithmic systems to assist us in many aspects of life. Search engines and recommendation systems amongst others are used as sources of information and to help us in making all sort of decisions from selecting restaurants and books, to choosing friends and careers. This has given rise to important concerns regarding the fairness of such systems. This tutorial aims at presenting a toolkit of definitions, models and methods used for ensuring fairness in rankings and recommendations. Our objectives are three-fold: (a) to provide a solid framework on a novel, quickly evolving, and impactful domain, (b) to present related methods and put them into perspective, and (c) to highlight challenges and research paths for researchers and practitioners that work in data management and applications.}
}


@inproceedings{DBLP:conf/icde/BorattoM21,
	author = {Ludovico Boratto and
                  Mirko Marras},
	title = {Countering Bias in Personalized Rankings : From Data Engineering to
                  Algorithm Development},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2362--2364},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00266},
	doi = {10.1109/ICDE51399.2021.00266},
	timestamp = {Mon, 03 Jan 2022 22:33:27 +0100},
	biburl = {https://dblp.org/rec/conf/icde/BorattoM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This tutorial presents recent advances on the assessment and mitigation of data and algorithmic bias in personalized rankings. We first introduce fundamental concepts and definitions associated with bias issues, covering the state of the art and describing real-world examples of how bias can impact ranking algorithms from several perspectives (e.g., ethics and system’s objectives). Then, we continue with a systematic presentation of techniques to uncover, assess, and mitigate biases along the personalized ranking design process, with a focus on the role of data engineering in each step of the pipeline. Hands-on parts provide attendees with concrete implementations of bias mitigation algorithms, in addition to processes and guidelines on how data is organized and manipulated by these algorithms. The tutorial leverages open-source tools and public datasets, engaging attendees in designing bias countermeasures and in articulating impacts on stakeholders. We finally showcase open issues and future directions in this vibrant and rapidly evolving research area (Website: https://biasinrecsys.github.io/icde2021/).}
}


@inproceedings{DBLP:conf/icde/YanLCL21,
	author = {Zhengtong Yan and
                  Jiaheng Lu and
                  Naresh Chainani and
                  Chunbin Lin},
	title = {Workload-Aware Performance Tuning for Autonomous DBMSs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2365--2368},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00267},
	doi = {10.1109/ICDE51399.2021.00267},
	timestamp = {Thu, 14 Oct 2021 10:29:19 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YanLCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optimal configuration is vital for a DataBase Management System (DBMS) to achieve high performance. There is no one-size-fits-all configuration that works for different workloads since each workload has varying patterns with different resource requirements. There is a relationship between configuration, workload, and system performance. If a configuration cannot adapt to the dynamic changes of a workload, there could be a significant degradation in the overall performance of DBMS unless a sophisticated administrator is continuously re-configuring the DBMS. In this tutorial, we focus on autonomous workload-aware performance tuning, which is expected to automatically and continuously tune the configuration as the workload changes. We survey three research directions, including 1) workload classification, 2) workload forecasting, and 3) workload-based tuning. While the first two topics address the issue of obtaining accurate workload information, the third one tackles the problem of how to properly use the workload information to optimize performance. We also identify research challenges and open problems, and give real-world examples about leveraging workload information for database tuning in commercial products (e.g., Amazon Redshift). We will demonstrate workload-aware performance tuning in Amazon Redshift in the presentation.}
}


@inproceedings{DBLP:conf/icde/EchihabiZP21,
	author = {Karima Echihabi and
                  Kostas Zoumpatianos and
                  Themis Palpanas},
	title = {High-Dimensional Similarity Search for Scalable Data Science},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2369--2372},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00268},
	doi = {10.1109/ICDE51399.2021.00268},
	timestamp = {Thu, 23 Jun 2022 19:56:00 +0200},
	biburl = {https://dblp.org/rec/conf/icde/EchihabiZP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Similarity search is a core operation of many critical data science applications, involving massive collections of high-dimensional objects. Similarity search finds objects in a collection close to a given query according to some definition of sameness. Objects can be data series, text, multimedia, graphs, database tables or deep network embeddings. In this tutorial, we revisit the similarity search problem in light of the recent advances in the field and the new big data landscape. We discuss key data science applications that require efficient high-dimensional similarity search, we survey the state-of-the-art high-dimensional similarity search approaches and share surprising insights about their strengths and weaknesses, and we discuss the challenges and open research problems in this area.}
}


@inproceedings{DBLP:conf/icde/PanseN21,
	author = {Fabian Panse and
                  Felix Naumann},
	title = {Evaluation of Duplicate Detection Algorithms: From Quality Measures
                  to Test Data Generation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2373--2376},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00269},
	doi = {10.1109/ICDE51399.2021.00269},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PanseN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Duplicate detection identifies multiple records in a dataset that represent the same real-world object. Many such approaches exist, both in research and in industry. To investigate essential properties of duplicate detection algorithms, such as their result quality or runtime behavior, they must be executed on suitable test data. The quality evaluation requires that these test data are labeled, constituting a ground truth. Correctly labeled, sizable, and real or at least realistic test datasets, however, are not easy to obtain, creating an obstacle for the advancement of research. In this tutorial, we present common methods to evaluate duplicate detection algorithms and to generate labeled test data. We close with a discussion of open problems.}
}


@inproceedings{DBLP:conf/icde/MauererS21,
	author = {Wolfgang Mauerer and
                  Stefanie Scherzinger},
	title = {Nullius in Verba: Reproducibility for Database Systems Research, Revisited},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2377--2380},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00270},
	doi = {10.1109/ICDE51399.2021.00270},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MauererS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the last decade, reproducibility of experimental results has been a prime focus in database systems research, and many high-profile conferences award results that can be independently verified. Since database systems research involves complex software stacks that non-trivially interact with hardware, sharing experimental setups is anything but trivial: Building a working reproduction package goes far beyond providing a DOI to some repository hosting data, code, and setup instructions.This tutorial revisits reproducible engineering in the face of state-of-the-art technology, and best practices gained in other computer science research communities. In particular, in the hands-on part, we demonstrate how to package entire system software stacks for dissemination. To ascertain long-term reproducibility over decades (or ideally, forever), we discuss why relying on open source technologies massively employed in industry has essential advantages over approaches crafted specifically for research. Supplementary material shows how version control systems that allow for non-linearly rewriting recorded history can document the structured genesis behind experimental setups in a way that is substantially easier to understand, without involvement of the original authors, compared to detour-ridden, strictly historic evolution.}
}


@inproceedings{DBLP:conf/icde/MengA21,
	author = {Xiao Meng and
                  G{\"{u}}nes Alu{\c{c}}},
	title = {Exploratory Data Analysis in {SAP} {IQ} Using Query-Time Sampling},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2381--2386},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00271},
	doi = {10.1109/ICDE51399.2021.00271},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MengA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As businesses continue to consume and produce ever-growing volumes of data, exploratory data analysis (EDA) is becoming an integral part of everyday operations. While online analytical processing (OLAP) systems in general – and column-oriented relational database management systems (RDBMS) in particular – are equipped with powerful tools to plough through petabytes of data, analytical queries may take seconds to execute, which is not always desirable in exploratory data analysis. Data scientists often need tools for fast visualization of data, and they are interested in identifying subsets of data that need further drilling-down before running computationally expensive analytical functions. In this paper, we describe our early work on extending SAP IQ (a disk-based columnar RDBMS) to support approximate query processing for exploratory data analysis using a technique known as query-time sampling. Specifically, we introduce two classes of novel samplers: (i) a stratified sampler with randomized row access to address the early-row bias problem in sampling, and (ii) hash-based equi-join samplers that are outlier-aware. We demonstrate how SAP IQ’s polymorphic table function (PTF) technology can be utilized to implement these samplers as new query plan operators.}
}


@inproceedings{DBLP:conf/icde/WangHTLLGJJ21,
	author = {Bo Wang and
                  Zhenyu Hou and
                  Yangyu Tao and
                  Yifeng Lu and
                  Chao Li and
                  Tao Guan and
                  Xiaowei Jiang and
                  Jinlei Jiang},
	title = {Swift: Reliable and Low-Latency Data Processing at Cloud Scale},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2387--2398},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00272},
	doi = {10.1109/ICDE51399.2021.00272},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangHTLLGJJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, it is a rapidly rising demand yet challenging issue to run large-scale applications on shared infrastructures such as data centers and clouds with low execution latency and high resource utilization. This paper reports our experience with Swift, a system capable of efficiently running real-time and interactive data processing jobs at cloud scale. Taking directed acyclic graph (DAG) as the job model, Swift achieves the design goal by three new mechanisms: 1) fine-grained scheduling that can efficiently partition a job into graphlets (i.e., sub-graphs) based on new shuffle heuristics and that does scheduling in the unit of graphlet, thus avoiding resource fragmentation and waste, 2) adaptive memory-based in-network shuffling that reduces IO overhead and data transfer time by doing shuffle in memory and allowing jobs to select the most efficient way to fulfill shuffling, and 3) lightweight fault tolerance and recovery that only prolong the whole job execution time slightly with the help of timely failure detection and fine-grained failure recovery. Experimental results show that Swift can achieve an average speedup of 2.11× on TPC-H, and 14.18× on Terasort when compared with Spark. Swift has been deployed in production, supporting as many as 140,000 executors and processing millions of jobs per day. Experiments with production traces show that Swift outperforms JetScope and Bubble Execution by 2.44× and 1.23× respectively.}
}


@inproceedings{DBLP:conf/icde/FloratosGSCZ21,
	author = {Sofoklis Floratos and
                  Ahmad Ghazal and
                  Jason Sun and
                  Jianjun Chen and
                  Xiaodong Zhang},
	title = {DBSpinner: Making a Case for Iterative Processing in Databases},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2399--2410},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00273},
	doi = {10.1109/ICDE51399.2021.00273},
	timestamp = {Wed, 20 Oct 2021 13:39:18 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FloratosGSCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relational database management systems (RDBMS) have limited iterative processing support. Recursive queries were added to ANSI SQL, however, their semantics do not allow aggregation functions, which disqualifies their use for several applications, such as PageRank and shortest path computations. Recently, another SQL extension, iterative Common Table Expressions (CTEs), is proposed to enable users to perform general iterative computations on RDBMSs.In this work 1 , we demonstrate how iterative CTEs can be efficiently incorporated into a production RDBMS without major intrusion to the system. We have prototyped our approach on Futurewei’s MPPDB, a shared nothing relational parallel database engine. The implementation is based on a functional rewrite that translates iterative CTEs to other existing SQL operators. Thus, query plans of iterative CTEs can be optimized and executed by the engine with minimal modification to the code base. We have also applied several optimizations specifically for iterative CTEs to i) minimize data movement, ii) reuse results that remain constant and iii) push down predicates to avoid unnecessary data processing. We verified our implementation through extensive experimental evaluation using real world datasets and queries. The results show the feasibility of the rewrite approach and the effectiveness of the optimizations, which improve performance by an order of magnitude in some cases.}
}


@inproceedings{DBLP:conf/icde/Chu00STL21,
	author = {Guojun Chu and
                  Jingyu Wang and
                  Qi Qi and
                  Haifeng Sun and
                  Shimin Tao and
                  Jianxin Liao},
	title = {Prefix-Graph: {A} Versatile Log Parsing Approach Merging Prefix Tree
                  with Probabilistic Graph},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2411--2422},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00274},
	doi = {10.1109/ICDE51399.2021.00274},
	timestamp = {Sun, 12 Nov 2023 02:08:10 +0100},
	biburl = {https://dblp.org/rec/conf/icde/Chu00STL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Logs play an important part in analyzing system behavior and diagnosing system failures. As the basic step of log analysis, log parsing converts raw log messages into structured log templates. However, existing log parsing approaches are not adaptive and versatile enough to ensure their high accuracy on all types of datasets. In particular, it is required to design regular expressions or fine-tune the hyper-parameters manually for the best performance. In this paper, we propose Prefix-Graph, an online versatile log parsing approach. Prefix-Graph is a probabilistic graph structure extended from prefix tree. It iteratively merges together two branches which have high similarity in probability distribution, and represents log templates as the combination of cut-edges in root-to-leaf paths of the graph. Since no domain knowledge is used and all the parameters are fixed, Prefix-Graph can be easily applied to different log datasets without any additional manual work. We evaluate our approach on 10 real-world datasets and 117GB log messages obtained from Huawei. The experimental results demonstrate that Prefix-Graph achieves the highest average accuracy of 0.975 and the smallest standard deviation of 0.037. Our approach is superior to baseline methods in terms of adaptability and versatility.}
}


@inproceedings{DBLP:conf/icde/Jindal0SP21,
	author = {Alekh Jindal and
                  Shi Qiao and
                  Rathijit Sen and
                  Hiren Patel},
	title = {Microlearner: {A} fine-grained Learning Optimizer for Big Data Workloads
                  at Microsoft},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2423--2434},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00275},
	doi = {10.1109/ICDE51399.2021.00275},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Jindal0SP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Big data systems have become increasingly complex making the job of a query optimizer incredibly difficult. This is due to more complicated decision making, more complex query plans seen, and more tedious objective functions in cloud-based big data workloads. As a result, production cloud query optimizers are often far from optimal. In this paper, we describe building a learning query optimizer for big data workloads at Microsoft. We make four major contributions. First, we describe the challenges in cloud query optimizers based on our observations from the big data workloads at Microsoft. Second, we discuss what makes machine learning an attractive approach to aid the big data query optimizers in decision making. Third, we present Microlearner, a practical approach to characterize large cloud workloads into smaller subsets and build micromodels over each subset to tame the complexity of big data workloads And finally, we describe the productization of Microlearner, using learned cardinality as a concrete example, via performance results over very large production workloads and illustrating the various challenges involved in deployment.}
}


@inproceedings{DBLP:conf/icde/QiuZZWXXLY21,
	author = {Yiming Qiu and
                  Kang Zhang and
                  Han Zhang and
                  Songlin Wang and
                  Sulong Xu and
                  Yun Xiao and
                  Bo Long and
                  Wen{-}Yun Yang},
	title = {Query Rewriting via Cycle-Consistent Translation for E-Commerce Search},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2435--2446},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00276},
	doi = {10.1109/ICDE51399.2021.00276},
	timestamp = {Mon, 26 Jun 2023 20:41:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/QiuZZWXXLY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays e-commerce search has become an integral part of many people’s shopping routines. One critical challenge in today’s e-commerce search is the semantic matching problem where the relevant items may not contain the exact terms in the user query. In this paper, we propose a novel deep neural network based approach to query rewriting, in order to tackle this problem. Specifically, we formulate query rewriting into a cyclic machine translation problem to leverage abundant click log data. Then we introduce a novel cyclic consistent training algorithm in conjunction with state-of-the-art machine translation models to achieve the optimal performance in terms of query rewriting accuracy. In order to make it practical in industrial scenarios, we optimize the syntax tree construction to reduce computational cost and online serving latency. Offline experiments show that the proposed method is able to rewrite hard user queries into more standard queries that are more appropriate for the inverted index to retrieve. Comparing with human curated rule-based method, the proposed model significantly improves query rewriting diversity while maintaining good relevancy. Online A/B experiments show that it improves core e-commerce business metrics significantly. Since the summer of 2020, the proposed model has been launched into our search engine production, serving hundreds of millions of users.}
}


@inproceedings{DBLP:conf/icde/KersbergenS21,
	author = {Barrie Kersbergen and
                  Sebastian Schelter},
	title = {Learnings from a Retail Recommendation System on Billions of Interactions
                  at bol.com},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2447--2452},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00277},
	doi = {10.1109/ICDE51399.2021.00277},
	timestamp = {Tue, 21 Mar 2023 20:50:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/KersbergenS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems are ubiquitous in the modern internet, where they help users find items they might like. We discuss the design of a large-scale recommender system handling billions of interactions on a European e-commerce platform.We present two studies on enhancing the predictive performance of this system with both algorithmic and systems-related approaches. First, we evaluate neural network-based approaches on proprietary data from our e-commerce platform, and confirm recent results outlining that the benefits of these methods with respect to predictive performance are limited, while they exhibit severe scalability bottlenecks. Next, we investigate the impact of a reduction of the response latency of our serving system, and conduct an A/B test on the live platform with more than 19 million user sessions, which confirms that the latency reduction of the recommender system correlates with a significant increase in business-relevant metrics. We discuss the implications of our findings with respect to real world recommendation systems and future research on scalable session-based recommendation.}
}


@inproceedings{DBLP:conf/icde/XiaoJTLXXY21,
	author = {Zhuojian Xiao and
                  Yunjiang Jiang and
                  Guoyu Tang and
                  Lin Liu and
                  Sulong Xu and
                  Yun Xiao and
                  Weipeng Yan},
	title = {Adversarial Mixture Of Experts with Category Hierarchy Soft Constraint},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2453--2463},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00278},
	doi = {10.1109/ICDE51399.2021.00278},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XiaoJTLXXY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Product search is the most common way for people to satisfy their shopping needs on e-commerce websites. Products are typically annotated with one of several broad categorical tags, such as "Clothing" or "Electronics", as well as finer-grained categories like "Refrigerator" or "TV", both under "Electronics". These tags are used to construct a hierarchy of query categories. Distributions of features such as price and brand popularity vary wildly across query categories. In addition, feature importance for the purpose of CTR/CVR predictions differs from one category to another. In this work, we leverage the Mixture of Expert (MoE) framework to learn a ranking model that specializes for each query category. In particular, our gate network relies solely on the category ids extracted from the user query.While classical MoE’s pick expert towers spontaneously for each input example, we explore two techniques to establish more explicit and transparent connections between the experts and query categories. To help differentiate experts on their domain specialties, we introduce a form of adversarial regularization among the expert outputs, forcing them to disagree with one another. As a result, they tend to approach each prediction problem from different angles, rather than copying one another. This is validated by a much stronger clustering effect of the gate output vectors under different categories. In addition, soft gating constraints based on the categorical hierarchy are imposed to help similar products choose similar gate values. and make them more likely to share similar experts. This allows aggregation of training data among smaller sibling categories to overcome data scarcity.Experiments on a learning-to-rank dataset collected from the JD e-commerce search log demonstrate that MoE with these improvements consistently outperforms competing models, in terms of offline metrics and online AB tests.}
}


@inproceedings{DBLP:conf/icde/Xie0YYGO021,
	author = {Xu Xie and
                  Fei Sun and
                  Xiaoyong Yang and
                  Zhao Yang and
                  Jinyang Gao and
                  Wenwu Ou and
                  Bin Cui},
	title = {Explore User Neighborhood for Real-time E-commerce Recommendation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2464--2475},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00279},
	doi = {10.1109/ICDE51399.2021.00279},
	timestamp = {Sat, 09 Apr 2022 12:45:21 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Xie0YYGO021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems play a vital role in modern online services, such as Amazon and Taobao. Traditional personalized methods, which focus on user-item (UI) relations, have been widely applied in industrial settings, owing to their efficiency and effectiveness. Despite their success, we argue that these approaches ignore local information hidden in similar users. To tackle this problem, user-based methods exploit similar user relations to make recommendations in a local perspective. Nevertheless, traditional user-based methods, like userKNN and matrix factorization, are intractable to be deployed in the real-time applications since such transductive models have to be recomputed or retrained with any new interaction. To overcome this challenge, we propose a framework called self-complementary collaborative filtering (SCCF) which can make recommendations with both global and local information in real time. On the one hand, it utilizes UI relations and user neighborhood to capture both global and local information. On the other hand, it can identify similar users for each user in real time by inferring user representations on the fly with an inductive model. The proposed framework can be seamlessly incorporated into existing inductive UI approach and benefit from user neighborhood with little additional computation. It is also the first attempt to apply user-based methods in real-time settings. The effectiveness and efficiency of SCCF are demonstrated through extensive offline experiments on four public datasets, as well as a large scale online A/B test in Taobao.}
}


@inproceedings{DBLP:conf/icde/ZhangWYWZC21,
	author = {Wen Zhang and
                  Chi Man Wong and
                  Ganqiang Ye and
                  Bo Wen and
                  Wei Zhang and
                  Huajun Chen},
	title = {Billion-scale Pre-trained E-commerce Product Knowledge Graph Model},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2476--2487},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00280},
	doi = {10.1109/ICDE51399.2021.00280},
	timestamp = {Tue, 14 Jun 2022 15:15:02 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangWYWZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, knowledge graphs have been widely applied to organize data in a uniform way and enhance many tasks that require knowledge, for example, online shopping which has greatly facilitated people’s life. As a backbone for online shopping platforms, we built a billion-scale e-commerce product knowledge graph for various item knowledge services such as item recommendation. However, such knowledge services usually include tedious data selection and model design for knowledge infusion, which might bring inappropriate results. Thus, to avoid this problem, we propose a Pre-trained Knowledge Graph Model (PKGM) for our billion-scale e-commerce product knowledge graph, providing item knowledge services in a uniform way for embedding-based models without accessing triple data in the knowledge graph. Notably, PKGM could also complete knowledge graphs during servicing, thereby overcoming the common incompleteness issue in knowledge graphs. We test PKGM in three knowledge-related tasks including item classification, same item identification, and recommendation. Experimental results show PKGM successfully improves the performance of each task.}
}


@inproceedings{DBLP:conf/icde/HuangZY21,
	author = {Chao Huang and
                  Jiashu Zhao and
                  Dawei Yin},
	title = {Purchase Intent Forecasting with Convolutional Hierarchical Transformer
                  Networks},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2488--2498},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00281},
	doi = {10.1109/ICDE51399.2021.00281},
	timestamp = {Thu, 02 Mar 2023 20:19:45 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HuangZY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Purchase intent forecasting, which aims to model user consumption behavior over different categories of items, plays a key role in many services, like online retailing systems, computational advertising and personalized recommendations. While the recently emerged deep neural network models (e.g., recurrent neural network, or attention mechanism) have been proposed to understand user’s sequential behavior, we argue that the successes of these methods is largely rely on the data sufficiency. However, the practical purchase forecasting scenarios involve highly sparse data distributions across categories and time. In such cases, one has to deal with the data imbalance problem in order to encode the complex patterns of user purchase behaviors. To tackle this challenge, we develop a Convolutional Hierarchical TRansformer networks (CHTR), to enable the purchase pattern modeling with the multi-grained temporal dynamics, so as to alleviate the data imbalance issue. In our CHTR framework, we develop a multi-grained hierarchical transformer network, to make the learned behavior embeddings be reflective of the multi-level relational structures. Then, a dependency modeling component is proposed to aggregate the multi-relational context signals and capture the underlying dependent structures. Our experiments on real-world datasets show the significant improvements obtained by CHTR over different types of alternative methods.}
}


@inproceedings{DBLP:conf/icde/XinLZLZBZ21,
	author = {Shen Xin and
                  Zhao Li and
                  Pengcheng Zou and
                  Cheng Long and
                  Jie Zhang and
                  Jiajun Bu and
                  Jingren Zhou},
	title = {{ATNN:} Adversarial Two-Tower Neural Network for New Item's Popularity
                  Prediction in E-commerce},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2499--2510},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00282},
	doi = {10.1109/ICDE51399.2021.00282},
	timestamp = {Tue, 22 Oct 2024 20:38:20 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XinLZLZBZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The e-commerce era is witnessing rising new arrivals of items on e-commerce platforms every day. Identifying potential popular items accurately is of great importance in creating commercial value. Click-Through Rate (CTR) is a general indicator to evaluate item popularity. However, existing methods fail in new arrivals prediction because of sparse item features, missing item statistics and high time complexity of computing for all pairs of users and items. To tackle these challenges, we propose a novel Adversarial Two-tower Neural Network (ATNN) model for new arrivals CTR predictions by introducing an adversarial network to a two-tower network. We design a generator and a discriminator to better learn an item vector based on item profiles without item statistics. We also develop a strategy with an \\mathcal{O}(1)\ntime complexity for a new item’s popularity prediction by constructing a user group and utilizing its mean user vector in a time-efficient manner. We implement ATNN on a large-scale real-world dataset from one of the world’s largest e-commerce platforms, "Tmall.com". Empirical results show that ATNN is strongly capable of learning item vectors from item profiles for e-commerce. Furthermore, by introducing multi-task learning technology, we extend ATNN to food delivery service. Experimental results on one popular food delivery platform, "Ele.me", demonstrate that ATNN can recognize attractive and welcoming new restaurants that have higher Value per Page View (VpPV) and generate more Gross Merchandise Volume (GMV).}
}


@inproceedings{DBLP:conf/icde/LiLYWLWLZ21,
	author = {Xijun Li and
                  Weilin Luo and
                  Mingxuan Yuan and
                  Jun Wang and
                  Jiawen Lu and
                  Jie Wang and
                  Jinhu L{\"{u}} and
                  Jia Zeng},
	title = {Learning to Optimize Industry-Scale Dynamic Pickup and Delivery Problems},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2511--2522},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00283},
	doi = {10.1109/ICDE51399.2021.00283},
	timestamp = {Sat, 30 Sep 2023 09:44:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiLYWLWLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Dynamic Pickup and Delivery Problem (DPDP) is aimed at dynamically scheduling vehicles among multiple sites in order to minimize the cost when delivery orders are not known a priori. Although DPDP plays an important role in modern logistics and supply chain management, state-of-the-art DPDP algorithms are still limited on their solution quality and efficiency. In practice, they fail to provide a scalable solution as the numbers of vehicles and sites become large. In this paper, we propose a data-driven approach, Spatial-Temporal Aided Double Deep Graph Network (ST-DDGN), to solve industry-scale DPDP. In our method, the delivery demands are first forecast using spatial-temporal prediction method, which guides the neural network to perceive spatial-temporal distribution of delivery demand when dispatching vehicles. Besides, the relationships of individuals such as vehicles are modelled by establishing a graph-based value function. ST-DDGN incorporates attention-based graph embedding with Double DQN (DDQN). As such, it can make the inference across vehicles more efficiently compared with traditional methods. Our method is entirely data driven and thus adaptive, i.e., the relational representation of adjacent vehicles can be learned and corrected by ST-DDGN from data periodically. We have conducted extensive experiments over real-world data to evaluate our solution. The results show that ST-DDGN reduces 11.27% number of the used vehicles and decreases 13.12% total transportation cost on average over the strong baselines, including the heuristic algorithm deployed in our UAT (User Acceptance Test) environment and a variety of vanilla DRL methods. We are due to fully deploy our solution into our online logistics system and it is estimated that millions of USD logistics cost can be saved per year.}
}


@inproceedings{DBLP:conf/icde/Constantinou0ZC21,
	author = {Soteris Constantinou and
                  Andreas Konstantinidis and
                  Demetrios Zeinalipour{-}Yazti and
                  Panos K. Chrysanthis},
	title = {The IoT Meta-Control Firewall},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2523--2534},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00284},
	doi = {10.1109/ICDE51399.2021.00284},
	timestamp = {Sat, 09 Apr 2022 12:45:21 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Constantinou0ZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) devices have penetrated massively into smart environments (e.g., smart-homes, smart-cars or more generally smart-anything). Besides data collection, many IoT devices also enable the execution of Rule Automation Workflows (RAW), which span from simple predicate statements to procedural workflows capturing a smart actuation pipeline. RAW aim to meet the convenience (comfort) level of users under specific conditions (e.g., raise room temperature to 22 C if cold), but unfortunately cannot express long-term objectives of users (e.g., consume less than 400 kWh in December). In this paper, we present an innovative system, coined IoT Meta-Control Firewall (IMCF), which internally deploys an AI-inspired Energy-Planner (EP) algorithm that exploits domain-specific operators to balance the trade-off between convenience and energy consumption in satisfying the RAW pipelines of users. IMCF filters the RAW pipelines in a way that these do not conflict with the long-term objectives of users (like a network firewall). Our experimental evaluation with extensive real traces from an apartment, a house, and campus dorms shows that IMCF achieves very high levels of user convenience while remaining within the target energy consumption budgets expressed by users.}
}


@inproceedings{DBLP:conf/icde/BandilGCAHGCS21,
	author = {Ayush Bandil and
                  Vaishali Girdhar and
                  Hieu Chau and
                  Mohamed Ali and
                  Abdeltawab M. Hendawi and
                  Harsh Govind and
                  Peiwei Cao and
                  Ashley Song},
	title = {GeoDart: {A} System for Discovering Maps Discrepancies},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2535--2546},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00285},
	doi = {10.1109/ICDE51399.2021.00285},
	timestamp = {Thu, 28 Sep 2023 16:41:28 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BandilGCAHGCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Map service providers are working hard to maintain high-quality maps service to more than three billion digital maps users. As each provider presents its unique routing engine and road network graph (RNG) mapping techniques, inconsistencies in services provided are inevitable. These inconsistencies may be of two types- (1) inconsistencies in RNG, including missing or shifted road segments, missing turn restriction, or mislabeled road attributes; or (2) inconsistencies in routing service arising from the unique routing algorithm (RA). Discovering those inconsistencies would improve the routing services efficiency. This paper presents a system, named GeoDart, that compares publicly available routing data from the APIs of Bing Maps, Google Maps, and OpenStreetMaps (OSM) to automatically discover discrepancies. The system categorizes the detected discrepancies based on (1) routing data such as distance, duration, and route geometry, (2) the attributes of the road segments, and (3) the connectivity and turn restrictions of the RNG. Equipped with an ensemble of Multi-Layer Perception (MLP) and Support Vector Machine Classifiers (SVC), GeoDart can efficiently discover and classify maps discrepancies. Through its graphical interface, the GeoDart system enables users such as professional editors and cartographers to visually inspect, identify, and correct map discrepancies mutually across the three engines.}
}


@inproceedings{DBLP:conf/icde/SchoemansSZ21,
	author = {Maxime Schoemans and
                  Mahmoud Attia Sakr and
                  Esteban Zim{\'{a}}nyi},
	title = {Implementing Rigid Temporal Geometries in Moving Object Databases},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2547--2558},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00286},
	doi = {10.1109/ICDE51399.2021.00286},
	timestamp = {Sun, 02 Oct 2022 16:04:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SchoemansSZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Various applications process geospatial trajectories of moving objects, such as cars, ships and robots. There is thus a need for a common conceptual framework to model and manage these objects, as well as to enable data interoperability across tools. The International Organization for Standardization ISO® has responded to this need and created the standard ISO 19141– Schema for moving features. Among its types, it defines a schema for rigid temporal geometries, which represent the movement of spatial objects translating and rotating over time, while preserving a fixed shape. Despite the abundance of these objects in real-world, there exists no reference implementation of this type of data in a common system, which causes them to usually be represented as temporal points without taking into account their spatial extents and shapes. In this paper, we aim to provide an implementation of rigid temporal geometries into MobilityDB, an open-source moving object database, that extends PostgreSQL and PostGIS. We provide a data model for rigid temporal geometries and propose efficient algorithms for the operations defined in ISO 19141. A use case on real AIS ship trajectories is illustrated to validate the proposed implementation. A synthetic data generator for temporal geometries is also proposed. Finally, we review the standard from an implementation point of view and provide insights on possible improvements.}
}


@inproceedings{DBLP:conf/icde/YangCHCCZZ021,
	author = {Minghui Yang and
                  Shaosheng Cao and
                  Binbin Hu and
                  Xianling Chen and
                  Hengbin Cui and
                  Zhiqiang Zhang and
                  Jun Zhou and
                  Xiaolong Li},
	title = {IntelliTag: An Intelligent Cloud Customer Service System Based on
                  Tag Recommendation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2559--2570},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00287},
	doi = {10.1109/ICDE51399.2021.00287},
	timestamp = {Sun, 02 Oct 2022 16:04:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YangCHCCZZ021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To reduce the customer service pressure of small and medium-sized enterprises, we propose an intelligent cloud customer service system, called IntelliTag. Unlike traditional customer service, a cloud service based system has difficulty in collecting user personal information. Therefore, we add a tag recommendation function to quickly capture the user’s question intent by clicking on the tags. Specifically, IntelliTag is elaborately designed with the consideration of the following three aspects. First, how to mine high-quality tags is a challenging problem. Second, in the tag recommendation tasks, we have multifarious data types and relations that are used to build a sequential recommendation model. Finally, system implementation and deployment also need to be carefully designed to satisfy online service requirements. In this paper, we show the details of data construction, model designs, system implementation and deployment, and the empirical results compared with several state-of-the-art methods. Nowadays, our IntelliTag has already supported hundreds of thousands of enterprises and millions of users in our industrial production environment.}
}


@inproceedings{DBLP:conf/icde/ShiLCZCFCZLL21,
	author = {Rui Shi and
                  Yang Liu and
                  Jianjun Chen and
                  Xuan Zou and
                  Yanbin Chen and
                  Minghua Fan and
                  Zhihao Cai and
                  Guanghui Zhang and
                  Zhiwen Li and
                  Yuming Liang},
	title = {{IPS:} Unified Profile Management for Ubiquitous Online Recommendations},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2571--2582},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00288},
	doi = {10.1109/ICDE51399.2021.00288},
	timestamp = {Thu, 02 Sep 2021 14:46:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ShiLCZCFCZLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {ByteDance offers several massively popular products such as TikTok, Jinri Toutiao and Douyin for creating, sharing and discovering a variety of content, in which recommendation plays an indispensable role for helping billions of users to interact with highly personalized content. The personalized experience in products largely comes from the ability of sophisticated machine learning models to make accurate predictions based on users’ interests and one key component in such systems is the user profile service.In this paper, we introduce Instance Profile Service (IPS), a large scale distributed system for managing unstructured profile data as well as serving various feature computations at ByteDance. Different products leverage IPS in many different ways and place various demands on the system, in terms of complex computation logic and latency requirements. One major challenge in the design of a large scale user profile system is how to strike the right balance among efficiency, scalability, reliability and versatility. With deliberated choices made on its design and implementation, we demonstrate IPS can provide a simple yet flexible solution to all these products while meeting the targeted high availability and performance goals. At ByteDance, IPS has successfully replaced many legacy profile systems and runs on thousands of machines. One of our largest production instances can process a hundred million feature queries and tens of millions writes per second.}
}


@inproceedings{DBLP:conf/icde/Hu0ZJYLWCH021,
	author = {Sihao Hu and
                  Xuhong Zhang and
                  Junfeng Zhou and
                  Shouling Ji and
                  Jiaqi Yuan and
                  Zhao Li and
                  Zhipeng Wang and
                  Qi Chen and
                  Qinming He and
                  Liming Fang},
	title = {Turbo: Fraud Detection in Deposit-free Leasing Service via Real-Time
                  Behavior Network Mining},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2583--2594},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00289},
	doi = {10.1109/ICDE51399.2021.00289},
	timestamp = {Tue, 14 Mar 2023 13:13:47 +0100},
	biburl = {https://dblp.org/rec/conf/icde/Hu0ZJYLWCH021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online deposit-free leasing service has witnessed rapid growth in China and shows a promising market in the future. While eliminating the requirement of a deposit does attract more users to the service, it also lowers the cost for fraudsters. Since the emergence of this service is relatively new, there are few works in literature focusing on detecting fraud transactions in it. Existing efforts mainly fall into hard-coded solutions such as block-listing or scorecard methods, which can be impotent in the face of the diverse fraud tactics, e.g., identity theft, or even suffering concept drift problem as the tactics evolve. In this paper, we contribute Turbo, an efficient graph-based anti-fraud system, to fully exploit the abundant user behavior logs in a real-time manner. Turbo is able to additionally make use of the implicit user relationships beyond the user features in the logs. To capture the user relationships, we first propose a novel algorithm to construct a time-evolving user behavior network called BN. Empirical analysis demonstrates that fraudsters in BN exhibit unique temporal aggregation and homophilic patterns, which inspires us to develop a novel heterogeneous adaptive graph neural network algorithm called HAG. Specifically, in HAG two graph operators are presented to mitigate the over-smoothing problem and make better use of the heterogeneous behavior relations in BN. Extensive experiments on a real-world dataset show that our method outperforms state-of-the-art methods significantly and can give a response in seconds for each detection request.}
}


@inproceedings{DBLP:conf/icde/LiLHZWLZ21,
	author = {Jingdong Li and
                  Zhao Li and
                  Jiaming Huang and
                  Ji Zhang and
                  Xiaoling Wang and
                  Xingjian Lu and
                  Jingren Zhou},
	title = {Large-scale Fake Click Detection for E-commerce Recommendation Systems},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2595--2606},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00290},
	doi = {10.1109/ICDE51399.2021.00290},
	timestamp = {Fri, 02 Sep 2022 08:42:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiLHZWLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of e-commerce platforms, e-commerce recommendation systems are playing an increasingly important role for the purpose of product recommendation. As a new attack model against e-commerce recommendation systems, the "Ride Item\'s Coattails" attack creates fake click information to establish the deceptive correlation between popular products and low-quality products in order to mislead the recommendation system of e-commerce platform to boost the sales of low-quality products. This attack is characterized by high concealment and strong destructiveness, which can cause great damage to e-commerce recommendation systems, and adversely affect the usability of the e-commerce platform and users\' shopping experience. It is therefore of great practical significance to study how to quickly and effectively identify the false click information and the corresponding "Ride Item\'s Coattails" attack to better safeguard e-commerce recommendation systems. At present, there is no previously reported relevant research work conducted specifically for addressing the detection of the "Ride Item\'s Coattails" attack. In this work, we carried out pioneering work in analyzing and summarizing the characteristics of the false click information produced by attackers on the target products in the "Ride Item\'s Coattails" attack and designed a set of attack detection techniques suitable for e-commerce recommendation systems. Experimental results on real e-commerce datasets show that our proposed techniques can quickly and effectively detect the large-scale fake click information as well as the associated "Ride Item\'s Coattails" attack in e-commerce recommendation systems.}
}


@inproceedings{DBLP:conf/icde/WongFZVCZHCZC21,
	author = {Chi{-}Man Wong and
                  Fan Feng and
                  Wen Zhang and
                  Chi{-}Man Vong and
                  Hui Chen and
                  Yichi Zhang and
                  Peng He and
                  Huan Chen and
                  Kun Zhao and
                  Huajun Chen},
	title = {Improving Conversational Recommender System by Pretraining Billion-scale
                  Knowledge Graph},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2607--2612},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00291},
	doi = {10.1109/ICDE51399.2021.00291},
	timestamp = {Mon, 13 Nov 2023 08:58:38 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WongFZVCZHCZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conversational Recommender Systems (CRSs) in E-commerce platforms aim to recommend items to users via multiple conversational interactions. Click-through rate (CTR) prediction models are commonly used for ranking candidate items. However, most CRSs are suffer from the problem of data scarcity and sparseness. To address this issue, we propose a novel knowledge-enhanced deep cross network (K-DCN), a two-step (pretrain and fine-tune) CTR prediction model to recommend items. We first construct a billion-scale conversation knowledge graph (CKG) from information about users, items and converations, and then pretrain CKG by introducing knowledge graph embedding method and graph convolution network to encode semantic and structural information respectively. To make the CTR prediction model sensible of current state of users and the relationship between dialogues and items, we introduce user-state and dialogue-interaction representations based on pre-trained CKG and propose K-DCN. In K-DCN, we fuse the user-state representation, dialogue-interaction representation and other normal feature representations via deep cross network, which will give the rank of candidate items to be recommended. We experimentally prove that our proposal significantly outperforms baselines and show it's real application in Alime.}
}


@inproceedings{DBLP:conf/icde/ZhuPWHYYQZC21,
	author = {Rong Zhu and
                  Andreas Pfadler and
                  Ziniu Wu and
                  Yuxing Han and
                  Xiaoke Yang and
                  Feng Ye and
                  Zhenping Qian and
                  Jingren Zhou and
                  Bin Cui},
	title = {Efficient and Scalable Structure Learning for Bayesian Networks: Algorithms
                  and Applications},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2613--2624},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00292},
	doi = {10.1109/ICDE51399.2021.00292},
	timestamp = {Tue, 07 May 2024 20:05:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhuPWHYYQZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Structure Learning for Bayesian network (BN) is an important problem with extensive research. It plays central roles in a wide variety of applications in Alibaba Group. However, existing structure learning algorithms suffer from considerable limitations in real-world applications due to their low efficiency and poor scalability. To resolve this, we propose a new structure learning algorithm LEAST, which comprehensively fulfills our business requirements as it attains high accuracy, efficiency and scalability at the same time. The core idea of LEAST is to formulate the structure learning into a continuous constrained optimization problem, with a novel differentiable constraint function measuring the acyclicity of the resulting graph. Unlike with existing work, our constraint function is built on the spectral radius of the graph and could be evaluated in near linear time w.r.t. the graph node size. Based on it, LEAST can be efficiently implemented with low storage overhead. According to our benchmark evaluation, LEAST runs 1–2 orders of magnitude faster than state-of-the-art method with comparable accuracy, and it is able to scale on BNs with up to hundreds of thousands of variables. In our production environment, LEAST is deployed and serves for more than 20 applications with thousands of executions per day. We describe a concrete scenario in a ticket booking service in Alibaba, where LEAST is applied to build a near real-time automatic anomaly detection and root error cause analysis system. We also show that LEAST unlocks the possibility of applying BN structure learning in new areas, such as large-scale gene expression data analysis and explainable recommendation system.}
}


@inproceedings{DBLP:conf/icde/JoshiSR21,
	author = {Salil Rajeev Joshi and
                  Arpan Somani and
                  Shourya Roy},
	title = {ReLink: Complete-Link Industrial Record Linkage Over Hybrid Feature
                  Spaces},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2625--2636},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00293},
	doi = {10.1109/ICDE51399.2021.00293},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/JoshiSR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Record Linkage (ReL) is the task of identifying records from a pair of databases referring to the same realworld entity. This has many applications in organisations of all sizes where related data often exist in silos leading to inefficiency in data engineering and analytics applications as well as ineffectiveness of business applications (e.g., unable to personalise marketing campaigns).State-of-the-art (SOTA) machine learning and deep learning based ReL techniques use adaptive similarity measures and learn their relative contributions based on labeled data. However, we report here that they do not work with similar efficacy on industrial data owing to its fundamental differences such as magnitude of schema heterogeneity, need for leveraging structure of the data, lack of training data etc. Through our proposed system ‘ReLink’, we carefully mitigate these challenges and demonstrate that it not only significantly outperforms SOTA baselines on industrial datasets but also on majority of research benchmarks. ReLink introduces the notion of complete-linkage over attributes as well as uses hybrid feature spaces on lexical and semantic similarity measures using pre-trained models such as BERT. Going beyond empirical demonstration, we provide insights and prescriptive guidance on choice of ReL techniques in industrial settings from our observations and lessons learnt from the experience of transferring and deploying for real use-cases in a large financial services organization.}
}


@inproceedings{DBLP:conf/icde/GulinoCGSB21,
	author = {Andrea Gulino and
                  Stefano Ceri and
                  Georg Gottlob and
                  Emanuel Sallinger and
                  Luigi Bellomarini},
	title = {Distributed Company Control in Company Shareholding Graphs},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2637--2648},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00294},
	doi = {10.1109/ICDE51399.2021.00294},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GulinoCGSB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Company Control Problem is of central importance to banks, financial intermediaries, financial intelligence units, regulatory and supervisory authorities such as the Central Banks. It consists in understanding who takes decisions in a large company network, that is, who controls the majority of votes for each single company. This has an impact on a large number of business areas, with examples including evaluation of creditworthiness, economic analysis of the control dispersion, anti-money laundering, prevention of potentially hostile takeovers, evaluation of risks, and shock propagation.This paper is based on our experience with the Central Bank of Italy and presents an approach to the solution of the company control problem in distributed settings, especially relevant, as large and distributed ownership graphs reflect European-size applications where scalability is paramount.In particular, we formalize the problem as query answering on a large distributed database. We study how independent subqueries can be executed in each partition and the partial results assembled at a master site to produce the answer. We study the formal properties of the problem, that is not easily parallelizable, and then present a method that supports parallelism at best.We present a thorough experimental evaluation of our approach with the Italian company graph of the Bank of Italy and the European Register of Financial Intermediaries and Affiliates as well as many artificial graphs to fully assess scalability.}
}


@inproceedings{DBLP:conf/icde/AmsterdamerC21,
	author = {Yael Amsterdamer and
                  Yehuda Callen},
	title = {SPARQLIt: Interactive {SPARQL} Query Refinement},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2649--2652},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00295},
	doi = {10.1109/ICDE51399.2021.00295},
	timestamp = {Sun, 02 Oct 2022 16:04:34 +0200},
	biburl = {https://dblp.org/rec/conf/icde/AmsterdamerC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose to demonstrate an interactive system called SPARQLIt, assisting users in the formulation of SPARQL queries. In SPARQLIt, users start by specifying a well-structured SPARQL query; the challenge is that the query may not be grounded to a specific ontology schema or content. The space of possible grounded queries to which the user query may be mapped is then well-defined, and SPARQLIt allows the user to interactively explore this space in a systematic way. A key component of this exploration is a pruning mechanism that is based on identifying query parts that are correct/incorrect, and using them to efficiently filter candidate queries of a certain form. Our demonstration will use the Yago Ontology to exemplify the ease of constructing queries with SPARQLIt without prior knowledge of its contents. We will allow participants to specify queries of their choice and will walk them through the interactive process of query refinement.}
}


@inproceedings{DBLP:conf/icde/Amer-YahiaMY21,
	author = {Sihem Amer{-}Yahia and
                  Tova Milo and
                  Brit Youngmann},
	title = {SubDEx: Exploring Ratings in Subjective Databases},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2653--2656},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00296},
	doi = {10.1109/ICDE51399.2021.00296},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Amer-YahiaMY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We demonstrate SubDEx, a dedicated framework for Subjective Data Exploration (SDE). SubDEx enables the joint exploration of items, people, and people's opinions on items, in a guided multi-step process where each step aggregates the most useful and diverse trends in the form of rating maps. Because of the large search space of possible rating maps, we leverage pruning strategies to enable interactive running times. We demonstrate the need for a dedicated SDE framework and the effectiveness and efficiency of our approach, by interacting with the ICDE'21 participants who will act as data analysts.}
}


@inproceedings{DBLP:conf/icde/HuMXCJZ21,
	author = {Qi Hu and
                  Lingfeng Ming and
                  Ruijie Xi and
                  Lu Chen and
                  Christian S. Jensen and
                  Bolong Zheng},
	title = {{SOUP:} {A} Fleet Management System for Passenger Demand Prediction
                  and Competitive Taxi Supply},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2657--2660},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00297},
	doi = {10.1109/ICDE51399.2021.00297},
	timestamp = {Wed, 07 Dec 2022 23:09:58 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HuMXCJZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online car-hailing services have gained substantial popularity. An effective taxi fleet management strategy should not only increase taxi utilization by reducing taxi idle time, but should also improve passenger satisfaction by minimizing passenger waiting time. We demonstrate a fleet management system called SOUP that aims at minimizing taxi idle time and that monitors the fleet movement status. SOUP includes a passenger request prediction model called ST-GCSL that predicts the number of requests in the near future, and it includes a demand-aware route planning algorithm called DROP that provides idle taxis with search routes to serve potential requests. In addition, SOUP supports visualizing and analyzing historical passenger requests, simulating fleet movement, and computing evaluation metrics. We demonstrate how SOUP accurately predicts passenger demand and significantly reduces taxi idle time.}
}


@inproceedings{DBLP:conf/icde/KhelifatiKCHLH21,
	author = {Abdelouahab Khelifati and
                  Mourad Khayati and
                  Philippe Cudr{\'{e}}{-}Mauroux and
                  Adrian H{\"{a}}nni and
                  Qian Liu and
                  Manfred Hauswirth},
	title = {{VADETIS:} An Explainable Evaluator for Anomaly Detection Techniques},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2661--2664},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00298},
	doi = {10.1109/ICDE51399.2021.00298},
	timestamp = {Sun, 02 Oct 2022 16:04:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KhelifatiKCHLH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection is a fundamental problem that consists of identifying irregular patterns that do not conform to the expected behavior of a system or the generated data. Many anomaly detection techniques have been proposed for time series data. However, selecting the most suitable detection method remains challenging as the proposed techniques widely vary in performance. The appropriate choice of a detection method impacts many properties of mission-critical applications such as in monitoring a patient’s health, where anomalies are inevitable but need to be detected securely. In this demo, we present a new evaluator that allows to peruse the performance of several anomaly detection techniques and supports practitioners in understanding the behavior and (dis-)advantages of each technique for a given dataset. In a simple and well-structured way, practitioners can specify the desired anomaly detection setup, and our system would tune the parameters of each technique and analyze their properties in an easily understandable report. The tool also allows recommending the most appropriate technique for each anomaly type and evaluation metric.}
}


@inproceedings{DBLP:conf/icde/SamantMSKC21,
	author = {Kunal Samant and
                  Endrit Memeti and
                  Abhishek Santra and
                  Enamul Karim and
                  Sharma Chakravarthy},
	title = {CoWiz: Interactive Covid-19 Visualization Based On Multilayer Network
                  Analysis},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2665--2668},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00299},
	doi = {10.1109/ICDE51399.2021.00299},
	timestamp = {Sat, 30 Sep 2023 09:44:51 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SamantMSKC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Covid Wizard or CoWiz is a Covid-19 visualization dashboard based on Multilayer Network (MLN) analysis underneath 1 . Online dashboards typically plot/visualize statistical information gleaned from raw data, such as daily cases, deaths, recoveries, tests, etc. However, for a better understanding, we need aggregate analysis (e.g., community, centrality) and its visualization which is the purpose of CoWiz. As an example, grouping counties across a country/region based on similarity of increase/decrease in cases, deaths, hospitalizations over intervals is not possible without aggregate analysis. This is where CoWiz utilizes community and other concepts over MLNs that are inferred from Covid and other relevant data sets for visualization.This demo presents a flexible, interactive dashboard which is capable of visualizing various aspects of Covid-19 data, including composition of Covid data with demographics (population density, education level, average earning, vehicle movements, and change in purchase patterns) at the granularity of county for USA. This paper elaborates on the types of analysis, underlying model, and how a flexible visualization dashboard has been developed using open source software and data sets. As new data becomes available, they can be incorporated into the visualization with no manual intervention.}
}


@inproceedings{DBLP:conf/icde/BiC0HJZ21,
	author = {Lei Bi and
                  Juan Cao and
                  Guohui Li and
                  Nguyen Quoc Viet Hung and
                  Christian S. Jensen and
                  Bolong Zheng},
	title = {SpeakNav: {A} Voice-based Navigation System via Route Description
                  Language Understanding},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2669--2672},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00300},
	doi = {10.1109/ICDE51399.2021.00300},
	timestamp = {Wed, 07 Dec 2022 23:09:59 +0100},
	biburl = {https://dblp.org/rec/conf/icde/BiC0HJZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many navigation applications take natural language speech as input, which avoids typing in words with their hands and decreases the occurrence of traffic accidents. We propose the SpearkNav navigation system that enables users to describe intended routes via speech and supports clue-based route retrieval. SpeakNav includes a route description language understanding model for determining POIs and distances along expected routes, and it includes an efficient algorithm to compute desired routes. In addition, SpeakNav supports basic POI and location search and location-based route navigation. We demonstrate how SpeakNav accurately recognizes users' intentions and recommends appropriate routes in real application scenarios.}
}


@inproceedings{DBLP:conf/icde/ChibahAB21,
	author = {Abdelouahab Chibah and
                  Sihem Amer{-}Yahia and
                  Laure Berti{-}{\'{E}}quille},
	title = {QeNoBi: {A} System for QuErying and mining BehavIoral Patterns},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2673--2676},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00301},
	doi = {10.1109/ICDE51399.2021.00301},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChibahAB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We demonstrate QeNoBi, a system for mining and querying customer behavioral patterns. QeNoBi combines an interactive visual interface, on-demand mining, and efficient topk processing, to provide the exploration of customer behavior over time. QeNoBi relies on two distinct data models: a customercentric graph that represents customers with similar purchasing behaviors and is annotated with a change algebra to reflect their behavior evolution, and product-centric time series that reflect the evolution of customer purchases over time. Users can query both representations along three dimensions: shape (the sketched trend of the behavior), scope (the set of customers/products of interest), and time granularity. QeNoBi provides a holistic behavior exploration capability by allowing users to seamlessly switch between customer-centric and product-centric views in a coordinated manner, thereby catering to various needs. A demonstration of QeNoBi is available at https://bit.ly/2HlcO3S.}
}


@inproceedings{DBLP:conf/icde/ZhouCZLCCSP021,
	author = {Yichao Zhou and
                  Wei{-}Ting Chen and
                  Bowen Zhang and
                  David Lee and
                  J. Harry Caufield and
                  Kai{-}Wei Chang and
                  Yizhou Sun and
                  Peipei Ping and
                  Wei Wang},
	title = {CREATe: Clinical Report Extraction and Annotation Technology},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2677--2680},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00302},
	doi = {10.1109/ICDE51399.2021.00302},
	timestamp = {Thu, 01 Dec 2022 13:35:41 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhouCZLCCSP021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clinical case reports are written descriptions of the unique aspects of a particular clinical case, playing an essential role in sharing clinical experiences about atypical disease phenotypes and new therapies. However, to our knowledge, there has been no attempt to develop an end-to-end system to annotate, index, or otherwise curate these reports. In this paper, we propose a novel computational resource platform, CREATe, for extracting, indexing, and querying the contents of clinical case reports. CREATe fosters an environment of sustainable resource support and discovery, enabling researchers to overcome the challenges of information science. An online video of the demonstration can be viewed at https://youtu.be/Q8owBQYTjDc.}
}


@inproceedings{DBLP:conf/icde/Liu0LLFC21,
	author = {Baozhu Liu and
                  Xin Wang and
                  Pengkai Liu and
                  Sizhuo Li and
                  Qiang Fu and
                  Yunpeng Chai},
	title = {UniKG: {A} Unified Interoperable Knowledge Graph Database System},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2681--2684},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00303},
	doi = {10.1109/ICDE51399.2021.00303},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Liu0LLFC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graph currently has two main data models: RDF graph and property graph. The query language on RDF graph is SPARQL, while the query language on property graph is mainly Cypher. Different data models and query languages hinder the wider application of knowledge graphs. In this demonstration, we propose a unified interoperable knowledge graph database system, UniKG. (1) Based on the relational model, a unified storage scheme is utilized to efficiently store RDF graphs and property graphs, and support the query requirements of knowledge graphs. (2) Using the characteristicset-based method, the storage problem of untyped entities is addressed in UniKG. (3) UniKG realizes the interoperability of SPARQL and Cypher, and enables them to interchangeably operate on the same knowledge graph. (4) With a unified Web interface, users are allowed to query with two different languages over the same knowledge graph and visualize query results and explanations.}
}


@inproceedings{DBLP:conf/icde/Kossmann0DHMPSS21,
	author = {Jan Kossmann and
                  Martin Boissier and
                  Alexander Dubrawski and
                  Fabian Heseding and
                  Caterina Mandel and
                  Udo Pigorsch and
                  Max Schneider and
                  Til Schniese and
                  Mona Sobhani and
                  Petr Tsayun and
                  Katharina Wille and
                  Michael Perscheid and
                  Matthias Uflacker and
                  Hasso Plattner},
	title = {A Cockpit for the Development and Evaluation of Autonomous Database
                  Systems},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2685--2688},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00304},
	doi = {10.1109/ICDE51399.2021.00304},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Kossmann0DHMPSS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Databases are highly optimized complex systems with a multitude of configuration options. Especially in cloud scenarios with thousands of database deployments, determining optimized database configurations in an automated fashion is of increasing importance for database providers. At the same time, due to increased system complexity, it becomes more challenging to identify well-performing configurations. Therefore, research interest in autonomous or self-driving database systems has increased enormously in recent years. Such systems promise both performance improvements and cost reductions. In the literature, various fully or partially autonomous optimization mechanisms exist that optimize single aspects, e.g., index selection. However, database administrators and developers often distrust autonomous approaches, and there is a lack of practical experimentation opportunities that could create a better understanding. Moreover, the interplay of different autonomous mechanisms under complex workloads remains an open question. The presented cockpit enables an interactive assessment of the impact of autonomous components for database systems by comparing (autonomous) systems with different configurations side by side. Thereby, the cockpit enables users to build trust in autonomous solutions by experimenting with such technologies and observing their effects in practice.}
}


@inproceedings{DBLP:conf/icde/LamBMMWKBSWS21,
	author = {Hoang Thanh Lam and
                  Beat Buesser and
                  Hong Min and
                  Tran Ngoc Minh and
                  Martin Wistuba and
                  Udayan Khurana and
                  Gregory Bramble and
                  Theodoros Salonidis and
                  Dakuo Wang and
                  Horst Samulowitz},
	title = {Automated Data Science for Relational Data},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2689--2692},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00305},
	doi = {10.1109/ICDE51399.2021.00305},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LamBMMWKBSWS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature engineering is a crucial but tedious task that requires up to 80% of the total time in data science projects. A significant challenge is when data consists of tables from different data sources, thus data scientists need to wisely aggregate and join tables while performing feature engineering task. In this work, we demonstrate a novel system called OneBM (One Button Machine), that enables data scientists to increase their efficiency with automated feature engineering for relational data. OneBM takes as input a relational dataset with multiple tables and its entity relation diagram (ERD) which can be declared with a novel, easy-to-use drag-and-drop graphical user interface. The system then automatically identifies and executes relevant joins and aggregates in the data, and generates new features with a rich set of transformations for various types of data including but not limited to time-series, sequences, number sets and itemsets, etc. The generated features then can be used by automated model selection and hyper-parameter optimization algorithms to complete a fully end-to-end automated data science (or AutoDS) workflow. A follow-up user evaluation illustrated how data scientists can perform multi-table feature engineering tasks in minutes using our system, compared to repeatedly coding SQL-like queries to transform and aggregate relational data requiring weeks of manual labor for comparable performance. In the live demos we plan to show two use cases with real-world datasets (video demos are available at the links in the footnote): sale prediction 1 and call center user experience 2 . Pre-registered partcipants can play with these use-cases and the given datasets via Watson Studio on the cloud.}
}


@inproceedings{DBLP:conf/icde/FruthDS21,
	author = {Michael Fruth and
                  Kai Dauberschmidt and
                  Stefanie Scherzinger},
	title = {Josch: Managing Schemas for NoSQL Document Stores},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2693--2696},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00306},
	doi = {10.1109/ICDE51399.2021.00306},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FruthDS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {NoSQL document stores have become vastly popular. One major selling point is their flexibility w.r.t. schema management: With MongoDB, developers can actually switch back-and-forth between operating in schema-free mode, and schema fixed mode, where all write operations are validated. In this tool demo, we present Josch, which integrates state-ofthe-art third-party tools to support novel workflows for NoSQL document stores: Using Josch, DevOps teams may (1) extract a JSON Schema declaration from the production data instance, (2) manually refactor the schema (e.g., to account for upcoming schema changes), and (3) compare the extracted and the refactored schema, on a semantic level, e.g., to ensure that the rewritten schema is a generalization. (4) Finally, they may register the refactored schema with the NoSQL document store for schema validation. Apart from supporting this and other practical use cases, one further contribution of our demo is that we reveal current blind spots in NoSQL schema management tools that inspire novel research questions.}
}


@inproceedings{DBLP:conf/icde/ScherzingerMK21,
	author = {Stefanie Scherzinger and
                  Wolfgang Mauerer and
                  Haridimos Kondylakis},
	title = {DeBinelle: Semantic Patches for Coupled Database-Application Evolution},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2697--2700},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00307},
	doi = {10.1109/ICDE51399.2021.00307},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ScherzingerMK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Databases are at the core of virtually any software product. Changes to database schemas cannot be made in isolation, as they are intricately coupled with application code. Such couplings enforce collateral evolution, which is a recognised, important research problem. In this demonstration, we show a new dimension to this problem, in software that supports alternative database backends: vendor-specific SQL dialects necessitate a simultaneous evolution of both, database schema and program code, for all supported DB variants. These near-same changes impose substantial manual effort for software developers. We introduce DeBinelle, a novel framework and domain-specific language for semantic patches that abstracts DB-variant schema changes and coupled program code into a single, unified representation. DeBinelle further offers a novel alternative to manually evolving coupled schemas and code. DeBinelle considerably extends established, seminal results in software engineering research, supporting several programming languages, and the many dialects of SQL. It effectively eliminates the need to perform vendor-specific changes, replacing them with intuitive semantic patches. Our demo of DeBinelle is based on real-world use cases from reference systems for schema evolution.}
}


@inproceedings{DBLP:conf/icde/AvronGGMN21,
	author = {Uri Avron and
                  Shay Gershtein and
                  Ido Guy and
                  Tova Milo and
                  Slava Novgorodov},
	title = {ConCaT: Construction of Category Trees from Search Queries in E-Commerce},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2701--2704},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00308},
	doi = {10.1109/ICDE51399.2021.00308},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/AvronGGMN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Category trees play a central role in e-commerce platforms, enabling browsing-style information access. Building category trees that reflect users' dynamic information needs is a challenging task, mostly carried out by in-house taxonomists. This manual construction often leads to trees that are lacking or outdated since it is hard to keep track of market trends, seasonal changes, holidays, and special events.To support a browsing experience that better matches the user information needs, and to considerably reduce the manual work performed by taxonomists, we propose CONCAT - a system that leverages the demand-based nature of the query paradigm to automatically build a category tree that is maximally similar to the result sets for search queries. We demonstrate the effectiveness of CONCAT on real-world data, taken from a large e-commerce platform, by interacting with the ICDE'21 participants who act both as the consumers and the taxonomists.}
}


@inproceedings{DBLP:conf/icde/GaoSLXLQXMKS21,
	author = {Peng Gao and
                  Fei Shao and
                  Xiaoyuan Liu and
                  Xusheng Xiao and
                  Haoyuan Liu and
                  Zheng Qin and
                  Fengyuan Xu and
                  Prateek Mittal and
                  Sanjeev R. Kulkarni and
                  Dawn Song},
	title = {A System for Efficiently Hunting for Cyber Threats in Computer Systems
                  Using Threat Intelligence},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2705--2708},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00309},
	doi = {10.1109/ICDE51399.2021.00309},
	timestamp = {Wed, 08 Sep 2021 16:17:48 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GaoSLXLQXMKS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log-based cyber threat hunting has emerged as an important solution to counter sophisticated cyber attacks. However, existing approaches require non-trivial efforts of manual query construction and have overlooked the rich external knowledge about threat behaviors provided by open-source Cyber Threat Intelligence (OSCTI). To bridge the gap, we build ThreatRaptor, a system that facilitates cyber threat hunting in computer systems using OSCTI. Built upon mature system auditing frameworks, ThreatRaptor provides (1) an unsupervised, light-weight, and accurate NLP pipeline that extracts structured threat behaviors from unstructured OSCTI text, (2) a concise and expressive domain-specific query language, TBQL, to hunt for malicious system activities, (3) a query synthesis mechanism that automatically synthesizes a TBQL query from the extracted threat behaviors, and (4) an efficient query execution engine to search the big system audit logging data.}
}


@inproceedings{DBLP:conf/icde/LuckettCGC21,
	author = {Connor Luckett and
                  Andrew Crotty and
                  Alex Galakatos and
                  Ugur {\c{C}}etintemel},
	title = {Odlaw: {A} Tool for Retroactive {GDPR} Compliance},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2709--2712},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00310},
	doi = {10.1109/ICDE51399.2021.00310},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuckettCGC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this demo, we present ODLAW, a new tool for retroactive compliance with privacy laws like the European Union’s General Data Protection Regulation (GDPR). The GDPR enumerates the explicit rights of individuals regarding the use of their personal data, and regulators can impose strict penalties for organizations that fail to comply. While others have advocated for a completely new class of systems to address these regulations, ODLAW takes a different approach by achieving GDPR compliance while allowing an organization to keep its existing data management infrastructure intact. Using a variety of realistic datasets, the demo will show the specific ways that ODLAW can help with GDPR compliance, as well as highlight some of the key challenges that arise in real-world settings.}
}


@inproceedings{DBLP:conf/icde/DeutchFGM21,
	author = {Daniel Deutch and
                  Ariel Frankenthal and
                  Amir Gilad and
                  Yuval Moskovitch},
	title = {{PITA:} Privacy Through Provenance Abstraction},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2713--2716},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00311},
	doi = {10.1109/ICDE51399.2021.00311},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DeutchFGM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Provenance is a valuable tool for explaining and validating query results. On the other hand, provenance also reveals much of the details about the query that generated it, which may include proprietary logic that the query owner does not wish to disclose. To this end, we propose to demonstrate PITA, a system designed to allow the release of provenance information, while hiding the properties of the underlying query. We formalize the trade-off between the level of information encoded in a provenance expression and the breach of privacy it incurs. Following this model, we design PITA to abstract the provenance so that it incurs minimum loss of information, while keeping privacy above a given threshold, namely protecting details of the original query from being revealed.}
}


@inproceedings{DBLP:conf/icde/ForoniLV21a,
	author = {Daniele Foroni and
                  Matteo Lissandrini and
                  Yannis Velegrakis},
	title = {The {F4U} System for Understanding the Effects of Data Quality},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2717--2720},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00312},
	doi = {10.1109/ICDE51399.2021.00312},
	timestamp = {Tue, 07 May 2024 20:05:37 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ForoniLV21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We demonstrate a system that enables a data-centric approach in understanding data quality. Instead of directly quantifying data quality as traditionally done, it disrupts the quality of the dataset and monitors the deviations in the output of an analytic task at hand. It computes the correlation factor between the disruption and the deviation and uses it as the quality metric. This allows users to understand not only the quality of their dataset but also the effect that present and future quality issues have to the intended analytic tasks. This is a novel data-centric approach aimed at complementing existing solutions. On top of the new information that it provides, and in contrast to existing techniques of data quality, it neither requires knowledge of the clean datasets, nor of the constraints on which the data should comply.}
}


@inproceedings{DBLP:conf/icde/ConstantinouOKC21,
	author = {George Constantinou and
                  Onur Orhan and
                  Roopal Kondepudi and
                  Hyunjae Cho and
                  Seon Ho Kim and
                  Abdullah Alfarrarjeh and
                  Cyrus Shahabi},
	title = {FloraVision: {A} Spatial Crowd-based Learning System for California
                  Native Plants},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2721--2724},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00313},
	doi = {10.1109/ICDE51399.2021.00313},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ConstantinouOKC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the availability of massive amounts of visual data covering wide geographical regions, various image learning applications have emerged, including classifying the street cleanliness level, detecting forest fires or road hazards. Such applications share similar characteristics as they need to 1) detect specific objects or events (what), 2) associate the detected object with a location (where), and 3) know the time that the event happened (when). Advancements in image-based machine learning (ML) benefit these applications as they can automate the detection of objects of interest. Along with the edge computing (EC) paradigm, the processing cost is offloaded to the devices, hence reducing latency and communication cost. Moreover, sensors on the edge devices (e.g., GPS) enrich the collected data with metadata. However, a shortcoming of existing approaches is that they rely on pre-trained "static" models. Nonetheless, crowdsourced data at diverse locations can be leveraged to iteratively improve the robustness of a model. We refer to the aforementioned strategy as "spatial crowd-based learning".To showcase this class of applications, we present FloraVision, an end-to-end system that integrates ML, crowdsourcing, and EC to automate the detection, mapping, and exploration of California Native Plants. FloraVision implements a pipeline to collect and clean publicly available image data, train a lightweight MobileNet-based classification model, and then deploy the model on mobile devices. It leverages spatial crowd-based learning to iteratively evolve the initial model from crowdsourced data. Its mobile application facilitates detecting plants and mapping their geolocations. Finally, it allows end-users to submit ad hoc spatio-temporal nearest neighbor queries and visualizes the results in an augmented reality user interface. Although our application focuses on plants, several other applications follow similar architectural patterns.}
}


@inproceedings{DBLP:conf/icde/KonstaMDNK21,
	author = {Alyzia Konsta and
                  Ioannis Mytilinis and
                  Katerina Doka and
                  Sotiris Niarchos and
                  Nectarios Koziris},
	title = {Clouseau: Blockchain-based Data Integrity for {HDFS} Clusters},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2725--2728},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00314},
	doi = {10.1109/ICDE51399.2021.00314},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KonstaMDNK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the volume of produced data is exponentially increasing, companies tend to rely on distributed systems to meet the surging demand for storage capacity. With the business workflows becoming more and more complex, such systems often consist of or are accessed by multiple independent, untrusted entities, which need to interact with shared data. In such scenarios, the potential conflicts of interest incentivize malicious parties to act in a dishonest way and tamper the data to their own benefit. The decentralized nature of the systems renders verifiable data integrity a strenuous but necessary task: The various parties should be able to audit changes and detect tampering when it happens.In this work, we focus on HDFS, the most common storage substrate for Big Data analytics. HDFS is vulnerable to malicious users and participating nodes and does not provide a trustful lineage mechanism, thus jeopardizing the integrity of stored data and the credibility of extracted insights. As a remedy, we present Clouseau, a blockchain-based system that provides verifiable integrity over HDFS, while it does not incur significant overhead at the critical path of read/write operations. During the demonstration, the attendees will have the chance to interact with Clouseau, corrupt data themselves, and witness how Clouseau detects malicious actions.}
}


@inproceedings{DBLP:conf/icde/DaA0S21,
	author = {Yanan Da and
                  Ritesh Ahuja and
                  Li Xiong and
                  Cyrus Shahabi},
	title = {{REACT:} Real-Time Contact Tracing and Risk Monitoring via Privacy-Enhanced
                  Mobile Tracking},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2729--2732},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00315},
	doi = {10.1109/ICDE51399.2021.00315},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DaA0S21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contact tracing is an essential public health tool for controlling epidemic disease outbreaks such as the COVID-19 pandemic. Digital contact tracing using real-time locations or proximity of individuals can be used to significantly speed up and scale up contact tracing. In this demonstration, we present our system, REACT, for REAl-time Contact Tracing and risk monitoring via privacy-enhanced tracking of users’ locations. With privacy enhancement that allows users to control and refine the precision with which their information will be collected and used, REACT will enable: 1) contact tracing of individuals who are exposed to infected cases and identification of hot-spot locations, 2) individual risk monitoring based on the locations they visit and their contact with others. In this paper, we demonstrate the procedure of contact tracing using our application and the utility of contact tracing given the protected locations.}
}


@inproceedings{DBLP:conf/icde/WanS21,
	author = {Guihong Wan and
                  Haim Schweitzer},
	title = {Edge Sparsification for Graphs via Meta-Learning},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2733--2738},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00316},
	doi = {10.1109/ICDE51399.2021.00316},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WanS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a novel edge sparsification approach for semi-supervised learning on undirected and attributed graphs. The main challenge is to retain few edges while minimizing the loss of node classification accuracy. The task can be mathematically formulated as a bi-level optimization problem. We propose to use meta-gradients, which have traditionally been used in meta-learning, to solve the optimization problem, specifically, treating the graph adjacency matrix as hyperparameters to optimize. Experimental results show the effectiveness of the proposed approach. Remarkably, with the resulting sparse and light graph, in many cases the classification accuracy is significantly improved.}
}


@inproceedings{DBLP:conf/icde/Vervaet21,
	author = {Arthur Vervaet},
	title = {MoniLog: An Automated Log-Based Anomaly Detection System for Cloud
                  Computing Infrastructures},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2739--2743},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00317},
	doi = {10.1109/ICDE51399.2021.00317},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Vervaet21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Within today’s large-scale systems, one anomaly can impact millions of users. Detecting such events in real-time is essential to maintain the quality of services. It allows the monitoring team to prevent or diminish the impact of a failure. Logs are a core part of software development and maintenance, by recording detailed information at runtime. Such log data are universally available in nearly all computer systems. They enable developers as well as system maintainers to monitor and dissect anomalous events. For Cloud computing companies and large online platforms in general, growth is linked to the scaling potential. Automatizing the anomaly detection process is a promising way to ensure the scalability of monitoring capacities regarding the increasing volume of logs generated by modern systems. In this paper, we will introduce MoniLog, a distributed approach to detect real-time anomalies within large-scale environments. It aims to detect sequential and quantitative anomalies within a multi-source log stream. MoniLog is designed to structure a log stream and perform the monitoring of anomalous sequences. Its output classifier learns from the administrator’s actions to label and evaluate the criticality level of anomalies.}
}


@inproceedings{DBLP:conf/icde/Zhang21,
	author = {Weiqi Zhang},
	title = {Graph Based Approach to Real-Time Metro Passenger Flow Anomaly Detection},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2744--2749},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00318},
	doi = {10.1109/ICDE51399.2021.00318},
	timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Zhang21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time anomaly detection of passenger flows in the metro system is very important to maintain the URT system’s normal operation and ensure passengers’ safety. This paper proposes a novel abnormal passenger flow detection method based on smart card data. The method constructs a graphic model whose topological structure can capture the spatial distribution of anomalous passenger flow. It further incorporates external information (e.g. geographical information) to depict the latent passenger flow’s spatial dependence embedded in URT system. Considering abnormal flows may only exist in local regions of the metro system, a detection statistic is constructed by using graph community detection. The statistic also incorporates an adaptive sampling strategy for further signal selection and noise filter. It can be efficiently solved via a Min-Cut-based algorithm and can provide real-time solutions to anomaly detection and diagnosis. Preliminary experimental results demonstrate the efficiency of our method.}
}


@inproceedings{DBLP:conf/icde/HuangYLSS21,
	author = {Jiayu Huang and
                  Hao Yan and
                  Jing Li and
                  H. Milton Stewart and
                  Frank C. Setzer},
	title = {Combining Anatomical Constraints and Deep learning for 3-D {CBCT}
                  Dental Image Multi-label Segmentation},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2750--2755},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00319},
	doi = {10.1109/ICDE51399.2021.00319},
	timestamp = {Sat, 04 Mar 2023 22:47:15 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HuangYLSS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning research on medical images is becoming popular as advanced imaging technologies and equipment in medicine become more and more available. Dental Cone-beam Computed Tomography (Dental CBCT), a frequently-used visualization tool for oral diagnosis, provides valuable three-dimensional information, whose development for automation of Dental CBCT analysis, on the other hand, is relatively preliminary. Generally, there are three important characteristics for analyzing Dental CBCT with noisy labels and limited labeled sample size, and availability of oral medicine knowledge. Based on those characteristics, we develop an image segmentation method for Dental CBCT by integrating domain knowledge into deep U-Net for the 3D segmentation. Finally, depending on whether the knowledge can be decomposed into each pixel, the knowledge constraints are classified into two types: separable and non-separable constraints. All knowledge constraints can be represented as a posterior regularization term and solved in different ways in accordance with related types. For separable constraints, the mean-field theory is employed to solve an optimization problem with the independence assumption about the distributions of output variables on each pixel. For non-separable constraints, we propose to combine the importance sampling based approach and the stochastic optimization algorithm. Finally, we propose to formulate the domain knowledge to the learning stage to improve the accuracy and efficiency of automation of Dental CBCT segmentation. Finally, we will apply the proposed methods into the real datasets collected and manually labeled by the doctors at the University of Pennsylvania}
}


@inproceedings{DBLP:conf/icde/Li21,
	author = {Ziyue Li},
	title = {Tensor Topic Models with Graphs and Applications on Individualized
                  Travel Patterns},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2756--2761},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00320},
	doi = {10.1109/ICDE51399.2021.00320},
	timestamp = {Tue, 31 Aug 2021 19:36:13 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Li21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Individualized passenger travel pattern is of significant research value since the abundant information from individual trajectory data could help discover the useful insights about the multi-clustering of origin, destination, time, etc., and the passenger cluster. However, this task is rather challenging, given the data is high-dimensional, with complex spatiotemporal structure, and exhibits sparse patterns. Moreover, individual travel patterns are also affected by external information, such as the distances and functions of locations and points of interest, which are ignored in most current works. To tackle these challenges, we proposed two novel frameworks based on topic models with the external information incorporated as graphs: Trips and passengers are formulated as tensor words and tensor documents to preserve data nature; To learn multiclustering, we proposed a graph-regularized tensor Latent Dirichlet Allocation model, with graph structure formulated as Laplacian penalty; To learn passenger clustering, we proposed a graph-based tensor Dirich-let Multinomial Mixture model with graph Laplacian penalty and l 1 -norm penalty for cluster amount autodetermination.}
}


@inproceedings{DBLP:conf/icde/Li21a,
	author = {Man Li},
	title = {BERT-based Dynamic Clustering of Subway Stations Using Flow Information},
	booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
	pages = {2762--2765},
	publisher = {{IEEE}},
	year = {2021},
	url = {https://doi.org/10.1109/ICDE51399.2021.00321},
	doi = {10.1109/ICDE51399.2021.00321},
	timestamp = {Thu, 11 Aug 2022 17:04:46 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Li21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing volume and complexity of metro systems urge the transportation agencies to gain more knowledge of the mobility patterns of metro stations to improve public service, adjust future planning and even reconstruct the network. Therefore understanding the dynamic functions of metro stations becomes essential. In this work, we propose a BERT-based feature extraction framework to capture dynamic mobility patterns for metro stations time to time. Specifically, we adopt the flow counts gained from Automated Fare Collection (AFC) systems in every time interval as the representation of current mobility pattern and design the flow matrices for each station. The proposed feature extraction framework is implemented to learn the latent features and once the latent semantics are obtained, we apply affinity propagation clustering algorithm to segment subway stations into different clusters. Based on the dynamic clustering result, further work can be done like group flow prediction and anomaly detection.}
}
