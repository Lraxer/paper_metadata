@inproceedings{DBLP:conf/icde/ZhouLBJZ25,
	author = {Xiaofang Zhou and
                  Qing Li and
                  Angela Bonifati and
                  Hans{-}Arno Jacobsen and
                  Wenjie Zhang},
	title = {A Message from the Chairs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {lviii--lx},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00005},
	doi = {10.1109/ICDE65448.2025.00005},
	timestamp = {Wed, 10 Sep 2025 14:09:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhouLBJZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/icde/ZhangMXZPJ25,
	author = {Meng Zhang and
                  Kexin Ma and
                  Liyang Xu and
                  Kedi Zhang and
                  Yuanxi Peng and
                  Ruochun Jin},
	title = {{CLEAR:} {A} Parser-Independent Disambiguation Framework for {NL2SQL}},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--14},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00247},
	doi = {10.1109/ICDE65448.2025.00247},
	timestamp = {Wed, 10 Sep 2025 11:58:25 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangMXZPJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parsing Natural Language to SQL (NL2SQL) helps users who are not proficient in databases to efficiently query desired data through natural language. Although existing NL2SQL parsers demonstrate good capabilities in processing clear queries, ambiguity still remains an unresolved issue which makes parsers produce unstable outputs that deviate from the user's actual intent. To bridge the gap, this paper introduces the CLEAR framework, a systematic study of disambiguation for NL2SQL, including ambiguity detection, clarification, and reformulation, which benefits any NL2SQL parsers. Firstly, CLEAR employs a pipeline using Large Language Models (LLMs) and a series of rules to detect ambiguities, thus obtaining the “candidate mapping” for ambiguity representation. Secondly, an interactive selection module is employed to collect the clarification information from users through multiple-choice questions, thus obtaining the “selection mapping”. Finally, rewriting rules are employed to reformulate the question and schema, thus obtaining a clear input for parsers to generate clear SQLs. Furthermore, we construct CLAMBSQL, a novel benchmark for systematic evaluation for NL2SQL disambiguation, which contains fine-grained ambiguity and clarification annotations. Experiments on various datasets and baselines demonstrate that CLEAR can successfully address seven types of ambiguity. When parsers are integrated with CLEAR, the performance of ambiguous SQLs detection achieves a significant improvement of 30.5 % on AMBROSIA in the AllFound metric and 21.1 % on AmbiQT in the BothInTop-5 metric, the performance of ambiguity clarification achieves a remarkable improvement of 16.2 % on CLAMBSQL in the CEX metric, and the performance of the general prediction achieves an increase of 1.6 % in the EX metric and 7.7 % in the CSR metric on BIRD. The CLEAR code and CLAMBSQL dataset are available at https://github.com/mengzhang18/CLEAR.}
}


@inproceedings{DBLP:conf/icde/SiYJCMW25,
	author = {Jianing Si and
                  Haitao Yuan and
                  Nan Jiang and
                  Minxiao Chen and
                  Xiao Ma and
                  Shangguang Wang},
	title = {Towards Robust Trajectory Embedding for Similarity Computation: When
                  Triangle Inequality Violations in Distance Metrics Matter},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--14},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00242},
	doi = {10.1109/ICDE65448.2025.00242},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SiYJCMW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory similarity is a cornerstone of trajectory data management and analysis. Traditional similarity functions often suffer from high computational complexity and a reliance on specific distance metrics, prompting a shift towards deep representation learning in Euclidean space. However, existing Euclidean-based trajectory embeddings often face challenges due to the triangle inequality constraints that do not universally hold for trajectory data. To address this issue, this paper introduces a novel approach by incorporating non-Euclidean geometry, specifically hyperbolic space, into trajectory representation learning. We present the first-ever integration of hyperbolic space to resolve the inherent limitations of the triangle inequality in Euclidean embeddings. In particular, we achieve it by designing a Lorentz distance measure, which is proven to overcome triangle inequality constraints. Additionally, we design a model-agnostic framework LH-plugin to seamlessly integrate hyperbolic embeddings into existing representation learning pipelines. This includes a novel projection method optimized with the Cosh function to prevent the diminishment of distances, supported by a theoretical foundation. Furthermore, we propose a dynamic fusion distance that intelligently adapts to variations in triangle inequality constraints across different trajectory pairs, blending Lorentzian and Euclidean distances for more robust similarity calculations. Comprehensive experimental evaluations demonstrate that our approach effectively enhances the accuracy of trajectory similarity measures in state-of-the-art models across multiple real-world datasets. The LH-plugin not only addresses the triangle inequality issues but also significantly refines the precision of trajectory similarity computations, marking a substantial advancement in the field of trajectory representation learning.}
}


@inproceedings{DBLP:conf/icde/LuoMCZYCQ25,
	author = {Kecheng Luo and
                  Ruiyang Ma and
                  Peng Cai and
                  Aoying Zhou and
                  Zhiwei Ye and
                  Dunbo Cai and
                  Ling Qian},
	title = {Guiding Index Tuning Exploration with Potential Estimation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--13},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00116},
	doi = {10.1109/ICDE65448.2025.00116},
	timestamp = {Thu, 23 Oct 2025 23:00:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuoMCZYCQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Throughout index tuning, existing index advisors allocate tuning budget equally across all queries in the workload, even though a considerable portion of queries benefit negligible from index tuning, leading to high costs and inefficiency. This paper introduces a novel learning-based index advisor named GITEE, which increases tuning efficiency and effectiveness by intelligently guiding the exploration of the large search space on candidate index. Our solution consists of three components. First, we utilize execution plan and predicate information to accurately estimate the maximum improvement indexing can bring, which serves as preliminary knowledge for reasonable tuning budget allocation. Second, we filter out queries based on the impact of indexing on the individual queries and their influence on others, thereby reducing the number of candidate indexes. Third, we leverage a Monte Carlo Tree Search-based solution, guided by the knowledge, to accelerate the selection of high-quality index configurations within the valuable search space. Extensive experiments across various benchmarks demonstrate that GITEE achieves superior tuning performance compared to state-of-theart heuristic or learning-based index advisors, while reducing tuning overhead by 1-2 orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/LiuPZLLLWZL25,
	author = {Yuxin Liu and
                  Yuezhang Peng and
                  Hefeng Zhou and
                  Hongze Liu and
                  Xinyu Lu and
                  Jiong Lou and
                  Chentao Wu and
                  Wei Zhao and
                  Jie Li},
	title = {{LOVO:} Efficient Complex Object Query in Large-Scale Video Datasets},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--14},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00148},
	doi = {10.1109/ICDE65448.2025.00148},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuPZLLLWZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread deployment of cameras has led to an exponential increase in video data, creating vast opportunities for applications such as traffic management and crime surveillance. However, querying specific objects from large-scale video datasets presents challenges, including (1) processing massive and continuously growing data volumes, (2) supporting complex query requirements, and (3) ensuring low-latency execution. Existing video analysis methods struggle with either limited adaptability to unseen object classes or suffer from high query latency. In this paper, we present LOVO, a novel system designed to efficiently handle compLex Object queries in large-scale VideO datasets. Agnostic to user queries, LOVO performs one-time feature extraction using pre-trained visual encoders, generating compact visual embeddings for key frames to build an efficient index. These visual embeddings, along with associated bounding boxes, are organized in an inverted multi-index structure within a vector database, which supports queries for any objects. During the query phase, LOVO transforms object queries to query embeddings and conducts fast approximate nearest-neighbor searches on the visual embeddings. Finally, a cross-modal rerank is performed to refine the results by fusing visual features with detailed textual features. Evaluation on real-world video datasets demonstrates that LOVO outperforms existing methods in handling complex queries, with near-optimal query accuracy and up to 85x lower search latency, while significantly reducing index construction costs. This system redefines the state-of-theart object query approaches in video analysis, setting a new benchmark for complex object queries with a novel, scalable, and efficient approach that excels in dynamic environments.}
}


@inproceedings{DBLP:conf/icde/ChenHGHYL25,
	author = {Xueqin Chen and
                  Xiaoyu Huang and
                  Qiang Gao and
                  Li Huang and
                  Jiajing Yu and
                  Guisong Liu},
	title = {Birds of a Feather: Enhancing Multimodal Fake News Detection Via Multi-Element
                  Retrieval},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--14},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00270},
	doi = {10.1109/ICDE65448.2025.00270},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenHGHYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The automatic and accurate detection of online fake news is crucial to society, drawing significant attention from both industry and academia. With news content becoming increasingly multimodal, assessing its truthfulness has become more challenging. Existing efforts to combat multimodal fake news primarily follow a target-egocentric paradigm, which makes predictions based solely on features extracted from the target news and its associated social context. However, their performance is constrained by the inherent knowledge paucity within the target news. To address this challenge, we propose ReTIP, a novel retrieval-enhanced framework for multimodal fake news detection. ReTIP enriches the knowledge of target news by retrieving relevant news content, along with potential diffusion participants. Specifically, ReTIP retrieves relevant content from a local content pool using a key vector generated through the joint modeling of text and images, and employs a communitybased strategy to retrieve potential participants from a historical user interaction pool. Additionally, ReTIP employs a hypergraphbased information enhancement module to align knowledge across modalities and instances at a fine-grained level by capturing higher-order correlations. Finally, an attention-based fusion layer is employed to aggregate the multi-element knowledge from retrieved instances, which is then concatenated with the target news knowledge for the final prediction. Extensive experiments on three real-world multimodal fake news datasets not only demonstrate the superior performance of ReTIP compared to state-of-the-art baselines but also confirm the effectiveness of its individual components. Our code is made publicly available at https://github.com/xytitor/ReTIP.}
}


@inproceedings{DBLP:conf/icde/ZhangWYZXBW25,
	author = {Yudong Zhang and
                  Xu Wang and
                  Xuan Yu and
                  Zhengyang Zhou and
                  Xing Xu and
                  Lei Bai and
                  Yang Wang},
	title = {{DIFFODE:} Neural {ODE} with Differentiable Hidden State for Irregular
                  Time Series Analysis},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--14},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00160},
	doi = {10.1109/ICDE65448.2025.00160},
	timestamp = {Thu, 11 Sep 2025 09:20:24 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangWYZXBW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Irregular time series analysis is increasingly essential in data management due to the proliferation of complex data irregularly sampled by real-world systems. Traditional time series models, including RNN-based models and transformer variants, face significant challenges in generalizing to continuous-time paradigms, which are essential for capturing the ongoing dynamics of irregular time series. Neural Ordinary Differential Equations (NODEs) assume a continuous latent dynamic and provide an elegant framework for irregular time series analysis, yet they suffer from limitations like fragmented latent processes and the inability to fully exploit interdependencies among observations. To address these challenges, we propose a novel Differentiable hidden state enhanced neural ODE framework, termed DIFFODE, designed to effectively model irregular time series. Concretely, we introduce an attention-based differential hidden state that maps irregular observations into a continuous hidden state space, enabling the extraction of latent dynamics while preserving temporal continuity. Leveraging the theory of generalized inverses, DIFFODE innovatively derives ODEs to describe hidden state dynamics. Furthermore, we incorporate the Hoyer metric into our framework to enhance its capacity to capture subtle yet critical temporal shifts, significantly improving the accuracy of time series modeling. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of DIFFODE across three key tasks, including irregular time series classification, interpolation, and extrapolation.}
}


@inproceedings{DBLP:conf/icde/ZhangSLLF25,
	author = {Haoyi Zhang and
                  Guohao Sun and
                  Jinhu Lu and
                  Guanfeng Liu and
                  Xiu Susie Fang},
	title = {DELRec: Distilling Sequential Pattern to Enhance LLMs-Based Sequential
                  Recommendation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--14},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00089},
	doi = {10.1109/ICDE65448.2025.00089},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangSLLF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation (SR) tasks aim to predict users' next interaction by learning their behavior sequence and capturing the connection between users' past interactions and their changing preferences. Conventional SR models often focus solely on capturing sequential patterns within the training data, neglecting the broader context and semantic information embedded in item titles from external sources. This limits their predictive power and adaptability. Large language models (LLMs) have recently shown promise in SR tasks due to their advanced understanding capabilities and strong generalization abilities. Researchers have attempted to enhance LLMs-based recommendation performance by incorporating information from conventional SR models. However, previous approaches have encountered problems such as 1) limited textual information leading to poor recommendation performance, 2) incomplete understanding and utilization of conventional SR model information by LLMs, and 3) excessive complexity and low interpretability of LLMs-based methods. To improve the performance of LLMs-based SR, we propose a novel framework, Distilling Sequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec), which aims to extract knowledge from conventional SR models and enable LLMs to easily comprehend and utilize the extracted knowledge for more effective SRs. DELRec consists of two main stages: 1) Distill Pattern from Conventional SR Models, focusing on extracting behavioral patterns exhibited by conventional SR models using soft prompts through two well-designed strategies; 2) LLMs-based Sequential Recommendation, aiming to fine-tune LLMs to effectively use the distilled auxiliary information to perform SR tasks. Extensive experimental results conducted on four real datasets validate the effectiveness of the DELRec framework.}
}


@inproceedings{DBLP:conf/icde/TangLWDMH25,
	author = {Yafeng Tang and
                  Zheng Liang and
                  Hongzhi Wang and
                  Xiaoou Ding and
                  Tianyu Mu and
                  Huan Hu},
	title = {Description-Similarity Rules: Towards Flexible Feature Engineering
                  for Entity Matching},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--15},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00008},
	doi = {10.1109/ICDE65448.2025.00008},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/TangLWDMH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity Matching (EM) is a crucial task in data integration. Compared to deep learning-based EM solutions, tree-based machine learning models are more computationally effective and explainable, making them more applicable in real-world EM scenarios. However, Random Forest-based EM methods select features with a static feature engineering rule set for all attributes. Consequently, they suffer model retraining cost to select features, and can hardly customize to different EM tasks. To tackle this problem, we propose Description-Similarity Rules (DSR) for EM feature engineering. DSR introduces diverse attribute value distribution metrics and data-driven thresholds to traditional EM feature engineering rules. Unfortunately, both the DSR search space and its online model retraining costs are exponential. By pushing the model retraining to the offline stage, our DSR set mining algorithm is one order of magnitude faster than the baseline algorithms, taking only seconds for online selection. Empirically, DSR outperforms the feature engineering methods by 3.75 % on average F1 score, while reaching the state-of-the-art EM performance on several datasets.}
}


@inproceedings{DBLP:conf/icde/QiuY25,
	author = {Haiquan Qiu and
                  Quanming Yao},
	title = {Explore the Disentanglement Mechanism for Deep Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--5},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00380},
	doi = {10.1109/ICDE65448.2025.00380},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/QiuY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning's success is accompanied by challenges in interpretability and efficiency. This research explores disentanglement mechanisms in deep learning across three core dimensions: model expressivity, optimization paradigms, and interpretability. We first analyze how Graph Neural Networks learn logical rules through representation disentanglement, establishing theoretical foundations for their expressivity. We then develop efficient parameter merging strategies by leveraging feature decomposition in network parameters. Finally, we design neural architectures aligned with symbolic formulas for modeling complex network dynamics, enhancing transparency through architecture disentanglement. Our research provides both theoretical insights and practical methodologies for building more interpretable and efficient AI systems. Future work will further advance these directions through precision-focused expressivity analysis, semantic-aware parameter optimization, and crossdomain applications of interpretable modeling.}
}


@inproceedings{DBLP:conf/icde/ZhangLLLF25,
	author = {Chao Zhang and
                  Guoliang Li and
                  Leyao Liu and
                  Tao Lv and
                  Ju Fan},
	title = {CloudyBench: {A} Testbed for {A} Comprehensive Evaluation of Cloud-Native
                  Databases},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--13},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00191},
	doi = {10.1109/ICDE65448.2025.00191},
	timestamp = {Wed, 10 Sep 2025 14:09:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangLLLF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As more and more on-premise databases are moving towards the cloud service, it is crucial to have a benchmark to holistically evaluate the performance of their core features including elasticity, multi-tenancy, and cost-efficiency. However, existing benchmarks lack specific workload patterns and metrics for evaluating cloud-native databases, and the real workload is often unavailable due to privacy requirements. In this paper, we propose a new testbed for cloud-native databases, named CloudyBench. Its core contribution is to provide tailored workloads and metrics to evaluate the service quality of cloud-native databases in various dimensions. First, we design cloud-native workload patterns with peaks and valleys for elasticity evaluation. Second, we devise new multi-tenancy patterns by posing varied resource contention to evaluate the resource scheduling among tenants. Third, we propose a unified metric that considers performance, cost, elasticity, multitenancy, replication lag time, and fail-over. Fourth, we provide an evaluation testbed for evaluating cloud-native databases. To verify the effectiveness of CloudyBench, extensive experiments have been conducted over five commercial representatives from multiple cloud providers. We also obtain a number of insights for the performance implications of cloud-native databases from the architectural perspective.}
}


@inproceedings{DBLP:conf/icde/YangZLZ25,
	author = {Boyu Yang and
                  Weiguo Zheng and
                  Xiang Lian and
                  Lingfei Zheng},
	title = {Space-Efficient Compact Representations for Graph Analytics},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--14},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00255},
	doi = {10.1109/ICDE65448.2025.00255},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/YangZLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The volume of graph data is increasing substantially, exerting significant pressure on graph analytics, especially when computing resources are limited. To address this challenge, we investigate the problem of developing compact representations that directly support widely used graph analytics. Leveraging interval encoding, we introduce two compact graph representations: the unified interval (UI) representation and the hybrid vertex-interval (HVI) representation. To minimize the sizes of these representations, we mathematically formulate two graph reordering problems, MUIP and MHVIP, and provide an NPhardness analysis. To solve these problems, we propose a spaceefficient edge-dropping framework, which, powered by a weightpriority approach, offers approximation ratio guarantees. We also develop a sampling method based on random walks to accelerate the edge-dropping process. Extensive experiments on 15 graph datasets demonstrate that the UI and HVI representations achieve an average compactness of 34.54% and 26.99%, respectively. Moreover, the HVI representation significantly speeds up various graph analytics, such as edge existence determination, triangle counting, and PageRank.}
}


@inproceedings{DBLP:conf/icde/ChenSW25,
	author = {Zijie Chen and
                  Shaoxu Song and
                  Jianmin Wang},
	title = {OneRoundSTL: In-Database Seasonal-Trend Decomposition},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--13},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00060},
	doi = {10.1109/ICDE65448.2025.00060},
	timestamp = {Thu, 25 Dec 2025 12:47:48 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ChenSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Seasonal-trend decomposition has been widely used in time series analysis, e.g., time series forecasting and anomaly detection. Existing seasonal-trend decomposition methods, such as STL and its variations, assume that the time series is complete and sorted by timestamp. However, popular time series databases usually adopt LSM-Tree based storage, which stores data in pages not necessarily in time order. Moreover, time series stored in databases often suffer from missing values due to sensor failures, further compromising their integrity. A straightforward idea is to first merge and sort the data of different pages, and then decompose them. It obviously leads to heavy online computation, repeated calculations for multiple queries, and still cannot deal with the remaining missing data. In this paper, we propose OneRoundSTL, which pre-calculates offline some results in each individual page and concatenates the pre-calculated results online at query time to obtain the decomposition outcome. OneRoundSTL has been deployed and included as a function in an open source time series database, Apache IoTDB. Experiments on synthetic and real-world datasets in the system show that our OneRoundSTL exhibits high efficiency, far exceeding the state-of-the-art methods, while keeping decomposition effect.}
}


@inproceedings{DBLP:conf/icde/BaiCLRWZYH25,
	author = {Lu Bai and
                  Lixin Cui and
                  Ming Li and
                  Peng Ren and
                  Yue Wang and
                  Lichi Zhang and
                  Philip S. Yu and
                  Edwin R. Hancock},
	title = {{AEGK:} Aligned Entropic Graph Kernels Through Continuous-Time Quantum
                  Walks: (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1--2},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00399},
	doi = {10.1109/ICDE65448.2025.00399},
	timestamp = {Wed, 10 Sep 2025 14:09:56 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BaiCLRWZYH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a family of Aligned Entropic Graph Kernels (AEGK) for graph classification, based on the Averaged Mixing Matrix (AMM) of Continuous-time Quantum Walks (CTQWs). Specifically, we show how the AMM matrix allows us to compute a quantum Shannon entropy of each vertex for either un-attributed or attributed graphs. For pairwise graphs, the proposed AEGK kernels are defined by computing the kernel-based similarity between the quantum Shannon entropies of their pairwise aligned vertices. Theoretical analysis reveals that the AEGK kernels can not only integrate the structural correspondence information between graphs, but also discriminate the structural differences between aligned vertices. Moreover, the AEGK kernels can simultaneously capture both global and local structural characteristics through the quantum Shannon entropies. These theoretical properties explain the effectiveness.}
}


@inproceedings{DBLP:conf/icde/ZhangXXXWZ25,
	author = {Xichong Zhang and
                  Haotian Xu and
                  Yin Xu and
                  Mingjun Xiao and
                  Jie Wu and
                  Jinrui Zhou},
	title = {Online Federated Learning on Distributed Unknown Data Using UAVs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {16--28},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00009},
	doi = {10.1109/ICDE65448.2025.00009},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangXXXWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Along with the advance of low-altitude economy, a variety of applications based on Unmanned Aerial Vehicles (UAVs) have been developed to accomplish diverse tasks. In this paper, we focus on the scenario of multiple UAVs performing Federated Learning (FL) tasks. Specifically, a group of UAVs is scheduled to repeatedly visit some Points of Interest (PoIs), collect the data produced by these PoIs, and jointly train a machine learning model based on the collected data. The most challenging issue is how to schedule UAVs to collect data so as to optimize the generalization and convergence of model training under the case that the distributions of the data produced by PoIs have not been known in advance. To address this issue, we propose a novel framework for online FL on distributed unknown data, named OFL-UD2, which is dedicated to online decision-making for UAVs to optimize model training performance. Concretely, we formulate the optimization problem while considering the convergence and quality of trained models as well as energy constraints. Then, we define a utility metric for the data quality of different PoIs and conduct a rigorous convergence analysis for OFL-UD2. Based on the analysis results, we design a two-stage algorithm to determine the scheduling of UAVs. Extensive simulations demonstrate that OFL-UD2can improve model accuracy and speed up running time compared to existing benchmarks significantly.}
}


@inproceedings{DBLP:conf/icde/FanRHHW25,
	author = {Yuankai Fan and
                  Tonghui Ren and
                  Can Huang and
                  Zhenying He and
                  X. Sean Wang},
	title = {Grounding Natural Language to {SQL} Translation with Data-Based Self-Explanations},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {29--42},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00010},
	doi = {10.1109/ICDE65448.2025.00010},
	timestamp = {Mon, 03 Nov 2025 08:25:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/FanRHHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Natural Language Interfaces for Databases em-power non-technical users to interact with data using natural language (NL). Advanced approaches, utilizing either neural sequence-to-sequence or more recent sophisticated large-scale language models, typically implement NL to SQL (NL2SQL) translation in an end-to-end fashion. However, like humans, these end-to-end translation models may not always generate the best SQL output on their first try. In this paper, we propose Cyclesql, an iterative framework designed for end-to-end translation models to autonomously generate the best output through self-evaluation. The main idea of CyClesqlis to introduce data-grounded NL explanations of query results as self-provided feedback, and use the feedback to validate the correctness of the translation iteratively, hence improving the overall translation accuracy. Extensive experiments, including quantitative and qualitative evaluations, are conducted to study Cyclesql by applying it to seven existing translation models on five widely used benchmarks. The results show that 1) the feedback loop introduced in Cyclesql can consistently improve the performance of existing models, and in particular, by applying Cyclesql to Resdsql, obtains a translation accuracy of 82.0% (+2.6 %) on the validation set, and 81.6 % (+3.2 %) on the test set of Spider benchmark; 2) the generated NL explanations can also provide insightful information for users, aiding in the comprehension of translation results and consequently enhancing the interpretability of NL2SQL translation11Our code is available at https://github.com/Kaimary/CycleSQL..}
}


@inproceedings{DBLP:conf/icde/ZhangMWHQMQKLGXLRZZ25,
	author = {Chaoyun Zhang and
                  Zicheng Ma and
                  Yuhao Wu and
                  Shilin He and
                  Si Qin and
                  Minghua Ma and
                  Xiaoting Qin and
                  Yu Kang and
                  Yuyi Liang and
                  Xiaoyu Gou and
                  Yajie Xue and
                  Qingwei Lin and
                  Saravan Rajmohan and
                  Dongmei Zhang and
                  Qi Zhang},
	title = {AllHands :Ask Me Anything on Large-scale Verbatim Feedback via Large
                  Language Models},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {43--57},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00011},
	doi = {10.1109/ICDE65448.2025.00011},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhangMWHQMQKLGXLRZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Verbatim feedback constitutes a valuable repository of user experiences, opinions, and requirements, crucial for data engineering and software development. Extracting meaningful insights from large-scale feedback data presents a significant challenge. This paper introduces Allhands, an innovative ana-lytic framework that transforms traditional large-scale feedback analysis tasks through a natural language interface, leveraging large language models (LLMs). Allhands performs initial classification and topic modeling on feedback to convert it into a structurally augmented format, enhancing accuracy, robustness and generalization with the aid of LLMs. Subsequently, an LLM-based code-first agent interprets users' diverse natural language questions about the feedback, automatically translates them into executable call of analytic tools or code, and delivers comprehensive multi-modal responses, including text, code, tables, and images. This eliminates the need for developing individual feedback analytic tools for each request, reducing human effort and making the system more accessible and flexible to users. We evaluate Allhands across three diverse feedback datasets, demonstrating its superior efficacy in all stages of analysis, from classification and topic modeling to providing an “ask me anything” experience with comprehensive, accurate, and human-readable responses. To the best of our knowl-edge, Allhands is the first comprehensive feedback analysis framework supporting diverse and customized insight extraction requirements through a natural language interface.}
}


@inproceedings{DBLP:conf/icde/XueSLWLW25,
	author = {Jingjing Xue and
                  Sheng Sun and
                  Min Liu and
                  Yuwei Wang and
                  Zhuotao Liu and
                  Jingyuan Wang},
	title = {Learnable Sparse Customization in Heterogeneous Edge Computing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {58--71},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00012},
	doi = {10.1109/ICDE65448.2025.00012},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XueSLWLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To effectively manage and utilize massive distributed data at the network edge, Federated Learning (FL) has emerged as a promising edge computing paradigm across data silos. However, FL still faces two challenges: system heterogeneity (i.e., the diversity of hardware resources across edge devices) and statistical heterogeneity (i.e., non-IID data). Although sparsification can extract diverse submodels for diverse clients, most sparse FL works either simply assign submodels with artificially-given rigid rules or prune partial parameters using heuristic strategies, resulting in inflexible sparsification and poor performance. In this work, we propose Learnable Personalized Sparsification for heterogeneous Federated learning (FedLPS), which achieves the learnable customization of heterogeneous sparse models with importance-associated patterns and adaptive ratios to simultaneously tackle system and statistical heterogeneity. Specifically, FedLPS learns the importance of model units on local data representation and further derives an importance-based sparse pattern with minimal heuristics to accurately extract personalized data features in non-IID settings. Furthermore, Prompt Upper Confidence Bound Variance (P-UCBV) is designed to adaptively determine sparse ratios by learning the superimposed effect of diverse device capabilities and non-IID data, aiming at resource self-adaptation with promising accuracy. Extensive experiments show that FedLPS outperforms status quo approaches in accuracy and training costs, which improves accuracy by 1.28% – 59.34% while reducing running time by more than 68.80%.}
}


@inproceedings{DBLP:conf/icde/LiangLXQH25,
	author = {Wenyi Liang and
                  Jianchun Liu and
                  Hongli Xu and
                  Chunming Qiao and
                  Liusheng Huang},
	title = {Many Hands Make Light Work: Accelerating Edge Inference via Multi-Client
                  Collaborative Caching},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {72--85},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00013},
	doi = {10.1109/ICDE65448.2025.00013},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiangLXQH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge inference is a technology that enables real-time data processing and analysis on clients near the data source. To ensure compliance with the Service-Level Objectives (SLOs), such as a 30% latency reduction target, caching is usually adopted to reduce redundant computations in inference tasks on stream data. Due to task and data correlations, sharing cache information among clients can improve the inference performance. However, the non-independent and identically distributed (non-IID) nature of data across different clients and the long-tail distributions, where some classes have significantly more samples than others, will reduce cache hit ratios and increase latency. To address the aforementioned challenges, we propose an efficient inference framework, CoCa, which leverages a multi-client collaborative caching mechanism to accelerate edge inference. On the client side, the model is pre-set with multiple cache layers to achieve a quick inference. During inference, the model performs sequential lookups at cache layers activated by the edge server. On the server side, CoCa uses a two-dimensional global cache to periodically aggregate information from clients, mitigating the effects of non-IID data. For client cache allocation, CoCa first evaluates the importance of classes based on how frequently and recently their samples have been accessed. CoCa then selects frequently recurring classes to address long-tail distribution challenges. Finally, CoCa dynamically activates cache layers to balance lookup overhead and accuracy. Extensive experiments demonstrate that CoCa reduces inference latency by 23.0% to 45.2% on the VGG, ResNet and AST models with a slight loss of accuracy.}
}


@inproceedings{DBLP:conf/icde/YangWLZF25,
	author = {Yuxin Yang and
                  Fang Wang and
                  Mengya Lei and
                  Peng Zhang and
                  Dan Feng},
	title = {ALT-Index: {A} Hybrid Learned Index for Concurrent Memory Database
                  Systems},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {86--98},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00014},
	doi = {10.1109/ICDE65448.2025.00014},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YangWLZF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The learned index technique has been widely explored as a strong competitor to traditional indexes. It adopts static learning-based models to fit the distribution of sorted data and locate keys through predictions, which shows outstanding query speed. However, frequent retraining is required when it comes to concurrent insertion scenarios. Despite existing studies introducing sparse slots and delta buffers to mitigate this effect, the read-write performance of the learned index still falls short of expectations, especially in concurrent conditions. In this paper, we first propose a novel hybrid index scheme that combines a read-efficient learned index with an insert-efficient Adaptive Radix Tree (ART) to realize high performance for read-write scenarios. However, it is not trivial due to expensive model prediction errors, complicated model hierarchy, and redundant node traversals. Therefore, we then introduce ALT-index, an efficient hybrid learned index with high concurrency for memory database systems. ALT-index highlights a delicate two-tier architecture where linear data are stored in the learned index without prediction errors and conflict data are hosted in the lower layer as an optimized ART. Besides, we develop a Greedy Pessimistic Linear (GPL) algorithm to support flattened data structures for concurrency. In the optimized ART layer, we introduce a fast and compact pointer buffer to further improve the overall performance. Experimental results conducted on various real-world datasets with 32 threads illustrate that ALT-index improves performance by up to 1.9x, 2.1x, and 2.3x compared with ALEX+, FINEdex, and XIndex in read-write-balanced scenarios, respectively.}
}


@inproceedings{DBLP:conf/icde/ChanIZUWXJ25,
	author = {Tsz Nam Chan and
                  Pak Lon Ip and
                  Bojian Zhu and
                  Leong Hou U and
                  Dingming Wu and
                  Jianliang Xu and
                  Christian S. Jensen},
	title = {Large-Scale Spatiotemporal Kernel Density Visualization},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {99--113},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00015},
	doi = {10.1109/ICDE65448.2025.00015},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChanIZUWXJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatiotemporal kernel density visualization (STKDV) is used extensively for many geospatial analysis tasks, including traffic accident hotspot detection, crime hotspot detection, and disease outbreak detection. However, STKDV is a computationally expensive operation, which does not scale to large-scale datasets, high resolutions, and a large number of timestamps. Although a recent approach, the sliding-window-based solution (SWS), reduces the time complexity of STKDV, it (i) is unable to reduce the time complexity for supporting STKDV-based exploratory analysis, (ii) is not theoretically efficient, and (iii) does not provide optimization techniques for bandwidth tuning. To eliminate these drawbacks, we propose a prefix-set-based solution (PREFIX) that encompasses three methods, namely PREFIXsingle (addressing (i)), PREFIXmultiple (addressing (ii)), and PREFIXtuning (addressing (iii)). We offer theoretical and practical evidence that PREFIX is capable of outperforming the state-of-the-art solution (SWS). In particular, PREFIX achieves at least 115x to 1,906x speedups and is the first solution that can efficiently generate multiple high-resolution STKDVs for the large-scale New York taxi dataset with 13.6 million data points.}
}


@inproceedings{DBLP:conf/icde/GuoCYYLYWWWCLPCZ25,
	author = {Jiarui Guo and
                  Boxuan Chen and
                  Kaicheng Yang and
                  Tong Yang and
                  Zirui Liu and
                  Qiuheng Yin and
                  Sha Wang and
                  Yuhan Wu and
                  Xiaolin Wang and
                  Bin Cui and
                  Tao Li and
                  Xi Peng and
                  Renhai Chen and
                  Gong Zhang},
	title = {HourglassSketch: An Efficient and Scalable Framework for Graph Stream
                  Summarization},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {114--127},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00016},
	doi = {10.1109/ICDE65448.2025.00016},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GuoCYYLYWWWCLPCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph stream is a special kind of data stream, where every item coming in sequence represents an edge in a dynamic graph. Graph stream has wide application in many fields, including cyber security, social networks and financial fraud detection. In this paper, we propose HourglassSketch, a two-stage data structure, for high-accuracy graph stream summarization. In Stage 1, HourglassSketch uses a CocoSketch to accurately record a partial collection of large-weight edges. In Stage 2, HourglassSketch integrates a TowerSketch with a TCMSketch to approximately record the statistics of most small-weight edges. In addition, we propose a key technique named Error Funnel to further reduce its error margin. Theoretical analysis and experimental results demonstrate that HourglassSketch supports various kinds of query operation and adapts well to graph stream storage. HourglassSketch achieves up to 100x smaller error and 2.7x higher speed than prior work. We also explore the versatility of HourglassSketch as a hardware-friendly framework by implementing it on FPGA and P4 platforms. We have released our codes on GitHub.}
}


@inproceedings{DBLP:conf/icde/ZhengYCKHZ25,
	author = {Shangfei Zheng and
                  Hongzhi Yin and
                  Tong Chen and
                  Xiangjie Kong and
                  Jian Hou and
                  Pengpeng Zhao},
	title = {{CADRL:} Category-Aware Dual-Agent Reinforcement Learning for Explainable
                  Recommendations over Knowledge Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {128--141},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00017},
	doi = {10.1109/ICDE65448.2025.00017},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhengYCKHZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graphs (KGs) have been widely adopted to mitigate data sparsity and address cold-start issues in recommender systems. While existing KGs-based recommendation methods can predict user preferences and demands, they fall short in generating explicit recommendation paths and lack explainability. As a step beyond the above methods, recent advancements utilize reinforcement learning (RL) to find suitable items for a given user via explainable recommendation paths. However, the performance of these solutions is still limited by the following two points. (1) Lack of ability to capture contextual dependencies from neighboring information. (2) The excessive reliance on short recommendation paths due to efficiency concerns. To surmount these challenges, we propose a category-aware dual-agent reinforcement learning (CADRL) model for explainable recommendations over KGs. Specifically, our model comprises two components: (1) a category-aware gated graph neural network that jointly captures context-aware item representations from neighboring entities and categories, and (2) a dual-agent RL framework where two agents efficiently traverse long paths to search for suitable items. Finally, experimental results show that CADRL outperforms state-of-the-art models in terms of both effectiveness and efficiency on large-scale datasets.}
}


@inproceedings{DBLP:conf/icde/StolteSPGW25,
	author = {Hermann Stolte and
                  Iftach H. Sadeh and
                  Elisa Pueschel and
                  Avigdor Gal and
                  Matthias Weidlich},
	title = {{SOUND:} Sanity Checking of Pipelines for Uncertain and Sparse Data
                  Series},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {142--155},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00018},
	doi = {10.1109/ICDE65448.2025.00018},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/StolteSPGW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The analysis of data series forms the basis of decision-making in various domains, so that it is essential to ensure data validity. Yet, current solutions for sanity checking of processing pipelines, such as GX, TFDV, Pandera or Deequ, fall short in accounting for data quality issues. In particular, irregular cadences, sparsity and value uncertainty limit the applicability of sanity checking and pose risks of false conclusions. In this paper, we present Sound to enable sanity checking of pipelines in the presence of typical quality issues in data series. In particular, Sound evaluates a set of sanity constraints that formalize validity expectations on the data, while incorporating data quality issues, i.e., uncertainty of individual data points and sparsity in a whole data series. To this end, it defines a statistical framework for constraint checking that is based on adaptive resampling and Bayesian hypothesis testing, minimizing computational costs while ensuring accurate results. If a constraint violation has been identified, Sound also includes drill-down strategies to guide users in the identification of the root cause of the violation. We demonstrate the feasibility and utility of Sound by applying it for pipelines developed in the domains of smart grid monitoring and astrophysics.}
}


@inproceedings{DBLP:conf/icde/ZhaoWWWL25,
	author = {Gengda Zhao and
                  Dong Wen and
                  Xiaoyang Wang and
                  Kai Wang and
                  Xuemin Lin},
	title = {Preserving K-Connectivity in Dynamic Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {156--169},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00019},
	doi = {10.1109/ICDE65448.2025.00019},
	timestamp = {Wed, 10 Sep 2025 14:09:46 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoWWWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge connectivity is a crucial concept in graph theory, which often serves as a fundamental metric for analyzing and improving the robustness, efficiency, and reliability of various types of networks. A graph is k-connected if it is still connected after removing arbitrarily fewer than  k k  edges. Sparse certificate is the lossless compression of a graph in terms of k-connectivity with a bounded size, which enables processing connectivity-driven queries in a more efficient way. Existing studies mainly focus on computing a sparse certificate in static graphs. Motivated by the prevalence of highly dynamic graphs, we aim to propose algorithms to maintain a sparse certificate when an edge is in-serted or deleted. We propose efficient algorithms to significantly improve the theoretical running time for both edge insertion and edge deletion compared with the baseline. We also propose a novel strategy for the search process in edge deletion. The strategy improves the efficiency compared with other potential methods with a theoretical guarantee. Extensive performance studies have been conducted on fourteen real-world datasets. The results demonstrate the significant advantages of our algorithms.}
}


@inproceedings{DBLP:conf/icde/XuMSLPHXZY25,
	author = {Wenzheng Xu and
                  Honglin Mao and
                  Heng Shao and
                  Weifa Liang and
                  Jian Peng and
                  Wen Huang and
                  Zichuan Xu and
                  Pan Zhou and
                  Jeffrey Xu Yu},
	title = {An Adaptive Sampling Algorithm for the Top-{\textdollar}K{\textdollar}
                  Group Betweenness Centrality},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {170--182},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00020},
	doi = {10.1109/ICDE65448.2025.00020},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XuMSLPHXZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Betweenness centrality is one of the key centrality measures in many applications including community detections in biological networks, vulnerability detections in communication networks, misinformation filtering in social networks, etc. The top- K K  group betweenness centrality problem is to find a group of  K K  nodes from a network so that the total fraction of shortest paths that pass through the  K K  nodes is maximized. Existing studies proposed randomized sampling algorithms for the problem. We notice that the existing studies ensured that, the maximum deviation of the estimated centrality of every group from its expectation is no greater than a small given threshold for all potential groups with no more than  K K  nodes, thereby generating too many samples, as the number of such groups is prohibitively large. In contrast, in this paper we first devise a novel algorithm that enables to estimate the centrality of a tentative group adaptively, and the algorithm immediately stops once the centrality is large enough; otherwise, the algorithm uses more samples to find a better group. We then theoretically show that, even the algorithm uses much less samples, it still can find a performance-guaranteed group with a large success probability. Experimental results with real-world networks demonstrate that the number of samples used by the proposed algorithm is from 2 to 18 times smaller than the state-of-the-art, while the centrality of the group found by the algorithm is no more than 4% smaller than the latter.}
}


@inproceedings{DBLP:conf/icde/XuCPWG25,
	author = {Chenhao Xu and
                  Chunyu Chen and
                  Jinglin Peng and
                  Jiannan Wang and
                  Jun Gao},
	title = {BQSched: {A} Non-Intrusive Scheduler for Batch Concurrent Queries
                  via Reinforcement Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {183--196},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00021},
	doi = {10.1109/ICDE65448.2025.00021},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/XuCPWG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most large enterprises build predefined data pipelines and execute them periodically to process operational data using SQL queries for various tasks. A key issue in minimizing the overall makespan of these pipelines is the efficient scheduling of concurrent queries within the pipelines. Existing tools mainly rely on simple heuristic rules due to the difficulty of expressing the complex features and mutual influences of queries. The latest reinforcement learning (RL) based methods have the potential to capture these patterns from feedback, but it is non-trivial to apply them directly due to the large scheduling space, high sampling cost, and poor sample utilization. Motivated by these challenges, we propose BQSched, a non-intrusive Scheduler for Batch concurrent Queries via reinforcement learning. Specifically, BQSched designs an attention-based state representation to capture the complex query patterns, and proposes IQ-PPO, an auxiliary task-enhanced proximal policy optimization (PPO) algorithm, to fully exploit the rich signals of Individual Query completion in logs. Based on the RL framework above, BQSched further introduces three optimization strategies, including adaptive masking to prune the action space, scheduling gain-based query clustering to deal with large query sets, and an incremental simulator to reduce sampling cost. To our knowledge, BQSched is the first non-intrusive batch query scheduler via RL. Extensive experiments show that BQSched can significantly improve the efficiency and stability of batch query scheduling, while also achieving remarkable scalability and adaptability in both data and queries. For example, across all DBMSs and scales tested, BQSched reduces the overall makespan of batch queries on TPC-DS benchmark by an average of 34% and 13%, compared with the commonly used heuristic strategy and the adapted RL-based scheduler, respectively. The source code of BQSched is available at https://github.com/chxu2000/BQSched.}
}


@inproceedings{DBLP:conf/icde/KhazaieP25,
	author = {Ahmad Khazaie and
                  Holger Pirk},
	title = {DEPA-Delta Shifting and Distribution Shaping for Efficient Adaptive
                  Indexing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {197--209},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00022},
	doi = {10.1109/ICDE65448.2025.00022},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KhazaieP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In dynamic environments, such as exploratory data analysis where the query and data workload changes frequently, the absence of prior knowledge about the workload poses a significant challenge in defining appropriate indices for a database. Frequent changes in the workload demand prompt response times for queries to increase user satisfaction, making it crucial to adapt the index dynamically. To address these challenges, adaptive indexing has emerged as a promising solution. The fundamental idea is to build the index incrementally during query processing. However, state-of-the-art adaptive indexing approaches under-utilize hardware resources, while progressive indexes sacrifice adaptivity to the workload. In this paper, we propose two novel techniques, namely Delta Shift Partitioning and Distribution Shaping Partitioning, that achieve tenfold better performance than the competitors without compromising the adaptivity of the index. Through low-level tuning and efficient hardware implementation, our proposed methods offer an optimal and adaptive solution to address the challenges of real-time query processing in dynamic workloads.}
}


@inproceedings{DBLP:conf/icde/WangJLZY25,
	author = {Yanshu Wang and
                  Jianan Ji and
                  Chao{-}Hsuan Liu and
                  Hengyang Zhou and
                  Tong Yang},
	title = {DaVinci Sketch: {A} Versatile Sketch for Efficient and Comprehensive
                  Set Measurements},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {210--223},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00023},
	doi = {10.1109/ICDE65448.2025.00023},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangJLZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Set measurements are fundamental in numerous areas including network measurement, database queries, and data mining. These measurements are typically executed on multisets. Existing algorithms optimize a specific set measurement task, leading to sophisticated but narrowly focused solutions. This specialization often results in inefficiencies when multiple set measurement tasks are required simultaneously, consuming excessive computational and storage resources. This paper introduces DaVinci Sketch, a versatile sketch designed to efficiently handle various set measurement tasks using a single unified data structure. DaVinci Sketch employs a novel approach by utilizing a dedicated structure to store frequent elements, thereby reducing collisions among flows that have the most significant impact on results. Remarkably, DaVinci Sketch can simultaneously perform up to nine different measurement tasks with a single data structure and a unified operation, whereas other approaches typically support fewer tasks. The experimental results demonstrate that DaVinci Sketch achieves high accuracy across 9 measurement tasks. Furthermore, in multi-task scenarios, DaVinci Sketch significantly reduces the memory usage (by more than 59%) and achieves high throughput (more than 23 times faster than other methods).}
}


@inproceedings{DBLP:conf/icde/GolzadehGS25,
	author = {Kiarash Golzadeh and
                  Lukasz Golab and
                  Jarek Szlichta},
	title = {Explaining Expert Search and Team Formation Systems with ExES},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {224--237},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00024},
	doi = {10.1109/ICDE65448.2025.00024},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GolzadehGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Expert search and team formation systems operate on collaboration networks, with nodes representing individuals, labeled with their skills, and edges denoting collaboration relationships. Given a keyword query corresponding to the desired skills, these systems identify experts that best match the query. However, state-of-the-art solutions to this problem lack trans-parency. To address this issue, we propose ExES, a tool designed to explain expert search and team formation systems using factual and counterfactual methods from the field of explainable artificial intelligence (XAI). ExES uses factual explanations to highlight important skills and collaborations, and counterfactual explanations to suggest new skills and collaborations to increase the likelihood of being identified as an expert. To speed up the explanation search, ExES implements a suite of pruning techniques. When tasked with explaining recent expert search and team formation systems, our pruning strategies can make ExES an order of magnitude faster than exhaustive explanation search, while still producing concise and actionable explanations.}
}


@inproceedings{DBLP:conf/icde/WuZCLSW25,
	author = {Hao Wu and
                  Mingxing Zhang and
                  Kang Chen and
                  Xia Liao and
                  Yingdi Shan and
                  Yongwei Wu},
	title = {{OOCC:} One-Round Optimistic Concurrency Control for Read-Only Disaggregated
                  Transactions},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {238--250},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00025},
	doi = {10.1109/ICDE65448.2025.00025},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WuZCLSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Read-only transactions predominate in many critical real-world scenarios. Yet, the presence of even a small proportion of read-write transactions poses challenges for existing Two-Phase Locking (2PL) and Optimistic Concurrency Control (OCC) based disaggregated transaction solutions. These approaches require either atomic operations or double reads to maintain consistent data for serializability, leading to suboptimal performance. This paper introduces OOCC, a novel One-round Optimistic Concurrency Control method tailored for disaggregated trans-actions. We propose that by intentionally postponing updates in write transactions for a moderate duration (a lease), it's possible to skip the validation phase in most OCC cases. This method enables read-only transactions to be completed within a single Round Trip Time (RTT) without involving any atomic operations. Additionally, we introduce several enhancements to boost OOCC's effectiveness in high-contention and write-intensive scenarios by reducing lock durations to just 1 RTT. Our experimental results demonstrate that OOCC significantly boosts transaction throughput in read-heavy environments, showing improvements ranging from 1.2 to 4 times. OOCC consis-tently achieves the lowest average latency (40 % -45 % lower than the best counterpart) in both read- and write-heavy workloads.}
}


@inproceedings{DBLP:conf/icde/XiangXCWZ25,
	author = {Sheng Xiang and
                  Chenhao Xu and
                  Dawei Cheng and
                  Xiaoyang Wang and
                  Ying Zhang},
	title = {Efficient Learning-Based Graph Simulation for Temporal Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {251--264},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00026},
	doi = {10.1109/ICDE65448.2025.00026},
	timestamp = {Wed, 10 Dec 2025 08:08:41 +0100},
	biburl = {https://dblp.org/rec/conf/icde/XiangXCWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph simulation has recently received a surge of attention in graph processing and analytics. In real-life applications, e.g. social science, biology, and chemistry, many graphs are composed of a series of evolving graphs (i.e., temporal graphs). While most of the existing graph generators focus on static graphs, the temporal information of the graphs is ignored. In this paper, we focus on simulating temporal graphs, which aim to reproduce the structural and temporal properties of the observed real-life temporal graphs. In this paper, we first give an overview of the existing temporal graph generators, including recently emerged learning-based approaches. Most of these learning-based methods suffer from one of the limitations: low efficiency in training or slow generating, especially for temporal random walk-based methods. Therefore, we propose an efficient learning-based approach to generate graph snapshots, namely temporal graph autoencoder (TGAE). Specifically, we propose an attention-based graph encoder to encode temporal and structural characteristics on sampled ego-graphs. And we proposed an ego-graph decoder that can achieve a good trade-off between simulation quality and efficiency in temporal graph generation. Finally, the experimental evaluation is conducted among our proposed TGAE and representative temporal graph generators on real-life temporal graphs and synthesized graphs. It is reported that our proposed approach outperforms the state-of-the-art temporal graph generators by means of simulation quality and efficiency.}
}


@inproceedings{DBLP:conf/icde/WangGBC25,
	author = {Xuemin Wang and
                  Tianlong Gu and
                  Xuguang Bao and
                  Liang Chang},
	title = {Towards Fair Graph Neural Networks via Graph Counterfactual Without
                  Sensitive Attributes},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {265--277},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00027},
	doi = {10.1109/ICDE65448.2025.00027},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangGBC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-structured data is ubiquitous in today's connected world, driving extensive research in graph analysis. Graph Neural Networks (GNNs) have shown great success in this field, leading to growing interest in developing fair GNNs for critical applications. However, most existing fair GNNs focus on statistical fairness notions, which may be insufficient when dealing with statistical anomalies. Hence, motivated by the causal theory, there has been growing attention to mitigating root causes of unfairness utilizing graph counterfactuals. Unfortunately, existing methods for generating graph counterfactuals invariably require the sensitive attribute. Nevertheless, in many real-world applications, it is usually infeasible to obtain sensitive attributes due to privacy or legal issues, which challenge existing methods. In this paper, we propose a framework named Fairwos (improving Fairness withQut sensitive attributes). In particular, we first propose a mechanism to generate pseudo-sensitive attributes to remedy the problem of missing sensitive attributes, and then design a strategy for finding graph counterfactuals from the real dataset. To train fair GNNs, we propose a method to ensure that the embeddings from the original data are consistent with those from the graph counterfactuals, and dynamically adjust the weight of each pseudo-sensitive attribute to balance its contribution to fairness and utility. Furthermore, we theoretically demonstrate that minimizing the relation between these pseudo-sensitive attributes and the prediction can enable the fairness of GNNs. Experimental results on six real-world datasets show that our approach outperforms state-of-the-art methods in balancing utility and fairness.}
}


@inproceedings{DBLP:conf/icde/YuanSCCLL25,
	author = {Long Yuan and
                  Xiaotong Sun and
                  Zi Chen and
                  Peng Cheng and
                  Longbin Lai and
                  Xuemin Lin},
	title = {{HINSCAN:} Efficient Structural Graph Clustering Over Heterogeneous
                  Information Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {278--291},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00028},
	doi = {10.1109/ICDE65448.2025.00028},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YuanSCCLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Structural graph clustering (SCAN) is one of the most popular graph clustering paradigms, and has attracted plenty of attention recently. Existing solutions assume that the input graphs is homogeneous, i.e., the vertices are of the same type. However, in many real applications, such as bibliographic networks and knowledge graphs, the input graphs is heterogeneous information networks which consist of multi-typed and interconnected objects, which makes SCAN cannot be applied to cluster. Therefore, in this paper, we study the SCAN problem over heterogeneous information networks. Based on the concept of meta-path, we propose two new structural graph clustering models first. Following these two new models, we design new algorithms to support the efficient clustering of a heterogeneous information network. We conduct extensive experiments on six real heterogeneous information networks, and the results demonstrate the effectiveness of our new models and the efficiency of our proposed clustering algorithms.}
}


@inproceedings{DBLP:conf/icde/LiaoSYXCYWW25,
	author = {Changyue Liao and
                  Mo Sun and
                  Zihan Yang and
                  Jun Xie and
                  Kaiqi Chen and
                  Binhang Yuan and
                  Fei Wu and
                  Zeke Wang},
	title = {Ratel: Optimizing Holistic Data Movement to Fine-tune 100B Model on
                  a Consumer {GPU}},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {292--306},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00029},
	doi = {10.1109/ICDE65448.2025.00029},
	timestamp = {Mon, 29 Sep 2025 15:55:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiaoSYXCYWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, AI researchers become more and more interested in fine-tuning a pre-trained LLM, whose size has grown to up to over 100B parameters, for their downstream tasks. One approach to fine-tune such huge models is to aggregate device memory from many GPUs. However, this approach introduces prohibitive costs for most data scientists with a limited budget for high-end GPU servers. In this paper, we focus on LLM fine-tuning on a single consumer-grade GPU in a commodity server with limited main memory capacity, which is accessible to most AI researchers. In such a scenario, existing offloading-based methods fail to fine-tune an LLM efficiently due to a lack of holistic intra-server tensor movement management. To this end, we present Ratel, a low-cost, high-performance deep learning training framework that enables efficient 100B-scale model fine-tuning on a commodity server with a consumergrade GPU and limited main memory capacity. The key idea is to add holistic offloading traffic as an optimization dimension for 1) active gradient offloading, and 2) holistic traffic-aware activation swapping mechanism. The experimental results show that 1) Ratel is the first to fine-tune a 175B model on an RTX 4090 and 256 GB main memory, 2) Ratel achieves 2.32x throughput than the state-of-the-art baselines when fine-tuning a small 13B model, and 3) Ratel enables a cheap low-end consumer GPU to have higher cost-effectiveness than a DGX-A100 cluster when fine-tuning a 175B model.}
}


@inproceedings{DBLP:conf/icde/WangWS25,
	author = {Ruoyu Wang and
                  Raymond Wong and
                  Daniel Sun},
	title = {Efficient Pruning via Entailment Cardinality Estimation for Fast Top-Down
                  Logic Rule Mining},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {307--320},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00030},
	doi = {10.1109/ICDE65448.2025.00030},
	timestamp = {Thu, 11 Sep 2025 15:16:39 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangWS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The cognitive capabilities of AI rely heavily on inter-pretable approaches, where logic rules and programs have been widely employed. Mining for expressive logic rules, especially in a top-down manner, has been improved over decades. Nevertheless, the efficiency of mining algorithms does not match the volume of target datasets, such as open-domain RDF knowledge bases. Most time and space have been wasted on exploring low-quality candidates in search routines, though various pruning efforts have been made in existing techniques. Estimation of rule quality or confidence has been applied in recent rule mining systems, but the efficiency of such approaches is still limited. In this paper, we propose an efficient pruning method based on entailment cardinality estimation. This approach is derived from a similar-distribution assumption and estimates the order of possible branches in beam search at a low cost. As a result, the number of entailment computations in each iteration of top-down mining has been reduced from a polynomial to a constant. The experimental results show that the rule mining procedure has been accelerated by up to 70x when the target KB is large and the used observation ratio is small. More than 90% of memory consumption has been reduced, while the memory overhead of the estimation technique is less than 1% of the overall memory cost.}
}


@inproceedings{DBLP:conf/icde/SunSZSXYZWW25,
	author = {Jie Sun and
                  Mo Sun and
                  Zheng Zhang and
                  Zuocheng Shi and
                  Jun Xie and
                  Zihan Yang and
                  Jie Zhang and
                  Zeke Wang and
                  Fei Wu},
	title = {Hyperion: Co-Optimizing {SSD} Access and {GPU} Computation for Cost-Efficient
                  {GNN} Training},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {321--335},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00031},
	doi = {10.1109/ICDE65448.2025.00031},
	timestamp = {Mon, 29 Sep 2025 15:55:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SunSZSXYZWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {SSDs are traditionally regarded as a cheap but slow way to scale up GNN training. Several GNN systems explore cheap single-machine single-GPU out-of-core training but fall short in terms of TPC (throughput per monetary cost). The underlying reason is that the existing systems 1) overly focus on minimizing the number of SSD accesses, which results in substantial unnecessary overhead on the CPU side, or 2) exhaust all GPU parallelism to saturate SSD but fail to overlap SSD accesses with GNN computation. In this work, we present Hyperion, a cost-efficient system for terabyte-scale GNN training. We argue that co-optimizing GPU-initiated asynchronous SSD access and GNN computation pipeline enables us to only add cheap NVMe SSDs, rather than expensive GPU servers, to achieve in- memory-like throughput and thus maximal TPC of GNN training. However, this is non-trivial due to imbalanced workloads and interference among IO submission, IO completion, and cache lookup. To tackle the challenges, Hyperion proposes three key designs. First, Hyperion proposes the first GPU-initiated pipeline- friendly asynchronous disk IO stack, which only requires about 1% GPU cores to saturate SSD throughput and wastes no GPU cores between IO submission and completion to fully overlap disk IO and computation. Second, we propose a new GPU-managed, disaggregated, and unified cache that disaggregates cache lookup from disk IO and fully utilizes CPU/GPU memory hierarchy by a unified static cache policy. Third, we propose a GNN-aware general TPC-analytical model that precisely predicts TPC under diverse hardware settings and GNN models and provide a hint to guide users to select hardware, e.g., number of SSDs, under a limited budget to maximize TPC. Experiments demonstrate that Hyperion can improve the TPC by over 3.1x on terabyte-scale graphs compared to SOTA out-of-core baselines and improve 60 x TPC compared to distributed in-memory baselines.}
}


@inproceedings{DBLP:conf/icde/WangWLBMWS25,
	author = {Hongya Wang and
                  Wenlong Wu and
                  Cong Luo and
                  Aobei Bian and
                  Chunguang Meng and
                  Yishuo Wu and
                  Ji Sun},
	title = {Boosting Accuracy and Efficiency for Vector Retrieval with Local Scaling
                  Graph},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {336--348},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00032},
	doi = {10.1109/ICDE65448.2025.00032},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangWLBMWS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vector database systems have been gaining more and more attention in recent years with the prevalence of Large Language Models. As the most important algorithmic component behind vector database systems, nearest neighbor search has been studied for decades and various approaches are proposed for efficient vector retrieval. Among these proposals, the graph-based search paradigm is able to achieve desirable accuracy-efficiency tradeoff, and thus has been widely used in many industrial vector retrieval engines. In this paper, however, we claim that its efficiency is largely handicapped by two unnoticed performance issues - accuracy saturation and long-tail queries, especially when the number of links is limited. Through both empirical and theoretical analysis, we identify that the existence of antihubs is the root cause of these performance limitations. To mitigates the negative impact of antihubs, we propose a highly efficient graph-based vector retrieval framework named Local Scaling Graph (LSG) by introducing more incident edges for them in a systematic way. We conduct comprehensive experiments using four state-of-the-art algorithms, i.e., HNSW, NSG, DiskANN and HNSWPQ, over a collection of 12 real-world datasets to validate the effectiveness and broad applicability of LSG. Empirical results show a speedup of up to two orders of magnitude over the state-of-the-art algorithms for approximate nearest neighbor search.}
}


@inproceedings{DBLP:conf/icde/DingZLMZD25,
	author = {Zhenghao Ding and
                  Xinyi Zhang and
                  Wei Lu and
                  Wenlong Ma and
                  Wenliang Zhang and
                  Xiaoyong Du},
	title = {Promi: Progressive Live Migration in Distributed Database Systems},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {349--362},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00033},
	doi = {10.1109/ICDE65448.2025.00033},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DingZLMZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data partitioning serves as a fundamental technique in distributed database systems, but skewed and dynamic work-loads often cause imbalanced load distribution among nodes. Live migration is crucial for addressing this imbalance by redistributing data partitions across nodes. However, existing migration methods either continue processing heavy transaction loads on overloaded nodes or block and abort live transactions during migration, failing to achieve both fast load balance and transactional zero downtime simultaneously. This paper introduces Promi, a live data migration method that progressively migrates data at the granularity of mini-partitions instead of entire partitions. To ensure fast load balance, we propose a graph-based migration scheduler that prioritizes the migration of hot mini - partitions and minimizes potential distributed transactions during migration. To achieve zero downtime and improve system performance, we propose a transaction manager that judiciously routes and schedules the involved transactions based on the current migration state. We conduct extensive experiments com-paring Promi against various live migration methods. The results show that Promi achieves up to 1.5 × higher throughput and reduces load balance time by up to 60% compared to state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/TianSY25,
	author = {Wei Tian and
                  Jieming Shi and
                  Man Lung Yiu},
	title = {Efficient Methods for Accurate Sparse Trajectory Recovery and Map
                  Matching},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {363--375},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00034},
	doi = {10.1109/ICDE65448.2025.00034},
	timestamp = {Fri, 17 Oct 2025 07:35:04 +0200},
	biburl = {https://dblp.org/rec/conf/icde/TianSY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-world trajectories are often sparse with low-sampling rates (i.e., long intervals between consecutive GPS points) and misaligned with road networks, yet many applications demand high-quality data for optimal performance. To improve data quality with sparse trajectories as input, we systematically study two related research problems: trajectory recovery on road network, which aims to infer missing points to recover high-sampling trajectories, and map matching, which aims to map GPS points to road segments to determine underlying routes. Capturing latent patterns in complex sparse trajectory data on road networks is challenging, especially with large-scale datasets. In this paper, we present efficient methods TRMMA and MMA for accurate trajectory recovery and map matching, respectively, where MMA serves as the first step of TRMMA. In MMA, we carefully formulate a classification task to map a GPS point from sparse trajectories to a road segment over a small candidate segment set, rather than the entire road network. We develop techniques in MMA to generate effective embeddings that capture the patterns of GPS data, directional information, and road segments, to accurately align sparse trajectories to routes. For trajectory recovery, TRMMA focuses on the segments in the route returned by MMA to infer missing points with position ratios on road segments, producing high-sampling trajectories efficiently by avoiding evaluation of all road segments. Specifically, in TRMMA, we design a dual-transformer encoding process to cohesively capture latent patterns in trajectories and routes, and an effective decoding technique to sequentially predict the position ratios and road segments of missing points. We conduct extensive experiments to compare TRMMA and MMA with numerous existing methods for trajectory recovery and map matching, respectively, on 4 large real-world datasets. TRMMA and MMA consistently achieve the best result quality, often by a significant margin. Moreover, TRMMA and MMA are highly efficient during training and inference, being up orders of magnitude faster than the next best competitors. The implementation is at https://github.com/derekwtian/TRMMA.}
}


@inproceedings{DBLP:conf/icde/ChaudharyBKLZM25,
	author = {Ankit Chaudhary and
                  Kaustubh Beedkar and
                  Jeyhun Karimov and
                  Felix Lang and
                  Steffen Zeuch and
                  Volker Markl},
	title = {Incremental Stream Query Placement in Massively Distributed and Volatile
                  Infrastructures},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {376--390},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00035},
	doi = {10.1109/ICDE65448.2025.00035},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChaudharyBKLZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {More and more data is produced outside the cloud by edge devices that provide basic processing capabilities. This trend enables a new class of data management systems that use both edge and cloud infrastructures for efficient data processing. Such systems push down operations by placing query operators close to the data-producing devices. A key challenge for these systems is handling the evolution of continuous queries and the dynamic changes in the infrastructure. In particular, frequent arrival or removal of queries and potential volatility of the infrastructure might invalidate or reduce the efficiency of previous operator placement decisions and thus might lead to constant, expensive re-optimizations of queries. These changes require new solutions for operator placement, which adjust existing placement decisions upon changes to the queries and infrastructure. In this paper, we propose ISQP, a framework that keeps the operator placements valid under query and infrastructure changes. ISQP performs a fine-grained identification of invalid operator placements and takes concurrent, incremental placement decisions to reduce the optimization time. ISQP works for arbitrary placement strategies, making it a general-purpose framework. Our evaluations show that ISQP reduces the optimization overhead by one order of magnitude compared to the baseline.}
}


@inproceedings{DBLP:conf/icde/MiaoDXZY25,
	author = {Ruijie Miao and
                  Xiangwei Deng and
                  Zicang Xu and
                  Ziyun Zhang and
                  Tong Yang},
	title = {LETFramework: Let the Universal Sketch be Accurate},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {391--404},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00036},
	doi = {10.1109/ICDE65448.2025.00036},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MiaoDXZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sketching algorithms are considered as promising solutions for approximate query tasks on large volumes of data streams. An ideal general-purpose data processing engine requires a sketch to achieve (1) high genericness in supporting a broad range of query tasks; (2) highfidelity in providing accuracy guarantee; and (3) high performance in practice. Although the universal sketch achieves high genericness and fidelity, its accuracy falls short of expectations. In this paper, we propose LETFramework (short for Lossless ExTraction Framework) to optimize the performance of the universal sketch. With the key technique of lossless extraction, LETFramework losslessly extracts the main body of the frequent items and stores the remaining information in the universal sketch, thereby achieving higher accuracy while maintaining high fidelity. We further introduce a unified methodology to incorporate the substitution strategies from top-k algorithms into LETFramework. Experiment results show that, LETFramework outperforms the universal sketch, achieving accuracy improvements ranging from 1 to 3 orders of magnitude on most query tasks and up to 15.73 times higher throughput. All the related source code is open-sourced and available at Github.}
}


@inproceedings{DBLP:conf/icde/HuZZQZZJTWX25,
	author = {Hao Hu and
                  Qiyang Zheng and
                  Xiangyu Zou and
                  Lisha Qin and
                  Chengwei Zhang and
                  Wanchuan Zhang and
                  Zhaoheng Jiang and
                  Dingwen Tao and
                  Hongpeng Wang and
                  Wen Xia},
	title = {A Cost-Effective and Decompression-Transparent Compressor for OLTP-Oriented
                  Databases},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {405--418},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00037},
	doi = {10.1109/ICDE65448.2025.00037},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuZZQZZJTWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The row-oriented store model is the cornerstone component of modern online transaction processing (OLTP) database systems. In response to the massive increase in data within database systems, compression techniques are employed to enhance storage efficiency. Regrettably, current compression methods suffer from either the amplification issue due to coarse compression granularity or inefficient decompression operations, thus usually decreasing the speed of query processing. To this end, we present DPTC, a cost-effective and decompression-transparent approach designed to compress data pages, the basic storage unit of OLTP database systems. Specifically, (1) DPTC applies a row-wise decompression-oriented structure to track the first occurrence of redundant data in compressed data, which effectively supports the decompression of individual records from pages, thereby avoiding unwarranted decompression in record access. Moreover, (2) DPTC employs an in-page dynamic packing strategy, which determines the compression units based on the impact of each data reduction operation on the compression gains and eliminates gains-inefficient data reductions. Furthermore, (3) DPTC utilizes a SIMD-based mechanism that leverages the characteristics of operations within the decompression process to improve the decompression speed. Our evaluation results confirm that DPTC is efficient in terms of decompression speed and compression ratio. Within an OLTP database system, DPTC yields throughput improvements of up to 4.28 x in TPC-C and reduces latency by up to 33.3% for data point queries in a row-oriented storage engine.}
}


@inproceedings{DBLP:conf/icde/ZengFHWCG25,
	author = {Zhihao Zeng and
                  Ziquan Fang and
                  Yuting Huang and
                  Qilong Wang and
                  Lu Chen and
                  Yunjun Gao},
	title = {Heterogeneous-Aware Traffic Prediction: {A} Privacy-Preserving Federated
                  Learning Framework},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {419--432},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00038},
	doi = {10.1109/ICDE65448.2025.00038},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZengFHWCG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatio-temporal traffic series prediction is essential in intelligent transportation systems, benefiting various applications such as route planning, vehicle dispatching, and congestion prediction. To tackle privacy leakage aroused by centralized forecasting methods, Federated Learning (FL), a privacy-preserving approach for decentralized model training into disjointed federated clients, has garnered widespread adoption in numerous traffic prediction endeavors. However, existing FL-based approaches ignore spatio-temporal heterogeneity among federated clients, including spatial feature skew, temporal coverage skew, and data quality skew. This makes them inapplicable and unsuitable to real-world scenarios and exhibits subpar prediction performance. To this end, we perform the first study of heterogeneous-aware traffic prediction in the federated environment, proposing a unified and effective framework named Fed4TP. It offers general federated capability for various centralized forecast models, supporting flow, speed, and occupancy prediction tasks. To address spatial feature heterogeneity, Fed4TP develops multi-dimensional personalized federated learning with positive samples contrastive learning for clustering to achieve personalized aggregation and global sharing across diverse clients. To overcome temporal coverage heterogeneity, Fed4TP designs a time window-based federated training mechanism, sequentially training client models and learning missed traffic information with varying time coverage. To tackle data quality heterogeneity, Fed4TP introduces a dual-driven method, i.e., global detection and local denoising, to improve client data quality. Extensive experiments on 4 real-life datasets verify the effectiveness and scalability superiority of Fed4TP in various federated-based traffic prediction tasks, compared with 24 well-known and state-of-the-art baselines. The source code and data of this work are available at https://github.com/ZJU-DAILY/Fed4TP.}
}


@inproceedings{DBLP:conf/icde/ZhuangSLLZCLPD25,
	author = {Qiyu Zhuang and
                  Xinyue Shi and
                  Shuang Liu and
                  Wei Lu and
                  Zhanhao Zhao and
                  Yuxing Chen and
                  Tong Li and
                  Anqun Pan and
                  Xiaoyong Du},
	title = {GeoTP: Latency-Aware Geo-Distributed Transaction Processing in Database
                  Middlewares},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {433--445},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00039},
	doi = {10.1109/ICDE65448.2025.00039},
	timestamp = {Thu, 15 Jan 2026 07:56:53 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhuangSLLZCLPD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread adoption of database middleware for supporting distributed transaction processing is prevalent in numerous applications, with heterogeneous data sources deployed across national and international boundaries. However, transaction processing performance significantly drops due to the high network latency between the middleware and data sources and the long lock contention span, where transactions may be blocked while waiting for the locks held by concurrent transactions. In this paper, we propose GeoTP, a latency-aware geo-distributed transaction processing approach in database middleware. GeoTP incorporates three key techniques to enhance performance in geo-distributed scenarios. First, we propose a decentralized prepare mechanism to reduce network round-trips for distributed transactions. Second, we design a latency-aware scheduler to minimize the lock contention span by strategically delaying the lock acquisition. Third, heuristic optimizations are proposed for the scheduler to reduce the lock contention span further. We implemented GeoTP on Apache Shardingsphere, a state-of-the-art middleware, and extended it into Apache ScalarDB. Experimental results on YCSB and TPC-C demonstrate that GeoTP achieves up to 17.7x performance improvement.}
}


@inproceedings{DBLP:conf/icde/KangSW25,
	author = {Rui Kang and
                  Shaoxu Song and
                  Jianmin Wang},
	title = {Exploring {SIMD} Vectorization in Aggregation Pipelines for Encoded
                  IoT Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {446--459},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00040},
	doi = {10.1109/ICDE65448.2025.00040},
	timestamp = {Thu, 25 Dec 2025 12:47:48 +0100},
	biburl = {https://dblp.org/rec/conf/icde/KangSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-series databases have been critical for collecting and analyzing data in industries where sensors send large amounts of IoT data by network devices. Both data received from networks and data collected in database storage are sufficiently encoded to reduce I/O occupation and latency. The IoT encoders successively combine the Delta, Repeat, and Packing operators, yielding a higher compression ratio than simply adopting each. However, efficient compression makes query execution even harder, requiring serial decoding before processing queries. Among them, selective aggregations, such as down-sampling, are the core of time series analytical queries. This paper identifies operators to process and accelerate IoT aggregation queries based on encoded data arrays, extensible to integrate thread-level and instruction-level designs. In addition, encoded data could aggregate directly in parallel without decoding, and encoding statistics can help to reduce unnecessary computation. Identified operators construct a pipeline query engine to integrate into an existing open source database, the Apache IoTDB. Remarkably, our systemic evaluations show vast improvements in the efficiency of selective aggregation over existing works.}
}


@inproceedings{DBLP:conf/icde/ChenXGLC25,
	author = {Jiazun Chen and
                  Yikuan Xia and
                  Jun Gao and
                  Zhao Li and
                  Hongyang Chen},
	title = {CommunityDF: {A} Guided Denoising Diffusion Approach for Community
                  Search},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {460--473},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00041},
	doi = {10.1109/ICDE65448.2025.00041},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ChenXGLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Denoising Diffusion Probabilistic Models (DDPMs) have recently demonstrated exceptional performance in generating high-quality data. In this work, we propose CommunityDF, a novel framework that applies DDPMs to the community search problem, which involves identifying subgraphs containing nodes closely related to a given query node. However, three key challenges arise in this context: (I) learning effective node representations from limited examples, (II) discretizing continuous node representations into community members, and (III) reducing the number of diffusion steps without sacrificing performance. To tackle these, CommunityDF introduces several innovations. First, we focus on subgraphs around the query node to reduce interference from unrelated nodes, improve scalability. We then employ a contrastive learning approach, treating node states at different diffusion steps as positive examples and designing various negative sampling strategies to learn high-quality node representations from limited examples. Second, we propose a dynamic thresholding mechanism that effectively converts continuous representations into community members. Finally, we reduce the number of diffusion steps by leveraging the rough communities to initialize the process with rough community structures, which accelerates convergence while maintaining high accuracy. Extensive experiments on seven real-world datasets demonstrate that CommunityDF outperforms existing methods by 16%-47%, establishing it as a state-of-the-art solution for community search. The source code is available at https://github.com/JiazunChen/CommunityDF.}
}


@inproceedings{DBLP:conf/icde/SiYLJMLW25,
	author = {Jianing Si and
                  Haitao Yuan and
                  Xiang Li and
                  Nan Jiang and
                  Xiao Ma and
                  Guoliang Li and
                  Shangguang Wang},
	title = {Having It Both Ways: Single Trajectory Embedding for Similarity Computation
                  with Pairwise Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {474--486},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00042},
	doi = {10.1109/ICDE65448.2025.00042},
	timestamp = {Wed, 10 Sep 2025 14:09:53 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SiYLJMLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory similarity measure is a fundamental component in trajectory databases, supporting many down-stream trajectory tasks. Existing similarity functions often exhibit unacceptable time complexities, hampering their efficiency for real-world scenarios. To address this limitation, learning-based approximation techniques utilizing trajectory embeddings have been proposed. However, creating a robust embedding model presents challenges, including the lack of direct involvement in the computational similarity process, adherence to non-metric similarity spaces, and the integration of precise similarity computation alignments. To address these challenges, we introduce DTisT, a novel embedding framework that enhances trajectory embeddings by pairwise learning from dual-trajectory input models. DTisT not only captures the dynamics of trajectory similarity computation through a dual-trajectory learning model but also integrates a learnable virtual trajectory to align the embedding space with non-metric similarity spaces effectively. Additionally, we incorporate aligned information from actual similarity computations into our embedding process using an attention mask mechanism. To ensure effective learning, we adopt a pre-train and fine-tune strategy, utilizing contrastive learning during the pre-training stage. Extensive experiments conducted on two real datasets demonstrate that DTisT surpasses state-of-the-art methods, showcasing its effectiveness in trajectory similarity embedding.}
}


@inproceedings{DBLP:conf/icde/MuWLS25,
	author = {Tianyu Mu and
                  Hongzhi Wang and
                  Chen Liang and
                  Xinyue Shao},
	title = {Auto-TSF: Towards Proxy-Model-Based Meta-Learning for Automatic Time
                  Series Forecasting Algorithm Selection},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {487--500},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00043},
	doi = {10.1109/ICDE65448.2025.00043},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MuWLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series forecasting (TSF) is a prominent chal-lenge in data analytics, relevant to both scientific research and real-world industrial applications. The rapid increase in high-dimensional time series data has led researchers to develop numerous models capable of handling complex forecasting tasks across diverse scenarios. Nevertheless, selecting an appropriate model and optimizing its parameters-an issue known as the Combined Algorithm Selection and Hyperparameter optimization (CASH) problem-remains a significant challenge. It is worth investigating how to satisfy both accuracy and efficiency in selecting an optimal algorithm and its hyperparameter configu-ration for a given time series with minimal human intervention. Unfortunately, there is no such work in the field of TSF, which has been developed for more than a decade. Existing methods suffer low selection rate of optimal algorithms. Meanwhile, the TSF task is extremely algorithm-sensitive, and the prediction performance of different algorithms and hyperparameter settings on the same data varies greatly. In this paper, we propose a Proxy-Model-based meta-learning TSF-CASH approach named Auto- Tsf. In the offline training phase, Auto- Tsfextracts the historical experience based on the proxy models, which is used to guide the automatic algorithm selection in the online working phase. The historical experience extracted in the offline phase not only significantly reduces the time consumption for algorithm selection, but also the introduction of the proxy model enhances the optimal algorithm selection rate. Moreover, we propose an asynchronous parallel HPO method in the most time-consuming HPO stage, which further improves the efficiency of the whole TSF -CASH. The experimental results demonstrate that Auto- Tsfachieves SOTA in terms of performance and efficiency compared to existing CASH methods.}
}


@inproceedings{DBLP:conf/icde/LiKZGT25,
	author = {Zhonggen Li and
                  Xiangyu Ke and
                  Yifan Zhu and
                  Yunjun Gao and
                  Yaofeng Tu},
	title = {HC-SpMM: Accelerating Sparse Matrix-Matrix Multiplication for Graphs
                  with Hybrid {GPU} Cores},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {501--514},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00044},
	doi = {10.1109/ICDE65448.2025.00044},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiKZGT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in graph computing and analytics. However, the irregularity of real-world graphs poses significant challenges to achieving efficient SpMM for graph data on GPUs. Recently, the introduction of new efficient computing cores within GPUs offers new opportunities for acceleration. In this paper, we present HC-SpMM, a pioneering algorithm that leverages Hybrid GPU Cores (Tensor cores and CUDA cores) to accelerate SpMM for graphs. To adapt to the computing characteristics ofdifferent GPU cores, we investigate the impact of sparse graph features on the performance of different cores, develop a data partitioning technique for the graph adjacency matrix, and devise a novel strategy for intelligently selecting the most efficient cores for processing each submatrix. Additionally, we optimize it by considering memory access and thread utilization. To support complex graph computing workloads, we integrate HC-SpMM into the GNN training pipeline. Furthermore, we propose a kernel fusion strategy to enhance data reuse, as well as a cost-effective graph layout reorganization method to mitigate the irregularity of real-world graphs, better fitting the computational models of hybrid GPU cores. Extensive experiments on 14 real-world datasets demonstrate that HC-SpMM achieves an average speedup of 1.33× and 1.23× over state-of-the-art SpMM kernels and GNN frameworks.}
}


@inproceedings{DBLP:conf/icde/ChenLZG25,
	author = {Yu Chen and
                  Qing Liu and
                  Yifan Zhu and
                  Yunjun Gao},
	title = {Efficient {\textdollar}{\textbackslash}eta{\textdollar}-Threshold
                  Maintenance in Dynamic Uncertain Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {515--528},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00045},
	doi = {10.1109/ICDE65448.2025.00045},
	timestamp = {Wed, 10 Sep 2025 14:09:48 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenLZG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The  \\eta \\eta -threshold decomposition in uncertain graphs, which calculates the  \\eta \\eta -thresholds for each vertex, is a fundamental problem for graph analysis. While existing studies on  \\eta \\eta -threshold decomposition primarily focus on static uncertain graphs, numerous real-world scenarios involve highly dynamic uncertain graphs. It is costly to recompute all  \\eta \\eta -thresholds from scratch whenever the uncertain graphs face update operations, e.g., edge insertion and deletion, and the modifications on edge probability. Motivated by this, we introduce efficient  \\eta \\eta -threshold maintenance algorithms tailored for dynamic uncertain graphs in this paper. Firstly, we investigate the impact of edge insertion and deletion on  \\eta \\eta -thresholds. Building upon this analysis, we introduce the maintenance algorithms designed to adjust the  \\eta \\eta  thresholds for edge insertions or deletions within the uncertain graphs. Our approaches involve identifying a compact subgraph encompassing all vertices necessitating  \\eta \\eta -threshold updates, followed by an iterative process of vertex deletion to complete the  \\eta \\eta -threshold updates. To improve the efficiency, we devise three optimizations to further reduce the number of candidate  \\eta \\eta -thresholds requiring adjustment. Moreover, we extend the proposed algorithms to handle the  \\eta \\eta -threshold maintenance for edge probability change. Extensive experiments on both real and synthetic datasets demonstrate the efficiency of the proposed algorithms. The results reveal that our proposed algorithms consistently outperform the baselines, exhibiting improvements ranging from at least three orders of magnitude to as high as seven orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/JiLBC25,
	author = {Daomin Ji and
                  Hui Luo and
                  Zhifeng Bao and
                  J. Shane Culpepper},
	title = {Dataset Discovery via Line Charts},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {529--542},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00046},
	doi = {10.1109/ICDE65448.2025.00046},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/JiLBC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Line charts are a valuable tool for data analysis and exploration, distilling essential insights from a dataset. However, access to the underlying data used to create a line chart is rarely readily available. In this paper, we explore a novel dataset discovery problem, dataset discovery via line charts, focusing on the use of line charts as queries to discover datasets within a large data repository that are capable of generating similar line charts. To solve this problem, we propose a novel approach called Fine-grained Cross-modal Relevance Learning Model (FCM), which aims to estimate the relevance between a line chart and raw data from a candidate dataset. To achieve this goal, FCM first applies a visual element extractor to extract visual elements, i.e., lines and y-axis ticks, from a line chart. Then, two novel segment-level encoders are applied to learn representations for a line chart and a candidate dataset, preserving fine-grained information, followed by a cross-modal matcher that matchs the learned representations in a fine-grained manner. Furthermore, we extend FCM to support line chart query generated based on data aggregation. Last, we provide a benchmark tailored for this problem since no such dataset exists. Extensive evaluation on the new benchmark verifies the effectiveness of our proposed method. Specifically, our proposed approach surpasses the best baseline by 30.1% and 41.0% in terms of prec@50 and ndcg@50, respectively.}
}


@inproceedings{DBLP:conf/icde/FuCCWH25,
	author = {Yujian Fu and
                  Cheng Chen and
                  Yao Chen and
                  Weng{-}Fai Wong and
                  Bingsheng He},
	title = {Vista: Vector Indexing and Search for Large-Scale Imbalanced Datasets},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {543--556},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00047},
	doi = {10.1109/ICDE65448.2025.00047},
	timestamp = {Wed, 10 Sep 2025 14:09:49 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FuCCWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rise of machine learning models, particularly generative models like sequence-to-vector models, there is a high demand for constructing efficient approximate nearest neighbor search (ANNS) indexes on the embedding vectors they generate. Despite the development of numerous indexes for efficient vector retrieval, the complex distributions of vectors generated by these models and their impact on ANNS tasks remain underexplored. In this work, we address the challenges faced by current advanced ANNS approaches when dealing with vectors characterized by imbalanced distributions, which negatively impact search efficiency. We identify the difficulty in indexing and searching certain vectors using previous ANNS graph indexes due to skewed distributions and propose a novel index, Vista, that improves efficiency by introducing dynamic index construction patterns based on vector distribution. Our experimental evaluation confirms Vista's efficiency advantage, demonstrating that on both public and industrial-grade real-world imbalanced datasets, Vista achieves several to tens of times performance improvement compared to advanced ANNS indexes while ensuring high search accuracy and good scalability.}
}


@inproceedings{DBLP:conf/icde/ChangFLGZ25,
	author = {Xueqin Chang and
                  Jiajie Fu and
                  Qing Liu and
                  Yunjun Gao and
                  Baihua Zheng},
	title = {Time-Aware Influence Minimization via Blocking Social Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {557--570},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00048},
	doi = {10.1109/ICDE65448.2025.00048},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChangFLGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate the Time-aware Influence Minimization (TIMIN) problem in social networks, focusing on minimizing negative influence concerning a critical deadline by temporarily blocking specific nodes in the given social network. First, we introduce the Temporal Linear Threshold (TLT) model, a novel framework that incorporates time delay in influence propagation, the decay of influence power over time, and the lifecycle of influence. Building on this model, we formally define the Timin problem and prove its NP-hardness, monotonicity, and supermodularity. To tackle the Timin problem, we develop the Timin-Greedy, a greedy algorithm that achieves  (1\\ -1/e) (1\\ -1/e)  approximation. Since exact computation of negative influence spread for any node set in Timin-Greedy is #P-hard, we propose TESTIM, a scalable implementation that provides  (1-1/e-\\epsilon) (1-1/e-\\epsilon)  approximation. To further enhance the efficiency, we introduce NReplacer, a heuristic algorithm leveraging the insight that potential blocking nodes often cluster near the negative source. Our extensive experimental evaluations demonstrate several key findings: (1) TESTIM is up to 10× faster than the baselines while achieving 30%–50% more reductions in negative influence spread, and (2) NReplacer exhibits a 5× speedup compared to TESTIM, with comparable reductions in negative influence spread.}
}


@inproceedings{DBLP:conf/icde/ShenYCWLJJCSDCWSRL25,
	author = {Zhitao Shen and
                  Shiyu Yang and
                  Weibo Chen and
                  Kunming Wang and
                  Yue Li and
                  Jiabao Jin and
                  Wei Jia and
                  Junwei Chen and
                  Yuan Su and
                  Xiaoxia Duan and
                  Wei Chen and
                  Lei Wang and
                  Jie Song and
                  Ruoyi Ruan and
                  Xuemin Lin},
	title = {TierBase: {A} Workload-Driven Cost-Optimized Key-Value Store},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {571--584},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00049},
	doi = {10.1109/ICDE65448.2025.00049},
	timestamp = {Wed, 10 Sep 2025 14:09:47 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ShenYCWLJJCSDCWSRL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the current era of data-intensive applications, the demand for high-performance, cost-effective storage solutions is paramount. This paper introduces a Space-Performance Cost Model for key-value store, designed to guide cost-effective storage configuration decisions. The model quantifies the trade-offs between performance and storage costs, providing a framework for optimizing resource allocation in large-scale data serving environments. Guided by this cost model, we present Tier-Base, a distributed key-value store developed by Ant Group that optimizes total cost by strategically synchronizing data between cache and storage tiers, maximizing resource utilization and effectively handling skewed workloads. To enhance cost-efficiency, TierBase incorporates several optimization techniques, including pre-trained data compression, elastic threading mechanisms, and the utilization of persistent memory. We detail TierBase's architecture, key components, and the implementation of cost optimization strategies. Extensive evaluations using both synthetic benchmarks and real-world workloads demonstrate TierBase's superior cost-effectiveness compared to existing solutions. Furthermore, case studies from Ant Group's production environments showcase TierBase's ability to achieve up to 62% cost reduction in primary scenarios, highlighting its practical impact in large-scale online data serving.}
}


@inproceedings{DBLP:conf/icde/YangWCSP25,
	author = {Wenzhe Yang and
                  Sheng Wang and
                  Zhiyu Chen and
                  Yuan Sun and
                  Zhiyong Peng},
	title = {Joinable Search Over Multi-Source Spatial Datasets: Overlap, Coverage,
                  and Efficiency},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {585--598},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00050},
	doi = {10.1109/ICDE65448.2025.00050},
	timestamp = {Tue, 25 Nov 2025 07:46:53 +0100},
	biburl = {https://dblp.org/rec/conf/icde/YangWCSP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The search for joinable data is pivotal for numerous applications, such as data integration, data augmentation, and data analysis. Although there have been many successful joinable search studies for table discovery, the study of finding joinable spatial datasets for a given query from multiple spatial data sources has not been well considered. This paper studies two cases of joinable search problems from multiple spatial data sources. In addition to the overlap joinable search problem (OJSP), we also propose a novel coverage joinable search problem (CJSP) that has not been considered before, motivated by many real-world applications in the field of spatial search. To support two cases of joinable search over multiple spatial data sources seamlessly, we propose a multi-source spatial dataset search framework. Firstly, we design a DIstributed Tree-based Spatial index structure called DITS, which is used not only to design acceleration strategies to speed up joinable searches, but also to support efficient communication between multiple data sources. Additionally, we prove that the CJSP is NP-hard and design a greedy approximate algorithm to solve the problem. We evaluate the efficiency of our search framework on five real-world data sources, and the experimental results show that our framework can significantly reduce running time and communication costs compared with baselines.}
}


@inproceedings{DBLP:conf/icde/WangYL25,
	author = {Kaixin Wang and
                  Kaiqiang Yu and
                  Cheng Long},
	title = {Maximal Clique Enumeration with Hybrid Branching and Early Termination},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {599--612},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00051},
	doi = {10.1109/ICDE65448.2025.00051},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Maximal clique enumeration (MCE) is crucial for tasks like community detection and biological network analysis. Existing algorithms typically adopt the branch-and-bound frame-work with the vertex-oriented Bron-Kerbosch (BK) branching strategy, which forms the sub-branches by expanding the partial clique with a vertex. In this paper, we present a novel approach, HBBMC, a hybrid framework combining vertex-oriented BK branching and edge-oriented BK branching, where the latter adopts a branch-and-bound framework which forms the sub-branches by expanding the partial clique with a edge. This hybrid strategy enables more effective pruning and helps achieve a worst-case time complexity better than the best-known one under a condition which holds for the majority of real-world graphs. To further enhance efficiency, we introduce an early termination technique, which leverages the topological information of the graphs and constructs the maximal cliques directly without branching. Our early termination technique is applicable to all branch-and-bound frameworks. Extensive experiments demonstrate the superior performance of our techniques.}
}


@inproceedings{DBLP:conf/icde/WeiLHCZZ25,
	author = {Rukai Wei and
                  Yu Liu and
                  Yufeng Hou and
                  Heng Cui and
                  Yongqiang Zhang and
                  Ke Zhou},
	title = {TopTune: Tailored Optimization for Categorical and Continuous Knobs
                  Towards Accelerated and Improved Database Performance Tuning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {613--626},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00052},
	doi = {10.1109/ICDE65448.2025.00052},
	timestamp = {Sat, 18 Oct 2025 08:54:35 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WeiLHCZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Using a machine learning (ML) model as a core component in database knob tuning has demonstrated remarkable advancements in recent years. However, a model that optimizes both categorical and continuous values in the same way may not guarantee efficiency and effectiveness in knob tuning. This is due to the fact that the usual assumption of a differentiable input space for efficient exploration of continuous spaces does not hold true in categorical spaces. Moreover, the inherent complexity of interdependences among knobs and the high-dimensionality of the configuration space compound the challenges of tuning. In this paper, we propose TopTune, which employs tailored optimization for continuous and categorical knobs, to achieve accelerated tuning efficiency and improved tuning performance. Specifically, we decompose the configuration space into two orthogonal subspaces: categorical and continuous spaces. Subsequently, we employ Bayesian optimization models, i.e., SMAC and GP to explore the categorical and continuous subspaces, respectively. These two models will alternately explore the two spaces with the proposed communication mechanism to ensure TopTune can capture the dependence between continuous and categorical knobs. Furthermore, to balance efficiency and accuracy, we utilize a knob-dimensional projection strategy to reduce the exploration domain by embedding the high-dimension configuration space into a lower-dimensional proxy space. In addition, we implement batch Bayesian optimization technology, which enables parallel knob evaluation while balancing exploration and exploitation. We evaluate TopTune under different benchmarks (SYSBENCH, TPC-C, and JOB), metrics (throughput and latency), and DBMSs (MySQL and Dameng). Extensive experiments demonstrate that TopTune identifies better configurations in up to approximately 12.2× less time while achieving a 10.7% improvement in throughput compared to state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/YuanYWCW25,
	author = {Qin Yuan and
                  Ye Yuan and
                  Zhenyu Wen and
                  Chi Chen and
                  Guoren Wang},
	title = {CrossEM: {A} Prompt Tuning Framework for Cross-Modal Entity Matching},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {627--640},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00053},
	doi = {10.1109/ICDE65448.2025.00053},
	timestamp = {Wed, 15 Oct 2025 07:24:02 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YuanYWCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity matching (EM) aims to identify equivalent entities across different data sources. Current EM assumes that these data are either homogeneous with aligned schema or heterogeneous but can be transformed into a unified modality. There is an urgent need to consider the entities with different modalities to support practical application scenarios over data lakes such as multi-modal data integration and recommendation system. It is impractical to unify their data modalities. To support EM on heterogeneous entity with different data formats and modalities, we propose cross-modal entity matching in this paper. Inspired by the promising performance achieved by recent pre-trained models, we perform cross-modal entity matching by prompt-tuning pre-trained multi-modal large models (MMLMs) in an unsupervised manner. However, the prompt-tuning faces three challenging issues: (i) objective gap between pre-training and tuning of MMLMs; (ii) data modality gap between the inputs of MMLMs and our matching task; (iii) prompt efficiency on large data. Therefore, we firstly propose a novel EM framework (namely, CrossEM) that addresses cross-modal EM as a matching probability problem with specific prompt-tuning. Secondly, two alternative prompt generation methods are designed to extract structural knowledge from heterogeneous data to overcome the data modality gap with pre-trained models. Thirdly, we present an improved matching framework (namely, CrossEM+) to boost the prompt efficiency on large heterogeneous data. Experimental evaluations verify that our methods significantly outperform the state-of-the-art approaches on three benchmarks. Furthermore, our case study highlights the considerable potential of cross-modal EM in improving the performance of downstream tasks, thereby benefitting a wider range of research areas.}
}


@inproceedings{DBLP:conf/icde/YuanWQYW25,
	author = {Qin Yuan and
                  Zhenyu Wen and
                  Jiaxu Qian and
                  Ye Yuan and
                  Guoren Wang},
	title = {CrossETR: {A} Semantic-Driven Framework for Entity Matching Across
                  Images and Graph},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {641--654},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00054},
	doi = {10.1109/ICDE65448.2025.00054},
	timestamp = {Wed, 15 Oct 2025 07:24:01 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YuanWQYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity matching (EM) aims to identify whether two entities from different data sources refer to the same real-world entity. Most existing cross-modal EM assume that images have simple scenes containing few objects, or do not fully consider the cross-modal knowledge associated with entities. To support more practical application scenarios such as multi-modal knowledge graph integration and visual question answering in data lakes, we introduce our problem of semantic-driven EM across graph and images in this paper. Current semantically matching solutions over cross-modal data face the obstacle of low training efficiency, since their time complexity quadratically grows with the number of entities. To alleviate this issue, we present a novel framework (namely CrossETR) that follows an exploration-then-refinement paradigm. Firstly, a candidate exploration policy is proposed to boost the training efficiency. It explores candidate pairs according to entity correlations and captures structural semantics by adaptive sampling the most informative neighborhood subgraphs. Secondly, the cross-modal entity representations are refined to break modality heterogeneity to support unsupervised matching prediction. Extensive experimental evaluations on three publicly available benchmarks demonstrate the superiority of CrossETR over state-of-the-art approaches in terms of effectiveness and efficiency. Furthermore, a case study highlights that our proposed semantic-driven EM is promising to improve the performance of downstream tasks such as multi-modal knowledge graph integration.}
}


@inproceedings{DBLP:conf/icde/ZhanCCCNL25,
	author = {Jiexi Zhan and
                  Yu Chen and
                  Peng Cheng and
                  Lei Chen and
                  Wangze Ni and
                  Xuemin Lin},
	title = {StructRide: {A} Framework to Exploit the Structure Information of
                  Shareability Graph in Ridesharing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {655--668},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00055},
	doi = {10.1109/ICDE65448.2025.00055},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhanCCCNL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ridesharing services play an essential role in modern transportation, which significantly reduces traffic congestion and exhaust pollution. In the ridesharing problem, improving the sharing rate between riders can not only save the travel cost of drivers but also utilize vehicle resources more efficiently. The existing online-based and batch-based methods for the ridesharing problem lack the analysis of the sharing relationship among riders, leading to a compromise between efficiency and accuracy. In addition, the graph is a powerful tool to analyze the structure information between nodes. Therefore, in this paper, we propose a framework, namely StructRide, to utilize the structure information to improve the results for ridesharing problems. Specifically, we extract the sharing relationships between riders to construct a shareability graph. Then, we define a novel measurement, namely shareability loss, for vehicles to select groups of requests such that the unselected requests still have high probabilities of sharing with other requests. Our SARD algorithm can efficiently solve dynamic ridesharing problems to achieve dramatically improved results. Through extensive experiments, we demonstrate the efficiency and effectiveness of our SARD algorithm on two real datasets. Our SARD can run up to 72.68 times faster and serve up to 50% more requests than the state-of-the-art algorithms.}
}


@inproceedings{DBLP:conf/icde/GuoWCS25,
	author = {Yang Guo and
                  Tianyu Wang and
                  Zizhan Chen and
                  Zili Shao},
	title = {A Storage Model with Fine-Grained In-Storage Query Processing for
                  Spatio-Temporal Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {669--682},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00056},
	doi = {10.1109/ICDE65448.2025.00056},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/GuoWCS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive spatio-temporal data are continuously generated by various moving objects. To process these data for applications such as traffic forecasting, existing spatio-temporal systems all employ the move-data-to-computation paradigm. However, this approach suffers from significant data movement overhead between hosts and drives. To address this issue, this work introduces Groundhog, an efficient in-storage computing technique designed specifically for spatio-temporal queries, aimed at reducing unnecessary data movement and computations. Groundhog introduces three key designs for efficient in-storage computing: (i) a self-contained and segment-based storage model, which is lightweight for in-storage computing and enables fine-grained pruning for spatio-temporal queries; (ii) a set of fine-grained techniques to optimize query processing inside storage devices for spatio-temporal queries; and (iii) an in-storage-computing-aware query planner, which offloads spatio-temporal queries in a fine-grained manner using a cost-based approach. We implemented Groundhog on a real hardware board. Extensive experiments conducted on real-world datasets demonstrate that Groundhog achieves significant performance improvements, with latency reductions of up to 81 % for widely used spatio-temporal queries compared to host computing solutions.}
}


@inproceedings{DBLP:conf/icde/DuCZLCXN25,
	author = {Leilei Du and
                  Peng Cheng and
                  Libin Zheng and
                  Xiang Lian and
                  Lei Chen and
                  Wei Xi and
                  Wangze Ni},
	title = {Numerical Estimation of Spatial Distributions Under Differential Privacy},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {683--695},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00057},
	doi = {10.1109/ICDE65448.2025.00057},
	timestamp = {Thu, 06 Nov 2025 20:21:55 +0100},
	biburl = {https://dblp.org/rec/conf/icde/DuCZLCXN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating spatial distributions is important in data analysis, such as traffic flow forecasting and epidemic prevention. To achieve accurate spatial distribution estimation, the analysis needs to collect sufficient user data. However, collecting data directly from individuals could compromise their privacy. Most previous works focused on private distribution estimation for one-dimensional data, which does not consider spatial data relation and leads to poor accuracy for spatial distribution estimation. In this paper, we address the problem of private spatial distribution estimation, where we collect spatial data from individuals and aim to minimize the distance between the actual distribution and estimated one under Local Differential Privacy (LDP). To leverage the numerical nature of the domain, we project spatial data and its relationships onto a one-dimensional distribution. We then use this projection to estimate the overall spatial distribution. Specifically, we propose a reporting mechanism called Disk Area Mechanism (DAM), which projects the spatial domain onto a line and optimizes the estimation using the sliced Wasserstein distance. Through extensive experiments, we show the effectiveness of our DAM approach on both real and synthetic data sets, compared with the state-of-the-art methods, such as Multi-dimensional Square Wave Mechanism (MDSW) and Subset Exponential Mechanism with Geo-I (SEM-Geo-I). Our results show that our DAM always performs better than MDSW and is better than SEM-Geo-I when the data granularity is fine enough.}
}


@inproceedings{DBLP:conf/icde/LiaoLJCHX25,
	author = {Xuankun Liao and
                  Qing Liu and
                  Jiaxin Jiang and
                  Byron Choi and
                  Bingsheng He and
                  Jianliang Xu},
	title = {Accelerating D-Core Maintenance over Dynamic Directed Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {696--709},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00058},
	doi = {10.1109/ICDE65448.2025.00058},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiaoLJCHX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a directed graph  G G  and two non-negative integers  k k  and  l l , a D-core, or ( k k , l)-core, is the maximal subgraph  H\\subseteq G H\\subseteq G  where each vertex in  H H  has an in-degree and out-degree not smaller than  k k  and  I I , respectively. D-cores have found extensive applications, such as social network analysis, fraud detection, and graph visualization. In these applications, graphs are highly dynamic and frequently updated with the insertions and deletions of vertices and edges, making it costly to recompute the D-cores from scratch to handle the updates. In the literature, the peeling-based algorithm has been proposed to handle D-core maintenance. However, the peeling-based method suffers from efficiency issues, e.g., it may degenerate into recomputing all the D-cores and is inefficient for batch updates due to sequential processing. To address these limitations, we introduce novel algorithms for incrementally maintaining D-cores in dynamic graphs. We begin by presenting the theoretical findings to identify the D-cores that should be updated. By leveraging these theoretical analysis results, we propose a local-search-based algorithm with optimizations to handle single-edge insertions and deletions. We further propose an H-index-based algorithm for scenarios involving batch updates. Several novel edge-grouping strategies are proposed to improve the efficiency of the H-index-based algorithm. Extensive empirical evaluations over both real-world and synthetic networks demonstrate that our proposed algorithms are up to 5 orders of magnitude faster than the peeling-based method.}
}


@inproceedings{DBLP:conf/icde/ShaoCLYNL25,
	author = {Yu Shao and
                  Peng Cheng and
                  Longbin Lai and
                  Long Yuan and
                  Wangze Ni and
                  Xuemin Lin},
	title = {Most Probable Maximum Weighted Butterfly Search},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {710--723},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00059},
	doi = {10.1109/ICDE65448.2025.00059},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ShaoCLYNL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncertain butterflies are fundamental and popular graphlet motifs within uncertain bipartite networks, serving as a crucial metric in structural analysis. Despite extensive research have studied butterflies sufficiently on deterministic networks, few of works explore uncertain butterflies. In this paper, we introduce the Most Probable Maximum Weighted Butterfly (MPMB), which holds the highest probability of becoming a maximum weighted butterfly on an uncertain bipartite network. Proved that searching MPMBs is NP-Hard, we then proposed two samplingbased methods, namely Ordering Sampling (OS), and Ordering-Listing Sampling (OLS). The OS method is suitable for singletrial sampling, while the OLS method is optimized for multiple trials, which first finds candidate butterflies in rough before searching MPMBs. Our experimental results indicate that our basic method (OS) performs 1000× faster than the baseline and the optimized method (OLS) achieves another 180× speedup.}
}


@inproceedings{DBLP:conf/icde/EsmailoghliSMA25,
	author = {Mahdi Esmailoghli and
                  Christoph Schnell and
                  Ren{\'{e}}e J. Miller and
                  Ziawasch Abedjan},
	title = {{BLEND:} {A} Unified Data Discovery System},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {737--750},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00061},
	doi = {10.1109/ICDE65448.2025.00061},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/EsmailoghliSMA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most research on data discovery has so far focused on improving individual discovery operators such as join, correlation, or union discovery. However, in practice, a combination of these techniques and their corresponding indexes may be necessary to support arbitrary discovery tasks. We propose BLEND, a comprehensive data discovery system that supports existing operators and enables their flexible pipelining. BLEND is based on a set of lower-level operators that serve as fundamental building blocks for more complex and sophisticated user tasks. To reduce the execution runtime of discovery pipelines, we propose a unified index structure and a rule- and cost-based optimizer that rewrites SQL statements into low-level operators when possible. We show the superior flexibility and efficiency of our system compared to ad-hoc discovery pipelines and stand-alone solutions.}
}


@inproceedings{DBLP:conf/icde/WangOWWL25,
	author = {Yikun Wang and
                  Dian Ouyang and
                  Zhuoran Wang and
                  Dong Wen and
                  Xuemin Lin},
	title = {Efficient Route and Area Matching Query in Dynamic Road Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {751--764},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00062},
	doi = {10.1109/ICDE65448.2025.00062},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangOWWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, ride-sharing is developing rapidly because of its economic and environmental advantages. Recent studies investigate the benefits of introducing meeting points during task assignments, allowing riders to be picked up or dropped off near their requested locations. In this paper, we study the route and area matching (ROAM) problem in dynamic road networks. ROAM query aims to find a detour path from source to destination visiting an area, meanwhile satisfying a detour budget. Existing method excludes unmatched queries in a compacted sketch graph, but the Dijkstra-based routing process is still time-consuming. Moreover, maintaining the sketch graph in frequently changing road networks is challenging because it requires computing from scratch. To overcome the limitations, we propose a simple yet effective framework named meeting point search (MPS). A novel index structure named GS-Tree is constructed to integrate spatial information for selecting meeting points and graph shortcuts for routing queries. GSTree has structural stability and can be efficiently maintained in dynamic networks. Theoretical analysis and experimental studies demonstrate the superiority of our methods. The optimal MPS with GS-Tree averagely outperforms the existing methods by two orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/XieLZYLZ25,
	author = {Yuan Xie and
                  Yumeng Liu and
                  Xu Zhou and
                  Yifang Yin and
                  Kenli Li and
                  Roger Zimmermann},
	title = {{PBSM:} Predictive Bi-Preference Stable Matching in Spatial Crowdsourcing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {765--778},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00063},
	doi = {10.1109/ICDE65448.2025.00063},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/XieLZYLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Task assignment is a fundamental challenge in Spatial Crowdsourcing which aims to assign location-based tasks to workers under spatial-temporal constraints. Recently, some exciting research has introduced the preference of workers and tasks to improve assignment quality. However, they either primarily focus on the current preferences of both workers and tasks or only consider the unilateral prediction-based preference of workers, overlooking the impact of workers' interconnection and tasks' completed sequences. As a result, they gain suboptimal assignment results in most cases. Inspired by this, we propose a novel problem, named the Predictive Bi-preference Stable Match problem (PBSM), with the goal of maximizing the preferences of both workers and tasks by taking into account the social network of workers and task completion sequence. The PBSM problem is proven to be NP-hard. To tackle this challenging problem, we develop a GCN-enhanced Transformer-based Prediction and Bi-preference Stable Matching (GETBM) framework with two stages: the bi-preference prediction stage and the bilateral assignment stage. In the prediction stage, the Worker Preference Model (WPM) and Task Preference Model (TPM) models are presented to predict the worker-to-task (Worker2Task) and task-to-worker (Task2Worker) preference lists, respectively. Then, we design a bilateral preference-aware stable matching (BPM) algorithm and prove it can gain stable results. To generalize to multiple scenarios, three optimization strategies are devised based on spatial-temporal constraints and priority consideration to gain better assignment performance. Extensive experiments are conducted to prove the superiority of the GETBM framework on two real datasets.}
}


@inproceedings{DBLP:conf/icde/XiaZCCYD25,
	author = {Hong Xia and
                  Xiao Zhang and
                  Yuan Cao and
                  Lei Cao and
                  Yanwei Yu and
                  Junyu Dong},
	title = {Self-Supervised Trajectory Representation Learning with Multi-Scale
                  Spatio-Temporal Feature Exploration},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {779--792},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00064},
	doi = {10.1109/ICDE65448.2025.00064},
	timestamp = {Wed, 10 Sep 2025 14:09:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XiaZCCYD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory representation learning transforms the complex spatio-temporal features of trajectories into a dense, low-dimensional embedding, which supports various downstream analytics tasks such as trajectory classification, travel time estimation, and similar trajectory search. Existing trajectory representation learning methods treat trajectories merely as general point sequences and use sequence models to learn the correlations between points. However, the complex spatio-temporal features of trajectories are multi-scale, meaning they are not only reflected in the correlations between trajectory points but also in the correlations between trajectory segments. Moreover, most existing methods do not sufficiently capture the multi-faceted temporal features within trajectories. To fill these gaps, we propose a novel self-supervised Trajectory  R R epresentation  L L earning model with multi-scale spatio-temporal features exploration called TrajRL. Specifically, we utilize trajectory augmentation to generate two views to achieve self-supervised pre-training exploiting multiple self-supervisory signals. In each view, we can learn the multi-scale spatio-temporal correlations both within and between road segments and road segment sequences in trajectories through the proposed multi-scale trajectory encoder. Additionally, we perform multi-faceted temporal information encoding, especially leveraging time intervals to learn multi-scale context-aware time patterns within the trajectories. Extensive experiments demonstrate the superiority of our TrajRL as compared to state-of-the-art baselines on two real-world datasets across various downstream tasks. The source code of our model is available at https://github.com/Xfc30/TrajRL.}
}


@inproceedings{DBLP:conf/icde/LuSZZWW25,
	author = {Jinwei Lu and
                  Yuanfeng Song and
                  Haodi Zhang and
                  Chen Jason Zhang and
                  Kaishun Wu and
                  Raymond Chi{-}Wing Wong},
	title = {Towards Robustness of Text-to-Visualization Translation Against Lexical
                  and Phrasal Variability},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {793--806},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00065},
	doi = {10.1109/ICDE65448.2025.00065},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuSZZWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Text-to-Vis is an emerging task in the data engineering and mining area that aims to automatically generate data visualizations from natural language questions (NLQs). Despite their progress, existing text-to-vis models often heavily rely on lexical matching between words in the questions and tokens in data schemas. This overreliance on lexical matching may lead to a diminished level of model robustness against input variations. In this study, we thoroughly examine the robustness of current text-to-vis models, an area that has not previously been explored. In particular, we construct the first robustness dataset nvBench-Rob, which contains diverse lexical and phrasal variations based on the original text-to-vis benchmark nvBench. Then, we found that the performance of existing text-to-vis models on this new dataset dramatically drops, implying that these methods exhibit inadequate robustness overall. Finally, we propose a novel framework based on Retrieval-Augmented Generation (RAG) technique, named GRED, specifically designed to address input perturbations in these two variants. The framework consists of three parts: NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and Annotation-based Debugger, which are used to tackle the challenges posed by natural language variants, programming style differences and data schema variants, respectively. Extensive experimental evaluations show that, compared to the state-of-the-art model Prompt4Vis in the Text-to-Vis field, GRED performs better in terms of model robustness, with a 40% increase in accuracy on the proposed nvBench-Rob dataset.}
}


@inproceedings{DBLP:conf/icde/StreviniotisBGD25,
	author = {Errikos Streviniotis and
                  Dimitrios Banelas and
                  Nikos Giatrakos and
                  Antonios Deligiannakis},
	title = {DAG*: {A} Novel A*-Alike Algorithm for Optimal Workflow Execution
                  Across IoT Platforms},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {807--820},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00066},
	doi = {10.1109/ICDE65448.2025.00066},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/StreviniotisBGD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many IoT applications from diverse domains rely on real-time, online analytics workflow execution to timely support decision making procedures. The efficient execution of analytics workflows requires the utilization of the processing power available across the cloud to edge continuum. Nonetheless, suggesting the optimal workflow execution over a large network of heterogeneous devices is a challenging task. The increased IoT network size increases the complexity of the optimization problem at hand. The ingested data streams exhibit highly volatile properties. The population of network devices dynamically changes. We introduce DAG*, an A*-alike algorithm that prunes large amounts of the search space explored for suggesting the most efficient workflow execution with formal optimality guarantees. We provide an incremental version of DAG* retaining the optimality property. Our experimentation in real-world scenarios shows that DAG* suggests the optimal workflow execution with 3 to 31 orders of magnitude fewer iterations compared to the entire search space size, outperforming heuristics employed in prior state of the art up to x4.S wrt the goodness of the suggested workflow.}
}


@inproceedings{DBLP:conf/icde/HeKH25,
	author = {Haoyu He and
                  Isaiah J. King and
                  H. Howie Huang},
	title = {Revelio: Revealing Important Message Flows in Graph Neural Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {821--835},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00067},
	doi = {10.1109/ICDE65448.2025.00067},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HeKH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Explainability is crucial for the deployment of Graph Neural Networks (GNNs) in real-world applications. Unfortunately, existing explanation methods primarily focus on identifying important graph components, such as nodes and edges, rather than providing insights into the fundamental message passing mechanisms of GNNs. This shortcoming impedes our understanding of how GNNs make predictions and limits their deployment in critical applications. In this paper, we introduce Revelio, a novel method to provide faithful explanations of message flows in GNNs. Revelio leverages a learning-based approach to quantify the importance of message flows, excelling in terms of faithfulness, compatibility, and efficiency. Our extensive experiments on both synthetic and real-world datasets demonstrate the superiority of Revelio through quantitative and qualitative assessments.}
}


@inproceedings{DBLP:conf/icde/DanPZM25,
	author = {Tangpeng Dan and
                  Xiao Pan and
                  Bolong Zheng and
                  Xiaofeng Meng},
	title = {{FAHL:} An Efficient Labeling Index for Flow-Aware Shortest Path Querying
                  in Road Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {836--849},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00068},
	doi = {10.1109/ICDE65448.2025.00068},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DanPZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a fundamental operation of location-based services, shortest path querying is widely adopted in real-time applications. Regrettably, most prior works overlook the impact of traffic-flow on shortest path querying. Taking traffic-flow into account is essential for finding a more convenient path through the Flow-Aware Shortest Path Querying (FSPQ). FSPQ faces the following challenges: (1) index restriction, existing indexes are only constructed by the relative spatial distance, if we leverage the traffic-flow to build the index, we can reduce the index size and improve its query efficiency. (2) maintenance latency, the traffic-flow and edges' weights undergo high-frequency changes with different traffic conditions, meaning that our index must be able to support high-frequency updates. To end this, we propose a novel Flow-Aware Hierarchical Labeling Index (FAHL) in this paper. In the index construction aspect, we propose a degree-flow joint ordering method to obtain the joint vertex ordering, and then build the index on it. In this way, FAHL can not only perceive both spatial distance and traffic-flow information but also reduce the index overhead during the query. In the index maintenance aspect, we propose Improved Structure Update (ISU) and Index Label Update (ILU) algorithms to support the index updating when high-frequency flow  \\backslash \\backslash  weight changes. Moreover, a flow priority shortest path search algorithm with pruning query bounds is proposed to speed up the query processing. Extensive experiments demonstrate that our proposed method achieves 33.1% speedup on average for the flow-aware shortest path querying compared to the state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/ChenGQLYZZ25,
	author = {Jinwen Chen and
                  Jiannan Guo and
                  Dazhuo Qiu and
                  Yawen Li and
                  Guanhua Ye and
                  Yan Zhao and
                  Kai Zheng},
	title = {{DATA-WA:} Demand-Based Adaptive Task Assignment with Dynamic Worker
                  Availability Windows},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {850--862},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00069},
	doi = {10.1109/ICDE65448.2025.00069},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenGQLYZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid advancement of mobile networks and the widespread use of mobile devices, spatial crowdsourcing, which involves assigning location-based tasks to mobile workers, has gained significant attention. However, most existing research focuses on task assignment at the current moment, overlooking the fluctuating demand and supply between tasks and workers over time. To address this issue, we introduce an adaptive task assignment problem, which aims to maximize the number of assigned tasks by dynamically adjusting task assignments in response to changing demand and supply. We develop a spatial crowdsourcing framework, namely demand-based adaptive task assignment with dynamic worker availability windows, which consists of two components including task demand prediction and task assignment. In the first component, we construct a graph adjacency matrix representing the demand dependency relationships in different regions and employ a multivariate time series learning approach to predict future task demands. In the task assignment component, we adjust tasks to workers based on these predictions, worker availability windows, and the current task assignments, where each worker has an availability window that indicates the time periods they are available for task assignments. To reduce the search space of task assignments and be efficient, we propose a worker dependency separation approach based on graph partition and a task value function with reinforcement learning. Experiments on real data demonstrate that our proposals are both effective and efficient.}
}


@inproceedings{DBLP:conf/icde/JiLWSP25,
	author = {Yushuai Ji and
                  Zepeng Liu and
                  Sheng Wang and
                  Yuan Sun and
                  Zhiyong Peng},
	title = {On Simplifying Large-Scale Spatial Vectors: Fast, Memory-Efficient,
                  and Cost-Predictable {\textdollar}k{\textdollar}-Means},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {863--876},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00070},
	doi = {10.1109/ICDE65448.2025.00070},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/JiLWSP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The  k k -means algorithm can simplify large-scale spatial vectors, such as 2D geo-Locations and 3D point clouds, to support fast analytics and learning. However, when processing large-scale datasets, existing  k k -means algorithms have been developed to achieve high performance with significant compu-tational resources, such as memory and CPU usage time. These algorithms, though effective, are not well-suited for resource-constrained devices. In this paper, we propose a fast, memory-efficient, and cost-predictable  k k -means called Dask-means. We first accelerate  k k -means by designing a memory-efficient accelerator, which utilizes an optimized nearest neighbor search over a memory-tunable index to assign spatial vectors to clusters in batches. We then design a lightweight cost estimator to predict the memory cost and runtime of the k-means task, allowing it to request appropriate memory from devices or adjust the accelerator's required space to meet memory constraints, and ensure sufficient CPU time for running k-means. Experiments show that when simplifying datasets with scale such as  10^{6} 10^{6} , Dask-means uses less than 30MB of memory, and achieves over 168 times speedup compared to the widely-used Lloyd's algorithm. We also validate Dask-means on mobile devices, where it demonstrates significant speedup and low memory cost compared to other state-of-the-art (SOTA)  k k -means algorithms. Our cost estimator achieves a memory cost estimation error with a difference of less than 3% from the actual ones and an MSE for predicted runtime at least 52.1 % lower than SOTA methods.}
}


@inproceedings{DBLP:conf/icde/ZhongZFLYW25,
	author = {Tianxiong Zhong and
                  Zhiwei Zhang and
                  Yihang Fu and
                  Guo Lu and
                  Ye Yuan and
                  Guoren Wang},
	title = {QaVA: Query-Aware Video Analysis Framework Based on Data Access Pattern},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {877--890},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00071},
	doi = {10.1109/ICDE65448.2025.00071},
	timestamp = {Fri, 26 Sep 2025 12:58:38 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhongZFLYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive growth of video data, efficient video analysis technology has garnered widespread attention. Existing online methods train proxy neural networks upon query arrival and use these networks to scan the entire dataset, guiding the invocation of the expensive deep neural network. While index-based methods advance this process to the index-building stage, significantly reducing the time overhead of video queries. However, the data to query often presents a long-tail distribution, and different types of queries are sensitive to different parts of the distribution. Since the index-based methods cannot predict the queries, they can only provide ad-hoc proxy score generating strategies. This paper proposes a query-aware video analysis framework, QaVA, to improve query performance further. QaVA retains the time-consuming, query-independent semantic extraction process during the index-building stage and employs a tunable lightweight adapter network to accurately and quickly focus on the data parts most relevant to the query after it arrives. Meanwhile, QaVA can automatically tune the training strategy of the adapter network by analyzing the data access pattern of historical queries, thus meeting the needs of general users. Experimental results demonstrate that QaVA can significantly reduce the cost of various queries across multiple datasets, and can speed up query processing by up to  9.2\\times 9.2\\times  compared to the most advanced index-based method. Our code is available: https://github.com/InkosiZhong/QaVA.}
}


@inproceedings{DBLP:conf/icde/LiWCCP25,
	author = {Yiqi Li and
                  Sheng Wang and
                  Zhiyu Chen and
                  Shangfeng Chen and
                  Zhiyong Peng},
	title = {Approximate Vector Set Search: {A} Bio-Inspired Approach for High-Dimensional
                  Spaces},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {891--903},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00072},
	doi = {10.1109/ICDE65448.2025.00072},
	timestamp = {Tue, 25 Nov 2025 07:46:51 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiWCCP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vector set search, an underexplored similarity search paradigm, aims to find vector sets similar to a query set. This search paradigm leverages the inherent structural alignment between sets and real-world entities to model more fine-grained and consistent relationships for diverse applications. This task, however, faces more severe efficiency challenges than traditional single-vector search due to the combinatorial explosion of pairings in set-to-set comparisons. In this work, we aim to address the efficiency challenges posed by the combinatorial explosion in vector set search, as well as the curse of dimensionality inherited from single-vector search. To tackle these challenges, we present an efficient algorithm for vector set search, BioVSS (Bio-inspired Vector Set Search). BioVSS simulates the fly olfactory circuit to quantize vectors into sparse binary codes and then designs an index based on the set membership property of the Bloom filter. The quantization and indexing strategy enables BioVSS to efficiently perform vector set search by pruning the search space. Experimental results demonstrate over 50 times speedup compared to linear scanning on million-scale datasets while maintaining a high recall rate of up to 98.9%, making it an efficient solution for vector set search.}
}


@inproceedings{DBLP:conf/icde/YangJZWNWLWYLZLM25,
	author = {Xihong Yang and
                  Heming Jing and
                  Zixing Zhang and
                  Jindong Wang and
                  Huakang Niu and
                  Shuaiqiang Wang and
                  Yu Lu and
                  Junfeng Wang and
                  Dawei Yin and
                  Xinwang Liu and
                  En Zhu and
                  Defu Lian and
                  Erxue Min},
	title = {DaRec: {A} Disentangled Alignment Framework for Large Language Model
                  and Recommender System},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {904--917},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00073},
	doi = {10.1109/ICDE65448.2025.00073},
	timestamp = {Thu, 01 Jan 2026 19:11:51 +0100},
	biburl = {https://dblp.org/rec/conf/icde/YangJZWNWLWYLZLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benefiting from the strong reasoning capabilities, Large language models (LLMs) have demonstrated remarkable performance in recommender systems. Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment. In this work, we prove that directly aligning the representations of LLMs and collaborative models is suboptimal for enhancing downstream recommendation tasks performance, based on the information theorem. Consequently, the challenge of effectively aligning semantic representations between collaborative models and LLMs remains unresolved. Inspired by this viewpoint, we propose a novel plug-and-play alignment framework for LLMs and collaborative models. Specifically, we first disentangle the latent representations of both LLMs and collaborative models into specific and shared components via projection layers and representation regularization. Subsequently, we perform both global and local structure alignment on the shared representations to facilitate knowledge transfer. Additionally, we theoretically prove that the specific and shared representations contain more pertinent and less irrelevant information, which can enhance the effectiveness of downstream recommendation tasks. Extensive experimental results on benchmark datasets demonstrate that our method is superior to existing state-of-the-art algorithms.}
}


@inproceedings{DBLP:conf/icde/ChenCMQWXZCLSJW25,
	author = {Shaoyuan Chen and
                  Hongtao Chen and
                  Shaonan Ma and
                  Yajie Qin and
                  Zheng Wang and
                  Weiyu Xie and
                  Mingxing Zhang and
                  Kang Chen and
                  Xia Liao and
                  Yingdi Shan and
                  Jinlei Jiang and
                  Yong Wei Wu},
	title = {Scaling Asynchronous Graph Query Processing via Partitioned Stateful
                  Traversal Machines},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {918--931},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00074},
	doi = {10.1109/ICDE65448.2025.00074},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenCMQWXZCLSJW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the escalating demand to analyze large graphs, many organizations are now collecting billion-level property graph datasets, concurrently executing many complex graph queries against them, and expecting interactive-level response latency. However, such requirements are particularly challenging because of the notoriously irregular data access pattern and complex dependencies between heterogeneous subtasks. Despite the widespread availability of many-core CPUs and high-speed networking in modern datacenters, existing distributed graph query systems struggle with their inherent inefficiencies, resulting in low hardware utilization and poor query performance on these state-of-the-art hardware. To address these challenges, we introduce the Partitioned Stateful Traversal Machine (PSTM), which extends the Gremlin graph traversal machine. PSTM retains the expressive power of the Gremlin query language, enabling it to accommodate a wide range of graph query tasks, including traversal, pattern matching, filtering, and result aggregation. It additionally introduces query memoranda, allowing for more efficient implementation and execution of numerous graph queries in distributed environments. Moreover, PSTM facilitates various system-level optimizations, such as massively parallel execution, overlapping computation with communication, locality-aware data access, and lightweight progress tracking. Building upon PSTM, we develop GraphDance, a distributed graph database featuring an efficient asynchronous PSTM run-time. Our evaluations, conducted on an 8-node cluster, show that GraphDance achieves millisecond-level query latency for complex queries on terabyte-scale graphs, with an average latency reduction of 89.2% across all interactive complex queries in the LDBC SNB benchmark compared to existing distributed graph query systems.}
}


@inproceedings{DBLP:conf/icde/OhYHK25,
	author = {Seyeon Oh and
                  Heeyong Yoon and
                  Donghyoung Han and
                  Min{-}Soo Kim},
	title = {GFlux: {A} Fast GPU-Based Out-of-Memory Multi-Hop Query Processing
                  Framework for Trillion-Edge Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {932--945},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00075},
	doi = {10.1109/ICDE65448.2025.00075},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/OhYHK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs are continually growing in size, and processing complex queries, such as multi-hop pattern queries, on them is becoming increasingly important. Although GPUs have received significant attention recently, there is still a notable shortage of efficient GPU-based out-of-memory methods for handling these queries. Three key issues arise when processing multi-hop queries on large-scale graphs using GPUs: the need for an efficient graph format, effective scheduling of accesses to graph partitions on storage, and dynamic buffer management on both the host and GPUs. To address these issues, we propose an efficient GPU-based out-of-memory multi-hop query processing framework called GFlux. Through extensive experiments, we have demonstrated that GFlux significantly improves both the speed and scalability compared to existing state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/CuiWLD25,
	author = {Shuangshuang Cui and
                  Hongzhi Wang and
                  Xianglong Liu and
                  Xiaoou Ding},
	title = {TempSched: {A} Temperature-Aware Storage Scheduler for Time Series
                  Across Cloud-Edge-Device},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {946--958},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00076},
	doi = {10.1109/ICDE65448.2025.00076},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CuiWLD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Storage scheduling is crucial for time series storage. However, designing an efficient hot and cold tiered storage scheduling strategy for time series across Cloud-Edge-Device (CED) architecture remains challenging. Although numerous research have studied hot and cold classification for relational data, these methods are not suitable for time series which has strong timeliness and complex access patterns. Therefore, in this paper, we present TempSched, a temperature-aware storage scheduler for time series across CED, which can identify hot and cold time series and predict data temperature efficiently to perform storage scheduling in advance. By employing Newton's law of cooling and the thermal radiation law, TempSched establishs a temperature model and encapsulates data temperature. It supports classifying hot and cold data and scheduling time series across CED. Subsequently, TempSched designs a workload prediction model and a frequent timestamp discovery algorithm to forecast access patterns and predict the future temperature. This can timely adjust to hot and cold storage. We validate TempSched on a public dataset, and the experimental results show that it can achieve about 94% hit rate for data access on the edge and device, which is 12% better than existing methods. It can help CED avoid storage overhead caused by storing the full data at all three sides, and greatly reduce data transfer overhead.}
}


@inproceedings{DBLP:conf/icde/MiaoLZZZJ25,
	author = {Hao Miao and
                  Ziqiao Liu and
                  Yan Zhao and
                  Kai Zheng and
                  Yupu Zhang and
                  Christian S. Jensen},
	title = {Federated Trajectory Similarity Learning with Privacy-Preserving Clustering},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {959--972},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00077},
	doi = {10.1109/ICDE65448.2025.00077},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MiaoLZZZJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Movement trajectory similarity computation is important when supporting functionalities such as outlier detection and prediction that may, in turn, fuel a variety of transportation-related applications. Recent trajectory similarity learning solutions often assume that trajectories are available at a central location. Yet, we are witnessing the decentralized collection of increasingly massive volumes of trajectories due to the deployment of edge devices. To enable decentralized training and improved privacy, we propose a federated trajectory similarity learning framework that features privacy-preserving clustering based on a client-server architecture. The framework encompasses local, client-side trajectory preprocessing and representation learning. This is combined with a novel privacy-preserving clustering mechanism that ensures consistent model updates between clients and the server, thus alleviating the effects of trajectory heterogeneity across clients. In addition, the framework features a hierarchical central aggregation mechanism that supports clustered federated learning. Experiments on real data offer evidence that the effectiveness of the proposed framework performs as intended.}
}


@inproceedings{DBLP:conf/icde/KaridiP25,
	author = {Danae Pla Karidi and
                  Evaggelia Pitoura},
	title = {Path-Based Summary Explanations for Graph Recommenders},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {973--986},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00078},
	doi = {10.1109/ICDE65448.2025.00078},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KaridiP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Path-based explanations provide intrinsic insights into graph-based recommendation models. However, most previous work has focused on explaining an individual recommendation of an item to a user. In this paper, we propose summary explanations, i.e., explanations that highlight why a user or a group of users receive a set of item recommendations and why an item, or a group of items, is recommended to a set of users as an effective means to provide insights into the collective behavior of the recommender. We also present a novel method to summarize explanations using efficient graph algorithms, specifically the Steiner Tree and the Prize-Collecting Steiner Tree. Our approach reduces the size and complexity of summary explanations while preserving essential information, making explanations more comprehensible for users and more useful to model developers. Evaluations across multiple metrics demonstrate that our summaries outperform baseline explanation methods in most scenarios, in a variety of quality aspects.}
}


@inproceedings{DBLP:conf/icde/HeHYH25,
	author = {Xi He and
                  Kai Huang and
                  Qingqing Ye and
                  Haibo Hu},
	title = {Data Poisoning Attacks to Local Differential Privacy Protocols for
                  Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {987--1000},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00079},
	doi = {10.1109/ICDE65448.2025.00079},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HeHYH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph analysis has become increasingly popular with the prevalence of big data and machine learning. Traditional graph data analysis methods often assume the existence of a trusted third party to collect and store the graph data, which does not align with real-world situations. To address this, some research has proposed utilizing Local Differential Privacy (LDP) to collect graph data or graph metrics (e.g., clustering coefficient). This line of research focuses on collecting two atomic graph metrics (the adjacency bit vectors and node degrees) from each node locally under LDP to synthesize an entire graph or generate graph metrics. However, they have not considered the security issues of LDP for graphs. In this paper, we bridge the gap by demonstrating that an attacker can inject fake users into LDP protocols for graphs and design data poisoning attacks to degrade the quality of graph metrics. In particular, we present three data poisoning attacks to LDP protocols for graphs. As a proof of concept, we focus on data poisoning attacks on two classical graph metrics: degree centrality and clustering coefficient. We further design two countermeasures for these data poisoning attacks. Experimental study on real-world datasets demonstrates that our attacks can largely degrade the quality of collected graph metrics, and the proposed countermeasures cannot effectively offset the effect, which calls for the development of new defenses.}
}


@inproceedings{DBLP:conf/icde/CuiYLLZLDZ25,
	author = {Yue Cui and
                  Liuyi Yao and
                  Zitao Li and
                  Yaliang Li and
                  Keqin Zhong and
                  Bingyi Liu and
                  Bolin Ding and
                  Xiaofang Zhou},
	title = {A Bargaining-Based Approach for Feature Trading in Vertical Federated
                  Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1001--1014},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00080},
	doi = {10.1109/ICDE65448.2025.00080},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CuiYLLZLDZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vertical Federated Learning (VFL) has emerged as a popular machine learning paradigm, enabling model training between the data and the task parties with different features about the same user set while preserving data privacy. In a production environment, VFL usually involves one task party and one data party. Fair and economically efficient feature trading is crucial to the commercialization of VFL, where the task party is considered the data consumer who buys the data party's features. However, current VFL feature trading practices often price the data party's data as a whole and assume transactions occur before performing VFL. Neglecting the performance gains resulting from traded features may lead to underpayment and overpayment issues. In this study, we propose a bargaining-based feature trading approach in VFL to facilitate economically efficient transactions. Our model incorporates performance gain-based pricing, taking into account the revenue-based optimization objectives of both parties. We analyze the proposed bargaining model under perfect and imperfect performance information settings, proving the existence of an equilibrium that optimizes the parties' objectives. Moreover, we develop performance gain estimation-based bargaining strategies for imperfect performance information scenarios and discuss potential security concerns and solutions. Experiments on three real-world datasets demonstrate the effectiveness of the proposed bargaining model.}
}


@inproceedings{DBLP:conf/icde/GaoLSLH25,
	author = {Sen Gao and
                  Shengliang Lu and
                  Shixuan Sun and
                  Yuchen Li and
                  Bingsheng He},
	title = {An Efficient Memoization Engine for Concurrent Graph Query Processing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1015--1028},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00081},
	doi = {10.1109/ICDE65448.2025.00081},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GaoLSLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Concurrent graph query (CGQ) processing has been used to solve a wide range of graph applications. By analyzing real-world workloads of CGQs, we observe significant repeated computations among the queries. In this work, we present KGraph, a novel graph processing memoization engine to efficiently handle CGQs on large graphs by performing memoization on graphs. However, the efficacy of memoization in optimizing CGQs on large graphs is constrained by substantial computational and memory overheads, coupled with the potential amount of sharing opportunities. Thus, we develop two novel approaches in KGraph to address the memoization overhead. First, we develop a fine-grained memoization method, which only maintains query results within their associated graph partitions. This approach not only reduces the overhead but also enhances the potential for sharing. Secondly, we selectively perform memoization on pivotal queries, those with a high likelihood of promoting substantial computation sharing among CGQs, while avoiding the excessive overhead associated with managing unnecessary memoization across a large number of queries. We comprehensively analyze KGraph's performance using five popular CGQ applications. Experimental results show that our system achieves an average speedup of 4.2× over the state-of-the-art CGQ systems.}
}


@inproceedings{DBLP:conf/icde/HuanZLCWJMW25,
	author = {Chengying Huan and
                  Heng Zhang and
                  Yongchao Liu and
                  Likang Chen and
                  Xuran Wang and
                  Yongchun Jiang and
                  Shaonan Ma and
                  Yanjun Wu},
	title = {TeMatch: {A} Fast Temporal Subgraph Matching Framework with Temporal-Aware
                  Subgraph Matching Algorithms},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1029--1042},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00082},
	doi = {10.1109/ICDE65448.2025.00082},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HuanZLCWJMW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal subgraph matching aims to identify occurrences of a query graph within a large target graph, subject to certain temporal constraints that require that timestamps on the edges increase in accordance with the direction of the path. Current research on temporal subgraph matching typically identifies each non-temporal match and then filters out occurrences by examining all paths within each occurrence for compliance with temporal constraints. However, this approach proves to be highly inefficient, as it involves excessive unnecessary computation on non-temporal occurrences that do not meet the temporal constraints and can be pruned early during the matching process. Moreover, the constraint examination on all paths within each occurrence further results in numerous redundant timestamp comparisons. Therefore, a high-performance solution is demanded to overcome these drawbacks. In this paper, we introduce TeMatch, a high-performance framework designed to be compatible with any enumeration-based solution for temporal subgraph matching. TeMatch features a novel topological representation of temporal constraints in the query graph, along with three temporal-aware subgraph matching algorithms that enable rapid constraint checking and enhance early pruning and filtration. Extensive experiments reveal that TeMatch efficiently harnesses temporal information to enable early pruning and achieves a speedup of 313.57x while being parallel-friendly, highly compatible, and yielding identical matching results.}
}


@inproceedings{DBLP:conf/icde/ZhangLZQYW25,
	author = {Qi Zhang and
                  Rong{-}Hua Li and
                  Zifan Zheng and
                  Hongchao Qin and
                  Ye Yuan and
                  Guoren Wang},
	title = {Efficient Maximum Fair Clique Search Over Large Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1043--1055},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00083},
	doi = {10.1109/ICDE65448.2025.00083},
	timestamp = {Wed, 10 Sep 2025 14:09:53 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangLZQYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mining cohesive subgraphs in attributed graphs is an essential problem in the domain of graph data analysis. The integration of fairness considerations significantly fuels interest in models and algorithms for mining fairness-aware cohesive subgraphs. Notably, the relative fair clique emerges as a robust model, ensuring not only comprehensive attribute coverage but also greater flexibility in distributing attribute vertices. Motivated by the strength of this model, we for the first time pioneer an investigation into the identification of the maximum relative fair clique in large-scale graphs. We introduce a novel concept of colorful support, which serves as the foundation for two innovative graph reduction techniques. These techniques effectively narrow the graph's size by iteratively removing edges that do not belong to relative fair cliques. Furthermore, a series of upper bounds of the maximum relative fair clique size is proposed by incorporating consideration of vertex attributes and colors. The pruning techniques derived from these upper bounds can significantly trim unnecessary search space during the branch-and-bound procedure. Adding to this, we present a heuristic algorithm with a linear time complexity, employing both a degree-based greedy strategy and a colored degree-based greedy strategy to identify a larger relative fair clique. This heuristic algorithm can serve a dual purpose by aiding in branch pruning, thereby enhancing overall search efficiency. Extensive experiments conducted on six real-life datasets demonstrate the efficiency, scalability, and effectiveness of our algorithms.}
}


@inproceedings{DBLP:conf/icde/DengWWZXZ25,
	author = {Liwei Deng and
                  Fei Wang and
                  Tianfu Wang and
                  Yan Zhao and
                  Yuyang Xia and
                  Kai Zheng},
	title = {Exact and Efficient Similar Subtrajectory Search: Integrating Constraints
                  and Simplification},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1056--1069},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00084},
	doi = {10.1109/ICDE65448.2025.00084},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/DengWWZXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Similar subtrajectory search (SimSub) aims to find a subtrajectory (i.e., a segment) from a data trajectory (the trajectory to be queried) that closely resembles the query trajectory. Compared with similar trajectory search, SimSub can capture finer-grained similarity and is vital for various trajectory analysis tasks, such as trajectory clustering and join. However, SimSub may return a subtrajectory with extremely limited length, e.g., a single point, which may not align with the expectations of real-world applications. To solve this issue, we propose a constrained SimSub (cSimSub) problem, where the length of the returned subtrajectory must be greater than or equal to a user-specified integer  C C . We demonstrate that this problem can be solved exactly with a time complexity equivalent to  C C  times the complexity of the trajectory distance measurement, given that the distance function can be computed using dynamic programming (DP). We also observe that when  C=1 C=1 , the solution of cSimSub differs from the vanilla trajectory distance computation (e.g., DTW) only in the state initialization of the DP matrix. Moreover, SimSub focuses on finding a subtrajectory with successive point indexes, which limits its applicability in certain scenarios, e.g., trajectory simplification. Thus, we extend it to sSimSub for trajectory simplification, aiming to find the most similar non-continuous subsequence of a trajectory to itself, with a length constraint of  C C . The subsequence, i.e., the simplified subtrajectory, obtained from sSimSub can achieve the best self-similarity. We conduct experiments on three public available datasets to demonstrate the effectiveness of the proposals. The results show that integrating sSimSub into typical query methods, e.g., KNN query, can achieve higher accuracy of these methods in simplified trajectory databases compared with other well-known trajectory simplification algorithms.}
}


@inproceedings{DBLP:conf/icde/ZhuSZYL25,
	author = {Wenbin Zhu and
                  Zhaoyan Shen and
                  Mengying Zhao and
                  Dongxiao Yu and
                  Bingzhe Li},
	title = {A Deep Dive into Protocol Design: How to Improve {IPFS} Performance
                  without Sacrificing Decentralization},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1070--1083},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00085},
	doi = {10.1109/ICDE65448.2025.00085},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhuSZYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The InterPlanetary File System (IPFS) is a prominent decentralized storage solution; however, it struggles with performance issues. To address this challenge, the IPFS team has patched a series of centralized components, resulting in improved performance while giving more significant roles to specific entities. Balancing speed and decentralization has always posed a complex dilemma for storage systems. In this paper, we conduct a series of experiments and analyses to identify the performance advantages and constraints associated with the IPFS decentralized protocol. Based on thorough analysis, we propose a novel scheme named XIPFS to optimize IPFS. This scheme includes facilitating parallel block exchange across multiple nodes, refining content Publication strategies, and improving node selection algorithms for content routing. Our goal is to maximize the benefits of decentralized multi-source parallel downloading while minimizing the negative impact of decentralized indexing on execution time. Compared to previous approaches, the proposed optimizations are lightweight and fully compatible with the decentralized protocol, enabling autonomous execution and utility realization at each node. Experimental results demonstrate that XIPFS significantly improves node performance without compromising decentralization.}
}


@inproceedings{DBLP:conf/icde/YangXMLGL25,
	author = {Jianye Yang and
                  Lei Xing and
                  Ziyi Ma and
                  Xi Luo and
                  Cuiyun Gao and
                  Xuemin Lin},
	title = {Maximal Similar-Weight Biclique Enumeration for Large Bipartite Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1084--1097},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00086},
	doi = {10.1109/ICDE65448.2025.00086},
	timestamp = {Wed, 10 Sep 2025 14:09:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YangXMLGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the problem of maximal similar-weight biclique enumeration for large bipartite graphs. Given an edge-weighted bipartite graph  G=(U,\\ V,\\ E) G=(U,\\ V,\\ E)  and a weight difference threshold  \\delta \\delta , we aim to efficiently enumerate all maximal similar-weight bicliques in  G G , where a maximal similar-weight biclique is a maximal complete subgraph  B(L,\\ R) B(L,\\ R)  of  G G  such that the weight difference of edges in  E(B) E(B)  is not larger than  \\delta \\delta . This problem has many applications, such as item recommendation, fraud detection, and biclustering of gene expression data, etc. To the best of our knowledge, we are the first to systematically study this problem. It is very challenging to efficiently solve this problem due to its #P-completeness. In this paper, we propose a two-phase branch-and-bound baseline method, namely MSWBE, which explores the search space in a depth-first manner. Although MSWBE offers a useful computation framework to our problem, its performance is not yet satisfactory due to the large candidate set during the enumeration. To alleviate this, we propose an advanced approach, called MSWBE++. In particular, MSWBE++ exploits the search space by utilizing the edge connectivity and weight information simultaneously, and therefore refines the candidate set significantly. Observing that a straightforward implementation of MSWBE++ by following a depth-first search strategy may generate non-maximal bicliques, we develop a breadth-first search strategy to realize MSWBE++, which can discard the non-maximal sets at an early stage. To accelerate the computation, we introduce effective graph reduction techniques. Our extensive experimental results on 10 real-life datasets demonstrate that MSWBE++ significantly outperforms the baseline methods by up to 2 orders of magnitude. We conduct a case study to show that maximal similar-weight bicliques can provide useful searching hints for fraudulent rating detection.}
}


@inproceedings{DBLP:conf/icde/YangLJZWSJW25,
	author = {Mingyu Yang and
                  Wentao Li and
                  Jiabao Jin and
                  Xiaoyao Zhong and
                  Xiangyu Wang and
                  Zhitao Shen and
                  Wei Jia and
                  Wei Wang},
	title = {Effective and General Distance Computation for Approximate Nearest
                  Neighbor Search},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1098--1110},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00087},
	doi = {10.1109/ICDE65448.2025.00087},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/YangLJZWSJW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate K Nearest Neighbor (AKNN) search in high-dimensional spaces is a critical yet challenging problem. In AKNN search, distance computation is the core task that dominates the runtime. Existing approaches typically use approx-imate distances to improve computational efficiency, often at the cost of reduced search accuracy. To address this issue, the state-of-the-art method, ADSampling, employs random projections to estimate approximate distances and introduces an additional distance correction process to mitigate accuracy loss. However, ADSampling has limitations in both effectiveness and generality, primarily due to its heavy reliance on random projections for distance approximation and correction. Motivated by this, we leverage data distribution to improve distance approximation via orthogonal projection, thereby ad-dressing the effectiveness limitation of ADSampling; we also adopt a data-driven approach to distance correction, decoupling the correction process from the distance approximation process, thereby overcoming the generality limitation of ADSampling. Ex-tensive experiments demonstrate the superiority and effectiveness of our method. In particular, compared to ADSampling, our method achieves a speedup of 1.6 to 2.1 times on real-world datasets while providing higher accuracy. In addition, our method shows superior performance in Ant Group image search scenarios and has been integrated into their search engine.}
}


@inproceedings{DBLP:conf/icde/SunYSZY25,
	author = {Yu Sun and
                  Xinyu Yang and
                  Shaoxu Song and
                  Ying Zhang and
                  Xiaojie Yuan},
	title = {Collaborative Imputation for Multivariate Time Series with Convergence
                  Guarantee},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1111--1124},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00088},
	doi = {10.1109/ICDE65448.2025.00088},
	timestamp = {Thu, 25 Dec 2025 12:47:48 +0100},
	biburl = {https://dblp.org/rec/conf/icde/SunYSZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Missing values often occur in multivariate time series, affecting data analysis and applications. Existing studies typically use complete data to train imputation models, which are then used to fill missing values. However, in practice, missing values could appear in various cells. Such varieties unfortunately prevent imputation models performing, even making fillings unavailable without the convergence guarantee, i.e., lacking the ensurance of obtaining the optimal solution when the iteration tends to infinite. The reasons are that (1) the imputed values of multiple cells could affect each other towards the conformance to models, and (2) dependencies obtained from complete data may not be accurate enough to impute many unobserved values, which poses a tougher challenge of the convergence. In this work, we study the collaborative imputation with the convergence guarantee. By “collaborative”, we mean (1) all the missing cells can be collaboratively imputed with the guaranteed conformance to models, and (2) the imputation models are collaboratively optimized according to fillings as well. Our major technical highlights include 1) introducing the statistically explainable collaborative imputation via likelihood maximization, 2) designing a collaborative imputation algorithm for multiple missing cells and extending it into a parallel version equivalently, 3) improving the algorithm by both imputation values and models collaboratively optimized with the convergence guarantee in parallel, 4) designing the streaming imputation and adaptive parameter determination strategies. Experiments on real incomplete datasets demonstrate the superiority of our methods against twelve baselines, in both imputation accuracy and downstream applications.}
}


@inproceedings{DBLP:conf/icde/GiouroukisPZM25,
	author = {Dimitrios Giouroukis and
                  Varun Pandey and
                  Steffen Zeuch and
                  Volker Markl},
	title = {Chameleon: Adaptive and Scalable Stream Processing Over Sensor Sources},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1139--1152},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00090},
	doi = {10.1109/ICDE65448.2025.00090},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/GiouroukisPZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) applications make use of live data from numerous sensors that reside outside cloud datacenters. As a result, it is imperative for IoT data management systems to reduce their network footprint while simultaneously scaling to larger numbers of sensors. One way of achieving this is to adapt data generation to the rate of changes in the real world. In this systems paper, we propose Chameleon, a sensor-driven protocol for network-efficient data management that treats sensors as first-class components of a stream processing system. Chameleon combines local knowledge from the sensors with global knowledge from the cloud to improve data acquisition. Our empirical evaluation shows that systems employing Chameleon outperform baselines for aggregate queries by up to one order of magnitude in terms of network utilization while keeping query re-sults similar with negligible difference (down to 0.8%) from base-lines. Chameleon enables data management systems to handle up to 80% more sensors without needing extra network resources.}
}


@inproceedings{DBLP:conf/icde/CohenS25,
	author = {Sara Cohen and
                  Helen Sternbach},
	title = {Facility Location for Fair and Equitable Query Results},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1153--1165},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00091},
	doi = {10.1109/ICDE65448.2025.00091},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CohenS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding a subset of representative items from a large set of data items has been studied extensively, under a variety of conditions and constraints. In our setting, data items belong to a metric space and also have a sensitive attribute (e.g., gender, race). Our focus is on effectively choosing a set of representatives while taking into consideration two distinct notions of fairness. First, each data item in the dataset should be similar to a representative (while precisely how similar depends on data distributions). Second, representatives should satisfy a given social equity constraint specifying the number of representatives with each attribute value. To satisfy these two fairness requirements, we build upon previous results in fair facility location, extending this work to allow for social equity constraints. Our extension is parameterized by requirements on the neighborhood of data items, and we show lower and upper bounds for an optimal algorithm for some cases, and NP-completeness results for others. We then further extend this work to ensure that representatives should be similar, in their attribute values, to the set of data that they represent. To this end, we develop methods to choose items that are highly representative of their surrounding data items, while still satisfying a social equity constraint. Combining these results yields a method that can be leveraged to choose representative data items while simultaneously meeting several fairness requirements. Experimental results show the quality of our results and demonstrate that, in practice, the cost for social equity (in terms of increased distance to representatives) is low.}
}


@inproceedings{DBLP:conf/icde/ZhouZLZ25,
	author = {Xinjie Zhou and
                  Mengxuan Zhang and
                  Lei Li and
                  Xiaofang Zhou},
	title = {High Throughput Shortest Distance Query Processing on Large Dynamic
                  Road Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1166--1179},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00092},
	doi = {10.1109/ICDE65448.2025.00092},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhouZLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shortest path (SP) computation is the building block for many location-based services, and achieving high throughput SP query processing with real-time response is crucial for those services. However, existing solutions can hardly handle high throughput queries on large dynamic road networks due to either slow query efficiency or poor dynamic adaption. In this paper, we leverage graph partitioning and propose novel Partitioned Shortest Path (PSP) indexes to address this problem. Specifically, we first put forward a cross-boundary strategy to accelerate the query processing of PSP index and analyze its efficiency upper bound theoretically. After that, we propose a non-trivial Partitioned Multi-stage Hub Labeling (PMHL) that subtly aggregates multiple PSP strategies to achieve fast index maintenance and consecutive query efficiency improvement during index update. Lastly, to further optimize throughput, we design tree decomposition-based graph partitioning and propose Post-partitioned MHL (PostMHL) with faster query processing and index update. Experiments on real-world road networks show that our methods outperform state-of-the-art baselines in query throughput, yielding up to 2 orders of magnitude improvement.}
}


@inproceedings{DBLP:conf/icde/SkitsasMK25,
	author = {Konstantinos Skitsas and
                  Davide Mottin and
                  Panagiotis Karras},
	title = {Pilos: Scalable Large-Subgraph Matching by Online Spectral Filtering},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1180--1193},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00093},
	doi = {10.1109/ICDE65448.2025.00093},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SkitsasMK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph matching seeks all the occurrences of a query graph inside another graph. As it reduces to subgraph isomorphism, it is NP-hard. Current methods reduce the computation by filtering the candidates on which they run subgraph isomorphism. Nevertheless, when the query is large, the number of candidates grows rapidly, rendering current methods largely ineffective in pruning and incapable to answer even within one hour. A primary reason for this ineffectiveness is their inability to effectively consider the query graph structure in the computation. In this paper, we propose Pilos,a novel matching algorithm that substantially improves the filtering phase of a typical matching algorithm and computes up to 60% fewer candidates for verification. Pilosuses ( i i i ) an offline light-weight index-based phase, which leverages the top graph Laplacian eigenvalues of query and data node neighborhoods to reduce candidates via neighborhood filtering and (ii) an online phase, which further prunes candidates stored in an auxiliary data structure; both phases apply the interlacing theorem on graph Laplacian spectra. Our thorough experimental study shows that, on average, Pilosresolves queries in 19% less time and leaves 23% fewer unresolved queries after a lapse of 10 minutes than the best previous work.}
}


@inproceedings{DBLP:conf/icde/ZardbaniLMK25,
	author = {Fatemeh Zardbani and
                  Konstantinos Lampropoulos and
                  Nikos Mamoulis and
                  Panagiotis Karras},
	title = {Updating an Adaptive Spatial Index},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1194--1206},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00094},
	doi = {10.1109/ICDE65448.2025.00094},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZardbaniLMK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive indexing allows for the progressive and simultaneous query-driven exploration and indexing of memoryresident data, starting as soon as they become available without upfront indexing. This technique has been so far applied to onedimensional and multidimensional data, as well as to objects with spatial extent arising in geographic information systems. However, existing spatial adaptive indexing methods cater to static data made available in an one-off manner. To date, no spatial adaptive indexing method can ingest data updates interleaved with data exploration. In this paper we introduce GLIDE, a novel method that intertwines the adaptive indexing and incremental updating of a spatial-object data set. GLIDE builds a hierarchical spatial index incrementally in response to queries and also ingests updates judiciously into it. We examine several design choices and settle for a variant that combines gradual self-driven top-down insertions with query-driven indexing operations. In an extensive experimental comparison, we show that GLIDE achieves a lower cumulative cost than upfront-indexing methods and adaptiveindexing baselines.}
}


@inproceedings{DBLP:conf/icde/KingRBH25,
	author = {Isaiah J. King and
                  Ramiro Ramirez and
                  Benjamin Bowman and
                  H. Howie Huang},
	title = {Trail: {A} Knowledge Graph-Based Approach for Attributing Advanced
                  Persistent Threats},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1207--1220},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00095},
	doi = {10.1109/ICDE65448.2025.00095},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KingRBH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Open-source intelligence exchanges provide a rich repository of indicators of compromise (IOCs). These IOCs are used to build detection signatures and blocklists in production cybersecurity environments as well as prior works. In this work, we investigate their utility for cyberattack attribution. To do this, we create a novel system called Trail that builds a knowledge graph of network-based IOC co-occurrences in cyberattacks, and their relations to other IOCs. After analyzing 4,500 cybersecurity events attributed to 22 different advanced persistent threats (APTs), the knowledge graph holds over 2.1 million nodes with 7.9 million edges. We analyze the knowledge graph this system produces using conventional machine learning, graph analytics, and a graph neural network to quantify the degree to which APTs leave identifiable clues in their IOCs. Using the Trail method to enrich the IOC feature space, IOCs can individually be attributed to the APT that generated them with 45% accuracy. When attributing groups of IOCs that made up cyberattacks, indirect resource reuse alone accurately attributed 82% of samples. When we used both graph topology and feature analysis and analyzed events with a graph neural network, attribution accuracy increased to 84%. Finally, we conducted a 6-month study of new cyber events our models had never seen. We found that our models continue to achieve similar accuracy on real-world data to what was observed experimentally, so long as the database is no more than 1 month out of date.}
}


@inproceedings{DBLP:conf/icde/LassigH25,
	author = {Nico L{\"{a}}ssig and
                  Melanie Herschel},
	title = {Experimental Analysis of Multi-Step Pipelines for Fair Classifications
                  - More than the Sum of Their Parts?},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1221--1235},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00096},
	doi = {10.1109/ICDE65448.2025.00096},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LassigH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of biased machine learning predictions has led to many alternative approaches to mitigate the problem. They are typically studied and evaluated by focusing on the input data, the trained model, and the performance of the model predictions. We take a broader perspective, considering approaches in the context of a multi-step pipeline. We study fair classification in a pipeline comprising multiple data preparation steps, parameter optimization, and three types of approaches (pre-, in-, and post-processing) designed to reduce bias that may be applied consecutively. This pipeline leads to a trained model to be evaluated in terms of quality (e.g., accuracy) and fairness. We experimentally evaluate the effect differently combined implementations of the pipeline components have on the performance of more than 40 fairness-inducing algorithms. Key findings made possible by this pipeline perspective include: (1) Choosing a bias reducing algorithm greatly simplifies when implementing suited data preparation or parameter optimization, as the difference in performance between methods shrinks, making almost any choice a good one. (2) Several component or pipeline implementations often assumed to have positive or negative effects on performance prove to have little or even contrary effects to the expectations. (3) While many approaches have been published for fair classification in the last decade and shown to improve on previous solutions in specific settings, our broad analysis reveals a stagnating performance trend. Our analysis shows that synergetic effects between pipeline components need to be carefully taken into account for further research on fair end-to-end data processing. It further raises the more fundamental question of how the study of the problem evolves, both in terms of proposed solutions and benchmarking.}
}


@inproceedings{DBLP:conf/icde/MiaoLG25,
	author = {Hao Miao and
                  Zida Liu and
                  Jun Gao},
	title = {BSG4Bot:Efficient Bot Detection Based on Biased Heterogeneous Subgraphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1236--1249},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00097},
	doi = {10.1109/ICDE65448.2025.00097},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/MiaoLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The detection of malicious social bots has become a crucial task, as bots can be easily deployed and manipulated to spread disinformation, promote conspiracy messages, and more. Most existing approaches utilize graph neural networks (GNNs) to capture both user profile and structural features, achieving promising progress. However, they still face limitations including the expensive training on large underlying graph, the performance degradation when “similar neighborhood patterns” assumption preferred by GNNs is not satisfied, and the distinguishable features of bots in a highly adversarial context. Motivated by these limitations, this paper proposes a method named BSG4Bot with an intuition that GNNs training on Biased SubGraphs can improve both performance and time/space efficiency in bot detection. Specifically, BSG4Bot first pre-trains a classifier on node features efficiently to define the node similarities, and constructs biased subgraphs by combining the similarities computed by the pre-trained classifier and the node importances computed by Personalized PageRank (PPR scores). BSG4Bot then introduces a heterogeneous GNN over the constructed subgraphs to detect bots effectively and efficiently. The relatively stable features, including the content category and temporal activity features, are explored and incorporated into BSG4Bot after preliminary verification on sample data. The extensive experimental studies show that BSG4Bot outperforms the state-of-the-art bot detection methods, while only needing nearly 1/4 training time.}
}


@inproceedings{DBLP:conf/icde/PengZFZLY25,
	author = {Zeshun Peng and
                  Yanfeng Zhang and
                  Tinghao Feng and
                  Weixing Zhou and
                  Xiaohua Li and
                  Ge Yu},
	title = {MassBFT: Fast and Scalable Geo-Distributed Byzantine Fault-Tolerant
                  Consensus},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1250--1264},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00098},
	doi = {10.1109/ICDE65448.2025.00098},
	timestamp = {Thu, 23 Oct 2025 23:00:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PengZFZLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geo-distributed consensus protocols provide high availability and resilience for distributed database services. These protocols group nodes by their data centers to leverage the network topology that spans across multiple data centers, thereby reducing costly cross-datacenter communication. However, they still face performance and scalability challenges due to inefficient log replication mechanisms. 1) These protocols rely on the leader node in each group to perform cross-datacenter log replication, creating a single-node performance bottleneck. 2) Byzantine receivers can behave arbitrarily, forcing the group leader to send multiple log copies during replication to prevent loss, thus causing redundant transmissions. 3) Since all groups must execute these logs in the same order, synchronizations across groups are necessary to maintain consistency when multiple groups are proposing concurrently, which also slow down log replication. This paper presents MassBFT, a Byzantine fault-tolerant geo-consensus protocol that achieves high performance and scalability. We design an encoded bijective log replication to eliminate the leader bottleneck and reduce the cross-datacenter network consumption. We also propose asynchronous log ordering to eliminate synchronization across groups. Experimental results show that MassBFT is scalable, fault-tolerant, and outperforms state-of-the-art protocols with 5.49-29.96 times higher throughput under YCSB, SmallBank, and TPC-C workloads.}
}


@inproceedings{DBLP:conf/icde/FanCLGFYC25,
	author = {Zhuochen Fan and
                  Yalun Cai and
                  Zirui Liu and
                  Jiarui Guo and
                  Xin Fan and
                  Tong Yang and
                  Bin Cui},
	title = {CuckooGraph: {A} Scalable and Space-Time Efficient Data Structure
                  for Large-Scale Dynamic Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1265--1277},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00099},
	doi = {10.1109/ICDE65448.2025.00099},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FanCLGFYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs play an increasingly important role in various big data applications. However, existing graph data structures cannot simultaneously address the performance bottlenecks caused by the dynamic updates, large scale, and high query complexity of current graphs. This paper proposes a novel data structure for large-scale dynamic graphs called CuckooGraph. It does not require any prior knowledge of the upcoming graphs, and can adaptively resize to the most memory-efficient form while requiring few memory accesses for very fast graph data processing. The key techniques of CuckooGraph include TRANSFORMATION and DENYLIST. TRANSFORMATION fully utilizes the limited memory by designing related data structures that allow flexible space transformations to smoothly expand/tighten the required space depending on the number of incoming items. DENYLIST efficiently handles item insertion failures and further improves processing speed. Our experimental results show that compared with the most competitive solution Spruce, Cuckoo-Graph achieves about 33× higher insertion throughput while requiring only about 68% of the memory space.}
}


@inproceedings{DBLP:conf/icde/WangYYLGYC25,
	author = {Meng Wang and
                  Jintao Yang and
                  Bin Yang and
                  Hui Li and
                  Tongxin Gong and
                  Bo Yang and
                  Jiangtao Cui},
	title = {Towards Lightweight Time Series Forecasting: {A} Patch-Wise Transformer
                  with Weak Data Enriching},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1278--1291},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00100},
	doi = {10.1109/ICDE65448.2025.00100},
	timestamp = {Wed, 10 Sep 2025 14:09:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangYYLGYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Patch-wise Transformer based time series forecasting achieves superior accuracy. However, this superiority relies heavily on intricate model design with massive parameters, rendering both training and inference expensive, thus preventing their deployments on edge devices with limited resources and low latency requirements. In addition, existing methods often work in an autoregressive manner, which take into account only historical values, but ignore valuable, easy-to-obtain context information, such as weather forecasts, date and time of day. To contend with the two limitations, we propose LiPFormer, a novel Lightweight Patch-wise Transformer with weak data enriching. First, to simplify the Transformer backbone, LiPFormer employs a novel lightweight cross-patch attention and a linear transformationbased attention to eliminate Layer Normalization and Feed Forward Network, two heavy components in existing Transformers. Second, we propose a lightweight, weak data enriching module to provide additional, valuable weak supervision to the training. It enhances forecasting accuracy without significantly increasing model complexity as it does not involve expensive, human-labeling but using easily accessible context information. This facilitates the weak data enriching to plug-and-play on existing models. Extensive experiments on nine benchmark time series datasets demonstrate that LiPFormer outperforms state-of-the-art methods in accuracy, while significantly reducing parameter scale, training duration, and GPU memory usage. Deployment on an edge device reveals that LiPFormer takes only 1/3 inference time compared to classic Transformers. In addition, we demonstrate that the weak data enriching can integrate seamlessly into various Transformer based models to enhance their accuracy, suggesting its generality.}
}


@inproceedings{DBLP:conf/icde/JiaLDRLL25,
	author = {Lianyin Jia and
                  Shiqi Luo and
                  Jiaman Ding and
                  Suprio Ray and
                  Mengjuan Li and
                  Xiuxing Li},
	title = {A Length Enhanced B\({}^{\mbox{+}}\)-Tree Based Index for Efficient
                  Set Similarity Query},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1292--1304},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00101},
	doi = {10.1109/ICDE65448.2025.00101},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/JiaLDRLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Set Similarity Query (SSQ) is widely applied in various fields. The existing B+-tree-based SSQ approaches fail to fully exploit length filtering and require calculating similarity bounds in a node-wise manner, leading to low efficiency. To address these issues, we propose LeB, a novel length-enhanced B+-tree index, whose keys integrate set lengths and bucket mapping, enabling the direct pruning of sets that do not meet the length requirements. Building upon LeB, we present an efficient algorithm, LeBQ, which leverages length filtering and symmetric difference allocation to determine the key bounds for a query, enabling the key bounds computation only once for each query  Q Q Q  and avoiding costly similarity bounds computation in a node-wise manner. Efficient key filtering strategies are proposed to prune sets that cannot be similar, significantly reducing the number of candidates. Based on LeBQ, LeBQ+ further reduces the number of candidates by introducing length-independent key bounds. Experimental results on four real datasets demonstrate that LeBQ+ has a higher node access efficiency and accesses only 3.08% to 27.47% nodes compared to the existing B+ -tree-based SSQ algorithm. LeBQ+ is up to 99.8 × faster than the state-of-the-art algorithms.}
}


@inproceedings{DBLP:conf/icde/MiaoQZZTLLDZ25,
	author = {Shuyi Miao and
                  Wangjie Qiu and
                  Hongwei Zheng and
                  Qinnan Zhang and
                  Xiaofan Tu and
                  Xunan Liu and
                  Yang Liu and
                  Jin Dong and
                  Zhiming Zheng},
	title = {Know Your Account: Double Graph Inference-Based Account De-Anonymization
                  on Ethereum},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1305--1318},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00102},
	doi = {10.1109/ICDE65448.2025.00102},
	timestamp = {Thu, 13 Nov 2025 08:04:06 +0100},
	biburl = {https://dblp.org/rec/conf/icde/MiaoQZZTLLDZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The scaled Web 3.0 digital economy, represented by decentralized finance (DeFi), has sparked increasing interest in the past few years, which usually relies on blockchain for token transfer and diverse transaction logic. However, illegal behaviors, such as financial fraud, hacker attacks, and money laundering, are rampant in the blockchain ecosystem and seriously threaten its integrity and security. In this paper, we propose a novel double graph-based Ethereum account de-anonymization inference method, dubbed DBG4ETH, which aims to capture the behavioral patterns of accounts comprehensively and has more robust analytical and judgment capabilities for current complex and continuously generated transaction behaviors. Specifically, we first construct a global static graph to build complex interactions between the various account nodes for all transaction data. Then, we also construct a local dynamic graph to learn about the gradual evolution of transactions over different periods. Different graphs focus on information from different perspectives, and features of global and local, static and dynamic transaction graphs are available through DBG4ETH. In addition, we propose an adaptive confidence calibration method to predict the results by feeding the calibrated weighted prediction values into the classifier. Experimental results show that DBG4ETH achieves state-of-the-art results in the account identification task, improving the F1-score by at least 3.75% and up to 40.52% compared to processing each graph type individually and outperforming similar account identity inference methods by 5.23 % to 12.91 %.}
}


@inproceedings{DBLP:conf/icde/HennebergSKB25,
	author = {Justus Henneberg and
                  Felix Martin Schuhknecht and
                  Rosina Kharal and
                  Trevor Brown},
	title = {More Bang for Your Buck(et): Fast and Space-Efficient Hardware-Accelerated
                  Coarse-Granular Indexing on GPUs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1320--1333},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00103},
	doi = {10.1109/ICDE65448.2025.00103},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HennebergSKB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent work, it has been shown that NVIDIA's ray tracing cores on RTX video cards can be exploited to realize hardware-accelerated lookups for GPU-resident database indexes. This is done by materializing all keys as triangles in a 3D scene. Lookups are performed by firing rays into the scene and utilizing the built-in index structure to detect collisions with triangles in a hardware-accelerated fashion. While this approach, called RTIndeX (or RX for short), is indeed promising, it currently suffers from three limitations: (1) significant memory overhead per key, (2) slow range lookups, and (3) poor updateability. In this work, we show that all three problems can be tackled by a single design change: Generalizing RX to become a coarse-granular index cgRX, which no longer indexes individual keys, but key buckets. We show that representing buckets in 3D space such that the lookup of a key is performed both correctly and efficiently is highly nontrivial and requires a careful orchestration of positioning triangles and firing rays in a specific sequence. Our experimental evaluation shows that cgRX offers the most bang for the buck(et) by providing a up to 6.9 x higher ratio of throughput to memory footprint than comparable baselines (that support range lookups). At the same time, cgRX improves the range-lookup performance over RX by up to 15 x and offers practical updatability that is up to 5.6x faster than rebuilding from scratch}
}


@inproceedings{DBLP:conf/icde/ZhangYH25,
	author = {Sen Zhang and
                  Qingqing Ye and
                  Haibo Hu},
	title = {Structure-Preference Enabled Graph Embedding Generation Under Differential
                  Privacy},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1334--1347},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00104},
	doi = {10.1109/ICDE65448.2025.00104},
	timestamp = {Wed, 10 Sep 2025 14:09:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangYH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph embedding generation techniques aim to learn low-dimensional vectors for each node in a graph and have recently gained increasing research attention. Publishing low-dimensional node vectors enables various graph analysis tasks, such as structural equivalence and link prediction. Yet, improper publication opens a backdoor to malicious attackers, who can infer sensitive information of individuals from the low-dimensional node vectors. Existing methods tackle this issue by developing deep graph learning models with differential privacy (DP). However, they often suffer from large noise injections and cannot provide structural preferences consistent with mining objectives. Recently, skip-gram based graph embedding generation techniques are widely used due to their ability to extract customizable structures. Based on skip-gram, we present SE-PrivGEmb, a structure-preference enabled graph embedding generation under DP. For arbitrary structure preferences, we design a unified noise tolerance mechanism via perturbing non-zero vectors. This mechanism mitigates utility degradation caused by high sensitivity. By carefully designing negative sampling probabilities in skip-gram, we theoretically demonstrate that skip-gram can preserve arbitrary proximities, which quantify structural features in graphs. Extensive experiments show that our method outperforms existing state-of-the-art methods under structural equivalence and link prediction tasks.}
}


@inproceedings{DBLP:conf/icde/LiuDCYLY25,
	author = {Shang Liu and
                  Hao Du and
                  Yang Cao and
                  Bo Yan and
                  Jinfei Liu and
                  Masatoshi Yoshikawa},
	title = {{PGB:} Benchmarking Differentially Private Synthetic Graph Generation
                  Algorithms},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1348--1361},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00105},
	doi = {10.1109/ICDE65448.2025.00105},
	timestamp = {Wed, 10 Sep 2025 14:09:53 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuDCYLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Differentially private graph analysis is a powerful tool for deriving insights from diverse graph data while protecting individual information. Designing private analytic algorithms for different graph queries often requires starting from scratch. In contrast, differentially private synthetic graph generation offers a general paradigm that supports one-time generation for multiple queries. Although various differentially private graph generation algorithms have been proposed, comparing them effectively remains challenging due to various factors, including differing privacy definitions, diverse graph datasets, varied privacy requirements, and multiple utility metrics. To this end, we propose PGB (Private Graph Benchmark), a comprehensive benchmark designed to enable researchers to compare differentially private graph generation algorithms fairly. We begin by identifying four essential elements of existing works as a 4-tuple: mechanisms, graph datasets, privacy requirements, and utility metrics. We discuss principles regarding these elements to ensure the comprehensiveness of a benchmark. Next, we present a benchmark instantiation that adheres to all principles, establishing a new method to evaluate existing and newly proposed graph generation algorithms. Through extensive theoretical and empirical analysis, we gain valuable insights into the strengths and weaknesses of prior algorithms. Our results indicate that there is no universal solution for all possible cases. Finally, we provide guidelines to help researchers select appropriate mechanisms for various scenarios.}
}


@inproceedings{DBLP:conf/icde/ZhangYSJW25,
	author = {Jiaxuan Zhang and
                  Haitao Yuan and
                  Jianing Si and
                  Nan Jiang and
                  Shangguang Wang},
	title = {Think Twice Before Imputation: Optimizing Data Imputation Order for
                  Machine Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1362--1374},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00106},
	doi = {10.1109/ICDE65448.2025.00106},
	timestamp = {Wed, 10 Sep 2025 14:09:49 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangYSJW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data imputation (DI) is a common means of enhancing data quality. To adapt to the flourishing field of machine learning (ML), an innovative class of imputation methods that consider downstream models in the imputation process has been proposed, denoted as DI for ML. A critical challenge within this context is establishing the optimal order for imputing a set of incomplete samples. To address this, we propose an iterative approach that strategically determines the imputation order based on the potential impact on model performance. At first, we design the impact score in a what-if manner to evaluate the significance of each incomplete data point for downstream ML models. In addition, to tackle the challenge of insufficient reliable complete data in real-world scenarios, we ingeniously leverage meta-learning mechanisms to enhance the robustness of the impact score computation. Finally, to avoid the risk of converging to local optima and non-diverse data selection during iterative imputation, we introduce a real-time feedback strategy using the Multi-Armed Bandit mechanism. By balancing immediate rewards with long-term strategic gains, our approach effectively navigates the complex optimization landscape, leading to globally optimal imputation orders. We experimentally validated our method on eight real-world datasets and five types of ML models, with the results indicating that the imputation order optimized by our method outperforms the current state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/XiaoGS25,
	author = {Jinzhao Xiao and
                  Zihan Guo and
                  Shaoxu Song},
	title = {{BOS:} Bit-Packing with Outlier Separation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1375--1387},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00107},
	doi = {10.1109/ICDE65448.2025.00107},
	timestamp = {Thu, 25 Dec 2025 12:47:49 +0100},
	biburl = {https://dblp.org/rec/conf/icde/XiaoGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bit-packing serves as the fundamental operator in various data encoding and compression methods. The idea is to use a fixed bit-width to represent all the (processed) values in a sequence. Some extremely large values, known as outliers, obviously amplify the bit-width, and thus lead to wasted bits for most other small values. We notice that not only the large values (upper outliers) but also the small ones (lower outliers) could incur wasted bit-width. In this paper, we propose to store both the upper and lower outliers separately, namely Bit-packing with Outlier Separation (BOS). While the remaining center values have a narrow spread, i.e., condensed bit-width, the separated outliers need some extra cost to denote their positions. The problem is thus how to determine better thresholds for separating the upper and lower outliers, yielding smaller storage cost. Rather than enumerating all the possible values as upper and lower outlier separators, in  O(n^{2}) O(n^{2})  time, we consider bit-width as the separators, with  O(n\\log n) O(n\\log n)  search time. Theoretical analysis illustrates all the possible cases such that the bit-width separation still returns the optimal solution as the value separation, and further leads to an approximate separation strategy with both median and bit-width, in  O(n) O(n)  time. Remarkably, our BOS is compatible to any existing compression methods using Bit-packing, and has replaced Bit-packing in Apache IoTDB and Apache TsFile. The extensive experiments on many real-world datasets demonstrate that by replacing Bit-packing with the proposed BOS in various compression methods, the compression ratio is significantly improved from about 2.75 to 3.25.}
}


@inproceedings{DBLP:conf/icde/ZhangLS25,
	author = {Jintao Zhang and
                  Guoliang Li and
                  Jinyang Su},
	title = {{SAGE:} {A} Framework of Precise Retrieval for {RAG}},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1388--1401},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00108},
	doi = {10.1109/ICDE65448.2025.00108},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There's a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved. It is hard to make an ideal balance. In this paper, we introduce a RAG framework, named SAGE, designed to overcome these limitations. First, to address the issue of segmentation without considering semantics, we propose to train a semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design a chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score of chunks, leading to a more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experimental results show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves a 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG, contributing to the development of more effective RAG systems.}
}


@inproceedings{DBLP:conf/icde/LiuCLZ25,
	author = {Xuan Liu and
                  Lu Chen and
                  Chengfei Liu and
                  Rui Zhou},
	title = {Searching Society Over Large Heterogeneous Information Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1402--1414},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00109},
	doi = {10.1109/ICDE65448.2025.00109},
	timestamp = {Wed, 10 Sep 2025 14:09:48 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuCLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community Search in heterogeneous information networks HINs has received great attention recently, which aims to group community members extensively connected via derived relationships based on a meta-path since there exists no real relationship between members. However, in many applications, it is desired that the derived relationships are more focused on certain query requirements and that the community members are seriously engaged. What is more, to ensure sufficient flexibility, i.e., query requirements could be closely related or relatively loose, we may need multiple communities to collectively cover all query requirements. To the best of our knowledge, there is no existing work providing such flexibility. In this paper, we propose a novel model called society. It first ensures that each derived relationship is related to query requirements, and each community member should be involved at least k in such derived relationships, i.e., the community members exhibit high homogeneous cohesiveness. To the best of our knowledge, there is no existing work providing such flexibility. Then, to ensure the serious engagement of each member, we propose a novel constraint set called heterogeneous constraints, which ensures each member seriously interacts with heterogeneous vertices consisting of the derived relationship. At last, the society model allows for finding a set of communities that collectively cover all requirements. The main challenge of searching society is to efficiently and dynamically maintain the derived relationships since the deletion of a heterogeneous vertex against a heterogeneous constraint can induce dramatic changes over the derived relationships. We propose a novel unified peeling algorithm so that we can control deletions of vertices against homogeneous and heterogeneous cohesiveness and, therefore, provide opportunities for dynamically maintaining the derived relationships. An effective dynamic data structure is then proposed to avoid re-computations of the derived relationships. After that, batch update techniques are studied, which ensure that the time complexity of updating a batch is equivalent to a single update. Extensive experimental studies are conducted on real datasets to justify the effectiveness of our proposed model and the efficiency of the proposed techniques.}
}


@inproceedings{DBLP:conf/icde/LiWCCZL25,
	author = {Fan Li and
                  Xiaoyang Wang and
                  Dawei Cheng and
                  Cong Chen and
                  Ying Zhang and
                  Xuemin Lin},
	title = {Efficient Dynamic Attributed Graph Generation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1415--1428},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00110},
	doi = {10.1109/ICDE65448.2025.00110},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiWCCZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data generation is a fundamental research problem in data management due to its diverse use cases, ranging from testing database engines to data-specific applications. However, real-world entities often involve complex interactions that cannot be effectively modeled by traditional tabular data. Therefore, graph data generation has attracted increasing attention recently. Although various graph generators have been proposed in the literature, there are three limitations: i) They cannot capture the co-evolution pattern of graph structure and node attributes. ii) Few of them consider edge direction, leading to substantial information loss. iii) Current state-of-the-art dynamic graph generators are based on the temporal random walk, making the simulation process time-consuming. To fill the research gap, we introduce VRDAG, a novel variational recurrent framework for efficient dynamic attributed graph generation. Specifically, we design a bidirectional message-passing mechanism to encode both directed structural knowledge and attribute information of a snapshot. Then, the temporal dependency in the graph sequence is captured by a recurrence state updater, generating embeddings that can preserve the evolution pattern of early graphs. Based on the hidden node embeddings, a conditional variational Bayesian method is developed to sample latent random variables at the neighboring timestep for new snapshot generation. The proposed generation paradigm avoids the time-consuming path sampling and merging process in existing random walk-based methods, significantly reducing the synthesis time. Finally, comprehensive experiments on real-world datasets are conducted to demonstrate the effectiveness and efficiency of the proposed model.}
}


@inproceedings{DBLP:conf/icde/ZhangZPLXL25,
	author = {Jiayao Zhang and
                  Chirong Zhang and
                  Jian Pei and
                  Xuan Luo and
                  Jianliang Xu and
                  Jinfei Liu},
	title = {Computing Shapley Values in Preference Queries},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1429--1442},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00111},
	doi = {10.1109/ICDE65448.2025.00111},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangZPLXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper tackles the novel problem of computing Shapley values when multiple data owners collaborate to answer preference queries. Despite extensive existing research on preference queries and Shapley value computation separately, the evaluation of data owners' contributions to cooperatively answering such queries has not been systematically explored. To address this gap, we first establish that, for a linear preference utility function with one data point per owner, the Shapley value can be computed in polynomial time. This finding is applicable to attribute weight spaces that are subsets of a simplex and represent various linear preference utility functions. For scenarios involving multiple data points per owner, we observe that only the locally optimal points from each data owner can make non-zero marginal contributions. Thus, we partition the attribute weight space into a polynomial number of subsets, ensuring that in each subset, only one data point per owner needs to be considered. Experimental results on real Airbnb Listing data and synthetic data sets validate the effectiveness and efficiency of our algorithms, which significantly outperform baseline methods.}
}


@inproceedings{DBLP:conf/icde/WangWXJFY25,
	author = {Weicheng Wang and
                  Victor Junqiu Wei and
                  Min Xie and
                  Di Jiang and
                  Lixin Fan and
                  Haijun Yang},
	title = {Interactive Search with Reinforcement Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1443--1455},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00112},
	doi = {10.1109/ICDE65448.2025.00112},
	timestamp = {Tue, 07 Oct 2025 07:34:59 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangWXJFY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The interactive regret query is one of the most representative multi-criteria decision-making queries. It identifies tuples that satisfy users' preferences via iterative user interaction. In each interactive round, it asks users a question to learn about their preferences. Once the users' preferences are sufficiently learned, it returns tuples based on the learned preferences. Nevertheless, existing algorithms for this query are typically short-term focused, i.e., they ask questions by only considering each individual interactive round, without taking the overall interaction process as a whole. This may harm the long-term benefit, leading to a large number of rounds in the overall process. To address this, we propose two algorithms based on reinforcement learning, aiming to effectively improve the overall interaction process. We first formalize the interactive regret query as a Markov Decision Process. Then, we propose two interactive algorithms, namely EA and AA, which utilize reinforcement learning to learn a good policy for selecting questions during the interaction. Both algorithms are optimized not only for the current interactive round but also for the overall interaction process, with the goal of minimizing the total number of questions asked (i.e., the total number of interactive rounds). Extensive experiments were conducted on synthetic and real datasets, showing that our algorithms reduce the number of questions asked by approximately 50% compared to existing ones under typical settings.}
}


@inproceedings{DBLP:conf/icde/BiLRWC25,
	author = {Yuran Bi and
                  Jinfei Liu and
                  Kui Ren and
                  Yihang Wu and
                  Yang Cao},
	title = {Bargaining-Based Data Markets},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1456--1469},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00113},
	doi = {10.1109/ICDE65448.2025.00113},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BiLRWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the prevalence of data-driven business, data markets where data can be commoditized, circulated, and ex-ploited are gaining considerable interest in the data management community. However, the uncertainty in data value poses great challenges for data pricing and thus data trading, which is magnified by the externality arising from the replicable nature of data. In this paper, we present the first bargaining-based data market framework to resolve the externality in data markets. Gearing toward raw data trading, we propose a three-stage bar-gaining model to formulate trading dynamics, which ascertains the data price agreed by both sellers and buyers. With parameters instantiated in preparation stage, an iterative bidding algorithm with provable convergence is designed in negotiation stage to solve the data pricing problem by eliciting equilibrium bids from participants with their profits optimized. Approximation algorithms with guaranteed bounds are presented in settlement stage to solve the NP-hard data allocation problem for profit maximization for the data seller with individual rationality satisfied for data buyers. Experiments on real datasets verify the effectiveness and efficiency of our framework.}
}


@inproceedings{DBLP:conf/icde/ZhaoXJ25,
	author = {Xuan Zhao and
                  Xike Xie and
                  Christian S. Jensen},
	title = {{HIGGS:} HIerarchy-Guided Graph Stream Summarization},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1470--1482},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00114},
	doi = {10.1109/ICDE65448.2025.00114},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoXJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph stream summarization refers to the process of processing a continuous stream of edges that form a rapidly evolving graph. The primary challenges in handling graph streams include the impracticality of fully storing the ever-growing datasets and the complexity of supporting graph queries that involve both topological and temporal information. Recent advancements, such as PGSS and Horae, address these limitations by using domainbased, top-down multi-layer structures in the form of compressed matrices. However, they either suffer from poor query accuracy, incur substantial space overheads, or have low query efficiency. This study proposes a novel item-based, bottom-up hierarchical structure, called HIGGS. Unlike existing approaches, HIGGS leverages its hierarchical structure to localize storage and query processing, thereby confining changes and hash conflicts to small and manageable subtrees, yielding notable performance improvements. HIGGS offers tighter theoretical bounds on query accuracy and space cost. Extensive empirical studies on real graph streams demonstrate that, compared to state-of-the-art methods, HIGGS is capable of notable performance enhancements: it can improve accuracy by over 3 orders of magnitude, reduce space overhead by an average of 30%, increase throughput by more than 5 times, and decrease query latency by nearly 2 orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/YangZLZZ25,
	author = {Zhong Yang and
                  Bolong Zheng and
                  Guohui Li and
                  Xi Zhao and
                  Xiaofang Zhou},
	title = {Universal Set Similarity Search via Multi-Task Representation Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1483--1495},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00115},
	doi = {10.1109/ICDE65448.2025.00115},
	timestamp = {Thu, 25 Dec 2025 12:47:49 +0100},
	biburl = {https://dblp.org/rec/conf/icde/YangZLZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Set similarity search, as a foundational operation in data processing with diverse applications in different domains, has been extensively studied. However, in the era of big data where sets sizes and quantities are rapidly increasing, set similarity search suffers from significant computational and storage overheads. Additionally, traditional approaches struggle to universally address the search problem across different similarity measures and query types. To tackle these challenges, AI techniques, with their powerful learning capabilities, may provide a viable solution. In this paper, we first propose a multi-task representation learning approach with box embeddings that accurately simulates different similarity measures simultaneously by estimating the overlap and union relationships between set pairs in latent box space. Based on the compressed representations of sets, we then introduce a universal search approach designed to answer various set similarity queries with parallel implementation. Extensive experiments conducted on real-world datasets demonstrate the universality, accuracy and efficiency of the proposed approach, showing that it outperforms competing methods. For reproduction, we release our source code on https://github.com/yangzhong901/MTBUS.}
}


@inproceedings{DBLP:conf/icde/WangWCSMW25,
	author = {Chunnan Wang and
                  Junzhe Wang and
                  Xiang Chen and
                  Xintong Song and
                  Tianyu Mu and
                  Hongzhi Wang},
	title = {Meta-Learning Based {CTR} Algorithm Selection and Hyperparameter Optimization},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1509--1522},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00117},
	doi = {10.1109/ICDE65448.2025.00117},
	timestamp = {Wed, 10 Sep 2025 14:09:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangWCSMW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The existing Click-Through Rate (CTR) algorithms have their own advantages and are sensitive to hyperparameters. Quickly obtaining a high-performance CTR model for the new task can bring good application effects. However, ordinary users fail to do so due to the lack of domain knowledge. In this paper, we remedy this deficiency by proposing AutoCTR, an efficient meta-learning based Combined Algorithm Selection and Hyperparameter Optimization (CASH) algorithm, to help non-expert users quickly find the best CTR model. In AutoCTR, we introduce the meta-learning technique to make full use of the meta-information w.r.t. CTR to guide for the new CTR task. Specifically, we utilize the meta-information to learn characteristics and representations of CTR algorithms with different settings. We use these meta experiences combined with few evaluation information on the target CTR dataset to efficiently exploring the huge CTR CASH search space for the new task. The CTR model representation method has significant influence on the quality of the learned meta experiences. To further enhance the experiences quality, we also design a Graph Neural Network (GNN) based embedding learning method. This method can link different CTR models through their components, and thus quickly learning higher-quality model representations. Extensive experimental results show that AutoCTR can quickly select suitable CTR models for different CTR tasks. Compared with the existing CASH algorithms, which ignore meta-information or rely on a huge amount of meta-information, AutoCTR is more reasonable and efficient.}
}


@inproceedings{DBLP:conf/icde/KhatiwadaKACDHHPSS25,
	author = {Aamod Khatiwada and
                  Harsha Kokel and
                  Ibrahim Abdelaziz and
                  Subhajit Chaudhury and
                  Julian Dolby and
                  Oktie Hassanzadeh and
                  Zhenhan Huang and
                  Tejaswini Pedapati and
                  Horst Samulowitz and
                  Kavitha Srinivas},
	title = {TabSketchFM: Sketch-Based Tabular Representation Learning for Data
                  Discovery Over Data Lakes},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1523--1536},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00118},
	doi = {10.1109/ICDE65448.2025.00118},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KhatiwadaKACDHHPSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enterprises have a growing need to identify relevant tables in data lakes; e.g. tables that are unionable, joinable, or subsets of each other. Tabular neural models can be help-ful for such data discovery tasks. In this paper, we present TabSketchFM, a neural tabular model for data discovery over data lakes. First, we propose novel pre-training: a sketch-based approach to enhance the effectiveness of data discovery in neural tabular models. Second, we finetune the pretrained model for identifying unionable, joinable, and subset table pairs and show significant improvement over previous tabular neural models. Third, we present a detailed ablation study to highlight which sketches are crucial for which tasks. Fourth, we use these finetuned models to perform table search; i.e., given a query table, find other tables in a corpus that are unionable, joinable, or that are subsets of the query. Our results demonstrate significant improvements in F1 scores for search compared to state-of-the-art techniques. Finally, we show significant transfer across datasets and tasks establishing that our model can generalize across different tasks and over different data lakes.}
}


@inproceedings{DBLP:conf/icde/FangZRYL25,
	author = {Shuheng Fang and
                  Kangfei Zhao and
                  Yu Rong and
                  Jeffrey Xu Yu and
                  Zhixun Li},
	title = {All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating
                  Prediction},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1537--1550},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00119},
	doi = {10.1109/ICDE65448.2025.00119},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FangZRYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different entities may be unreliable and irrelevant, which limits the performance ceiling of a specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE does not solely rely on pre-defined interaction patterns or a manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model heterogeneous interactions and directly infer the important interactions via the observed data. In the experiments, we evaluate our framework under 3 cold-start settings on 3 real-world datasets. The experimental results show that HIRE outperforms other baselines by a large margin. Furthermore, we visualize the inferred interactions of HIRE to reveal the intuition behind our framework.}
}


@inproceedings{DBLP:conf/icde/LiYP25,
	author = {Kaiyu Li and
                  Xiaohui Yu and
                  Jian Pei},
	title = {{CDA:} Cost-Sensitive Data Acquisition for Incomplete Datasets},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1551--1564},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00120},
	doi = {10.1109/ICDE65448.2025.00120},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiYP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces the novel concept of cost-sensitive data acquisition (CDA), a desirable addition to the data preparation process in a data science pipeline that focuses on strategically acquiring data from various priced sources, such as data markets, under budget constraints. CDA improves data quality by identifying the best set of values to acquire and integrating them into incomplete datasets, optimizing a particular objective defined in the resulting tables (data products). This paper focuses on CDA for a single relational table while also exploring possible extensions to multi-table contexts. First, we introduce an algorithm that utilizes conformal risk control to select rows likely to be included in the data product with probabilistic guarantees. We then investigate ways to acquire data to complete these rows under various CDA scenarios. We start with a scenario where data records are available on a row-wise basis, which proves to be an NP-hard problem. To solve this problem, we introduce an efficient row-wise greedy algorithm (RGreedy), which approaches an approximation ratio of 1. Subsequently, we explore a more generic scenario where each unit of data for acquisition may involve multiple records with a subset of the attributes. We propose a coverage minimum option selection (CMOS) algorithm for its solution, focusing on scalability. Through empirical evaluations on three real-world datasets and one synthetic dataset, we demonstrate that our methods yield performance improvements of 20 % to 40 % over applicable baselines.}
}


@inproceedings{DBLP:conf/icde/PantZYGZHWDZ25,
	author = {Zhicheng Pan and
                  Yuanjia Zhang and
                  Chengcheng Yang and
                  Ahmad Ghazal and
                  Rong Zhang and
                  Huiqi Hui and
                  Xiaoju Wu and
                  Yu Dong and
                  Xuan Zhou},
	title = {Hyper: Hybrid Physical Design Advisor with Multi-agent Reinforcement
                  Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1565--1578},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00121},
	doi = {10.1109/ICDE65448.2025.00121},
	timestamp = {Wed, 14 Jan 2026 12:35:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/PantZYGZHWDZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Various physical design (PD) options within a single database have emerged to optimize diverse workloads, including row-based PDs (e.g., index) and column-based PDs (e.g., column-store replica), each with its own acceleration advantages for different workloads. Determining the optimal combination of these two PDs is a labor-intensive and challenging task, yet it could result in significant performance improvements for the system. Recent automated index advisors (AIAs) have concentrated on identifying the most advantageous combination of row-based PDs. However, the extension of these efforts to the present problem has proven challenging due to 1) the larger search space of hybrid PD selections, 2) the inadequate consideration of the complex interactions between heterogeneous PDs, and 3) the inaccurate evaluation made by the what-if optimizer. To address these issues, we propose a Hybrid physical design advisor (Hyper) with multi-agent reinforcement learning. Hyper excels at recommending the optimal combination of PDs under any specific workload, with an overarching emphasis on both efficiency and quality. Comprehensive evaluations on well-established benchmarks show that our approach outperforms state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/GuerraVBF25,
	author = {Andrea Guerra and
                  Giorgio Vinciguerra and
                  Antonio Boffa and
                  Paolo Ferragina},
	title = {Learned Compression of Nonlinear Time Series with Random Access},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1579--1592},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00122},
	doi = {10.1109/ICDE65448.2025.00122},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GuerraVBF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series play a crucial role in many fields, including finance, healthcare, industry, and environmental monitoring. The storage and retrieval of time series can be challenging due to their unstoppable growth. In fact, these applications often sacrifice precious historical data to make room for new data. General-purpose compressors like Xz and Zstd can mitigate this problem with their good compression ratios, but they lack efficient random access on compressed data, thus preventing real-time analyses. Ad-hoc streaming solutions, instead, typically optimise only for compression and decompression speed, while giving up compression effectiveness and random access functionality. Furthermore, all these methods lack awareness of certain special regularities of time series, whose trends over time can often be described by some linear and nonlinear functions. To address these issues, we introduce NeaTS, a randomly-accessible compression scheme that approximates the time series with a sequence of nonlinear functions of different kinds and shapes, carefully selected and placed by a partitioning algorithm to minimise the space. The approximation residuals are bounded, which allows storing them in little space and thus recovering the original data losslessly, or simply discarding them to obtain a lossy time series representation with maximum error guarantees. Our experiments show that NeaTS improves the compression ratio of the state-of-the-art lossy compressors that use linear or nonlinear functions (or both) by up to 14%. Compared to lossless compressors, NeaTS emerges as the only approach to date providing, simultaneously, compression ratios close to or better than the best existing compressors, a much faster decompression speed, and orders of magnitude more efficient random access, thus enabling the storage and real-time analysis of massive and ever-growing amounts of (historical) time series data.}
}


@inproceedings{DBLP:conf/icde/WuLQMZCGY25,
	author = {Yangyang Wu and
                  Shuwei Liang and
                  Lei Qiang and
                  Xiaoye Miao and
                  Xinkui Zhao and
                  Junlan Cai and
                  Yunjun Gao and
                  Jianwei Yin},
	title = {{MISS:} An Incomplete Tabular Data Representation System with Missing
                  Mechanism Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1593--1606},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00123},
	doi = {10.1109/ICDE65448.2025.00123},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WuLQMZCGY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The missing data problem widely exists in real-life scenarios. The incomplete data analysis through imputation can amplify the errors or bias, hindering the effective analysis. Ex-isting tabular data representation methods overlook the missing state of data values, and thus cannot effectively deal with the incomplete data. In this paper, we propose a novel incomplete tabular data representation system, named MISS. It is capable of enabling all Transformer-based tabular representation methods to effectively handle incomplete data. MISS consists of two modules, i.e., missing mechanism learning (MML) and incomplete data representation (IDR). MML leverages a new missingness propensity score calculation strategy to learn the observed data distribution and missing mechanisms within incomplete data. IDR introduces a novel probability-driven Transformer block, in conjunction with an unbiased representation loss function, for effective representation. We prove that, MISS can eliminate the bias resulting from missingness. Extensive experiments on four public real-world datasets demonstrate that, MISS yields a more than 57 % accuracy gain with competitive efficiency, compared with the state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/icde/JiaLLC25,
	author = {Zhifeng Jia and
                  Hanmo Liu and
                  Haoyang Li and
                  Lei Chen},
	title = {{SIT:} Selective Incremental Training for Dynamic Knowledge Graph
                  Embedding},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1607--1621},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00124},
	doi = {10.1109/ICDE65448.2025.00124},
	timestamp = {Fri, 28 Nov 2025 08:07:21 +0100},
	biburl = {https://dblp.org/rec/conf/icde/JiaLLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, dynamic knowledge graph embedding (DKGE) has been widely studied to deal with large-scale dynamic knowledge graphs (DKG). The core idea is to encode dynamic information within DKGs into embedding vectors and decode them for various downstream tasks on the DKGs. Plenty of contributions have been made to this field. Full retraining DKG models additionally encode temporal information for higher performance, while neighboring retraining models view time data as dynamic changes in graph topology for better efficiency. However, existing approaches within these categories suffer from either effectiveness-insufficient or efficiency-insufficient issues. Recent contributions in graph area propose solutions to selectively retrain the models by choosing training data following certain criteria, but the majority of selective retraining models are designed for homogeneous graphs. The heterogeneous graph information and large graph sizes make it improper to transfer methods across scenarios. In this paper, we propose an efficient selective incremental training framework for DKGE, namely SIT. Given a restriction on training data size, we select a set of important triples instead of all triples in the DKG to improve training efficiency. In detail, we design a novel importance criteria considering DKGE model parameters, historical embedding and graph topology. Extensive experiments on open-source datasets demonstrate the effectiveness and efficiency of the SIT framework against different DKGE models.}
}


@inproceedings{DBLP:conf/icde/PangZX25,
	author = {Jianye Pang and
                  Xinjie Zhu and
                  Xiaofei Xiong},
	title = {DyFMVP: Say Goodbye to Staleness! Fresh Memory Vigorous Preserver
                  for Continuous-Time Dynamic Graph},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1622--1635},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00125},
	doi = {10.1109/ICDE65448.2025.00125},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PangZX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continuous-time dynamic graphs (CTDG) composed of chronological events, arise in many real-world complex network applications with high dynamics. Memory-based temporal graph neural networks, due to capacity to update and retain historical node states, excel at capturing dynamic changes, handling long-term dependencies with better interpretability compared to memory-less methods. Despite the effectiveness, memory-based methods suffer from two fatal fundamental flaws (i.e. long-standing staleness and memory offline update time issues), which are usually overlooked by constraints of rigid assumptions in recent works. To address the above flaws, in this work, we propose a novel Dynamic Fresh Memory Vigorous Preserver named DyFMVP, a well-designed state-traceable dual memory architecture for efficient anti-staling representation learning. To eliminate staleness within memory, a continuous memory neural ordinary differential equation process (NDP) is proposed to model the underlying memory transition distribution, rejuvenating the memory to a fresh state. In addition, a history-augmented temporal graph attention is proposed to enhance neighbor aggregation in a history-aware manner. Furthermore, a time-aligning RNN-ODE (TARO) restarter is proposed to be offline updating-free and restart at any future timestamps flexibly. Extensive experiments on various downstream tasks demonstrate the primary model DyFMVP and restarter TARO significantly outperform other state-of-the-art CTDG methods by a large margin, while also exhibiting strong robustness against extreme sparsity and long-term interval. Our code has been available on GitHub11https://github.com/CrescentMoon3/icde2025DyFMVP.}
}


@inproceedings{DBLP:conf/icde/JiaGZZKJ25,
	author = {Jinping Jia and
                  Yichen Gao and
                  Yifei Zhen and
                  Zhao Zhang and
                  Qian Kun and
                  Cheqing Jin},
	title = {{MEST:} An Efficient Authenticated Secondary Index in Blockchain Systems},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1636--1649},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00126},
	doi = {10.1109/ICDE65448.2025.00126},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/JiaGZZKJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing blockchain systems can quickly respond to verifiable primary key queries based on authenticated indexes. However, many blockchain applications also require high-performance queries on non-primary keys. For example, traders query NFT or tokenized RWA with certain features, e.g., type and return. Therefore, it necessitates authenticated secondary indexes to support efficient verifiable queries on non-primary keys. However, the existing approach to authenticated index design that couples index and authenticated digest together does not adapt well to the phased nature of non-primary key queries, making the most time-consuming process of commitment generation severely block the query process. In this study, we propose the first authenticated secondary index MEST for verifiable non-primary key queries. MEST decouples the data index and authenticated digest, which can parallelize commitment generation on the secondary index and the query processing on the primary index, thus greatly reducing the latency of the non-primary key query. Furthermore, we adopt an Extendible Hash Table to index data and propose a Merkle Growth Tree to generate commitment, which can dynamically adapt to the rapid growth of data and the skew in data access pattern. Extensive experiments on both synthetic and real datasets demonstrate that MEST improves throughput by 3.17×, reduces latency by 59%, and exhibits better scalability than baselines.}
}


@inproceedings{DBLP:conf/icde/CornellJKG25,
	author = {Filip Cornell and
                  Yifei Jin and
                  Jussi Karlgren and
                  Sarunas Girdzijauskas},
	title = {Are We Wasting Time? {A} Fast, Accurate Performance Evaluation Framework
                  for Knowledge Graph Link Predictors},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1650--1663},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00127},
	doi = {10.1109/ICDE65448.2025.00127},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CornellJKG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The standard evaluation protocol for measuring the quality of Knowledge Graph Completion methods - the task of inferring new links to be added to a graph - typically involves a step which ranks every entity of a Knowledge Graph to assess their fit as a head or tail of a candidate link to be added. In Knowledge Graphs on a larger scale, this task rapidly becomes prohibitively heavy. Previous approaches mitigate this problem by using random sampling of entities to assess the quality of links predicted or suggested by a method. However, we show that this approach has serious limitations since the ranking metrics produced do not properly reflect true outcomes. In this paper, we present a thorough analysis of these effects along with the following findings. First, we empirically find and theoretically motivate why sampling uniformly at random vastly overestimates the ranking performance of a method. We show that this can be attributed to the effect of easy versus hard negatives. Second, we propose a framework that uses relational recommenders to guide the selection of candidates for evaluation. We provide both theoretical and empirical justification of our methodology, and find that simple and fast methods work extremely well, matching advanced neural approaches. Even when a large portion of the true candidates for a property are missed, the estimation of the ranking metrics on a downstream model barely deteriorates. With our proposed framework, we can reduce the time and computation needed similar to random sampling strategies while vastly improving the estimation; on ogbl-wikikg2, we show that accurate estimations of the full ranking can be obtained in 20 seconds instead of 30 minutes. We conclude that considerable computational effort can be saved by effective preprocessing and sampling methods and still reliably predict performance accurately of the true performance for the entire ranking procedure. We make our code available to the community 1 1 Accessible at https://github.com/Filco306/are-we-wasting-time.}
}


@inproceedings{DBLP:conf/icde/CuiWLZYX25,
	author = {Ningning Cui and
                  Dong Wang and
                  Jianxin Li and
                  Huaijie Zhu and
                  Xiaochun Yang and
                  Jianliang Xu},
	title = {Towards Dynamic Boolean Range Query Over Hybrid-Storage Blockchains:
                  {A} Secure and Reliably Verifiable Framework},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1664--1676},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00128},
	doi = {10.1109/ICDE65448.2025.00128},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CuiWLZYX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hybrid-storage blockchains have become a promising paradigm for scaling up query processing services. This paradigm provides a hybrid on/off-chain approach, where only small metadata is stored on-chain while the raw data is outsourced to off-chain storage. The key concerns for query processing in such a system are security and query integrity. However, existing schemes mostly address an impractical honest-but-curious model, rather than a more realistic malicious model, due to the intricate interplay between security and query integrity. Additionally, supporting efficient dynamic update with forward security is a significant challenge for blockchain systems. To address these issues, in this paper, we first formally define the problem of secure, reliably verifiable, and dynamic Boolean range queries in hybrid-storage blockchains (SRVF). Then, we propose a novel index called Virtual Keyword Forest (VKF), which achieves secure and reliably verifiable query processing in hybrid-storage blockchains. To support dynamic queries with forward security, we propose an adaptive version-control update scheme to integrate into VKF. Furthermore, to reduce communication overhead and gas consumption, we design an aggregation signature algorithm that ensures the verification information is of a fixed size. We provide theoretical security analysis and empirical evaluations, comparing our proposed approaches with the state-of-the-art approaches, to demonstrate their feasibility.}
}


@inproceedings{DBLP:conf/icde/YuanJW25,
	author = {Yigui Yuan and
                  Peiquan Jin and
                  Xiaoliang Wang},
	title = {twCache: Thread-Wise Cache Management with High Concurrency Performance},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1677--1689},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00129},
	doi = {10.1109/ICDE65448.2025.00129},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YuanJW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cache management is a critical concern for both key-value stores and relational DBMSs. The most significant challenge in cache management is the cache replacement strategy, which directly affects the throughput and latency of the cache manager. While the Least Recently Used (LRU) policy is widely adopted by many systems, it suffers from severe performance degradation in multi-threaded environments due to lock contention. This contention arises when multiple threads attempt to update the LRU list simultaneously. Motivated by this issue, we propose a new cache management scheme called twCache, designed to deliver high performance in concurrent environments. The novelty of twCache lies in two key aspects. First, it proposes to partition the replacement policy data structure into thread-wise sublists, each corresponding to one thread. Such a structure can enable thread isolation so that the requests from one thread will not introduce lock contention with other threads, yielding high concurrency performance. Second, we propose a low-cost technique to combine recency and hotness for victim selection during cache replacement. Each sublist is maintained as an LRU list, representing the recency of object requests. Each cached object's hot count is proposed to reflect its hotness, defined as the number of sublists visiting the object. We conducted extensive experiments to compare twCache with traditional algorithms (LRU, FIFO, and 2Q) and the state-of-the-art FrozenHot policy. Three types of trace are used, including 39 Twitter traces, 23 MSR traces, and 6 YCSB workloads. The results show that twCache achieves  12\\times 12\\times  and  7\\times 7\\times  higher throughputs than LRU on the Twitter and MSR traces, respectively. Meanwhile, twCache outperforms LRU by  4.8\\times 4.8\\times  in the average throughput under YCSB workloads.}
}


@inproceedings{DBLP:conf/icde/QinLXWWSW25,
	author = {Zheng Qin and
                  Zheheng Liang and
                  Lijie Xu and
                  Wentao Wu and
                  Mingchao Wu and
                  Wuqiang Shen and
                  Wei Wang},
	title = {FreewayML: An Adaptive and Stable Streaming Learning Framework for
                  Dynamic Data Streams},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1690--1703},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00130},
	doi = {10.1109/ICDE65448.2025.00130},
	timestamp = {Mon, 15 Sep 2025 07:03:02 +0200},
	biburl = {https://dblp.org/rec/conf/icde/QinLXWWSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Streaming (machine) learning (SML) can capture dynamic changes in real-time data and perform continuous updates. It has been widely applied in real-world scenarios such as network security, financial regulation, and energy supply. How-ever, due to the sensitivity and lightweight nature of SML models, existing work suffers from low robustness, sudden decline, and catastrophic forgetting when facing unexpected data distribution drifts. Previous studies have attempted to enhance the stability of SML through methods such as data selection, replay, and constraints. However, these methods are typically designed for specific feature spaces and specific ML algorithms. In this paper, we introduce a shift graph based on the distances between data distributions and define three distinct data shift patterns. For these three patterns, we design three adaptive mechanisms, (a) multi-time granularity models, (b) coherent experience clustering, and (c) historical knowledge reuse, that are triggered by a strategy selector, with the goal of enhancing the accuracy and stability of SML. We implement an adaptive and stable SML framework, FreewayML, on top of PyTorch, which is suitable for most SML models. Experimental results show that FreewayML significantly outperforms existing SML systems in both stability and accuracy, with a comparable throughput and latency.}
}


@inproceedings{DBLP:conf/icde/WanSLZW25,
	author = {Zhuoyue Wan and
                  Yuanfeng Song and
                  Shuaimin Li and
                  Chen Jason Zhang and
                  Raymond Chi{-}Wing Wong},
	title = {DataVisT5: {A} Pre-Trained Language Model for Jointly Understanding
                  Text and Data Visualization},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1704--1717},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00131},
	doi = {10.1109/ICDE65448.2025.00131},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WanSLZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data visualization (DV) is the fundamental and premise tool to improve the efficiency in conveying the insights behind the big data, which has been widely accepted in existing data-driven world. Task automation in DV, such as converting natural language queries to visualizations (i.e., text-to-vis), gener-ating explanations from visualizations (i.e., vis-to-text), answering DV-related questions in free form (i.e. Fe VisQA), and explicating tabular data (i.e., table-to-text), is vital for advancing the field. Despite their potential, the application of pre-trained language models (PLMs) like T5 and BERT in DV has been limited by high costs and challenges in handling cross-modal information, leading to few studies on PLMs for DV. We introduce Data VisT5, a novel PLM tailored for DV that enhances the T5 architecture through a hybrid objective pre-training and multi-task fine-tuning strategy, integrating text and DV datasets to effectively interpret cross-modal semantics. Extensive evaluations on public datasets show that Data VisT5 consistently outperforms current state-of-the-art models and higher-parameter Large Language Models (LLMs) on various DV-related tasks. We anticipate that Data VisT5 will not only inspire further research on vertical PLMs but also expand the range of applications for PLMs.}
}


@inproceedings{DBLP:conf/icde/LiangZGYCYTC25,
	author = {Yuxuan Liang and
                  Wentao Zhang and
                  Xinyi Gao and
                  Ling Yang and
                  Chong Chen and
                  Hongzhi Yin and
                  Yunhai Tong and
                  Bin Cui},
	title = {Training-Free Heterogeneous Graph Condensation via Data Selection},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1718--1731},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00132},
	doi = {10.1109/ICDE65448.2025.00132},
	timestamp = {Sat, 15 Nov 2025 16:38:54 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiangZGYCYTC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient training of large-scale heterogeneous graphs is of paramount importance in real-world applications. However, existing approaches typically explore simplified models to mitigate resource and time overhead, neglecting the crucial aspect of simplifying large-scale heterogeneous graphs from the data-centric perspective. Addressing this gap, HGCond introduces graph condensation (GC) in heterogeneous graphs and generates a small condensed graph for efficient model training. Despite its efficacy in graph generation, HGCond encounters two significant limitations. The first is low effectiveness, HGCond excessively relies on the simplest relay model for the condensation procedure, which restricts the ability to exert powerful Heterogeneous Graph Neural Networks (HGNNs) with flexible condensation ratio and limits the generalization ability. The second is low efficiency, HGCond follows the existing GC methods designed for homogeneous graphs and leverages the sophisticated optimization paradigm, resulting in a time-consuming condensing procedure. In light of these challenges, we present the first Training Free Heterogeneous Graph Condensation method, termed FreeHGC, facilitating both efficient and high-quality generation of heterogeneous condensed graphs. Specifically, we reformulate the heterogeneous graph condensation problem as a data selection issue, offering a new perspective for assessing and condensing representative nodes and edges in the heterogeneous graphs. By leveraging rich meta-paths, we introduce a new, highquality heterogeneous data selection criterion to select target-type nodes. Furthermore, two training-free condensation strategies for heterogeneous graphs are designed to condense and synthesize other-types nodes effectively. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. Besides, FreeHGC exhibits excellent generalization ability across various heterogeneous graph neural networks. Our codes are available at https://github.com/PKU-DAIR/FreeHGC.}
}


@inproceedings{DBLP:conf/icde/LiWZP25,
	author = {Yan Li and
                  Liwei Wang and
                  Bolong Zheng and
                  Zhiyong Peng},
	title = {{LORE:} Learning-Based Resource Recommendation for Big Data Queries},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1732--1744},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00133},
	doi = {10.1109/ICDE65448.2025.00133},
	timestamp = {Wed, 10 Sep 2025 14:09:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiWZP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of modern cloud platforms, an increasing number of users are migrating their data analysis tasks to the cloud. Cloud platforms offer a “pay-as-you-go” model, prompting users to focus on both performance and resource costs. Existing query optimization methods primarily address query performance while neglecting resource costs. Mapping queries to their resource consumption is a complex task. To tackle this challenge, we propose a novel learning-based query resource recommendation method called LORE. LORE efficiently and accurately estimates the optimal resources for queries by leveraging dual information from SQL query statements and query execution plans. We model SQL queries and execution plans as directed acyclic graphs and utilize graph neural networks to derive comprehensive representations. To capture the dependencies among all nodes involved in data transmission within an execution plan, we assign path weights to the dependency edges of each node. Our approach integrates data distribution information and captures both direct and indirect dependencies among plan nodes while avoiding unnecessary redundant computations. Experimental results demonstrate that, compared to traditional and other learning-based methods, the LORE model achieves higher accuracy in predicting the optimal resources for queries.}
}


@inproceedings{DBLP:conf/icde/LiuNZHSYZWZFC25,
	author = {Zirui Liu and
                  Xian Niu and
                  Wei Zhou and
                  Yisen Hong and
                  Zhouran Shi and
                  Tong Yang and
                  Yuchao Zhang and
                  Yuhan Wu and
                  Yikai Zhao and
                  Zhuochen Fan and
                  Bin Cui},
	title = {Extendible RDMA-Based Remote Memory {KV} Store with Dynamic Perfect
                  Hashing Index},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1745--1758},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00134},
	doi = {10.1109/ICDE65448.2025.00134},
	timestamp = {Thu, 09 Oct 2025 07:59:56 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuNZHSYZWZFC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Perfect hashing is a special hashing function that maps each item to a unique location without collision, which enables the creation of a KV store with small and constant lookup time. Recent dynamic perfect hashing attains high load factor by increasing associativity, which impacts bandwidth and throughput. This paper proposes a novel dynamic perfect hashing index without sacrificing associativity, and uses it to devise an RDMA-based remote memory KV store called CuckooDuo. CuckooDuo simultaneously achieves high load factor, fast speed, minimal bandwidth, and efficient expansion without item movement. We theoretically analyze the properties of CuckooDuo, and implement it in an RDMA-network based testbed. The results show CuckooDuo achieves 1.9~17.6x smaller insertion latency and 9.0~18.5x smaller insertion bandwidth than prior works.}
}


@inproceedings{DBLP:conf/icde/ShenZDCFSC25,
	author = {Siqi Shen and
                  Wentao Zhang and
                  Chengshuo Du and
                  Chong Chen and
                  Fangcheng Fu and
                  Yingxia Shao and
                  Bin Cui},
	title = {Towards Scalable and Efficient Graph Structure Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1759--1772},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00135},
	doi = {10.1109/ICDE65448.2025.00135},
	timestamp = {Wed, 10 Sep 2025 14:09:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ShenZDCFSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, Graph Neural Networks (GNNs) have demonstrated remarkable capabilities in learning from graph-structured data. However, GNNs face challenges when dealing with imperfect graph structures, which often lead to performance degradation due to the underlying message propagation mechanism. In response to this issue, a class of data-centric techniques called Graph Structure Learning (GSL) has emerged, with a focus on improving the quality of graph structures. Our review of the existing GSL literature, combined with empirical studies, reveals two primary limitations: low scalability and low efficiency. To mitigate these limitations, we introduce Random Walk-based Graph Structure Learning (RWGSL), a new GSL method that utilizes random walk strategies and operates in a parameter-free manner. Extensive experiments demonstrate that Rwgsl consistently improves the classification performance of both vanilla GNNs and advanced GSL methods across various graph datasets, and Rwgsl can scale to extremely large graphs (e.g. Ogbn-Products) with acceptable time cost. In particular, the combination of Rwgsl and GCN significantly reduces the run time to approximately 5% of those observed in most GSL methods, while also achieving a superior classification accuracy. These findings validate the high scalability and robustness of Rwgsl.}
}


@inproceedings{DBLP:conf/icde/LiLCHXX25,
	author = {Huiling Li and
                  Yafei Li and
                  Wei Chen and
                  Shuo He and
                  Mingliang Xu and
                  Jianliang Xu},
	title = {Effective Task Assignment in Mobility Prediction-Aware Spatial Crowdsourcing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1773--1786},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00136},
	doi = {10.1109/ICDE65448.2025.00136},
	timestamp = {Thu, 25 Dec 2025 12:47:48 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiLCHXX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of mobile devices, spatial crowdsourcing has emerged as a promising paradigm for facilitating location-based services, encompassing various applications across academia and industries. Recently, pioneering works have attempted to infer workers' mobility patterns from historical data to improve the quality of task assignment. However, these studies have overlooked or under-examined issues such as the dynamic mobility patterns of crowd workers, especially in the context of newcomers, the misalignment between the objectives of mobility prediction and task assignment, and the effective utilization of predicted mobility patterns. In this paper, we investigate a problem we term Task Assignment in Mobility Prediction-aware Spatial Crowdsourcing (TAMP). To address the TAMP problem, we first propose a task-adaptive meta-learning algorithm, which trains a set of specific meta-knowledge for workers' mobility prediction models through game theory-based learning task clustering and meta-training within each cluster. Then, we design a task assignment-oriented loss function and develop a task assignment algorithm that incorporates prediction performance, prioritizing assignments with higher confidence of completion. Extensive experiments on real-world datasets validate that our proposed methods can effectively improve the quality of task assignment.}
}


@inproceedings{DBLP:conf/icde/ZengWMTG25,
	author = {Juxiang Zeng and
                  Pinghui Wang and
                  Linbo Ma and
                  Jing Tao and
                  Xiaohong Guan},
	title = {IsGCL: Informative Sample-Aware Progressive Graph Contrastive Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1787--1799},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00137},
	doi = {10.1109/ICDE65448.2025.00137},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZengWMTG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-level Contrastive Learning (GCL) has evolved as a powerful technique to derive representations from contrastive view pairs. Without access to labeled data, GCL typically takes two views augmented from the same graph as a positive pair and embeds them in nearby locations, while treating views from different graphs as negative pairs and pushing away their representations. Since the construction of contrastive pairs plays an important role in GCL, considerable attention has been paid to informative pairs mining. However, existing informative pairs mining methods suffer from the following two challenges: 1) Previous studies merely pay attention to the informative negative pairs while neglecting the informative positive pairs. Nevertheless, most augmentation methods require random perturbations, which may destroy the critical semantics of a graph, leading to false positive pairs (uninformative positives). 2) For informative negatives mining, most existing studies either overly emphasize hard negatives despite their potential unreliability, or rely on precise clustering pseudo-labels, which are error-prone especially in the early training stage. To solve the above challenges, we propose an informative sample-aware progressive graph contrastive learning framework, which filters both uninformative positives and negatives. In particular, we first present a progressive views sampler to evaluate the learning hardness of each view via clustering. Then, we feed model views with appropriate hardness, meaning those that aren't too challenging for the current model to assign pseudo labels confidently. Furthermore, we propose two samplers to filter out uninformative positives and negatives, respectively. Empirical results demonstrate the efficacy of our method IsGCL, which outperforms baselines by a margin of 2.5% on both MUTAG and PTC-MR in unsupervised learning settings. Furthermore, IsGCL maintains competitive training efficiency 1 1 Code available at https://github.com/jxzeng-git/IsGCL.}
}


@inproceedings{DBLP:conf/icde/ZhaoZHQ25,
	author = {Weichen Zhao and
                  Minghao Zhao and
                  Huiqi Hu and
                  Weining Qian},
	title = {Columnar Formatted Inverted Index for Highly-Paralleled, Vectorized
                  Query Processing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1800--1813},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00138},
	doi = {10.1109/ICDE65448.2025.00138},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoZHQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inverted index is a basic tool in many data-intensive applications. Though numerous efforts have been made on efficient inverted index-based query processing, existing schemes do not achieve the expected performance for modern data centers, in which servers are equipped with powerful CPUs and relatively large memory. Through comprehensive measurement studies, we identify the root course is that the data formats for index representation make it unfeasible to design efficient query execution approaches on top of it, which results in poor parallel query support and waste CPU computation. Driven by the findings, we propose to reconcile the in-memory index as columnar structures. To enable this idea, we construct the compact columnar format (i.e., Cocoa) that achieves both desirable space efficiency and maintains the capability for efficient searching support. With Cocoa, we design an efficient query executing scheme that utilizes vectorized batch processing to avoid frequent branch prediction, as well as clause enumeration with pruning to save the overhead of intermediate batch materialization. We build an open-source system VeloSearch to embody our design; experimental results show that VeloSearch achieves ~30× better performance compared with state-of-the-art search libraries such as Lucene and Tantivy.}
}


@inproceedings{DBLP:conf/icde/WangKKAPJZJX25,
	author = {Ziheng Wang and
                  Sasha Krassovsky and
                  Conor Kennedy and
                  Alex Aiken and
                  Weston Pace and
                  Rain Jiang and
                  Huayi Zhang and
                  Chenyu Jiang and
                  Wei Xu},
	title = {Rottnest: Indexing Data Lakes for Search},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1814--1827},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00139},
	doi = {10.1109/ICDE65448.2025.00139},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangKKAPJZJX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data lakes have become widely popular in managing enterprise data. Their widespread integration with query engines has allowed them to displace specialized data warehouses as the single source of truth for enterprise data. While the columnar storage format and block min-max indices allow query engines to achieve competitive performance on relational data analytics queries, they are not yet suitable for other search-oriented queries like full text and vector nearest neighbor search. We present Rottnest, a general system that builds additional lightweight indices on top of data lakes. We show that our system is more cost efficient compared to un-indexed data lakes or specialized databases across several orders of magnitude of total query loads and operating time horizons.}
}


@inproceedings{DBLP:conf/icde/LiZMZQY25,
	author = {Guojing Li and
                  Yuanyuan Zhu and
                  Junchao Ma and
                  Ming Zhong and
                  Tieyun Qian and
                  Jeffrey Xu Yu},
	title = {{TDT:} Tensor Based Directed Truss Decomposition},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1828--1840},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00140},
	doi = {10.1109/ICDE65448.2025.00140},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiZMZQY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Truss decomposition is to find the hierarchy of all the k-trusses in a graph for  k\\geq 2 k\\geq 2 . Existing GPU-based algorithms first compute edge support by parallelly counting the number of triangles each edge is contained in, and then iteratively peel off edges with the smallest support and update support of the affected edges in parallel. However, these algorithms perform truss decomposition on undirected graphs, which causes large storage space and numerous triangle existence checks during support update. Moreover, they are developed based on CUDA, which cannot naturally adapt to emerging hardware accelerators and support the end-to-end downstream graph machine learning (ML) tasks. In this paper, we propose a truss decomposition framework based on tensors (TDT), which can leverage the parallelism of heterogeneous hardware backends to speed up the computation and seamlessly integrate with downstream graph ML tasks. We first convert the original input graph into a directed graph and represent it by compacted tensors. Then we perform truss decomposition on the tensorized directed graph by efficient tensor operators. Such a directed-graph storage model not only saves the storage space but also naturally supports efficient support computation/update during the truss decomposition. To further accelerate truss decomposition, we also partition vertex neighbors into blocks to balance the computation workload and optimize key steps such as support computation/update in our framework. Extensive experimental studies show that our Python-based TDT algorithm not only achieves  2.3\\times-8.5\\times 2.3\\times-8.5\\times  speedup in most cases compared with the state-of-the-art CUDA-based algorithms, but also can efficiently deal with large graphs with hundreds of millions of nodes and billions of edges while the baseline fails due to large storage cost. Our source code is publicly available at https://github.com/LiGuojing194/TDTdecomposition.}
}


@inproceedings{DBLP:conf/icde/KandibedalaSPIG25,
	author = {Bhimesh Kandibedala and
                  Gyanendra Shrestha and
                  Anna Pyayt and
                  Todor Ivanov and
                  Michael N. Gubanov},
	title = {Scalable Tabular Hierarchical Metadata Classification in Heterogeneous
                  Structured Large-Scale Datasets Using Contrastive Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1841--1854},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00141},
	doi = {10.1109/ICDE65448.2025.00141},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KandibedalaSPIG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tabular metadata (i.e., attributes in a table) identification and classification is a fundamental problem in large-scale data management of structured corpora, especially for complex tables rich in multi-level hierarchical metadata with nesting. Medical, security, data science research literature, Web tables, contain thousands of such complex tables, but often lack or incorrectly label their complex metadata. In this work, we describe an unsupervised, scalable, contrastive-learning approach for classification of multi-layer, hierarchical metadata in such tables. We compared it to the state of the art (SOTA) as well as the latest Large Language Models (LLMs), such as OpenAI GPT 3.5 and 4 with and without Retrieval Augmented Generation (RAG) on several large-scale heterogeneous datasets. We outperform SOTA and LLMs in classifying horizontal metadata (HMD) of deep levels (3–5) and for all levels (1–3) of vertical metadata (VMD). For HMD levels 1–2, SOTA outperforms us insignificantly, with a delta of ≈1%. LLMs with/without RAG slightly outperform us with deltas of 4–5% in accuracy for HMD level 1, but we significantly outperformed LLMs/LLMs+RAG with delta up to 29% for all other levels 2–5 HMD and up to 87% delta for VMD.}
}


@inproceedings{DBLP:conf/icde/LiLYLHJZW25,
	author = {Yangning Li and
                  Qingsong Lv and
                  Tianyu Yu and
                  Yinghui Li and
                  Xuming Hu and
                  Wenhao Jiang and
                  Haitao Zheng and
                  Hui Wang},
	title = {UltraWiki: Ultra-Fine-Grained Entity Set Expansion with Negative Seed
                  Entities},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1855--1868},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00142},
	doi = {10.1109/ICDE65448.2025.00142},
	timestamp = {Wed, 10 Sep 2025 14:09:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiLYLHJZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as the given set of seed entities. Traditional methods solely relied on positive seed entities to represent the target fine-grained semantic class, rendering them tough to represent ultra-fine-grained semantic classes. Specifically, merely relying on positive seed entities leads to two inherent shortcomings: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define “unwanted” semantics. Hence, previous ESE methods struggle to address the ultra-fine-grained ESE (Ultra-ESE) task. To solve this issue, we first introduce negative seed entities in the inputs, which jointly describe the ultra-fine-grained semantic class with positive seed entities. Negative seed entities eliminate the semantic ambiguity by providing a contrast between positive and negative attributes. Meanwhile, it provides a straightforward way to express “unwanted”. To assess model performance in Ultra-ESE and facilitate further research, we also constructed UltraWiki, the first large-scale dataset tailored for Ultra-ESE. UltraWiki encompasses 50,973 entities and 394,097 sentences, alongside 236 ultra-fine-grained semantic classes, where each class is represented with 3–5 positive and negative seed entities. Moreover, a retrieval-based framework RetExpan and a generation-based framework GenExpan are proposed to provide powerful baselines for Ultra-ESE. Additionally, we devised two strategies to enhance models' comprehension of ultra-fine-grained entities' semantics: contrastive learning and chain-of-thought reasoning. Extensive experiments confirm the effectiveness of our proposed strategies and also reveal that there remains a large space for improvement in Ultra-ESE. All the codes, dataset, and supplementary notes are available at https://github.com/THUKElab/UltraWiki.}
}


@inproceedings{DBLP:conf/icde/HeCGLLPKJLCZ25,
	author = {Weiming He and
                  Qi Chen and
                  Qian Gong and
                  Jing Li and
                  Qing Liu and
                  Norbert Podhorszki and
                  Scott Klasky and
                  Ki Sung Jung and
                  Cristian Lacey and
                  Jackie Chen and
                  Hongjian Zhu},
	title = {Understanding and Estimating Error Propagation in Neural Networks
                  for Scientific Data Analysis},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1869--1881},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00143},
	doi = {10.1109/ICDE65448.2025.00143},
	timestamp = {Thu, 27 Nov 2025 20:40:45 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HeCGLLPKJLCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Neural networks are increasingly integrated into scientific discovery, where input data reduction and model quantization play a key role in accelerating inference. However, understanding and mitigating the impact of these techniques on output error is critical for ensuring reliable results, particularly in tasks demanding high numerical precision. This paper introduces a comprehensive framework for optimizing neural network inference in scientific computing by combining data reduction and model quantization while maintaining error-controlled outcomes. We develop theoretical analyses to bound error propagation under these techniques and propose a framework that balances computational performance with error constraints. Evaluation on real-world learning-based combustion simulations and satellite image classification shows that our derived error bounds accurately predict observed errors while enabling significant computational speedup under our framework. This work highlights the potential for further leveraging advancements in modern lossy compression algorithms and hardware accelerators that support lower-precision formats.}
}


@inproceedings{DBLP:conf/icde/YouFWHLX25,
	author = {Jinguo You and
                  Wanting Fu and
                  Yuxuan Wang and
                  Peilei He and
                  Kaiqi Liu and
                  Quanqing Xu},
	title = {Query Weak Equivalence and its Verification in Analytical Databases},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1882--1894},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00144},
	doi = {10.1109/ICDE65448.2025.00144},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YouFWHLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern database applications operate on massive data and support a range of complex queries, especially OLAP queries which are time-consuming. To accelerate query processing, a variety of methods for automatically verifying query equivalence have been proposed to avoid redundant executions of equivalent queries, mainly in a semantic sense. However, we have observed some queries that are not semantically equivalent also return the same tuples under the specific data distribution, which cannot be detected by most current automated verification of query equivalence. To deal with this issue, this paper proposes weak equivalence for identifying queries that are not semantically equivalent but produce the same results under the read-mostly scenarios such as OLAP. Specifically, for posed queries, we extract their filter condition expressions, which are then transformed into symbolic representations, namely first-order logic formulae. In terms of their partial order, i.e. containment relationship, we introduce Query Lattice, a novel structure that is constructed as a lattice which is partitioned into equivalence classes that are convex to answer queries if we determine they belong to the classes. The equivalence class enables stored queries to respond to future unseen queries so that redundant generation of query plan and execution can be bypassed. Experimental evaluation of Query Lattice built on top of a prevailing open-source DBMS, PostgreSQL shows that the maximum improvement that Query Lattice can achieve is 44.95 % over the original PostgreSQL, when running on the datasets of both TPC-H and TPC-H Skew benchmarks.}
}


@inproceedings{DBLP:conf/icde/ChengDLH25,
	author = {Jin Cheng and
                  Ningning Ding and
                  John C. S. Lui and
                  Jianwei Huang},
	title = {{OSTOR:} Online Scheduling Framework for Trading Continuous Queries},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1895--1909},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00145},
	doi = {10.1109/ICDE65448.2025.00145},
	timestamp = {Wed, 10 Sep 2025 14:09:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChengDLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data trading significantly enhances data utility by enabling data sharing across diverse applications. Despite being crucial for real-time analytics and online machine learning, trading continuous queries with streaming data output remains largely unexplored. The inherent characteristics of trading continuous queries pose distinctive technical challenges in scheduling query execution. First, the streaming nature demands online scheduling under information uncertainty, where data utilities and execution costs vary unpredictably during query execution. Second, the intrinsic NP-hardness of the optimization problem, coupled with repeated invocation requirements, necessitates efficient algorithmic solutions to address computational complexity. We present OSTOR, the first online scheduling framework for trading continuous queries. OSTOR aims to maximize social welfare, defined as the difference between buyers' obtained utilities and sellers' execution costs, while achieving both theoretical guarantees and practical efficiency. To handle the information uncertainty, we present a primary-dual decomposition method that transforms the online scheduling problem into multiple one-round integer programming problems, enabling adaptive decision-making that only needs current system information. To address the computational complexity, we design an adaptive dual descent (ADD) algorithm that iteratively optimizes dual variables, achieving a bounded constant approximation ratio in polynomial time. We further enhance OSTOR through structureaware greedy optimization strategies with provable performance guarantees. Extensive experiments demonstrate that OSTOR substantially improves social welfare and reduces query execution costs on both real-world and synthetic datasets, compared to existing data trading methods.}
}


@inproceedings{DBLP:conf/icde/WuWWCDZZM25,
	author = {Suzhen Wu and
                  Zuocheng Wang and
                  Shengzhe Wang and
                  Jiahong Chen and
                  Chunfeng Du and
                  Ke Zhou and
                  Jie Zhang and
                  Bo Mao},
	title = {BL-Tree: The Best of Both Worlds by Combining {B+-} Tree on Top and
                  {LSM} - Tree on Bottom},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1910--1923},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00146},
	doi = {10.1109/ICDE65448.2025.00146},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WuWWCDZZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The shattered and overlapped Level-0 data organization is the primary cause of write stall and read amplification problems in LSM-Tree-based Key-Value (KV) stores: (1) Level-0 to Level-1 compaction involves a large amount of data which induces write stalls, and (2) A point lookup needs to access multiple files in Level-0 which leads to significant read amplification. To address the problem, we propose BL-Tree by replacing the shattered Level-0 in LSM-Tree with a B+-Tree in byte-addressable Persistent Memory (PM). The sorted B+-Tree of Level-0 can accelerate the point lookup speed and reduce read/write amplification. BL-Tree further conducts the locality-aware and parallel compaction from the B+-Tree in PM (Level-0) to the lower levels of LSM-Tree in SSDs by only moving cold data downward, thus alleviating the write stalls and reducing the read/write amplification simultaneously. The extensive experiments on the prototype of BL- Tree show that it definitely avoids the write stalls and significantly reduces the read/write amplification. As a result, BL-Tree reduces the P99 tail latency by 65.2 × than LevelDB-PM and speeds up the throughput by more than 2 × under workloads with spatial locality than other KV stores.}
}


@inproceedings{DBLP:conf/icde/LiuLL25,
	author = {Haoyu Liu and
                  Ningyi Liao and
                  Siqiang Luo},
	title = {{SIGMA:} An Efficient Heterophilous Graph Neural Network with Fast
                  Global Aggregation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1924--1937},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00147},
	doi = {10.1109/ICDE65448.2025.00147},
	timestamp = {Thu, 23 Oct 2025 23:00:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) realize great success in graph learning but suffer from performance loss when meeting heterophily, i.e. neighboring nodes are dissimilar, due to their local and uniform aggregation. Existing attempts of heterophilous GNNs incorporate long-range or global aggregations to distinguish nodes in the graph. However, these aggregations usually require iteratively maintaining and updating full-graph information, which limits their efficiency when applying to large-scale graphs. In this paper, we propose SIGMA, an efficient global heterophilous GNN aggregation integrating the structural similarity measurement SimRank. Our theoretical analysis illustrates that SIGMA inherently captures distant global similarity even under heterophily, that conventional approaches can only achieve after iterative aggregations. Furthermore, it enjoys efficient one-time computation with a complexity only linear to the node set size  O(n) O ( n ) O(n) . Comprehensive evaluation demonstrates that SIGMA achieves state-of-the-art performance with superior aggregation and overall efficiency. Notably, it obtains 5 x acceleration on the large-scale heterophily dataset pokec with over 30 million edges compared to the best baseline aggregation.}
}


@inproceedings{DBLP:conf/icde/ChenHCCRSYPG25,
	author = {Yuxuan Chen and
                  Shanshan Huang and
                  Yunyao Cheng and
                  Peng Chen and
                  Zhongwen Rao and
                  Yang Shu and
                  Bin Yang and
                  Lujia Pan and
                  Chenjuan Guo},
	title = {AimTS: Augmented Series and Image Contrastive Learning for Time Series
                  Classification},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1952--1965},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00149},
	doi = {10.1109/ICDE65448.2025.00149},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenHCCRSYPG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series classification (TSC) is an important task in time series analysis. Existing TSC methods mainly train on each single domain separately, suffering from a degradation in accuracy when the samples for training are insufficient in certain domains. The pre-training and fine-tuning paradigm provides a promising direction for solving this problem. However, time series from different domains are substantially divergent, which challenges the effective pre-training on multi-source data and the generalization ability of pre-trained models. To handle this issue, we introduce Augmented Series and Image Contrastive Learning for Time Series Classification (AimTS), a pre-training framework that learns generalizable representations from multi-source time series data. We propose a two-level prototype-based contrastive learning method to effectively utilize various augmentations in multi-source pre-training, which learns representations for TSC that can be generalized to different domains. In addition, considering augmentations within the single time series modality are insufficient to fully address classification problems with distribution shift, we introduce the image modality to supplement structural information and establish a series-image contrastive learning to improve the generalization of the learned representations for TSC tasks. Extensive experiments show that after multi-source pre-training, AimTS achieves good generalization performance, enabling efficient learning and even few-shot learning on various downstream TSC datasets.}
}


@inproceedings{DBLP:conf/icde/HuangFPWWYJLYY25,
	author = {Zhuo Huang and
                  Hao Fan and
                  Junhui Peng and
                  Qi Wu and
                  Song Wu and
                  Chen Yu and
                  Hai Jin and
                  Qiming Liu and
                  Wei Yang and
                  Shuo Yu},
	title = {{WAF:} An Efficient WebAssembly-Based Execution Environment for User-Defined
                  Functions},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1966--1980},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00150},
	doi = {10.1109/ICDE65448.2025.00150},
	timestamp = {Thu, 11 Sep 2025 09:20:25 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangFPWWYJLYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User-Defined Functions (UDFs) have long served as the standard method for extending the capabilities of data management systems. With the advent of WebAssembly (WASM), UDFs' dependencies, such as language runtimes and libraries, can be compiled into a WASM module, which is then instantiated to execute the UDF. This approach offers several key advantages: 1) it allows developers to write UDFs in their preferred programming language, rather than being limited to those natively supported by the database engine; 2) it isolates UDFs' dependencies within the WASM module, mitigating the risk of errors caused by conflicting dependencies on the same host; and 3) it promotes cross-platform compatibility, enabling seamless execution of UDFs across different engines, operating systems, and architectures. However, our analysis reveals that executing a WASM-based UDF incurs overhead due to data transfer between the database engine and the WASM runtime. This process involves data copying and data layout adjustments, which can significantly impact performance. To address these challenges, we present WAF, a WASM-based UDF execution environment. WAF leverages shared memory to eliminate data copying and shifts data layout adjustments from the execution phase to the compilation phase. Experimental results show that WAF reduces the execution overhead of WASM-based UDFs by 3.1x and achieves an 18.1x speedup compared to the container-based approach, eliminating nearly all data transfer delays.}
}


@inproceedings{DBLP:conf/icde/YiYWLL25,
	author = {Liping Yi and
                  Han Yu and
                  Gang Wang and
                  Xiaoguang Liu and
                  Xiaoxiao Li},
	title = {pFedAFM: Adaptive Feature Mixture for Data-Level Personalization in
                  Heterogeneous Federated Learning on Mobile Edge Devices},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1981--1994},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00151},
	doi = {10.1109/ICDE65448.2025.00151},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YiYWLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL), an emerging distributed machine learning paradigm, utilizes edge decentralized data from multiple edge nodes (clients) to train a shared model under preserved data privacy. Furthermore, model-heterogeneous personalized federated learning (MHPFL) enables FL clients to train structurally different personalized models on non-independent and identically distributed (non-lID) local data. Existing MHPFL methods focus on data distribution differences among clients, and they propose various client-level personalization approaches to alleviate non-lID issues. However, different data samples in one client may also have different features, which are often ignored, resulting in constrained model performances. To bridge this gap, we propose a novel model-heterogeneous personalized Federated learning approach with Adaptive Feature Mixture (pFedAFM) to achieve data-level personalization while maintaining efficient communication and computation. It consists of three innovative designs: 1) We add a homogeneous small feature extractor alongside each client's local heterogeneous model, and the server aggregates these homogeneous small feature extractors for cross-client knowledge fusion. 2) We design an iterative training strategy to alternately train the global homogeneous small feature extractor and the local heterogeneous client model, for effective bidirectional exchange between global generalized knowledge and local personalized knowledge. 3) During model training, we devise a trainable weight vector to adaptively mix the features (representation) extracted by the global homogeneous and local heterogeneous models for different data samples, i.e., fulfilling data-level personalized feature mixture. Theoretical analysis proves that pFedAFM converges over time. Extensive experiments on 3 computer vision (CV) and 1 natural lan-guage processing (NLP) benchmark datasets demonstrate that pFedAFM significantly outperforms 8 state-of-the-art MHPFL methods, achieving up to 7.93% accuracy improvement while incurring low communication and computation costs.}
}


@inproceedings{DBLP:conf/icde/LiCZBX25,
	author = {Guozhong Li and
                  Byron Choi and
                  Rundong Zuo and
                  Sourav S. Bhowmick and
                  Jianliang Xu},
	title = {leSAX Index: {A} Learned {SAX} Representation Index for Time Series
                  Similarity Search},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {1995--2008},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00152},
	doi = {10.1109/ICDE65448.2025.00152},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiCZBX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series similarity search (TSSS) is a fundamental task across various applications, including classification, motif discovery, and anomaly detection. However, existing iSAX-based index methods, while known for their efficiency, often rely on hand-crafted techniques (e.g., PAA and SAX) for z-normalized time series data. However, these techniques do not fully exploit the full representation space and pose challenges to indexing. In this paper, we propose a learned index approach for TSSS. Specifically, we introduce SAXnet, a novel two-stage neural network that generates the learned SAX representation (leSAX representation) for both z-normalized and non-z-normalized time series data. The benefits of SAXnet are threefold: ① full exploitation of latent space, ② preservation of time series shapes and global information for indexing, and ③ elimination of the need for hand-crafted techniques. We then propose leSaxindex, a novel learned SAX representation index, which consists of a leSAX tree and a learned index. The distribution of the leSAX representations in the leSAX tree is adjusted to achieve a near-uniform distribution for index efficiency. Furthermore, we propose a learned index structure that works alongside the leSAX tree, applied recursively in case of large index leaf nodes. We have conducted comprehensive experiments on exact similarity search using our SAXnet and leSAX index on both real and synthetic time series datasets. The results demonstrate that our leSAX method outperforms state-of-the-art methods in efficiency, achieving performance improvements ranging from 3.6× to 17×.}
}


@inproceedings{DBLP:conf/icde/KhayatiCTC25,
	author = {Mourad Khayati and
                  Guillaume Chacun and
                  Zakhar Tymchenko and
                  Philippe Cudr{\'{e}}{-}Mauroux},
	title = {{A-DARTS:} Stable Model Selection for Data Repair in Time Series},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2009--2023},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00153},
	doi = {10.1109/ICDE65448.2025.00153},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KhayatiCTC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series often present gaps in the data. This phenomenon, also called missing values, is so prevalent that a cottage industry of missing-value imputation algorithms exists, each with different capabilities and efficacy/efficiency tradeoffs. So far, however, there has been no way to accurately select the most appropriate approach among all algorithms, given a new time series requiring imputation. In this paper, we introduce a new configuration-free system, A-DARTS (for Automated DAta Repair in Time Series), to automatically select the best imputation technique for a given faulty time series. A-DARTS's recommendation engine is trained via an iterative process that carefully learns the behavior of imputation algorithms using an extensive dataset of time series that we curated. The selection process is made efficient by several new pruning techniques particularly adjusted to time series data. Applications that manipulate time series can now easily embed A-DARTS's recommendation engine and impute data on the fly. Our experiments show that our system picks, on average, the best imputation algorithm 20% more frequently than the best-in-class AutoML technique. Moreover, it produces stable recommendations across datasets by incurring 2.5x less error variance, eliminating the stability issue observed in all state-of-the-art methods we tested.}
}


@inproceedings{DBLP:conf/icde/ManeLSL25,
	author = {Teias Mane and
                  Xiao Li and
                  Mohammad Sadoghi and
                  Mohsen Lesani},
	title = {Hamava: Fault-tolerant Reconfigurable Geo-Replication on Heterogeneous
                  Clusters},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2024--2037},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00154},
	doi = {10.1109/ICDE65448.2025.00154},
	timestamp = {Wed, 10 Dec 2025 19:16:58 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ManeLSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fault-tolerant replicated database systems consume significantly less energy than the compute-intensive proof-of-work blockchain. Thus, they are promising technologies for the building blocks that assemble global financial infrastructure. To facilitate global scaling, clustered replication protocols are essential in orchestrating nodes into clusters based on proximity. However, existing approaches often assume a homogeneous and fixed model in which the number of nodes across clusters is the same and fixed, and often limited to a fail-stop fault model. This paper presents heterogeneous and reconfigurable clustered replication for the general environment with arbitrary failures. In particular, we present Hamava,a fault-tolerant reconfigurable geo-replication that allows dynamic membership: replicas are allowed to join and leave clusters. We formally state and prove the safety and liveness properties of the protocol. Furthermore, our replication protocol is consensus-agnostic, meaning each cluster can utilize any local replication mechanism. In our comprehensive evaluation, we instantiate our replication with both HotStuff and BFT-SMaRt. Experiments on geo-distributed deployments on Google Cloud demonstrates that members of clusters can be reconfigured without significantly affecting transaction processing, and that heterogeneity of clusters may significantly improve throughput.}
}


@inproceedings{DBLP:conf/icde/LiuCLWL25,
	author = {Zeyu Liu and
                  Heyan Chai and
                  Chaoyang Li and
                  Lingzhi Wang and
                  Qing Liao},
	title = {Adaptive Data and Task Joint Scheduling for Multi-Task Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2038--2051},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00155},
	doi = {10.1109/ICDE65448.2025.00155},
	timestamp = {Thu, 11 Sep 2025 09:20:25 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuCLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-task Learning (MTL) involves training multiple tasks within a single model to improve overall performance by leveraging shared knowledge. However, this joint training can result in performance degradation due to task conflicts, typically manifesting as conflicts in task gradients. Existing solutions primarily focus on modeling task gradient relationships, which overlook the differences in how the same data sample influences different tasks. These differences are the source of intricate task gradient relationships and could further lead to varying degrees of impact from conflicts on tasks. To tackle these challenges, we propose DTJS, a novel adaptive Data and Task Joint Scheduling approach for MTL, which uniquely considers the influence of data within each task and the distinct task perception of gradient conflicts from an innovative scheduling perspective. Specifically, we design intra-task scheduling to quantify the difficulty level of the data based on its influence within each task, facilitating easy-to-hard data scheduling. Concurrently, inter-task scheduling is proposed to capture the diverse relationship among joint learning tasks via assessing the severity of conflicts between tasks and adaptively considering their effects on individual tasks through learnable task conflict perception. Furthermore, DTJS utilizes a bi-level optimization strategy that alternately updates model parameters and the learnable task conflict perception, taking into account their interdependence. Scheduled model gradients are used to optimize the MTL model, while implicit gradients refine the learnable task conflict perception. Extensive experimental results not only demonstrate that DTJS improves the performance of the MTL model over SOTA methods across various scenarios but also explain how DTJS schedules both data and tasks to bring performance improvements. The code is available at https://github.com/ZeyuLiu0706IDTJS.}
}


@inproceedings{DBLP:conf/icde/ZhengYX25,
	author = {Haoran Zheng and
                  Renchi Yang and
                  Jianliang Xu},
	title = {Adaptive Local Clustering Over Attributed Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2052--2065},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00156},
	doi = {10.1109/ICDE65448.2025.00156},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhengYX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a graph  \\mathcal{G} \\mathcal{G}  and a seed node  v_{s} v_{s} , the objective of local graph clustering (LGC) is to identify a subgraph  \\mathcal{C}_{s} \\in \\mathcal{G} \\mathcal{C}_{s} \\in \\mathcal{G}  (a.k.a. local cluster) surrounding  v_{s} v_{s}  in time roughly linear with the size of  \\mathcal{C}_{s} \\mathcal{C}_{s} . This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in  \\mathcal{G} \\mathcal{G} , rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an estimation of the bidirectional diffusion distribution (BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components of LACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over  \\mathcal{G} \\mathcal{G}  with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster.}
}


@inproceedings{DBLP:conf/icde/ZhouWWYDY25,
	author = {Dehua Zhou and
                  Bowei Wu and
                  Ke Wang and
                  Qifen Yang and
                  Yuhui Deng and
                  Siu{-}Ming Yiu},
	title = {Intervention-Driven Correlation Reduction: {A} Data Generation Approach
                  for Achieving Counterfactually Fair Predictors},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2066--2079},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00157},
	doi = {10.1109/ICDE65448.2025.00157},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhouWWYDY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Achieving counterfactual fairness is a critical objective in advancing fairness research within machine learning. Studies have shown that machine learning models often inherit biases from their training data, leading to unfair decision-making. Fair data generation methods aim to mitigate these biases, ensuring that predictors trained on such data uphold fairness. However, in the context of counterfactual fairness, existing methods for generating fair data are often limited in their applicability and lead to significant performance losses in downstream predictors. To address these issues, this paper proposes a new algorithm for generating counterfactually fair data, allowing predictors trained on this generated data to adhere to counterfactual fairness. We propose a new metric, Intervention-Driven Correlation (IDC), to evaluate the counterfactual fairness of generative models. IDC assesses fairness by applying random interventions to samples and measuring the statistical correlation between the degree of intervention and the outcome of interest. This metric is applicable to both discrete and continuous sensitive attributes and labels. Furthermore, our studies reveal a critical insight: counterfactually fair data does not always guarantee counterfactually fair predictors when deployed in real-world scenarios. We identify the root causes of this issue and propose a robust solution. To bridge this gap, we propose the IDC-Reduction method, which ensures the fairness of downstream predictors by generating counterfactually fair data. Experimentally, our method outperforms existing approaches and achieves counterfactual fairness regardless of the type of downstream predictors.}
}


@inproceedings{DBLP:conf/icde/Amagata25,
	author = {Daichi Amagata},
	title = {Random Sampling Over Spatial Range Joins},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2080--2093},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00158},
	doi = {10.1109/ICDE65448.2025.00158},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Amagata25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial range joins have many applications, including geographic information systems, location-based social networking services, neuroscience, and visualization. However, joins incur not only expensive computational costs but also too large result sets. A practical and reasonable approach to alleviating these issues is to return random samples of the join results. Although this is promising and sufficient for many applications involving spatial range joins, efficiently computing random samples is not trivial. This is because we must obtain random join samples without running spatial range joins. We address this challenging problem for the first time and aim at designing a time- and space-efficient algorithm. First, we design two baseline algorithms that employ existing techniques for random sampling and show that they are not efficient. Then, we propose a new data structure that can deal with our problem in  \\tilde{O}(n+m+t) \\tilde{O}(n+m+t)  expected time and  O(n+m) O(n+m)  space, where  n n  and  m m  are the sizes of two point sets and  t t  is the required number of samples. We conduct extensive experiments using four real spatial datasets, and the results demonstrate that our algorithm is significantly faster than the baselines in most tests.}
}


@inproceedings{DBLP:conf/icde/LiLJ25,
	author = {Zening Li and
                  Rong{-}Hua Li and
                  Fusheng Jin},
	title = {Triangle Counting Over Signed Graphs with Differential Privacy},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2094--2106},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00159},
	doi = {10.1109/ICDE65448.2025.00159},
	timestamp = {Thu, 25 Dec 2025 12:47:48 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiLJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Triangle counting serves as a foundational operator in graph analysis. Since graph data often contain sensitive information about entities, the release of triangle counts poses privacy concerns. While recent studies have addressed privacy-preserving triangle counting, they mainly concentrate on unsigned graphs. In this paper, we investigate a new problem of developing triangle counting algorithms for signed graphs that adhere to centralized differential privacy and local differential privacy, respectively. The inclusion of edge signs and more classes of triangles leads to increased complexity and overwhelms the statistics with noise. To overcome these problems, we first propose a novel algorithm for smooth-sensitivity computation to achieve differential privacy under the centralized model. In addition, to handle large signed graphs, we devise a computationally efficient function that calculates a smooth upper bound on local sensitivity. Finally, we release the approximate triangle counts after the introduction of Laplace noise, which is calibrated to the smooth upper bound on local sensitivity. In the local model, we propose a two-phase framework tailored for balanced and unbalanced triangle counting. The first phase utilizes the Generalized Randomized Response mechanism to perturb data, followed by a novel response mechanism in the second phase. Extensive experiments conducted over real-world datasets demonstrate that our proposed methods can achieve an excellent trade-off between privacy and utility.}
}


@inproceedings{DBLP:conf/icde/BaoWYXPD25,
	author = {Ergute Bao and
                  Fei Wei and
                  Yin Yang and
                  Xiaokui Xiao and
                  Tianyu Pang and
                  Chao Du},
	title = {Towards Learning on Vertically Partitioned Data with Distributed Differential
                  Privacy},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2121--2134},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00161},
	doi = {10.1109/ICDE65448.2025.00161},
	timestamp = {Tue, 14 Oct 2025 19:36:21 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BaoWYXPD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analysis of distributed data typically requires the collaboration of the data owners, as well as privacy protection. This paper focuses on the scenario where the database is vertically partitioned onto the data owners (referred to as vertical federated learning or VFL), e.g., an e-commerce platform and an online payment service collaborate to build a model to predict user behavior. To avoid revealing their private data during model fitting, the data owners commonly participate in a cryptographic protocol such as secure multiparty computation. However, the resulting model may still leak sensitive information under sophisticated data extraction attacks. A rigorous solution to this issue is to compute the model with differential privacy (DP), which provides strong and well-accepted privacy guarantees. Enforcing DP on VFL turns out to be highly challenging, and there does not yet exist an effective solution that avoids reliance on any trusted party. Consequently, practitioners are left with rather basic approaches for ensuring DP, e.g., each data owner perturbs her local data with additive noises, leading to suboptimal model utility. Can we achieve privacy-utility trade-offs for VFL with DP comparable to the centralized setting, without trusting any party? In this paper, we make a significant step towards providing a positive answer to this question. We focus on a subset of the data analysis and machine learning tasks-the class of tasks where the sensitive information to release can be expressed as a polynomial function of the input. Following the distributed DP framework that does not require any trusted party, we propose a generic mechanism to solve this class of problems, called the Skellam Quantization Mechanism (SQM). We formally prove the privacy guarantee of our solution, and show that it is able to match the privacy-utility trade-offs in the centralized setting. We then instantiate SQM on two classical tasks, principal component analysis and logistic regression. Extensive experiments on real-world datasets confirm the strong performance of SQM.}
}


@inproceedings{DBLP:conf/icde/DengPLXZQ25,
	author = {Xin Deng and
                  Peng Peng and
                  Chuanyu Liu and
                  Xianyan Xie and
                  Hui Zhou and
                  Zheng Qin},
	title = {Efficient Indexing for Label-Constrained Cohesive Subgraph Queries
                  Over Large Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2135--2147},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00162},
	doi = {10.1109/ICDE65448.2025.00162},
	timestamp = {Thu, 06 Nov 2025 20:21:55 +0100},
	biburl = {https://dblp.org/rec/conf/icde/DengPLXZQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many real-world relationships can be effectively represented as edge-labeled graphs, where edge labels encode semantic information vital for graph computations. Analyzing communities within such graphs is of great importance, with cohesive subgraph queries being a fundamental problem in graph analysis. Among these, the k-core model is one of the most widely studied frameworks for cohesive subgraph queries and has attracted significant attention over the past decade. However, most existing k-core models disregard edge labels, limiting their applicability to semantic-aware analyses. In this paper, we propose an index-based method to address the problem of querying k-cores with label constraints in edge-labeled graphs. We first introduce a basic index that maintains core decomposition results for each possible label set. Then, to further optimize performance, we propose an advanced index structure that captures the label containment properties of k- cores by computing canonical label sets for each possible  k k  and each vertex. This approach can greatly reduce the index size while ensuring efficient query processing. We also design an optimized algorithm for constructing our index, achieving a significantly faster runtime than naive construction methods. Extensive experiments on real graphs demonstrate the efficiency and effectiveness of our index-based algorithms.}
}


@inproceedings{DBLP:conf/icde/LiangLQZKFR25,
	author = {Jiaming Liang and
                  Chuan Lei and
                  Xiao Qin and
                  Jiani Zhang and
                  Asterios Katsifodimos and
                  Christos Faloutsos and
                  Huzefa Rangwala},
	title = {Featpilot: Automatic Feature Augmentation on Tabular Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2148--2160},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00163},
	doi = {10.1109/ICDE65448.2025.00163},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiangLQZKFR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tabular data within enterprises or open data repositories provide a huge opportunity for feature augmentation. Using these data sources to augment training data often boosts model performance, which is crucial in data-centric AutoML systems. Recent works on automatic feature augmentation have limited capabilities in utilizing useful features that cannot be joined with the base table without connecting through intermediate tables. We present Featpilot, a novel framework that explores and integrates high-quality features in tabular data for ML models. Featpilot evaluates a candidate feature from two aspects: (1) the efficacy of a join path connecting the feature to the base table and (2) the intrinsic value of a feature towards an ML task. Featpilot efficiently identifies high-quality features and their optimized join paths to augment the base table. Our experimental results show that Featpilot achieves up to a 10.27% improvement in ML model performance compared to state-of-the-art solutions across six public datasets.}
}


@inproceedings{DBLP:conf/icde/ShenXLCJXFZRJHC25,
	author = {Yu Shen and
                  Beicheng Xu and
                  Yupeng Lu and
                  Donghui Chen and
                  Huaijun Jiang and
                  Zhipeng Xie and
                  Senbo Fu and
                  Nan Zhang and
                  Yuxin Ren and
                  Ning Jia and
                  Xinwei Hu and
                  Bin Cui},
	title = {A-Tune-Online: Efficient and QoS-Aware Online Configuration Tuning
                  for Dynamic Workloads},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2161--2173},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00164},
	doi = {10.1109/ICDE65448.2025.00164},
	timestamp = {Mon, 15 Sep 2025 14:28:43 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ShenXLCJXFZRJHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automatic configuration tuning of online services with dynamic workloads has attracted increasing interest. Effective online tuning ensures configurations adapt to workload changes over time to maintain optimal online service performance. To be practical, online tuning must satisfy the dynamicity, efficiency, and Quality of Service (QoS) requirements. However, existing online tuning approaches fail to meet these requirements due to the inability to eliminate negative effects from historical observations. In this paper, we propose A-Tune-Online, an online configuration tuning system that tackles dynamic workloads, delivering superior tuning efficiency, and QoS guarantee simultaneously to a wide range of online scenarios. We identify that restarting the optimization based on explicit workload shift detection is necessary and critical to eliminate negative historical observations. First, to invoke optimization restarts appropriately, we design a multi-stage multi-indicator detection strategy based on heuristic rules and configuration replays. Then, to avoid initial efficiency drop after re-optimization, A-Tune-Online utilizes a similarity-based dual warm start scheme that transfers knowledge from similar historical workloads effectively. Finally, to prevent transient performance degradation from violating QoS guarantee after optimization restart, we leverage lower confidence bound to construct a safety region where each configuration is expected to perform better than the QoS requirement. Empirical study on five tuning scenarios showcases the superiority of A-Tune-Online compared with state-of-art tuning systems. A-Tune-Online achieves an average speedup of 2.90x and 1.72x compared with OnlineTune and DDPG+, respectively. We provide a version of our system in https://github.com/PKU-DAIR/A-Tune-Online.}
}


@inproceedings{DBLP:conf/icde/QiuGTY25,
	author = {Ermu Qiu and
                  Jun Gao and
                  Yaofeng Tu and
                  Jingru Yang},
	title = {LIFTus: An Adaptive Multi-Aspect Column Representation Learning for
                  Table Union Search},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2174--2187},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00165},
	doi = {10.1109/ICDE65448.2025.00165},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/QiuGTY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Table union search (TUS) represents a fundamental operation in data lakes to find tables unionable to the given one. Recent approaches to TUS mainly learn column representations for searching by introducing Pre-trained Language Models (PLMs), especially on columns with linguistic data. However, a significant amount of non-linguistic data, notably represented by domain-specific strings and numerical data in the data lake, are still under-explored in the existing methods. To address this issue, we propose LIFTus, an adaptive multi-aspect column representation for table unionable search, where aspect refers to a concept more flexible than data types, so that a single column can exhibit multiple aspects simultaneously. LIFTus aims at combining different aspects of a column (including both linguistic and non-linguistic aspects) to promote the effectiveness and generalization of TUS in a self-supervised manner. Specifically, besides employing PLMs to extract the linguistic aspects from an individual column, LIFTus trains a pattern encoder to learn possible character-level sequential patterns for the column, and builds a number encoder to capture numerical aspects of the column, including the distribution and magnitude features. LIFTus further utilizes a hierarchical cross-attention aided by aspect-relevant statistics to combine these aspects adaptively in producing the final column representations, which are indexed by vector retrieval techniques to achieve efficient search. Extensive experimental results demonstrate that LIFTus has outperformed the current state-of-the-art methods in terms of effectiveness, and achieved much better generalization capability to support unseen data.}
}


@inproceedings{DBLP:conf/icde/LiWW25,
	author = {Yuting Li and
                  Wenhua Wang and
                  Tian Wang},
	title = {pFSSL-D: Generalization Meets Personalization in Dual-Phase Federated
                  Semi-Supervised Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2188--2200},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00166},
	doi = {10.1109/ICDE65448.2025.00166},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Semi-Supervised Learning (FSSL) offers a distributed learning paradigm that addresses the critical issue of label scarcity while preserving client privacy. However, current FSSL methods are often hindered by an over-reliance on labeled data for initialization, high communication overhead, and suboptimal global model performance in heterogeneous data settings. To overcome these limitations, we propose pFSSL-D, a novel Dual-Phase Generalization and Personalization Pipeline designed to generate several models for unlabeled clients. In the first phase, decentralized contrastive learning with feature alignment is proposed to efficiently pre-train a robust and generalizable feature extraction model while minimizing communication overhead. In the personalization phase, we introduce a parameter-granularity federated fine-tuning approach with semantic consistency, which decouples model parameters into general and personalized components, providing specialized update strategies for each component. This method effectively balances global generalization with client-specific adaptation, ensuring robustness in heterogeneous environments. Extensive evaluations on benchmark datasets show that pFSSL-D consistently outperforms state-of-the-art FSSL methods in terms of accuracy, convergence speed, and resource overhead.}
}


@inproceedings{DBLP:conf/icde/HuWCDO25,
	author = {Guoyu Hu and
                  Yuncheng Wu and
                  Gang Chen and
                  Tien Tuan Anh Dinh and
                  Beng Chin Ooi},
	title = {SeSeMI: Secure Serverless Model Inference on Sensitive Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2201--2214},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00167},
	doi = {10.1109/ICDE65448.2025.00167},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuWCDO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Model inference systems are essential for implementing end-to-end data analytics pipelines that deliver the benefits of machine learning models to users. Existing cloud-based model inference systems are costly, not easy to scale, and must be trusted in handling the models and user request data. Serverless computing presents a new opportunity, as it provides elasticity and fine-grained npricing. Our goal is to design a serverless model inference system that protects models and user request data from untrusted cloud providers. It offers high performance and low cost, while requiring no intrusive changes to the current serverless platforms. To realize our goal, we leverage trusted hardware. We identify and address three challenges in using trusted hardware for serverless model inference. These challenges arise from the high-level abstraction of serverless computing, the performance overhead of trusted hardware, and the characteristics of model inference workloads. We present SeSeMI, a secure, efficient, and cost-effective serverless model inference system. It adds three novel features non-intrusively to the existing serverless infrastructure and nothing else. The first feature is a key service that establishes secure channels between the user and the serverless instances, which also provides access control to models and users' data. The second is an enclave runtime that allows one enclave to process multiple concurrent requests. The final feature is a model packer that allows multiple models to be executed by one serverless instance. We build SeSeMI on top of Apache Open Whisk, and conduct extensive experiments with three popular machine learning models. The results show that SeSeMI achieves low latency and low cost at scale for realistic workloads.}
}


@inproceedings{DBLP:conf/icde/YangLWLLGZL25,
	author = {Yaming Yang and
                  Zhuofeng Luo and
                  Zhe Wang and
                  Weigang Lu and
                  Yiheng Lu and
                  Ziyu Guan and
                  Wei Zhao and
                  Yuanhai Lv},
	title = {A Translation-Based Heterogeneous Graph Neural Network for Multiple
                  Knowledge Graphs Alignment},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2215--2226},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00168},
	doi = {10.1109/ICDE65448.2025.00168},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YangLWLLGZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graph (KG) alignment aims to integrate different KGs through the linkage of equivalent entities across them, enabling more comprehensive knowledge and facilitating information fusion. Existing methods, whether translation-based or GNN-based, typically solve this problem by projecting entities and relations into a low-dimensional embedding space, each demonstrating unique advantages in aligning a pair of KGs. However, few studies consider combining these approaches to model translation semantics of various orders. To fill this gap, we propose KG2HIN, a novel KG encoder, which innovatively views head entities, relations, and tail entities as three types of nodes, thereby transforming KGs into HINs (heterogeneous information networks). KG2HIN can adaptively learn the importance of various orders of translation semantics by seamlessly combining the HGNN aggregator operator with the translation operator in KG embedding methods. Building upon the KG2HIN encoder, we further develop a network to effectively and efficiently align multiple (more than two) KGs concurrently, a much more challenging task than the traditional pair-KG alignment task. Compared with the state-of-the-art baseline, KG2HIN significantly improves the M-Hits@1 (accuracy) score from 10.25% to 73.05% on the DBP4 dataset and from 41.19% to 97.81% on the DWY-3 dataset, while requiring significantly fewer model parameters and less training time.}
}


@inproceedings{DBLP:conf/icde/ZhouCZZXZ25,
	author = {Qian Zhou and
                  Wei Chen and
                  Li Zhang and
                  Pengpeng Zhao and
                  Jiajie Xu and
                  Lei Zhao},
	title = {Enhancing Large-Scale Entity Alignment with Critical Structure and
                  High-Quality Context},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2227--2239},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00169},
	doi = {10.1109/ICDE65448.2025.00169},
	timestamp = {Wed, 10 Sep 2025 14:09:48 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhouCZZXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity Alignment (EA) aims to identify equivalent entities across multiple Knowledge Graphs (KGs). However, when applied to larger-scale KGs, most existing EA approaches suffer from the scalability issue due to excessive GPU memory and time consumption. To mitigate this, recent advances have introduced the Large-scale EA (LsEA) task, which divides large-scale KG pairs into smaller sub-graph pairs. Despite their promising results, several notable challenges remain, preventing these advances from achieving optimal performance: 1) How to effectively utilize critical structures when generating sub-tasks? 2) How to supplement high-quality context to enhance LsEA performance? 3) How to address scenarios without alignment seeds? To tackle these challenges, we propose a novel method called ELsEA. It comprises three main components: (1) Source and Target Graph Partition, using a Metis-based weighted partitioner and a counter-part candidate generator to partition source and target graphs respectively, aiming to utilize critical structures effectively; (2) Supplement High-quality Context, which utilizes a value-based informativeness-evaluation module and a neighbor enrichment module to assess each entity's informativeness effectively, then supplement high-quality context based on this informativeness; and (3) Seed-free Setup, introducing a mixed-info pseudo-seed generation strategy to mitigate name bias, generating accurate pseudo-seeds when alignment seeds are unavailable. Extensive experiments demonstrate that ELsEA outperforms state-of-the-art baselines. The code of ELsEA is available online11https://githuh.com/wx-qzhou/ELsEA.git.}
}


@inproceedings{DBLP:conf/icde/HuangLZ25,
	author = {Shuai Huang and
                  Guoliang Li and
                  Wei Zhou},
	title = {FedRoad: Secure and Efficient Road Network Queries over Traffic Data
                  Federation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2240--2252},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00170},
	doi = {10.1109/ICDE65448.2025.00170},
	timestamp = {Thu, 09 Oct 2025 07:59:58 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated computing has emerged as a promising approach to address the data isolation problem, enabling multiple data owners to utilize secure multi-party computation (MPC) to collaboratively process queries while keeping the data decentralized, private, and secret. However, existing studies primarily focused on federated queries over structural data, which does not apply to non-structural road network queries prevalent in daily travel scenarios. To tackle this limitation, this paper proposes FedRoad, the first traffic data federation with secure and efficient road network shortest-path queries over it. In this context, the network topology is shared while each silo (e.g., mobility services platform) holds an individual traffic observation of edge weights (e.g., vehicle speeds), where we search the path with minimum joint weights (e.g., the least traveling time). To ensure security, we implement a secret-sharing-based MPC operator to secretly compare joint path weights and achieve a secure federated shortest-path search based on it. To improve the efficiency over road network structures, we (1) first minimize the search iterations by proposing federated shortcut indices and effective federated lower-bound estimation methods, (2) then reduce the cost in each iteration by designing a priority queue structure dedicated to minimizing the expensive MPC comparison operations. Extensive experiments demonstrate that FedRoad significantly outperforms the baselines  (100\\times \\text{faster}) ( 100 × faster ) (100\\times \\text{faster})  and is practical for usage (sub-second level running time).}
}


@inproceedings{DBLP:conf/icde/PengXCCS25,
	author = {Yuchen Peng and
                  Zhongle Xie and
                  Ke Chen and
                  Gang Chen and
                  Lidan Shou},
	title = {Towards Automatic and Efficient Prediction Query Processing in Analytical
                  Database},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2253--2266},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00171},
	doi = {10.1109/ICDE65448.2025.00171},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PengXCCS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data analysts nowadays are keen to have analytical capabilities involving deep learning (DL). Prediction queries, which combine relational operations with DL models to analyze multi-modal data, provide a powerful facility for smart in-database analysis. However, loose integration systems, which support such queries via User-Defined Functions (UDFs) and external runtimes, often impose high economic costs, particularly in cloud-based environments; while tight integration systems, which implement model inference through a sequence of explic-itly written SQL queries, incur heavy user burdens and huge optimization space. In this paper, we introduce PEPS, an end-to-end analytical database for automatic and efficient prediction query processing. PEPS automates the process of prediction query synthesis and ensures usability through declarative schemes. Additionally, it improves query performance with offline optimization using the DB-oriented Computation Graph Optimization (DBCGO) algorithm and online optimization via heuristic query rewriting. Empirical evaluations show that PEPS offers better usability than baseline methods, provides lower economic costs, and achieves performance speedup compared to advanced Python UDFs.}
}


@inproceedings{DBLP:conf/icde/WuLDWFLLT25,
	author = {Jiahao Wu and
                  Ning Lu and
                  Zeyu Dai and
                  Kun Wang and
                  Wenqi Fan and
                  Shengcai Liu and
                  Qing Li and
                  Ke Tang},
	title = {Backdoor Graph Condensation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2267--2280},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00172},
	doi = {10.1109/ICDE65448.2025.00172},
	timestamp = {Wed, 10 Sep 2025 14:09:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WuLDWFLLT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph condensation has recently emerged as a prevalent technique to improve the training efficiency for graph neural networks (GNNs). It condenses a large graph into a small one such that a GNN trained on this small synthetic graph can achieve comparable performance to a GNN trained on the large graph. However, while existing graph condensation studies mainly focus on the best trade-off between graph size and the GNNs' performance (model utility), they overlook the security issues of graph condensation. To bridge this gap, we first explore backdoor attack against the GNNs trained on the condensed graphs. We introduce an effective backdoor attack against graph condensation, termed BGC. This attack aims to (1) preserve the condensed graph quality despite trigger injection, and (2) ensure trigger efficacy through the condensation process, achieving a high attack success rate. Specifically, BGC consistently updates triggers during condensation and targets representative nodes for poisoning. Extensive experiments demonstrate the effectiveness of our attack. BGC achieves a high attack success rate (close to 1.0) and good model utility in all cases. Furthermore, the results against multiple defense methods demonstrate BGC's resilience under their defenses. Finally, we analyze the key hyperparameters that influence the attack performance. Our code is available at: https://github.com/JiahaoWuGitIBGC.}
}


@inproceedings{DBLP:conf/icde/LiangWDLLTQ25,
	author = {Zheng Liang and
                  Hongzhi Wang and
                  Xiaoou Ding and
                  Zhiyu Liang and
                  Chen Liang and
                  Yafeng Tang and
                  Jianzhong Qi},
	title = {Tailoring the Shapley Value for In-Context Example Selection Towards
                  Data Wrangling},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2281--2294},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00173},
	doi = {10.1109/ICDE65448.2025.00173},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiangWDLLTQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data wrangling (DW) is a fundamental step to prepare data for downstream mining tasks. Recent studies explore large language models (LLMs) to form a lightweight DW paradigm. Such studies typically require prompting an LLM with a DW task together with a few examples as task demonstrations (i.e., in-context learning). A problem yet to be explored is how to select the examples, to maximize task effectiveness given constraints on the size of the examples. To fill this gap, we introduce the constrained Shapley value (CSV), a tailored variant of the Shapley value with a constraint on the LLM prompt size, to guide example selection. We show that CSV has desirable properties in example importance estimation. Using CSV directly for LLM-based DW is still computationally intractable. We further propose activated contribution (ACSV) as an unbiased estimation for CSV and sample allocation algorithms with approximation guarantees. Empirical results show that, compared with DW examples manually selected by experts, CSV improves the effectiveness of LLMs for DW tasks including schema mapping, entity matching, error detection, and missing value imputation by 5.90% averagly in F1 score, demonstrating the general applicability of CSV for in-context learning example selection towards DW tasks.}
}


@inproceedings{DBLP:conf/icde/FanPT25,
	author = {Wenfei Fan and
                  Kehan Pang and
                  Chao Tian},
	title = {Imputing Sparse and Noisy Labels for GNNs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2295--2308},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00174},
	doi = {10.1109/ICDE65448.2025.00174},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FanPT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies how to impute labels in training data of GNNs for node classification. We introduce Label Boosting Rules (LBRs), which extend graded bisimilarity and embed ML labeling models as predicates. With LBRs, we show how to (a) assign labels to unlabeled nodes via graded bisimilarity, which is at least as expressive as node-classification GNNs; (b) correct the labels of mislabeled nodes by both logic reasoning and ML prediction; (c) improve the accuracy of ML label cleaning with logic conditions; and (d) leverage the interaction of (a) and (b) to improve the overall labeling quality. We develop an algorithm to recursively rectify noisy labels and enhance sparse labels in a unified process; we show that the algorithm is Church-Rosser, tractable and parallelly scalable. We empirically verify that the method improves the accuracy of GNNs by 14.4% on average, up to 18.2%, and it scales with large graphs.}
}


@inproceedings{DBLP:conf/icde/SongZSSYZCWTW25,
	author = {Ziyu Song and
                  Jie Zhang and
                  Jie Sun and
                  Mo Sun and
                  Zihan Yang and
                  Zheng Zhang and
                  Xuzheng Chen and
                  Fei Wu and
                  Huajin Tang and
                  Zeke Wang},
	title = {{CAM:} Asynchronous GPU-Initiated, CPU-Managed {SSD} Management for
                  Batching Storage Access},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2309--2322},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00175},
	doi = {10.1109/ICDE65448.2025.00175},
	timestamp = {Mon, 29 Sep 2025 15:55:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SongZSSYZCWTW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the wide adoption of GPU and the explosion in data volumes, existing accelerator-centric systems require massive storage access. They adopt high-performance storage devices like NVMe SSDs to scale up single-node systems cost-effectively and leverage the CPU to manage these SSDs. However, they suffer from performance bottlenecks because of the high CPU OS kernel overhead and the CPU memory intermediated data transfer. To address this issue, GPU-initiated and GPU-managed SSD management is proposed to allow the GPU to fully manipulate SSDs: 1) direct data transfer from SSD to GPU memory (data plane) and 2) GPU-managed SSD control (control plane). This can potentially enable these GPU systems to fully leverage the SSD bandwidth. However, we still identify two severe issues. First, the GPU-management SSD control leads to low GPU Streaming Multiprocessor utilization. Second, it leads to the serial execution of SSD accesses with GPU computation, which slows down the overall computing task. To this end, we propose CAM, the first asynchronous GPU-initialized, CPU-managed SSD management for batching storage access. It 1) offloads the SSD control plane from GPU to CPU, thus maximizing GPU streaming multiprocessor utilization, and 2) adopts asynchronous user-friendly APIs that allow programmers to easily overlap GPU computation and SSD I/O operations while keeping a synchronous programming experience. As such, CAM enables us to achieve the best of two worlds: high performance and high programmability. The experimental results show that CAM can perform GNN model training, mergesort, and GEMM up to  \\mathbf{1.84}\\times, \\mathbf{1.5}\\times \\mathbf{1.84}\\times, \\mathbf{1.5}\\times , and  \\mathbf{1.84}\\times \\mathbf{faster} \\mathbf{1.84}\\times \\mathbf{faster}  compared to the existing state-of-the-art GPU systems, while keeping high programmability.}
}


@inproceedings{DBLP:conf/icde/WangSCXZLZ25,
	author = {Hao Wang and
                  Jiyun Shi and
                  Yuhao Chen and
                  Haochen Xu and
                  Chi Zhang and
                  Zhaojing Luo and
                  Meihui Zhang},
	title = {{PFCA:} Efficient Path Filtering with Causal Analysis for Healthcare
                  Risk Prediction},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2323--2336},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00176},
	doi = {10.1109/ICDE65448.2025.00176},
	timestamp = {Tue, 02 Dec 2025 07:50:57 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WangSCXZLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electronic health records (EHRs) store patient medical history in the structured data format, which facilitates automatic healthcare risk prediction, thereby improving personalized healthcare management and treatment. There are two main categories of methods for automatic healthcare risk prediction. The first models time-series information or relationships between visits for enhanced patient representations. However, given the high dimensionality nature of the EHR data, it often obtains compromise results due to the lack of training data. The second exploits external knowledge, e.g., knowledge graphs (KGs), to augment the training data, but less attention has been paid to distinguishing the importance of features and filtering out irrelevant external knowledge, leading to overwhelming noise and inefficiency. Additionally, the joint relationships between patient features were not emphasized, which are highlighted in clinical practice. In this paper, we propose an efficient Path Filtering with Causal Analysis (PFCA) approach for enhanced healthcare risk prediction to address these challenges. PFCA first extracts personalized knowledge graphs (PKGs) consisting of paths linking the patient's features to targets and then devises a fine-grained filtering method based on path messages to remove irrelevant paths for better efficiency. Then we develop an effective similarity-based method to model different features' joint interactions with targets to learn augmented representations for each feature. Furthermore, we design a causal analysis method that includes a novel causal intervention mechanism to mine and prioritize causal features for improved predictive performance. Finally, by exploiting the attention weights of paths in the PKGs, PFCA provides target-oriented interpretations, showing how patients' features lead to targets through significant paths. Experimental results on three public real-world datasets and four healthcare risk prediction tasks confirm PFCA's effectiveness in improving predictive performance compared to ten state-of-the-art baselines, demonstrate its efficiency of path filtering and interpretability.}
}


@inproceedings{DBLP:conf/icde/DaltP25,
	author = {Francesco Da Dalt and
                  Adrian Perrig},
	title = {{BFES:} Towards Optimal Bayesian Frequency Estimation Sketches in
                  Data-Streams},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2337--2350},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00177},
	doi = {10.1109/ICDE65448.2025.00177},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DaltP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Measuring the frequency of items in data streams is a relevant and wide-spread problem in stream analysis and Internet traffic monitoring. This paper studies the problem of sketch-based frequency estimation from a Bayesian statistics point of view which captures uncertainties regarding the frequencies of items in a more flexible and quantitative way compared to the state of the art. We design and implement, based on Markov chain Monte Carlo, a Bayesian frequency estimation sketch that provides both state of the art accuracy, as well as greater functionality compared to other sketches such as confidence bounds for arbitrary levels, and error-function aware frequency estimates. In our theoretical work we derive information-theory related equations such as the expected information gain of a sketch, as well as the optimal least-squares Bayesian frequency estimator. In benchmarks comparing the state of the art, the proposed method achieves the lowest absolute error across all real world data streams, as well as outperforming all sketches on 4 out of 5 metrics on synthetic data. We also show that our method can provide, for multiple confidence levels simultaneously, good confidence levels on both synthetic as well as real data.}
}


@inproceedings{DBLP:conf/icde/ChlyahGL25,
	author = {Sarah Chlyah and
                  Pierre Genev{\`{e}}s and
                  Nabil Laya{\"{\i}}da},
	title = {Distributed Evaluation of Graph Queries Using Recursive Relational
                  Algebra},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2351--2365},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00178},
	doi = {10.1109/ICDE65448.2025.00178},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChlyahGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a method and its implementation Dist-Μ-RA for the optimized distributed evaluation of recursive relational algebraic terms. This method provides a systematic parallelisation technique by means of fixpoint splitting plan generation and selection. The goal is to offer expressivity for high-level queries while providing efficiency and reducing communication costs. Experimental results on both real and synthetic graphs show the effectiveness of the proposed approach compared to existing systems}
}


@inproceedings{DBLP:conf/icde/DuZZQLY25,
	author = {Zhongfan Du and
                  Ming Zhong and
                  Yuanyuan Zhu and
                  Tieyun Qian and
                  Mengchi Liu and
                  Jeffrey Xu Yu},
	title = {Efficient Frequency-Aware k-Core Query on Temporal Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2366--2379},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00179},
	doi = {10.1109/ICDE65448.2025.00179},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DuZZQLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In temporal graphs, time and topology are considered to be intertwined. As an evidence, it is observed that the vertices in more cohesive subgraphs have more frequent and more numerous interactions between each other in the history. Motivated by that, we study a novel frequency-aware k-core query problem. Different from previous studies that focus on finding k-cores in the projected subgraphs of given time intervals, we look for the subgraphs of k-core in which neighbor vertices have at least a certain number of high-frequency interactions. To address the problem, we propose 1) a minimum slope algorithm for computing the frequency in linear time, 2) a space-efficient index that stores the distinct “core frequency” of vertices for addressing arbitrary queries, 3) a propagation algorithm that collects core frequencies by message passing for index construction, and 4) efficient algorithms for retrieving a specific or all skyline results from the index respectively. The experimental results show that, our algorithms achieve several orders of magnitude improvement on efficiency compared to corresponding baselines, and meanwhile, the size of index is even smaller than that of graph unless the graph has very few timestamps on each edge. More importantly, by both statistics and case study, it is verified that the frequency-aware k-core query indeed find more cohesive subgraphs in the static k-core.}
}


@inproceedings{DBLP:conf/icde/DengCZFCZ25,
	author = {Liwei Deng and
                  Penghao Chen and
                  Ximu Zeng and
                  Yuchen Fang and
                  Jin Chen and
                  Yan Zhao},
	title = {Towards Accurate Distance Estimation for Distribution-Aware c-ANN
                  Search},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2380--2393},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00180},
	doi = {10.1109/ICDE65448.2025.00180},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/DengCZFCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Locality sensitive hashing (LSH) is a representative approach for nearest neighbor (NN) search in high-dimensional spaces, which is able to answer c-approximate NN (c-ANN) queries in sublinear time with constant probability. Existing advanced LSH methods leverage a plurality of novel techniques such as query-aware dynamic bucketing, virtual rehashing, and efficient indexing to achieve state-of-the-art performance. However, they rely on similar random LSH functions, which provides distance estimations that are irrelevant to the given data distribution. Therefore, the quality of the searched candi-dates is suboptimal. In this study, we reformulate the c-ANN query from the perspective of data distribution. Specifically, we propose a novel distribution-aware c-ANN query, which can guarantee the quality of searched results from the query distribution perspective. We introduce an accurately unbiased distance estimator into LSH methods, which can provide more precise distance estimations by modeling the data distribution. We also conduct rigorous theoretical analysis to prove that our methods can correctly answer the distribution-aware c-ANN query with at least a constant probability. Experiments on seven real datasets with different sizes and dimensionalities indicate that the proposed method can achieve better performance than existing LSH methods in terms of efficiency and effectiveness.}
}


@inproceedings{DBLP:conf/icde/PapadopoulosSP25,
	author = {Christos C. Papadopoulos and
                  Alkis Simitsis and
                  Torben Bach Pedersen},
	title = {{HAIDES:} Adaptive Approximation of Inference Queries over Unstructured
                  Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2394--2407},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00181},
	doi = {10.1109/ICDE65448.2025.00181},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/PapadopoulosSP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern analytics rely on insights derived from the execution of inference queries over vast amounts of unstructured data such as text, images, and video. Oftentimes, these queries evaluate predicates based on an expensive “oracle“ model in the likes of a deep neural network or human input that dominates the total query cost. Prior work has focused on training computationally cheap proxy models at query time that produce an approximate result. Alternatively, index-based methods apply the original oracle over a representative set of data points and generate the approximate result through an inference propagation process. Current state-of-the-art (SOTA) index-based methods require a memory -expensive index construction process which offsets their oracle cost-effectiveness and can make their usage prohibitive. In this work, we present HAIDES, an index-based, domain-agnostic framework for approximating inference on unstructured data. HAIDES consists of two main components: a coarse-to-fine framework that can be efficiently constructed using minimal memory, and a novel index adaptation component that makes use of oracle invocations during query execution in order to adaptively produce representative sets that yield high-quality approximate results. Our experimental results across three challenging domains-video, images, text-show that HAIDES (a) constructs indexes that produce performant representative sets with up to 2 orders of magnitude less memory than the SOTA baseline, while (b) requires up to 2x less oracle calls to produce the same result quality, and (c) achieves up to 10 percentage points better result quality when using the same oracle calls.}
}


@inproceedings{DBLP:conf/icde/ZhangQTZJZ25,
	author = {Huan Zhang and
                  Xiaodong Qi and
                  Haibo Tang and
                  Zhao Zhang and
                  Cheqing Jin and
                  Aoying Zhou},
	title = {Loom: {A} Deterministic Execution Framework Towards Nested Contract
                  Transactions},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2408--2421},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00182},
	doi = {10.1109/ICDE65448.2025.00182},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangQTZJZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart contracts have expanded blockchain applications, but permissioned blockchain systems face severe through-put challenges, especially with the increasing complexity of nested contract transactions. These transactions, involving cross-contract interactions and deep call chains, intensify execution conflicts and rollback overhead, ultimately limiting parallelism. We propose Loom, a deterministic execution framework that enhances the efficiency of nested contract transactions. Loom employs snapshot-based concurrent pre-execution to decompose transactions into fine-grained subtransactions. To reduce rollback overhead, it introduces a two-phase rollback algorithm to minimize computational redundancy and fine-grained rescheduling to improve subtransaction-level parallelism during re-execution. Additionally, a multi-phase parallelism mechanism optimizes resource utilization across transaction blocks. Experimental results show that Loom achieves 6.1 × to  10.2\\times 10.2\\times  higher throughput while reducing rollback overhead by 89.9% to 98.4%, significantly outperforming state-of-the-art solutions.}
}


@inproceedings{DBLP:conf/icde/LinHMSZPW25,
	author = {Yiming Lin and
                  Madelon Hulsebos and
                  Ruiying Ma and
                  Shreya Shankar and
                  Sepanta Zeighami and
                  Aditya G. Parameswaran and
                  Eugene Wu},
	title = {Querying Templatized Document Collections with Large Language Models},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2422--2435},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00183},
	doi = {10.1109/ICDE65448.2025.00183},
	timestamp = {Tue, 23 Dec 2025 19:11:16 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LinHMSZPW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Querying and extracting value from unstructured document collection remains a considerable challenge. While Large Language Models (LLMs) have made remarkable progress in document understanding, they fail to give high accuracy results for analytical queries on documents, and additionally incur high costs. While Retrieval-Augmented Generation (RAG) can reduce costs, accuracy degrades further. Our key insight is that documents in a collection often follow similar templates that impart a common semantic structure. We therefore introduce Zendb, a document analytics system that leverages this semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document collections. Zendb efficiently extracts semantic hierarchical structures from such templatized documents and introduces a novel query engine that leverages these structures for accurate and cost-effective query execution. Extensive experiments on three real-world document collections demonstrate ZENDB's benefits, achieving up to 31× cost savings compared to LLM-based baselines, while maintaining or improving accuracy, and surpassing RAG-based baselines by up to 61% in precision and 81% in recall, at a marginally higher cost.}
}


@inproceedings{DBLP:conf/icde/HeZDRB25,
	author = {Dong He and
                  Jieyu Zhang and
                  Maureen Daum and
                  Alexander Ratner and
                  Magdalena Balazinska},
	title = {MaskSearch: Querying Image Masks at Scale},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2436--2449},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00184},
	doi = {10.1109/ICDE65448.2025.00184},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HeZDRB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning tasks over image databases often generate masks that annotate image content (e.g., saliency maps, segmentation maps, depth maps) and enable a variety of applications (e.g., determine whether a model is learning spurious correlations or if an image was maliciously modified to mislead a model). While queries that retrieve examples based on mask properties are valuable to practitioners, existing systems do not support them efficiently. In this paper, we formalize the problem and propose MaskSearch, a system that focuses on accelerating queries over databases of image masks while guaranteeing the query result accuracy. MaskSearch leverages a novel indexing technique and an efficient filter-verification query execution framework. Experiments with our prototype show that MaskSearch, using indexes approximately 5% of the compressed data size, accelerates individual queries by up to two orders of magnitude and consistently outperforms existing methods on various multi-query workloads that simulate dataset exploration and analysis processes.}
}


@inproceedings{DBLP:conf/icde/WehrsteinBHB25,
	author = {Johannes Wehrstein and
                  Tiemo Bang and
                  Roman Heinrich and
                  Carsten Binnig},
	title = {{GRACEFUL:} {A} Learned Cost Estimator for UDFs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2450--2463},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00185},
	doi = {10.1109/ICDE65448.2025.00185},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WehrsteinBHB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User-Defined-Functions (UDFs) are a pivotal feature in modern DBMS, enabling the extension of native DBMS functionality with custom logic. However, the integration of UDFs into query optimization processes poses significant challenges, primarily due to the difficulty of estimating UDF execution costs. Consequently, existing cost models in DBMS optimizers largely ignore UDFs or rely on static assumptions, resulting in suboptimal performance for queries involving UDFs. In this paper, we introduce GRACEFUL, a novel learned cost model to make accurate cost predictions of query plans with UDFs enabling optimization decisions for UDFs in DBMS. For example, as we show in our evaluation, using our cost model, we can achieve 50× speedups through informed pull-up/push-down filter decisions of the UDF compared to the standard case where always a filter push-down is applied. Additionally, we release a synthetic dataset of over 90,000 UDF queries to promote further research in this area.}
}


@inproceedings{DBLP:conf/icde/SchaferBLPP25,
	author = {Patrick Sch{\"{a}}fer and
                  Jakob Brand and
                  Ulf Leser and
                  Botao Peng and
                  Themis Palpanas},
	title = {Fast and Exact Similarity Search in Less than a Blink of an Eye},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2464--2477},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00186},
	doi = {10.1109/ICDE65448.2025.00186},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SchaferBLPP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Similarity search is a fundamental operation for analyzing data series (DS), which are ordered sequences of real values. To enhance efficiency, summarization techniques are employed that reduce the dimensionality of DS. SAX-based approaches are the state-of-the-art for exact similarity queries, but their performance degrades for high-frequency signals, such as noisy data, or for high-frequency DS. In this work, we present the SymbOlic Fourier Approximation index (SOFA), which implements fast, exact similarity queries. SOFA is based on two building blocks: a tree index (inspired by MESSI) and the SFA symbolic summarization. It makes use of a learned summarization method called Symbolic Fourier Approximation (SFA), which is based on the Fourier transform and utilizes a data-adaptive quantization of the frequency domain. To better capture relevant information in high-frequency signals, SFA selects the Fourier coefficients by highest variance, resulting in a larger value range, thus larger quantization bins. The tree index solution employed by SOFA makes use of the GEMINI-approach to answer exact similarity search queries using lower bounding distance measures, and an efficient SIMD implementation. We further propose a novel benchmark comprising 17 diverse datasets, encompassing 1 billion DS. Our experimental results demonstrate that SOFA outperforms existing methods on exact similarity queries: it is up to 10 times faster than a parallel sequential scan, 3–4 times faster than FAISS, and 2 times faster on average than MESSI. For high-frequency datasets, we observe a remarkable 38-fold performance improvement.}
}


@inproceedings{DBLP:conf/icde/HuoYHV25,
	author = {Hongwei Huo and
                  Yongze Yu and
                  Zongtao He and
                  Jeffrey Scott Vitter},
	title = {Indexing Labeled Property Multidigraphs in Entropy Space, with Applications},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2478--2492},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00187},
	doi = {10.1109/ICDE65448.2025.00187},
	timestamp = {Wed, 12 Nov 2025 16:44:47 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HuoYHV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of online graph data - such as produced on social networks, citation networks, and online graph databases - calls for space-efficient graph indexing methods that support fast graph queries and graph analytics. Labeled property multidigraphs as a model of representing complicated graph data are widely used in practice. However, the fundamental problem of compressing and indexing labeled property multidigraphs has remained unsolved. In this paper, we focus on the static data case and propose a novel self-index, called CGraphIndex, to compress and index labeled property multidigraphs that for the first time achieves the high-order entropy space for multidigraph properties (the dominant term in practice) and the 1st-order graph entropy for multidigraph structures. A self-index actually encodes the original input and thus there is no need to store the input separately. CGraphIndex supports fundamental and navigational operations on the structures and on the properties in constant time, and supports fast property extraction on vertices and edges. Our experimental results on the large LDBC SNB benchmarks demonstrate that CGraphIndex outperforms the popular graph database systems (Community Editions), generally several times to orders of magnitude faster in query time and several times less in space usage for the compared interactive complex queries, business intelligence queries, as well as typical graph analytics BFS and PageRank.}
}


@inproceedings{DBLP:conf/icde/LengZQLL25,
	author = {Xiaoyu Leng and
                  Guang Zeng and
                  Hongchao Qin and
                  Longlong Lin and
                  Rong{-}Hua Li},
	title = {On Temporal-Constraint Subgraph Matching},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2493--2506},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00188},
	doi = {10.1109/ICDE65448.2025.00188},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LengZQLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal-constraint subgraph matching has emerged as a significant challenge in the study of temporal graphs, which model dynamic relationships across various domains, such as social networks and transaction networks. However, the problem of temporal-constraint subgraph matching is NP-hard. Furthermore, because each temporal-constraint contains a permutation of temporal parameters, existing subgraph matching acceleration techniques demonstrate limited applicability to temporal-constrained graphs. Traditional continuous subgraph matching approaches prove inadequate in addressing this complex problem due to their inability to effectively handle temporal constraints. This paper addresses the challenge of identifying subgraphs that not only structurally align with a given query graph but also satisfy specific temporal-constraints on the edges. We introduce three novel algorithms to tackle this issue: the TCSM-V2V algorithm, which uses a vertex-to-vertex expansion strategy and effectively prunes non-matching vertices by integrating both query and temporal-constraints into a temporal-constraint query graph; the TCSM-E2E algorithm, which employs an edge-to-edge expansion strategy, significantly reducing matching time by minimizing vertex permutation processes; and the TCSM-EVE algorithm, which combines edge-vertex-edge expansion to eliminate duplicate matches by avoiding both vertex and edge permutations. Notably, our optimal TCSM-EVE algorithm achieves an average three-order-of-magnitude speedup on large-scale datasets. Extensive experiments conducted across 6 datasets demonstrate that our approach outperforms existing methods in terms of both accuracy and computational efficiency.}
}


@inproceedings{DBLP:conf/icde/WeiZXBYCO25,
	author = {Jianxin Wei and
                  Yizheng Zhu and
                  Xiaokui Xiao and
                  Ergute Bao and
                  Yin Yang and
                  Kuntai Cai and
                  Beng Chin Ooi},
	title = {{GCON:} Differentially Private Graph Convolutional Network via Objective
                  Perturbation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2507--2520},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00189},
	doi = {10.1109/ICDE65448.2025.00189},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WeiZXBYCO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Convolutional Networks (GCNs) are a popular machine learning model with a wide range of applications in graph analytics, including healthcare, transportation, and finance. However, a GCN trained without privacy protection measures may memorize private interpersonal relationships in the training data through its model parameters. This poses a substantial risk of compromising privacy through link attacks, potentially leading to violations of privacy regulations such as GDPR. To defend against such attacks, a promising approach is to train the GCN with differential privacy (DP), a rigorous framework that provides strong privacy protection by injecting random noise into the training process. However, training a GCN under DP is a highly challenging task. Existing solutions either perturb the graph topology or inject randomness into the graph convolution operations, or overestimate the amount of noise required, resulting in severe distortions of the network's message aggregation and, thus, poor model utility. Motivated by this, we propose GCON, a novel and effective solution for training GCNs with edge differential privacy. GCON leverages the classic idea of perturbing the objective function to satisfy DP and maintains an unaltered graph convolution process. Our rigorous theoretical analysis offers tight, closed-form bounds on the sensitivity of the graph convolution results and quantifies the impact of an edge modification on the trained model parameters. Extensive experiments using multiple benchmark datasets across diverse settings demonstrate the consistent superiority of GCON over existing solutions, as well as its resilience to link inference attacks.}
}


@inproceedings{DBLP:conf/icde/LeeKMS25,
	author = {Jongha Lee and
                  Taehyung Kwon and
                  Heechan Moon and
                  Kijung Shin},
	title = {Simple yet Effective Node Property Prediction on Edge Streams under
                  Distribution Shifts},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2521--2534},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00190},
	doi = {10.1109/ICDE65448.2025.00190},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LeeKMS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of predicting node properties (e.g., node classes) in graphs has received significant attention due to its broad range of applications. Graphs from real-world datasets often evolve over time, with newly emerging edges and dynamically changing node properties, posing a significant challenge for this problem. In response, temporal graph neural networks (TGNNs) have been developed to predict dynamic node properties from a stream of emerging edges. However, our analysis reveals that most TGNN-based methods are (a) far less effective without proper node features and, due to their complex model architectures, (b) vulnerable to distribution shifts. In this paper, we propose SPLASH, a simple yet powerful method for predicting node properties on edge streams under distribution shifts. Our key contributions are as follows: (1) we propose feature augmentation methods and an automatic feature selection method for edge streams, which improve the effectiveness of TGNNs, (2) we propose a lightweight MLP-based TGNN architecture that is highly efficient and robust under distribution shifts, and (3) we conduct extensive experiments to evaluate the accuracy, efficiency, generalization, and qualitative performance of the proposed method and its competitors on dynamic node classification, dynamic anomaly detection, and node affinity prediction tasks across seven real-world datasets.}
}


@inproceedings{DBLP:conf/icde/CaoWLNCC25,
	author = {Hongtai Cao and
                  Qihao Wang and
                  Xiaodong Li and
                  Mohammad Matin Najafi and
                  Kevin Chen{-}Chuan Chang and
                  Reynold Cheng},
	title = {MuSha: Subgraph Matching by Multilevel Sharing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2548--2561},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00192},
	doi = {10.1109/ICDE65448.2025.00192},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/CaoWLNCC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph matching (SM) is a fundamental problem in graph data analysis. Real-world patterns used in graph analysis are often symmetric and contain isomorphic substructures, but existing SM algorithms fail to explore such properties. To fill this gap, we propose MuSha, a multi-objective optimization framework for SM, leveraging multilevel sharing of isomorphic substructure results to speed up SM and symmetry breaking to avoid directly computing symmetric results. To efficiently compute and cache intermediate results for sharing, MuSha applies worst-case optimal joins (WCOJs) and utilizes trie data structures to compress and index results. To enable multilevel sharing, MuSha solves a multi-objective optimization problem involving pattern decomposition, symmetry breaking, WCOJ orders, and trie structural orders. Experimental results demonstrate that MuSha outperforms the state of the art by up to two orders of magnitude on graphs of millions of vertices.}
}


@inproceedings{DBLP:conf/icde/DingZLQWWW25,
	author = {Xiaoou Ding and
                  Muyun Zhou and
                  Yida Liu and
                  Zekai Qian and
                  Chen Wang and
                  Hongzhi Wang and
                  Jianmin Wang},
	title = {{\textdollar}t{\textdollar}DCDiscover: Mining Threshold Denial Constraints
                  from Time Series Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2562--2574},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00193},
	doi = {10.1109/ICDE65448.2025.00193},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DingZLQWWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Denial constraints are vital in data quality management, but traditional mining algorithms struggle with time series data. To address this, we introduce a novel data quality rule, threshold Denial Constraints ( t t DCs), which enables predicate scaling in numerical contexts. We formalize the inference system for  t t DCs and demonstrate the monotonicity and abruptness of threshold predicates. To efficiently mine  t t DCs, we design the tDCDiscover algorithm, which leverages batch computation of differences and thresholds to significantly reduce the time required for acquiring homologous predicate evidence, achieving a 50% -66% decrease. Additionally, we introduce an evidence matrix to store evidence, lowering the complexity of evidence matching from  O(m) O(m)  to  O(1) O(1) . We propose two pruning strategies: triviality pruning and prediction coverage pruning, to effectively decrease the search paths to one-fifth of their original number and eliminating at least 90% of unnecessary paths. We theoretically prove that tDCDiscover ensures minimal, valid, and complete results. Experimental results on eight real-world datasets demonstrate that, compared to the current state-of-the-art denial constraint mining techniques, tDCDiscover achieves more than double the efficiency when processing high-dimensional time series data. In downstream data cleaning tasks, tDCDiscover improves error detection precision by an average of 40% and repair accuracy by 18%, further offering advantages in time series data quality management.}
}


@inproceedings{DBLP:conf/icde/LiangZQW25,
	author = {Juntao Liang and
                  Lan Zhang and
                  Xiangmou Qu and
                  Jun Wang},
	title = {FedEcover: Fast and Stable Converging Model-Heterogeneous Federated
                  Learning with Efficient-Coverage Submodel Extraction},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2575--2587},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00194},
	doi = {10.1109/ICDE65448.2025.00194},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiangZQW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has achieved favorable progress in addressing the data silo problem without compromising clients' data privacy. In real-world scenarios, there are numerous low-capacity clients, i.e., devices with limited resources like computational power, storage and bandwidth, holding unique and valuable data. Yet the conventional model-homogeneous paradigm is unsuitable due to its uniform model demands on all clients. To effectively utilize the data from clients of various capacities for learning a well-performing global model, researchers have proposed submodel extraction-based partial training methods allowing clients to locally train heterogeneous submodels of different sizes. However, existing partial training methods are inadequate in terms of parameter space coverage efficiency and convergence stability, which adversely affects convergence rate and the final performance. In this work, we introduce FedEcover, a model-heterogeneous framework to learn a fast and stable converging global model in challenging scenarios with dual heterogeneity of data and client capacity. Specifically, our framework incorporates an efficient submodel extraction scheme applying a random sampling without replacement strategy and a step-size decay mechanism in the global aggregation process, to enable the global model fully leveraging the heterogeneous data distributed across capacity-heterogeneous clients. Experimental results on multiple models and datasets demonstrate that our framework outperforms existing submodel extraction-based partial training methods and model-homogeneous FedAvg in both convergence rate and converged performance of the global model.}
}


@inproceedings{DBLP:conf/icde/WeiLZJG25,
	author = {Ziyao Wei and
                  Qing Liu and
                  Zhikun Zhang and
                  Shouling Ji and
                  Yunjun Gao},
	title = {Privacy-Preserving Triangle Counting in Directed Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2588--2600},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00195},
	doi = {10.1109/ICDE65448.2025.00195},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WeiLZJG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In directed graphs, the relationship between users is asymmetric, resulting in two types of triangles: cycle triangles and flow triangles. This paper studies the problem of privacy-preserving triangle counting in directed graphs. Based on different applications, we consider two scenarios, i.e., trusted and untrusted servers. In the literature, privacy-preserving triangle counting in undirected graphs has been widely studied. However, directly applying these algorithms to address our problem suffers from many issues. Concretely, for the trusted server scenario, the differentially private triangle counting algorithms, designed for undirected graphs, exhibit suboptimal performance when applied to directed graphs. Hence, we propose a new centralized differentially private algorithm that adds Laplacian noise to the exact numbers by analyzing global sensitivity. Furthermore, for the untrusted server scenario, the existing techniques cannot be used to count cycle and flow triangles with differential privacy because the local view of each user in directed graphs is limited to out-neighbors rather than all neighbors. Therefore, we design a novel locally differentially private algorithm to provide local unbiased estimation, which implies that after aggregating all the local estimations on the central server side, an unbiased estimation for the numbers of cycle and flow triangles is deduced. Empirical experiments on six real-world graph datasets demonstrate that our proposed algorithms achieve high efficiency and utility.}
}


@inproceedings{DBLP:conf/icde/ChangLGZCL25,
	author = {Xueqin Chang and
                  Qing Liu and
                  Yunjun Gao and
                  Baihua Zheng and
                  Yi Cai and
                  Qing Li},
	title = {The Most Influenced Community Search on Social Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2601--2614},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00196},
	doi = {10.1109/ICDE65448.2025.00196},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChangLGZCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we address a novel problem in social network analysis: the Most Influenced Community Search (MICS). Given a graph and a seed node set  S S , the MICS problem seeks to identify a densely connected sub graph that is most significantly impacted by  S S . We formally define MICS, prove its NP-hardness, and show that constant-factor approximation is not feasible. To solve MICS efficiently, we propose a two-phase framework. In the first phase, we compute the influenced expectation for each node, representing its likelihood of being influenced by  S S . We develop two algorithms: S-InfExp, a sampling-based method with theoretical guarantees, and L-InfExp, a learning-based approach for faster predictions. In the second phase, we introduce two algorithms, GlobalSearch and LocalSearch, to find the most influenced community. GlobalSearch uses a top-down, greedy approach, while LocalSearch applies a bottom-up strategy. Experiments on eight real-world datasets demonstrate that (1) L-InfExp is up to 100× faster than S-InfExp with comparable accuracy, (2) LocalSearch is 10× faster than GlobalSearch, with both algorithms effectively identifying the community with the highest influenced expectations, and (3) our algorithms outperform all baselines.}
}


@inproceedings{DBLP:conf/icde/LyuXNBZSF25,
	author = {Hanzheng Lyu and
                  Shaokang Xie and
                  Jianyu Niu and
                  Ivan Beschastnikh and
                  Yinqian Zhang and
                  Mohammad Sadoghi and
                  Chen Feng},
	title = {Orthrus: Accelerating Multi-BFT Consensus Through Concurrent Partial
                  Ordering of Transactions},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2615--2627},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00197},
	doi = {10.1109/ICDE65448.2025.00197},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LyuXNBZSF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-Byzantine Fault Tolerant (Multi-BFT) consensus allows multiple consensus instances to run in parallel, resolving the leader bottleneck problem inherent in classic BFT consensus. However, the global ordering of Multi-BFT consensus enforces a strict serialized sequence of transactions, imposing additional confirmation latency and also limiting concurrency. In this paper, we introduce Orthrus, a Multi-BFT protocol that accelerates transaction confirmation through partial ordering while reserving global ordering for transactions requiring stricter sequencing. To this end, Orthrus strategically partitions transactions to maximize concurrency and ensure consistency. Additionally, it incorporates an escrow mechanism to manage interactions between partially and globally ordered transactions. We evaluated Orthrus through extensive experiments in realistic settings, deploying 128 replicas in WAN and LAN environments. Our findings demonstrate latency reductions of up to 87% in WAN compared to existing Multi-BFT protocols.}
}


@inproceedings{DBLP:conf/icde/ZhuWWZQY25,
	author = {Ruicheng Zhu and
                  Xintong Wang and
                  Kai Wang and
                  Fan Zhang and
                  Zhengping Qian and
                  Long Yuan},
	title = {Efficient {\textdollar}k{\textdollar}-Truss Breaking and Minimization},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2628--2641},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00198},
	doi = {10.1109/ICDE65448.2025.00198},
	timestamp = {Sun, 01 Feb 2026 13:26:41 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhuWWZQY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The  k k -truss is a popular cohesive subgraph model for graph analysis, which requires each edge in the subgraph to be contained in at least  k-2 k-2  triangles, each consists three pairwisely connected edges. In this paper, we study the  k k -truss breaking problem (TBP) that aims to find the smallest set of edges whose removal makes the graph free of  k k -truss. The problem has been formulated in the literature with applications in community deception, critical connection identification, etc. However, existing solutions cannot scale to large graphs. We observe that chosen edges in a high-quality solution usually have high triangle support, while most share triangles with a significant number of easy-breaking edges (i.e., low-support edges). Motivated by these, we propose the Easy-Breaking Heuristic (EBH) that prioritizes the candidate edges based on their impact on easy-breaking edges. We also design several optimizations to further enhance the performance of EBH. Additionally, we extend our framework to efficiently handle the  k k -truss minimization problem (TMP), which aims to identify a set of at most  b b  edges whose removal minimizes the size of the remaining k-truss. Extensive experiments demonstrate that our proposed algorithm outperforms state-of-the-art approaches by up to three orders of magnitude in efficiency when solving TBP, while maintaining comparable effectiveness. Additionally, our proposed algorithm achieves up to four orders of magnitude improvement in efficiency for TMP, along with generally better effectiveness.}
}


@inproceedings{DBLP:conf/icde/LuoFDCFL25,
	author = {Tinghui Luo and
                  Ziquan Fang and
                  Kaixuan Duan and
                  Lu Chen and
                  Panpan Feng and
                  Mingfan Lu},
	title = {Towards Online Spatio-Temporal Prediction: {A} Knowledge Distillation
                  Driven Continual Learning Approach},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2642--2655},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00199},
	doi = {10.1109/ICDE65448.2025.00199},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuoFDCFL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatio-temporal data prediction is a fundamental task in urban computing, benefiting a variety of real-life applications such as traffic forecasting and environmental monitoring. Due to the dynamic and time-involving nature of spatio-temporal data, researchers have increasingly emphasized online prediction. However, existing approaches (e.g., URCL) typically rely on data-replay strategies, which require storing large volumes of historical data to frequently update their models with new inputs. These methods impose substantial costs, including frequent buffer construction, high storage requirements, and increased training complexity. Furthermore, the single-pass nature of online data, combined with the constrained resources of online environments, highlights the urgent need for more efficient and lightweight solutions for online spatio-temporal prediction. To address these challenges, we propose Storm, a knowledge distillation driven continual learning framework. Storm introduces Dynamic Knowledge Distillation (DKD), leveraging an ever-evolving teacher model to train an effective student model. To optimize efficiency, Storm employs a Mixture-of-Experts (MoE) mechanism, which dynamically switches between the original training mode and the DKD mode. This hybrid design enables low-cost online learning while addressing the stabilityplasticity dilemma. To fully leverage single-pass online data, Storm integrates effective data augmentation methods tailored to the dynamic nature of spatio-temporal data. Moreover, Storm incorporates a Gradual Parameter Freezing (GPF) module to progressively reduce computational costs during online training. Extensive experiments conducted on four real-world datasets, evaluated across short-term, medium-term, and long-term prediction horizons, demonstrate the superiority of Storm. Specifically, Storm: (i) provides a general online training extension for various offline spatio-temporal models, and (ii) achieves remarkable improvements, e.g., up to 14.24% accuracy gains while requiring only 0.3% of the training and inference time compared to the state-of-the-art URCL framework. The source code is publicly available at https://github.com/ZJU-DAILY/Storm.}
}


@inproceedings{DBLP:conf/icde/WuGJHZC25,
	author = {Wenhan Wu and
                  Yili Gong and
                  Jiawei Jiang and
                  Chuang Hu and
                  Xiaobo Zhou and
                  Dazhao Cheng},
	title = {Defending against Attribute Inference Attacks in Post-Training of
                  Recommendation Systems via Unlearning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2656--2669},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00200},
	doi = {10.1109/ICDE65448.2025.00200},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WuGJHZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Attribute Inference Attacks (AIAs) pose a significant threat to recommendation systems (RS) by enabling adversaries to use threat models to infer sensitive user attributes like gender or race from user embeddings, resulting in privacy breaches such as unauthorized profiling and discriminatory policies against specific groups. Existing attribute protection methods are primarily applied during training, suffering from significant limitations, such as architectural inflexibility, dependence on interaction data, and potential catastrophic degradation in recommendation performance. To overcome these challenges, we propose AttrCloak, an efficient and effective post-training attribute unlearning (AU) framework that removes sensitive information from user embeddings without altering RS training architectures. AttrCloak employs dual-objective optimization with parameter self-sharing to minimize mutual information between user embeddings and sensitive attributes while preserving recommendation quality. Furthermore, it accommodates data-free scenarios by leveraging regularization loss when interaction data is unavailable. Comprehensive evaluations on four real-world datasets demonstrate AttrCloak's good performance in privacy protection and recommendation performance.}
}


@inproceedings{DBLP:conf/icde/ZhangNWHL25,
	author = {Yuting Zhang and
                  Wei Ni and
                  Kai Wang and
                  Yizhang He and
                  Conggai Li},
	title = {Truss Decomposition Under Edge Local Differential Privacy},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2670--2683},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00201},
	doi = {10.1109/ICDE65448.2025.00201},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangNWHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {k-truss is a widely studied cohesive sub graph model that has gained significant attention over the past decades. Truss decomposition, a fundamental task in graph analysis, aims to compute the largest k for which an edge belongs to a k-truss. However, directly performing truss decomposition on sensitive graphs risks exposing the private information of user connections in real-world applications. Edge local differential privacy (edge LDP) is extensively used to protect the privacy of edges in graph analysis. This paper, for the first time, addresses the problem of truss decomposition under edge LDP. A naive approach allows each vertex to perturb its neighbor list locally and generate a noisy graph for truss decomposition. However, it often produces excessive truss number estimations, since the noisy graph is generally much denser and fails to preserve the input graph structure. To obtain more accurate estimates, we propose the Local algorithm that leverages the local information during the truss decomposition process. Furthermore, to avoid adding substantial noise to truss numbers to satisfy edge LDP, we introduce the Global algorithm that optimizes the noise scale of support numbers, enhancing the accuracy of truss decom-position results. We further propose the Global * algorithm that eliminates the need for vertices to download noisy edges by utilizing noisy degrees to adjust support numbers during truss decomposition, achieving high accuracy with significantly lower communication costs. Extensive experiments on 9 real-world datasets demonstrate the effectiveness and efficiency of our proposed algorithms.}
}


@inproceedings{DBLP:conf/icde/FangLPHT25,
	author = {Yujie Fang and
                  Xin Li and
                  Yuangang Pan and
                  Xin Huang and
                  Ivor W. Tsang},
	title = {Boosting with Fewer Tokens: Multi-Query Optimization for LLMs Using
                  Node Text and Neighbor Cues},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2684--2697},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00202},
	doi = {10.1109/ICDE65448.2025.00202},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/FangLPHT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent studies have explored querying large language models (LLMs) to serve as predictors for graph mining tasks on text-attributed graphs (TAGs), establishing a promising paradigm that surpasses Graph Neural Networks (GNNs) in scalability and generalization. However, the high token costs of LLMs make this approach prohibitively expensive for large-scale node queries, and effective multi-query optimization solutions are currently lacking. By conducting information theory analysis at the single query level, we have gained insights that enabled the development of two multi-query optimization strategies: token pruning and query boosting. The token pruning strategy is designed to reduce token usage without compromising task performance by identifying saturated node queries and pruning tokens for these queries. Meanwhile, the query boosting strategy is designed to enhance task performance by enriching the context of unexecuted queries with pseudo-labels derived from previous queries through strategic scheduling, thereby maximizing the utility of these pseudo-labels. Extensive experiments applying these two strategies, either jointly or individually, to various existing methods demonstrate that the proposed approach serves our intentions well. Besides, this paper offers a fresh methodology for optimizing LLM processing of graph tasks, demonstrating great potential. For most natural graph data benchmarks in the field, it can save tokens by several orders of magnitude. For example, on the Ogbn-Products dataset, it could theoretically save up to  2\\times 10^{9} 2\\times 10^{9}  tokens.}
}


@inproceedings{DBLP:conf/icde/DongWWZC25,
	author = {Fangming Dong and
                  Pinghui Wang and
                  Yuance Wang and
                  Chen Zhang and
                  Lizhen Cui},
	title = {Fast Private Retrieval on Key-Value Store with Multiple Values per
                  Key},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2698--2711},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00203},
	doi = {10.1109/ICDE65448.2025.00203},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DongWWZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Querying desired data from the key-value store on a cloud server is a prevalent scenario. Client queries might include sensitive information that the client prefers to keep confidential from the server. This occasion resembles the Keyword Private Information Retrieval (KPIR). Prior works on keyword PIR consider that there are no duplicated key-value pairs in the store, i.e., each key only occurs once with only a single value attached. This is one of the cases in practical applications. However, there is also a typical case where a key may occur multiple times with different values. Straightly applying the existing keyword PIR to this case doesn't work and may finally obtain a false query result. We are the first to extend the setting that keys in the store may appear with different values multiple times. To solve this problem, we propose FEDPIR, a fast single-server keyword PIR protocol that supports querying a large-scale key-value store with multiple values per key. FEDPIR uses a novel encoding and decoding strategy combined with a high-throughput linear homomorphic encryption to improve performance significantly. Our extensive experiments on different store configurations show that our FEDPIR achieves 1.2-65.6x lower query latency and 1.5-37.9x lower cost monetarily compared with the baseline methods.}
}


@inproceedings{DBLP:conf/icde/ChenMQGLZ25,
	author = {Jinwen Chen and
                  Hao Miao and
                  Dazhuo Qiu and
                  Jiannan Guo and
                  Yawen Li and
                  Yan Zhao},
	title = {Sustainability-Oriented Task Recommendation in Spatial Crowdsourcing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2712--2725},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00204},
	doi = {10.1109/ICDE65448.2025.00204},
	timestamp = {Wed, 10 Sep 2025 14:09:47 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenMQGLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid evolution of sensing techniques and the proliferation of mobile devices, spatial crowdsourcing (SC) has gained significant attention in both academia and industry. SC involves assigning location-based tasks to mobile workers, with task recommendation playing a key role in helping workers identify suitable and appealing tasks. However, most existing studies focus on task completion rate, worker satisfaction, or efficiency, without consideration of the environmental impact, e.g., pollutant emissions from the increased vehicle usage associated with SC applications like Uber, Lyft, and FoodPanda. In this study, we consider a novel problem of sustainable task recommendation in SC, which aims to minimize the environmental footprint (i.e., pollution) while maintaining acceptable levels of task completion, worker satisfaction, and overall task recommendation efficiency. We develop an innovative Sustainability-Oriented Task Recommendation framework encompassing two major components: speed-driven pollutant emission estimation and task recommendation. Specifically, the pollutant emission estimation component aims to estimate future pollutant emissions based on worker trajectories and speeds, using a context-enhanced spatio-temporal network for road speed prediction. In the task recommendation component, we provide a completion-sensitive recommendation algorithm to maximize the expected number of completed tasks. Further, we design an efficient emission-optimized KM ranking algorithm to minimize emissions. Experiments on real data offer insight into the effectiveness and efficiency of the proposals, providing valuable insights into its potential for sustainable spatial crowdsourcing.}
}


@inproceedings{DBLP:conf/icde/SongLSCWZ25,
	author = {Yuanfeng Song and
                  Jinwei Lu and
                  Yuanwei Song and
                  Caleb Chen Cao and
                  Raymond Chi{-}Wing Wong and
                  Haodi Zhang},
	title = {FeVisQA: Free-Form Question Answering over Data Visualizations},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2726--2739},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00205},
	doi = {10.1109/ICDE65448.2025.00205},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SongLSCWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a massive dataset, data visualization (DV) could efficiently express the insights and summaries behind the massive raw data by employing vivid visual representations. To create suitable DVs, users are required to get a comprehensive understanding of the raw data and then transfer their ideas into DVs by composing a suitable and accurate specification in some declarative visualization languages (DVLs, e.g., Vega-Lite). A specification is a JSON object defining the properties of the DVs, like the selected data, the transformations, the visual details, and so on. Due to its complicated grammar and details, DV has quite a steep learning curve, even for data analysts. In this paper, we propose a new task named FeVisQA, referring to Free-form Question Answering over data Visualizations. More specifically,-given a raw dataset, a related DV (in the form of a specification), and a question, FeVisQA aims to predict a textual answer automatically. As a particular case of the general CodeQA (i.e., QA over general programming code like Python and Java) task, FeVisQA enables people to better comprehend data and its DVs by conducting logical reasoning when answering these questions. Since FeVisQA has not been studied in the literature, we first construct a benchmark dataset containing 152 datasets, 14,406 DVs, and 83,890 QA pairs. To tackle this new task, we design a novel neural network named FeVisQANet with advanced multi-modal encoder and adaptive decoder structures, and we also design a novel multi-step framework called VisQA for Multi-modal Large Language Models (MLLMs) based on Retrieval-augmented Generation (RAG) technology. Extensive experiments on our constructed datasets validate the rationale and effectiveness of this proposed FeVisQA task and the proposed model. While research on QA over text and table, machine reading comprehension, and CodeQA develops rapidly, prior works have yet to draw attention to question-answering over DVs. This study connects two important subareas, QA from the natural language process area and DV from the data engineering area. We hope this new dataset and model can serve as a helpful benchmark that would benefit the development of both fields.}
}


@inproceedings{DBLP:conf/icde/DaiDCZTLF25,
	author = {Yi Dai and
                  Yang Ding and
                  Lei Cao and
                  Kaisheng Zeng and
                  Junrui Tian and
                  Zexi Lin and
                  Ling Feng},
	title = {Interpretable Video based Stress Detection with Self-Refine Chain
                  Reasoning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2740--2754},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00206},
	doi = {10.1109/ICDE65448.2025.00206},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DaiDCZTLF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stress detection is critical for mental and physical well-being, yet traditional methods such as self-reports and physiological sensors face limitations in efficiency and scalability. Video-based stress detection, leveraging visual cues learned from an annotated video database, offers a non-invasive, cost-effective alternative. However, most models function as black boxes, lacking transparency in their decision-making process, which hinders their trustworthiness. To address this, we propose an interpretable video-based stress detection model that incorporates Chain-of-Thought (CoT) reasoning of large foundation models. Our model follows a structured reasoning chain “Describes Assess-e-Highlight”, mimicking the decision process of psychology experts. To further enhance model reliability, we integrate a self-refinement mechanism that allows the model to reflect on and improve its predictions using Direct Preference Optimization (DPO) to ensure accuracy and faithfulness. Experimental results on two video-based stress detection datasets demonstrate that our approach outperforms state-of-the-art models in both accuracy and interpretability. We release our code at https://github.com/debby1103/stressdetection.git.}
}


@inproceedings{DBLP:conf/icde/PangZOC25,
	author = {Yue Pang and
                  Lei Zou and
                  M. Tamer {\"{O}}zsu and
                  Jiaqi Chen},
	title = {Efficient Execution of {SPARQL} Queries with {OPTIONAL} and {UNION}
                  Expressions},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2755--2767},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00207},
	doi = {10.1109/ICDE65448.2025.00207},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PangZOC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of RDF datasets has resulted in studies focusing on optimizing SPARQL query processing. Most existing work focuses on basic graph patterns (BGPs) and ignores other vital operators in SPARQL, such as UNION and OPTIONAL. SPARQL queries with these operators, which we abbreviate as SPARQL-UO, pose serious query planning challenges. In this paper, we propose techniques for optimizing SPARQL-UO queries using BGP execution as a building block, based on a novel BGP-based Evaluation (BE)-Tree representation of query plans. On top of this, we propose a series of cost-driven BE-tree transformations to generate more efficient plans by reducing the search space and intermediate result sizes, and a candidate pruning technique that further enhances efficiency at query time. Experiments confirm that our method outperforms the state-of-the-art by orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/WangCCL25,
	author = {Jiayi Wang and
                  Lei Cao and
                  Chengliang Chai and
                  Guoliang Li},
	title = {Federated Data Analytics with Differentially Private Density Estimation
                  Model},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2768--2781},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00208},
	doi = {10.1109/ICDE65448.2025.00208},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangCCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated data analytics, aimed at extracting in-sights from decentralized private data while preserving privacy, is crucial for organizations holding sensitive data. Existing approaches, such as output perturbation that adds noise to query results based on differential privacy, often suffer from degraded accuracy due to cumulative privacy budget consumption. In this paper, we introduce ADAPT, a novel framework that addresses this problem by training a privacy-preserving density model over decentralized data. Unlike traditional methods, ADAPT avoids accessing raw data when answering queries, thereby avoiding additional privacy leakage. We tackle the technical challenges raised by privacy-preserving federated data analytics, including parameter misalignment and distribution discrepancy, through innovative techniques of pre-alignment of network parameters and fine-tuning towards accurate data distributions. Directly using the density model, ADAPT accurately infers the results of a wide range of analytical queries. Extensive experiments demonstrate that ADAPT outperforms existing methods in terms of accuracy. Notably, for answering 8,000 analytical queries, ADAPT reduces the median relative error from over 103 to less than 6%. Moreover, it achieves high accuracy comparable to centralized differential privacy training, demonstrating its effectiveness in practical federated data analytics scenarios.}
}


@inproceedings{DBLP:conf/icde/BernardiniCCGGLPP25,
	author = {Giulia Bernardini and
                  Huiping Chen and
                  Alessio Conte and
                  Roberto Grossi and
                  Veronica Guerrini and
                  Grigorios Loukides and
                  Nadia Pisanti and
                  Solon P. Pissis},
	title = {Indexing Strings with Utilities},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2782--2795},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00209},
	doi = {10.1109/ICDE65448.2025.00209},
	timestamp = {Wed, 29 Oct 2025 17:44:08 +0100},
	biburl = {https://dblp.org/rec/conf/icde/BernardiniCCGGLPP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Applications in domains ranging from bioinformatics to advertising feature strings (sequences of letters over some alphabet) that come with numerical scores (utilities). The utilities quantify the importance, interest, profit, or risk of the letters occurring at every position of a string. For instance, DNA fragments generated by sequencing machines come with a confidence score per position. Motivated by the ever-increasing rate of generating such data, as well as by their importance in several domains, we introduce Useful String Indexing (USI), a natural generalization of the classic String Indexing problem. Given a string $S$ (the text) of length $n$, USI asks for preprocessing $S$ into a compact data structure supporting the following queries efficiently: given a shorter string $P$ (the pattern), return the global utility $U(P)$ of $P$ in $S$, where $U$ is a function that maps any string $P$ to a utility score based on the utilities of the letters of every occurrence of $P$ in $S$. Our work also makes the following contributions: (1) We propose a novel and efficient data structure for USI based on finding the top- $K$ frequent substrings of $S$. (2) We propose a linear-space data structure that can be used to mine the top- $K$ frequent substrings of $S$ or to tune the parameters of the USI data structure. (3) We propose a novel space-efficient algorithm for estimating the set of the top- $K$ frequent substrings of $S$, thus improving the construction space of the data structure for USI. (4) We show that popular space-efficient top- $K$ frequent item mining strategies employed by state-of-the-art algorithms do not smoothly translate from items to substrings. (5) Using billion-letter datasets, we experimentally demonstrate that: (i) our top- $K$ frequent substring mining algorithms are accurate and scalable, unlike two state-of-the-art methods; and (ii) our USI data structures are up to 15 times faster in querying than 4 nontrivial baselines while occupying the same space with them.}
}


@inproceedings{DBLP:conf/icde/MoonLK25,
	author = {Jihoon Moon and
                  Ling Liu and
                  Hyuk{-}Yoon Kwon},
	title = {FedSDP: Federated Self-Derived Prototypes for Personalized Federated
                  Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2796--2809},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00210},
	doi = {10.1109/ICDE65448.2025.00210},
	timestamp = {Wed, 10 Dec 2025 08:08:39 +0100},
	biburl = {https://dblp.org/rec/conf/icde/MoonLK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a privacy-preserving machine learning algorithm that enables multiple clients to collaborate. To respond to non-independent and identically distributed (non-IID) environments between clients, personalized FL (PFL) has been actively investigated. The typical PFL model consists of two parts: 1) the head (i.e., classifier) for the final classification and 2) the body (i.e., feature extractor) for extracting representations from local datasets. The head is maintained separately in each client for personalization; the body is aggregated for generalization. FedSDP introduces a bridge layer, called a personalized layer, between the head and the body to preserve individual, non-shared local prototypes for each client. A personalized layer decouples the body and head, strengthening the generalization and personalization, respectively. Based on this architecture, this study proposes a new PFL framework, Federated Self-Derived Prototypes (FedSDP), to dynamically balance personalization and generalization. To this end, we introduce two dynamic adjustments for generating self-derived prototypes: 1) global-local similarity weight (GL-Sim Weight) and 2) personalization early stopping indicator (P-Stop Indicator). GL-Sim Weight based on the similarity between the global and local prototypes is utilized to adjust the degree of personalization of each local model. PStop Indicator is calculated based on the changed degree of local parameters in each client, determining the early stopping for personalization in the client and further concentrating on generalization. Our comprehensive experiments demonstrate that FedSDP outperforms existing state-of-the-art FL frameworks, showing superior effectiveness in non-IID settings. Our code and data are available at https://github.com/bigbases/FedSDP.}
}


@inproceedings{DBLP:conf/icde/ZhouYFLHXYDQJ25,
	author = {Xiaokai Zhou and
                  Xiao Yan and
                  Fangcheng Fu and
                  Xinyan Li and
                  Hao Huang and
                  Quanqing Xu and
                  Chuanhui Yang and
                  Bo Du and
                  Tieyun Qian and
                  Jiawei Jiang},
	title = {Hounding Data Diversity: Towards Participant Selection in Vertical
                  Federated Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2810--2823},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00211},
	doi = {10.1109/ICDE65448.2025.00211},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhouYFLHXYDQJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the rising concerns on privacy protection, how to build machine learning models from distributed databases with privacy guarantees has gained more popularity. Vertical federated learning (VFL) trains machine learning models in a privacy-preserving way when the data features are scattered over distributed databases. We study the participant selection problem (PSP) for VFL, which chooses a given number of participants to conduct training while maximizing model accuracy. Compared to training with all participants, PSP can filter out hitch-riders that contribute marginally to model quality and reduce training time by involving fewer participants. To achieve good model accuracy, we formulate PSP as choosing a set of participants that maximizes the likelihood of the data samples. Then, utilizing the k-nearest neighbors (KNN) classifier as the proxy model, we express the likelihood as a function of the selected participants and prove that the function is sub modular. The submodular property is favorable as it can account for the feature diversity among the participants and allows to greedily select the participant with the maximum gain in each step. However, the selection process requires finding the top-k neighbors of a data sample as the basic operation, which is expensive in VFL setting as it involves encrypted communication. As such, we adapt the Fagin's algorithm, a famous top-k query algorithm, to reduce the amount of encrypted communication. We deploy our solution VFPS-SM across five distributed nodes and conduct experiments with 10 datasets and 3 models to evaluate its performance. The results show that VFPS-SM can reduce the end-to-end running time by up to  35\\times 35\\times , selection time  365\\times 365\\times  and improve model accuracy by 6.0% compared with state-of-the-art baselines.}
}


@inproceedings{DBLP:conf/icde/ZhaoMRAA25,
	author = {Fuheng Zhao and
                  Zach Miller and
                  Leron Reznikov and
                  Divyakant Agrawal and
                  Amr El Abbadi},
	title = {Autumn: {A} Scalable Read Optimized LSM-Tree Based Key-Value Stores
                  with Fast Point and Range Reads},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2824--2837},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00212},
	doi = {10.1109/ICDE65448.2025.00212},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoMRAA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log Structured Merge Trees (LSM-tree) based key-value stores are widely used in many storage systems to support a variety of operations such as updates, point reads, and range reads. Traditionally, the merge policy of LSM-trees organizes data into multiple levels of exponentially increasing capacity to support high-speed writes. However, we contend that the traditional merge policies are not optimized for reads. In this work, we present Autumn, a scalable and read-optimized LSM-tree based key-value store with near-optimal worst-case point and range read costs. The key idea in improving read performance is to dynamically adjust the capacity ratio between two adjacent levels as more data are stored. As a result, lower levels gradually increase their capacities and more actively merges. In particular, point and range read cost improves from the previous known O(logN) complexity to  O(\\sqrt{logN}) O ( l o g N − − − − √ ) O(\\sqrt{logN})  in Autumn by applying the novel Garnering merge policy. While the Garnering merge policy optimizes for both point reads and range reads, it maintains high performance for writes by inherently prioritizing the merges in the lower levels, as Garnering schedules more merges for the lower levels. We implemented Autumn on top of RocksDB and LevelDB and experimentally show the gain in performance for real-world workloads.}
}


@inproceedings{DBLP:conf/icde/ZengLDFZZ25,
	author = {Ximu Zeng and
                  Jianxing Lin and
                  Liwei Deng and
                  Yuchen Fang and
                  Yan Zhao and
                  Kai Zheng},
	title = {Optimizing Multi-Center Collaboration for Task Assignment in Spatial
                  Crowdsourcing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2838--2851},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00213},
	doi = {10.1109/ICDE65448.2025.00213},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZengLDFZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of smart devices has fostered the growth of Spatial Crowdsourcing (SC), where workers complete spatial tasks by traveling to specific locations. Task assignment is a key issue in SC due to the inherent complexity of matching workers with these spatial tasks efficiently. Previous studies on task assignment have primarily focused on optimizing worker-task matching within a single, centralized area, often ignoring scenarios that involve multiple independent service centers across an area. To address this gap, we introduce a collaborative multi-center task assignment problem, which focuses on scenarios where an SC platform manages multiple independent service centers within an area, shifting the focus from worker-level cooperation to exploring the solutions specific to multi-center coordination. We target the imbalances between available workers and unassigned tasks among different centers, aiming to maximize the total number of assigned tasks and minimize unfairness in inter-center collaboration. In particular, we propose an Iterative Multi-center Task Assignment and Optimization (IMTAO) framework. IMTAO operates in two phases: (1) center-independent task assignment based on an efficient sequential task assignment algorithm, and (2) inter-center workforce transfer based on a game-theoretic multi-center collaboration algorithm that ensures fair collaboration through bi-directional optimization. Extensive experiments demonstrate the efficiency and effectiveness of IMTAO in enhancing task assignment and improving collaboration fairness compared to baseline methods.}
}


@inproceedings{DBLP:conf/icde/GeLMYGCLG25,
	author = {Yuhang Ge and
                  Fengyu Li and
                  Yuren Mao and
                  Yanbo Yang and
                  Congcong Ge and
                  Zhaoqiang Chen and
                  Jiang Long and
                  Yunjun Gao},
	title = {KnowTrans: Boosting Transferability of Data Preparation LLMs via Knowledge
                  Augmentation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2852--2866},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00214},
	doi = {10.1109/ICDE65448.2025.00214},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GeLMYGCLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data Preparation (DP), which involves tasks such as data cleaning, imputation and integration, is a fundamental process in data-driven applications. Recently, Large Language Models (LLMs) fine-tuned for DP tasks, i.e., DP-LLMs, have achieved state-of-the-art performance. However, transferring DP-LLMs to novel datasets and tasks typically requires a substantial amount of labeled data, which is impractical in many real-world scenarios. To address this, we propose a knowledge augmentation framework for data preparation, dubbed KNOWTRANS. This framework allows DP-LLMs to be transferred to novel datasets and tasks with a few data points, significantly decreasing the dependence on extensive labeled data. KNOWTRANS comprises two components: Selective Knowledge Concentration and Automatic Knowledge Bridging. The first component re-uses knowledge from previously learned tasks, while the second automatically integrates additional knowledge from external sources. Extensive experiments on 13 datasets demonstrate the effectiveness of KNOWTRANS. KNOWTRANS boosts the performance of the state-of-the-art DP-LLM, Jellyfish-7B, by an average of 4.93%, enabling it to outperform both GPT-4 and GPT-4o.}
}


@inproceedings{DBLP:conf/icde/XuTC25,
	author = {Weiqin Xu and
                  Riccardo Tommasini and
                  Olivier Cur{\'{e}}},
	title = {No Rule is Forever: Datalog Reasoning with Rule Amendments},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2867--2879},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00215},
	doi = {10.1109/ICDE65448.2025.00215},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XuTC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Datalog has grown in popularity with its integration into various commercial and open-source systems. One significant application of Datalog is in stream reasoning scenarios, particularly in the context of Internet of Things and Edge Computing, where it supports the derivation of implicit consequences from incomplete streaming data. The dynamic nature of edge infrastructure, with constantly changing data and deduction rules, poses additional challenges for incremental reasoning. This paper introduces Zodiac, a method for reasoning under rule amendments, and ZodiacEdge, a system implementing this method. Zodiac is based on Datalog stratification and semi-naive evaluation but includes a novel data structure that supports incremental maintenance of deductions for fast rule amendments, even in the presence of negation and aggregation. ZodiacEdge is designed to work with RDF data, accepting Datalog programs with predicates of a maximum arity of two.}
}


@inproceedings{DBLP:conf/icde/PearceMP25,
	author = {Jack Pearce and
                  Hubert Mohr{-}Daurat and
                  Holger Pirk},
	title = {White-Box Micro-Adaptive Query Processing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2880--2893},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00216},
	doi = {10.1109/ICDE65448.2025.00216},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PearceMP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Operator performance in in-memory data management systems (DMS) often suffers from micro-architectural hazards such as cache misses and branch mispredictions. While many operators have alternative implementations that are robust against such hazards, these generally perform worse when no hazards are encountered. Unfortunately, hazards are caused by order-dependent data characteristics that query optimizers struggle to capture (e.g., sortedness, clusteredness) making a priori hazard-conscious optimization difficult. Additionally, statically optimized plans fail to adapt when data characteristics vary within a table. To address these problems, we propose a hazardadaptive approach to query execution. Through hardwareassisted runtime profiling of low-level metrics, operators dynamically adapt to “hazardous” data. We propose an architecture for hazard-adaptive operators and integrate our approach into a DMS. We demonstrate that using hazard-adaptive operators provides a  \\sim \\mathbf{2-20} \\times ∼ 2 − 20 × \\sim \\mathbf{2-20} \\times  speedup across several TPC-H queries.}
}


@inproceedings{DBLP:conf/icde/HeFLG25,
	author = {Changhao He and
                  Ziquan Fang and
                  Linsen Li and
                  Yunjun Gao},
	title = {TrajEdge: An Efficient and Lightweight Trajectory Data Analysis Framework
                  in Edge Environments},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2894--2907},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00217},
	doi = {10.1109/ICDE65448.2025.00217},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HeFLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory data analysis benefits numerous real-world applications and has attracted substantial attention from the research community. With the rapid proliferation of IoT devices and the emergence of edge computing, there has been an increasing demand for efficient trajectory data analytics in edge environments. However, most existing trajectory analysis systems are designed for cloud-based architectures, which face significant limitations in edge settings. These include resource constraints, dynamic network conditions, and inefficient query handling, leading to sub-optimal performance in edge scenarios. To fill this gap, we propose TrajEdge, an efficient and lightweight framework for trajectory data analysis in edge environments. Implementing TrajEdge requires overcoming obstacles posed by limited resources and the dynamic nature of edge networks. To achieve this, we design a novel trajectory composite compression algorithm that delivers high compression ratios, significantly reducing storage pressure on edge devices. Additionally, we introduce three coflow control strategies optimized for varying network conditions, enabling higher system throughput. To further enhance the efficiency of trajectory queries, we develop a spatiotemporal-aware trie-based peer-to-peer (P2P) index. Experimental evaluations on two real-world datasets and one larger synthetic dataset demonstrate that TrajEdge achieves remarkable performance improvements: more than 200 × gains in storage and query efficiency, up to 64% increases in network throughput, compression ratios of up to 95%, and exceptional scalability compared to the state-of-the-art systems. Our source code is available at https://github.com/ZJU-DAILY/TrajEdge.}
}


@inproceedings{DBLP:conf/icde/SuZZSALBC25,
	author = {Can Su and
                  Haipeng Zhang and
                  Hanyu Zhao and
                  Wenting Shen and
                  Baole Ai and
                  Yong Li and
                  Kaigui Bian and
                  Bin Cui},
	title = {CaliEX: {A} Disk-Based Large-Scale {GNN} Training System with Joint
                  Design of Caching and Execution},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2908--2921},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00218},
	doi = {10.1109/ICDE65448.2025.00218},
	timestamp = {Thu, 04 Dec 2025 17:15:24 +0100},
	biburl = {https://dblp.org/rec/conf/icde/SuZZSALBC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) have proven to be powerful tools for learning from graph-structured data and have achieved great success in many applications. As the sizes of real-world graphs continue to grow, traditional GNN training methods face significant scalability challenges. Recently, disks have gained attention as a cost-effective solution to store large-scale graphs, and several disk-based GNN systems have been proposed to train large-scale graphs on a single machine. However, these systems either overlook the unique data characteristics of GNN workloads when designing cache plans or fail to fully exploit the multilevel hierarchy of storage and computation in system execution, thus resulting in disk I/O bottleneck and resource under-utilization. To address these issues, we present CaliEX, an advanced disk-based GNN system that employs joint optimizations of caching and execution within and across different training stages. CaliEX first designs tailored cache plans and execution policy for both graph topology and features to accelerate neighborhood sampling and feature gathering. Since these two training stages work on different types of data, CaliEX further auto-tunes the cache allocation and pipelines the execution across different stages to improve resource utilization and overall training throughput. Evaluations on multiple GNN models and various large-scale datasets show that CaliEX achieves 3.28 × speedup on average compared to existing disk-based GNN training systems.}
}


@inproceedings{DBLP:conf/icde/WeiTZHX25,
	author = {Shuyue Wei and
                  Yongxin Tong and
                  Zimu Zhou and
                  Tianran He and
                  Yi Xu},
	title = {Efficient Data Valuation Approximation in Federated Learning: {A}
                  Sampling-Based Approach},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2922--2934},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00219},
	doi = {10.1109/ICDE65448.2025.00219},
	timestamp = {Mon, 15 Dec 2025 14:16:47 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WeiTZHX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged as a prominent distributed learning paradigm to utilize datasets across multiple data providers. In FL, cross-silo data providers often hesitate to share their high-quality dataset unless their data value can be fairly assessed. Shapley value (SV) has been advocated as the standard metric for data valuation in FL due to its desirable properties. However, the computational overhead of SV is prohibitive in practice, as it inherently requires training and evaluating an FL model across an exponential number of dataset combinations. Furthermore, existing solutions fail to achieve high accuracy and efficiency, making practical use of SV still out of reach, because they ignore choosing suitable computation scheme for approximation framework and overlook the property of utility function in FL. We first propose a unified stratified-sampling framework for two widely-used schemes. Then, we analyze and choose the more promising scheme under the FL linear regression assumption. After that, we identify a phenomenon termed key combinations, where only limited dataset combinations have a high-impact on final data value. Building on these insights, we propose a practical approximation algorithm, IPSS, which strategically selects high-impact dataset combinations rather than evaluating all possible combinations, thus substantially reducing time cost with minor approximation error. Furthermore, we conduct extensive evaluations on the FL benchmark datasets to demonstrate that our proposed algorithm outperforms a series of representative baselines in terms of efficiency and effectiveness.}
}


@inproceedings{DBLP:conf/icde/LiuZ25,
	author = {Aoyu Liu and
                  Yaying Zhang},
	title = {CrossST: An Efficient Pre-Training Framework for Cross-District Pattern
                  Generalization in Urban Spatio-Temporal Forecasting},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2935--2948},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00220},
	doi = {10.1109/ICDE65448.2025.00220},
	timestamp = {Thu, 25 Dec 2025 12:47:48 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiuZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Urban spatio-temporal forecasting is critical for modern urban governance, especially in traffic management, resource planning, and emergency response. Despite advancements in pre-trained models for natural language processing, challenges persist in urban spatio-temporal forecasting. Existing methods struggle to identify and generalize universal cross-district spatio-temporal patterns, while computational limitations hinder the extraction of complex patterns from large-scale data. In this study, we propose CrossST, an efficient pre-training framework designed to capture universal spatio-temporal patterns across large-scale, cross-district scenarios. Specifically, CrossST performs pre-training on various large-scale spatio-temporal datasets to learn and store diverse valuable patterns in its pattern bank. It captures temporal dependencies, including periodicity and trends, through frequency domain and time domain analysis, while leveraging graph attention mechanisms to identify dynamic spatial propagation patterns. During fine-tuning, a spatio-temporal disentanglement strategy separates universal patterns from diverse spatio-temporal patterns stored during pre-training, improving generalization to downstream tasks and enabling efficient cross-district knowledge transfer. Additionally, temporal information aggregation and spatial linear optimization strategies enhance CrossST's efficiency and scalability, significantly reducing computational costs. Extensive experiments demonstrate that CrossST outperforms state-of-the-art baselines, improving downstream task generalization while maintaining low computational overhead. The datasets and code are available at https://github.com/Aoyu-Liu/CrossST.}
}


@inproceedings{DBLP:conf/icde/WuYZMNXZY25,
	author = {Yangyang Wu and
                  Chen Yang and
                  Mengying Zhu and
                  Xiaoye Miao and
                  Wei Ni and
                  Meng Xi and
                  Xinkui Zhao and
                  Jianwei Yin},
	title = {A Zero-Training Error Correction System with Large Language Models},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2949--2962},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00221},
	doi = {10.1109/ICDE65448.2025.00221},
	timestamp = {Thu, 25 Dec 2025 12:47:48 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WuYZMNXZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Correcting missing or erroneous data values is an essential task in data cleaning. Traditional pre-configuration error correction (EC) methods rely heavily on predefined rules or constraints, demanding significant domain knowledge and manual effort. While configuration-free EC approaches have been explored, they still demand extensive feature engineering or labeled data for intensive model training. In this paper, we propose a zero-training and interpretable EC system, named ZeroEC, that leverages large language models (LLMs) to generate chain-of-thoughts (CoTs) and correction rules for EC, without the need for model training. ZeroEC consists of two modules, contextual-relevant tuple search (CTS) and training-free explainable correction (TEC). CTS constructs a contextual-relevant tuple retriever using a weighted cosine similarity function to efficiently identify the most relevant tuples for each dirty tuple, reducing redundancy in the LLM prompts and lowering computational costs. TEC employs a clustering-based representative tuple sampling strategy to alleviate “hallucination” risk by exposing LLMs to diverse types of data errors. It further prompts for generating correction CoTs for user-corrected representative tuples, as well as prompts for creating correction rules and explainable ECs, which automatically provide explanations for EC, all without the need for model training. Extensive experiments conducted on various real-world datasets demonstrate that ZeroEC achieves a 66.82% increase in accuracy and a 6.87x speedup compared to state-of-the-art methods. The codes and datasets of this paper are available at https://github.com/YangChen32768/ZeroEC.}
}


@inproceedings{DBLP:conf/icde/ZhaoLGTFW25,
	author = {Ze Zhao and
                  Bin Lu and
                  Xiaoying Gan and
                  Gu Tang and
                  Luoyi Fu and
                  Xinbing Wang},
	title = {ChainsFormer: Numerical Reasoning on Knowledge Graphs From a Chain
                  Perspective},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2963--2976},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00222},
	doi = {10.1109/ICDE65448.2025.00222},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoLGTFW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reasoning over Knowledge Graphs (KGs) plays a pivotal role in knowledge graph completion or question answering systems, providing richer and more accurate triples and attributes. As numerical attributes become increasingly essential in characterizing entities and relations in KGs, the ability to reason over these attributes has gained significant importance. Existing graph-based methods such as Graph Neural Networks (GNNs) and Knowledge Graph Embeddings (KGEs), primarily focus on aggregating homogeneous local neighbors and implicitly embedding diverse triples. However, these approaches often fail to fully leverage the potential of logical paths within the graph, limiting their effectiveness in exploiting the reasoning process. To address these limitations, we propose ChainsFormer, a novel chain-based framework designed to support numerical reasoning. Chainsformer not only explicitly constructs logical chains but also expands the reasoning depth to multiple hops. Specially, we introduces Relation-Attribute Chains (RA-Chains), a specialized logic chain, to model sequential reasoning patterns. ChainsFormer captures the step-by-step nature of multi-hop reasoning along RA-Chains by employing sequential in-context learning. To mitigate the impact of noisy chains, we propose a hyperbolic affinity scoring mechanism that selects relevant logic chains in a variable-resolution space. Furthermore, ChainsFormer incorporates an attention-based numerical reasoner to identify critical reasoning paths, enhancing both reasoning accuracy and transparency. Experimental results demonstrate that ChainsFormer significantly outperforms state-of-the-art methods, achieving up to a 20.0% improvement in performance. The implementations are available at https://github.com/zhaodazhuang2333/ChainsFormer.}
}


@inproceedings{DBLP:conf/icde/ZhuDYWCLLGNFFZ25,
	author = {Guoying Zhu and
                  Haipeng Dai and
                  Kang Yuan and
                  Qian Wang and
                  Lida Chen and
                  Zhenghong Luo and
                  Meng Li and
                  Rong Gu and
                  Xizi Ni and
                  Hua Fan and
                  Dachao Fu and
                  Wenchao Zhou},
	title = {Local-to-Cloud Database Synchronization via Fine-Grained Hybrid Compression},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2977--2989},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00223},
	doi = {10.1109/ICDE65448.2025.00223},
	timestamp = {Thu, 23 Oct 2025 23:00:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhuDYWCLLGNFFZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing migration of business operations to the cloud, cloud service providers are facing a growing demand for faster database synchronization across diverse network conditions. Thus, compression methods are predominantly employed over the synchronized database binlog files to reduce the volume of data to be transmitted across the network. However, previous solutions typically rely on using one single compression method. This can result in data compression rates failing to align well with network bandwidth, causing data to wait for compression or transmission, thereby leading to inferior performance. To address the above issues, we propose a fine-grained hybrid adaptive compression system that (1) parses binlog files into multiple fine-grained blocks, and (2) applies a hybrid combination of multiple compression methods to seamlessly align compression speed with the network bandwidth. We have conducted extensive evaluations which demonstrate that, compared to the cutting-edge compression methods like ZSTD, LZ4, and Snappy, our approach can cut down the average latency by 66% and improve the synchronization throughput by 2.45×.}
}


@inproceedings{DBLP:conf/icde/JingCZYDPW25,
	author = {Xuyang Jing and
                  Qinghua Cao and
                  Chenhao Zhang and
                  Zheng Yan and
                  Wenxiu Ding and
                  Witold Pedrycz and
                  Pu Wang},
	title = {TardySketch: {A} Framework for Cardinality Estimation Adaptable to
                  Sliding Windows},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {2990--3002},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00224},
	doi = {10.1109/ICDE65448.2025.00224},
	timestamp = {Wed, 10 Sep 2025 14:09:49 +0200},
	biburl = {https://dblp.org/rec/conf/icde/JingCZYDPW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sliding cardinality estimation is crucial in many data analysis scenarios, e.g., detecting abnormal network behav-iors by monitoring unique connections in real time, detecting fraud in online transactions by monitoring unique user behavior patterns, and improving inventory management in supply chains by analyzing unique buyer behaviors. However, existing sliding cardinality estimation methods suffer from a cardinality barrel-down problem caused by unexpired item elimination in advance and item excessive removal, which remains unresolved so far. In this paper, we propose TardySketch, a sketch framework to make sliding cardinality estimation accurate and efficient by solving the above problem. The cornerstone of TardySketch is a Bidirectional Pointer-based Bitmap (BP-Bitmap), which stores the arrival sequence of items without timestamps. To prevent the premature elimination of unexpired items, we propose a Gap mechanism to enhance the accuracy of BP-Bitmap for identifying truly expired items through intermittent monitoring. To ensure an appropriate number of items are eliminated as the window moves, we design a Slow-Down mechanism to slacken the reset rate of bucket in BP- Bitmap to prevent over removal of items. Experimental results based on real-world datasets demonstrate that TardySketch significantly outperforms state-of-the-art methods, achieving a performance improvement of 5–40 times. The source code of TardySketch is available on GitHub.}
}


@inproceedings{DBLP:conf/icde/NiuLKW25,
	author = {Yudong Niu and
                  Yuchen Li and
                  Panagiotis Karras and
                  Yanhao Wang},
	title = {A Sketch Propagation Framework for Hub Queries on Unmaterialized Relational
                  Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3003--3016},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00225},
	doi = {10.1109/ICDE65448.2025.00225},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/NiuLKW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relational graphs encapsulate nontrivial inherent interactions among entities in heterogeneous data sources. Iden-tifying hubs in relational graphs is vital in various applications such as fraud detection, influence analysis, and protein complex discovery. However, building relational graphs induced by meta-paths on heterogeneous data entails substantial costs, thus hin-dering efficient hub discovery. In this paper, we propose a novel sketch propagation framework for approximate hub queries in induced relational graphs that avoids explicitly materializing those graphs. Our framework specifically supports hub queries that ask for all nodes whose centrality scores, based on degree or h-index, are in the top quantile with provable guarantees under the notion of ∊-separable sets. In addition, we devise pruning techniques that efficiently process personalized hub queries asking whether a given node is a hub. Extensive experiments on real-world and synthetic data confirm the efficacy and efficiency of our proposals, which achieve orders of magnitude speed-ups over exact methods while consistently attaining accuracy beyond 90%.}
}


@inproceedings{DBLP:conf/icde/LiuZXLYC25,
	author = {Yingfan Liu and
                  Yandi Zhang and
                  Jiadong Xie and
                  Hui Li and
                  Jeffrey Xu Yu and
                  Jiangtao Cui},
	title = {Privacy-Preserving Approximate Nearest Neighbor Search on High-Dimensional
                  Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3017--3029},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00226},
	doi = {10.1109/ICDE65448.2025.00226},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuZXLYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the era of cloud computing and AI, data owners outsource ubiquitous vectors to the cloud, which furnish approx-imate k-nearest neighbors (k-ANNS) services to users. To protect data privacy against the untrusted server, privacy-preserving k-ANNS (PP-ANNS) on vectors has been a fundamental and urgent problem. However, existing PP-ANNS solutions fall short of meeting the requirements of data privacy, efficiency, accuracy, and minimal user involvement concurrently. To tackle this challenge, we introduce a novel solution that primarily executes PP-ANNS on a single cloud server to avoid the heavy communication overhead between the cloud and the user. To ensure data privacy, we introduce a novel encryption method named distance comparison encryption, facilitating secure, efficient, and exact distance comparisons. To optimize the trade-off between data privacy and search performance, we design a privacy-preserving index that combines the state-of-the-art k-ANNS method with an approximate distance computation method. Then, we devise a search method using a filter-and - refine strategy based on the index. Moreover, we provide the security analysis of our solution and conduct extensive experiments to demonstrate its superiority over existing solutions. Based on our experimental results, our method accelerates PP-ANNS by up to 3 orders of magnitude compared to state-of-the-art methods, while not compromising the accuracy.}
}


@inproceedings{DBLP:conf/icde/CaoSXWLLZX25,
	author = {Lu Cao and
                  Qilong Shi and
                  Weiqiang Xiao and
                  Nianfu Wang and
                  Wenjun Li and
                  Zhijun Li and
                  Weizhe Zhang and
                  Mingwei Xu},
	title = {Hypersistent Sketch: Enhanced Persistence Estimation via Fast Item
                  Separation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3030--3042},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00227},
	doi = {10.1109/ICDE65448.2025.00227},
	timestamp = {Wed, 05 Nov 2025 07:37:02 +0100},
	biburl = {https://dblp.org/rec/conf/icde/CaoSXWLLZX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient data stream processing, particularly for persistence estimation, is crucial in handling high-velocity data streams characterized by skewed distributions of item frequencies. Unlike more straightforward frequency metrics, persistence captures items' recurrence across multiple time windows, requiring nuanced processing approaches. In response, we introduce the Hypersistent Sketch, an algorithm that significantly enhances persistence estimation through innovative filtering techniques. Our design incorporates a Cold Filter to address the skewed nature of data streams where a few high-frequency (hot) items dominate. This filter allows for differential treatment by using smaller counters for most low-frequency (cold) items, thus conservatively allocating memory resources that would otherwise be sized uniformly based on hot items. However, the Cold Filter can reduce throughput due to its segregative processing. To mitigate this, we implement a Burst Filter, which optimizes the processing of hot items. The Burst Filter significantly improves throughput by preventing repeated insertions within a single window—where persistence increases by at most one—and deferring the insertion until the window's end. Comparative evaluations demonstrate that the Hypersistent Sketch outperforms existing solutions like the On-Off Sketch, offering up to 3 times improved throughput while maintaining competitive accuracy and substantially reducing memory usage in handling large-scale data streams.}
}


@inproceedings{DBLP:conf/icde/WangHTZZ25,
	author = {Yuxiang Wang and
                  Ziyuan He and
                  Yongxin Tong and
                  Zimu Zhou and
                  Yiman Zhong},
	title = {Timestamp Approximate Nearest Neighbor Search Over High-Dimensional
                  Vector Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3043--3055},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00228},
	doi = {10.1109/ICDE65448.2025.00228},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WangHTZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unstructured data, such as images and texts, are increasingly represented as high-dimensional vectors for emerging AI applications like retrieval-augmented generation. A key operation in these applications is querying for vectors that are both semantically similar and temporally relevant. This operation can be formulated as Timestamp Approximate Nearest Neighbor Search (TANNS), where both the vectors and the query incorporate temporal attributes, aiming to retrieve the approximate nearest neighbors valid at the given timestamp. A naive solution is to create separate indexes for each timestamp, which enables accurate and fast searches but incurs high update latency and excessive storage demands. In this paper, we introduce the timestamp graph, a novel structure that supports rapid index updates while minimizing storage costs. Exploiting the temporal locality of changes in valid vectors, our timestamp graph effectively manages a unified index across all historical timestamps, thereby substantially reducing storage overhead. Moreover, we design the historic neighbor tree, which further compresses the space complexity to that of a single-timestamp index. Extensive evaluations on four standard datasets show that our method achieves over 99% accuracy while improving the query efficiency by 4.4× to 138.1× than existing solutions.}
}


@inproceedings{DBLP:conf/icde/NiuDZZT25,
	author = {Yiming Niu and
                  Jinliang Deng and
                  Lulu Zhang and
                  Zimu Zhou and
                  Yongxin Tong},
	title = {Accurate and Efficient Multivariate Time Series Forecasting via Offline
                  Clustering},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3056--3069},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00229},
	doi = {10.1109/ICDE65448.2025.00229},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/NiuDZZT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.}
}


@inproceedings{DBLP:conf/icde/WuWLHZL25,
	author = {Wenlong Wu and
                  Haofen Wang and
                  Bohan Li and
                  Peixuan Huang and
                  Xinzhe Zhao and
                  Lei Liang},
	title = {MultiRAG: {A} Knowledge-Guided Framework for Mitigating Hallucination
                  in Multi-Source Retrieval Augmented Generation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3070--3083},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00230},
	doi = {10.1109/ICDE65448.2025.00230},
	timestamp = {Fri, 12 Dec 2025 08:16:31 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WuWLHZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models (LLMs). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the sparse distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the sparse data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. Our code is available in https://github.com/wuwenlong123/MultiRAG.}
}


@inproceedings{DBLP:conf/icde/WangWLJ25,
	author = {Weicheng Wang and
                  Raymond Chi{-}Wing Wong and
                  Jinyang Li and
                  H. V. Jagadish},
	title = {Interactive Learning for Diverse Top-k Set},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3084--3097},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00231},
	doi = {10.1109/ICDE65448.2025.00231},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangWLJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The top-k query is a representative multi-criteria decision-making operator that assists users in finding the best  k k  tuples based on their criteria. However, it has certain limitations in the query process and the final output. First, the query process requires users to specify their criteria explicitly and accurately in advance, which may be difficult for some users. Second, the final output often lacks diversity, which potentially leads to user dissatisfaction. To address these limitations, in this paper, we propose an enhanced top-k query by incorporating an interactive learning framework and a diversity mechanism, expecting to return a diverse output that aligns with the user's criterion, even if the criterion is not specified in advance. We study our problem progressively. Initially, we examine a special case where tuples are described by two scoring attributes. We present the TDIA algorithm that is asymptotically optimal regarding the user effort needed for interaction. Then, we move on to the general case where tuples are described by multiple scoring attributes. We propose the HDIA algorithm which is asymptotically optimal w.r.t. the number of questions asked in expectation. Experiments were conducted on synthetic and real datasets. The results show that our algorithms can return a diverse output while requiring less user effort than existing ones.}
}


@inproceedings{DBLP:conf/icde/YuanXCMXQ25,
	author = {Long Yuan and
                  Junyue Xu and
                  Zi Chen and
                  Chuan Ma and
                  Jianqiu Xu and
                  Lu Qin},
	title = {Efficient Maximum Balanced k-biplex Search Over Bipartite Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3098--3112},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00232},
	doi = {10.1109/ICDE65448.2025.00232},
	timestamp = {Wed, 10 Sep 2025 14:09:51 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YuanXCMXQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bipartite graphs are widely used to model relationships among diverse entities in domains such as gene co-expression networks, collaboration networks, and customer-product interactions. A fundamental problem in analyzing bipartite graphs is the maximum balanced biclique (MBBC) search, which identifies the maximum fully connected subgraph with an equal number of vertices on both sides in the given bipartite graph. Despite its utility, the MBBC model suffers from practical limitations: its strict all-to-all connectivity and exact size-equality requirements make it impractical for noisy, incomplete real-world bipartite data. To overcome these limitations, we propose the maximum balanced k-biplex (MBKBP) model, which relaxes the stringent requirements of MBBC. In MBKBP, each vertex is allowed to miss up to k neighbors on the opposite side of the bipartite graph, and a user-defined parameter $\\delta$ ensures approximate balance between the two vertex sets. This flexibility enhances robustness to noise, accommodates incomplete data, and broadens the model's applicability. To compute the MBKBP in a given bipartite graph, a baseline approach involves enumerating all maximal balanced k-biplexes and identifying the largest one. However, as confirmed by our experiments, this approach is computationally inefficient. To address this challenge, we introduce the concept of $(z_{L},\\ z_{R})$ search space and propose a new framework to compute the MBKBP. By generating a series of smaller $(z_{L)}z_{R})$ search spaces, our framework significantly reduces the number of maximal k-biplexes that need to be explored. Additionally, we leverage the $\\delta$ -balance property to refine the search spaces further and develop three categories of pruning rules to minimize computational overhead. Extensive experiments on real-world bipartite graphs demonstrate that our algorithm achieves up to three orders of magnitude speedup compared to baseline approache, showcasing its efficiency and practicality for bipartite graph analysis.}
}


@inproceedings{DBLP:conf/icde/LeeLS25,
	author = {Kyuhan Lee and
                  Geon Lee and
                  Kijung Shin},
	title = {{MARIOH:} Multiplicity-Aware Hypergraph Reconstruction},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3113--3125},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00233},
	doi = {10.1109/ICDE65448.2025.00233},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LeeLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hypergraphs offer a powerful framework for modeling higher-order interactions that traditional pairwise graphs cannot fully capture. However, practical constraints often lead to their simplification into projected graphs, resulting in substantial information loss and ambiguity in representing higher-order relationships. In this work, we propose MARIOH, a supervised approach for reconstructing the original hypergraph from its projected graph by leveraging edge multiplicity. To overcome the difficulties posed by the large search space, MARIOH integrates several key ideas: (a) identifying provable size-2 hyperedges, which reduces the candidate search space, (b) predicting the likelihood of candidates being hyperedges by utilizing both structural and multiplicity-related features, and (c) not only targeting promising hyperedge candidates but also examining less confident ones to explore alternative possibilities. Together, these ideas enable MARIOH to efficiently and effectively explore the search space. In our experiments using 10 real-world datasets, MARIOH achieves up to 74.51% higher reconstruction accuracy compared to state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/NiZMZWWY25,
	author = {Wei Ni and
                  Kaihang Zhang and
                  Xiaoye Miao and
                  Xiangyu Zhao and
                  Yangyang Wu and
                  Yaoshu Wang and
                  Jianwei Yin},
	title = {ZeroED: Hybrid Zero-Shot Error Detection Through Large Language Model
                  Reasoning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3126--3139},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00234},
	doi = {10.1109/ICDE65448.2025.00234},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/NiZMZWWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Error detection (ED) in tabular data is crucial yet challenging due to diverse error types and the need for contextual understanding. Traditional ED methods often rely heavily on manual criteria and labels, making them labor-intensive. Large language models (LLM) can minimize human effort but struggle with errors requiring a comprehensive understanding of data context. In this paper, we propose ZeroED, a novel hybrid error detection framework, which combines LLM reasoning ability with the machine learning pipeline via zero-shot prompting. ZeroED operates in four steps, i.e., feature representation, error labeling, training data construction, and detector training. Initially, to enhance error distinction, ZeroED generates rich data representations using LLM-driven error reason-aware binary features, pre-trained embeddings, and statistical features. Then, ZeroED employs LLM to holistically label errors through incontext learning, guided by a two-step LLM reasoning process for detailed ED guidelines. To reduce token costs, LLMs are applied only to representative data selected via clustering-based sampling. High-quality training data is constructed through in-cluster label propagation and LLM augmentation with verification. Finally, a classifier is trained to detect all errors. Extensive experiments on seven datasets demonstrate that, ZeroED outperforms state-of-the-art methods by a maximum 30 % improvement in F1 score and up to 90% token cost reduction.}
}


@inproceedings{DBLP:conf/icde/PengMFZDY25,
	author = {Huanhuan Peng and
                  Xiaoye Miao and
                  Yicheng Fu and
                  Jinshan Zhang and
                  Shuiguang Deng and
                  Jianwei Yin},
	title = {On Scalable Query Pricing in Data Marketplaces},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3140--3152},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00235},
	doi = {10.1109/ICDE65448.2025.00235},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PengMFZDY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query-based pricing enables personalized data acquisition for data buyers, exhibiting potential in data markets. The state-of-the-art SQL query pricing strategy tackles the #P-hard arbitrage-free pricing task with the quadratic computational complexity, far from promptly fulfilling customer demands. In this paper, we propose a novel arbitrage-free and scalable pricing framework ARIA to calculate the prices for various query types in linear time, including select-project-join and simple aggregate (SPJA) queries. For the first time, we model what the query answer tells about the value of each tuple and formulate the tuple-level information of selection, projection, and simple aggregation queries. We develop several price functions based on the total information gain of all tuples. The containing relationship between the query information prevents possible arbitrage arising from query determinacy. We present efficient price computation algorithms to derive the prices of different types of queries with linear time complexity, which scan the common possible value set of tuples one time. In ARIA, the join query is decomposed as multiple single-relation queries for pricing in linear time. Extensive experiments on real and synthetic datasets demonstrate that, ARIA performs 3x faster than the state of the arts while enjoying desirable pricing characteristics.}
}


@inproceedings{DBLP:conf/icde/ZhaoAAMMR25,
	author = {Fuheng Zhao and
                  Divyakant Agrawal and
                  Amr El Abbadi and
                  Claire Mathieu and
                  Ahmed Metwally and
                  Michel de Rougemont},
	title = {The SpaceSaving{\(\pm\)} Family of Algorithms for Data Streams with
                  Bounded Deletions},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3153--3164},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00236},
	doi = {10.1109/ICDE65448.2025.00236},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoAAMMR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present an advanced analysis of near optimal algorithms that use limited space to solve the frequency estimation, heavy hitters, frequent items, and top-k approximation in the bounded deletion model. We define the family of SpaceSaving± algorithms and explain why the original SpaceSaving± algorithm only works when insertions and deletions are not interleaved. Next, we propose the new Double SpaceSaving±, Unbiased Double SpaceSaving±, and Integrated SpaceSaving± algorithms and prove their correctness. The three proposed algorithms represent different trade-offs, in which Double SpaceSaving± can be extended to provide unbiased estimations while Integrated SpaceSaving± uses less space. Since data streams are often skewed, we present an improved analysis of these algorithms and show that errors do not depend on the hot items. We also demonstrate how to achieve relative error guarantees under mild assumptions. Moreover, we establish that the important mergeability property is satisfied by all three algorithms, which is essential for running the algorithms in distributed settings.}
}


@inproceedings{DBLP:conf/icde/LiuMXZLZLZ25,
	author = {Chenxi Liu and
                  Hao Miao and
                  Qianxiong Xu and
                  Shaowen Zhou and
                  Cheng Long and
                  Yan Zhao and
                  Ziyue Li and
                  Rui Zhao},
	title = {Efficient Multivariate Time Series Forecasting via Calibrated Language
                  Models with Privileged Knowledge Distillation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3165--3178},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00237},
	doi = {10.1109/ICDE65448.2025.00237},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuMXZLZLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multivariate time series forecasting (MTSF) endeavors to predict future observations given historical data, playing a crucial role in time series data management systems. With advancements in large language models (LLMs), recent studies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF. However, the deployment of LLMs often suffers from low efficiency during the inference phase. To address this problem, we introduce TimeKD, an efficient MTSF framework that leverages the calibrated language models and privileged knowledge distillation. TimeKD aims to generate high-quality future representations from the proposed cross-modality teacher model and cultivate an effective student model. The cross-modality teacher model adopts calibrated language models (CLMs) with ground truth prompts, motivated by the paradigm of Learning Under Privileged Information (LUPI). In addition, we design a subtractive cross attention (SCA) mechanism to refine these representations. To cultivate an effective student model, we propose an innovative privileged knowledge distillation (PKD) mechanism including correlation and feature distillation. PKD enables the student to replicate the teacher's behavior while minimizing their output discrepancy. Extensive experiments on real data offer insight into the effectiveness, efficiency, and scalability of the proposed TimeKD.}
}


@inproceedings{DBLP:conf/icde/ZhangSMZGT25,
	author = {Jinchuan Zhang and
                  Ming Sun and
                  Chong Mu and
                  Jinhao Zhang and
                  Quanjiang Guo and
                  Ling Tian},
	title = {Historically Relevant Event Structuring for Temporal Knowledge Graph
                  Reasoning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3179--3192},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00238},
	doi = {10.1109/ICDE65448.2025.00238},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangSMZGT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal Knowledge Graph (TKG) reasoning focuses on predicting events through historical information within snapshots distributed on a timeline. Existing studies mainly concentrate on two perspectives of leveraging the history of TKGs, including capturing evolution of each recent snapshot or correlations among global historical facts. Despite the achieved significant accomplishments, these models still fall short of I) investigating the impact of multi-granular interactions across recent snapshots, and II) harnessing the expressive semantics of significant links accorded with queries throughout the entire history, particularly events exerting a profound impact on the future. These inadequacies restrict representation ability to reflect historical dependencies and future trends thoroughly. To overcome these drawbacks, we propose an innovative TKG reasoning approach towards Historically Relevant Events Structuring (HisRES). Concretely, HisRES comprises two distinctive modules excelling in structuring historically relevant events within TKGs, including a multi-granularity evolutionary encoder that captures structural and temporal dependencies of the most recent snapshots, and a global relevance encoder that concentrates on crucial correlations among events relevant to queries from the entire history. Furthermore, HisRES incorporates a self-gating mechanism for adaptively merging multi-granularity recent and historically relevant structuring representations. Extensive experiments on four event-based benchmarks demonstrate the state-of-the-art performance of HisRES and indicate the superiority and effectiveness of structuring historical relevance for TKG reasoning.}
}


@inproceedings{DBLP:conf/icde/WangW25,
	author = {Libin Wang and
                  Raymond Chi{-}Wing Wong},
	title = {{NRP:} An Efficient Index for Stochastic Routing in Road Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3193--3205},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00239},
	doi = {10.1109/ICDE65448.2025.00239},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The pervasiveness of shortest path queries is evident in real life, particularly in online mapping applications. However, in practice, the travel times of road segments can be uncertain due to various reasons, such as traffic congestion, which leads to the shortest path not to be the fastest, resulting in an unreliable path. The Reliable Shortest Path (RSP) query has been developed to fulfill individuals' reliability requirements by considering travel times as random variables. Extensive solutions have been proposed to efficiently find RSPs in stochastic road networks. However, they are either unscalable to large networks or incapable of handling rapid streams of routing queries. In this paper, we propose an efficient index-based solution for RSP queries, called Non-dominated Reliable Path (NRP). It stores partial path answers to support fast query processing and utilizes several tailored pruning techniques that can significantly reduce the query time. Experiments conducted on large city road networks verified the superiority of our solution, which can answer each query in around 100 microseconds and beat competitors by orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/ZhaoLZMZ25,
	author = {Jing Zhao and
                  Lei Li and
                  Mengxuan Zhang and
                  Haolun Ma and
                  Xiaofang Zhou},
	title = {A Just-In-Time Framework for Routing-Oriented Traffic Prediction},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3206--3219},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00240},
	doi = {10.1109/ICDE65448.2025.00240},
	timestamp = {Fri, 05 Dec 2025 07:48:04 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoLZMZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic prediction plays a crucial role in urban transportation systems, yet existing methods face challenges in achieving real-time performance when handling large-scale road networks. This paper introduces a novel Just-In-Time Traffic Prediction framework that integrates traffic condition with routing queries for efficient localized predictions in multi-query urban environments. Unlike traditional approaches that perform global predictions across entire networks, our framework partitions the road network into non-overlapping small regions and selectively updates traffic conditions based on query demands. Specifically, we propose three key components: (i) a Search Space Estimation (SSE) model that reformulates search space determination of routing queries as a binary classification task to accurately identify the searched regions; (ii) a Region-based Traffic Speed Prediction (RTSP) model that incorporates the temporal validity of speed profiles in adjacent regions and comprehensive spatio-temporal features for precise region-based traffic prediction; (iii) a Global Region Prediction Scheduling that efficiently coordinates the SSE and RTSP models to maintain up-to-date traffic data for running queries while minimizing computational overhead from both spatio and temporal dimensions. Experimental results on real-world road networks demonstrate significant improvements in both effectiveness and efficiency compared to state-of-the-arts.}
}


@inproceedings{DBLP:conf/icde/XiaZ25,
	author = {Haisong Xia and
                  Zhongzhi Zhang},
	title = {Fast Maximization of Current Flow Group Closeness Centrality},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3220--3233},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00241},
	doi = {10.1109/ICDE65448.2025.00241},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XiaZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Derived from effective resistances, the current flow closeness centrality (CFCC) for a group of nodes measures the importance of node groups in an undirected graph with  n n  nodes. Given the widespread applications of identifying crucial nodes, we investigate the problem of maximizing CFCC for a node group  S S  subject to the cardinality constraint  \\vert S \\vert =k<<n \\vert S \\vert =k<<n . Despite the proven NP-hardness of this problem, we propose two novel greedy algorithms for its solution. Our algorithms are based on spanning forest sampling and Schur complement, which exhibit nearly linear time complexities and achieve an approximation factor of 1- k/k-1 -∊ for any 0 < ∊ < 1. Extensive experiments on real-world graphs illustrate that our algorithms outperform the state-of-the-art method in terms of efficiency and effectiveness, scaling to graphs with millions of nodes.}
}


@inproceedings{DBLP:conf/icde/LiZCL25,
	author = {Xiaofan Li and
                  Rui Zhou and
                  Lu Chen and
                  Chengfei Liu},
	title = {Clique Comparator: {A} Fundamental Operator for Finding a Concise
                  Clique Summary},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3248--3260},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00243},
	doi = {10.1109/ICDE65448.2025.00243},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiZCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Maximal cliques are useful in many applications, e.g., community detection, bioinformatics, anomaly detection and graph visualization. Enumerating maximal cliques from a graph is a computationally challenging problem, as the output size can be exponentially large with respect to the vertex number. Such a large number of cliques typically overlap heavily, which brings information redundancy when being applied to aforementioned domains. This paper studies how to use a small set of maximal cliques, i.e., a summary, to summarize all the maximal cliques in a graph. The state-of-the-art suffers from inefficiency in updating the summary with progressively generated cliques, especially when the summary grows large. In this work, we identify the challenge of summary updating to be how to efficiently estimate the overlap between cliques in the summary and each newly found maximal clique. By exploiting the vertex order information, we propose the notion of clique comparator, and devise four types of operators to quickly identify clique overlap in less costly manners. We conduct extensive experiments on six real-world datasets to verify the effectiveness of our approach, which reduces unnecessary clique intersection calculations by at least seven orders of magnitude and achieves a speedup of 2.5 ~3.1 times compared to the state-of-the-art.}
}


@inproceedings{DBLP:conf/icde/HanTZ25,
	author = {Tingxuan Han and
                  Wei Tong and
                  Sheng Zhong},
	title = {Differentially Private Triangle Counting Assisted by {\textdollar}k{\textdollar}-Anonymity
                  in Two-Party Models},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3261--3273},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00244},
	doi = {10.1109/ICDE65448.2025.00244},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HanTZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Triangle counting is essential for analyzing network structures and optimizing recommendation systems, yet it can lead to privacy breaches if individual data is not adequately protected during the analysis. Differential privacy has become a widely adopted standard to safeguard personal privacy. Current research mainly focuses on central and local models, which differ in applicability and performance. The central model cannot be applied when graph data is distributed among multiple parties without a trusted central server, while the local model provides unsatisfactory performance. In this paper, we explore a two-party scenario where each party holds private information about a group of users and is not allowed to disclose this information to the other party. We have proposed a scheme called HTTC-DPk, which ensures both differential privacy and k-anonymity, and is better suited to the constraints of the two-party setting compared to both central and local models. Our method integrates the noisy maximum degree computation for both intra-party and inter-party edges, along with the differentially private inter-party triangle counting based on two-party interactions. Additionally, we introduce an enhanced scheme HTTC-DPk* that strikes a balance between accuracy and communication costs, particularly suitable for large-scale graphs. We have provided comprehensive theoretical proof of our scheme's privacy and demonstrated through extensive experiments that our approach performs well under various cases.}
}


@inproceedings{DBLP:conf/icde/BaR25,
	author = {Jinsheng Ba and
                  Manuel Rigger},
	title = {Towards a Unified Query Plan Representation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3274--3287},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00245},
	doi = {10.1109/ICDE65448.2025.00245},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BaR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In database systems, a query plan is a series of concrete internal steps to execute a query. Multiple testing approaches utilize query plans for finding bugs. However, query plans are represented in a database-specific manner, so implementing these testing approaches requires a non-trivial effort, hindering their adoption. We envision that a unified query plan representation can facilitate the implementation of these approaches. In this paper, we present an exploratory case study to investigate query plan representations in nine widely-used database systems. Our study shows that query plan representations consist of three conceptual components: operations, properties, and formats, which enable us to design a unified query plan representation. Based on it, existing testing methods can be efficiently adopted, finding 17 previously unknown and unique bugs. Additionally, the unified query plan representation can facilitate other applications. Existing visualization tools can support multiple database systems based on the unified query plan representation with moderate implementation effort, and comparing unified query plans across database systems provides actionable insights to improve their performance. We expect that the unified query plan representation will enable the exploration of additional application scenarios.}
}


@inproceedings{DBLP:conf/icde/XuLZXZ25,
	author = {Zizhuo Xu and
                  Lei Li and
                  Mengxuan Zhang and
                  Yehong Xu and
                  Xiaofang Zhou},
	title = {Ultra-Flexible, Explainable, and Scalable Traffic Prediction with
                  Dynamic Future Routes},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3288--3301},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00246},
	doi = {10.1109/ICDE65448.2025.00246},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XuLZXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic forecasting is essential for intelligent transportation systems, aiming to predict future traffic dynamics such as speed and travel time through the analysis of past observations. However, mainstream deep learning frameworks, which rely heavily on historical data, often struggle in realworld applications due to their inadaptability to dynamic future changes, neglect of future traffic flow as the root cause of traffic conditions, and the complexity of model structures for city-scale road networks. To solve these limitations, we propose a Route Data Management System (RouteSys) that integrates a macroscopic simulation module with lightweight traffic prediction models to estimate the future traffic conditions on individual road segments by accurately and efficiently simulating vehicle travel sequences and traffic states in advance. Additionally, we integrate the microscopic traffic simulation tool SUMO with the custom route planning logic to generate synthetic route data, supporting model training and application evaluation. RouteSys has been validated on real-world road networks in various scenarios, showing substantial improvements in prediction accuracy, efficiency, and scalability compared to the mainstream structures.}
}


@inproceedings{DBLP:conf/icde/ChenLXWL25,
	author = {Xin Chen and
                  Wenqing Lin and
                  Haoxuan Xie and
                  Sibo Wang and
                  Siqiang Luo},
	title = {Finding Near-Optimal Maximum Set of Disjoint {\textdollar}k{\textdollar}-Cliques
                  in Real-World Social Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3316--3328},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00248},
	doi = {10.1109/ICDE65448.2025.00248},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenLXWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A  k k -clique is a dense graph, consisting of  k k  fully-connected nodes, that finds numerous applications, such as community detection and network analysis. In this paper, we study a new problem, that finds a maximum set of disjoint  k k -cliques in a given large real-world graph with a user-defined fixed number  k k , which can contribute to a good performance of teaming collaborative events in online games. However, this problem is NP-hard when  k\\geq 3 k\\geq 3 , making it difficult to solve. To address that, we propose an efficient lightweight method that avoids significant overheads and achieves a  k k -approximation to the optimal, which is equipped with several optimization techniques, including the ordering method, degree estimation in the clique graph, and a lightweight implementation. Besides, to handle dynamic graphs that are widely seen in real-world social networks, we devise an efficient indexing method with careful swapping operations, leading to the efficient maintenance of a near-optimal result with frequent updates in the graph. In various experiments on several large graphs, our proposed approaches significantly outperform the competitors by up to 2 orders of magnitude in running time and 13.3% in the number of computed disjoint  k k -cliques, which demonstrates the superiority of the proposed approaches in terms of efficiency and effectiveness.}
}


@inproceedings{DBLP:conf/icde/ZhangSZSGTL25,
	author = {Kaiqi Zhang and
                  Chengyou Shen and
                  Siyuan Zhang and
                  Shengfei Shi and
                  Hong Gao and
                  Yaofeng Tu and
                  Jianzhong Li},
	title = {Hybrid {DRAM-NVM} R-Trees with Consistency Guarantee},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3329--3341},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00249},
	doi = {10.1109/ICDE65448.2025.00249},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangSZSGTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The non-volatile memory (NVM) with DRAM-like performance and disk-like persistency has attracted considerable attention in a variety of index structures, including hash table, B-Tree and R-Tree. However, existing NVM-optimized consistent R-Tree is still suboptimal because its single level system neglects the potential boost that DRAM can bring. In this paper, we first propose a hybrid DRAM-NVM consistent R-Tree (HR-Tree), which separately stores internal nodes in DRAM and leaf nodes in NVM. To avoid inconsistency, HR-Tree uses several auxiliary flag bits and pointers to record the process of writes to NVM and employs persistence operations to strictly control the order of writes to NVM. To reduce DRAM consumption, which mainly depends on the metadata size of a leaf node, we present a shared byte strategy to abolish restrictions on metadata size while still keeping HR-Tree consistency. Next, for further shortening search time, we propose an alternative Hilbert-curve-based hybrid R-Tree (HHR-Tree). It has better search efficiency yet leads to insertion performance degradation. Contrary to in-place update in HR-Tree, HHR-Tree applies out-of-place mechanism to enforce data consistency. We conduct comprehensive evaluations on Intel Optane DC Persistent Memory. The proposed HR-Tree outperforms FBR-Tree in terms of insertion, deletion and search throughput while HHR-Tree exhibits a significant improvement for search performance by sacrificing insertion efficiency.}
}


@inproceedings{DBLP:conf/icde/YaoDL25,
	author = {Yueyang Yao and
                  Xingyuan Dai and
                  Yisheng Lv},
	title = {Leveraging Heterogeneous Experts with Advantageous Pattern Memory
                  Learning for Traffic Prediction},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3342--3355},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00250},
	doi = {10.1109/ICDE65448.2025.00250},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YaoDL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate traffic prediction is essential for mitigating congestion and enabling convenient trip arrangements. However, a single modeling approach often struggles to excel across diverse traffic patterns due to the inherent complexities and external influences in traffic scenarios. To address these issues, we propose a method named Memory-enhanced Heterogeneous Mixture of Experts (MH-MoE), which leverages memory-enhanced gating to integrate multiple pretrained models. The proposed method first obtains spatio-temporal embeddings from historical traffic sequences, followed by a traffic pattern extractor to capture representative patterns. Furthermore, a memory gating module memorizes each expert's advantageous patterns and learns to allocate traffic patterns to suitable experts. Finally, by combining predictions from these experts, MH-MoE effectively leverages the strengths of heterogeneous modeling to excel across traffic patterns. Experiments on multiple traffic datasets demonstrate that MH-MoE outperforms existing methods by leveraging diverse expert strengths, improving predictive accuracy, and offering scalability and efficiency for complex traffic prediction tasks.}
}


@inproceedings{DBLP:conf/icde/YangWXQWZ25,
	author = {Peilun Yang and
                  Hanchen Wang and
                  Zhangyi Xu and
                  Zhengping Qian and
                  Yongheng Wang and
                  Ying Zhang},
	title = {Structure and Position-Aware Graph Modeling for Trajectory Similarity
                  Computation Over Road Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3356--3368},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00251},
	doi = {10.1109/ICDE65448.2025.00251},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YangWXQWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory similarity computation is critical to various spatial data-related applications. To date, many deep learning-based approaches have been proposed to approximate trajectory similarity. However, most of previous models focus on trajectories in Euclidean space, neglecting the information of road networks, which is an important prerequisite in many applications, such as traffic analytics, social recommendation. In this paper, we study the trajectory similarity learning over road networks. Different from previous task, trajectories over road networks contain richer and more complex information, e.g., the geographical and structure information of road networks. To this end, we propose SPGMT, a graph modeling based approach that leverages abundant structure and position information inherent in road networks for trajectory similarity learning. Particularly, our graph model learns informative node representations by simultaneously incorporating structure information of nodes from a local perspective and position information from a global perspective. This road network oriented module is the first proposal to learn from a broad context of graph topology. Afterwards, SPGMT designs a self-attention network and employs an LSTM to learn the sequential information from trajectories. We conduct experiments on real-life datasets to demonstrate the superiority of SPGMT in terms of effectiveness. Besides, additional study shows the flexibility and robustness of SPGMT.}
}


@inproceedings{DBLP:conf/icde/FangLWZJFPW25,
	author = {Peng Fang and
                  Siqiang Luo and
                  Fang Wang and
                  Bolong Zheng and
                  Hong Jiang and
                  Dan Feng and
                  Hechang Pan and
                  Xingyu Wan},
	title = {OMeGa: Boosting Large-scale Graph Embeddings with Heterogeneous Memory
                  Processing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3369--3383},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00252},
	doi = {10.1109/ICDE65448.2025.00252},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FangLWZJFPW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph embedding, which maps graph nodes to lowdimensional vectors, is a widely used technique for graph representation learning. However, most existing graph embedding models suffer from high memory consumption, limiting their scalability to large graphs. Heterogeneous memory systems that combine DRAM and Persistent Memory (PM) offer new opportunities for scaling up memory capacity. Despite this advantage, the performance gap (on the order of 5x) between DRAM and PM is magnified (by 3.3-4.2x) under non-uniform memory access (NUMA) architecture. Additionally, the inherent sparsity of graphs induces numerous random accesses in the fundamental Sparse Matrix and Dense Matrix Multiplication (SpMM) operations of graph embedding, hindering high-performance heterogeneous memory processing. To address these challenges, this paper presents OMeGa that focuses on Optimizing heterogeneous Memory processing for large-scale Graph embedding. OMeGa leverages an entropy-aware thread allocation, simultaneously achieving workload balancing and tail latency reduction across threads. It also incorporates a workload feature-aware prefetcher to alleviate random accesses during streaming heterogeneous processing. In addition, OMeGa devises a NUMA-aware data placement, aiming to minimize the adverse impact of NUMA on heterogeneous memory. The experiments conducted on billion-scale graphs demonstrate that OMeGa exhibits an average acceleration of 32.03x with strong scalability. This pioneering capability enables the efficient generation of large-scale graph embeddings, free from the memory size constraints and performance disparities typically encountered in heterogeneous memory systems.}
}


@inproceedings{DBLP:conf/icde/QiuSCW25,
	author = {Hongbo Qiu and
                  Renjie Sun and
                  Chen Chen and
                  Xiaoyang Wang},
	title = {Enhance Stability of Network by Edge Anchor},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3384--3396},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00253},
	doi = {10.1109/ICDE65448.2025.00253},
	timestamp = {Tue, 25 Nov 2025 07:46:50 +0100},
	biburl = {https://dblp.org/rec/conf/icde/QiuSCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of online social networks, strengthening their stability has emerged as a key research focus. This study aims to identify influential relationships that significantly impact community stability. In this paper, we introduce and explore the anchor trussness reinforcement problem to reinforce the overall user engagement of networks by anchoring some edges. Specifically, for a given graph  G G  and a budget  b b , we aim to identify  b b  edges whose anchoring maximizes the trussness gain, which is the cumulative increment of trussness across all edges in  G G . We establish the NP-hardness of the problem. To address this problem, we introduce a greedy framework that iteratively selects the current best edge. To scale for larger networks, we first propose an upward-route method to constrain potential trussness increment edges. Augmented with a support check strategy, this approach enables the efficient computation of the trussness gain for anchoring one edge. Then, we design a classification tree structure to minimize redundant computations in each iteration by organizing edges based on their trussness. We conduct extensive experiments on 8 real-world networks to validate the efficiency and effectiveness of the proposed model and methods.}
}


@inproceedings{DBLP:conf/icde/PanZZXZSBCCZ25,
	author = {Qingfeng Pan and
                  Jiahe Zhi and
                  Chenyang Zhang and
                  Chen Xu and
                  Zhao Zhang and
                  Anita Shao and
                  Guanglei Bao and
                  Qiu Cui and
                  Xiaowei Chen and
                  Aoying Zhou},
	title = {Machine Learning Inference Pipeline Execution Using Pure {SQL} Based
                  on Operator Fusion},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3397--3410},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00254},
	doi = {10.1109/ICDE65448.2025.00254},
	timestamp = {Thu, 11 Sep 2025 09:20:25 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PanZZXZSBCCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deploying machine learning (ML) inference pipelines in databases become increasingly prevalent in many applications. In order to avoid data transfer between the database and ML runtimes, existing ML2SQL frameworks parse ML pipelines to a graph consisting of ML operators and then translate it into pure SQL. Nevertheless, they typically rewrite the graph without operator fusion or only consider the fusion between certain operators such as StandardScaler and tree inference. However, there are various operators in ML pipelines, which have rich fusion opportunities between each other. To fully exploit operator fusion for graph rewriting, we classify widely used ML operators and design fusion rules driven by their characteristics. Moreover, rewriting the original graph by fusion rules produces candidate graphs that generate SQLs with different execution time. We employ an enumeration-based strategy to search for the graph with the lowest cost. However, this strategy may suffer from the combination explosion on search space for complex ML pipelines. To reduce this space, we propose a greedy-based strategy by exploiting the independence among ML operators. We implement a novel ML2SQL framework as a portable plugin for databases, namely Craftsman. Our experimental evaluations show that, in comparison to the existing approaches, Craftsman generates efficient SQL queries which achieves an average speedup of 2.9x on popular databases such as DuckDB.}
}


@inproceedings{DBLP:conf/icde/ChenQYJCG25,
	author = {Fangke Chen and
                  Xiaotian Qiu and
                  Yihan Ye and
                  Ruyue Jing and
                  Yining Chen and
                  Dawei Gao},
	title = {{KARMAD:} KAN-Based Adversarial Robust Model for Anomaly Detection},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3425--3438},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00256},
	doi = {10.1109/ICDE65448.2025.00256},
	timestamp = {Tue, 13 Jan 2026 10:06:48 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ChenQYJCG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series anomaly detection (TSAD) is critical for ensuring the reliability of equipment in complex industrial environments. However, existing methods face significant challenges, including imbalanced data, lack of labeled samples, reliance on prior knowledge, poor generalization across diverse industrial scenarios, and low sensitivity to subtle anomalies. To address these limitations, we propose KARMAD, a novel framework that integrates Kolmogorov-Arnold Networks (KANs) for bidirectional function learning, adversarial training to enhance sensitivity to minor anomalies, and an adaptive thresholding strategy for improved precision and transferability. Evaluated on five public datasets against 14 state-of-the-art methods, KARMAD achieves state-of-the-art performance on all datasets, with an average F1 score improvement of 13.14%. Further experiments showcase its robustness to noise and adaptability in real-world industrial applications. KARMAD represents a significant advancement in developing scalable and accurate TSAD models suitable for diverse and high-stakes environments.}
}


@inproceedings{DBLP:conf/icde/DuanHYS25,
	author = {Jiawei Duan and
                  Haibo Hu and
                  Qingqing Ye and
                  Xinyue Sun},
	title = {Analyzing and Optimizing Perturbation of {DP-SGD} Geometrically},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3439--3452},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00257},
	doi = {10.1109/ICDE65448.2025.00257},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DuanHYS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Differential privacy (DP) has become a prevalent privacy model in a wide range of machine learning tasks, especially after the debut of DP-SGD. However, DP-SGD, which directly perturbs gradients in the training iterations, fails to mitigate the negative impacts of noise on gradient direction. As a result, DP-SGD is often inefficient. Although various solutions (e.g., clipping to reduce the sensitivity of gradients and amplifying privacy bounds to save privacy budgets) are proposed to trade privacy for model efficiency, the root cause of its inefficiency is yet unveiled. In this work, we first generalize DP-SGD and theoretically derive the impact of DP noise on the training process. Our analysis reveals that, in terms of a perturbed gradient, only the noise on direction has eminent impact on the model efficiency while that on magnitude can be mitigated by optimization techniques, i.e., fine-tuning gradient clipping and learning rate. Besides, we confirm that traditional DP introduces biased noise on the direction when adding unbiased noise to the gradient itself. Overall, the perturbation of DP-SGD is actually sub-optimal from a geometric perspective. Motivated by this, we design a geometric perturbation strategy GeoDP within the DP framework, which perturbs the direction and the magnitude of a gradient, respectively. By directly reducing the noise on the direction, GeoDP mitigates the negative impact of DP noise on model efficiency with the same DP guarantee. Extensive experiments on two public datasets (i.e., MNIST and CIFAR-10), one synthetic dataset and three prevalent models (i.e., Logistic Regression, CNN and ResNet) confirm the effectiveness and generality of our strategy.}
}


@inproceedings{DBLP:conf/icde/ZhangLLXZZLL25,
	author = {Xinmiao Zhang and
                  Cheng Liu and
                  Shengwen Liang and
                  Chenwei Xiong and
                  Yu Zhang and
                  Lei Zhang and
                  Huawei Li and
                  Xiaowei Li},
	title = {FrontOrder: Frontier-Guided Graph Reordering},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3453--3466},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00258},
	doi = {10.1109/ICDE65448.2025.00258},
	timestamp = {Thu, 25 Dec 2025 12:47:49 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhangLLXZZLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph processing suffers from severe locality challenges due to considerable inefficient irregular memory accesses, which mainly originate from random accesses to neighbors of active vertices (a.k.a frontiers). Graph reordering, which assigns continuous IDs to vertices that are more likely to be accessed consecutively, can improve access locality effectively and has demonstrated significant speedups across various architectures and systems. Existing graph reordering methods primarily explore the overlapping intensity of in-neighbor vertices for the data access locality characterization. However, many graph algorithms often activate a fraction of the vertices across the graphs, which vary substantially over different inputs and processing iterations. Many of these vertices are neither connected nor have any shared neighbors, but they are actually processed at the same time and exhibit potential data access locality, which is generally overlooked in prior graph reordering methods. We notice that the data locality between concurrently activated vertices are usually attributed to the overlapped $k$-order in-neighbors. As the number of $k$-order in-neighbors grows explosively, it is unacceptably time-consuming to analyze the overlapping of $k$-order in-neighbors for graph reordering directly. In this case, we propose to replace the overlapping calculation of $k$-order in-neighbors with frontier distribution analysis of a few BFS samplings. Specifically, we profile the frontiers distributed across iterations of different BFS samplings first and build a feature vector based on the activated iteration order of each vertex in the BFS samplings. On top of the feature vectors, we propose FrontOrder, which has a customized distance metric to characterize the locality between different vertices and leverages $K$-means to cluster vertices with high locality to guide graph reordering. In addition, FrontOrder also takes the load balance into consideration by predicting the runtime computing intensity with the learned clusters of vertices. According to our experiments, FrontOrder delivers an average performance speedup of ${2.33\\times}$ and ${1.57\\times}$ on Ligra and GPOP, respectively, and consistently outperforms the state-of-the-art graph reordering methods on a set of representative graph algorithms and datasets with moderate preprocessing overhead.}
}


@inproceedings{DBLP:conf/icde/HouYRZH25,
	author = {Renxuan Hou and
                  Qingqing Ye and
                  Xun Ran and
                  Sen Zhang and
                  Haibo Hu},
	title = {PrivIM: Differentially Private Graph Neural Networks for Influence
                  Maximization},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3467--3479},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00259},
	doi = {10.1109/ICDE65448.2025.00259},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HouYRZH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence Maximization (IM), aiming to identify a small set of highly influential nodes in social networks, is a critical problem in graph analysis. Recently, Graph Neural Networks (GNNs) have demonstrated superior effectiveness in addressing IM. However, a trained GNN still raises significant privacy concerns, as it may expose sensitive node features and structural information. While Differential Privacy (DP) techniques have been widely applied to GNNs for node-level tasks, they cannot be directly extended to 1M problems. This is because IM requires more complex structural information for training, resulting in an extremely larger DP noise scale than node-level tasks. To tackle these issues, we propose PrivIM, a novel differentially private subgraph-based GNNs framework for IM tasks, which ensures node-level DP guarantees. Within PrivIM, we design a unique dual-stage adaptive frequency sampling scheme to optimize the model utility. First, it reduces the correlation between nodes by dynamically adjusting each node's sampling probability. Then additional subgraphs are incorporated to supplement boundary structural information, enhancing utility without increasing privacy budget. Extensive experiments on six real-world datasets demonstrate that PrivIM maintains high utility in IM compared to baseline methods.}
}


@inproceedings{DBLP:conf/icde/PanZLXYML25,
	author = {Dong Pan and
                  Xu Zhou and
                  Lingwei Li and
                  Quanqing Xu and
                  Chuanhui Yang and
                  Chenhao Ma and
                  Kenli Li},
	title = {Efficient Structural Clustering Over Hypergraphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3480--3493},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00260},
	doi = {10.1109/ICDE65448.2025.00260},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/PanZLXYML25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Structural Graph Clustering is a well-known problem that aims to identify clusters and distinguish between special roles, such as hub and outlier. However, SCAN, the fundamental structural clustering model, is designed for pairwise graphs and fails to capture the unique structural information inherent in hypergraphs when clustering hypergraphs. Motivated by this, we propose a new structural clustering model, HSCAN, specifically for hypergraphs. We further design an Order-Index to accelerate fetching the key information of the HSCAN and a Lightweight Similarity Bucket Index to reduce the index cost. Next, we present an index-based sequential query algorithm with high performance and a parallel query algorithm to process large hypergraphs faster. Additionally, we provide the algorithms for constructing Order-Index and Lightweight Similarity Bucket Index. Extensive experiments on both real-world and synthetic datasets show that HSCAN performs better than existing models, and the two index-based query algorithms are up to three orders of magnitude faster than the existing algorithm.}
}


@inproceedings{DBLP:conf/icde/ZhangYHX25,
	author = {Sen Zhang and
                  Qingqing Ye and
                  Haibo Hu and
                  Jianliang Xu},
	title = {AdvSGM: Differentially Private Graph Learning via Adversarial Skip-Gram
                  Model},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3494--3507},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00261},
	doi = {10.1109/ICDE65448.2025.00261},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangYHX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The skip-gram model (SGM), which employs a neural network to generate node vectors, serves as the basis for numerous popular graph embedding techniques. However, since the training datasets contain sensitive linkage information, the parameters of a released SGM may encode private information and pose significant privacy risks. Differential privacy (DP) is a rigorous standard for protecting individual privacy in data analysis. Nevertheless, when applying differential privacy to skip-gram in graphs, it becomes highly challenging due to the complex link relationships, which potentially result in high sensitivity and necessitate substantial noise injection. To tackle this challenge, we present AdvSGM, a differentially private skip-gram for graphs via adversarial training. Our core idea is to leverage adversarial training to privatize skip-gram while improving its utility. Towards this end, we develop a novel adversarial training module by devising two optimizable noise terms that correspond to the parameters of a skip-gram. By fine-tuning the weights between modules within AdvSGM, we can achieve differentially private gradient updates without additional noise injection. Extensive experimental results on six real-world graph datasets show that AdvSGM preserves high data utility across different downstream tasks.}
}


@inproceedings{DBLP:conf/icde/ChenCWYLZ25,
	author = {Zebin Chen and
                  Kaiyu Chen and
                  Dong Wen and
                  Zhengyi Yang and
                  Wentao Li and
                  Ying Zhang},
	title = {Accelerating Shortest Path Counting on Road Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3508--3521},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00262},
	doi = {10.1109/ICDE65448.2025.00262},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ChenCWYLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Counting the number of shortest paths between two query vertices on road networks has a wide range of applications and has recently drawn significant research attention. The state-of-the-art solution builds a tree-based index using the concept of tree decomposition. However, its performance deteriorates when the tree decomposition results in an unbalanced tree and may not perform well when the query vertices are close to each other. This paper aims to improve the efficiency of shortest path counting. We propose a novel indexing scheme that combines hub labeling with a balanced tree hierarchy. This approach significantly reduces the number of visited labels compared to the state-of-the-art solution. Furthermore, we introduce several optimizations to enhance the efficiency of index construction and minimize its size. Extensive experiments conducted on real-world road networks demonstrate that our method achieves up to 4.1 times higher query efficiency and reduces the index size by a factor of 2.35 compared to the state-of-the-art solution.}
}


@inproceedings{DBLP:conf/icde/DuYXYFH25,
	author = {Rong Du and
                  Qingqing Ye and
                  Yaxin Xiao and
                  Liantong Yu and
                  Yue Fu and
                  Haibo Hu},
	title = {Dual Utilization of Perturbation for Stream Data Publication Under
                  Local Differential Privacy},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3522--3534},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00263},
	doi = {10.1109/ICDE65448.2025.00263},
	timestamp = {Mon, 27 Oct 2025 16:11:18 +0100},
	biburl = {https://dblp.org/rec/conf/icde/DuYXYFH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stream data from real-time distributed systems such as IoT, tele-health, and crowdsourcing has become an important data source. However, the collection and analysis of usergenerated stream data raise privacy concerns due to the potential exposure of sensitive information. To address these concerns, local differential privacy (LDP) has emerged as a promising standard. Nevertheless, applying LDP to stream data presents significant challenges, as stream data often involves a large or even infinite number of values. Allocating a given privacy budget across these data points would introduce overwhelming LDP noise to the original stream data. Beyond existing approaches that merely use perturbed values for estimating statistics, our design leverages them for both perturbation and estimation. This dual utilization arises from a key observation: each user knows their own ground truth and perturbed values, enabling a precise computation of the deviation error caused by perturbation. By incorporating this deviation into the perturbation process of subsequent values, the previous noise can be calibrated. Following this insight, we introduce the Iterative Perturbation Parameterization (IPP) method, which utilizes current perturbed results to calibrate the subsequent perturbation process. To enhance the robustness of calibration and reduce sensitivity, two algorithms, namely Accumulated Perturbation Parameterization (APP) and Clipped Accumulated Perturbation Parameterization (CAPP) are further developed. We prove that these three algorithms satisfy  w w -event differential privacy while significantly improving utility. Experimental results demonstrate that our techniques outperform state-of-the-art LDP stream publishing solutions in terms of utility, while retaining the same privacy guarantee.}
}


@inproceedings{DBLP:conf/icde/HanCWCZYHY25,
	author = {Yuxing Han and
                  Lixiang Chen and
                  Haoyu Wang and
                  Zhanghao Chen and
                  Yifan Zhang and
                  Chengcheng Yang and
                  Kongzhang Hao and
                  Zhengyi Yang},
	title = {Learning from the Past: Adaptive Parallelism Tuning for Stream Processing
                  Systems},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3535--3548},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00264},
	doi = {10.1109/ICDE65448.2025.00264},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HanCWCZYHY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed stream processing systems rely on the dataflow model to define and execute streaming jobs, organizing computations as Directed Acyclic Graphs (DAGs) of operators. Adjusting the parallelism of these operators is crucial to handling fluctuating workloads efficiently while balancing resource usage and processing performance. However, existing methods often fail to effectively utilize execution histories or fully exploit DAG structures, limiting their ability to identify bottlenecks and determine the optimal parallelism. In this paper, we propose StreamTune, a novel approach for adaptive parallelism tuning in stream processing systems. StreamTune incorporates a pre-training and fine-tuning framework that leverages global knowledge from historical execution data for job-specific parallelism tuning. In the pre-training phase, StreamTune clusters the historical data with Graph Edit Distance and pre-trains a Graph Neural Network-based encoder per cluster to capture the correlation between the operator parallelism, DAG structures, and the identified operator-level bottlenecks. In the online tuning phase, Stream-Tu ne iteratively refines operator parallelism recommendations using an operator-level bottleneck prediction model enforced with a monotonic constraint, which aligns with the observed system performance behavior. Evaluation results demonstrate that StreamTune reduces reconfigurations by up to 29.6% and parallelism degrees by up to 30.8% in Apache Flink under a synthetic workload. In Timely Dataflow, StreamTune achieves up to an 83.3% reduction in parallelism degrees while maintaining comparable processing performance under the Nexmark benchmark, when compared to the state-of-the-art methods.}
}


@inproceedings{DBLP:conf/icde/MaoYDWHH25,
	author = {Yulian Mao and
                  Qingqing Ye and
                  Rong Du and
                  Qi Wang and
                  Kai Huang and
                  Haibo Hu},
	title = {Multi-Class Item Mining Under Local Differential Privacy},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3549--3561},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00265},
	doi = {10.1109/ICDE65448.2025.00265},
	timestamp = {Mon, 27 Oct 2025 16:11:18 +0100},
	biburl = {https://dblp.org/rec/conf/icde/MaoYDWHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Item mining, a fundamental task for collecting statistical data from users, has raised increasing privacy concerns. To address these concerns, local differential privacy (LDP) was proposed as a privacy-preserving technique. Existing LDP item mining mechanisms primarily concentrate on global statistics, i.e., those from the entire dataset. Nevertheless, they fall short of usertailored tasks such as personalized recommendations, whereas classwise statistics can improve task accuracy with fine-grained information. Meanwhile, the introduction of class labels brings new challenges. Label perturbation may result in invalid items for aggregation. To this end, we propose frameworks for multi-class item mining, along with two mechanisms: validity perturbation to reduce the impact of invalid data, and correlated perturbation to preserve the relationship between labels and items. We also apply these optimized methods to two multi-class item mining queries: frequency estimation and top- k k  item mining. Through theoretical analysis and extensive experiments, we verify the effectiveness and superiority of these methods.}
}


@inproceedings{DBLP:conf/icde/YaoYLX25,
	author = {Jiajun Yao and
                  Lei Yang and
                  Hao Liu and
                  Hui Xiong},
	title = {Joint Dependency and Conflicting Task Allocation in Collaboration-Aware
                  Spatial Crowdsourcing},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3562--3574},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00266},
	doi = {10.1109/ICDE65448.2025.00266},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YaoYLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial crowdsourcing (SC) is a new form of crowdsourcing that utilizes users (i.e., workers) equipped with smart devices to complete tasks at specific locations. Previous studies usually focus on single task relationships (e.g., dependencies or conflicts) without considering task allocation under multiple relationships. To address this limitation, we jointly consider task dependency and conflict while also considering collaboration among workers for task allocation. In this paper, we define and formulate a new problem, called Joint Dependency and Conflicting Task Allocation in Collaboration-aware Spatial Crowdsourcing (JDCTA), which is proved to be NP-hard. To tackle the JDCTA problem, we first design an approximation algorithm, JDCTA-Greedy, which constructs a set of associated task groups based on task relationships and then greedily allocates these groups, in which we can obtain results with a theoretical bound on the approximate ratio. We then propose JDCTA-Game, a both dependency and conflict aware game approach. JDCTA -Game reduces the strategy space by defining dependency and conflict trees, combined with a dynamic payoff function based on the multiple relationships between tasks, to achieve high-quality solutions. Theoretical analysis demonstrates that this method guarantees the existence of at least one Nash equilibrium, and the solution quality is bounded. Experimental results on both synthetic and real datasets show that our proposed approach outperforms the representative approaches in terms of overall utility.}
}


@inproceedings{DBLP:conf/icde/WangWNW25,
	author = {Songyao Wang and
                  Chaokun Wang and
                  Fang Niu and
                  Cheng Wu},
	title = {LSM-Community: {A} Graph Storage System Exploiting Community Structure
                  in Social Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3575--3588},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00267},
	doi = {10.1109/ICDE65448.2025.00267},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WangWNW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, several social network analysis algorithms have been optimized by leveraging the community structure commonly found in graphs. Since community structure is fun-damental to these algorithms, storing graphs based on their community structure can significantly enhance the performance of graph algorithms that rely on it for optimization. However, existing graph storage systems do not natively store graphs according to the community structure, which limits their per-formance in retrieving communities. To fill this gap, we pro-pose LSM-Community, a graph storage system inspired by the LSM - Tree design that stores graphs on disk based on their community structure. To dynamically maintain the community structure during graph updates, we present the community-centric dynamic community detection algorithm  (C^{3}D) (C^{3}D) . Experimental results demonstrate that LSM-Community outperforms other storage systems in classical community discovery tasks (e.g., performing CD on UK-2007 dataset with LSM-Community is  86.12\\times 86.12\\times  faster than Neo4j) while maintaining high performance on classical graph analytic algorithms. This indicates that LSM-Community efficiently supports community discovery and query processing while preserving the performance of classical analytic algorithms,}
}


@inproceedings{DBLP:conf/icde/TangWZCWZ25,
	author = {Zhiyang Tang and
                  Yanping Wu and
                  Xiangjun Zai and
                  Chen Chen and
                  Xiaoyang Wang and
                  Ying Zhang},
	title = {Efficient Temporal Simple Path Graph Generation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3589--3601},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00268},
	doi = {10.1109/ICDE65448.2025.00268},
	timestamp = {Tue, 25 Nov 2025 07:46:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/TangWZCWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interactions between two entities often occur at specific timestamps, which can be modeled as a temporal graph. Exploring the relationships between vertices based on temporal paths is one of the fundamental tasks. In this paper, we conduct the first research to propose and investigate the problem of generating the temporal simple path graph (tspG), which is the subgraph consisting of all temporal simple paths from the source vertex to the target vertex within the given time interval. Directly enumerating all temporal simple paths and constructing the tspG is computationally expensive. To accelerate the processing, we propose an efficient method named Verification in Upper-bound Graph. It first incorporates the temporal path constraint and simple path constraint to exclude unpromising edges from the original graph, which obtains a tight upper-bound graph as a high-quality approximation of the tspG in polynomial time. Then, an Escape Edges Verification algorithm is further applied in the upper-bound graph to construct the exact tspG without exhaustively enumerating all temporal simple paths between given vertices. Finally, comprehensive experiments on 10 real-world graphs are conducted to demonstrate the efficiency and effectiveness of the proposed techniques.}
}


@inproceedings{DBLP:conf/icde/YaoLXLWWSJ25,
	author = {Zhongming Yao and
                  Tianyi Li and
                  Junchang Xin and
                  Yushuai Li and
                  Chenxu Wang and
                  Zhiqiong Wang and
                  Divesh Srivastava and
                  Christian S. Jensen},
	title = {{VGQ:} Enabling Verifiable Graph Queries on Blockchain Systems},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3602--3614},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00269},
	doi = {10.1109/ICDE65448.2025.00269},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YaoLXLWWSJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain technology has transformed financial services sectors by providing security, transparency, and immutability through decentralized ledger systems. However, while blockchain data can support a range of applications-such as user quality analysis, illegal activity detection, and transaction pattern identification-existing systems are restricted to basic queries on blocks and transactions due to their sequential data storage. To support queries more generally, we propose VGQ, the first verifiable graph query (VGQ) framework that enables efficient graph queries on blockchain systems without altering blockchain storage structures. VGQ integrates a query layer with an external graph database system and represents blockchain data as a directed transaction graph to improve the efficiency of graph query execution. To ensure reliable results, VGQ includes result verification with three key performance enhancing optimizations: (i) computing connected components to exclude irrelevant vertices and edges during verification; (ii) merging information from edges to accelerate completeness verification; and (iii) employing a dual pointer algorithm for efficient soundness verification. Experiments offer evidence that VGQ can improve on the state-of-the-art framework in terms of query efficiency by up to one order of magnitude and in terms of verification efficiency by up to two orders of magnitude.}
}


@inproceedings{DBLP:conf/icde/WangTSLD25,
	author = {Qianyu Wang and
                  Wei{-}Tek Tsai and
                  Tianyu Shi and
                  Zhuang Liu and
                  Bowen Du},
	title = {Catch Me If You Can: {A} Multi-Agent Synthetic Fraud Detection Framework
                  for Complex Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3629--3641},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00271},
	doi = {10.1109/ICDE65448.2025.00271},
	timestamp = {Wed, 28 Jan 2026 07:40:53 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WangTSLD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting fraudulent behavior across diverse domains presents a significant challenge due to the adaptive and elusive activities of fraud agents. Furthermore, imbalanced data distributions and limited labeled examples increase the difficulty of detecting fraud agents. To address these challenges, we propose Catch Me If You Can—a Multi-Agent Framework to generate synthetic datasets and simulate various types of fraudulent behavior, including but not limited to anti-money laundering (AML), credit card fraud, bot attacks, and malicious traffic. Our framework comprises two core agent types: (1) Detectors, trained to identify suspicious patterns in scenarios, and (2) Transaction Agents, including both legitimate participants and adversarial fraud agents employing strategies to evade detection. In this framework, detectors iteratively refine their detection strategies while fraud agents evolve adaptive tactics to disguise illicit activities, creating an adversarial coevolutionary environment. This dynamic fosters the generation of high-dimensional and realistic datasets for training and testing. By integrating synthetic pre-training with transfer learning, the framework leverages a variety of real-world datasets—including IEEE-CIS Fraud Detection, Credit Card Fraud Detection, and Elliptic++—demonstrating its broad applicability across multiple fraud domains. Our approach significantly improves detection performance, bridging the gap between simulation and real-world applications. It enables robust training across heterogeneous fraud behaviors, contributing to the development of resilient, generalizable solutions for financial security and fraud prevention.}
}


@inproceedings{DBLP:conf/icde/LuoQZD25,
	author = {Jiarui Luo and
                  Miao Qiao and
                  Chaoji Zuo and
                  Dong Deng},
	title = {Tag-Filtered Approximate Nearest Neighbor Search},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3642--3654},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00272},
	doi = {10.1109/ICDE65448.2025.00272},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuoQZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate Nearest Neighbor Search (ANNS) plays an important role in the search and recommendation of objects represented with high-dimensional vectors. For objects that are associated with tags such as the origin location, color, and type, it is common to perform ANNS with tag constraints, i.e., conduct search on objects that carry the query tags. We call such search Tag-Filtered Approximate Nearest Neighbor Search (TFANNS). The state-of-the-art TFANNS method Filtered-DiskANN is a graph-based method which suffers from a low recall for queries with low-to-medium frequent tags. Pre-filtering on these tags could boost the recall but lead to a large memory footprint. To address this issue, we propose three strategies in constructing a graph that strikes a balance between the performance and memory footprint; note that we are the first work on tag-frequency-aware graph-based indexing for TFANNS. Our extensive experiments show the superiority of our proposed methods over existing baselines: under  \\geq 0.9 \\geq 0.9  recall, our QPS is up to 13 times that of the best baseline.}
}


@inproceedings{DBLP:conf/icde/HanWRYLYLYZ25,
	author = {Huaxu Han and
                  Shuliang Wang and
                  Sijie Ruan and
                  Qianyu Yang and
                  Yuxuan Liang and
                  Ziqiang Yuan and
                  Cheng Long and
                  Hanning Yuan and
                  Yu Zheng},
	title = {AdaMove: Efficient Test-Time Adaptation for Human Mobility Prediction},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3655--3667},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00273},
	doi = {10.1109/ICDE65448.2025.00273},
	timestamp = {Fri, 14 Nov 2025 07:30:21 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HanWRYLYLYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human mobility prediction is a fundamental technique for many urban applications, e.g., location-based recommendation, traffic scheduling, and travel demand prediction. Over the past decades, many methods, e.g., Markov Model, RNN, Transformer, have been leveraged to tackle the problem. However, existing approaches mainly train a supervised model based on an offline training dataset, which overlooks the phenomenon that the mobility behaviors of humans vary across time, and the trained models may not achieve ideal performance when applied to the testing data. To tackle this challenge, in this paper, we propose AdaMove, an efficient Test-Time Adaptive (TTA) model for human mobility prediction. AdaMove has a Preference-aware Test-Time Adaptation module called PTTA, which can adjust the parameters of a trained model based on the input test trajectory such that the model can generalize to the test distribution. In addition, to address the issue of reduced inference efficiency caused by parameter adjustment during the testing phase, AdaMove is equipped with a Lightweight human Mobility prediction model called LightMob, which only requires the recent trajectory as input to accelerate the inference. It is enhanced by historical trajectory knowledge via contrastive learning during the training time, so it has competitive performance compared with existing models. Extensive experiments on three real-world human mobility datasets demonstrate that AdaMove outperforms the best baseline by 9.3% on average in accuracy, and accelerates the inference speed by 28.5% on average compared with the original TTA - based inference.}
}


@inproceedings{DBLP:conf/icde/KaluminD25,
	author = {Hasara Kalumin and
                  Amol Deshpande},
	title = {Optimizing Queries with Many-to-Many Joins},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3668--3681},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00274},
	doi = {10.1109/ICDE65448.2025.00274},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KaluminD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As database query processing techniques are being used to handle diverse workloads, a key emerging challenge is how to efficiently handle multi-way join queries containing multiple many-to-many joins. While uncommon in traditional enterprise settings, such queries are seen frequently in other contexts such as graph workloads. This has led to much work on developing join algorithms for handling cyclic queries, on compressed (factorized) representations for more efficient storage of intermediate results, and on use of semi-joins or predicate transfer to avoid generating large redundant intermediate results. In this paper, we address a core query optimization problem in this context. Specifically, we introduce an improved cost model that more accurately captures the cost of a query plan in such scenarios, and we present several optimization algorithms for query optimization that incorporate these new cost functions. We present an extensive experimental evaluation, that compares the factorized representation approach with a full semi-join reduction approach as well as to an approach that uses bitvectors to eliminate tuples early through sideways information passing. We also present new analyses of robustness of these techniques to the choice of the join order, potentially eliminating the need for more complex query optimization and selectivity estimation techniques.}
}


@inproceedings{DBLP:conf/icde/XiaWLJLG25,
	author = {Mingze Xia and
                  Bei Wang and
                  Yuxiao Li and
                  Pu Jiao and
                  Xin Liang and
                  Hanqi Guo},
	title = {TspSZ: An Efficient Parallel Error-Bounded Lossy Compressor for Topological
                  Skeleton Preservation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3682--3695},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00275},
	doi = {10.1109/ICDE65448.2025.00275},
	timestamp = {Wed, 10 Sep 2025 14:09:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XiaWLJLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data compression is a powerful solution for addressing big data challenges in database and data management. In scientific data compression for vector fields, preserving topological information is essential for accurate analysis and visualization. The topological skeleton, a fundamental component of vector field topology, consists of critical points and their connectivity (i.e., separatrices). While previous work has focused on preserving critical points in error-controlled lossy compression, little attention has been given to preserving separatrices, which are equally important. In this work, we introduce TspSZ, an efficient error-bounded lossy compression framework designed to preserve both critical points and separatrices. Our key contributions are threefold. First, we propose TspSZ, a topological-skeleton-preserving lossy compression framework that integrates two algorithms, enabling existing critical-point-preserving compressors to also retain separatrices, significantly enhancing their topology preservation capabilities. Second, we optimize TspSZ for efficiency through tailored improvements and parallelization. Specifically, we introduce a new error control mechanism to achieve high compression ratios and implement a shared-memory parallelization strategy to boost compression throughput. Third, we evaluate TspSZ against state-of-the-art lossy and lossless compressors using four real-world scientific datasets. Experimental results show that TspSZ achieves compression ratios of up to 7.7× while effectively preserving the topological skeleton, ensuring efficient storage and transmission of scientific data without compromising topological integrity.}
}


@inproceedings{DBLP:conf/icde/DengQPZ25,
	author = {Xin Deng and
                  Zheng Qin and
                  Peng Peng and
                  Hui Zhou},
	title = {TopK-BC: Efficient Maintenance of Top k (p,q)-bicliques over Streaming
                  Bipartite Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3696--3709},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00276},
	doi = {10.1109/ICDE65448.2025.00276},
	timestamp = {Thu, 06 Nov 2025 20:21:55 +0100},
	biburl = {https://dblp.org/rec/conf/icde/DengQPZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bipartite graphs are ubiquitous, such as E-commerce network and gene networks. Efficient analysis of (p, q)- biclique is one of the important problems over bipartite graphs. However, existing works over (p, q)-biclique suffer from two main challenges. Firstly, most of them only focus on static graphs, while lots of bipartite graph-structured data are constantly created in real world, forming streaming bipartite graphs. Secondly, results of (p, q)-biclique could be of exponential scale, which may overwhelm analysts. Hence, computing top  k k  most important (p, q)-bicliques is worth considering. In this paper, we study a new problem to maintain top  k k  densest (p, q)-bicliques over a streaming bipartite graph. We propose a new framework, called as TopK-BC, to compute the proposed problem effectively. We design an efficient pruning strategy for edge deletion stage, called IDpruning. In particular, we maintain an intermediate density for each edge to efficiently compute high-density (p, q)-bicliques. Also, we introduce effective optimization technologies to filter out unpromising intermediate results and further enhance the performance. Extensive experiments over real world datasets confirm the efficiency and effectiveness of our solution.}
}


@inproceedings{DBLP:conf/icde/WangCY25,
	author = {Zhiyi Wang and
                  Lijun Chang and
                  Jeffrey Xu Yu},
	title = {Identifying Maximum Defective Bicliques in Large Bipartite Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3710--3723},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00277},
	doi = {10.1109/ICDE65448.2025.00277},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding dense subgraphs in a bipartite graph is a powerful tool for uncovering meaningful patterns and extracting valuable insights across various domains. In this paper, we relax the definition of biclique to  k k -defective biclique by allowing up-to  k k  missing edges, such that larger, but still dense, substructures can be identified. Then, we propose algorithms to find the defective biclique with the largest number of vertices, which is an NP-hard problem. Nevertheless, we prove that our algorithm runs in  \\mathcal{O}^{*}\\left(\\gamma^{n+k}\\right) \\mathcal{O}^{*}\\left(\\gamma^{n+k}\\right)  time, beating the trivial  \\mathcal{O}^{*}\\left(2^{n}\\right) \\mathcal{O}^{*}\\left(2^{n}\\right)  time complexity; here the  \\mathcal{O}^{*} \\mathcal{O}^{*}  notation hides polynomial factors,  n n  is the number of vertices in the input graph  G G  and  \\gamma \\approx 1.8393 \\gamma \\approx 1.8393  is a constant. We further prove the diameter-three property of  k k -defective bicliques with at least  k+1 k+1  vertices on each side, and utilize it to reduce the exponent from  n+k n+k  to  \\alpha \\Delta^{2}+k \\alpha \\Delta^{2}+k  where  \\alpha \\alpha  and  \\Delta \\Delta  are the degeneracy and maximum degree of  G G , respectively. Finally, we propose several practical techniques (i.e., upper bounds, reduction rules, an iterative computation framework, and finding a large initial solution) to improve the practical efficiency of our algorithm. Extensive empirical studies on real bipartite graphs are conducted to evaluate our techniques. As a by-product, our analysis techniques can also be used to prove a time complexity of  \\mathcal{O}^{*}\\left(\\gamma^{n+k}\\right) \\mathcal{O}^{*}\\left(\\gamma^{n+k}\\right)  for maximum defective clique computation in traditional unipartite graphs, improving the state-of-the-art time complexity.}
}


@inproceedings{DBLP:conf/icde/LiQZZCDY25,
	author = {Xiang Li and
                  Jianpeng Qi and
                  Zhongying Zhao and
                  Guanjie Zheng and
                  Lei Cao and
                  Junyu Dong and
                  Yanwei Yu},
	title = {{UMGAD:} Unsupervised Multiplex Graph Anomaly Detection},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3724--3737},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00278},
	doi = {10.1109/ICDE65448.2025.00278},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiQZZCDY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph anomaly detection (GAD) is a critical task in graph machine learning, with the primary objective of identifying anomalous nodes that deviate significantly from the majority. This task is widely applied in various real-world scenarios, including fraud detection and social network analysis. However, existing GAD methods still face two major challenges: (1) They are often limited to detecting anomalies in single-type interaction graphs and struggle with multiple interaction types in multiplex heterogeneous graphs. (2) In unsupervised scenarios, selecting appropriate anomaly score thresholds remains a significant challenge for accurate anomaly detection. To address the above challenges, we propose a novel Unsupervised Multiplex Graph Anomaly Detection method, named UMGAD. We first learn multi-relational correlations among nodes in multiplex heterogeneous graphs and capture anomaly information during node attribute and structure reconstruction through graph-masked autoencoder (GMAE). Then, to further extract abnormal information, we generate attribute-level and subgraph-level augmented-view graphs, respectively, and perform attribute and structure reconstruction through GMAE. Finally, we learn to optimize node attributes and structural features through contrastive learning between original-view and augmented-view graphs to improve the model's ability to capture anomalies. Meanwhile, we propose a new anomaly score threshold selection strategy, which allows the model to be independent of ground truth information in real unsupervised scenarios. Extensive experiments on six datasets show that our UMGAD significantly outperforms state-of-the-art methods, achieving average improvements of 12.25% in AUC and 11.29% in Macro-F1 across all datasets. The source code of our model is available at https://github.com/lx970414/UMGAD.}
}


@inproceedings{DBLP:conf/icde/LiWOCYZP25,
	author = {Hexu Li and
                  Hengfeng Wei and
                  Hongrong Ouyang and
                  Yuxing Chen and
                  Na Yang and
                  Ruohao Zhang and
                  Anqun Pan},
	title = {Online Timestamp-Based Transactional Isolation Checking of Database
                  Systems},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3738--3750},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00279},
	doi = {10.1109/ICDE65448.2025.00279},
	timestamp = {Thu, 20 Nov 2025 14:47:10 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiWOCYZP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serializability (SER) and snapshot isolation (SI) are widely used transactional isolation levels in database systems. The isolation checking problem asks whether a given execution history of a database system satisfies a specified isolation level. However, existing SER and SI checkers, whether traditional black-box checkers or recent timestamp-based white-box ones, operate offline and require the entire history to be available to construct a dependency graph, making them unsuitable for continuous and ever-growing histories. This paper addresses online isolation checking by extending the timestamp-based isolation checking approach to online settings. Specifically, we design Chronos, an efficient timestamp-based offline SI checker. Chronos is incremental and avoids constructing a start-ordered serialization graph for the entire history, making it well-suited for online scenarios. We further extend Chronos into an online SI checker, Aion, addressing several key challenges unique to online settings. Additionally, we develop Aion-SER for online SER checking. Experiments highlight that Chronos processes offline histories with up to one million transactions in seconds, greatly outperforming existing SI checkers. Furthermore, Aion and Aion-ser sustain a throughput of approximately 12K transactions per second, demonstrating their practicality for online isolation checking.}
}


@inproceedings{DBLP:conf/icde/HuangZCZRCH25,
	author = {Chenyu Huang and
                  Fan Zhang and
                  Huangxun Chen and
                  Yongjun Zhao and
                  Huaming Rao and
                  Peng Chen and
                  Danqing Huang},
	title = {Privacy-Preserving Screening for Record Linkage},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3751--3764},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00280},
	doi = {10.1109/ICDE65448.2025.00280},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangZCZRCH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In an era dominated by big data and machine learning, establishing valuable data collaboration has never been more critical. However, such collaborations must operate under regulatory and legal constraints. Two-party Privacy-Preserving Record Linkage (PPRL) emerges to assess the potential collaboration value and also ensure the privacy and security of the involved data. Nevertheless, the substantial computational and communication overheads associated with PPRL hinder its practical adoption in data markets with numerous potential collaborators. Therefore, we present the Screening-then-Linkage framework, which incorporates a lightweight Screening phase prior to the resource-intensive PPRL phase, i.e., PPRS, to mitigate the scalability issue of PPRL. We propose a circuit-PSI-based system, named Appraisal to realize a secure, effective, and efficient PPRS. To reconcile the approximate matching and/or schema-aware setting required in PPRS with the limitations of the circuit-PSI supporting only symmetric functions, we propose a more communication-efficient secure permutation, i.e., Oblivious Attribute/Feature Alignment protocol tailored for PPRS. This protocol supports a broader range of comparison functions and significantly improves efficiency, i.e., reducing communication costs by a factor of 14 compared to the conventional protocol. Our rigorous analysis and comprehensive empirical evaluations demonstrate the security, effectiveness, and efficiency of Appraisal. Appraisal can accommodate up to 850x more records than the SOTA PPRS system, SFour, within the same constraints. Moreover, it is 165x faster than SOTA PPRL, indicating the Screening-then-Linkage framework substantially decreases the computation time required to identify the most valuable collaborators from a large pool of candidates.}
}


@inproceedings{DBLP:conf/icde/ShuaiRCSG25,
	author = {Sujun Shuai and
                  Xuan Rao and
                  Lisi Chen and
                  Shuo Shang and
                  Shen Gao},
	title = {Real-Time Single-Source Personalized PageRank Over Evolving Social
                  Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3765--3777},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00281},
	doi = {10.1109/ICDE65448.2025.00281},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ShuaiRCSG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Single-Source Personalized PageRank (SSPPR) is a fundamental problem in social network analytics, yet maintaining accurate SSPPR query results in evolving social networks poses significant challenges, especially for real-time applications. Existing approaches often overlook the role of subgraphs and struggle with frequent graph updates, resulting in inefficiency regarding dynamic scenarios. In this study, we define a novel personalized PageRank query, n-steps SSPPR, designed to address the challenges of dynamic environments. To support this query, we propose a baseline solution, Pn-FORA, as a foundational approach. While effective, Pn-FORA is inefficient due to its computationally expensive information update scheme. To overcome these limitations, we propose a multithreaded framework for processing massive-scale n-steps SSPPR queries in real-time over evolving graphs. Central to our framework is the Global Walk Synchronization (GWS) method, ensuring the accuracy of SSPPR scores by synchronizing walk information across nodes as the graph evolves. To further enhance GWS, we introduce an influence-aware graph representation to optimize update propagation. Furthermore, we develop a dynamic workload balancing strategy and precision-aware concurrency controls, which achieve an effective balance between efficiency and accuracy. Extensive experiments on real-world datasets demonstrate that our approach significantly outperforms existing methods, offering superior scalability and efficiency for real-time n-steps SSPPR query processing over large-scale social networks. The source code of our implementation is publicly available at https://github.com/SujunShuai/Work2023.}
}


@inproceedings{DBLP:conf/icde/CuiWZLCLY25,
	author = {Ningning Cui and
                  Dong Wang and
                  Huaijie Zhu and
                  Mo Li and
                  Jingxian Cheng and
                  Jianxin Li and
                  Xiaochun Yang},
	title = {Consistency-Aware Scalable and Authenticated Learned Index for Range
                  Query},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3778--3791},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00282},
	doi = {10.1109/ICDE65448.2025.00282},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/CuiWZLCLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A corpus of recent work has revealed that authenticated query services have been under the spotlight due to the untrustworthiness of outsourced service provider. To enrich scalable functionality, there is an increasing demand for dynamically authenticated query. However, when implementing query and update simultaneously, traditional approaches heavily suffer from the inconsistency between verification digest and requested index and therefore are infeasible in reality. Moreover, the efficiency of storage, query, verification, and update is still a huge hinder when processing large scale data. To address these challenging issues, in this paper, we propose a novel idea of authenticated learned index that is carefully designed and actively optimized for authenticated query processing. Specifically, we first propose a version control update mechanism for consistency guarantee by maintaining historical index versions. Following this, we propose two basic authenticated learned indexes, i.e., query-friendly PVL-tree and update-friendly PVLB-tree, to support efficient scalable authenticated range query. Furthermore, to improve the efficiency, we introduce a hybrid index framework HPVL-tree based on two basic indexes. Extensive theoretical and experimental analysis demonstrate that our proposed HPVL-tree outperforms the state-of-the-art approaches by up to  2.28\\times, 3.96\\times 2.28\\times, 3.96\\times , and  2.51\\times 2.51\\times  in search time, update time, and verification time, respectively. Moreover, the storage overhead and communication overhead occupy only 38 % and 2.25 % of existing approach, respectively.}
}


@inproceedings{DBLP:conf/icde/ZhaoZTLYDZ25,
	author = {Runhao Zhao and
                  Weixin Zeng and
                  Jiuyang Tang and
                  Yawen Li and
                  Guanhua Ye and
                  Junping Du and
                  Xiang Zhao},
	title = {Towards Unsupervised Entity Alignment for Highly Heterogeneous Knowledge
                  Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3792--3806},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00283},
	doi = {10.1109/ICDE65448.2025.00283},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoZTLYDZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Highly Heterogeneous Entity Alignment (HHEA) represents a more realistic application scenario of Entity Alignment (EA). This challenging task aims to align equivalent entities between highly heterogeneous knowledge graphs (HHKGs) with significant differences in structure, scale, and overlap. In practice, obtaining labeled data for HHEA is often difficult, necessitating research into unsupervised HHEA. This involves addressing several challenges, including the difficulty in capturing structural and semantic associations between HHKGs, the absence of explicit HHEA paradigms, and the high time and computational costs. Unfortunately, there is no solution for unsupervised HHEA. To bridge this gap, this paper formally investigates the unsupervised HHEA problem and proposes an effective unsupervised HHEA solution, AdaCoAgentEA, which addresses the challenges of unsupervised HHEA from the perspective of multi-agent collaboration. Specifically, we design an adaptive collaboration framework with three functional areas powered by multi-agent LLMs and small models, effectively eliminating dependence on labeled data while capturing structural and semantic correlations between HHKGs. Furthermore, we design a suite of optimization tools for AdaCoAgentEA, including meta-alignment mechanisms and communication protocols, which facilitate effective associations between HHKGs and provide explicit HHEA paradigms while reducing time and computational costs. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in both unsupervised HHEA and classic EA tasks across five datasets, rivaling fully supervised models while maintaining high efficiency and scalability.}
}


@inproceedings{DBLP:conf/icde/SunPHLYZZ25,
	author = {Wenwen Sun and
                  Zhicheng Pan and
                  Zirui Hu and
                  Yu Liu and
                  Chengcheng Yang and
                  Rong Zhang and
                  Xuan Zhou},
	title = {Rabbit: Retrieval-Augmented Generation Enables Better Automatic Database
                  Knob Tuning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3807--3820},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00284},
	doi = {10.1109/ICDE65448.2025.00284},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/SunPHLYZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The large language model (LLM)-based knob tuning method has attracted considerable attention due to its excellent in-context learning ability and generalizability. However, the existing LLM-based tuning methods do not effectively harmonize multi-source external knowledge, leading to missed opportunities for enhanced knob tuning. In light of this, we propose Rabbit, a novel approach that leverages Retrieval-augmented generation to enhance database knob tuning tools, which seamlessly integrates structured historical tuning experience with graph-encoded static knowledge. First, we introduce an experience-driven knob selection strategy, enhanced by dependency-aware external knowledge integration, to systematically select key knobs. Second, we develop a cutting-edge multi-agent knob domain pruning method, which ensures the reduced search space remains compact yet effective. Finally, we leverage the few-shot capabilities of LLMs to act as surrogate models, enabling rapid exploration of the pruned search space, followed by incremental optimization that expands the search space using historical insights. Moreover, we also design an adaptive strategy to transition between these two search spaces, striking an optimal balance between exploration and exploitation. Extensive experiments on well-established bench-marks demonstrate that Rabbit outperforms the state-of-the-art methods in both effectiveness and efficiency, pointing to a new paradigm for this area.}
}


@inproceedings{DBLP:conf/icde/HalimZWKGC25,
	author = {Sadaf Md. Halim and
                  Chen Zhao and
                  Xintao Wu and
                  Latifur Khan and
                  Christan Earl Grant and
                  Feng Chen},
	title = {Fairness-Aware Active Online Learning with Changing Environments},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3821--3834},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00285},
	doi = {10.1109/ICDE65448.2025.00285},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HalimZWKGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world applications, data-driven classifiers often grapple with a three-pronged challenge: data arrives in a continuous stream, most data in the wild are often unlabeled, and there is a critical need to maintain fairness in predictions across different sub-groups. Existing methods falter when addressing all these three factors concurrently. This work tackles this challenge by addressing a novel paradigm: Fairness-Aware Active Online Learning. We introduce a simple yet effective approach - FACTION, which actively selects the most crucial data points for labeling, going beyond traditional methods by considering both model uncertainty (epistemic uncertainty) and a newly introduced fairness notion derived from this very uncertainty. Additionally, FACTION leverages a system adept at identifying out-of-distribution samples within online learning environ-ments. Extensive evaluations on real-world datasets, coupled with theoretical analysis, demonstrate FACTION's effectiveness in handling this complex challenge. Our model demonstrably outperforms relevant baselines adapted for this new setting.}
}


@inproceedings{DBLP:conf/icde/QingZ25,
	author = {Yunfan Qing and
                  Wenli Zheng},
	title = {Towards Fine-Grained Scalability for Stateful Stream Processing Systems},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3835--3848},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00286},
	doi = {10.1109/ICDE65448.2025.00286},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/QingZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic scaling is critical to stream processing engines, as their long-running nature demands adaptive resource management. Existing scaling approaches easily cause performance degradation due to coarse-grained synchronization and inefficient state migration, resulting in system halt or high processing latency. In this paper, we propose DRRS, an on-the-fly scaling method that reduces performance overhead at the system level with three key innovations: (i) fine-grained scaling signals coupled with a re-routing mechanism that significantly mitigates propagation delay, (ii) a sophisticated record-scheduling mechanism that substantially reduces processing suspension, and (iii) subscale division, a mechanism that partitions migrating states into independent subsets, thereby reducing dependency-related overhead to enable finer-grained control and better runtime adaptability during scaling. DRRS is implemented on Apache Flink and, when compared to state-of-the-art approaches, reduces peak and average latencies by up to 81.1% and 95.5% respectively, while achieving a 72%-86% reduction in scaling duration, without disruption in non-scaling periods.}
}


@inproceedings{DBLP:conf/icde/ChenMR25,
	author = {Zixuan Chen and
                  Panagiotis Manolios and
                  Mirek Riedewald},
	title = {Synthesizing Scoring Functions for Rankings Using Symbolic Gradient
                  Descent},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3849--3862},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00287},
	doi = {10.1109/ICDE65448.2025.00287},
	timestamp = {Thu, 11 Sep 2025 20:25:49 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenMR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a relation and a ranking of its tuples, but no information about the ranking function, we are interested in synthesizing simple scoring functions that reproduce the ranking. Our system RankHow identifies linear scoring functions that minimize position-based error, while supporting flexible constraints on their weights. It is based on a new formulation as a mixed-integer linear program (MILP). While MILP is NP-hard in general, we show that RankHow is orders of magnitude faster than a tree-based algorithm that guarantees polynomial time complexity (PTIME) in the number of input tuples by reducing the MILP problem to many linear programs (LPs). We hypothesize that this is caused by 2 properties: First, the PTIME algorithm is equivalent to a naive evaluation strategy for the MILP program. Second, MILP solvers rely on advanced heuristics to reason holistically about the entire program, while the PTIME algorithm solves many sub-problems in isolation. To further improve RankHow's scalability, we propose a novel approximation technique called symbolic gradient descent (SYM-GD). It exploits problem structure to more quickly find local minima of the error function. Experiments demonstrate that RankHow can solve realistic problems, finding more accurate linear scoring functions than the state of the art.}
}


@inproceedings{DBLP:conf/icde/LiGSWL25,
	author = {Yiran Li and
                  Gongyao Guo and
                  Jieming Shi and
                  Sibo Wang and
                  Qing Li},
	title = {Efficient Integration of Multi-View Attributed Graphs for Clustering
                  and Embedding},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3863--3875},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00288},
	doi = {10.1109/ICDE65448.2025.00288},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiGSWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A multi-view attributed graph (MVAG) $\\mathcal{G}$ captures the diverse relationships and properties of real-world entities through multiple graph views and attribute views. Effectively utilizing all views in $\\mathcal{G}$ is essential for MVAG clustering and embedding, which are important for applications like recommendation systems, anomaly detection, social network analysis, etc. Existing methods either achieve inferior result quality or incur significant computational costs to handle large-scale MVAGs. In this paper, we present a spectrum-guided Laplacian aggregation scheme with an effective objective formulation and two efficient algorithms SGLA and SGLA+, to cohesively integrate all views of $\\mathcal{G}$ into an MVAG Laplacian matrix, which readily enables classic graph algorithms to handle $\\mathcal{G}$ with superior performance in clustering and embedding tasks. We begin by conducting a theoretical analysis to design an integrated objective that consists of two components, the eigengap and connectivity objectives, aiming to link the spectral properties of the aggregated MVAG Laplacian with the underlying community and connectivity properties of $\\mathcal{G}$. A constrained optimization problem is then formulated for the integration, which is computationally expensive to solve. Thus, we first develop the SGLA algorithm, which already achieves excellent performance compared with existing methods. To further enhance efficiency, we design SGLA+ to reduce the number of costly objective evaluations via sampling and approximation to quickly find an approximate optimum. Extensive experiments compare our methods against 12 baselines for clustering and 8 baselines for embedding on 8 multi-view attributed graphs, validating the superior performance of SGLA and SGLA+ in terms of result quality and efficiency. Compared with the most effective baselines, our methods are significantly faster, often by up to orders of magnitude. Our implementation is available at https://github.com/CyanideCentral/SGLA/.}
}


@inproceedings{DBLP:conf/icde/WuZLKXLYZLZ25,
	author = {Yang Wu and
                  Xuanhe Zhou and
                  Xiaoguang Li and
                  Jinhuai Kang and
                  Chunxiao Xing and
                  Tongliang Li and
                  Xinjun Yang and
                  Wenchao Zhou and
                  Feifei Li and
                  Yong Zhang},
	title = {MemQ: {A} Graph-Based Query Memory Prediction Framework for Effective
                  Workload Scheduling},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3876--3889},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00289},
	doi = {10.1109/ICDE65448.2025.00289},
	timestamp = {Wed, 10 Sep 2025 14:09:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WuZLKXLYZLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query memory prediction is an essential yet underexplored problem in self-driving databases, particularly for high-concurrency workload scheduling where efficient resource utilization is critical. Existing works mainly focus on cost and latency estimation (e.g., using plan representation learning), while memory prediction poses new challenges such as requiring (1) numerous memory-specific training data, (2) memory-relevant query plan featurization strategies, and (3) a prediction model suitable for capturing the complexities of memory usage in query operations. Moreover, most learning-based approaches do not consider transferability across different datasets and database systems. This paper introduces Mem Q Q Q , a graph-based memory prediction framework designed for effective workload scheduling. First, we build a comprehensive training dataset for memory prediction by executing diverse query workloads across multiple datasets and recording their diverse peak memory consumptions. Second, our MemQ model leverages operator-level features of query plans, achieving high prediction accuracy, compact model size, and fast training and inference times. Third, we integrate the MemQ model into memory-aware First Fit Decreasing (FFD) and Bidrectional Fit (BF) scheduling strategy to optimize resource utilization. Extensive experiments demonstrate the effectiveness of our homogeneous query plan graph model. Moreover, our FFD scheduling strategy reduces makespan (total query execution time) by up to 55% and decreases retry counts by over 99% compared to default strategies when batch executing analytical queries on PostgreSQL. Furthermore, our novel BF strategy reduces makespan by 15.17% and reduces sum of total time by 41.41% compared with FFD strategy when batch executing mixed workloads.}
}


@inproceedings{DBLP:conf/icde/SunZLTW25,
	author = {Pengcheng Sun and
                  Lan Zhang and
                  Jiandong Liu and
                  Chen Tang and
                  Jialiang Wang},
	title = {{\textdollar}{\textbackslash}mathrm\{E\}{\^{}}\{3\}{\textbackslash}text\{FS\}{\textdollar}:
                  Efficient, Secure, and Verifiable Fuzzy Search with Data Updates in
                  Hybrid-Storage Blockchains},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3890--3903},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00290},
	doi = {10.1109/ICDE65448.2025.00290},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SunZLTW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The hybrid-storage blockchain (HSB) facilitates flexible data sharing and search applications across decentralized clients. However, ensuring data privacy and result integrity, while enhancing query and result verification efficiency in HSB-based search applications over dynamic datasets, poses significant challenges. In this paper, we propose E3FS, the first efficient, secure, and verifiable search scheme over dynamically updatable datasets in HSB systems, supporting multi-keyword fuzzy search, an important search function. E3FS accelerates search and verification through an updatable hybrid index with an efficient on-chain process. This design integrates encrypted LSH-based Bloom filters for maintaining file keyword information and an inverted index linking each keyword to a novel authenticated index tree spanning multiple files. Lightweight digests of these trees are stored on-chain to assist with verification, achieving sublinear search and verification costs. Moreover, the framework guarantees forward privacy by securely updating and refreshing both on-chain and off-chain index with new secrets upon each data update. Experimental results demonstrate that our solution outperforms state-of-the-art methods, achieving at least  \\mathbf{58.6}\\times \\mathbf{58.6}\\times  faster search and  \\mathbf{34.4}\\times \\mathbf{34.4}\\times  faster verification while reducing communication overhead by approximately  \\mathbf{60}\\times \\mathbf{60}\\times .}
}


@inproceedings{DBLP:conf/icde/HuangJH25,
	author = {Jinbin Huang and
                  Zihan Jia and
                  Xin Huang},
	title = {Efficient Core Propagation Based Hierarchical Graph Clustering},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3904--3916},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00291},
	doi = {10.1109/ICDE65448.2025.00291},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangJH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Communities, formed by a subset of vertices that are densely connected to each other and loosely connected to outside community members, widely exist to represent functional modules in real-world complex systems. Most existing community detection and search methods aim at finding communities at one single level, neglecting the natural properties of overlapping and hierarchy in communities. Therefore, the discovery of hierarchical graph clustering (HGC) to find communities at different levels, which is particularly useful in many applications. However, existing HGC studies suffer from two significant limitations: 1) inefficiency over large-scale networks, and 2) generating too many levels of community hierarchy without distinguishing the hierarchy differences. To address the above limitations, we revisit the problem of hierarchical graph clustering and formulate the problem based on our proposed three important properties. To tackle it, we propose theoretical-guaranteed fast solutions, in terms of algorithm complexity and hierarchy levels. We first formulate our HGC problem to admit three key properties of hierarchical communities. Based on the natural hierarchical structure of $k$-core, we develop a simple and importantly useful technique of core propagation. The key idea of core propagation is to take each $k$-core as one seed of hierarchical communities and find disjoint communities within $k$-core using a linear-time algorithm of label propagation. We propose two core propagation approaches of top-down and bottom-up algorithms, in terms of different search directions of $k$-cores by increment and decrement on $k$, respectively. The top-down method can find a given level of hierarchical communities in $O(t m)$ time, where $t$ is an input of hierarchy levels and $m$ is the graph size. To dismiss the hardness of users' input hierarchy parameter $t$, the bottom-up algorithm is equipped with a well-designed strategy of auto-adjusting hierarchical levels based on the graph structure itself. We also develop the coreness weight-based label propagation to ensure the accurate label voting of compressed communities at low levels. The bottom-up method runs fast in $O(m \\log \\delta(G))$, where $\\log \\delta(G)$ is a small value of the maximum coreness in graph $G$. Extensive experiments conducted on real-world graphs with ground-truth HGCs validate the effectiveness and efficiency of our proposed core propagation methods against state-of-the-art methods. Two case studies on the world-wide flight network and the Hong Kong road network demonstrate the particular usage of our HGC methods.}
}


@inproceedings{DBLP:conf/icde/LvZZLGLYYY25,
	author = {Rui Lv and
                  Zaixi Zhang and
                  Kai Zhang and
                  Qi Liu and
                  Weibo Gao and
                  Jiawei Liu and
                  Jiaxia Yan and
                  Linan Yue and
                  Fangzhou Yao},
	title = {GraphPrompter: Multi-Stage Adaptive Prompt Optimization for Graph
                  In-Context Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3917--3930},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00292},
	doi = {10.1109/ICDE65448.2025.00292},
	timestamp = {Wed, 10 Sep 2025 14:09:51 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LvZZLGLYYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the k-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate sam-ples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8 %. Our code is released at https://ithub.com/karin0018/GraphPrompter.}
}


@inproceedings{DBLP:conf/icde/TangDZXWWH25,
	author = {Lei Tang and
                  Wensheng Dou and
                  Yingying Zheng and
                  Lijie Xu and
                  Wei Wang and
                  Jun Wei and
                  Tao Huang},
	title = {Proving Cypher Query Equivalence},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3931--3944},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00293},
	doi = {10.1109/ICDE65448.2025.00293},
	timestamp = {Wed, 10 Sep 2025 14:09:48 +0200},
	biburl = {https://dblp.org/rec/conf/icde/TangDZXWWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph database systems store graph data as nodes and relationships, and utilize graph query languages (e.g., Cypher) for efficiently querying graph data. Proving the equivalence of graph queries is an important foundation for optimizing graph query performance, ensuring graph query reliability, etc. Although researchers have proposed many SQL query equivalence provers for relational database systems, these provers cannot be directly applied to prove the equivalence of graph queries. The difficulty lies in the fact that graph query languages (e.g., Cypher) adopt significantly different data models (property graph model vs. relational model) and query patterns (graph pattern matching vs. tabular tuple calculus) from SQL. In this paper, we propose GraphQE, an automated prover to determine whether two Cypher queries are semantically equivalent. We design a U-semiring based Cypher algebraic representation to model the semantics of Cypher queries. Our Cypher algebraic representation is built on the algebraic structure of unbounded semirings, and can sufficiently express nodes and relationships in property graphs and complex Cypher queries. Then, determining the equivalence of two Cypher queries is transformed into determining the equivalence of the corresponding Cypher algebraic representations, which can be verified by SMT solvers. To evaluate the effectiveness of GraphQE, we construct a dataset consisting of 148 pairs of equivalent Cypher queries. Among them, we have successfully proven 138 pairs of equivalent Cypher queries, demonstrating the effectiveness of GraphQE.}
}


@inproceedings{DBLP:conf/icde/LiCSGY25,
	author = {Xiuwen Li and
                  Qifeng Cai and
                  Yang Shu and
                  Chenjuan Guo and
                  Bin Yang},
	title = {{AID-SQL:} Adaptive In-Context Learning of Text-to-SQL with Difficulty-Aware
                  Instruction and Retrieval-Augmented Generation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3945--3957},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00294},
	doi = {10.1109/ICDE65448.2025.00294},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiCSGY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent research in Text-to-SQL translation has primarily adopted in-context learning methods leveraging large language models (LLMs), achieving significant progress. However, these methods face challenges in adapting to natural language questions of varying difficulty and the relevance of the few-shot examples provided. In this paper, we propose an adaptive in-context learning approach with difficulty-aware instruction and retrieval-augmented generation to enhance the performance of Text-to-SQL translation (AID-SQL). First, we introduce adaptive instructions for LLMs, which employ precise difficulty classification to apply difficulty-adaptive generative guidelines and chain of thought (CoT) templates for varying difficulty levels. We automatically incorporate few-shot examples retrieved through the knowledge base into the CoT template to construct CoT-enhanced examples, which improves the capability of LLMs with retrieval-augmented generation (RAG). Furthermore, considering that current RAG methods struggle to effectively measure the contribution of retrieved examples in solving the specific task of Text-to-SQL translation, we train a ranking model that can better bridge the semantic and structural gap between NL questions and SQL queries. This approach can better understand semantic information and allows for retrieving examples that are more beneficial to the final problem-solving. We evaluate our method on five benchmarks. Our method achieves competitive performance compared with existing methods.}
}


@inproceedings{DBLP:conf/icde/XieZX25,
	author = {Qin Xie and
                  Qinghua Zhang and
                  Shuyin Xia},
	title = {Approximate Borderline Sampling Using Granular-Ball for Classification
                  Tasks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3958--3970},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00295},
	doi = {10.1109/ICDE65448.2025.00295},
	timestamp = {Wed, 10 Sep 2025 14:09:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XieZX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data sampling enhances classifier efficiency and robustness through data compression and quality improvement. Recently, the sampling method based on granular-ball (GB) has shown promising performance in generality and noisy clas-sification tasks. However, some limitations remain, including the absence of borderline sampling strategies and issues with class boundary blurring or shrinking due to overlap between GBs. In this paper, an approximate borderline sampling method using GBs is proposed for classification tasks. First, a restricted diffusion-based GB generation (RD-GBG) method is proposed, which prevents GB overlaps by constrained expansion, preserving precise geometric representation of GBs via redefined ones. Second, based on the concept of heterogeneous nearest neighbor, a GB-based approximate borderline sampling (GBABS) method is proposed, which is the first general sampling method capable of both borderline sampling and improving the quality of class noise datasets. Additionally, since RD-GBG incorporates noise detection and GBABS focuses on borderline samples, GBABS performs outstandingly on class noise datasets without the need for an opti-mal purity threshold. Experimental results demonstrate that the proposed methods outperform the GB-based sampling method and several representative sampling methods. Our source code is publicly available at https://github.com/CheryITse/GBABS.}
}


@inproceedings{DBLP:conf/icde/WangLYHGCY25,
	author = {Xinrui Wang and
                  Zilong Liu and
                  Shixin Ye and
                  Xin Huang and
                  Hong Gao and
                  Xiuzhen Cheng and
                  Dongxiao Yu},
	title = {With Anchors or Not: Fairness-Aware Truss-Based Community Search on
                  Attributed Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3971--3983},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00296},
	doi = {10.1109/ICDE65448.2025.00296},
	timestamp = {Fri, 17 Oct 2025 07:35:04 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangLYHGCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community search, which finds cohesive subgraphs containing given query vertices, has attracted much attention in decades. On attributed graphs, when considering the fairness of members' attributes in a community, the cohesiveness constraint of a clique is too strong, which often causes no fair clique based communities can be found. Thus, in this paper, we use the k-truss model, which is a relaxation of the clique but whose members have large engagement and high tie strength, to describe fair communities, namely fair k-truss communities (FTC) and anchored fair k-truss communities (AFTC, using anchored vertices to help satisfying the fairness constraint). We formulate the FTC and AFTC search problems to find the FTC or AFTC containing a given query vertex  q q  which has the largest  k k  and the smallest diameter. We prove the hardness of both problems. We develop several greedy algorithms and acceleration strategies to solve FTC and AFTC search problems. Experiments on 8 real-world networks show the significance of our FTC and AFTC models, and high performance of our algorithms and acceleration strategies.}
}


@inproceedings{DBLP:conf/icde/YuZLCL25,
	author = {Xintong Yu and
                  Rui Zhou and
                  Xiaofan Li and
                  Lu Chen and
                  Chengfei Liu},
	title = {Finding a Summary for All Maximal Bicliques},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3984--3997},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00297},
	doi = {10.1109/ICDE65448.2025.00297},
	timestamp = {Wed, 10 Sep 2025 14:09:49 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YuZLCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The number of bicliques in a bipartite graph may grow exponentially as its vertices increase. A biclique summary is a subset of all maximal bicliques and can somehow represent all maximal bicliques. In practical application scenarios, a summary helps users obtain more representative results. Due to its compact size, it enables users to efficiently locate and select the information they need. For instance, in the biomedical field, when researchers explore relationships between genes and proteins, they are often faced with an excessive number of combinations. Using a summary of these gene-protein relationships not only provides more representative insights but also significantly reduces the time needed for analysis. To find such representative maximal bicliques faster, we propose a method to determine whether to terminate the current search by computing lower bounds. We begin by introducing a baseline method, MBS, followed by two algorithms that incorporate bound pruning: MBSL, a neighborhood-based search algorithm, and MBSc, an  (\\alpha,\\beta) (\\alpha,\\beta) - core-based search algorithm. We also provide three strategies for optimizing the algorithms. They are the Upper Bound Deflation Pruning method, the Intersection Deflation Heuristic method, and the Lazy Lower Bound Evaluation method. Based on the above optimization strategies, we present the advanced algorithms MBSA and MBScA. In experiments, we demonstrate the efficiency and result quality of the proposed algorithms. After incorporating three optimization strategies, MBSA and MBScA show improvements in computation time compared to the baseline MBS and are able to generate smaller summaries.}
}


@inproceedings{DBLP:conf/icde/WeiXYLYCP25,
	author = {Hengfeng Wei and
                  Jiang Xiao and
                  Na Yang and
                  Si Liu and
                  Zijing Yin and
                  Yuxing Chen and
                  Anqun Pan},
	title = {Boosting End-to-End Database Isolation Checking via Mini-Transactions},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {3998--4010},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00298},
	doi = {10.1109/ICDE65448.2025.00298},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WeiXYLYCP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transactional isolation guarantees are crucial for database correctness. However, recent studies have uncovered numerous isolation bugs in production databases. The common black-box approach to isolation checking stresses databases with large, concurrent, randomized transaction workloads and verifies whether the resulting execution histories satisfy specified isolation levels. For strong isolation levels such as strict serializability, serializability, and snapshot isolation, this approach often incurs significant end-to-end checking overhead during both history generation and verification. We address these inefficiencies through the novel design of Mini-Transactions (MTs). MTs are compact, short transactions that execute much faster than general workloads, reducing overhead during history generation by minimizing database blocking and transaction retries. By leveraging MTs' read-modify-write pattern, we develop highly efficient algorithms to verify strong isolation levels in linear or quadratic time. Despite their simplicity, MTs are semantically rich and effectively capture common isolation anomalies described in the literature. We implement our verification algorithms and an MT workload generator in a tool called MTC. Experimental results show that MTC outperforms state-of-the-art tools in both history generation and verification. Moreover, MTC can detect bugs across various isolation levels in production databases while maintaining the effectiveness of randomized testing with general workloads, making it a cost-effective solution for black-box isolation checking.}
}


@inproceedings{DBLP:conf/icde/ZhangWZWLF25,
	author = {Ziqian Zhang and
                  Chaokun Wang and
                  Shuwen Zheng and
                  Cheng Wu and
                  Ziyang Liu and
                  Hao Feng},
	title = {Effective and Scalable Heterogeneous Graph Neural Network Framework
                  with Convolution-oriented Attention},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4011--4024},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00299},
	doi = {10.1109/ICDE65448.2025.00299},
	timestamp = {Mon, 10 Nov 2025 08:08:11 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhangWZWLF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The heterogeneous graph, as an effective representation of real-world data, encapsulates rich structural and semantic information. In recent years, numerous Heterogeneous Graph Neural Networks (HGNNs) have been proposed to learn node representations on heterogeneous graphs. Although existing methods have introduced various unique information aggregation and semantic fusion mechanisms, they still exhibit limitations in effectiveness and scalability. In this study, we introduce the gatekeeping theory in heterogeneous graph learning and investigate the primary challenges limiting current HGNNs. To address these challenges, we propose a novel, effective, and scalable heterogeneous graph neural network framework, the Heterogeneous Convolution-oriented Attention Network (HCAN). HCAN enhances the heterogeneous attention mechanism to learn far-sighted weights by encoding long-range relation information into node representation with a convolutional subgraph encoder. To further improve heterogeneous graph representation learning, we propose effective and scalable models based on the HCAN framework. We evaluate HCAN on various commonly used heterogeneous datasets and show that it outperforms the state-of-the-art methods, especially on challenging datasets.}
}


@inproceedings{DBLP:conf/icde/ChenCSWW25,
	author = {Xiaolei Chen and
                  Jia Chen and
                  Jie Shi and
                  Peng Wang and
                  Wei Wang},
	title = {{EPAS:} Efficient Online Log Parsing via Asynchronous Scheduling of
                  {LLM} Queries},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4025--4037},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00300},
	doi = {10.1109/ICDE65448.2025.00300},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenCSWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {System logs are critical for understanding the runtime behavior of information systems, yet their semi-structured nature poses challenges for effective utilization. Log parsing addresses this by transforming logs into structured representations, facilitating downstream tasks such as anomaly detection and failure analysis. Traditional approaches relying on statistical features or deep learning struggle with semantic understanding, efficiency, and adaptability to unseen data. Large language models (LLMs) offer new opportunities with their advanced semantic capabilities, yet current LLM-based log parsing methods are limited by sequential processing inefficiencies, suboptimal sampling strategies, and lack of robust template refinement mechanisms. To address these challenges, we propose EPAS (Efficient Parsing via Asynchronous Scheduling), a novel parser that utilizes asynchronous scheduling to optimize LLM- based log parsing. EPAS introduces three key innovations: a dynamic asynchronous mechanism to decouple LLM-based parsing from scheduling, maximizing efficiency; a controversy-based sampling mechanism to provide examples that help accurately parse challenging words; and an LLM-based validation task to semantically refine templates with minimal overhead. Experiments on benchmark datasets demonstrate that EPAS significantly outperforms state-of-the-art methods, achieving over a 360% improvement in parsing efficiency while maintaining high accuracy. In addition, EPAS achieves the best average performance across all four metrics, highlighting its effectiveness in both efficiency and accuracy.1}
}


@inproceedings{DBLP:conf/icde/MoustafaKG25,
	author = {Samir Moustafa and
                  Nils M. Kriege and
                  Wilfried N. Gansterer},
	title = {Efficient Mixed Precision Quantization in Graph Neural Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4038--4052},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00301},
	doi = {10.1109/ICDE65448.2025.00301},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MoustafaKG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have become essential for handling large-scale graph applications. However, the computational demands of GNNs necessitate the development of efficient methods to accelerate inference. Mixed precision quantization emerges as a promising solution to enhance the efficiency of GNN architectures without compromising prediction performance. Compared to conventional deep learning architectures, GNN layers contain a wider set of components that can be quantized, including message passing functions, aggregation functions, update functions, the inputs, learnable parameters, and outputs of these functions. In this paper, we introduce a theorem for efficient quantized message passing to aggregate integer messages. It guarantees numerical equality of the aggregated messages using integer values with respect to those obtained with full (FP32) precision. Based on this theorem, we introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which flexibly selects effective integer bit-widths for all components within GNN layers. Our approach systematically navigates the wide set of possible bit-width combinations, addressing the challenge of optimizing efficiency while aiming at maintaining comparable prediction performance. MixQ-GNN integrates with existing GNN quantization methods, utilizing their graph structure advantages to achieve higher prediction performance. On average, MixQ-GNN achieved reductions in bit operations of 5.5x for node classification and 5.1x for graph classification compared to architectures represented in FP32 precision.}
}


@inproceedings{DBLP:conf/icde/YanZQYCXLZYZ25,
	author = {Lingsen Yan and
                  Bolong Zheng and
                  Junjie Qing and
                  Wenlong You and
                  Tingyang Chen and
                  Zhi Xu and
                  Shuncheng Liu and
                  Kai Zeng and
                  Tao Ye and
                  Xiaofang Zhou},
	title = {Anomaly Diagnosis with Siamese Discrepancy Networks in Distributed
                  Cloud Databases},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4053--4065},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00302},
	doi = {10.1109/ICDE65448.2025.00302},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YanZQYCXLZYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly diagnosis is a fundamental problem in operation and maintenance of distributed cloud databases. Existing deep learning based methods solve this problem by classifying the anomalies with different root causes. However, since anomalies seldom occur, and anomalies with the same root cause may exhibit significantly different behaviors across different cloud database clusters, existing methods often lack sufficient training data, and they cannot generalize well from some clusters to others. Therefore, we take both anomaly and normal data into consideration, based on an observation that the discrepancy between the anomaly and normal data is relatively consistent compared to the behaviours of anomalies themselves. We design a Siamese Discrepancy Network (SDN) to learn representations of such discrepancy under the case that only a small amount of training data is available. In addition, a discrepancy-based diagnosis paradigm is proposed to construct training data for SDN and diagnose based on representations of discrepancy learned by SDN. Finally, we develop an anomaly interpretation method based on SDN, which accurately locates the symptom KPIs and root cause KPIs. Extensive experiments are conducted on both synthetic and real-world datasets. The experimental results demonstrate that the proposed method outperforms existing methods with respect to anomaly diagnosis and anomaly interpretation. In particular, the anomaly diagnosis framework has already been applied in Huawei's GaussDB (DWS) system.}
}


@inproceedings{DBLP:conf/icde/ChenZLFXYZYZ25,
	author = {Tingyang Chen and
                  Bolong Zheng and
                  Shuncheng Liu and
                  Zhujiong Fan and
                  Zhi Xu and
                  Lingsen Yan and
                  Kai Zeng and
                  Tao Ye and
                  Xiaofang Zhou},
	title = {Compatible Unsupervised Anomaly Detection with Multi-Perspective Spatio-Temporal
                  Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4066--4078},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00303},
	doi = {10.1109/ICDE65448.2025.00303},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenZLFXYZYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection is one of the most significant tasks in industrial automatic maintenance, such as in distributed cloud systems. However, the implementation of existing anomaly detection methods is still challenging in (i) capturing the complex spatial and temporal correlations of multivariate time series, (ii) effectively adapting to the unsupervised condition, and (iii) generalizing across nodes in distributed systems. To address these challenges, we design a multi-perspective spatio-temporal attention model, called STAMP, which consists of a prediction module ST-ATTN, a reconstruction module AutoEncoder, and an adversarial optimizing module. Specifically, ST-ATTN leverages multiple attention mechanisms to perform spatio-temporal learning from both local and global perspectives, AutoEncoder is utilized to fit implicit representations, and the adversarial optimization module employs a min-max training strategy to enhance the learning capability. By introducing pre-training strategies, STAMP can be effectively adapted to distributed systems with a strong generalization ability. Furthermore, to cope with the practical unlabeled data conditions, we propose an unsupervised framework compatible with not only STAMP but also other advanced detection models. In this framework, a screening process is first conducted by traditional methods to generate a training set of pseudo-normal samples. Second, the models are trained and then used for detection. The framework can be further optimized by performing feature selection based on model-derived information for a better detectability. Extensive experiments in real-world datasets demonstrate that the proposed model and framework achieve superior performance when compared with baselines under both semi-supervised and unsupervised conditions. In particular, the detection framework has already been applied in Huawei's GaussDB (DWS) system.}
}


@inproceedings{DBLP:conf/icde/HouZT25,
	author = {Wenzhe Hou and
                  Xiang Zhao and
                  Bo Tang},
	title = {OptMatch: An Efficient and Generic Neural Network-Assisted Subgraph
                  Matching Approach},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4079--4091},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00304},
	doi = {10.1109/ICDE65448.2025.00304},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HouZT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The graph has been widely used to model the entities and the relationships among them in real-world applications. Subgraph matching is a core operation in graph data analysis. However, existing exact matching methods may incur high cost as their searched branches are always unpromising. In recent years, several approximate matching solutions have been proposed by exploiting neural networks. Nevertheless, the accuracy of the returned approximate results could be improved significantly. Motivated by these observations, we proposed OptMatch, an efficient and generic neural network-assisted subgraph matching approach, in this work. In particular, OptMatch proposes a novel subgraph partial embedding network and implements carefully designed search strategies to optimize search processes during the subgraph matching process. First, it can be used to accelerate existing exact matching methods. Moreover, it is also an approximate matching solution, which offers better accuracy compared to existing approximate solutions. We conduct exten-sive experiments on seven real-world data graphs to demonstrate the superiority of OptMatch in both exact and approximate subgraph matching.}
}


@inproceedings{DBLP:conf/icde/LiSCZZSPTL25,
	author = {Xiaodong Li and
                  Jiawei Sheng and
                  Jiangxia Cao and
                  Xinghua Zhang and
                  Wenyuan Zhang and
                  Yong Sun and
                  Shirui Pan and
                  Zhihong Tian and
                  Tingwen Liu},
	title = {Personalized Multi-Interest Modeling for Cross-Domain Recommendation
                  to Cold-Start Users},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4092--4105},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00305},
	doi = {10.1109/ICDE65448.2025.00305},
	timestamp = {Wed, 10 Sep 2025 14:09:56 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiSCZZSPTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain recommendation (CDR) has demon-strated to be an effective solution for alleviating the user cold-start issue. By leveraging rich user-item interactions available in a richly informative source domain, CDR could improve the recommendation performance for cold-start users in the target domain. Previous CDR approaches mostly adhere the Embedding and Mapping (EMCDR) paradigm, which learns a user-shared mapping function to transfer users' preference from the source domain to the target domain, neglecting users' personalized preference. Recent CDR approaches further leverage the meta-learning paradigm, considering the CDR task for each user independently and learning user-specific mapping functions for each user. However, they mostly learn representations for each user individually, which ignores the common preference between different users, neglecting valuable information for CDR. In addition, all these approaches usually summarize the user's preference into an overall representation, which can hardly capture the user's multi-interest preference. To this end, we propose a personalized multi-interest modeling framework for CDR to cold-start users, termed as NF-NPCDR. Specifically, we propose a personalized preference encoder that enhances the neural process (NP) with the normalizing flow (NF) to convert the Gaussian (unimodal) distribution to a multimodal distribution, providing a novel way to capture the user's personalized multi-interest preference. Then, we propose a common preference encoder with a preference pool to capture the common preference between different users. Furthermore, we introduce a stochastic adaptive decoder to incorporate both the personalized and common preference for cold-start users, adaptively modulating both preference for better recommendation. Experimental evalu-ations demonstrate that NF-NPCDR outperforms previous SOTA approaches in five benchmark CDR scenarios.}
}


@inproceedings{DBLP:conf/icde/ChengJXW25,
	author = {Dongdong Cheng and
                  Xiaocui Jiang and
                  Shuyin Xia and
                  Guoyin Wang},
	title = {Pseudo-label-Based Unsupervised Granular-Ball Division and Fast Spectral
                  Clustering for High-Dimensional Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4106--4119},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00306},
	doi = {10.1109/ICDE65448.2025.00306},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChengJXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the swift advancement of information technology, vast amounts of high-dimensional data have accumulated across various domains. Clustering such data presents a significant challenge, as existing methods often suffer from slow execution speeds and reduced clustering accuracy. To tackle these issues, we introduce the granular-ball approach, which aims to decrease the number of sample points and enhance processing speed, while also improving clustering accuracy through feature selection. Granular-ball computing, a coarse-grained data representation technique, has demonstrated its advantages in enhancing classification and clustering models in recent studies. However, current granular-ball division techniques are inadequate for high-dimensional data. To confront the complexities arising from clustering high-dimensional data and improve upon existing granular-ball methods, this paper proposes a novel granular-ball division approach that leverages pseudo-labels and feature selection. This new method enables the identification of anchor points through an improved granular-ball division process, leading to the development of a fast spectral clustering algorithm for high-dimensional data, termed PLGB-FSC. Specifically, we initially employ weighted K-Means for feature to generate pseudo-labels. Subsequently, we conduct a primary stage of feature selection by utilizing the mutual information between pseudo-labels and features, thereby eliminating the interference caused by irrelevant features. We further refine the feature selection by combining standard deviation and pearson correlation coefficients to choose mutually independent features. Using these pseudo-labels, we then perform granular-ball division to obtain anchor points. Lastly, we construct a similarity matrix between all sample points and the anchor points, and leveraging spectral clustering for definitive clustering outcomes. Experimental evaluations reveal that PLGB-FSC surpasses state-of-the-art algorithms such as W-KMeans, WGB, GB-USC, RC-PCA-SC, GLUFC, FGOC, SFESA, SPCAFS, and LLSRFS, and it achieves higher accuracy and faster execution speed. The source code is available at https://github.com/DongdongCheng/PLGB-FSC.}
}


@inproceedings{DBLP:conf/icde/HeHDLQWFZX25,
	author = {Tianqi He and
                  Xiaohan Huang and
                  Yi Du and
                  Qingqing Long and
                  Ziyue Qiao and
                  Min Wu and
                  Yanjie Fu and
                  Yuanchun Zhou and
                  Meng Xiao},
	title = {Fastft: Accelerating Reinforced Feature Transformation via Advanced
                  Exploration Strategies},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4120--4133},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00307},
	doi = {10.1109/ICDE65448.2025.00307},
	timestamp = {Fri, 28 Nov 2025 08:07:19 +0100},
	biburl = {https://dblp.org/rec/conf/icde/HeHDLQWFZX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature Transformation is crucial for classic machine learning that aims to generate feature combinations to enhance the performance of downstream tasks from a data-centric perspective. Current methodologies, such as manual expert-driven processes, iterative-feedback techniques, and exploration-generative tactics, have shown promise in automating such data engineering workflow by minimizing human involvement. However, three challenges remain in those frameworks: (1) It predominantly depends on downstream task performance metrics, as assessment is time-consuming, especially for large datasets. (2) The diversity of feature combinations will hardly be guaranteed after random exploration ends. (3) Rare significant transformations lead to sparse valuable feedback that hinders the learning processes or leads to less effective results. In response to these challenges, we introduce FASTFT, an innovative framework that leverages a trio of advanced strategies. We first decouple the feature transformation evaluation from the outcomes of the generated datasets via the performance predictor. To address the issue of reward sparsity, we developed a method to evaluate the novelty of generated transformation sequences. Incorporating this novelty into the reward function accelerates the model's exploration of effective transformations, thereby improving the search productivity. Additionally, we combine novelty and performance to create a prioritized memory buffer, ensuring that essential experiences are effectively revisited during exploration. Our extensive experimental evaluations validate the performance, efficiency, and traceability of our proposed framework, showcasing its superiority in handling complex feature transformation tasks11The code and data are publicly accessible via Github..}
}


@inproceedings{DBLP:conf/icde/HeWHGYZSZDRZ25,
	author = {Yongjun He and
                  Roger Waleffe and
                  Zhichao Han and
                  Johnu George and
                  Binhang Yuan and
                  Zitao Zhang and
                  Yinan Shan and
                  Yang Zhao and
                  Debojyoti Dutta and
                  Theodoros Rekatsinas and
                  Ce Zhang},
	title = {{MLKV:} Efficiently Scaling up Large Embedding Model Training with
                  Disk-based Key-Value Storage},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4134--4141},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00308},
	doi = {10.1109/ICDE65448.2025.00308},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HeWHGYZSZDRZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many modern machine learning (ML) methods rely on embedding models to learn vector representations (embeddings) for a set of entities (embedding tables). As increasingly diverse ML applications utilize embedding models and embedding tables continue to grow in size and number, there has been a surge in the ad-hoc development of specialized frameworks targeted to train large embedding models for specific tasks. Although the scalability issues that arise in different embedding model training tasks are similar, each of these frameworks independently reinvents and customizes storage components for specific tasks, leading to substantial duplicated engineering efforts in both development and deployment. This paper presents MLKV, an efficient, extensible, and reusable data storage framework designed to address the scalability challenges in embedding model training, specifically data stall and staleness. MLKV augments disk-based key-value storage by democratizing optimizations that were previously exclusive to individual specialized frameworks and provides easy-to-use interfaces for embedding model training tasks. Extensive experiments on open-source workloads, as well as applications in eBay's payment transaction risk detection and seller payment risk detection, show that MLKV outperforms offloading strategies built on top of industrial-strength key-value stores by 1.6-12.6 ×. MLKV is open-source at https://github.com/llm-db/MLKV.}
}


@inproceedings{DBLP:conf/icde/LiGLCHX25,
	author = {Rui Li and
                  Pengyuan Gao and
                  Haihan Li and
                  Ling Chai and
                  Shaohao Huang and
                  Ting Xie},
	title = {A Bilateral Perspective for Modeling Real-Time Traffic Trends in Live-Streaming
                  Recommendation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4142--4155},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00309},
	doi = {10.1109/ICDE65448.2025.00309},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiGLCHX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the live-streaming industry has grown significantly and become a crucial form of online interaction and marketing. Douyin has emerged as one of the largest information streaming platforms. Personalized recommendation, effectively matching users' interests with relevant content, is essential for the prosperity of live streaming and short videos. Unlike short-video recommendations, live-streaming necessitates a bilateral optimization framework that jointly considers real-time content evolution and mutual influence between streamers and audiences. However, existing approaches predominantly focus on static user-item modeling or unilateral consumer-side metrics, failing to capture the spatiotemporal dependencies in multi-behavior traffic or address the interdependence between production and consumption. To bridge these gaps, we propose a novel paradigm for live-streaming recommendation from a bilateral perspective. First, we design Self Flow, a real-time data-streaming engine that generates multi-behavior traffic sequences at minute-level granularity, redefining traffic efficiency metrics to align streamer incentives with audience engagement. Leveraging this infrastructure, we present the Time-Slice level Spatial-Temporal Fusion Network (TSSTFN), which integrates spatial-temporal fusion module to model cross-behavior dependencies and temporal evolution patterns simultaneously. Crucially, we design deployment strategies from a bilateral perspective to optimize the experiences of both the consumption side and the supply side simultaneously. Extensive experiments on Douyin platforms demonstrate significant bilateral improvements: +1.173%/+0.721% watch duration (Douyin/Douyin Lite) for audiences and + 1.037% live duration for streamers. Our fully-deployed solution proves that bilateral value alignment fosters sustainable growth for both content creators and consumers.}
}


@inproceedings{DBLP:conf/icde/WuMLTZCHZZ25,
	author = {Wenyu Wu and
                  Jiali Mao and
                  Jiafan Liu and
                  Yixiao Tong and
                  Lisheng Zhao and
                  Shaosheng Cao and
                  Jilin Hu and
                  Aoying Zhou and
                  Lin Zhou},
	title = {CDMap: Complementarity and Disparity-aware Map Inference Quality Enhancement},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4156--4168},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00310},
	doi = {10.1109/ICDE65448.2025.00310},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WuMLTZCHZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the high coverage and low cost nature of trajectory data, an increasing number of works have utilized trajectory data to infer maps. Nevertheless, limited by the sparse trajectories in some areas and intermingled trajectories on parallel roads, the existing inferring methods still face a high missed detection rate of the roads. In view of that, we propose a Complementarity and Disparity-aware Map Inference Framework, called CDMap, consisting of grid dual feature extraction, contextual road difference-embedded grid representation, dual feature complementary network-based road topology prediction and parallel roads disparity-enhanced model optimization. To improve the prediction accuracy of the roads in areas with sparse trajectories, we extract point-wise features and segment-wise features separately for the grids, then design a dual feature complementary network to adaptively model the importance of both types of features in different road scenarios. Further, to proliferate the detection accuracy of parallel roads, we incorporate the contextual roads' differences between parallel roads into grid representations, then put forward a parallel roads disparity-enhanced model optimization strategy. Extensive comparative experiments conducted on three real-world datasets demonstrate the superiority of CDMap over the state-of-the-art methods, especially by achieving the most significant reduction in missed detection rate (30.23%) on the trajectory data collected from DidiChuxing platform.}
}


@inproceedings{DBLP:conf/icde/WangLQLGZZZZ25,
	author = {Haoyu Wang and
                  Zhicheng Liu and
                  Yeliang Qiu and
                  Haozhe Li and
                  Hongke Guo and
                  Zhaoliang Zhu and
                  You Zhang and
                  Yu Zhou and
                  Xudong Zheng},
	title = {Stability is Not Downtime: Comprehensive Stability Evaluation for
                  Large-Scale Cloud Servers in Alibaba Cloud},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4169--4182},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00311},
	doi = {10.1109/ICDE65448.2025.00311},
	timestamp = {Sat, 15 Nov 2025 13:46:28 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WangLQLGZZZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the most critical cloud products, the stability of cloud servers is of paramount importance. Traditionally, stability is evaluated based on downtime, i.e., the duration when cloud servers are unavailable. However, through our extensive engagement in stability-related work, we have discovered that only 27% of stability issues are related to unavailability, which is in fact a subset of stability. Therefore, in this paper, we propose the Comprehensive Damage Indicator (CDI), comprising three distinct sub-metrics: the Unavailability Indicator, Performance Indicator, and Control-plane Indicator. The CDI is able to evaluate the stability of large-scale cloud servers more comprehensively. In the production environment of Alibaba Cloud, the CDI has been implemented on top of Apache Spark. Over the past two years, we have employed the CDI to guide our stability-related works, including architecture comparison, potential problem detection and operation action optimization. The overall results are very favorable. Taking Fiscal Year 2024 as an example, the three sub-metrics have respectively decreased by 40%, 80%, and 35%.}
}


@inproceedings{DBLP:conf/icde/XuSYLWHWZ25,
	author = {Quanqing Xu and
                  Wei Sun and
                  Chuanhui Yang and
                  Jinlong Liu and
                  Ziyun Wei and
                  Fusheng Han and
                  Liang Wang and
                  Xiaowei Zhai},
	title = {OceanBase Unitization: Building the Next Generation of Online Map
                  Applications},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4183--4196},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00312},
	doi = {10.1109/ICDE65448.2025.00312},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XuSYLWHWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed database systems are extensively utilized to provide cloud services for online map platforms, offering consistency, disaster recovery, and high performance, whereas traditional systems relying on singly-homed architecture face challenges in scaling for large-scale services. In this paper, we propose the architectural design of OceanBase (OB), a distributed database system that “unitizes” services and operations into individual machines. The unitization approach migrates from a singly-homed to a multi-homed design across multiple regions. By leveraging this feature, OceanBase ensures data replication and seamless service handover when a machine goes offline. However, communication overhead between regions can sometimes be burdensome. To address this issue, OceanBase unitizes read and write operations, and employs a hybrid centralization and unitization approach that is dynamically optimized for Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP). To validate our design, we deploy OceanBase on AMap, an online map application platform supporting large-scale distributed services. Through a series of experiments, we demonstrate that OceanBase exhibits enhanced disaster tolerance capabilities and achieves improved performance for both write-intensive and read-intensive benchmarks.}
}


@inproceedings{DBLP:conf/icde/ZhengLYCZZ25,
	author = {Miao Zheng and
                  Hao Liang and
                  Fan Yang and
                  Bin Cui and
                  Zenan Zhou and
                  Wentao Zhang},
	title = {{PAS:} Plug-and-Play Prompt Augmentation System},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4197--4210},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00313},
	doi = {10.1109/ICDE65448.2025.00313},
	timestamp = {Mon, 03 Nov 2025 20:25:59 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhengLYCZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the rise of Large Language Models (LLMs) has spurred a growing demand for plug-and-play AI systems. Among the various AI techniques, prompt engineering stands out as particularly significant. However, users often face challenges in writing prompts due to the steep learning curve and significant time investment, and existing automatic prompt engineering (APE) models can be difficult to use. To address this issue, we propose PAS, an LLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality, automatically generated prompt augmentation datasets, resulting in exceptional performance. In comprehensive benchmarks, PAS achieves state-of-the-art (SOTA) results compared to previous APE models, with an average improvement of 6.09 points. Moreover, PAS is highly efficient, achieving SOTA performance with only 9000 data points. Additionally, PAS can autonomously generate prompt augmentation data without requiring additional human labor. Its flexibility also allows it to be compatible with all existing LLMs and applicable to a wide range of tasks. Moreover, we deployed PAS for Baichuan online model, and then tested PAS using internal human evaluations in Baichuan underscoring its strong performance. This combination of high performance, efficiency, and flexibility makes PAS a valuable system for enhancing the usability and effectiveness of LLMs through automatic prompt engineering. The codebase is available at https://github.com/PKU-Baichuan-MLSystemLab/PAS.}
}


@inproceedings{DBLP:conf/icde/HuLPK25,
	author = {Jack Hu and
                  Eric Lee and
                  Prashanth Purnananda and
                  Hanuma Kodavalla},
	title = {Scaling and Hardening {XLOG:} The {SQL} Azure Hyperscale Log Service},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4211--4221},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00314},
	doi = {10.1109/ICDE65448.2025.00314},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuLPK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {XLOG is the centralized log service of the SQL Azure Hyperscale (Hyperscale) distributed database-as-a-service (DBaaS) within Microsoft Azure. It is responsible for disseminating transaction log to all clients within the distributed database, such as Page Servers and secondary Compute replicas. As the size of the Hyperscale database increases, the number of XLOG clients also increases, thus presenting scalability challenges in the request handling and IO subsystem. This paper describes how to address scalability challenges by applying various techniques. To solve the thread exhaustion problem, a coroutine-based asynchronous log processing framework was implemented, allowing better management of long-polling requests and eliminating thread starvation. The XLOG Pool, a centralized I/O management component, was introduced to reduce redundant I/Os and improve cold start performance by consolidating log reads across clients. A RateLimiter using a token bucket algorithm was introduced to prevent throttling by Azure storage. To ensure data integrity, several layers of validation, including egress and end-to-end checksum validation, were added to detect and prevent data corruption. Performance evaluations showed significant improvements in log processing and read rates with the new asynchronous framework, XLOG Pool and RateLimiter supporting larger databases more efficiently, enabling Hyperscale to support up to 128 TB size databases in production. This paper illustrates how a real-world cloud database service, responsible for hosting mission-critical applications and managing hundreds of petabytes of data, innovates its logging service to enhance scalability, reliability, and cost efficiency.}
}


@inproceedings{DBLP:conf/icde/RamosPKGAAS25,
	author = {Rog{\'{e}}rio Ramos and
                  Prashanth Purnananda and
                  Hanuma Kodavalla and
                  Chaitanya Gottipati and
                  Harshil Ambagade and
                  Ankit Anvesh and
                  Srikanth Sampath},
	title = {Hyperscale Resilient Buffer Pool Extension in Azure {SQL} Database},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4222--4233},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00315},
	doi = {10.1109/ICDE65448.2025.00315},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/RamosPKGAAS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Azure SQL DB offers disaggregated storage architecture called Hyperscale. While this architecture provides storage scale out, it comes at the cost of performance of I/O from remote storage. In-memory caches on the Compute nodes are small and lost on process restarts. This paper introduces Resilient Buffer Pool Extension (RBPEX) which is a persistent cache present on both compute and storage nodes. These caches significantly improve performance of I/O from Compute while at the same time maintaining correctness. This paper presents the architecture of RBPEX and how it stores the most relevant pages in an efficient and correct way. It uses innovative techniques like statistics, selective caching and reduction of small writes to improve performance in Hyperscale. It also presents details about handling multiple versions of pages from storage to maintain correctness.}
}


@inproceedings{DBLP:conf/icde/LuNLPZZCZDCZ25,
	author = {Keer Lu and
                  Xiaonan Nie and
                  Zheng Liang and
                  Da Pan and
                  Shusen Zhang and
                  Keshi Zhao and
                  Weipeng Chen and
                  Zenan Zhou and
                  Guosheng Dong and
                  Bin Cui and
                  Wentao Zhang},
	title = {DataSculpt: {A} Holistic Data Management Framework for Long-Context
                  LLMs Training},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4234--4247},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00316},
	doi = {10.1109/ICDE65448.2025.00316},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuNLPZZCZDCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, foundation models, particularly large language models (LLMs), have demonstrated significant improvements across a variety of tasks. One of their most important features is long-context capability, which enables them to generate extended text with high semantic coherence, retrieving relevant information, and handling tasks with substantial amounts of text efficiently. The key to improving long-context performance lies in effective data organization and management strategies that integrate data from multiple domains and optimize the context window during training. Through extensive experimental analysis, we identified three key challenges in designing effective data management strategies that enable the model to achieve long-context capability without sacrificing performance in other tasks: (1) a shortage of long documents across multiple domains, (2) effective construction of context windows, and (3) efficient organization of large-scale datasets. To address these challenges, we introduce DataSculpt, a novel data management framework designed for long-context training. We first formulate the organization of training data as a multi-objective optimization problem, focusing on attributes including the relevance among documents within the same training sequence, the quantity of concatenated instances, individual document integrity, and computational cost. Specifically, our approach utilizes a coarse-to-fine method to optimize training data organization effectively. We begin by clustering the data based on semantic similarity (coarse), followed by a multi-objective greedy search within each cluster to score and concatenate documents into various context windows (fine). We have deployed DataSculpt as the data management backend for long-context training in Baichuan Inc. Extensive experiments with diverse downstream tasks show that DataSculpt enhances the model's long-context performance by an average of 15.73%, while maintaining the general capabilities with a 4.63% improvement.}
}


@inproceedings{DBLP:conf/icde/PengFCMFZW25,
	author = {Xiaoshuang Peng and
                  Xiaopeng Fan and
                  Shi Cheng and
                  Lingbin Meng and
                  Cuiyun Fu and
                  Wenchao Zhou and
                  Chuliang Weng},
	title = {{BBS:} Batch-Based Snapshot for the Cloud Database Backup},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4248--4261},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00317},
	doi = {10.1109/ICDE65448.2025.00317},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PengFCMFZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many cloud databases provide fine-grained regular snapshots and sparsely deleted snapshots based on importance, and dynamically maintain large-scale snapshots to ensure data security and mine the value of cold data. However, in existing snapshot technologies, the write amplification feature of Copy-on-Write (CoW) introduces additional expensive I/O operations in a cloud environment. In Redirect-on-Write (RoW), the modified data blocks are scattered among the snapshots, resulting in a dependency between the snapshots, which seriously affects the recovery performance. In this paper, we observed that access to snapshots has the characteristics of locality and continuity. We therefore propose an efficient Batch-Based Snapshot index, called BBS, which batches snapshot indexes according to database workload and access behavior of snapshots. Specifically, we use two key techniques: Shared-Subtrees Indexing and Batch-Based Dividing, to perform split dependency of the snapshot index. The snapshot index dependency chain is divided into batches, and there is no dependency on snapshot indexes between batches. In-batch snapshot indexes reduce memory overhead by sharing subtrees. The index can directly locate data blocks instead of iterative traversal. At the same time, the design of the snapshot index deletion method is adapted to the snapshot sparse deletion model. We have implemented a working system in Ceph. Evaluation results on datasets demonstrate that, compared with existing techniques, BBS can effectively balance the overhead between index memory capacity and recovery time.}
}


@inproceedings{DBLP:conf/icde/HuangE25,
	author = {Hao Huang and
                  Scott Evans},
	title = {FlowFill: {A} Fast and Energy-Efficient Streaming Framework for Recovering
                  Missing Values in Industrial Sensor Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4262--4268},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00318},
	doi = {10.1109/ICDE65448.2025.00318},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Industrial systems such as wind turbines and nuclear facilities depend on continuous sensor monitoring to ensure operational safety and efficiency. However, missing data—caused by sensor malfunctions or environmental interference—can compromise real-time analyses, delaying critical maintenance decisions and system optimizations. Existing imputation methods often struggle to efficiently handle large data volumes in a streaming manner, typically requiring full access to historical time series and consuming excessive energy and processing time. To address these limitations, we propose FlowFill, a fast and energy-efficient streaming framework for recovering missing values in industrial sensor data. FlowFill leverages multiple dynamic sketches to capture local temporal patterns and encodes long-term and diverse data characteristics. Operating incrementally, FlowFill updates its sketches with new data samples, ensuring real-time adaptability. An attention mechanism dynamically prioritizes relevant sketches, dynamically weighting their contributions for precise imputations. This framework excels in computational efficiency, resilience to diverse missing patterns, and energy conservation, making it ideal for industrial applications that demand high-speed processing and energy efficiency. Experiments on industrial datasets demonstrate that FlowFill surpasses state-of-the-art methods in both accuracy and efficiency, establishing a new benchmark for streaming imputation in industrial contexts.}
}


@inproceedings{DBLP:conf/icde/ChaiBCGSZ25,
	author = {Andrew Chai and
                  Alexander Bianchi and
                  Vincent Corvinelli and
                  Parke Godfrey and
                  Jarek Szlichta and
                  Calisto Zuzarte},
	title = {{GEX:} Guiding Expert Tuning with eXplainable {AI}},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4269--4276},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00319},
	doi = {10.1109/ICDE65448.2025.00319},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ChaiBCGSZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern database systems, such as IBM Db2, rely on cost-based optimizers to improve workload performance. However, their decision-making processes are difficult to interpret. Tuning them for specific workloads remains challenging due to their complexity and numerous configuration options. Automatic tuning tools often rely on black-box machine-learning models, which lack interpretability, hindering expert trust and debugging. We present GEX, a system that provides interpretable insights into database optimizer behavior using explainable AI techniques. By employing saliency maps generated from surrogate models, GEX guides experts in system tuning tasks such as statistical view creation, configuration parameter adjustment, and query rewrite. Our experimental results demonstrate that GEX enhances performance and ensures transparency, addressing key challenges in data systems tuning.}
}


@inproceedings{DBLP:conf/icde/MagnanimiCBBCM25,
	author = {Davide Magnanimi and
                  Andrea Colombo and
                  Luigi Bellomarini and
                  Anna Bernasconi and
                  Stefano Ceri and
                  Davide Martinenghi},
	title = {Enabling Light-Weight Reasoning via Cypher Triggers},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4277--4290},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00320},
	doi = {10.1109/ICDE65448.2025.00320},
	timestamp = {Tue, 14 Oct 2025 19:36:22 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MagnanimiCBBCM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deductive rules over graph data are a commonly accepted way to address complex reasoning tasks; among them, we mention the company control problem, which consists of determining who exercises control - directly or indirectly, through aggregation and recursion - over ownership graphs. Solving this and similar problems is crucial for the Central Bank of Italy; the Bank uses Vadalog, a state-of-the-art proprietary reasoner based on an extended Datalog, to routinely manage changes (insertions and deletions) of ownership in large graphs covering all Italian companies. However, at a smaller scale, similar activities are also relevant in more targeted activities, e.g., for financial intelligence tasks in the public and private sectors. In this paper, we present a general scheme for generating active rules that correctly handle recursion, aggregation, and stratified negation, so as to deploy reactive reasoners over graph data managers. We show how to convert high-level reasoning rules expressed in Datalog into triggers as Cypher statements, the most aligned language with the recently standardized Graph Query Language. We discuss how Cypher triggers can be managed by a dedicated controller that replicates the reasoning capabilities of a deductive reasoner engine within a graph database system. We implement the controller within Neo4j, the most widespread open-source graph database, demonstrating that our implementation achieves adequate performance over small-to-medium property graphs. We also show that our approach is general and applicable to other domains (e.g., laws), directly allowing reasoning with deductive rules over graph databases. Finally, we discuss how the translation process from Datalog to Cypher can be facilitated by state-of-the-art pre-trained Large Language Models, capable of accurately performing the translation task.}
}


@inproceedings{DBLP:conf/icde/SongOGGCM25,
	author = {Tailai Song and
                  Mukharbek Organokov and
                  Lennart Gulikers and
                  Giulio Grassi and
                  Giovanna Carofiglio and
                  Michela Meo},
	title = {Advancing Cloud-Native Cyber Threat Detection with Graph-Based Feature
                  Engineering},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4291--4297},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00321},
	doi = {10.1109/ICDE65448.2025.00321},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/SongOGGCM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In light of the unprecedented proliferation of cloud-native applications, cyber threats targeting cloud services have escalated markedly, emerging as a critical concern for stake-holders. The intrinsic nature of cloud infrastructures renders them particularly susceptible to diverse attacks. In this context, effective attack identification becomes pivotal, facilitating swift responses and preventive measures to mitigate risks and bolster overall security resilience. Despite a range of solutions in literature, attack classification remains an arduous task in industrial environments. To address this, we propose a comprehensive and deployment-friendly graph-based framework. It leverages cloud activity traces, transforming system events to graph structures, and we enumerate 4 different types of attack within a controlled environment. We frame a multi-classification problem, and construct multi-level features to characterize distinct attacks amidst background activities, bypassing the complexity of Deep Learning (DL). To evaluate the efficacy, we compare with multiple Graph Neural Networks (GNNs), and our solution yields comparable performance, demonstrating a promising candidate for the practical and efficient cyber threat detection.}
}


@inproceedings{DBLP:conf/icde/ChenCZL25,
	author = {Minghao Chen and
                  Haodong Chen and
                  Yi Zeng and
                  Zhenfeng Liang},
	title = {{MSPR:} {A} Multi-Scenario Player Recommendation Framework for Online
                  Gaming},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4298--4310},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00322},
	doi = {10.1109/ICDE65448.2025.00322},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChenCZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The social dimension of online gaming spans a wide range of interaction scenarios, each defined by unique dynamics and requirements. Players may seek connections for purposes such as cooperative gameplay, casual interaction, mentorship, or other context-specific needs. However, existing recommendation systems are typically designed for single gaming scenarios, making them unable to accommodate diverse needs, which limits their accuracy and relevance. To overcome these challenges, this paper presents a multi-scenario player recommendation (MSPR) framework that delivers context-aware recommendations by capturing the nuanced distinctions and commonalities across multiple scenarios. The MSPR framework incorporates three key components: (1) a role adaptation module that enables the final player embeddings to incorporate role-specific information tailored to players' dual roles (i.e., initiating or receiving friend requests); (2) scenario-specific and scenario-shared feature extraction modules that balance domain-specific nuances with cross-scenario commonalities; and (3) a scenario-personalized contrastive loss to enhance contextual adaptability. Experiments on both anonymized industrial datasets and public benchmarks demonstrate significant performance improvements. Furthermore, real-world deployment validates the framework's effectiveness, achieving notable increases in friend recommendation acceptance rates. This work advances recommendation technologies in gaming, fostering deeper engagement and enriching social ecosystems.}
}


@inproceedings{DBLP:conf/icde/GanYHLSZGZZ25,
	author = {Chunjing Gan and
                  Dan Yang and
                  Binbin Hu and
                  Ziqi Liu and
                  Yue Shen and
                  Zhiqiang Zhang and
                  Jinjie Gu and
                  Jun Zhou and
                  Guannan Zhang},
	title = {Effectively PAIRing LLMs with Online Marketing via Progressive Prompting
                  Augmentation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4311--4318},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00323},
	doi = {10.1109/ICDE65448.2025.00323},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GanYHLSZGZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we seek to carefully prompt a Large Language Model (LLM) with domain-level knowledge as a better marketing-oriented knowledge miner for marketing-oriented knowledge graph construction, which is non-trivial, suffering from several inevitable issues in real-world marketing scenarios, i.e., uncontrollable relation generation of LLMs, insufficient prompting ability of a single prompt, unaffordable deployment cost of LLMs. To this end, we propose PAIR, a novel Progressive prompting Augmented mIning fRamework for harvesting marketing-oriented knowledge graph with LLMs. In particular, we reduce the pure relation generation to an LLM-based adaptive relation filtering process through knowledge-empowered prompting technique. Next, we steer LLMs for entity expansion with progressive prompting augmentation, followed by a reliable aggregation with comprehensive consideration of both self-consistency and semantic relatedness. In terms of online serving, we specialize in a small and white-box PAIR (i.e., LightPAIR), which is fine-tuned with a high-quality corpus provided by a strong teacher-LLM. Extensive experiments and practical applications in audience targeting verify the effectiveness of the proposed (Light)PAIR.}
}


@inproceedings{DBLP:conf/icde/HuangYCD25,
	author = {Shanshan Huang and
                  Zhiwei Ye and
                  Peng Cai and
                  Qiwen Dong},
	title = {Model-Accuracy Aware Query Routing for Smart Logistics Service},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4319--4331},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00324},
	doi = {10.1109/ICDE65448.2025.00324},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuangYCD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data driven business applications in logistics industry often issue prediction queries over relational databases to retrieve the newly generated transaction data for feature computations. In some cases even using slightly outdated data can result in significant inaccuracies in predictions. In another cases, we also observed the accuracy of model prediction is not sensitive to the data freshness. In the setting of primary-backup databases, one may choose to fetch the freshest data from the primary database to ensure model accuracy. However, this may hurt the performance of read-write transactions on the primary, especially when subjected to a high volume of prediction requests. In this work, we propose a Model-Accuracy Aware Service that facilitates a flexible trade-off between model prediction accuracy and primary database performance. This service implements an automated routing strategy aimed at minimizing the impact on the primary database's performance while meeting the requirements of model accuracy. It achieves this by leveraging the maintained database freshness information and the predictive results feedback from the model to learn the relationship between data discrepancy and prediction discrepancy in primary-backup scenarios. We report the experimental results on a real logistics application and also show its effectiveness on a public dataset.}
}


@inproceedings{DBLP:conf/icde/NiuTPC25,
	author = {Zhaojie Niu and
                  Xinhui Tian and
                  Xindong Peng and
                  Xing Chen},
	title = {BlendHouse: {A} Cloud-Native Vector Database System in ByteHouse},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4332--4345},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00325},
	doi = {10.1109/ICDE65448.2025.00325},
	timestamp = {Thu, 15 Jan 2026 11:18:03 +0100},
	biburl = {https://dblp.org/rec/conf/icde/NiuTPC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of unstructured data retrieval in the AI era has created an urgent need for vector databases that manage high-dimensional vector embeddings and provide efficient vector search capabilities for AI applications. Performance, elasticity, and isolation are the key factors for vector databases to serve modern AI applications effectively. Disaggregation of storage and compute is widely recognized as the most effective approach in both academia and industry. Existing work either redesigns specialized vector databases according to the disaggregated architecture or integrates vector search into generalized databases that already use this architecture. However, challenges still remain in building elastic and efficient vector search systems within the disaggregated architecture, such as higher data fetching latency and the highly stateful nature of vector index, which hinder the system's ability to simultaneously achieve high performance, high elasticity and resource isolation. Additionally, a recent trend has emerged to integrate vector search into general-purpose databases, yet the extensibility and generality of integration methodologies have not been systematically studied. In this paper, we present BlendHouse, a cloud-native and generalized vector database system built on top of the disaggregated storage and computation architecture. BlendHouse achieves high performance, high elasticity and resource isolation simultaneously via a suite of optimizations specific to the vector search workload regarding the disaggregated architecture and the relational database. Experimental results demonstrate that BlendHouse outperforms Milvus and pgvector in terms of read and write performance. The integration methodology illustrated in this paper is extensible and general, paving the way for more powerful data management systems in the AI era.}
}


@inproceedings{DBLP:conf/icde/WengTFCCFHHLRWWYZZHZMCC25,
	author = {Luoxuan Weng and
                  Yinghao Tang and
                  Yingchaojie Feng and
                  Zhuo Chang and
                  Ruiqin Chen and
                  Haozhe Feng and
                  Chen Hou and
                  Danqing Huang and
                  Yang Li and
                  Huaming Rao and
                  Haonan Wang and
                  Canshi Wei and
                  Xiaofeng Yang and
                  Yuhui Zhang and
                  Yifeng Zheng and
                  Xiuqi Huang and
                  Minfeng Zhu and
                  Yuxin Ma and
                  Bin Cui and
                  Peng Chen and
                  Wei Chen},
	title = {DataLab: {A} Unified Platform for LLM-Powered Business Intelligence},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4346--4359},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00326},
	doi = {10.1109/ICDE65448.2025.00326},
	timestamp = {Sat, 03 Jan 2026 11:00:31 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WengTFCCFHHLRWWYZZHZMCC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Business intelligence (BI) transforms large volumes of data within modern organizations into actionable insights for informed decision-making. Recently, large language model (LLM)-based agents have streamlined the BI workflow by automatically performing task planning, reasoning, and actions in executable environments based on natural language (NL) queries. However, existing approaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS. The fragmentation of tasks across different data roles and tools lead to inefficiencies and potential errors due to the iterative and collaborative nature of BI. In this paper, we introduce DataLab, a unified BI platform that integrates a one-stop LLM-based agent framework with an augmented computational notebook interface. DataLab supports various BI tasks for different data roles in data preparation, analysis, and visualization by seamlessly combining LLM assistance with user customization within a single environment. To achieve this unification, we design a domain knowledge incorporation module tailored for enterprise-specific BI tasks, an inter-agent communication mechanism to facilitate information sharing across the BI workflow, and a cell-based context management strategy to enhance context utilization efficiency in BI notebooks. Extensive experiments demonstrate that DataLab achieves state-of-the-art performance on various BI tasks across popular research benchmarks. Moreover, DataLab maintains high effectiveness and efficiency on real-world datasets from Tencent, achieving up to a 58.58% increase in accuracy and a 61.65 % reduction in token cost on enterprise-specific BI tasks.}
}


@inproceedings{DBLP:conf/icde/ZhuYHCZZCQXTL25,
	author = {Jun{-}Peng Zhu and
                  Zhiwei Ye and
                  Xiaolong He and
                  Peng Cai and
                  Xuan Zhou and
                  Aoying Zhou and
                  Dunbo Cai and
                  Ling Qian and
                  Kai Xu and
                  Liu Tang and
                  Qi Liu},
	title = {SylphDB: An Active and Adaptive {LSM} Engine for Update-Intensive
                  Workloads},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4360--4372},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00327},
	doi = {10.1109/ICDE65448.2025.00327},
	timestamp = {Thu, 23 Oct 2025 23:00:54 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhuYHCZZCQXTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Update-intensive workloads are prevalent in contemporary OLTP and AI/ML scenarios. An update operation typically involves deleting the old version of the target record and then inserting a new version. In this work, we demonstrate that an LSM-tree faces two issues when dealing with update-intensive workloads. Firstly, the deleted old versions are not promptly garbage collected until they merge with their new versions during compaction. This may lead to space waste and write amplification. Secondly, it is common for an update operation to modify only a small fraction of a data record, such as one of a hundred attributes. However, state-of-the-art LSM-trees fail to effectively utilize the incremental storage strategy, which involves storing only the updated fraction rather than the entire new version to enhance efficiency. In this paper, we propose two techniques, active and fast garbage collection, and adaptive incremental updating, to address these issues, respectively. Active and fast garbage collection probes the distribution of invalid data versions in an LSM-tree and performs garbage collection in a more promptly manner. Adaptive incremental updating applies different storage modes to the update operation to achieve balanced write and read amplification ratios as much as possible. Based on the techniques, we introduce SylphDB implemented based on the codebase of RocksDB and optimized for update-intensive workloads. Experimental results demonstrated that, compared to traditional LSM-tree based systems, SylphDB can improve the efficiency of garbage collection by 2× and reduce write amplification by 20%.}
}


@inproceedings{DBLP:conf/icde/ZhaoSQLZ25,
	author = {Dandan Zhao and
                  Karthick Sharma and
                  Yuxin Qi and
                  Qixun Liu and
                  Shuhao Zhang},
	title = {Scalable Machine Learning for Real-Time Fault Diagnosis in Industrial
                  IoT Cooling Roller Systems},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4373--4385},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00328},
	doi = {10.1109/ICDE65448.2025.00328},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoSQLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The CISDI Hot Rolled Strip Cooling Roller Health Monitoring System (HRSCR-HMS) is a platform widely deployed in hot rolled steel manufacturing, providing real-time monitoring, fault diagnosis, and health management for cooling rollers. Operating in industrial IoT environments with high data velocity and volume, the system faces challenges in scaling real-time fault diagnosis due to evolving fault patterns, severe data imbalance, and the need for both diagnostic accuracy and efficiency. Evaluations of state-of-the-art fault diagnosis (FD) methods, including online continual learning (OCL) algorithms like Camel, reveal their limitations in meeting the real-time adaptability and data processing demands of HRSCR-HMS. To address these challenges, we propose SRTFD, a scalable framework tailored for real-time fault diagnosis in industrial IoT systems. SRTFD processes high-velocity data streams using three core innovations: Retrospect Coreset Selection (RCS) for optimizing training efficiency by reducing redundant data, Global Balance Technique (GBT) for robust performance with imbalanced data streams, and Confidence and Uncertainty-driven Pseudo-label Learning (CUPL) for adaptive updates with unlabeled data. Experiments on industrial datasets demonstrate that SRTFD outperforms competing methods, addressing challenges of imbalance, redundancy, and scalability. Integration within HRSCR-HMS validates SRTFD as a practical, scalable solution aligned with the stringent demands of stream data processing in industrial IoT environments.}
}


@inproceedings{DBLP:conf/icde/PetraliaBCP25,
	author = {Adrien Petralia and
                  Paul Boniol and
                  Philippe Charpentier and
                  Themis Palpanas},
	title = {Few Labels are All you Need: {A} Weakly Supervised Framework for Appliance
                  Localization in Smart-Meter Series},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4386--4399},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00329},
	doi = {10.1109/ICDE65448.2025.00329},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/PetraliaBCP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Improving smart grid system management is crucial in the fight against climate change, and enabling consumers to play an active role in this effort is a significant challenge for electricity suppliers. In this regard, millions of smart meters have been deployed worldwide in the last decade, recording the main electricity power consumed in individual households. This data produces valuable information that can help them reduce their electricity footprint; nevertheless, the collected signal aggregates the consumption of the different appliances running simultaneously in the house, making it difficult to apprehend. Non-Intrusive Load Monitoring (NILM) refers to the challenge of estimating the power consumption, pattern, or on/off state activation of individual appliances using the main smart meter signal. Recent methods proposed to tackle this task are based on a fully supervised deep-learning approach that requires both the aggregate signal and the ground truth of individual appliance power. However, such labels are expensive to collect and extremely scarce in practice, as they require conducting intrusive surveys in households to monitor each appliance. In this paper, we introduce CamAL, a weakly supervised approach for appliance pattern localization that only requires information on the presence of an appliance in a household to be trained. CamAL merges an ensemble of deep-learning classifiers combined with an explainable classification method to be able to localize appliance patterns. Our experimental evaluation, conducted on 4 real-world datasets, demonstrates that CamAL significantly outperforms existing weakly supervised baselines and that current SotA fully supervised NILM approaches require significantly more labels to reach CamAL performances. The source of our experiments is available at: https://github.com/adrienpetralia/CamAL.}
}


@inproceedings{DBLP:conf/icde/MishraDWZYNLM25,
	author = {Ashirbad Mishra and
                  Soumik Dey and
                  Hansi Wu and
                  Jinyu Zhao and
                  He Yu and
                  Kaichen Ni and
                  Binbin Li and
                  Kamesh Madduri},
	title = {GraphEx: {A} Graph-Based Extraction Method for Advertiser Keyphrase
                  Recommendation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4400--4413},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00330},
	doi = {10.1109/ICDE65448.2025.00330},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MishraDWZYNLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online sellers and advertisers are recommended keyphrases for their listed products, which they bid on to enhance their sales. One popular paradigm that generates such recommendations is Extreme Multi-Label Classification (XMC), which involves tagging/mapping keyphrases to items. We outline the limitations of training XMC models on click data for keyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an innovative graph-based approach that recommends keyphrases to sellers using extraction of token permutations from item titles. Additionally, we demonstrate traditional metrics such as precision/recall isn't reliable on click-based data in practical applications, thereby necessitating a robust framework to evaluate performance in real-world scenarios. Our evaluation is designed to assess the relevance of keyphrases to items and the potential for buyer outreach. GraphEx outperforms production models at eBay, achieving the objectives mentioned above. It supports near real-time inferencing in resource-constrained production environments and scales effectively for billions of items.}
}


@inproceedings{DBLP:conf/icde/SuiWCXHZZYSP25,
	author = {Yicheng Sui and
                  Xiaotian Wang and
                  Tianyu Cui and
                  Tong Xiao and
                  Chenghao He and
                  Shenglin Zhang and
                  Yuzhi Zhang and
                  Xiao Yang and
                  Yongqian Sun and
                  Dan Pei},
	title = {Bridging the Gap: LLM-Powered Transfer Learning for Log Anomaly Detection
                  in New Software Systems},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4414--4427},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00331},
	doi = {10.1109/ICDE65448.2025.00331},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/SuiWCXHZZYSP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For large IT companies, maintaining numerous software systems presents considerable complexity. Logs are invaluable for depicting the state of systems, making log-based anomaly detection crucial for ensuring system reliability. Existing methods require extensive log data for training, hindering their rapid deployment for new systems. Cross-system log anomaly detection methods attempt to transfer knowledge from mature systems to new ones but often struggle with syntax differences and system-specific knowledge, which hinders their effectiveness. To address these issues, this paper proposes LogSynergy, a novel transfer learning-based log anomaly detection framework. LogSynergy employs (1) LLM-based event interpretation (LEI) to standardize log syntax across different systems, and (2) system-unified feature extraction (SUFE) to disentangle system-specific features from system-unified features. These bridge the gap among different systems and enhance LogSynergy's generalizability. LogSynergy has been deployed in the production environment of a top-tier global Internet Service Provider (ISP), where it was evaluated on three real-world datasets. Additionally, we conducted evaluations on three public datasets. The results demonstrate that LogSynergy significantly outperforms existing methods. It achieves F1-scores over 89% on the real-world datasets and over 83% on the public datasets, using only 5000 labeled log sequences from the new system. These results underscore LogSynergy's effectiveness in rapidly deploying anomaly detection models for new systems. The code of LogSynergy has been open-sourced at https://github.com/DDUtian/LogSynergy}
}


@inproceedings{DBLP:conf/icde/ZhouEL25,
	author = {Liangkai Zhou and
                  Agbonlahor Edomwonyi and
                  Shan Lin},
	title = {mAP: Abandoned Property Detection Using Multifaceted Urban Service
                  Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4428--4440},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00332},
	doi = {10.1109/ICDE65448.2025.00332},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhouEL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Abandoned property detection is a tedious and labor-intensive task for field inspectors of local governments. Recent research employs computer vision-based approaches to identify abandoned properties, using Google Street View [1], mobile sensing images [2], and remote sensing images [3]. However, such approaches rely on large-scale up-to-date image datasets of city properties, which are usually unavailable and very costly to obtain. In this work, we propose a novel approach to abandoned property detection by combining urban service data, such as utility payment records, service requests, and complaints reported by residents, which are available from digital city management systems. Since such data potentially reflect abandoned properties from different perspectives, such as financial status, maintenance, and environment, we design mAP: a multi-view framework with an intra-view to inter-view encoder architecture to fuse multifaceted service data for abandoned property detection. Such service data are inherently spatiotemporally imbalanced due to the diversity of the community. To address these issues, a Bayesian Network is designed to assess each report with the socioeconomic factors of the local community and infer its value as evidence for an abandoned property. We evaluate our solution with extensive experiments using datasets collected from the City of Newark, New Jersey. mAP outperforms state-of-the-art solutions by 1.94 to 13.42 times in F1 score and by 21% in recall rate.}
}


@inproceedings{DBLP:conf/icde/LiZZMYSWXYC25,
	author = {Yuchen Li and
                  Hao Zhang and
                  Yongqi Zhang and
                  Xinyu Ma and
                  Wenwen Ye and
                  Naifei Song and
                  Shuaiqiang Wang and
                  Haoyi Xiong and
                  Dawei Yin and
                  Lei Chen},
	title = {M\({}^{\mbox{2}}\)oERank: Multi-Objective Mixture-of-Experts Enhanced
                  Ranking for Satisfaction-Oriented Web Search},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4441--4454},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00333},
	doi = {10.1109/ICDE65448.2025.00333},
	timestamp = {Tue, 16 Dec 2025 08:07:08 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiZZMYSWXYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pre-trained language models (PLMs) have been successfully used to build high-performance ranking models for large-scale information retrieval systems. However, traditional PLM-based ranking approaches face two key challenges: (1) these models use both sparse and dense content (such as the query/title and content of documents) as inputs, which may require different attention allocations; and (2) traditional PLM-based ranking approaches have identified multiple objectives to gauge user satisfaction with ranking results, but integrating these objectives into the end-to-end training process and the subsequent feature updates and iterations usually involves significant computational resource overhead. In this paper, we propose a novel PLM-based ranking approach M2oE Rank, Multi-objective Mixture-of-Experts (MoE) enhanced Ranking. Specifically, M2oERank lever-ages a context-aware PLM-based hierarchical encoder to extract semantic relevance between the query and the document title and content, while allowing for separate dense and sparse attention for different inputs. With the extracted semantic relevance repre-sentations, multifacet user satisfaction features and task-specific annotations, M2oERank employs an MoE module to perform multi-objective pre-training of ranking models focused on user satisfaction. Finally, M2oERank uses a weight fusion module that fuses outputs from the above experts to predict ranking scores. Moreover, we present a three-stage offline training strategy and the online system workflow for deploying M2oERank at web-scale search. To demonstrate the effectiveness of our proposed approach, we conduct extensive offline and online evaluations using real-world web traffic from Baidu Search. The comparisons against numbers of advanced baselines confirmed the advantages of M2oERank in producing high-performance ranking models for web-scale search.}
}


@inproceedings{DBLP:conf/icde/YuWYHQ25,
	author = {Xie Yu and
                  Jingyuan Wang and
                  Yifan Yang and
                  Qian Huang and
                  Ke Qu},
	title = {BIGCity: {A} Universal Spatiotemporal Model for Unified Trajectory
                  and Traffic State Data Analysis},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4455--4469},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00334},
	doi = {10.1109/ICDE65448.2025.00334},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YuWYHQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatiotemporal (ST) data analysis is a critical area of research in data engineering. Typical dynamic ST data includes trajectory data (representing individual-level mobility) and traffic state data (representing population-level mobility). Traditional studies often treat trajectory and traffic state data as distinct, independent modalities, each tailored to specific tasks within a single modality. However, real-world applications, such as navigation apps, require joint analysis of trajectory and traffic state data. Treating these data types as two separate domains can lead to suboptimal model performance. Although recent advances in ST data pre-training and ST foundation models aim to develop universal models for ST data analysis, most existing models are “multi-task, solo-data modality” (MTSM), meaning they can handle multiple tasks within either trajectory data or traffic state data, but not both simultaneously. To address this gap, this paper introduces BIGCity, a pioneer multi-task, multi-data modality (MTMD) model for ST data analysis. The model targets two key challenges in designing an MTMD ST model: (1) unifying the representations of different ST data modalities, and (2) unifying heterogeneous ST analysis tasks. To overcome the first challenge, BIGCity introduces a novel ST-unit that represents both trajectories and traffic states in a unified format. Additionally, for the second challenge, BIGCity adopts a tunable large model with ST task-oriented prompt, enabling it to perform a range of heterogeneous tasks without the need for fine-tuning. Extensive experiments on real-world datasets demonstrate that BIGCity achieves state-of-the-art performance across 8 tasks, outperforming 17 baselines. Our code are available at https://github.com/bigscity/BIGCity.}
}


@inproceedings{DBLP:conf/icde/TongTZZLC25,
	author = {Bing Tong and
                  Jianheng Tang and
                  Yan Zhou and
                  Chen Zhang and
                  Jia Li and
                  Lei Chen},
	title = {GalaxyView: Property Graph Transformation for Materialized View Generation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4470--4482},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00335},
	doi = {10.1109/ICDE65448.2025.00335},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/TongTZZLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the practical use of graph databases, storing graphs separately enhances maintainability, while integrating them into a unified graph facilitates advanced analytics. To address these dual needs, we present a GQL-compatible framework for creating graph views across multiple property graphs. Leveraging insights from commercial graph database environments and user-driven requirements, we identify two key graph transformations—merging and expansion—that significantly boost query efficiency. By incorporating these transformations into view generation, we effectively minimize redundant queries. Furthermore, we streamline view creation through implicit edge creation and automated property merging. To ensure views meet user expectations before full generation, we introduce a preview feature that uses sampling to effectively represent the graph structure. We assess the performance of GalaxyView across diverse workloads, with comprehensive experiments on real-world graphs demonstrating significant improvements in query performance and operational efficiency.}
}


@inproceedings{DBLP:conf/icde/Demertzis25,
	author = {Ioannis Demertzis},
	title = {Large-Scale Private Computation for Real-World Applications via Trusted
                  Hardware and Obliviousness},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4483--4485},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00336},
	doi = {10.1109/ICDE65448.2025.00336},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Demertzis25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy-preserving computation that exposes memory access patterns can inadvertently reveal significant information about plaintext inputs through leakage-abuse attacks. While industrial approaches utilizing hardware enclaves for confidential computing are vulnerable to side-channel attacks, they offer an affordable and cost-effective solution for any privacy-preserving computation. Oblivious primitives, as robust cryptographic tools, can mitigate leakage-abuse and software side-channel attacks (when integrated with hardware enclaves). In this tutorial, we present the latest advancements in TEE-based oblivious primitives, highlighting multiple applications such as private contact discovery for Signal, our new proposal for anonymous communication resistant to long-term traffic analysis (SP'25), oblivious relational (USENIX'25) and graph databases (PVLDB'24), key transparency, searchable encryption, large-scale software activity monitoring, federated learning, privacy-preserving LLMs, Google's Privacy Sandbox initiative, Google's FLEDGE, Google's Titan Security Key and Google's Asylo. Our recent state-of-the-art oblivious primitives include a high-throughput oblivious key-value store (used in Signal's commercial solution for contact discovery, SOSP'21), a reduced-latency solution (PVLDB'24), the most scalable oblivious sort and shuffle primitives to date (SP'24), optimized for shared-memory and distributed architectures, and the first scalable oblivious filter, group-by, join approaches (USENIX'25).}
}


@inproceedings{DBLP:conf/icde/LuoXY25,
	author = {Qiyao Luo and
                  Quanqing Xu and
                  Chuanhui Yang},
	title = {How to Answer Secure and Private {SQL} Queries?},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4486--4491},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00337},
	doi = {10.1109/ICDE65448.2025.00337},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuoXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the era of big data, the ability to process and analyze large volumes of data is critical for decision-making, marketing and sales, healthcare and scientific research, etc. However, this capability also brings significant challenges related to data security and privacy. Secure and private query processing is essential to address these challenges. It ensures the protection of private and confidential data, compliance with stringent regulatory requirements, and the maintenance of user trust. Additionally, it enables safe data sharing and collaborative analysis while preserving the utility of the data. By incorporating advanced cryptographic and privacy techniques, secure and private query processing can defend against sophisticated cyber threats. This tutorial highlights the importance of integrating robust security and privacy measures into query processing to build trustworthy database systems. It reviews current systems and protocols that achieve these goals and discusses future directions for easy-to-use query processing under secure and private protection.}
}


@inproceedings{DBLP:conf/icde/PangZO25,
	author = {Yue Pang and
                  Lei Zou and
                  M. Tamer {\"{O}}zsu},
	title = {A Unified Narrative for Query Processing in Graph Databases},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4492--4496},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00338},
	doi = {10.1109/ICDE65448.2025.00338},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/PangZO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advent of graph data, graph databases have garnered significant research interest and efforts in recent years, especially with respect to graph query processing. There have been a vast suite of methods for efficient graph query processing, especially for the core graph query constructs, regular path queries (RPQs) and subgraph matching queries (SMQs). In the meantime, there is an observable divide among these methods as well as confusion between them and their relational counterparts. We thus propose this tutorial to provide a unified narrative for graph query processing, so as to bridge the gap between existent lines of work and offer a comprehensive view of the query processing workflow in graph databases.}
}


@inproceedings{DBLP:conf/icde/LiWQZ25,
	author = {Wentao Li and
                  Dong Wen and
                  Lu Qin and
                  Ying Zhang},
	title = {An Overview of Path Queries on Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4497--4503},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00339},
	doi = {10.1109/ICDE65448.2025.00339},
	timestamp = {Wed, 10 Sep 2025 14:09:53 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiWQZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs are powerful tools for modeling entities and their relationships. Among the fundamental operations on graphs, path queries play a vital role in identifying paths between pairs of vertices. These queries underpin a broad range of applications by enabling efficient navigation and insightful analysis of graph-structured data. Path queries can be categorized based on the types of paths they return, with each category addressing specific application needs. In this tutorial, we focus on four major categories of path queries: plain shortest path queries, constrained shortest path queries, shortest path summary queries, and non-shortest path queries. We begin by introducing the basic concepts and practical applications of path queries. We then provide an in-depth exploration of the four categories listed above. Finally, we conclude with a discussion of the future research directions in this area.}
}


@inproceedings{DBLP:conf/icde/WangCZZ25,
	author = {Hanchen Wang and
                  Dawei Cheng and
                  Ying Zhang and
                  Wenjie Zhang},
	title = {{AIGC} for Graphs: Current Techniques and Future Trends},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4504--4508},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00340},
	doi = {10.1109/ICDE65448.2025.00340},
	timestamp = {Wed, 10 Sep 2025 14:09:51 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangCZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As artificial intelligence technology continues to advance, artificial intelligence-generated content (AIGC) has begun to evolve towards generating complex and structured data, particularly graph data. As an important topic in many fields such as database, data mining, and machine learning, graph generation holds significant value for simulating complex relationships between entities and has shown vast potential for applications in fields such as molecular generation, drug design, and material discovery. In this context, AIGC technology for graph generation has received widespread attention. This tutorial outlines the latest developments in AIGC for graph generation. We categorize existing methods into two main types according to their objectives and motivations: similarity-based generation and function-driven generation. We first provide an overview of AIGC models for graph generation. Then, we conduct a thorough review of the existing works. Finally, we explore the current trends and future directions, discussing potential ways to integrate database and machine learning techniques for graph generation.}
}


@inproceedings{DBLP:conf/icde/FanWDNWL25,
	author = {Wenqi Fan and
                  Pangjing Wu and
                  Yujuan Ding and
                  Liang{-}Bo Ning and
                  Shijie Wang and
                  Qing Li},
	title = {Towards Retrieval-Augmented Large Language Models: Data Management
                  and System Design},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4509--4512},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00341},
	doi = {10.1109/ICDE65448.2025.00341},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FanWDNWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by integrating external, reliable, and up-to-date knowledge. This addresses critical limitations such as hallucinations and outdated internal information. This tutorial delves into the evolution and frameworks of RAG, emphasizing the pivotal role of data management technologies in optimizing query processing, storage, indexing, and efficiency. It explores how RAG systems can deliver high-quality, context-aware outputs through efficient retrieval and integration, covering key topics such as retrieval-augmented LLM (RA-LLM) architectures, retrieval techniques, learning methodologies, and applications in NLP and domain-specific tasks. Challenges like customized query and generation, real-time retrieval, and trustworthy RAG are discussed alongside future directions and opportunities for innovation. Designed for students, researchers, and industry practitioners with basic artificial intelligence and data engineering knowledge, this tutorial offers practical insights into designing data management-powered RAG systems. It inspires the exploration of novel solutions in this rapidly evolving field.}
}


@inproceedings{DBLP:conf/icde/GomesGS25,
	author = {Heitor Murilo Gomes and
                  Nuwan Gunasekara and
                  Yibin Sun},
	title = {Machine Learning on the Fly: {A} Hands-On Tutorial for Streaming Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4513--4516},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00342},
	doi = {10.1109/ICDE65448.2025.00342},
	timestamp = {Thu, 11 Sep 2025 20:25:49 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GomesGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data stream learning is an emerging machine learning paradigm designed for environments where data arrive continuously and must be processed in real time. Unlike traditional batch learning, which assumes access to a fixed dataset, stream learning addresses the unique challenges of non-stationary distributions, bounded memory, and strict computational constraints. These challenges are increasingly relevant across domains such as IoT, finance, cybersecurity, and environmental monitoring, where timely and adaptive decision-making is essential. This tutorial introduces key concepts and techniques in data stream learning, blending foundational theory with practical demonstrations. It features CapyMOA, an open-source library that provides efficient algorithm implementations through a high-level Python API. We demonstrate the use of this tool through practical examples, with all source code available at https://github.com/adaptive-machine-learning/CapyMOA, and supporting tutorials and installation guides accessible at https://capymoa.org/.}
}


@inproceedings{DBLP:conf/icde/YangLGJ25,
	author = {Bin Yang and
                  Yuxuan Liang and
                  Chenjuan Guo and
                  Christian S. Jensen},
	title = {Data Driven Decision Making with Time Series and Spatio-Temporal Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4517--4522},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00343},
	doi = {10.1109/ICDE65448.2025.00343},
	timestamp = {Fri, 14 Nov 2025 07:30:21 +0100},
	biburl = {https://dblp.org/rec/conf/icde/YangLGJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series data captures properties that change over time. Such data occurs widely, ranging from the scientific and medical domains to the industrial and environmental domains. When the properties in time series exhibit spatial variations, we often call the data spatio-temporal. As part of the continued digitalization of processes throughout society, increasingly large volumes of time series and spatio-temporal data are available. In this tutorial, we focus on data-driven decision making with such data, e.g., enabling greener and more efficient transportation based on traffic time series forecasting. The tutorial adopts the holistic paradigm of “data-governance-analytics-decision.” We first introduce the data foundation of time series and spatio-temporal data, which is often heterogeneous. Next, we discuss data governance methods that aim to improve data quality. We then cover data analytics, focusing on five desired characteristics: automation, robustness, generality, explainability, and resource efficiency. We finally cover data-driven decision making strategies and briefly discuss promising research directions. We hope that the tutorial will serve as a primary resource for researchers and practitioners who are interested in value creation from time series and spatio-temporal data.}
}


@inproceedings{DBLP:conf/icde/KarlasSS25,
	author = {Bojan Karlas and
                  Babak Salimi and
                  Sebastian Schelter},
	title = {Navigating Data Errors in Machine Learning Pipelines: Identify, Debug,
                  and Learn},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4523--4529},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00344},
	doi = {10.1109/ICDE65448.2025.00344},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KarlasSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Addressing data errors such as wrong, missing, noisy, biased, or out-of-distribution values has become a crucial part of the machine learning (ML) development lifecycle. Un-fortunately, traditional approaches have relied either on treating the symptom by refining the model architecture, or on improving data quality by repairing incorrect values regardless of their significance to the downstream model. Both strategies end up tackling the problem in isolation, and disregard the structure of modern ML pipelines, which involve a series of steps for data preprocessing, model training, and prediction processing. Consequently, they miss the opportunity to consider how different data errors propagate through the pipeline and how they impact its ability to perform downstream tasks. In recent years, the research community has made significant strides towards more holistic approaches for identifying the most harmful data errors, performing the most beneficial repairs, and ensuring reliable performance even if some data errors remain present. This tutorial will survey some prominent work published in this space and showcase several tools that have been developed. By combining theoretical foundations with practical demonstrations, attendees will gain actionable strategies to diagnose and mitigate data quality issues, improving the reliability, fairness, and transparency of ML systems in real-world settings.}
}


@inproceedings{DBLP:conf/icde/GuoLL25,
	author = {Yunyan Guo and
                  Zhuopeng Li and
                  Guoliang Li},
	title = {{DBMS} with {CXL} Memory: What's New and What's Next},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4530--4535},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00345},
	doi = {10.1109/ICDE65448.2025.00345},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GuoLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compute Express Link (CXL) is an open industry-standard interconnect protocol designed for communication between processors and devices, e.g., memory expansions. It brings new opportunities for enhancing system performance, particularly for workloads constrained by memory capacity. Consequently, CXL opens new avenues for the design of database management systems (DBMS), which is highly beneficial for managing complex data tasks. However, traditional techniques focused on addressing I/O bottlenecks are insufficient to meet the new challenges brought by CXL memory management. In this tutorial, we introduce the advantages and application scenarios of tiered CXL memory, pooled CXL memory, and shared CXL memory, as well as the new challenges that arise, including: (1) reducing data access, data exchange, and data transfer cost; (2) optimizing memory allocation and competition to improve memory utilization; (3) managing shared data for failure, series operators, and distributed transactions. We also review emerging techniques aimed at addressing these issues. Finally, we summarize new opportunities and highlight open research problems in this evolving area.}
}


@inproceedings{DBLP:conf/icde/SharmaTLWFZNRCZ25,
	author = {Ankita Sharma and
                  Jaykumar Tandel and
                  Xuanmao Li and
                  Lanjun Wang and
                  Anna Fariha and
                  Liang Zhang and
                  Syed Arsalan Ahmed Naqvi and
                  Irbaz Bin Riaz and
                  Lei Cao and
                  Jia Zou},
	title = {DataMorpher: Automatic Data Transformation Using LLM-Based Zero-Shot
                  Code Generation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4536--4539},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00346},
	doi = {10.1109/ICDE65448.2025.00346},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/SharmaTLWFZNRCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data transformation is a critical challenge in modern data management systems, particularly when handling complex operations over multiple data sources. However, existing approaches rely on supervised learning, which requires tremendous data labeling and training overhead. To alleviate such overhead while improving accuracy, we demonstrate a novel system DataMorpher that leverages Large Language Models (LLMs) to generate code that transforms source datasets into a user-specified target format. To generate a high-quality and token-efficient prompt, we leverage data profiling to extract features from the source datasets and historical examples of the target data. We also select a subset of features to reduce noise and costs using a ranking algorithm. These selected features are finally translated into a declarative language, which is inspired by SQL's data definition language (DDL), before being added to the prompt. We will demonstrate the workflow and effectiveness of DATAMORPHER using real-world data transformation workflows from Microsoft's GitHub benchmark, smart building, and medical data integration. (A5-min video of our demo is available at https://youtu.be/CuDm46K-_eA.)}
}


@inproceedings{DBLP:conf/icde/BianchiDCCGSZ25,
	author = {Alexander Bianchi and
                  Rafael Dolores and
                  Andrew Chai and
                  Vincent Corvinelli and
                  Parke Godfrey and
                  Jarek Szlichta and
                  Calisto Zuzarte},
	title = {Db2une: Tuning {IBM} Db2 with Deep Learning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4540--4543},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00347},
	doi = {10.1109/ICDE65448.2025.00347},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/BianchiDCCGSZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern database systems such as IBM Db2 have many configurable parameters (“knobs”) which must be precisely adjusted (“tuned”) to ensure optimal workload performance. Manual tuning of these settings is challenging, even for seasoned experts. We introduce the design of, and a demonstration plan for, Db2une, an automated, query-aware tuning system employing deep-learning techniques to enhance performance while also conserving resources. We showcase how Db2une interactively tunes data systems to boost performance, and how its query representation model, QBERT, aids users in understanding the query plans generated from complex analytical workloads.}
}


@inproceedings{DBLP:conf/icde/FanLLLOWXY25,
	author = {Wenfei Fan and
                  Yang Leng and
                  Daji Li and
                  Shuhao Liu and
                  Mingliang Ouyang and
                  Yaoshu Wang and
                  Min Xie and
                  Qiang Yuan},
	title = {DreamCreek: {AI} for Battery Formation and Grading},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4544--4547},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00348},
	doi = {10.1109/ICDE65448.2025.00348},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FanLLLOWXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DreamCreek is a system for grading the capacity of lithium-ion battery cells. An electric vehicle (EV) battery pack consists of thousands of lithium-ion cells; these cells must have a balanced capacity, measured by a formation and grading phase. This phase is costly, taking 14-20+ hours. DreamCreek aims to optimize this process, by collecting data from partial charge. Using the data, it determines the capacity of lithium-ion cells by employing both machine learning prediction and logic deduction, to reduce the usage of energy and increase the production. We will demonstrate how DreamCreek works with a guided tour, and show how it reduces the time of the formation and grading phase to 4 hours, with an error rate in the range of [0.06 %, 1%].}
}


@inproceedings{DBLP:conf/icde/LiuLCHPLC25,
	author = {Qiyu Liu and
                  Yuxin Luo and
                  Mengke Cui and
                  Siyuan Han and
                  Jingshu Peng and
                  Jin Li and
                  Lei Chen},
	title = {BitTuner: {A} Toolbox for Automatically Configuring Learned Data Compressors},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4548--4551},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00349},
	doi = {10.1109/ICDE65448.2025.00349},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuLCHPLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compressing sorted keys is a fundamental operation in data management and information retrieval. Inspired by the success of learned index, recent studies apply simple ML models to compress large-scale sorted keys, leading to the concept of learned compressor. Intuitively, learned compressors losslessly encode sorted keys by approximating them with an error-bounded ML model (e.g., a piecewise linear function) and a residual array to ensure lossless key restoration. However, determining the optimal configuration of underlying ML models to maximize compression efficacy is nontrivial. To address this, by analyzing the distribution characteristics of input keys, we propose BitTuner, a novel framework that automatically sets model hyper-parameters to provably achieve the best compression ratio. We demonstrate BitTuner on two real-world scenarios: inverted list compression and vectorDB codebook compression. The results show that BitTuner automates the parameter tuning procedure and achieves superior compression efficacy when compared to generic compressors such as LZ4 and LZMA.}
}


@inproceedings{DBLP:conf/icde/PetraliaBCP25a,
	author = {Adrien Petralia and
                  Paul Boniol and
                  Philippe Charpentier and
                  Themis Palpanas},
	title = {DeviceScope: An Interactive App to Detect and Localize Appliance Patterns
                  in Electricity Consumption Time Series},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4552--4555},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00350},
	doi = {10.1109/ICDE65448.2025.00350},
	timestamp = {Sun, 07 Dec 2025 22:10:52 +0100},
	biburl = {https://dblp.org/rec/conf/icde/PetraliaBCP25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, electricity suppliers have installed millions of smart meters worldwide to improve the management of the smart grid system. These meters collect a large amount of electrical consumption data to produce valuable information to help consumers reduce their electricity footprint. However, having non-expert users (e.g., consumers or sales advisors) understand these data and derive usage patterns for different appliances has become a significant challenge for electricity suppliers because these data record the aggregated behavior of all appliances. At the same time, ground-truth labels (which could train appliance detection and localization models) are expensive to collect and extremely scarce in practice. This paper introduces DeviceScope [1], an interactive tool designed to facilitate understanding smart meter data by detecting and localizing individual appliance patterns within a given time period. Our system is based on CamAL (Class Activation Map-based Appliance Localization), a novel weakly supervised approach for appliance localization that only requires the knowledge of the existence of an appliance in a household to be trained.}
}


@inproceedings{DBLP:conf/icde/AnC25,
	author = {Shuai An and
                  Yang Cao},
	title = {Towards On-Database Contextual Model Explanation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4556--4559},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00351},
	doi = {10.1109/ICDE65448.2025.00351},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/AnC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We demonstrate DBxAI, a system for explaining predictions of arbitrary machine learning models even if model owners opt not to, giving the right-to-explanation to model users as requested by GDPR. The enabling idea of DBxAI is to accumulate prediction histories at the client side during model serving, by collecting inference instances and predictions as a database. DBxAI then deduces explanations by using this database, without the need for any coordination from model owners. Using real-world traces, we demonstrate that, by retrieving different contexts from the database, DBxAI explanations are contextual, accurate, and orders of magnitude faster to compute than existing explainers that are developed for and operated by model owners.}
}


@inproceedings{DBLP:conf/icde/MaoZLLYG25,
	author = {Yuren Mao and
                  Yifan Zhu and
                  Qing Liu and
                  Peigen Liu and
                  Haoran Yu and
                  Yunjun Gao},
	title = {scRAG: an Efficient Retrieval Augmented Generation System for scRNA-seq
                  Data Analysis},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4560--4563},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00352},
	doi = {10.1109/ICDE65448.2025.00352},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MaoZLLYG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An average person usually contains more than 10 trillion human cells, and each cell's transcriptome can be profiled by a single-cell RNA sequence (scRNA-seq). The huge volume and high complexity of scRNA-seq data put challenges on fundamental tasks of scRNA-seq data analysis, i.e., cell type identification and new cell type discovery. In this paper, we demonstrate scRAG, which can efficiently remove batch effect in cell-type identification and enable reliable new cell discovery, facilitated by GPU-based scRNA-seq data management and Large Language Models (LLMs). The GPU-based scRNA-seq data management enables high throughput scRNA-seq data retrieval and update, while the LLM utilizes the retrieval results to remove the batch effect and discover novel cells. We demonstrate scRAG for its: (a) interfaces, (b) GPU-based scRNA-seq data management, and (c) applications in batch effect removal and cancer cell discovery.}
}


@inproceedings{DBLP:conf/icde/QiuLPPWYHSLYGZJY25,
	author = {Xiangfei Qiu and
                  Xiuwen Li and
                  Ruiyang Pang and
                  Zhicheng Pan and
                  Xingjian Wu and
                  Liu Yang and
                  Jilin Hu and
                  Yang Shu and
                  Xuesong Lu and
                  Chengcheng Yang and
                  Chenjuan Guo and
                  Aoying Zhou and
                  Christian S. Jensen and
                  Bin Yang},
	title = {EasyTime: Time Series Forecasting Made Easy},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4564--4567},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00353},
	doi = {10.1109/ICDE65448.2025.00353},
	timestamp = {Tue, 14 Oct 2025 19:36:23 +0200},
	biburl = {https://dblp.org/rec/conf/icde/QiuLPPWYHSLYGZJY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series forecasting has important applications across diverse domains. EasyTime, the system we demonstrate, facilitates easy use of time-series forecasting methods by researchers and practitioners alike. First, EasyTime enables one-click evaluation, enabling researchers to evaluate new forecasting methods using the suite of diverse time series datasets collected in the preexisting time series forecasting benchmark (TFB). This is achieved by leveraging TFB's flexible and consistent evaluation pipeline. Second, when practitioners must perform forecasting on a new dataset, a nontrivial first step is often to find an appropriate forecasting method. EasyTime provides an Automated Ensemble module that combines the promising forecasting methods to yield superior forecasting accuracy compared to individual methods. Third, EasyTime offers a natural language Q&A module leveraging large language models. Given a question like “Which method is best for long term forecasting on time series with strong seasonality?”, EasyTime converts the question into SQL queries on the database of results obtained by TFB and then returns an answer in natural language and charts. By demonstrating EasyTime11https://decisionintelligence.github.io/EasyTime, we aim to show how it simplifies the use of time-series forecasting and facilitates the development of new generations of time series forecasting methods.}
}


@inproceedings{DBLP:conf/icde/Watanabe25,
	author = {Satoru Watanabe},
	title = {Data Backup System with No Impact on Business Processing Utilizing
                  Storage and Container Technologies},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4568--4571},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00354},
	doi = {10.1109/ICDE65448.2025.00354},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Watanabe25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data backup is a core technology for improving system resilience to system failures. Data backup in enterprise systems is required to minimize the impacts on business processing, which can be categorized into two factors: system slowdown and downtime. To eliminate system slowdown, asynchronous data copy (ADC) technology is prevalent, which copies data asynchronously with original data updates. However, the ADC can collapse backup data when applied to enterprise systems with multiple resources. Then, the demonstration system employed consistency group technology, which makes the order of data updates the same between the original and backup data. In addition, we developed a container platform operator to unravel the complicated correspondence between storage volumes and applications. The operator automates the configuration of the ADC with the setting of consistency groups. We integrated the storage and container technologies into the demonstration system, which can eliminate both system slowdown and downtime. The demonstration video can be accessed at https://photos.app.goo.gl/CkocfRasTUxuzzgr5.}
}


@inproceedings{DBLP:conf/icde/LiZW25,
	author = {Liangwei Li and
                  Yiyi Zhang and
                  Ning Wang},
	title = {SwiftDP: An Efficient Framework for Automated Data Preparation Pipeline
                  Generation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4572--4575},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00355},
	doi = {10.1109/ICDE65448.2025.00355},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a crucial step of machine learning, data preparation is the most time and energy consuming task for data scientists, entailing several data processing techniques to improve the performance of output results for ML models. However, end-to-end AutoML researchers primarily focus on automated ML pipelines consisting of algorithm selection and hyperparameter tuning, while automated data preparation has not been widely explored and applied. In this paper, we propose SwiftDP, an efficient framework for automated data preparation based on Monte Carlo Tree Search. To guide the search more effectively, a fully connected neural network with an attention mechanism is designed to estimate the subsequent maximum performance gain of each tree node. In addition, in order to reduce search space and improve system efficiency, two optimization strategies, meta-learning and accelerated training strategy, are used to determine the type and order of tasks in the data preparation process in advance, and speed up the pipeline creation process. We have built SwiftDP as an open-source Python library with intuitive APIs and demonstrated its better performance in 10 seconds than SOTA systems in 1 hour.}
}


@inproceedings{DBLP:conf/icde/ZhaoLCN25,
	author = {Wenyue Zhao and
                  Jia Li and
                  Yang Cao and
                  Nikos Ntarmos},
	title = {MITra: Populating Graph Traversal Algorithms},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4576--4579},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00356},
	doi = {10.1109/ICDE65448.2025.00356},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhaoLCN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We demonstrate MITra, a system for synthesizing Multi-Instance graph Traversal algorithms that traverse from multiple source vertices simultaneously over a single thread. Underlying MITra is an abstraction that expresses traversal logic via arithmetic operations over a numeric runtime property called vertex ranks, and separates it from computation logic. Based on this, Mitra implements an interface that allows users to express traversals by declaring vertex ranks and specify computation logic by “plugging in” an edge function adopted from classic single-instance algorithms. It synthesizes multi-instance traversal algorithms from declared vertex ranks and edge functions, automatically sharing computation across instances and benefiting from SIMD. We demonstrate its expressiveness, ease-of-use and performance for supporting multi-instance graph traversal computations. We also demonstrate a use case of MITra for shortest-path tree (SPT) computation over data center IP networks.}
}


@inproceedings{DBLP:conf/icde/BoniolTBP25,
	author = {Paul Boniol and
                  Donato Tiano and
                  Angela Bonifati and
                  Themis Palpanas},
	title = {Graphint: Graph-Based Time Series Clustering Visualisation Tool},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4580--4583},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00357},
	doi = {10.1109/ICDE65448.2025.00357},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BoniolTBP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the exponential growth of time series data across diverse domains, there is a pressing need for effective analysis tools. Time series clustering is important for identifying patterns in these datasets. However, prevailing methods often encounter obstacles in maintaining data relationships and ensuring interpretability. We present Graphint, an innovative system based on the  k k -Graph methodology that addresses these challenges. Graphint integrates a robust time series clustering algorithm with an interactive tool for comparison and interpretation. More precisely, our system allows users to compare results against competing approaches, identify discriminative subsequences within specified datasets, and visualize the critical information utilized by  k k -Graph to generate outputs. Overall, Graphint offers a comprehensive solution for extracting actionable insights from complex temporal datasets.}
}


@inproceedings{DBLP:conf/icde/ZhangZZSW25,
	author = {Haodi Zhang and
                  Xiangyu Zeng and
                  Chen Zhang and
                  Yuanfeng Song and
                  Kaishun Wu},
	title = {{HRLMS:} {A} Data-Driven Hierarchical Reinforcement Learning System
                  for Interactive Rule Intervention and Visualization},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4584--4587},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00358},
	doi = {10.1109/ICDE65448.2025.00358},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangZZSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, an increasing number of deep reinforcement learning methods have achieved success in domains such as gaming, yet their inherent black-box nature poses significant challenges to the interpretability of the training process. Moreover, there is an urgent need for the ability to intervene directly and simply during training. To address these issues, we present the Interactive Hierarchical Reinforcement Learning Monitoring System (HRLMS). This framework integrates a set of rules derived from both autonomously generated rules and those input through user interaction, showcasing these rules in real-time during the training process. Throughout the system's operation, the input, integration, display, and reuse of rules form a comprehensive chain, enhancing the entirety of the workflow by seamlessly blending the training and display processes.}
}


@inproceedings{DBLP:conf/icde/YanWGWLW25,
	author = {Yu Yan and
                  Hongzhi Wang and
                  Jian Geng and
                  Zixuan Wang and
                  Xingyan Li and
                  Tianqing Wang},
	title = {{EAST:} An Interpretable Knob Estimation System for Cloud Database},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4588--4591},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00359},
	doi = {10.1109/ICDE65448.2025.00359},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YanWGWLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database vendors have made significant redesigns and developments to the relational database for providing cloud-hosted and cloud-native database services. Thus, the original knob-tuning experiences of DBAs are no longer applicable to the cloud database era. An interpretable estimation service is urgently needed to provide explicit guidance for database knob tuning. Unfortunately, less attention has been paid to estimating the performance of the knob configuration. To fill this gap, we propose EAST, a knob estimation system to provide interpretable & transferable knob estimation services for cloud databases. Firstly, we design an interpretable knob-embedding-based estimator to achieve the trusted and white-box knob estimation for researchers, practitioners, and even artificial intelligence knob tuners. Secondly, we design a two-stage transfer estimation approach by stacking ensemble learning to utilize historical experiences, improving time efficiency. Thirdly, EAST provides a user-friendly interface to support direct knob estimation and transfer knob estimation services. We have deployed our EAST11https://gitee.com/opengauss/openGauss-DBMind/tree/incubator/dbmind/components/knob_estimator to the DBMind component of OpenGauss and demonstrated the effectiveness of our system under the open-source benchmark TPCC.}
}


@inproceedings{DBLP:conf/icde/LiuYMCZJ25,
	author = {Zebang Liu and
                  Anran Yang and
                  Mengyu Ma and
                  Luo Chen and
                  Jiali Zhou and
                  Ning Jing},
	title = {HiVQ: {A} Real-time Interactive Visual Query System on Geospatial
                  Big Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4592--4595},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00360},
	doi = {10.1109/ICDE65448.2025.00360},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuYMCZJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interactive visual query systems are essential for the exploration and analysis of geospatial data. However, developing such systems has become increasingly challenging in recent years due to the conflict between the unprecedented volume of data and the need for instantaneous feedback. To address this challenge, we present HiVQ, a High-performance Visual Query system for real-time interactive visual query of geospatial big data. HiVQ adopts an innovative “Query as Visualization” paradigm, transforming user interactions into pixel value queries which can be processed efficiently with specialized indices and optimization strategies. Unlike conventional solutions that query and visualize geospatial objects sequentially, HiVQ effectively omits most geospatial objects and unnecessary computations that do not affect the final visualization, ensuring minimal sensitivity to data volume. Experimental results show that HiVQ accelerates visual queries by at least seven times compared to SOTA methods. This demonstration enables users to interactively explore and analyze spatial data with billions of nodes at any scale, receiving responses in milliseconds as they dynamically adjust analysis parameters, query conditions, or map styling. The demonstration video is available at https://gitee.com/kyrie-Bang/HiVQ-Demo.}
}


@inproceedings{DBLP:conf/icde/AhmedJB25,
	author = {Adnan Shakeel Ahmed and
                  Abhilash Jindal and
                  Kaustubh Beedkar},
	title = {Popper: {A} Dataflow System for In-Flight Error Handling in Machine
                  Learning Workflows},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4596--4599},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00361},
	doi = {10.1109/ICDE65448.2025.00361},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/AhmedJB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present Popper, a dataflow system for building Machine Learning (ML) workflows. A novel aspect of Popper is its built-in support for inflight error handling, which is crucial in developing effective ML workflows. Popper provides a convenient API that allows users to create and execute complex work-flows comprising traditional data processing operations (such as map, filter, and join) and user-defined error handlers. The latter enables inflight detection and correction of errors introduced by ML models in the workflows. Inside Popper, we model the workflow as a reactive dataflow, a directed cyclic graph, to achieve efficient execution through pipeline parallelization. We demonstrate the inflight error-handling capabilities of Popper, for which we have built a graphical interface, allowing users to specify workflows, visualize and interact with its reactive dataflow, and delve into the internals of Popper.}
}


@inproceedings{DBLP:conf/icde/DingQWSCSH25,
	author = {Xiaoou Ding and
                  Zekai Qian and
                  Hongzhi Wang and
                  Zhe Sun and
                  Siying Chen and
                  Hongbin Su and
                  Huan Hu},
	title = {UniClean: {A} Multi-Signal Fusion Pipeline for Optimizing Data Cleaning
                  Workflow},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4600--4603},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00362},
	doi = {10.1109/ICDE65448.2025.00362},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DingQWSCSH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data quality issues are prevalent in information systems, making data cleaning a complex and time-consuming task, particularly with large-scale datasets and the lack of standardized automated processes. Existing cleaning pipelines often lack automated schemes to guide the execution of cleaning algorithms and the sequence of error corrections, limiting their practicality in real-world big data applications. To address the growing demand for advanced cleaning tools driven by the complexity of data activities, we propose the UniClean framework for on-demand big data cleaning. UniClean employs a unified cleaning operation (Uniop) from multiple cleaners to optimize the data cleaning workflow. It integrates a cleaning parameter generation pipeline, a cleaning parameter selection pipeline, and a module for cleaning process preparation and optimization, covering the entire workflow from cleaner modeling and data preparation to optimal cleaning operation generation. UniClean provides an adaptive (data-driven cleaning workflow generation) and flexible (multi-signal extension system) solution to meet the urgent need for high-quality data in today's data-driven decision-making environments. We demonstrate how UniClean effectively addresses the challenges of big data cleaning across diverse information system landscapes.}
}


@inproceedings{DBLP:conf/icde/ZhangMW25,
	author = {Shi Heng Zhang and
                  Zhengjie Miao and
                  Jiannan Wang},
	title = {LineageX: {A} Column Lineage Extraction System for {SQL}},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4604--4607},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00363},
	doi = {10.1109/ICDE65448.2025.00363},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangMW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As enterprise data grows in size and complexity, column-level data lineage, which records the creation, transformation, and reference of each column in the warehouse, has been the key to effective data governance that assists tasks like data quality monitoring, storage refactoring, and workflow migration. Unfortunately, existing systems introduce overheads by integration with query execution or fail to achieve satisfying accuracy for column lineage. In this paper, we demonstrate LineageX, a lightweight Python library that infers column-level lineage from SQL queries and visualizes it through an interactive interface. LineageX achieves high coverage and accuracy for column lineage extraction by intelligently traversing query parse trees and handling ambiguities. The demonstration walks through use cases of building lineage graphs and troubleshooting data quality issues. LineageX is open sourced at https://github.com/sfu-db/lineagex and our video demonstration is at https://youtu.be/5LaBBDDitlw}
}


@inproceedings{DBLP:conf/icde/Chan25,
	author = {Harry Kai{-}Ho Chan},
	title = {{CSKQS:} {A} Query System for Collective Spatial Keyword Queries},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4608--4611},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00364},
	doi = {10.1109/ICDE65448.2025.00364},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Chan25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of location-based services, geo-textual data is becoming ubiquitous. Objects involved in geo-textual data include geospatial locations, textual descriptions or keywords, and often numerical attributes (e.g., expenses and users' ratings for points of interest). One popular spatial keyword queries on geo-textual data is the Collective Spatial Keyword Query (CoSKQ), which is to find, for a query consisting of a query location and some query keywords, a set of multiple objects such that the objects in the set collectively cover all the query keywords, and the object set is of good quality according to some criterion. In this work, we demonstrate a Collective Spatial Keyword Queries System (CSKQS) that supports both the conventional CoSKQ, and a variant called Cost-Aware and Distance-Constrained CoSKQ. CSKQS adopts the client/server system architecture, enabling users to access it through a web browser. The client side interface allows users to specify their queries and view the results, while the server side handles query processing and stores the data. CSKQS also offers an interactive map visualization, providing an intuitive view of the spatial relationship between the result object set and query location.}
}


@inproceedings{DBLP:conf/icde/BianGLCA25,
	author = {Haoqiong Bian and
                  Dongyang Geng and
                  Haoyang Li and
                  Yunpeng Chai and
                  Anastasia Ailamaki},
	title = {PixelsDB: Serverless and NL-Aided Data Analytics with Flexible Service
                  Levels and Prices},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4612--4615},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00365},
	doi = {10.1109/ICDE65448.2025.00365},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BianGLCA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serverless query processing has become increasingly popular due to its advantages, including automated resource management, high elasticity, and pay-as-you-go pricing. For users who are not system experts, serverless query processing greatly reduces the cost of owning a data analytic system. However, it is still a significant challenge for non-expert users to transform their complex and evolving data analytic needs into proper SQL queries and select a serverless query service that delivers satisfactory performance and price for each type of query. This paper presents PixelsDB, an open-source data analytic system that allows users who lack system or SQL expertise to explore data efficiently. It allows users to generate and debug SQL queries using a natural language interface powered by fine-tuned language models. The queries are then executed by a serverless query engine that offers varying prices for different performance service levels (SLAs). The performance SLAs are natively supported by dedicated architecture design and heterogeneous resource scheduling that can apply cost-efficient resources to process non-urgent queries. We demonstrate that the combination of a serverless paradigm, a natural-language-aided interface, and flexible SLAs and prices will substantially improve the usability of cloud data analytic systems.}
}


@inproceedings{DBLP:conf/icde/WangW25a,
	author = {Songyao Wang and
                  Chaokun Wang},
	title = {{RAISIN:} {A} Parallel Subgraph Matching Tool Exploiting Community
                  Structures in Social Networks},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4616--4619},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00366},
	doi = {10.1109/ICDE65448.2025.00366},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangW25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subgraph matching is a fundamental operation in graph data management and facilitates the analysis of complex datasets in various domains such as computer vision, bioinformatics, and social networks. With the scale of graph data getting larger and larger, the performance of subgraph matching algorithms is facing significant challenges. However, existing tools are not efficient enough for subgraph matching tasks. In this paper, we present a subgraph matching tool called RAISIN. To improve the efficiency of subgraph matching tasks, RAISIN accelerates the algorithms of the matching engine with community-structure-based and parallelism optimization. Moreover, to improve user-friendliness, RAISIN provides a graphical user interface to monitor the matching process and visualize matching results. Finally, three use cases are presented to demonstrate the efficiency and user-friendliness of RAISIN.}
}


@inproceedings{DBLP:conf/icde/XuCSHC25,
	author = {Boyan Xu and
                  Yuyuan Cai and
                  Shaobin Shi and
                  Zhifeng Hao and
                  Ruichu Cai},
	title = {Chat2DB: Chatting to the Database with Interactive Agent Assisted
                  Language Models},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4620--4623},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00367},
	doi = {10.1109/ICDE65448.2025.00367},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XuCSHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain Text-to-SQL necessitates the capability of semantic parsers to generalize to unseen databases, thus simplifying the process of creating natural language interfaces for databases. The existing Text-to-SQL parser exhibits limitations in its adaptability to new databases, and its execution accuracy is not sufficient for building conversational applications, typically necessitating further fine-tuning for specific databases. In this paper, we introduce Chat2DB, a conversational system designed for database interactions that enhances parser capabilities, rendering them applicable in real-world contexts. Within Chat2DB, we implement an interactive schema-ranking agent that optimizes the performance of LMs-based parsers cost-effectively. We further propose an adaptive retraining stage to allow trained Text-to-SQL parsers to quickly adapt to the target database. Experimental evaluations were conducted to validate the performance of the key components of Chat2DB. In the demonstration, we showcase the interactive visualization interface of Chat2DB to achieve more accurate querying of databases by natural language.}
}


@inproceedings{DBLP:conf/icde/XiaoCJZ25,
	author = {Yixiong Xiao and
                  Jingjia Cao and
                  Yangxin Jiang and
                  Jingbo Zhou},
	title = {{ARAG:} Analysis and Retrieval Augmented Generation for Comprehensive
                  Reasoning over Socioeconomic Data},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4624--4627},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00368},
	doi = {10.1109/ICDE65448.2025.00368},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XiaoCJZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advancements in Large Language Models (LLMs) have significantly impacted the field of question answering systems, particularly with LLM-based data analysis and Retrieval-Augmented Generation (RAG). Yet, applying them independently has limited their effectiveness in scenarios that require a synthesis of both data analysis and contemporary information retrieval. To bridge this gap, we introduce the Analysis and Retrieval Augmented Generation (ARAG) framework, which integrates data analysis with the retrieval of up-to-date information. Based on the framework, we build a system to showcase how ARAG interprets the dynamics of socioeconomic indicators by examining correlated data and retrieving relevant information from news sources. The comparison of ARAG with ChatGPT Search and Perplexity showed that ARAG significantly outperformed them in delivering indepth analytical insights. Moreover, ARAG is observed to have a stronger ability to verify facts and reject misinformation in users' queries, thus reducing LLM's susceptibility to hallucination.}
}


@inproceedings{DBLP:conf/icde/HuZYZXY25,
	author = {Zirui Hu and
                  Rong Zhang and
                  Chengcheng Yang and
                  Xuan Zhou and
                  Quanqing Xu and
                  Chuanhui Yang},
	title = {Artemis: {A} Customizable Workload Generation Toolkit for Benchmarking
                  Cardinality Estimation},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4628--4631},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00369},
	doi = {10.1109/ICDE65448.2025.00369},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HuZYZXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardinality Estimation (CardEst) is crucial for query optimization. Despite the remarkable achievement in DBMS, there is a pressing need to test or tune the work of CardEst. To satisfy the need, we introduce Artemis, a customizable workload generator, which can be used to generate various scenarios with the sensitive features for CardEst, including various data dependencies, complex SQL structures, and diverse cardinalities. It designs a PK-oriented deterministic data generation mechanism to plot various data characteristics; a search-based workload generation is proposed for composing queries with various complexities; it takes a constraint optimization-guided way to achieve a cost-effective cardinality calculation. In this demonstration, users can explore the core features of Artemis in generating workloads.}
}


@inproceedings{DBLP:conf/icde/ZengZPTXZ25,
	author = {Weixin Zeng and
                  Shiqi Zhang and
                  Huang Peng and
                  Zhen Tan and
                  Weidong Xiao and
                  Xiang Zhao},
	title = {{IKGA:} An Interactive Visualization Tool for Knowledge Graph Alignment},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4632--4635},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00370},
	doi = {10.1109/ICDE65448.2025.00370},
	timestamp = {Tue, 28 Oct 2025 15:43:30 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZengZPTXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graph Alignment (KGA) identifies elements in different knowledge graphs (KGs) that refer to the same real-world object. It is a key step towards improving the coverage of KGs, which in turn can better facilitate downstream tasks. However, there is currently no interactive tool to support KGA research, particularly for visualizing alignment results, hence limiting the understanding of the procedure and also the development of more advanced solutions. To fill in this gap, in this paper, we introduce IKGA, an interactive visualization tool for KGA, which visualizes the alignment process by integrating various algorithms of representation learning and alignment inference-two key steps in KGA. The system allows users to observe KGA results, compare existing algorithms, and apply to domain-specific KGs at hand.}
}


@inproceedings{DBLP:conf/icde/DingSQCCLWW25,
	author = {Xiaoou Ding and
                  Hongbin Su and
                  Zekai Qian and
                  Wenxuan Cui and
                  Siying Chen and
                  Zheng Liang and
                  Chen Wang and
                  Hongzhi Wang},
	title = {CBAClean:A Comprehensive System for Recommending Data Cleaning Solutions
                  Through Cost-Benefit Analysis in Data Quality Management},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4636--4639},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00371},
	doi = {10.1109/ICDE65448.2025.00371},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DingSQCCLWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The scale of data analysis tasks have increased, highlighting the critical importance of data quality. Data quality assessment and repair have become pivotal in data preparation. Despite the availability of numerous algorithms for data cleaning, these often focus on optimizing efficiency and minimizing labor costs, neglecting the explicit relationship between data quality management costs and benefits. This omission can lead to the failure of promising data analysis solutions. To address this, we propose CBAClean, a comprehensive system that integrates cost-benefit analysis into data cleaning. CBAClean aims to assist users in quantifying the costs of data quality management and providing optimal data cleaning solutions tailored to their needs. Key features include task-centered multi-perspective data quality assessment, a comprehensive data quality repair operator library, fine-grained human role division for effective cost control, and recommendation of optimal data cleaning solutions based on cost-benefit calculations. By incorporating cost-benefit analysis, CBAClean enhances the practical application of data quality management on real-world data governance platforms.}
}


@inproceedings{DBLP:conf/icde/LiangZYCZZ25,
	author = {Hao Liang and
                  Keshi Zhao and
                  Yajie Yang and
                  Bin Cui and
                  Zenan Zhou and
                  Wentao Zhang},
	title = {Training Data Distribution Estimation for Optimized Pre-training Data
                  Management},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4640--4648},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00372},
	doi = {10.1109/ICDE65448.2025.00372},
	timestamp = {Mon, 03 Nov 2025 20:25:59 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiangZYCZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks and domains, with data preparation playing a critical role in achieving these results. Pretraining data typically combines information from multiple domains. To maximize performance when integrating data from various domains, determining the optimal data distribution is essential. However, state-of-the-art (SOTA) LLMs rarely disclose details about their pretraining data, making it difficult for researchers to identify ideal data distributions. In this paper, we introduce a new approach, data distribution estimation, which enables the automatic estimation of pretraining data distributions by analyzing the generated outputs of LLMs. We provide rigorous theoretical proofs, practical algorithms, and preliminary experimental results for data distribution estimation. Based on these findings, we offer valuable insights into the challenges and future directions for effective data distribution estimation and data management. The source code, data, and other artifacts are available at https://github.com/yangyajie0625/data_detection}
}


@inproceedings{DBLP:conf/icde/BerensCCT25,
	author = {Maximilian Berens and
                  Yun{-}Chih Chen and
                  Jian{-}Jia Chen and
                  Jens Teubner},
	title = {Beyond Bandwidth Doubling: Embrace Bit-Flips and Unlock Processing-in-NAND},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4649--4661},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00373},
	doi = {10.1109/ICDE65448.2025.00373},
	timestamp = {Thu, 25 Dec 2025 12:47:48 +0100},
	biburl = {https://dblp.org/rec/conf/icde/BerensCCT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {NVMe SSDs offer unprecedented capacity and bandwidth and upcoming PCIe standards promise even more. However, the underlying technology, NAND memory, already struggles with significant heat and power consumption challenges. Just like microprocessors before, NAND also experiences Dark Silicon, preventing performance from improving at the same pace as capacity. Much of the power (and thus heat) within a NAND chip results from transferring data at a high rate, another symptom of a compute-centric style of processing. Therefore, we argue for data-centric Processing-in-NAND (PiN). However, PiN comes with significant challenges, such as limited capabilities and the need to cope with bit-flip errors. Even beyond Processing-in-Memory (PiM), databases may soon have to accept that memory is not error-free, an assumption that comes at a significant cost in power, capacity and performance. Our discussion indicates that no PiN design will serve as a singular, universally applicable solution to the limit of bandwidth scaling. Instead, successful integration into database architecture requires carefully identifying PiN-compatible functionality and abstractions, and cooperation with other innovations, such as Computational Storage and CXL. Lastly, we analyze the fundamental error tolerance of Bloom filters and binary sketches as PiM-compatible data structures, which we believe may be of independent interest.}
}


@inproceedings{DBLP:conf/icde/WangF25,
	author = {Jiayi Wang and
                  Jianhua Feng},
	title = {Unify: An Unstructured Data Analytics System},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4662--4674},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00374},
	doi = {10.1109/ICDE65448.2025.00374},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite unstructured data constituting over 80% of the information available today, no specialized analytics system currently exists to process this type of data. The lack of a predefined schema in unstructured data renders traditional structured query languages, such as SQL, unsuitable for unstructured data analytics. A seemingly straightforward solution is to utilize natural language for crafting queries, thereby making analytics more accessible to users without technical expertise. However, understanding and executing queries posed in natural language presents significant challenges. A brute-force approach involves relying on users to manually derive solutions, tapping into their knowledge and experience. However, this method of generating query plans by human intervention is neither scalable nor efficient. Therefore, a pertinent question arises: how can we automate unstructured data analytics? To address these challenges, this paper introduces Unify, an innovative system leveraging the capabilities of large language models (LLMs) to automatically generate, optimize, and execute query plans for unstructured data analytics, where queries are articulated in natural language. Unify initializes by defining common operators used in unstructured data analytics and creates both pre-programmed implementations and LLM-based implementations for physically executing these operators. It then guides LLMs to devise logical plans by methodically deconstructing queries into smaller steps, ensuring accurate logic by aligning with suitable operators. For translating a logical plan into an optimal physical plan, we further introduce a technique for physical plan optimization that employs a semantic cost model alongside semantic cardinality estimation. Comprehensive tests conducted on real-world datasets demonstrate that Unify can expedite query processing by up to $40 \\times$, while preserving high accuracy, thus positioning Unify as an effective tool for largescale unstructured data analytics.}
}


@inproceedings{DBLP:conf/icde/WeintraubGD25,
	author = {Grisha Weintraub and
                  Ehud Gudes and
                  Shlomi Dolev},
	title = {Optimizing Cloud Data Lake Queries by Minimizing the Query Coverage
                  Set},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4675--4679},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00375},
	doi = {10.1109/ICDE65448.2025.00375},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WeintraubGD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud data lakes provide a modern solution for managing large volumes of data. The fundamental principle behind these systems is the separation of compute and storage layers. In this architecture, inexpensive cloud storage is utilized for data storage, while compute engines are employed to perform analytics on this data in an “on-demand” mode. However, to execute any calculations on the data, it must be transferred from the storage layer to the compute layer over the network for each query. This transfer can negatively impact calculation performance and requires significant network bandwidth. In our work, we examine various strategies to enhance query performance within a cloud data lake architecture. We begin by formalizing the problem and proposing a straightforward yet robust theoretical framework that clearly outlines the associated trade-offs. Central to our framework is the concept of a “query coverage set,” which is defined as the collection of files that need to be accessed from storage to fulfill a specific query. Our objective is to identify the minimal coverage set for each query and execute the query exclusively on this subset of files. This approach enables us to significantly improve query performance across three different domains: indexing, caching, and genetic data.}
}


@inproceedings{DBLP:conf/icde/WangZGW25,
	author = {Maolin Wang and
                  Xiangyu Zhao and
                  Ruocheng Guo and
                  Junhui Wang},
	title = {MetaLoRA: Tensor-Enhanced Adaptive Low-Rank Fine-Tuning},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4680--4684},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00376},
	doi = {10.1109/ICDE65448.2025.00376},
	timestamp = {Sun, 01 Feb 2026 13:26:40 +0100},
	biburl = {https://dblp.org/rec/conf/icde/WangZGW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There has been a significant increase in the deployment of neural network models, presenting substantial challenges in model adaptation and fine-tuning. Efficient adaptation is crucial in maintaining model performance across diverse tasks and domains. While Low-Rank Adaptation (LoRA) has emerged as a promising parameter-efficient fine-tuning method, its fixed parameter nature limits its ability to handle dynamic task requirements effectively. Adapting models to new tasks can be challenging due to the need for extensive fine-tuning. Current LoRA variants primarily focus on general parameter reduction while overlooking the importance of dynamic parameter adjustment and meta-learning capabilities. Moreover, existing approaches mainly address static adaptations, neglecting the potential benefits of task-aware parameter generation in handling diverse task distributions. To address these limitations, this Ph.D. research proposes a LoRA generation approach to model task relationships and introduces MetaLoRA, a novel parameter-efficient adaptation framework incorporating meta-learning principles. This work develops a comprehensive architecture that integrates meta-parameter generation with adaptive low-rank decomposition, enabling efficient handling of both task-specific and task-agnostic features. MetaLoRA accurately captures task patterns by incorporating meta-learning mechanisms and dynamic parameter adjustment strategies. To our knowledge, this research represents the first attempt to provide a meta-learning enhanced LoRA variant, offering improved adaptation capability while maintaining computational efficiency in model fine-tuning.}
}


@inproceedings{DBLP:conf/icde/Shang25,
	author = {Wenbo Shang},
	title = {Leveraging LLMs for Diffusion Prediction in Social Networks: {A} Fused
                  Attention-aware Model},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4685--4689},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00377},
	doi = {10.1109/ICDE65448.2025.00377},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Shang25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information diffusion prediction, which aims to forecast future infected users during the information spreading process in social networks, is a challenging and critical task for applications such as social recommendation and fake news detection. Most existing methods primarily leverage GNNs and RNNs to capture structural and temporal patterns on social networks. However, the rich information contained in user profiles and post content is neglected, and the diffusion influence between users is challenging to model. To address these limitations, this paper proposes a fused attention-aware diffusion prediction method based on our fine-tuned Diffusion-LLM to improve the accuracy and interpretability of prediction results. The fused attention mechanism incorporates the diffusion influence analyzed by Diffusion-LLM and the user similarity matrix derived from a Transformer model based on user profiles. Preliminary experiments are conducted to verify the feasibility of our Diffusion-LLM.}
}


@inproceedings{DBLP:conf/icde/Lin25,
	author = {Teng Lin},
	title = {Simplifying Data Integration: SLM-Driven Systems for Unified Semantic
                  Queries Across Heterogeneous Databases},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4690--4693},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00378},
	doi = {10.1109/ICDE65448.2025.00378},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/Lin25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of heterogeneous databases into a unified querying framework remains a critical challenge, particularly in resource-constrained environments. This paper presents a novel Small Language Model (SLM)-driven system that synergizes advancements in lightweight Retrieval-Augmented Generation (RAG) and semantic-aware data structuring to enable efficient, accurate, and scalable query resolution across diverse data formats. By integrating semantic-aware heterogeneous graph indexing and topology-enhanced retrieval with SLM- powered structured data extraction, our system addresses the limitations of traditional methods in handling Multi-Entity Question Answering (Multi-Entity QA) and complex semantic queries. The introduction of semantic entropy as an unsupervised evaluation metric provides robust insights into model uncertainty. Together, these innovations establish a domain-agnostic, resource-efficient paradigm for executing complex queries across structured, semi-structured, and unstructured data sources, aiming at foundational advancement for next-generation intelligent database systems.}
}


@inproceedings{DBLP:conf/icde/XieZ25,
	author = {Zhen Xie and
                  Xiang Zhao},
	title = {StreamSC: {A} Learning-Based Framework for Efficient Subgraph Counting
                  in Stream Graphs},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4694--4698},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00379},
	doi = {10.1109/ICDE65448.2025.00379},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/XieZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs serve as essential data structures for modeling intricate relationships among entities across various domains, including social networks and chemical reactions. When dealing with large-scale graph data, the stream graph approach is frequently employed to process and manage the data efficiently. The subgraph counting problem in stream graphs is recognized as a important and challenging task, primarily because its core component, subgraph matching, is classified as NP-complete. In this work, we propose deep learning-based solution framework named StreamSC to solve the subgraph counting in stream graphs. This framework offers two key advantages: (i) It's the first learning-based framework to address the subgraph counting problem focused on stream graphs; and(ii) this framework addresses the issue of dynamic changes in the topology of the data graph caused by the addition or deletion of edges in stream graphs through specialized design and optimization.}
}


@inproceedings{DBLP:conf/icde/BouAK25,
	author = {Savong Bou and
                  Toshiyuki Amagasa and
                  Hiroyuki Kitagawa},
	title = {O(1)-Time Complexity for Fixed Sliding-Window Aggregation Over Out-of-Order
                  Data Streams: (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4704--4705},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00381},
	doi = {10.1109/ICDE65448.2025.00381},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BouAK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sliding-window aggregation is important in analyzing data streams, but it seriously suffers from out-of-order streams, which contain late events. The existing approaches are not efficient because they are dependent on the window size. They ignore the past windows affected by the late records. This paper proposes two solutions: (1) CMiX for computing the current window, and (2) PWiX for updating the past windows. Experiments show that CMiX and PWiX can deal with out-of-order streams significantly better than other approaches.}
}


@inproceedings{DBLP:conf/icde/LiuWFC25,
	author = {Ziyang Liu and
                  Chaokun Wang and
                  Hao Feng and
                  Ziyang Chen},
	title = {Efficient Unsupervised Graph Embedding with Attributed Graph Reduction
                  and Dual-Level Loss: (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4706--4707},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00382},
	doi = {10.1109/ICDE65448.2025.00382},
	timestamp = {Wed, 10 Sep 2025 14:09:47 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuWFC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph embedding aims to extract low-dimensional representation vectors, commonly referred to as embeddings, from graph data. The generated embeddings simplify subsequent data analysis and machine learning tasks. Recently, researchers have proposed the use of contrastive learning on graphs to extract node embeddings in an unsupervised manner. Although existing graph contrastive learning methods have significantly advanced this field, there is still potential for further exploration, particularly in optimizing training efficiency and enhancing embedding quality. In this paper, we propose an efficient unsupervised graph embedding method named GEARED. First, the method involves an attributed graph reduction module that converts the raw graph into a reduced graph, greatly improving training efficiency. Second, GEARED employs a dual-level loss with adaptive scaling factors to obtain high-quality embeddings. Experimental results demonstrate that GEARED achieves high classification accuracy, good scalability, and enhanced embedding quality.}
}


@inproceedings{DBLP:conf/icde/LuZLGLYZYT25,
	author = {Weigang Lu and
                  Yibing Zhan and
                  Binbin Lin and
                  Ziyu Guan and
                  Liu Liu and
                  Baosheng Yu and
                  Wei Zhao and
                  Yaming Yang and
                  Dacheng Tao},
	title = {SkipNode: On Alleviating Performance Degradation for Deep Graph Convolutional
                  Networks (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4708--4709},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00383},
	doi = {10.1109/ICDE65448.2025.00383},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LuZLGLYZYT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Convolutional Networks (GCNs) are powerful tools for learning representations in graph-structured data. However, their performance tends to degrade with increased model depth due to over-smoothing. Although previous studies attribute degradation to over-smoothing, this work identifies the mutually reinforcing effects of over-smoothing and gradient vanishing as the root cause. In this paper, we propose SkipNode, a plug-and-play module that mitigates degradation in deep GCNs. SkipNode introduces node-sampling in each convolutional layer to selectively skip convolutions, preventing over-smoothing by reducing the depth experienced by specific nodes and facilitating gradient backpropagation. We demonstrate both theoretically and experimentally that SkipNode effectively curtails over-smoothing and gradient vanishing, improving deep GCN performance across diverse tasks. Extensive evaluations show SkipNode's robustness and superior performance over state-of-the-art (SOTA) baselines, establishing it as a practical solution for training deep GCNs.}
}


@inproceedings{DBLP:conf/icde/ZhangWSWTQCWY25,
	author = {Weixu Zhang and
                  Yifei Wang and
                  Yuanfeng Song and
                  Victor Junqiu Wei and
                  Yuxing Tian and
                  Yiyan Qi and
                  Jonathan H. Chan and
                  Raymond Chi{-}Wing Wong and
                  Haiqin Yang},
	title = {Natural Language Interfaces for Tabular Data Querying and Visualization:
                  {A} Survey (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4710--4711},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00384},
	doi = {10.1109/ICDE65448.2025.00384},
	timestamp = {Thu, 25 Dec 2025 12:47:49 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ZhangWSWTQCWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Natural Language Interfaces (NLIs) have transformed data interaction by enabling natural language querying and visualization of tabular data. Despite the growing importance of NLIs, prior research has examined querying and visualization tasks separately, lacking a unified perspective, especially in the era of Large Language Models (LLMs). To fill this gap, this survey provides a comprehensive analysis of NLIs for tabular data, examining their evolution and fundamental components: datasets, evaluation metrics, and architectural designs. By analyzing over 60 approaches and 38 datasets, we explore recent advancements in Text-to-SQL and Text-to-Vis tasks, focusing on semantic parsing techniques for natural language translation to SQL queries and visualization specifications. We evaluate the impact of LLMs on these systems, discussing their capabilities and limitations. Our systematic review serves as a roadmap for developing NLIs in the foundation model era.}
}


@inproceedings{DBLP:conf/icde/GazaB25,
	author = {Haifa Gaza and
                  Jaewook Byun},
	title = {Kairos: Enabling Prompt Monitoring of Information Diffusion Over Temporal
                  Networks (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4712--4713},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00385},
	doi = {10.1109/ICDE65448.2025.00385},
	timestamp = {Thu, 11 Sep 2025 20:25:49 +0200},
	biburl = {https://dblp.org/rec/conf/icde/GazaB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analyses of temporal graphs provide valuable insights into temporal data through the use of two analytical approaches: temporal evolution and temporal information diffusion. The former shows how a network evolves over time; the latter explains how information spreads throughout a network over time. Systems have been mainly proposed to efficiently handle graph snapshots, which are suitable for temporal evolution but inappropriate for temporal information diffusion. For analyses of temporal information diffusion, temporal graph traversal platforms have recently been proposed; however, it is still infeasible to handle infinitely evolving temporal data, especially for monitoring applications. In this paper, we propose an incremental approach and its graph processing engine, Kairos, to enable prompt monitoring of temporal information diffusion. This approach makes it possible to immediately process diffusion results for sources of interest by traversing a part of the whole network, which avoids full traversals influenced by a small change in the network, thus making monitoring applications feasible. The recipes for implementing incremental versions of existing temporal graph traversal algorithms and metrics will make it easier for users to build their ad-hoc programs.}
}


@inproceedings{DBLP:conf/icde/KongLYZGZ25,
	author = {Lingbai Kong and
                  Wengen Li and
                  Hanchen Yang and
                  Yichao Zhang and
                  Jihong Guan and
                  Shuigeng Zhou},
	title = {CausalFormer: An Interpretable Transformer for Temporal Causal Discovery
                  (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4714--4715},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00386},
	doi = {10.1109/ICDE65448.2025.00386},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/KongLYZGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal causal discovery aims to uncover causal relations in time series data. Current deep learning-based methods usually analyze the parameters of some components of the trained models, which is an incomplete mapping process from the model parameters to the causality and fails to investigate the other components. To address this, this paper presents an interpretable transformer-based causal discovery model termed CausalFormer, which consists of: 1) the causality-aware transformer which learns the causal representation with the multi-kernel causal convolution under the temporal priority constraint, and 2) the decomposition-based causality detector which identifies causality by interpreting the global structure of the trained transformer with the regression relevance propagation.}
}


@inproceedings{DBLP:conf/icde/WuFCLLHLT25,
	author = {Jiahao Wu and
                  Wenqi Fan and
                  Jingfan Chen and
                  Shengcai Liu and
                  Qijiong Liu and
                  Rui He and
                  Qing Li and
                  Ke Tang},
	title = {Condensing Pre-Augmented Recommendation Data via Lightweight Policy
                  Gradient Estimation (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4716--4717},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00387},
	doi = {10.1109/ICDE65448.2025.00387},
	timestamp = {Wed, 10 Sep 2025 14:09:50 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WuFCLLHLT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Training recommendation models on large datasets is time- and resource-intensive. It is desired to construct concise yet informative datasets for efficient training. Recent advances in dataset condensation offer a promising solution by synthesizing compact datasets. However, existing methods face two key limitations when applied to recommendation: (1) they fail to generate discrete user-item interactions, and (2) they could not preserve users' potential preferences. To address the limitations, we propose a lightweight condensation framework tailored for recommendation (DConRec), focusing on condensing user-item historical interaction sets. Specifically, we model the discrete user-item interactions via a probabilistic approach and design a pre-augmentation module to incorporate the potential user preferences into the condensed datasets. While the substantial size of datasets leads to costly optimization, we propose a lightweight policy gradient estimation to accelerate the data synthesis. Experimental results on multiple real-world datasets demonstrate the effectiveness and efficiency of DConRec. Besides, we theoretically examine the provable convergence of DConRec.}
}


@inproceedings{DBLP:conf/icde/FengWHNL25,
	author = {Wenjie Feng and
                  Li Wang and
                  Bryan Hooi and
                  See{-}Kiong Ng and
                  Shenhua Liu},
	title = {Interrelated Dense Pattern Detection in Multilayer Networks (Extended
                  Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4718--4719},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00388},
	doi = {10.1109/ICDE65448.2025.00388},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/FengWHNL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a heterogeneous multilayer network with various connections in pharmacology, how can we detect components with intensive interactions and strong dependencies? Can we accurately capture suspicious groups in a multi-lot transaction network under camouflage? These challenges related to dense subgraph detection have been extensively studied in simple graphs but remain under-explored in complex networks. Existing methods struggle to effectively handle the intricate dependencies, let alone accurately identify the interrelated dense connected patterns within a series of complex heterogeneous networks. Here, we introduce INDUEN, a novel algorithm designed to detect interrelated densest subgraphs in multilayer networks by leveraging joint optimization of coupled factorization and local search for an elaborate-designed joint density measure. Experimental results demonstrate that INDUEN outperforms the state-of-the-art baselines in accurately detecting interrelated densest sub graphs under various settings. Furthermore, INDUEN uncovers some intriguing patterns in real-world data; it is linearly scalable and achieves more than 35 × speedup compared to the state-of-the-art method Destine.}
}


@inproceedings{DBLP:conf/icde/LiPLLLLTL25,
	author = {Liang Li and
                  Yuangang Pan and
                  Jie Liu and
                  Yue Liu and
                  Xinwang Liu and
                  Kenli Li and
                  Ivor W. Tsang and
                  Keqin Li},
	title = {{BGAE:} Auto-encoding Multi-view Bipartite Graph Clustering (Extended
                  Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4720--4721},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00389},
	doi = {10.1109/ICDE65448.2025.00389},
	timestamp = {Thu, 27 Nov 2025 07:55:26 +0100},
	biburl = {https://dblp.org/rec/conf/icde/LiPLLLLTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of multimodal and multi-view data, multi-view bipartite graph clustering (MVBGC) has emerged as a promising solution for large-scale tasks, which with linear complexity. However, most methods adhere to a unidirectional “encoding” design, where the bipartite graph is directly constructed from input data. Enlightened by the prevalent encoding-decoding in deep learning, this paper rethinks existing paradigms and proposes a novel “auto-encoding” MVBGC framework, named BGAE. Our model seamlessly integrates encoding, bipartite graph learning, and decoding modules within a self-supervised learning framework. The encoding module extracts a joint representation from input data, the bipartite graph learning module learns a discriminative bipartite graph in latent semantic space, and the decoding module reconstructs the input data by the structural information. Extensive experiments verify the superiority of our novel design, particularly highlighting the critical role of “decoding” learning. This work represents the first attempt to explore encoding-decoding design in MVBGC.}
}


@inproceedings{DBLP:conf/icde/LiLDWLC25,
	author = {Yanni Li and
                  Bing Liu and
                  Tihua Duan and
                  Zhi Wang and
                  Hui Li and
                  Jiangtao Cui},
	title = {A Novel Key Point based {MLCS} Algorithm for Big Sequences Mining
                  (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4722--4723},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00390},
	doi = {10.1109/ICDE65448.2025.00390},
	timestamp = {Wed, 10 Sep 2025 14:09:47 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiLDWLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mining multiple longest common subsequences (MLCS) from a set of sequences of three or more over a finite alphabet  \\Sigma \\Sigma  (a classical NP-hard problem [1]) is an important task in many fields, e.g., bio-informatics, computational genomics, pattern recognition, information extraction, etc. Applications in these fields often involve generating very long sequences (length  \\geq 10_{,}000) \\geq 10_{,}000) , referred to as big sequences. However, both existing exact and approximate MLCS algorithms face severe challenges in handling big sequences due to the over-whelming size of their problem-solving graph model MLCS\xad-  DAG DAG  (Directed Acyclic Graph), leading to the issue of memory explosion or extremely high time complexity.}
}


@inproceedings{DBLP:conf/icde/WangZLCYX25,
	author = {Meng Wang and
                  Mengfei Zhao and
                  Hui Li and
                  Jiangtao Cui and
                  Bo Yang and
                  Tao Xue},
	title = {MC\({}^{\mbox{2}}\)LS: Towards Efficient Collective Location Selection
                  in Competition: (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4724--4725},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00391},
	doi = {10.1109/ICDE65448.2025.00391},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WangZLCYX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collective Location Selection (CLS) aims to identify  k k  optimal sites for facility establishment to collectively maximize user attraction. Traditional CLS approaches often overlook user mobility and inter-facility competition, critical factors in real-world scenarios. This paper introduces MC2LS, the first effort on CLS that addresses these gaps by considering user mobility and peer competition. Solving MC2LS is nontrivial due to its NP-hardness. To overcome the challenge of pruning multi-point users with highly overlapping minimum boundary rectangles (MBRs), we develop a position count threshold and two square-based pruning rules. We propose IQuad-tree, a user-MBR-free index, to benefit the hierarchical and batch-wise properties of the pruning rules. We present an  (1-\\frac{1}{e}) (1-\\frac{1}{e}) -approximate greedy solution to MC2LS, and empirical studies demonstrate the superiority of our proposed solution over the state-of-the-art techniques.}
}


@inproceedings{DBLP:conf/icde/DuLLXYC25,
	author = {Xinqi Du and
                  Ziyue Li and
                  Cheng Long and
                  Yongheng Xing and
                  Philip S. Yu and
                  Hechang Chen},
	title = {FELight: Fairness-Aware Traffic Signal Control via Sample-Efficient
                  Reinforcement Learning (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4726--4727},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00392},
	doi = {10.1109/ICDE65448.2025.00392},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/DuLLXYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic congestion is becoming an increasingly prominent problem, and intelligent traffic signal control methods can effectively alleviate it. Recently, there has been a growing trend of applying reinforcement learning to traffic signal control for adaptive signal scheduling. However, most existing methods focus on improving traffic performance while neglecting the issue of scheduling fairness, resulting in long waiting time for some vehicles. Some works attempt to address fairness issues but often sacrifice transport performance. Furthermore, existing methods overlook the challenge of sample efficiency, especially when dealing with diversity-limited traffic data. Therefore, we propose a Fairess-aware and sample-Efficient traffic signal control method called FELight. Specifically, we first design a novel fairness metric and integrate it into decision process to penalize cases with high latency by setting a threshold for activating the fairness mechanism. Theoretical comparison with other fairness works proves why and when our fairness could bring advantages. Moreover, counterfactual data augmentation is employed to enrich interaction data, enhancing the sample efficiency of FELight. Self-supervised state representation is introduced to extract informative features from raw states, further improving sample efficiency. Experiments on real traffic datasets demonstrate that FELight provides relatively fairer traffic signal control without compromising performance compared to state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/icde/MaLLXJSM25,
	author = {Chengyan Ma and
                  Di Lu and
                  Chaoyue Lv and
                  Ning Xi and
                  Xiaohong Jiang and
                  Yulong Shen and
                  Jian Feng Ma},
	title = {BiTDB: Constructing {A} Built-in {TEE} Secure Database for Embedded
                  Systems (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4728--4729},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00393},
	doi = {10.1109/ICDE65448.2025.00393},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/MaLLXJSM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose BiTDB, a built-in Trusted Execution Environment (TEE) database for embedded systems, to realize higher system availability while ensuring data confidentiality. With BiTDB, dilemmas that the state-of-the-art research work on secure embedded databases has to face can be significantly reduced and eliminated, including (i) complicated research and realization on searchable encryption algorithms (SEA), (ii) limited support to all database operations, and (iii) almost none of specific design and optimizations toward built-in TEE embedded databases. Through BiTDB, all database operations can process plaintext in TEE instead of retrieving ciphertext by developing complicated SEAs. To enable BiTDB to handle database files in Rich Execution Environment (REE) as local ones, we extend the TEE OS with generic file I/O libraries. Then, we contribute three critical optimizations to significantly reduce redundant memory and file operations between TEE and REE, and BiTDB achieve better system performance and availability in embedded systems. Finally, we have implemented the prototype system based on OP-TEE and SQLite for several typical platforms, including virtualization and hardware environments. The TPC-H test shows BiTDB can achieve 85% (on average) of the original database performance while guaranteeing data confidentiality and integrity.}
}


@inproceedings{DBLP:conf/icde/ChangLS25,
	author = {Zhao Chang and
                  Feifei Li and
                  Yulong Shen},
	title = {Generalized Measure-Biased Sampling and Priority Sampling: (Extended
                  Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4730--4731},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00394},
	doi = {10.1109/ICDE65448.2025.00394},
	timestamp = {Sun, 04 Jan 2026 13:43:24 +0100},
	biburl = {https://dblp.org/rec/conf/icde/ChangLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sampling schemes can provide fast approximate answers to aggregation queries. However, weighted sampling must create a sample for each measure column, which leads to expensive storage cost for any table with dozens of columns. To address this issue, we generalize both measure-biased sampler and priority sampler, which can compress the samples but still provide fast approximate answers to both distribution query and subset-sum query within a user-specified error bound. We also extend the priority sampler to support multiple types of aggregates for arbitrary subset. Our generalized samplers achieve a remarkable improvement over the original samplers in terms of the error metrics.}
}


@inproceedings{DBLP:conf/icde/LiFZMBHW25,
	author = {Xuhui Li and
                  Zhen Fang and
                  Yonggang Zhang and
                  Ning Ma and
                  Jiajun Bu and
                  Bo Han and
                  Haishuai Wang},
	title = {Characterizing Submanifold Region for Out-of-Distribution Detection:
                  (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4732--4733},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00395},
	doi = {10.1109/ICDE65448.2025.00395},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiFZMBHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting out-of-distribution (OOD) samples poses a significant safety challenge when deploying models in open-world scenarios. Advanced works assume that OOD and in-distributional (ID) samples exhibit a distribution discrepancy, showing an encouraging direction in estimating the uncertainty with embedding features or predicting outputs. In this work, we propose a data structure-aware approach to mitigate the sensitivity of distances to the “curse of dimensionality”, where high-dimensional features are mapped to the manifold of ID samples, leveraging the well-known manifold assumption. Specifically, we present a novel distance termed as tangent distance, which tackles the issue of generalizing the meaningfulness of distances on testing samples to detect OOD inputs. Extensive experiments show that the tangent distance performs competitively with other post hoc OOD detection baselines on common and large-scale benchmarks.}
}


@inproceedings{DBLP:conf/icde/ChengCSL25,
	author = {Yao Cheng and
                  Minjie Chen and
                  Caihua Shan and
                  Xiang Li},
	title = {Learning Prioritized Node-Wise Message Propagation in Graph Neural
                  Networks (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4734--4735},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00396},
	doi = {10.1109/ICDE65448.2025.00396},
	timestamp = {Fri, 26 Sep 2025 08:04:30 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ChengCSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs are ubiquitous in the real world, in graphs, nodes represent entities and edges capture their relationships. Recently, graph neural networks (GNNs) [3]–[6] have been proposed to integrate these two sources of information. In GNNs, a node's embedding is learned by aggregating messages from its neighbors.}
}


@inproceedings{DBLP:conf/icde/YangRF25,
	author = {Jiawei Yang and
                  Susanto Rahardja and
                  Pasi Fr{\"{a}}nti},
	title = {Smoothing Outlier Scores is All You Need to Improve Outlier Detectors
                  (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4736--4737},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00397},
	doi = {10.1109/ICDE65448.2025.00397},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/YangRF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing outlier detectors calculate outlier scores for data objects independently, ignoring the consistency between score similarity and object similarity. As a result, these detectors may produce inconsistent scores for similar objects, leading the scores of some normal objects to exceed some of outlier objects, increasing the possibility of misclassification. To address this issue, we first assume that similar objects should have similar scores. Then, based on this assumption, we propose neighborhood averaging, an outlier score post-processing technique to improve any single outlier detector, which is the first of its kind.}
}


@inproceedings{DBLP:conf/icde/BaiCWLLYH25,
	author = {Lu Bai and
                  Lixin Cui and
                  Yue Wang and
                  Ming Li and
                  Jing Li and
                  Philip S. Yu and
                  Edwin R. Hancock},
	title = {{HAQJSK:} Hierarchical-Aligned Quantum Jensen-Shannon Kernels for
                  Graph Classification (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4738--4739},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00398},
	doi = {10.1109/ICDE65448.2025.00398},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/conf/icde/BaiCWLLYH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a family of Hierarchical Aligned Quantum Jensen-Shannon Kernels (HAQJSK) for un-attributed graphs. The HAQJSK kernels can incorporate hierarchical correspondence information between graphs, and thus transform arbitrary sized graphs into fix-sized aligned structures, i.e., the hierarchical transitive aligned Adjacency Matrix of vertices or Density Matrix of Continuous-Time Quantum Walks (CTQWs). For pairwise graphs, the resulting HAQJSK kernels are defined by computing the Quantum Jensen-Shannon Divergence (QJSD) between their aligned structures. Unlike classical graph kernels, the HAQJSK kernels can either reflect global intrinsic structure characteristics through CTQWs, or address the drawback of neglecting structural correspondence information, theoretically explaining the effectiveness.}
}


@inproceedings{DBLP:conf/icde/LiuLWLZ25,
	author = {Suyuan Liu and
                  Qing Liao and
                  Siwei Wang and
                  Xinwang Liu and
                  En Zhu},
	title = {Robust and Consistent Anchor Graph Learning for Multi-View Clustering
                  (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4742--4743},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00400},
	doi = {10.1109/ICDE65448.2025.00400},
	timestamp = {Wed, 10 Sep 2025 14:09:52 +0200},
	biburl = {https://dblp.org/rec/conf/icde/LiuLWLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anchor-based multi-view graph clustering has recently gained popularity as an effective approach for clustering data with multiple views. However, existing methods have limitations in terms of handling inconsistent information and noise across views, resulting in an unreliable consensus representation. Additionally, post-processing is needed to obtain final results after anchor graph construction, which negatively affects clustering performance. In this paper, we propose a Robust and Consistent Anchor Graph Learning method (RCAGL) for multi-view clustering to address these challenges. RCAGL constructs a consistent anchor graph that captures inter-view commonality and filters out view-specific noise by learning a consistent part and a view-specific part simultaneously. A k-connectivity constraint is imposed on the consistent anchor graph, leading to a clear graph structure and direct generation of cluster labels without additional post-processing. Experimental results on several benchmark datasets demonstrate the superiority of RCAGL in terms of clustering accuracy, scalability to large-scale data, and robustness to view-specific noise, outperforming advanced multi-view clustering methods. Our code is publicly available at https://github.com/Tracesource/RCAGL.}
}


@inproceedings{DBLP:conf/icde/ZhangZTZ25,
	author = {Junfeng Zhang and
                  Weixin Zeng and
                  Jiuyang Tang and
                  Xiang Zhao},
	title = {Hyperedge Graph Contrastive Learning [Extended abstract]},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4744--4745},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00401},
	doi = {10.1109/ICDE65448.2025.00401},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhangZTZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although various graph contrastive learning (GCL) techniques have been employed to generate augmented views and maximize their mutual information, current solutions only consider the pairwise relationships based on edges, neglecting the high-order information that can help generate more informative augmented views and make better contrast. To fill in this gap, we propose to leverage hyperedge to facilitate GCL, as it connects two or more nodes and can model high-order relationships among multiple nodes. More specifically, hyperedges are constructed based on the original graph. Then, we conduct node-level Page Rank based on hyperedges and hyperedge-level PageRank based on nodes to generate augmented views. As to the contrasting stage, different from existing GCL methods that simply treat the corresponding nodes of the anchor in different views as positives and overlook certain nodes strongly associated with the anchor, we build the positives and negatives based on hyperedges, where whether a node is a positive is determined by the number of hyperedges it coexists with the anchor. We compare our hyperedge GCL with state-of-the-art methods on downstream tasks, and the empirical results validate the superiority of our proposal. Further experiments on graph augmentation and graph contrastive loss also demonstrate the effectiveness of the proposed modules.}
}


@inproceedings{DBLP:conf/icde/ZhuoQWH25,
	author = {Shengda Zhuo and
                  Jin{-}Jie Qiu and
                  Chang{-}Dong Wang and
                  Shuqiang Huang},
	title = {Online Feature Selection with Varying Feature Spaces (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4746--4747},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00402},
	doi = {10.1109/ICDE65448.2025.00402},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/ZhuoQWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature selection, an essential technique in data mining, is often confined to batch learning or online idealization of data scenarios despite its significance. Existing online feature selection methods have specific assumptions regarding the data stream, such as requiring a fixed feature space with an explicit pattern and complete labeling of samples. Unfortunately, data streams generated in many real scenarios commonly exhibit arbitrarily incomplete feature spaces and scarcity labels, making existing approaches unsuitable for real applications. To fill these gaps, this study proposes a new problem called Online Feature Selection with Varying Features Spaces (OFSVF). OFSVF has a three-fold main idea: 1) it leverages Gaussian Copula to model the incomplete feature correlation in a complete latent space, encoded by continuous variables, 2) it employs a novel tree-ensemble-based approach to select the most informative features on-the-fly, and 3) it develops the underlying geometric structure of instances to establish the relationship between unlabeled and labels. Experimental results are documented to demonstrate the feasibility and effectiveness of our proposed method.}
}


@inproceedings{DBLP:conf/icde/WengLSCZLT25,
	author = {Tongfeng Weng and
                  Yumeng Liu and
                  Mo Sha and
                  Xinyuan Chen and
                  Xu Zhou and
                  Kenli Li and
                  Kian{-}Lee Tan},
	title = {Efficient Projection-Based Algorithms for Tip Decomposition on Dynamic
                  Bipartite Graphs (Extended Abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4748--4749},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00403},
	doi = {10.1109/ICDE65448.2025.00403},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/WengLSCZLT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the pressing need for effective k-tips decomposition in dynamic bipartite graphs, a crucial aspect of real-time applications that analyze and mine binary relationship patterns. Recognizing the dynamic nature of these graphs, our study is the first to provide a solution for k-tips decomposition in such evolving environments. We introduce a pioneering projection-based algorithm, coupled with advanced incremental maintenance strategies for edge modifications, tailored specifically for dynamic graphs. This novel approach not only fills a significant gap in the analysis of dynamic bipartite graphs but also substantially enhances the accuracy and timeliness of data-driven decisions in critical areas like public health. Our contributions set a new benchmark in the field, paving the way for more nuanced and responsive analyses in various domains reliant on dynamic data interpretation.}
}


@inproceedings{DBLP:conf/icde/HeHWW25,
	author = {Jingxuan He and
                  Xixian Han and
                  Xiaolong Wan and
                  Jinbao Wang},
	title = {Efficient Skyline Frequent-Utility Itemset Mining Algorithm on Massive
                  Data (Extended abstract)},
	booktitle = {41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
                  Hong Kong, May 19-23, 2025},
	pages = {4750--4751},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/ICDE65448.2025.00404},
	doi = {10.1109/ICDE65448.2025.00404},
	timestamp = {Fri, 05 Sep 2025 21:24:36 +0200},
	biburl = {https://dblp.org/rec/conf/icde/HeHWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Itemset mining is a crucial technology for extracting interesting patterns that meet predefined thresholds from transaction databases, such as frequent itemset mining (FIM) and high-utility itemset mining (HUIM). Despite their significance, few studies have explored the simultaneous consideration of both support and utility. This gap arises from the theoretical complexity of integrating these dimensions and the practical difficulty of setting appropriate thresholds for both. To overcome this limitation, we introduce Skyline frequent-utility itemset mining (SFUIM), a method that examines frequent and high-utility itemsets without requiring predefined thresholds. Nevertheless, SFUIM faces significant challenges due to its expansive search space and intensive computational requirements. In this paper, we propose a PSI algorithm and its enhanced version PSI*, which confines calculations to specific partitions by prefix-based partitioning. Experiments demonstrate that PSI* outperforms state-of-the-art methods, especially on large-scale datasets.}
}
