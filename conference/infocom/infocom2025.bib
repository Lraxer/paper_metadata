@inproceedings{DBLP:conf/infocom/MaHLXWL25,
	author = {Liyuan Ma and
                  Dengcheng Hu and
                  Xiulong Liu and
                  Hao Xu and
                  Jianrong Wang and
                  Keqiu Li},
	title = {{AIGC-CM:} An Efficient and Scalable Blockchain Solution for {AIGC}
                  Copyright Management},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044774},
	doi = {10.1109/INFOCOM55648.2025.11044774},
	timestamp = {Sat, 23 Aug 2025 07:42:37 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MaHLXWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the emergence of distributed Artificial Intelligence Generated Content (AIGC) collaboration, leveraging immutability, transparency, and traceability of blockchain for copyright management has become a key focus. However, existing solutions, such as CopyrightLY and PoAIGC, utilize tokenization and chained ledgers with on-chain hash storage, which exhibit limitations in metadata interpretability, efficient traceability, and formal analysis during distributed collaboration. To this end, we propose AIGC Copyright Management (AIGC-CM), an efficient and scalable blockchain-based framework for AIGC copyright management. Firstly, we propose the blockchain-based MCU concept, integrating feature-based hash and structured metadata to achieve precise copyright authentication and onchain self-description for AIGC. Next, we design an authentication mechanism based on Hysteresis Signature DAG (HSD) to achieve efficient and secure copyright traceability for MCUs. Furthermore, we employ the UC framework to conduct a formal security analysis of AIGC-CM. To validate the performance and scalability, we implement it on Fabric 2.4 with 4000 LOC and deploy AIGC-CM prototype on 140 nodes. The experimental results show that the average operation time is reduced by 98.86% compared to existing proposals. Compared to the linear increase of chained structure, the average operation time increases by less than 2x when the node scale is expanded 14x.}
}


@inproceedings{DBLP:conf/infocom/GrafDCYNE25,
	author = {J{\'{e}}r{\^{o}}me Graf and
                  Vitalii Demianiuk and
                  Pavel Chuprikov and
                  Feiran Yang and
                  Sergey I. Nikolenko and
                  Patrick Eugster},
	title = {{CGFE:} Efficient Range Encoding for TCAMs},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044571},
	doi = {10.1109/INFOCOM55648.2025.11044571},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GrafDCYNE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-performance packet classification is essential for a wide range of fundamental network functions, including access control, firewalls, and advanced programmable network applications. Ternary content addressable memory (TCAM) is heavily used for packet classification due to its impressive performance but it is size-limited, expensive, and power-intensive. Since TCAM requires special encoding for ranges in packet classifiers, minimizing the encoding size is essential. Classical methods such as DIRPE and SRGE work well for different types of ranges and have different limitations. We present the chunked Gray fence encoding (CGFE), a novel encoding that combines the advantages of DIRPE's fence encoding and Gray code reflectivity in SRGE, achieving the best of both worlds. CGFE reduces the number of TCAM entries needed for range-based packet classifiers, improving TCAM efficiency and lowering energy consumption. We prove that CGFE uses the same or smaller number of TCAM entries than both DIRPE and SRGE for every possible range, reducing the number of ternary strings up to 2× in theory and, as we show in a comprehensive practical evaluation, by 40.8% and 9.3% on average, respectively, for rules with two 16-bit range fields.}
}


@inproceedings{DBLP:conf/infocom/ZhangLZ25,
	author = {Yan Zhang and
                  Tao Li and
                  Yanchao Zhang},
	title = {{GNN-SML:} Graphic Neural Network-Based Spectrum Misuser Localization},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044575},
	doi = {10.1109/INFOCOM55648.2025.11044575},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectrum misuser localization (SML) is essential for dynamic spectrum sharing (DSS) to ensure that only authorized users access and utilize shared spectrum. In this paper, we introduce GNN-SML, an innovative framework for crowdsourcing-based DSS systems that accurately and simultaneously localizes multiple spectrum misusers with unknown locations and transmission powers, even when they are in close proximity. GNN-SML employs location-centric, sensor-agnostic Graph Neural Networks (GNNs) with inductive power. In each online SML instance, these GNNs predict the Received Signal Strength (RSS) residue between each potential transmitter and available spectrum sensors, which may be located arbitrarily and not involved in the training phase. These predicted RSS residues, combined with real-time RSS measurements from spectrum sensors, are used with the zero-forcing technique to estimate the locations and transmission powers of all potential transmitters. This enables the identification of spectrum misusers as localized transmitters lacking proper spectrum authorizations. We validate the effectiveness and efficiency of GNN-SML with real indoor and outdoor datasets. Compared to leading methods, GNN-SML reduces localization error by up to 58% and transmitter power estimation error by up to 24%, while maintaining a comparable localization time of 1.2 s.}
}


@inproceedings{DBLP:conf/infocom/QiaoW0CL25,
	author = {Zhongkang Qiao and
                  Chuyu Wang and
                  Lei Xie and
                  Yuanmin Chen and
                  Sanglu Lu},
	title = {SweaTag: Fine-Grained Sweat Amount Sensing with {COTS} {RFID} Tags},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044754},
	doi = {10.1109/INFOCOM55648.2025.11044754},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QiaoW0CL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate sweat amount sensing can provide key insights into sports scenarios (e.g., exercise load monitoring) and medical scenarios (e.g., hyperhidrosis monitoring). However, current research on sweat either focuses on sensing its composition or requires sophisticated sensors to measure sweat amount. To fill this research gap, in this paper, we propose a novel RFID-based solution, Sweatag,to perform sweat amount sensing with COTS RFID tags. When the sweat amount varies, the equivalent impedance formed between the sweat and the tag antenna changes, affecting the corresponding signal phase. Capturing this phase change is challenging under the interference of translation-type and rotation-type moving effect introduced by human motion. To remove the translation-type interference, we propose a tag-pair method by taking the phase difference between two adjacent tags. To remove the rotation-type interference, we propose a multi-frequency differential method based on the observation that the moving effect has an identical impact on two close-by frequencies. To provide multiple frequencies simultaneously, we propose to leverage Orthogonal Frequency Division Multiplexing (OFDM) and provide a whole process to extract the sweat amount fingerprint from the physical layer signals. We use Partial Least Squares Regression (PLSR) to reduce the dimensionality of the fingerprint and estimate the sweat amount. Experiment results show that Sweatagachieves 93% average sensing accuracy for detecting five sweat amount levels.}
}


@inproceedings{DBLP:conf/infocom/HuangJZYQZHXT0025,
	author = {Xinpeng Huang and
                  Wanqing Jie and
                  Shiwen Zhang and
                  Haofu Yang and
                  Wangjie Qiu and
                  Qinnan Zhang and
                  Huawei Huang and
                  Zehui Xiong and
                  Shaoting Tang and
                  Hongwei Zheng and
                  Zhiming Zheng},
	title = {ContribChain: {A} Stress-Balanced Blockchain Sharding Protocol with
                  Node Contribution Awareness},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044497},
	doi = {10.1109/INFOCOM55648.2025.11044497},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuangJZYQZHXT0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing blockchain sharding protocols have focused on eliminating imbalanced workload distributions. However, even with workload balance, disparities in processing capabilities can lead to differential stress among shards, resulting in transaction backlogs in certain shards. Therefore, achieving stress balance among shards in the dynamic and heterogeneous environment presents a significant challenge of blockchain sharding. In this paper, we propose ContribChain, a blockchain sharding protocol that can automatically be aware of node contributions to achieve stress balance. We calculate node contribution values based on the historical behavior to evaluate the performance and security of nodes. Furthermore, we propose node allocation algorithm NACV and account allocation algorithm P-Louvain, which both match shard performance with workload to achieve stress balance. Finally, we conduct extensive experiments to compare our work with state-of-the-art baselines based on real Ethereum transactions. The evaluation results show that P-Louvain reduces allocation execution time by 86% and the cross-shard transaction ratio by 7.5%. Meanwhile, ContribChain improves throughput by 35.8% and reduces the cross-shard transaction ratio by 16%.}
}


@inproceedings{DBLP:conf/infocom/AbdiH0A025,
	author = {Mohammad Abdi and
                  Khandaker Foysal Haque and
                  Francesca Meneghello and
                  Jonathan D. Ashdown and
                  Francesco Restuccia},
	title = {PhyDNNs: Bringing Deep Neural Networks to the Physical Layer},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044671},
	doi = {10.1109/INFOCOM55648.2025.11044671},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/AbdiH0A025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging applications require mobile devices to continuously execute complex deep neural networks (DNNs). While mobile edge computing (MEC) may reduce the computation burden of mobile devices, it exhibits excessive latency as it relies on encapsulating and decapsulating frames through the network protocol stack. To address this issue, we propose PhyDNNs, an approach where DNNs are modified to operate directly at the physical layer (PHY), thus significantly decreasing latency, energy consumption, and network overhead. Conversely from recent work in Joint Source and Channel Coding (JSCC), PhyDNNs adapt already trained DNNs to work at the PHY. To this end, we developed a novel information-theoretical framework to fine-tune PhyDNNs based on the trade-off between commu-nication efficiency and task performance. We have prototyped PhyDNNs with an experimental testbed using a Jetson Orin Nano as the mobile device and two USRP software-defined radios (SDRs) for wireless communication. We evaluated PhyDNNs performance considering various channel conditions, DNN models, and datasets. We also tested PhyDNNs on the Colosseum net-work emulator considering two different propagation scenarios. Experimental results show that PhyDNNs can reduce the end-to-end inference latency, amount of transmitted data, and power consumption by up to 48×, 1385×, and 13 × while keeping the accuracy within 7 % of the state-of-the-art approaches. Moreover, we show that PhyDNNs experience 4.3 times less latency than the most recent JSCC method while incurring in only 1.79% performance loss. For replicability, we shared the source code for the PhyDNNs implementation.}
}


@inproceedings{DBLP:conf/infocom/AhmedGWS25,
	author = {Youssef Ahmed and
                  Arnob Ghosh and
                  Chih{-}Chun Wang and
                  Ness B. Shroff},
	title = {Communication Efficient Asynchronous Stochastic Gradient Descent},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044686},
	doi = {10.1109/INFOCOM55648.2025.11044686},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/AhmedGWS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we address the challenges of asynchronous gradient descent in distributed learning environments, particularly focusing on addressing the challenges of stale gradients and the need for extensive communication resources. We develop a novel communication efficient framework that incorporates a gradient evaluation algorithm to assess and utilize delayed gradients based on their quality, ensuring efficient and effective model updates while significantly reducing communication overhead. Our proposed algorithm requires agents to only send the norm of the gradients rather than the computed gradient. The server then decides whether to accept the gradient if the ratio between the norm of the gradient and the distance between the global model parameter and the local model parameter exceeds a certain threshold. With the proper choice of the threshold, we show that the convergence rate achieves the same order as the synchronous stochastic gradient without depending on the staleness value unlike most of the existing works. Given the computational complexity of the initial algorithm, we introduce a simplified variant that prioritizes the practical applicability without compromising on the convergence rates. Our simulations demonstrate that our proposed algorithms outperform existing state-of-the-art methods, offering improved convergence rates, stability, accuracy, and resource consumption.}
}


@inproceedings{DBLP:conf/infocom/XuSL0W0S025,
	author = {Hongbo Xu and
                  Chengxiang Si and
                  Shuhao Li and
                  Zhenyu Cheng and
                  Chenxu Wang and
                  Jiang Xie and
                  Peishuai Sun and
                  Qingyun Liu},
	title = {FlowMiner: {A} Powerful Model Based on Flow Correlation Mining for
                  Encrypted Traffic Classification},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044724},
	doi = {10.1109/INFOCOM55648.2025.11044724},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XuSL0W0S025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continuous development of traffic encryption techniques, the encrypted traffic classification task faces increasing challenges. Existing encrypted traffic classification methods primarily address the encryption problem by extracting side channel features, such as packet lengths and timing sequences. However, existing methods usually classify traffic at the flow or packet level. There are generally no mechanisms for mining correlations between different flow samples. Because of the loss of correlation information, the performance of these models in multiple tasks is greatly affected. To solve the above problems, we propose FlowMiner, a traffic graph classification model that mines correlations among different flow samples. It first extracts the temporal, length, content, byte distribution, and crossover features. FlowMiner uses these features as initial node features. Then, it constructs flow interaction graphs by analyzing the association between different flows. FlowMiner utilizes Graph Neural Networks to extract association features between different flows. It then generates a robust graph-level representation vector through an integrated pooling module, which is subsequently used for classification. Extensive experimental results on eight datasets show that FlowMiner significantly outperforms multiple state-of-the-art methods. Additionally, results from two real-world evaluation scenarios show that FlowMiner achieves over 95% precision in identifying malicious traffic, proving its effectiveness for practical applications.}
}


@inproceedings{DBLP:conf/infocom/Ge0025,
	author = {Shuxin Ge and
                  Xiaobo Zhou and
                  Tie Qiu},
	title = {MARL-Based Pricing Strategy via Mutual Attention for MoD Systems with
                  Ridesharing and Repositioning},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044656},
	doi = {10.1109/INFOCOM55648.2025.11044656},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Ge0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The pricing strategy plays a vital role in increasing the revenue of mobility on-demand (MoD) systems by balancing supply-demand relationship across urban zones. MoD systems often incorporate both ridesharing and vehicle repositioning to maintain this balance, ultimately improving revenue and order completion rate. However, many existing pricing strategies overlook the effects when the price differences across zones meet ridesharing and repositioning, leading to supply-demand mismatch and reduced revenue. To tackle this problem, this paper presents a multi-agent reinforcement learning (MARL) based pricing strategy, named MAP, which employs a mutual attention mechanism to effectively account for price differences with ridesharing and repositioning. We transform the pricing problem as a MARL model that aiming to maximizing total revenue. These agents are tasked with making about order fare with ridesharing, vehicle income with repositioning for each zone. Pricing strategy decision-making is guided by mutual information theory and further enhanced by an attention mechanism to estimate how pricing variations impact adjacent zones. Simulation studies utilizing real-world datasets are performed to illustrate the advantages of MAP compared to existing benchmarks.}
}


@inproceedings{DBLP:conf/infocom/ZhouYMCZYPL25,
	author = {Lu Zhou and
                  Ruoxu Yang and
                  Lichuan Ma and
                  Guoxing Chen and
                  Haojin Zhu and
                  Li Yang and
                  Qingqi Pei and
                  Qiang Li},
	title = {The Feasibility of Location Anonymity: An Empirical Study towards
                  a Real-world Location Privacy Protection System in Takeout Services},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044635},
	doi = {10.1109/INFOCOM55648.2025.11044635},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouYMCZYPL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Various anonymity methods have been proposed to safeguard the privacy of human mobility trajectories, ranging from trajectory anonymity that uses a fixed pseudonym to location anonymity which involves using different pseudonyms for each location. While location anonymity appears to offer robust privacy protection, there is growing concern that trajectories can still be reconstructed even if this method is deployed. Due to the lack of evaluations on real-world systems utilizing location anonymity, its practical effectiveness remains uncertain. Two popular takeout platforms, Ele.me and Meituan, which have adopted location anonymity to protect riders' trajectories, provide a suitable environment for such real-world evaluations. We design a large-scale data collection system to gather anonymized location data of riders, creating two anonymized datasets containing millions of riders' locations. Then we propose an innovative multi-stage trajectory inference framework specifically tailored to location anonymity, containing location linking stage for short-term tracking and segment matching stage for long-term tracking. Extensive evaluations refute the effectiveness of location anonymity for short-term tracking (achieving 83.4% and 74.5% inference accuracy on Ele.me and Meituan) but confirm its utility for long-term tracking. Analysis highlights the crucial role of strong aggregation properties of riders, previously deemed unrealistic across multiple scenarios, in thwarting long-term tracking.}
}


@inproceedings{DBLP:conf/infocom/WangZ00LYQ25,
	author = {Xiaoyu Wang and
                  Yangming Zhao and
                  Chen Tian and
                  Kai Chen and
                  Qi Li and
                  Kun Yang and
                  Chunming Qiao},
	title = {Combating Deep Leakage from Gradients in Cross-Silo Federated Learning
                  with {QKD}},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044743},
	doi = {10.1109/INFOCOM55648.2025.11044743},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangZ00LYQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep Leakage from Gradients (DLG) could reveal training data privacy from gradients transmitted over an insecure channel in Cross-Silo Federated Learning (CSFL) systems. So far, One-Time Pad (OTP) based on secret keys generated by Quantum Key Distribution (QKD) is the only perfectly secure approach to defending channel security and preserving privacy. Nevertheless, current QKD systems cannot generate keys at a rate high enough to support OTP in practical CSFL systems, while we find that encrypting only part of the gradients or several bits of each gradient is not adequate to preserve data privacy. To overcome these challenges, we propose QuGrad to encrypt each gradient using only one bit of secret keys. In QuGrad, it is unpredictable which or how many bits of each gradient will be changed and the encrypted gradient vector will be orthogonal to the original one, which potentially hides the maximum amount of training data information. By implementing QuGrad on a testbed and conducting extensive experiments, we show that QuGrad can reduce the average Jaccard similarity between the recovered data and the original ones by up to 89% compared with the state-of-the-art technique to defend training data against DLG.}
}


@inproceedings{DBLP:conf/infocom/Qiu0TSZF25,
	author = {Shuting Qiu and
                  Fang Dong and
                  Siyu Tan and
                  Dian Shen and
                  Ruiting Zhou and
                  Qilin Fan},
	title = {CoCaR: Enabling Efficient Dynamic DNN-Based Model Caching and Request
                  Routing in {MEC}},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044457},
	doi = {10.1109/INFOCOM55648.2025.11044457},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Qiu0TSZF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near end-users, providing low-latency services and improving users' quality of experience (QoE). However, caching all DNN models at edge servers with limited capacity is difficult, and the impact of model loading time on QoE is underexplored. Hence, we introduce dynamic DNNs in edge scenarios, disassembling a complete DNN model into interrelated submodels for more fine-grained and flexible model caching and request routing solutions. Further, this raises the pressing issue of joint deciding request routing and sub model caching for dynamic DNNs to balance model inference precision and loading latency for QoE optimization. In this paper, we study the joint dynamic model caching and request routing problem in MEC networks, aiming to maximize user request inference precision under constraints of server resources, latency, and model loading time. To tackle this problem, we propose CoCaR, an algorithm based on linear programming and random rounding that leverages dynamic DNNs to optimize caching and routing schemes, achieving near-optimal performance. Simulation results show that the proposed CoCaR achieves significant performance improvements compared to state-of-the-art baselines.}
}


@inproceedings{DBLP:conf/infocom/LiuLCSYW25,
	author = {Xiaochen Liu and
                  Fan Li and
                  Yetong Cao and
                  Binghui Shi and
                  Song Yang and
                  Yu Wang},
	title = {SignParser: Empowering Dual-Handed Sign Language Translation with
                  a Single Wearable},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044541},
	doi = {10.1109/INFOCOM55648.2025.11044541},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuLCSYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sign language translation (SLT) is essential for promoting communicative equity and social integration for hearing-impaired individuals. However, computer-vision-based and wireless-signal-based SLT solutions mainly involve inconvenient operation, poor portability, and susceptibility to interference. Recently, wearable-device-based methods have emerged as a potential alternative, offering services anytime and anywhere. However, these methods fall into two extremes: employing complex device combinations to achieve dual-handed SLT or opting for single-device solutions that compromise comprehensive data capture from both hands. Consequently, such a dilemma constrains the widespread adoption of wearable devices in the field of SLT. In this paper, we propose SignParser, a unique dual-handed SLT system leveraging a single IMU sensor in commercial smartwatches. SignParser is superior to other wearable-device-based approaches in i) exploiting large-scale labeled virtual IMU data to achieve generalization capability across different users, ii) enabling single-device solution for dual-handed SLT via estimating non-dominant hand IMU data, and iii) ensuring real-time, contextual-guided, and unseen sentence-adaptive SLT by a lightweight sign spotter network integrated with large language models. Extensive experiments with 27 participants show that SignParser can achieve the average word error rate of 4.8% and 8.3% for new users and unseen sentences, respectively. The excellent performance demonstrates the SignParser's effectiveness in real-world scenarios.}
}


@inproceedings{DBLP:conf/infocom/XiaoZYL025,
	author = {Xuedou Xiao and
                  Yingying Zuo and
                  Mingxuan Yan and
                  Kezhong Liu and
                  Wei Wang},
	title = {PDStream: Slashing Long- Tail Delay in Interactive Video Streaming
                  via Pseudo-Dual Streaming},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044674},
	doi = {10.1109/INFOCOM55648.2025.11044674},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XiaoZYL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {End-to-end (E2E) delay is critical for interactive video streaming (IVS) experiences, but remains unsatisfactory for its long-tail distribution caused by periodic large keyframes. Conventional optimization strategies, such as jitter buffer, bitrate adaptation, and customized encoding, either sacrifice clarity, average delay, or compatibility. To address this issue, we propose PDStream, a novel pseudo-dual streaming algorithm, aimed at minimizing E2E delay while maintaining video clarity. The core idea is to split the two functions, delay-sensitive playback and delay-tolerant reference, on keyframes through dual streaming. Specifically, the playback function is held by a second parallel stream, which comprises much smaller non-keyframes and is allocated more immediate bandwidth for real-time performance. The reference function is ensured by the first stream with keyframe preservation, allocated more subsequent bandwidth to smooth out bursty traffic. Additionally, “pseudo” minimizes computational and transmission overheads by restricting dual streams to brief activation only when keyframes appear, supported by corresponding dual-stream bitrate allocation and adaptation to ensure delay and clarity. We implement PDStream on a WebRTC-based IVS testbed with real-world network traces. Results show that PDStream significantly outperforms prior algorithms, reducing average E2E delay by 17.5% and slashing its 97th percentile by 33.3%, while keeping clarity under varying bandwidth.}
}


@inproceedings{DBLP:conf/infocom/0004MMGC25,
	author = {Haibo Wang and
                  Chaoyi Ma and
                  Dimitrios Melissourgos and
                  Guoju Gao and
                  Shigang Chen},
	title = {Towards Guaranteed Accuracy for Flow Spread Measurement with {\textdollar}({\textbackslash}epsilon,
                  {\textbackslash}beta){\textdollar}-Nonduplicate Sampling},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044612},
	doi = {10.1109/INFOCOM55648.2025.11044612},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0004MMGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Per-flow spread measurement in high-speed networks is important to many practical applications. To fit in the limited on-chip memory, sketch-based solutions allow multiple flows to share space, causing inter-flow noise and thus sacrificing in accuracy. Recent progress on non-duplicate sampling creates a new direction of sampling-based solutions for spread estimation, which performs better than memory-sharing sketches. However, the current sampling-based solutions either use a system-wide sampling probability or lack the flexibility of setting the sampling probability dynamically and at per-flow level. This paper advances the theory and design of non-duplicate sampling by introducing a new  ( ϵ , β ) (\\epsilon, \\beta) -RE accuracy model for spread estimation and a new  ( ϵ , β ) (\\epsilon, \\beta) -nonduplicate sampling type, establishing their equivalency, proposing the idea of individualized per-flow sampling, and designing a novel algorithm based on this idea to implement  ( ϵ , β ) (\\epsilon, \\beta) -nonduplicate sampling and thus achieving  ( ϵ , β ) (\\epsilon, \\beta) -RE accuracy. Trace-driven experiments demonstrate that our new solution outperforms the best state of the art significantly in terms of maximum supported packet stream size under a given accuracy requirement or in terms of accuracy with the same packet stream size, and outperforms sketch-based solutions to spread estimation significantly in accuracy.}
}


@inproceedings{DBLP:conf/infocom/HeX0025,
	author = {Yinghui He and
                  Mingming Xu and
                  Fu Xiao and
                  Jun Luo},
	title = {VersaBeam: Versatile Beamforming for Integrated Sensing and Communication
                  over Commodity Wi-Fi},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044469},
	doi = {10.1109/INFOCOM55648.2025.11044469},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HeX0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reusing Wi-Fi communication packets for sensing purpose has been regarded as one of the most cost-effective ways to realize integrated sensing and communication (ISAC) on commodity Wi-Fi, However, the channel state information (CSI) measured from these packets can be heavily compromised by modern Wi-Fi beamforming protocols tailored primarily to maximize communication throughput, hence inadvertently affecting Wi-Fi sensing performance. To this end, we propose VersaBeam, a practical ISAC system actively leveraging beamforming to simultaneously achieve high-performance communication and sensing in commodity Wi-Fi, Instead of attempting to mitigate beamforming's negative impact on sensing, VersaBeam boasts a novel beamforming design to balance the demands of both sensing and communication. Additionally, we propose a user selection strategy to determine the sources for packet reuse and introduce a method to unify CSI measured from different users' packets, accommodating variations in beamforming direction. We implement a prototype of VersaBeam on commodity Wi-Fi devices and further demonstrate its efficacy through micro-benchmarking and practical experiments in gesture recognition.}
}


@inproceedings{DBLP:conf/infocom/ChenFQM0LZWLR25,
	author = {Zhongjie Chen and
                  Yingchen Fan and
                  Kun Qian and
                  Qingkai Meng and
                  Ran Shu and
                  Xiaoyu Li and
                  Yiran Zhang and
                  Bo Wang and
                  Wei Li and
                  Fengyuan Ren},
	title = {ScalaTap: Scalable Outbound Rate Limiting in Public Cloud},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044509},
	doi = {10.1109/INFOCOM55648.2025.11044509},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenFQM0LZWLR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud providers limit the outbound traffic rate at cloud gateways for billing and performance guarantees. A scalable and accurate tenant-level outbound rate limiting system is critical. The state-of-the-art solution utilizes a centralized controller to orchestrate distributed rate limiting at gateway servers. However, this solution suffers from an inherent tension between accuracy and scalability due to the difficulty in timely adjusting budget allocations between gateway servers. In this paper, we explore a new paradigm for tenant-level outbound rate limiting by decoupling the budget allocator and demand aggregator, which were originally co-located in the centralized controller. We present ScalaTap, a switch-host co-design system that leverages a programmable switch to collect and aggregate local demands, and then distributes global demands to gateway servers for independent budget computation. As a result, the centralized controller is completely removed, and the expensive communication and computational overheads are also eliminated. ScalaTap provides more flexibility in achieving a reasonable trade-off between scalability and accuracy by adjusting the update interval at will. Comprehensive experiments show that ScalaTap can (1) increase rate limiting accuracy by 15.2%-27.3% under the same number of tenants, and (2) support  8 . 3 × \\mathbf{8}.\\mathbf{3}\\times  more tenants under the same accuracy guarantee compared with the state-of-the-art solution.}
}


@inproceedings{DBLP:conf/infocom/WangL00HLT25,
	author = {Xuchuang Wang and
                  Maoli Liu and
                  Xutong Liu and
                  Zhuohua Li and
                  Mohammad Hajiesmaili and
                  John C. S. Lui and
                  Don Towsley},
	title = {Learning Best Paths in Quantum Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044499},
	doi = {10.1109/INFOCOM55648.2025.11044499},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangL00HLT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum networks (QNs) transmit delicate quantum information across noisy quantum channels. Crucial applications, like quantum key distribution (QKD) and distributed quantum computation (DQC), rely on efficient quantum information transmission. Learning the best path between a pair of end nodes in a QN is key to enhancing such applications. This paper addresses learning the best path in a QN in the online learning setting. We explore two types of feedback: “link-level” and “path-level”. Link-level feedback pertains to QNs with advanced quantum switches that enable link-level benchmarking. Path-level feedback, on the other hand, is associated with basic quantum switches that permit only path-level benchmarking. We introduce two online learning algorithms, BeQuP-Link and BeQuP-Path, to identify the best path using link-level and path-level feedback, respectively. To learn the best path, BeQuP-Link benchmarks the critical links dynamically, while BeQuP-Path relies on a subroutine, transferring path-level observations to estimate link-level parameters in a batch manner. We analyze the quantum resource complexity of these algorithms and demonstrate that both can efficiently and, with high probability, determine the best path. Finally, we perform NetSquid-based simulations and validate that both algorithms accurately and efficiently identify the best path.}
}


@inproceedings{DBLP:conf/infocom/GanLALLZ025,
	author = {Maolin Gan and
                  Lanpeng Li and
                  Samiul Alam and
                  Li Liu and
                  Luyang Liu and
                  Mi Zhang and
                  Zhichao Cao},
	title = {GeoFL: {A} Framework for Efficient Geo-Distributed Cross-Device Federated
                  Learning},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044713},
	doi = {10.1109/INFOCOM55648.2025.11044713},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GanLALLZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, GeoFL develops a hierarchical federated learning (FL) framework to address the unique challenges in large-scale geo-distributed scenarios. The key idea is to deploy multiple aggregators to geo-distributed clients and aggregate the local model and the global model efficiently and effectively. By assigning each aggregator as a relay layer, GeoFL can elaborately aggregate the geo-distributed clients and systematically determine when to upload the model to the central server based on bandwidth to efficiently update the global model under inadequate and heterogeneous WAN bandwidth constraints. GeoFL designs three key components to optimize the inefficient model aggregation and cope with the non-importance model updates. It further addresses the statistical heterogeneity across geo-distributed aggregators by considering the clients' graph relationship, delivering an end-to-end client-aggregator-server architecture for large-scale clients. Compared with existing works, our results on large-scale real-life datasets show that GeoFL speeds up the training process by 1.4×-8× and reduces 6%-80% unnecessary communication rounds between the aggregator and the central server.}
}


@inproceedings{DBLP:conf/infocom/0017ZZCZ00LLY0Z25,
	author = {Xiang Chen and
                  Longlong Zhu and
                  Linying Zheng and
                  Lingfei Cheng and
                  Jianshan Zhang and
                  Xu Yang and
                  Dong Zhang and
                  Xuan Liu and
                  Xiaoming Lu and
                  Xun Yi and
                  Ibrahim Khalil and
                  Albert Y. Zomaya and
                  Haifeng Zhou and
                  Chunming Wu},
	title = {TurboCache: Empowering Switch-Accelerated Key-Value Caches with Accurate
                  and Fast Cache Updates},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044585},
	doi = {10.1109/INFOCOM55648.2025.11044585},
	timestamp = {Tue, 08 Jul 2025 07:37:28 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0017ZZCZ00LLY0Z25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent key-value (KV) caches are offloaded to programmable switches to offer high query processing performance. However, they suffer from both low accuracy in hot key detection and high latency in cache updates due to the strict limitations on switch registers. We propose TurboCache, a switch-accelerated KV cache with accurate hot key detection and fast cache updates. Our key idea is to leverage the switch recirculation capability to build a novel data structure that caches hot KV pairs. With this hardware-compatible cache data structure, TurboCache designs efficient data plane algorithms that accurately detects new hot keys and quickly updates its cache entirely within switch ASIC pipelines. We have implemented TurboCache on a  64 × 100 {64}\\times {100}  Gbps Tofino switch. Testbed results indicate that TurboCache improves the hot key detection accuracy and decreases the cache update latency of existing KV caches by several orders of magnitude.}
}


@inproceedings{DBLP:conf/infocom/LiuCHZC0W25,
	author = {Chao Liu and
                  Hao Chen and
                  Jingyang Hu and
                  Qibo Zhang and
                  Siyu Chen and
                  Hongbo Jiang and
                  Penghao Wang},
	title = {EchoHealth: Non-Contact Rehabilitation Exercises via Active Acoustic
                  Sensing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044476},
	doi = {10.1109/INFOCOM55648.2025.11044476},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuCHZC0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the aging population, there is an increasing demand for rehabilitation services for people with chronic diseases. However, limitations such as medical resources, geographic barriers, and cost make home rehabilitation an option for more patients. Existing wearable devices and vision methods are effective but face problems with portability, cost, and privacy concerns. As for existing wireless sensing methods, they can only extract coarse features for activity recognition. Therefore, we present EchoHealth, which utilizes a smart speaker for rehabilitation exercise detection and assessment. We upgrade the smart speaker into an active sonar system without hardware modification to generate acoustic micro-distance images with motion information. Then, time-domain motion detection and distance-domain feature extraction are utilized to filter out the effects of non-motion time and distance to extract patient motion features for motion recognition. We further assess the patient's rehabilitation exercises from five aspects, based on which EchoHealth provides rehabilitation guidance. Extensive experiments with 15 participants performing 12 rehabilitation motions confirmed that EchoHealth can achieve 97.4% average accuracy in recognition of rehabilitation motion and provide accurate rehabilitation indicators in various environments.}
}


@inproceedings{DBLP:conf/infocom/WangHXLD025,
	author = {Wentao Wang and
                  Jiangping Han and
                  Kaiping Xue and
                  Jian Li and
                  Kunpeng Ding and
                  Ruidong Li},
	title = {NetRT: Enhancing {RDMA} with Retransmission Offloading in Data Center
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044493},
	doi = {10.1109/INFOCOM55648.2025.11044493},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangHXLD025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RDMA over Converged Ethernet (RoCE) enables the deployment of RDMA in Ethernet-based data centers and is becoming a mainstream solution. Due to the limited processing capability of RDMA NICs (RNICs), Priority-based Flow Control (PFC) is enabled to prevent decreased transmission efficiency caused by packet loss and retransmission. However, PFC brings performance impairments and cannot fully ensure reliability in the presence of various packet loss and out-of-order delivery cases in large-scale data centers. To achieve high resilience against lossy conditions and disorders while considering feasibility, this paper proposes NetRT, an innovative retransmission offloading solution. NetRT deploys selective retransmission between top-of-rack switches instead of RNICs, leveraging modern switches' hardware resources and programmable features for efficient and deployable in-network recovery. We develop key mechanisms to manage in-network retransmissions, including a ternary state machine and congestion avoidance, aiming for efficiency and transparency. Evaluations show that NetRT can handle diverse packet loss and out-of-order cases with acceptable overhead, reducing end - to-end retransmissions and decreasing flow completion times by up to 75 %.}
}


@inproceedings{DBLP:conf/infocom/YangZD0WL25,
	author = {Ziming Yang and
                  Zichuan Zheng and
                  Liyou Deng and
                  Shan Zhang and
                  Zhiyuan Wang and
                  Hongbin Luo},
	title = {ACBatch: Adaptive and Cooperative Batching for Edge Inference},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044583},
	doi = {10.1109/INFOCOM55648.2025.11044583},
	timestamp = {Thu, 14 Aug 2025 07:32:15 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangZD0WL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Batching is a key technique in deep learning in-ference that enhances computational efficiency. Although widely applied in the cloud, batching may suffer from longer batch latency at edge servers due to highly dynamic task arrivals. In this paper, we propose an Adaptive and Cooperative Batching (ACBatch) framework for edge inference, wherein temporal adaptive batching and spatial task steering are jointly devised to balance the trade-off between batch latency and computational efficiency. To this end, a batch efficiency model is built to quantify the relationship between computational efficiency and batch size based on empirical measurements across diverse computing platforms and mainstream neural networks. Then, an optimization problem is formulated to minimize the completion time of a task sequence under ACBatch. For the simplified single-server case, the problem exhibits an optimal substructure and is solved by our proposed Dynamic Programming-based Adaptive Batching algorithm. For the general multi-server case, the optimization of ACBatch is proved NP-hard, and we propose the Multi-Server Cooperative Batching algorithm by iteratively optimizing batching and steering. Real-trace experiments show that ACBatch achieves an average improvement of 89.17% in completion time and 76.52% in latency compared to state-of-the-art methods.}
}


@inproceedings{DBLP:conf/infocom/WuJL25,
	author = {Xiaoyi Wu and
                  Bo Ji and
                  Bin Li},
	title = {On the Low-Complexity of Fair Learning for Combinatorial Multi-Armed
                  Bandit},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044728},
	doi = {10.1109/INFOCOM55648.2025.11044728},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WuJL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Combinatorial Multi-Armed Bandit with fairness constraints is a framework where multiple arms form a super arm and can be pulled in each round under uncertainty to maximize cumulative rewards while ensuring the minimum average reward required by each arm. The existing pessimistic-optimistic algorithm linearly combines virtual queue-lengths (tracking the fairness violations) and Upper Confidence Bound estimates as a weight for each arm and selects a super arm with the maximum total weight. The number of super arms could be exponential in the number of arms in many scenarios. In wireless networks, due to interference constraints the number of super arms can grow exponentially with the number of arms. Evaluating all the feasible super arms to find the one with the maximum total weight can incur extremely high computational complexity in the pessimistic-optimistic algorithm. To tackle this issue, we develop a low-complexity fair learning algorithm based on the so-called pick-and-compare approach that involves randomly picking  M M  feasible super arms to evaluate. By setting  M M  to a constant, the number of comparison steps in the pessimistic-optimistic algorithm can be reduced to a constant, thereby significantly reducing the computational complexity. The theoretical analysis shows that our low-complexity design sacrifices fairness and regret performance only marginally. Finally, we validate our theoretical results through extensive simulations.}
}


@inproceedings{DBLP:conf/infocom/QiLZDW25,
	author = {Taoran Qi and
                  Shuo Li and
                  Xingqi Zou and
                  Liangce Deng and
                  Guodong Wei},
	title = {{PC4:} Precision Collective Communication Congestion Control for {AI}
                  Cluster},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044740},
	doi = {10.1109/INFOCOM55648.2025.11044740},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QiLZDW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The evolution of AI has driven the trend towards utilizing larger models with increasing parameter sizes, necessitating deployment in datacenters comprising tens of thousands of GPUs. However, as cluster sizes expand, the overall computational capacity of the system fails to scale linearly with the number of compute nodes. This non-linear scaling law is primarily due to decreased GPU utilization caused by network congestion, which significantly impacts model training speed. In this paper, we analyze the network topology and traffic characteristics of AI clusters and propose PC4, a precision collective communication congestion control algorithm tailored for AI clusters. PC4 optimizes congestion control based on collective communication operations (e.g., all-reduce, all-to-all) inherent in collective communication libraries (e.g., MPI, NCCL) used in distributed training. By leveraging information from these collective communication operations, we compute a base rate that enables rapid system convergence. Furthermore, PC4 employs a datacenter time synchronization mechanism, utilizing one-way delay as congestion signal. This approach allows for more precise control of link queue lengths and queuing time, thereby efficiently managing congestion. In our all-to-all evaluation, PC4 achieves a 72% reduction in tail FCT compared to DCQCN.}
}


@inproceedings{DBLP:conf/infocom/Hong0D25,
	author = {Shu Hong and
                  Xiaojun Lin and
                  Lingjie Duan},
	title = {Lightweight Federated Learning with Differential Privacy and Straggler
                  Resilience},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044562},
	doi = {10.1109/INFOCOM55648.2025.11044562},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Hong0D25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables collaborative model training through model parameter exchanges instead of raw data. To avoid potential inference attacks from exchanged parameters, differential privacy (DP) offers rigorous guarantee against various attacks. However, conventional methods of ensuring DP by adding local noise alone often result in low training accuracy. Combining secure multi-party computation (SMPC) with DP, while improving the accuracy, incurs high communication and computation overheads as well as straggler vulnerability, in either client-to-server or client-to-client links. In this paper, we propose LightDP-FL, a novel lightweight scheme that ensures provable DP against untrusted peers and server, while maintaining straggler resilience, low overheads and high training accuracy. Our scheme incorporates both individual and pairwise noise into each client's parameter, which can be implemented with minimal overheads. Given the uncertain straggler and colluder sets, we utilize the upper bound on the numbers of stragglers and colluders to prove sufficient noise variance conditions to ensure DP in the worst case. Moreover, we optimize the expected convergence bound to ensure accuracy performance by flexibly controlling the noise variances. Using the CIFAR-10 dataset, our experimental results demonstrate that LightDP-FL achieves faster convergence and stronger straggler resilience compared to baseline methods of the same DP level.}
}


@inproceedings{DBLP:conf/infocom/ZhangGCHH25,
	author = {Kuiyuan Zhang and
                  Shouwan Gao and
                  Pengpeng Chen and
                  Kangjia He and
                  Jiayi He},
	title = {Edge-Aided Multi-Modal Collaborative {SLAM} for Resource-Constrained
                  Underground Robots},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044723},
	doi = {10.1109/INFOCOM55648.2025.11044723},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangGCHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Simultaneous localization and mapping (SLAM) of coal mine robots (CMRs) have become inevitable for monitoring, rescue, and transportation in coal mines. However, low illumination and single structures inevitably lead to sensors degradation in mine tunnels. Multi-modal fusion SLAM offers sensing complements but aggravates the consumption of computing resources, which is a huge challenge for CMRs with resource constrained memory and processors. In this paper, we design and deploy an edge-aided multi-modal collaborative SLAM algorithm (EM-CoSLAM) with LiDAR-inertial-visual fusion for constrained CMRs. EM-CoSLAM provides real-time mobile LiDAR tracking and precise edge visual optimization in parallel by decoupling and restructuring task modules. To reduce end-to-end latency and enhance system performance, EM-CoSLAM devises an adaptive offloading strategy to determine the optimal image frame by minimizing the pose uncertainty and further proposes a fusion pose graph optimization method. We completely implement EM-CoSLAM on the CMR and extensively evaluate it in various datasets and scenarios. The results indicate that EM-CoSLAM can achieve 6.7cm and 16fps performance on the CMR, outperforming existing solutions by more than 14%.}
}


@inproceedings{DBLP:conf/infocom/YouH00Q25,
	author = {Chaoqun You and
                  Xingqiu He and
                  Yao Sun and
                  Gang Feng and
                  Tony Q. S. Quek},
	title = {GreenRAN: {A} Channel-Aware Green {O-RAN} Framework for NextG Mobile
                  Systems},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044706},
	doi = {10.1109/INFOCOM55648.2025.11044706},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YouH00Q25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Green communications have always been a target for Open Radio Access Network (O-RAN), given the exploding infrastructures and data in 5G and beyond cellular networks (NextG). Existing research either explores the sleeping mode or workload consolidation of the RAN components, radio units (RUs), distributed units (DUs), and centralized units (CUs). However, current research on green O-RAN scenarios rarely takes into account the channel qualities between user equipment (UEs) and base stations (BSs), more precisely, between UEs and RUs. A cyclic dependency between the inter-RU and intra-RU resource schedulers makes it challenging to incorporate channel awareness at both levels. Armed with this insight, in this paper, we propose GreenRAN, a channel-aware O-RAN framework that executes two power-saving steps: channel-aware RU switching ON/OFF and then CU-DU workload consolidation. The two algorithms act as an add-on rApp, Channel-Aware RU ON/OFF controller (CARC), and an add-on xApp, CU-DU Placement Module (CDPM), in the O-RAN framework, respectively. The output of CARC serves as the input of CDPM, minimizing the power consumption of GreenRAN while providing Quality-of-Service (QoS) guarantees to UEs. Extensive experiments verify the effectiveness of GreenRAN in power saving and the feasibility of deploying the proposed algorithms within the O-RAN architecture” compared with other baselines.}
}


@inproceedings{DBLP:conf/infocom/BerardiZMF25,
	author = {Davide Berardi and
                  Ivan D. Zyrianoff and
                  Federico Montori and
                  Marco Di Felice},
	title = {Extreme Edge Sensing-as-a-Service: Bridging Containerization for IoT
                  End Devices},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044737},
	doi = {10.1109/INFOCOM55648.2025.11044737},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/BerardiZMF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {On the extreme edge of the IoT deployment, micro-controllers are often managed as semi-static software environments, with restricted multitenancy capabilities. This limitation reduces potential monetization opportunities for the IoT infras-tructure owners and restricts the flexibility for external clients to define custom sensing or edge processing strategies. In this paper, we introduce a radical shift in vision through the new paradigm of Extreme Edge-Sensing-as-a-Service, which allows clients to securely and efficiently rent sensing, computation, and communication resources from third-party IoT end devices. We support this new concept with the design and implementation of the FleXE framework that leverages research contributions in IoT security, edge computing and resource allocation algorithms. The core component of FleXE is a novel, lightweight containerization mechanism for embedded systems with secure access to shared sensing peripherals based on Berkeley Packet Filter (BPF). Furthermore, we incorporate this contribution with QoS-based orchestration algorithms designed to allocate containers to IoT end devices seamlessly for clients, while maximizing revenue for owners. Experimental results demonstrate that our containerization solution introduces negligible overhead compared to bare-metal or pure-Kernel (e.g., Zephyr) solutions, while also effectively handling secure sensing operations. Finally, we demonstrated the efficiency of the proposed container allocation heuristic through extensive simulations.}
}


@inproceedings{DBLP:conf/infocom/WangZMW25,
	author = {Ningning Wang and
                  Tianya Zhao and
                  Shiwen Mao and
                  Xuyu Wang},
	title = {Privacy-Preserving Wi-Fi Data Generation via Differential Privacy
                  in Diffusion Models},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044673},
	doi = {10.1109/INFOCOM55648.2025.11044673},
	timestamp = {Tue, 05 Aug 2025 22:40:41 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangZMW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the considerable effort and time needed to collect and label wireless data, there is a compelling need for data generation to facilitate data augmentation. To ensure the reliability of the data, the generated data needs to perform well in common evaluation metrics. However, this process can lead to the model memorizing some training data, resulting in potential privacy leaks. One major threat is the membership inference attack (MIA), which determines whether a specific sample was used in training the target model. While MIA has been extensively studied for discriminative models, its impact and defenses for generative models remain less explored. In this paper, we propose a hybrid training method for the diffusion model applied to Wi-Fi data as a defense against MIAs. The approach involves initially training the model without privacy constraints. After a specified number of training rounds, differential privacy (DP) is incorporated for fine-tuning. During this second phase, a co-optimization process is conducted in parallel to counteract the effects of the added noise. Experimental results demonstrate that the hybrid training method effectively defends against state-of-the-art MIAs for generative models without compromising model performance or requiring additional training efforts, showing significant promise for practical applications.}
}


@inproceedings{DBLP:conf/infocom/SeoYKP0K25,
	author = {Minjae Seo and
                  Myoungsung You and
                  Jaehan Kim and
                  Taejune Park and
                  Seungwon Shin and
                  Jinwoo Kim},
	title = {{MUFFLER:} Secure Tor Traffic Obfuscation with Dynamic Connection
                  Shuffling and Splitting},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044472},
	doi = {10.1109/INFOCOM55648.2025.11044472},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SeoYKP0K25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tor, a widely utilized privacy network, enables anonymous communication but is vulnerable to flow correlation attacks that deanonymize users by correlating traffic patterns from Tor's ingress and egress segments. Various defenses have been developed to mitigate these attacks; however, they have two critical limitations: (i) significant network overhead during obfuscation and (ii) a lack of dynamic obfuscation for egress segments, exposing traffic patterns to adversaries. In response, we introduce MUFFLER, a novel connection-level traffic obfuscation system designed to secure Tor egress traffic. It dynamically maps real connections to a distinct set of virtual connections between the final Tor nodes and targeted services, either public or hidden. This approach creates egress traffic patterns fundamentally different from those at ingress segments without adding intentional padding bytes or timing delays. The mapping of real and virtual connections is adjusted in real-time based on ongoing network conditions, thwarting adversaries' efforts to detect egress traffic patterns. Extensive evaluations show that MUFFLER mitigates powerful correlation attacks with a TPR of 1% at an FPR of 10−2 while imposing only a 2.17% bandwidth overhead. Moreover, it achieves up to 27x lower latency overhead than existing solutions and seamlessly integrates with the current Tor architecture.}
}


@inproceedings{DBLP:conf/infocom/RonYKRSS25,
	author = {Dara Ron and
                  Faisal Ahmed Yusufzai and
                  Sebastian Kwakye and
                  Satyaki Roy and
                  Nishanth Sastry and
                  Vijay K. Shah},
	title = {Time- Dependent Network Topology Optimization for {LEO} Satellite
                  Constellations},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044725},
	doi = {10.1109/INFOCOM55648.2025.11044725},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/RonYKRSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today's Low Earth Orbit (LEO) satellite networks, exemplified by SpaceX's Starlink, play a crucial role in delivering global internet access to millions of users. However, managing the dynamic and expansive nature of these networks poses significant challenges in designing optimal satellite topologies over time. In this paper, we introduce the Dynamic Time-Expanded Graph (DTEG)-based Optimal Topology Design (DoTD) algorithm to tackle these challenges effectively. We first formulate a novel space network topology optimization problem encompassing a multi-objective function - maximize network capacity, minimize latency, and mitigate link churn - under key inter-satellite link constraints. Our proposed approach addresses this optimization problem by transforming the objective functions and constraints into a time-dependent scoring function. This empowers each LEO satellite to assess potential connections based on their dynamic performance scores, ensuring robust network performance over time without scalability issues. Additionally, we provide proof of the score function's boundary to prove that it will not approach infinity, thus allowing each satellite to consistently evaluate others over time. For evaluation purposes, we utilize a realistic Mininet-based LEO satellite emulation tool that leverages Starlink's Two-Line Element (TLE) data. Comparative evaluation against two baseline methods - Greedy and +Grid, demonstrates the superior performance of our algorithm in optimizing network efficiency and resilience.}
}


@inproceedings{DBLP:conf/infocom/WanLTLHZWYCNLLC25,
	author = {Xinchen Wan and
                  Luyang Li and
                  Han Tian and
                  Xudong Liao and
                  Xinyang Huang and
                  Chaoliang Zeng and
                  Zilong Wang and
                  Xinyu Yang and
                  Ke Cheng and
                  Qingsong Ning and
                  Guyue Liu and
                  Layong Luo and
                  Kai Chen},
	title = {A Generic and Efficient Communication Framework for Message-Level
                  In-Network Computing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044670},
	doi = {10.1109/INFOCOM55648.2025.11044670},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WanLTLHZWYCNLLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Message-Ievel in-network computing (MINC) emerges as a promising hardware acceleration method that utilizes accelerators to offload message-level computation and enhance application performance in the datacenter. However, the development of MIN C applications is challenging in the communication aspect due to poor portability and under-utilized resource. In this paper, we present Leo, a generic and efficient commu-nication framework for MINC. Leo facilitates portability across both application and hardware via introducing a communication path abstraction, which is capable of describing generic applications with predictable communication performance across diverse hardware. It further incorporates a built-in multi-path communication over CPU and accelerator to enhance communication effi-ciency. We have implemented a prototype of Leo and evaluated it with four case studies on testbeds covering FPGA-based, SoC-based smartNICs and GPU. Experiments show that Leo achieves genericity and efficiency across MINC applications, yielding 1.2-4.7 x speedup over baselines with negligible overhead.}
}


@inproceedings{DBLP:conf/infocom/KolosovYBBL25,
	author = {Oleg Kolosov and
                  Gala Yadgar and
                  Rasoul Behravesh and
                  David Breitgand and
                  Dean H. Lorenz},
	title = {The Power of Alternatives in Network Embedding},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044449},
	doi = {10.1109/INFOCOM55648.2025.11044449},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/KolosovYBBL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the virtual network embedding problem, the goal is to map (embed) a set of virtual network instances to a given physical network substrate at minimal cost, while respecting the capacity constraints of the physical network. This NP-hard problem is fundamental to network virtualization, embodying essential properties of resource allocation problems faced by service providers in the edge-to-cloud spectrum. Due to its centrality, this problem and its variants have been extensively studied and remain in the focus of the research community. In this paper, we present a new variant, the virtual network embedding with alternatives problem (VNEAP). This new problem captures the power of a common network virtualization practice, in which virtual network topologies are malleable-embedding of a given virtual network instance can be performed using any of the alternatives from a given set of topology alternatives. We provide two efficient heuristics for VNEAP and show that having multiple virtual network alternatives for the same application is superior to the best results known for the classic formulation. We conclude that capturing the problem domain via VNEAP can facilitate more efficient network virtualization solutions.}
}


@inproceedings{DBLP:conf/infocom/ZhangSG25,
	author = {Ziyan Zhang and
                  Florian Speelman and
                  Paola Grosso},
	title = {Enhancing Position Verification in Multi-Node Quantum Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044770},
	doi = {10.1109/INFOCOM55648.2025.11044770},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangSG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum position verification (QPV) is emerging as a promising application for quantum networks, leveraging spatial and quantum information to verify locations. However, current discussions on QPV protocols are largely confined to perfect experimental setups and one-dimensional scenarios. To address this gap, we extend the  Q P V f B B 84 \\mathbf{QPV}_{BB84}^{f}  protocol to two dimensions. First, we define the requirements for the 2D QPV task and assess its performance under real-world constraints, which potentially expose the system to external attacks. To strengthen the protocol against these vulnerabilities, we study the 'danger zones', defining a region in spacetime within which attackers can manipulate these real-world constraints to convince verifiers. We then develop two algorithms to implement this theory: the Verifiable Vertices Selection algorithm, which identifies nodes that can validate their locations with designated verifiers, and the Malicious Prover Location Identification algorithm, which determines the ‘danger zone’ around the prover. Finally, we present a case study to demonstrate the conceptual implementation of the protocol. Our findings advance the development of secure and practical QPV protocols while highlighting the potential of quantum networks in the noisy intermediate-scale quantum (NISQ) era.}
}


@inproceedings{DBLP:conf/infocom/ZhouKE25,
	author = {Xujin Zhou and
                  Irem Koprulu and
                  Atilla Eryilmaz},
	title = {Novel Drift-Based Design and Analysis for Synchronized and Fresh Communication
                  over Broadcast Channels},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044525},
	doi = {10.1109/INFOCOM55648.2025.11044525},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouKE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Synchronized and fresh communication of common information is vitally important in numerous multi-user network scenarios, whereby end-users must perform coordinated real-time action with the available information. However, developing efficient policies with performance guarantees is greatly complicated by the abruptly changing nature of related age and synchronization metrics. In particular, powerful approaches that are based on so-called drift- plus- penalty (D PP) methods could not be employed due to the non-traditional multiplicative update dynamics of age and synchronization. In this paper, we overcome these limitations by designing and analyzing a Lyapunov-drift-based algorithm under the non-traditional age and synchronization dynamics that is not only low-complexity and analyzable, but also performs better than all the prior designs in numerical investigations. By comparing our design with two alternatives using the DPP approach, we also shed some light on the key aspect of our design that enables the performance analysis, which may be useful in future studies in multiplicative update dynamics. Moreover, our design reveals that a previously unconsidered feature of ‘maximum age’ also contains key information that governs a good policy. As such, our design naturally supplies a new critical feature that can be integrated into future reinforcement-learning-based designs.}
}


@inproceedings{DBLP:conf/infocom/ZhangL025,
	author = {Yulong Zhang and
                  Xuanheng Li and
                  Yi Sun},
	title = {3DVidar: {A} Contact-free 3D Vibration Sensing System Based on a Single
                  mmWave Radar},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044442},
	doi = {10.1109/INFOCOM55648.2025.11044442},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vibration sensing is vital for machinery health, which can be used for condition monitoring or anomalies detection. Tra-ditional methods mainly rely on contact-based sensors, which may face deployment challenges. Recently, millimeter wave (mmWave) radar has been regarded as a promising sensing method for vibration monitoring. However, since radar is mainly sensitive to the vibrations perpendicular to its antennas, existing works can only achieve 1D/2D vibration sensing based on single radar, or 3D sensing by multiple ones. In this paper, we propose a contact-free 3D vibration sensing system using single mm Wave radar, called 3DVidar. Considering the insufficient information provided by single radar, we develop a multi-point multi-path signal enhancement method to compensate for the lack of 3D vibration information. Additionally, we design a virtual antenna combination approach to further enrich the information from different perspectives. To reconstruct the 3D vibration trajectory from the complex radar information, we develop a data-driven approach, named TriVFormer, where multi-scale convolution and attention mechanism are employed to effectively fuse the multi-view information. We implement 3DVidar on a commercial mm Wave radar, and the results show that 3DVidar can accurately reconstruct 3D trajectory across various distances and angles, with mean error of 0.4526Hz and 0.0202mm, respectively.}
}


@inproceedings{DBLP:conf/infocom/LuoWHW25,
	author = {Zhiqing Luo and
                  Yi Wang and
                  Yingying He and
                  Wei Wang},
	title = {Improving Multi-Vehicle Perception Fusion with Millimeter-Wave Radar
                  Assistance},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044604},
	doi = {10.1109/INFOCOM55648.2025.11044604},
	timestamp = {Sun, 10 Aug 2025 07:21:29 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LuoWHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cooperative perception enables vehicles to share sensor readings and has become a new paradigm to improve driving safety, where the key enabling technology for realizing this vision is to real-time and accurately align and fuse the perceptions. Recent advances to align the views rely on high-density LiDAR data or fine-grained image feature representations, which however fail to meet the requirements of accuracy, real-time, and adaptability for autonomous driving. To this end, we present MMatch, a lightweight system that enables accurate and real-time perception fusion with mm Wave radar point clouds. The key insight is that fine-grained spatial information provided by the radar present unique associations with all the vehicles even in two separate views. As a result, by capturing and understanding the unique local and global position of the targets in this association, we can quickly find out all the co-visible vehicles for view alignment. We implement MMatch on both the datasets collected from the CARLA platform and the real-world traffic with over 15,000 radar point cloud pairs. Experimental results show that MMatch achieves decimeter-level accuracy within 59ms, which significantly improves the reliability for autonomous driving.}
}


@inproceedings{DBLP:conf/infocom/WangZHK25,
	author = {Jiaming Wang and
                  Samin Beheshti Zavareh and
                  Haitham Hassanieh and
                  Bhuvana Krishnaswamy},
	title = {Heartbeat Aware Decoding in Molecular Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044658},
	doi = {10.1109/INFOCOM55648.2025.11044658},
	timestamp = {Fri, 08 Aug 2025 16:41:34 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangZHK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Molecular communication (MC) is a competitive candidate of implementing the long-lasting in-body communication for the body-area medical network, considering its unparalleled advantages over conventional wireless communication in bio-compatibility. Blood vessel is a common topic and the influence of blood flow on the propagation of molecular signals has been studied. However, one of the most significant features was overlooked by all past work-blood is a periodic flow with varying rate, so the channel response varies across time. This is similar to the fast-varying channel in conventional wireless systems, but the molecular condition could be much worse because the channel can vary from symbol to next, so using average flow speed for approximation is far from reality. This paper proposes FlowLink to compensate for the drastic channel variation. FlowLink employs a flow meter and records the real-time flow rate, which accumulates over time and forms a “flow axis”. By resampling the receiver signal on this flow axis with a fixed flow interval, FlowLink compensates for the varying channel response and restores the coherence time requirement for packet-based communication. FlowLink is evaluated on a vessel-like testbed with a periodic flow mimicking blood, which demonstrates an average improvement in decoding BER of 40% compared to conventional flow-unaware decoder.}
}


@inproceedings{DBLP:conf/infocom/ZhaoK25,
	author = {Zhuoyi Zhao and
                  Igor Kadota},
	title = {Optimizing Age of Information without Knowing the Age of Information},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044597},
	doi = {10.1109/INFOCOM55648.2025.11044597},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consider a network where a wireless base station (BS) connects multiple source-destination pairs. Packets from each source are generated according to a renewal process and are enqueued in a single-packet queue that stores only the freshest packet. The BS decides, at each time slot, which sources to schedule. Selected sources transmit their packet to the BS via unreliable links. Successfully received packets are forwarded to corresponding destinations. The connection between the BS and destinations is assumed unreliable and delayed. Information freshness is captured by the Age of Information (AoI) metric. The objective of the scheduling decisions is leveraging the delayed and unreliable AoI knowledge to keep the information fresh. In this paper, we derive a lower bound on the achievable AoI by any scheduling policy. Then, we develop an optimal randomized policy for any packet generation processes. Next, we develop minimum mean square error estimators of the AoI and system times, and a Max-Weight Policy that leverages these estimators. We evaluate the AoI of the Optimal Randomized Policy and the Max-Weight Policy both analytically and through simulations. The numerical results suggest that the Max-Weight Policy with estimation outperforms the Optimal Randomized Policy even when the BS has no AoI knowledge.}
}


@inproceedings{DBLP:conf/infocom/Nolan025,
	author = {John Nolan and
                  Xinyu Zhang},
	title = {Ricochet: Scalable Passive Beamforming for mmWave Networks Using Reflectarrays},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044474},
	doi = {10.1109/INFOCOM55648.2025.11044474},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Nolan025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter-wave (mmWave) technologies have seen explosive growth in the past few years with bleeding edge solutions offering unprecedented communication capacity. Unfortunately, they also introduce new challenges such as limitations in range, coverage blind spots and severe disruptions due to blockages from obstructions. Reconfigurable intelligent surfaces (RIS) have been used to alleviate some of these problems. However, RIS solutions create additional problems such as limited NLoS range and complicated beam management protocols making it incompatible with 5G. In this paper, we propose Ricochet, a fully passive, PCB-fabricated reflective surface which uses reflect-arrays for far-field communications. Ricochet can re-steer and reshape incidental mmWave signals towards anomalous directions to divert around obstructions, and generate wider/narrower or multi-beam patterns to fill coverage holes. To overcome the reflect-array's lack of reconfigurability, we introduce post-fabrication techniques that enable incremental deployment of Ricochet to adapt to environment/specification changes, enhancing the SNR or broadening the coverage of an existing surface. We further design a multi-user algorithm that enables multiple transmitters to share the same Ricochet surface while avoiding interference. Our experiments with 5G towers and WiGig access points, demonstrate that Ricochet can effectively improve coverage without any modification to existing network infrastructure or mobile devices.}
}


@inproceedings{DBLP:conf/infocom/ChakrabortyAPST25,
	author = {Shuvam Chakraborty and
                  Steven Arbogast and
                  Claire Parisi and
                  Dola Saha and
                  Ngwe Thawdar},
	title = {WIT-Waveform Independent Tunable Channel Model for sub-Terahertz Communication},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044550},
	doi = {10.1109/INFOCOM55648.2025.11044550},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChakrabortyAPST25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-broadband communication in emerging spectrum, like the sub-Terahertz (THz) and THz band is envisioned as one of the leading technologies to meet the exponentially growing data rate requirements of future wireless communication networks. However, there is a lack of availability of differentiable channel models for ultra-broadband links for such less explored spectra. Furthermore, the channel and hardware impairments are indistinguishable from each other making the problem complex to model in closed form. In this work, we propose a Generative Adversarial Network (GAN) based channel model that captures the non-linearity in both channel and the underlying hardware and can be used in an end-to-end optimization framework. The proposed model exploits the adversarial learning properties to approximate the stochastic channel distribution while the domain knowledge is used to maintain a information theoretic relevance of the generated channel model by guaranteeing high mutual information. The channel model is developed without any dependence on the baseband signal, which is essential for embracing any new modulation or pre-equalization techniques at the transmitter side. Our proposed model is tunable to various sub-THz frequencies, ultra-broad bandwidths and channel power. Results from comprehensive over-the-air experiments in 140 GHz and 240 GHz frequencies show high model accuracy of up to 93% for different types of waveforms and provides tunability using domain knowledge.}
}


@inproceedings{DBLP:conf/infocom/HeidariGJB25,
	author = {Adel Heidari and
                  Agrim Gupta and
                  Ish Kumar Jain and
                  Dinesh Bharadia},
	title = {PhaseMO: {A} Universal Massive {MIMO} Architecture for Sustainable
                  NextG},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044781},
	doi = {10.1109/INFOCOM55648.2025.11044781},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HeidariGJB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid proliferation of devices and increasing data traffic in cellular networks necessitate advanced solutions to meet these escalating demands. Massive MIMO (Multiple Input Multiple Output) technology offers a promising approach, significantly enhancing throughput, coverage, and spatial multiplexing. Despite its advantages, Massive MIMO systems often lack flexible software controls over hardware, limiting their ability to optimize operational expenditure (OpEx) by reducing power consumption while maintaining performance. Current software-controlled methods, such as antenna muting combined with digital beamforming and hybrid beamforming, have notable limitations. Antenna muting struggles to maintain throughput and coverage, while hybrid beamforming faces hardware constraints that restrict scalability and future-proofing. This work presents PhaseMO, a versatile approach that adapts to varying network loads. PhaseMO effectively reduces power consumption in low-load scenarios without sacrificing coverage and overcomes the hardware limitations of hybrid beamforming, offering a scalable and future-proof solution. We will show that PhaseMO can achieve up to 30% improvement in energy efficiency while avoiding about 10% coverage reduction and a 5dB increase in UE transmit power.}
}


@inproceedings{DBLP:conf/infocom/XueHZ0L25,
	author = {Jiahao Xue and
                  Xiao Han and
                  Shangqing Zhao and
                  Yao Liu and
                  Zhuo Lu},
	title = {An Active Identification Overriding Attack Against {RFID:} Attack
                  Strategy and Defense Design},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044779},
	doi = {10.1109/INFOCOM55648.2025.11044779},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XueHZ0L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The security of radio frequency identification (RFID) has been extensively studied in terms of eavesdropping, jamming, relay and tag cloning attacks in the literature. In this paper, we aim to explore a new type of attack, called identification overriding (IDO), in which an attacker tries into inject malicious signals to a tag's reflected signal to override the unique information transmitted by the tag. The IDO attack is designed without the knowledge of the tag's transmitted data and has low computational complexity to inject the malicious data. In addition, the attacker also minimizes the overall energy of the injection signal to make the received signal look normal to the reader. Extensive experiments show that the IDO attack maintains a high success probability generally ranging from 60% to 99% in various evaluation scenarios. Finally, we provide a defense method for the reader to detect the presence of the IDO attack from the received signal.}
}


@inproceedings{DBLP:conf/infocom/LiuJZ25,
	author = {Yimin Liu and
                  Peng Jiang and
                  Liehuang Zhu},
	title = {Preference Profiling Attacks Against Vertical Federated Learning Over
                  Graph Data},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044459},
	doi = {10.1109/INFOCOM55648.2025.11044459},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuJZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-based vertical federated learning (GVFL) enables a service provider (i.e., active party) who owns a labeled graph to collaborate with passive parties who possess auxiliary node features and edges to improve model performance. The labeled training graph reflects the active party's class preference, whose leakage brings about the exposure of commercial trade secrets. However, the potential for class preference leakage in GVFL has not been investigated. In this paper, we propose SGPP, a generic attack framework for profiling the active party's class preference in G VFL where the adversary is allowed to only access a trained extractor and a labeled graph from a non-training domain. SGPP generates a compatible surrogate classifier with the extractor to extract sensitivity and a preference classifier to predict its preferred class, thereby profiling the class preference. To ensure accurate sensitivity extraction and prediction, we introduce an Adversarial Correction Block (ACB) to adapt classifiers for generalizing cross-domain inputs. Evaluation with two attack scenarios on diverse graph datasets confirms the effectiveness of SGPP.}
}


@inproceedings{DBLP:conf/infocom/ChenXZM25,
	author = {Zheyi Chen and
                  Longxiang Xue and
                  Luying Zhong and
                  Geyong Min},
	title = {FedGPA: Federated Learning with Global-Personalized Collaboration
                  for Edge Anomaly Detection},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044589},
	doi = {10.1109/INFOCOM55648.2025.11044589},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenXZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge anomaly detection guarantees the security of Internet-of-Things (IoT). The emerging Federated Learning (FL) can ameliorate the privacy-leakage and data-island issues in edge anomaly detection. However, existing FL-based solutions still reveal limitations in handling statistical and system heterogeneity, thus they cannot adapt to anomaly detection in complex edge environments. To address these problems, we propose FedGPA, a novel Federated learning with Global-Personalized collaboration for edge Anomaly detection. First, we design a conditional calculation component to transform traffic features into global and personalized feature vectors. Next, we introduce contrast and magnitude losses in the global-class embedding module and guide the learning of global feature vectors with the embedding of sample classes. Then, we adopt cross-entropy loss to guide the learning of personalized feature vectors. Finally, the cosine similarity between the updated gradients of cross-entropy and overall losses is used to determine the loss replacement, thereby accelerating the model training. Notably, we prove the FedGPA can converge stably during the aggregation process. Using real-world testbed and traffic datasets, extensive experiments verify the effectiveness of the FedGPA, which efficiently solves statistical and system heterogeneity. Compared to state-of-the-art methods, the FedGPA achieves higher detection accuracy and shorter training time, exhibiting better scalability and convergence.}
}


@inproceedings{DBLP:conf/infocom/PingCZ0C25,
	author = {Chan Tin Ping and
                  Yunlong Cheng and
                  Yizhan Zhu and
                  Xiaofeng Gao and
                  Guihai Chen},
	title = {Symmetry-Preserving Architecture for Multi-NUMA Environments {(SPANE):}
                  {A} Deep Reinforcement Learning Approach for Dynamic {VM} Scheduling},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044446},
	doi = {10.1109/INFOCOM55648.2025.11044446},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/PingCZ0C25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As cloud computing continues to evolve, the adoption of multi-NUMA (Non-Uniform Memory Access) architecture by cloud service providers has introduced new challenges in virtual machine (VM) scheduling. To address these challenges and more accurately reflect the complexities faced by modern cloud environments, we introduce the Dynamic VM Allocation problem in Multi-NUMA PM (DVAMP). We formally define both offline and online versions of DVAMP as mixed-integer lin-ear programming problems, providing a rigorous mathematical foundation for analysis. A tight performance bound for greedy online algorithms is derived, offering insights into the worst-case optimality gap as a function of the number of physical machines and VM lifetime variability. To address the challenges posed by DVAMP, we propose SPANE (Symmetry-Preserving Architecture for Multi-NUMA Environments), a novel deep rein-forcement learning approach that exploits the problem's inherent symmetries. SPANE produces invariant results under arbitrary permutations of physical machine states, enhancing learning effi-ciency and solution quality. Extensive experiments conducted on the Huawei-East-1 dataset demonstrate that SPANE outperforms existing baselines, reducing average VM wait time by 45%. Our work contributes to the field of cloud resource management by providing both theoretical insights and practical solutions for VM scheduling in multi-NUMA environments, addressing a critical gap in the literature and offering improved performance for real-world cloud systems.}
}


@inproceedings{DBLP:conf/infocom/WangW025,
	author = {Shuo Wang and
                  Tianxin Wang and
                  Xudong Wang},
	title = {FedPDA: Collaborative Learning for Reducing Online-Adaptation Frequency
                  of Neural Receivers},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044747},
	doi = {10.1109/INFOCOM55648.2025.11044747},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless neural receivers provide a promising alternative to conventional receivers. To perform well in different channel environments, online adaption is required. However, during this process, performance remains low. Thus, an approach called federated collaborative learning with pruned-data aggregation (FedPDA) is developed to reduce online-adaptation frequency. The basic idea is that, upon online adaptation, mobile terminals further update their neural receivers collaboratively via federated learning. To reduce memory consumption, neural receivers follow a main-side network architecture where only the side network needs retraining during collaborative learning. To avoid catastrophic forgetting during continual learning, local data on terminals are pruned, with only a small percent sent to the base station. With such data, the base station also trains a neural receiver before conducting model aggregation. FedPDA is distinct with several features: 1) small memory footprint and no storage burden on terminals; 2) no catastrophic forgetting issue; 3) low communication cost. Performance results show that FedPDA reduces online adaptation by more than 90% and memory footprint by 70%. It achieves comparable performance as centralized schemes, but reducing communication cost by 78%. Compared to vanilla federated learning, FedPDA resolves the catastrophic forgetting issue without storage burden, and also reduces the communication cost by 50%.}
}


@inproceedings{DBLP:conf/infocom/HuangQHLW25,
	author = {Kang Huang and
                  Chao Qiu and
                  Chenxuan Hou and
                  Xiuhua Li and
                  Xiaofei Wang},
	title = {HyperJet: Joint Communication and Computation Scheduling for Hypergraph
                  Tasks in Distributed Edge Computing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044587},
	doi = {10.1109/INFOCOM55648.2025.11044587},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuangQHLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed Edge Computing (DEC) has emerged as a novel paradigm, owing to its superior performance in communication latency, parallel computing efficiency, and energy consumption. With the surge of tasks in generative artificial intelligence, DEC faces higher demands for parallel computing efficiency. Scheduling multiple tasks for simultaneous processing, rather than one-by-one handling, could enhance parallel efficiency. Multiple tasks have multi-dependencies, i.e., sequence dependency, attribute similarity, and attribute correlation. Utilizing the bidirectional edges of traditional graphs to represent multi-dependencies can lead to an explosion in quantity. A hypergraph, with its hyperedges capable of connecting any number of vertices, can significantly solve the above problem. However, the multi-dependencies are rarely studied in the current research, posing the challenges, including incapable representing and unable capturing of multi-dependency hypergraph. In this work, we introduce a Joint communication and computation scheduling for hypErgraph Tasks in DEC, namely HypeJet, To effectively represent multi-dependencies, we employ hypergraph construction to represent task attributes and utilize hypergraph partitioning to clarify and refine task attribute correlations, enhancing parallel efficiency. In response to the challenge of capturing multi-dependencies, we employ a scheduling mechanism with the hypergraph neural network that efficiently acquires higher-order attribute correlated information among convolution matrices, providing enriched contextual information on multi-dependencies that supports decision-making in scheduling tasks. The evaluations using real-world traces demonstrate an 18.07% improvement in parallel efficiency of task scheduling.}
}


@inproceedings{DBLP:conf/infocom/HuanLG25,
	author = {Xu Huan and
                  Jian Li and
                  Haibing Guan},
	title = {CoINT2: {A} Heuristic Coordinator for Responsive Receive-Side Network
                  {I/O} Virtualization in Overcommitment Cloud},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044460},
	doi = {10.1109/INFOCOM55648.2025.11044460},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuanLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud computing providers are committed to CPU overcommitment for maximizing resource utilization. However, CPU overcommitment degrades network I/O performance by deferred interrupt acceptance as well as interruptibility holder preemption under CPU overcommitment scenarios. Previous research focused on bridging the semantic gap between the hypervisor scheduler and the source of interrupt delivery. However, unlike virtual interrupt delivery is highly mediated by the hypervisor in para-virtualization, pass-through I/O utilizes interrupt posting hardware for non-exit-to-hypervisor interrupt delivery. Therefore, the lack of exposed interfaces poses challenges in transferring existing solutions. In this paper, we implement a software and hardware co-design design, COPINT2. First, we reintroduces the hypervisor to interrupt delivery, leveraging its capability to select target vCPUs for interrupt delivery from hardware. Second, we heuristically divide the VCPU execution cycle into interruptibility holding and enabling phases to eliminating interrupt holding preemption. CoINT2 increase throughput by up to 126% and achieve up to 82% lower latency in both micro and macro benchmarks, which is proven to be a practical solution in that provides near non-overcommitment performance for I/O workloads in overcommitted cloud.}
}


@inproceedings{DBLP:conf/infocom/GaoJL0HL25,
	author = {Demin Gao and
                  Wenchao Jiang and
                  Ruofeng Liu and
                  Weizheng Wang and
                  Zhaoyang Han and
                  Yunhuai Liu},
	title = {LoFi: Physical-layer {CTC} from LoRa to WiFi with {IEEE} 802.11ax},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044480},
	doi = {10.1109/INFOCOM55648.2025.11044480},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GaoJL0HL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-Power Wide-Area Networks (LPWANs) have emerged as a promising communication technology for facilitating connectivity in the realm of the Internet of Things. A novel LoRa chip has been introduced to provide extensive coverage and energy-efficient support within the 2.4 GHz spectrum. However, establishing a direct connection between Wireless Personal Area Network (WPAN) technologies, such as WiFi, and LPWAN presents challenges due to the inherent asymmetry of the Cross-Technology Communication (CTC) channel. To overcome this obstacle, a novel communication method called LoFi has been introduced. LoFi serves as a cross-technology communication method that enables LoRa devices to establish connections and engage in communication with WiFi-based WPAN networks. A key observation of this study is that when a LoRa frame collides with an ongoing WiFi transmission, the WiFi receiver captures and retains the LoRa data, even post-demodulation within the WiFi hardware. By analyzing the decoded WiFi payload, LoFi can retrieve the LoRa data, and this method remains fully compatible with existing commodity WiFi hardware. Moreover, evaluations conducted using both USRP and commodity devices have demonstrated that LoFi can achieve concurrent wireless communication from LoRa commercial chips to WiFi networks with a frame reception rate exceeding 98 %.}
}


@inproceedings{DBLP:conf/infocom/Sun0YJL025,
	author = {Xikai Sun and
                  Fan Dang and
                  Zihao Yang and
                  Xinqi Jin and
                  Junhao Li and
                  Yunhao Liu},
	title = {6Loda: Pattern Filtering and Ensemble Learning for IPv6 Target Generation
                  and Scanning},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044482},
	doi = {10.1109/INFOCOM55648.2025.11044482},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Sun0YJL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IPv6 target generation is crucial for surveying the vast IPv6 address space, which is essential for network management and IPv6 deployment policies. However, existing techniques often suffer from low hit rates due to ineffective space partitioning caused by outlier addresses and limitations in current outlier removal algorithms. To address these challenges, we propose 6Loda, a novel approach that combines pattern filtering and ensemble learning to efficiently remove outlier addresses and discover active IPv6 addresses. Given a set of known active addresses, 6Loda first employs a pattern-based filter to preliminarily eliminate some outlier addresses. It then utilizes a two-level (divisive hierarchical clustering) DHC algorithm to partition the seed set and applies the Loda algorithm to automatically remove remaining outliers in address spaces. Finally, 6Loda implements the random generation algorithm to produce addresses with high hit rates. Experiments conducted on large-scale datasets demonstrate that 6Loda achieves a × 2.26 improvement in hit rate compared to state-of-the-art methods, while maintaining the same budget constraints.}
}


@inproceedings{DBLP:conf/infocom/LozanoAGC25,
	author = {J. Xavier Salvat Lozano and
                  Jose A. Ayala{-}Romero and
                  Andres Garcia{-}Saavedra and
                  Xavier Costa{-}P{\'{e}}rez},
	title = {Kairos: Energy-Efficient Radio Unit Control for {O-RAN} via Advanced
                  Sleep Modes},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044736},
	doi = {10.1109/INFOCOM55648.2025.11044736},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LozanoAGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The high energy footprint of 5G base stations, particularly the radio units (RUs), poses a significant environmental and economic challenge. We introduce Kairos, a novel approach to maximize the energy-saving potential of O-RAN's Advanced Sleep Modes (ASMs). Unlike state-of-the-art solutions, which often rely on complex ASM selection algorithms unsuitable for time-constrained base stations and fail to guarantee stringent QoS demands, Kairos offers a simple yet effective joint ASM selection and radio scheduling policy capable of real-time operation. This policy is then optimized using a data-driven algorithm within an xApp, which enables several key innovations: ( i i ) a dimensionality-invariant encoder to handle variable input sizes (e.g., time-varying network slices), (ii) distributional critics to accurately model QoS metrics and ensure constraint satisfaction, and (iii) a single-actor-multiple-critic architecture to effectively manage multiple constraints. Through experimental analysis on a commercial RU and trace-driven simulations, we demonstrate Kairos's potential to achieve energy reductions ranging between 15% and 72% while meeting QoS requirements, offering a practical solution for cost- and energy-efficient 5G networks.}
}


@inproceedings{DBLP:conf/infocom/ZhouXWLWCZL25,
	author = {Qiannan Zhou and
                  Fei Xu and
                  Lingxuan Weng and
                  Ruixing Li and
                  Xudong Wu and
                  Li Chen and
                  Zhi Zhou and
                  Fangming Liu},
	title = {Espresso: Cost-Efficient Large Model Training by Exploiting {GPU}
                  Heterogeneity in the Cloud},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044693},
	doi = {10.1109/INFOCOM55648.2025.11044693},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouXWLWCZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Transformer-based models deepen and datasets expand, training large models demands numerous accelerators, particularly GPUs, bringing high cloud expenses. However, conventional homogeneous resource provisioning is inefficient due to limited cloud resources and low GPU utilization. This challenge necessitates heterogeneous GPU provisioning for training in clouds. Current research on large model training often focuses on load balancing of stages, neglecting the varying computing and memory demands across stages. Additionally, the allocation of heterogeneous G PU s for training has surprisingly received little attention. This paper introduces Espresso, a cost-efficient GPU provisioning framework that unifies the heterogeneous GPU allocation (GPU allocator) and adequate stage placement (stage placer) for large model training in the cloud. Specifically, the GPU allocator proposes a cost tree-based provisioning strategy to prioritize searching allocation plans with lower costs and reduce unnecessary branches by multi-dimensional pruning methods. The resource-aware stage placer further devises a compute-memory ratio to optimize communication and computation efficiency during training. We have open-sourced a prototype of Espresso and conducted prototype experiments on four representative large models in public clouds. Extensive experiment results demonstrate that Espresso guarantees the performance for large model training while saving costs by up to 49.8 % compared to state-of-the-art solutions, yet with acceptable runtime overhead.}
}


@inproceedings{DBLP:conf/infocom/GaoLXXMH25,
	author = {Luyao Gao and
                  Jianchun Liu and
                  Hongli Xu and
                  Sun Xu and
                  Qianpiao Ma and
                  Liusheng Huang},
	title = {Accelerating End-Cloud Collaborative Inference via Near Bubble-Free
                  Pipeline Optimization},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044632},
	doi = {10.1109/INFOCOM55648.2025.11044632},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GaoLXXMH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {End-cloud collaboration offers a promising strategy to enhance the Quality of Service (QoS) in DNN inference by offloading portions of the inference workload from end devices to cloud servers. Despite the potential, the complex model architectures and dynamic network conditions will introduce numerous bubbles (i.e., idle waiting time) in pipeline execution, resulting in inefficient resource utilization and degraded QoS. To address these challenges, we introduce a novel framework named COACH, designed for near bubble-free pipeline collaborative inference, thereby achieving low inference latency and high system throughput. Initially, COACH employs an offline component that utilizes an efficient recursive divide-and-conquer algorithm to optimize both model partitioning and transmission quantization, aiming to minimize the occurrence of pipeline bubbles. Subsequently, the online component in COACH employs an adaptive quantization adjustment and a context-aware caching strategy to further stabilize pipeline execution. Specifically, COACH analyzes the correlation between intermediate data and label semantic centers in the cache, along with its influence on the quantization adjustment, thereby effectively accommodating network fluctuations. Our experiments demonstrate the efficacy of COACH in reducing inference latency and enhancing system throughput. Notably, while maintaining comparable accuracy, COACH achieves up to 1.7× faster inference and 2.1× higher system throughput than baselines.}
}


@inproceedings{DBLP:conf/infocom/JiXZZGXYM25,
	author = {Xiang Ji and
                  Changqiao Xu and
                  Zekun Zhang and
                  Lujie Zhong and
                  Kai Gao and
                  Han Xiao and
                  Shujie Yang and
                  Gabriel{-}Miro Muntean},
	title = {HydraCC: Finding the Pareto Frontiers of Congestion Control via Multi-objective
                  Evolutionary Exploration},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044577},
	doi = {10.1109/INFOCOM55648.2025.11044577},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/JiXZZGXYM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Current congestion control algorithms often fall short when balancing multiple objectives, leading to inefficient competition and subsequently diminishes network efficiency. To address this challenge, we introduce HydraCC, an innovative multi-objective congestion control solution. Through theoretical derivation, we elucidate the impact of algorithm parameters on several performance metrics, such as QoS fairness, responsiveness, throughput, and window fluctuation. Our analysis reveals that these objectives often correlate or even compete with each other. Building on this insight, HydraCC is designed to expand a Pareto solution set from an initial solution. Specifically, using a carefully architected method based on Krylov subspace iteration, we efficiently explore the evolutionary direction of Pareto solutions. Crucially, each solution presents its own strengths and costs, thereby offering a broader space for trade-offs. Through rigorous real-world experiments and simulations, we demonstrate HydraCC's adaptability, and its superiority to existing state-of-the-art congestion control algorithms across various performance metrics. Moreover, it significantly mitigates the issues associated with multiple coexisting flows. When compared to contemporary leading algorithms such as Orca, Cubic, and PCC-Vivace, the convergence stability of HydraCC shows an improvement in the range of 47.89~77.21%.}
}


@inproceedings{DBLP:conf/infocom/YinWGYY025,
	author = {Yuan Yin and
                  Di{\'{e}} Wu and
                  Jing Gao and
                  Jin Yang and
                  Jilin Yang and
                  Tang Liu},
	title = {{SANE:} Safe Charging with Wave Interference},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044512},
	doi = {10.1109/INFOCOM55648.2025.11044512},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YinWGYY025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With wireless power transfer becoming a popular technology for recharging devices in various fields, the accompanying potential threats of electromagnetic radiation (EMR) have attracted widespread attention from researchers. Although some state-of-the-art safe charging solutions have been developed to address EMR threats, they all ignore wave interference, resulting in a gap between their theoretical models and real-world applications. In this work, we focus on developing a safe charging scheme considering wave interference, termed Safe chArging with wave iNterferencE (SANE), aiming to maximize the overall charging utility while ensuring the EMR safety of critical locations. Specifically, we first build a charging model with wave interference to explore the impact of wave interference on the EMR intensity. Then, we devise a charger placement method that meets safe charging requirements while enabling devices to harvest maximum power from constructive interference. Finally, we conduct extensive simulations and field experiments to evaluate the effectiveness of SANE scheme. The results demonstrate that SANE not only ensures the EMR intensity at all critical locations remains below the safety threshold but also outperforms the comparison algorithms by an average of 89.65% in overall charging utility.}
}


@inproceedings{DBLP:conf/infocom/WangWSEL25,
	author = {Xuan Wang and
                  Xiaoyi Wu and
                  Ying Sun and
                  Atilla Eryilmaz and
                  Bin Li},
	title = {Optimal Real-Time Synchronized Scheduling for Collaborative Content
                  Delivery},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044596},
	doi = {10.1109/INFOCOM55648.2025.11044596},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangWSEL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by applications such as cloud gaming and collaborative mixed reality, we consider the real-time synchronized scheduling for collaborative content delivery from a server to a group of users, ensuring that all users can view the content simultaneously for each delivery. We introduce latency compensation to control the server's content delivery time for each user, aiming to maximize synchronization performance (i.e., minimizing the gap between maximum and minimum latencies) while keeping the overall latency as low as possible. Our objective function includes a non-smooth component. To address this, we employ the stochastic proximal point (SPP) that sequentially updates the delay compensation, requiring only the previous latency samples instead of latency distribution. We then explore the structure of the subproblems and utilize the alternating direction method of multipliers (ADMM) to decompose the computation, resulting in a low-computational update with the complexity of at most  O ( N log N ) \\mathcal{O}(N\\log N)  per iteration, where  N N  is the number of users. Furthermore, we capture the convergence rate of our proposed algorithm. Finally, we demonstrate the superiority of our proposed algorithm against two baselines through simulations, including those based on real network latency trace datasets.}
}


@inproceedings{DBLP:conf/infocom/LiXZL25,
	author = {Mingzhe Li and
                  Zhen Xiao and
                  Cui Zhao and
                  Zhenjiang Li},
	title = {Using Weak Light Sources to Power Sensor Nodes for Sustainable IoT},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044560},
	doi = {10.1109/INFOCOM55648.2025.11044560},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiXZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces a novel Internet of Things (IoT) node design, WISE, which can operate autonomously using weak light sources, e.g., indoor lights or weak sunlight. As IoT systems are increasingly deployed, a fundamental question is drawing attention: How can we sustainably power the vast number of IoT nodes? Traditional batteries are insufficient due to their limited lifetime and high replacement costs in scalable IoT systems. Energy harvesting thus recently emerged as a promising solution, where nodes can harvest enough energy from outdoor sunlight. Regarding recent works, this paper focuses on an important but understudied segment of deployment scenarios that mainly contain weak light sources, e.g., indoors where lamps are the main light source. Our key innovation for powering nodes using weak light sources is a new node architecture featuring a timing supply control part, reducing quiescent current to nano-ampere (nA) levels, and optimizing energy use. We further propose a scheme to optimize the selection of hardware components in customizing WISE nodes, according to different design requirements, and an energy-aware scheduling method. We develop and test three WISE node configurations in real-world environments, achieving substantial improvements in communication rate and energy efficiency compared to the state-of-the-art node designs.}
}


@inproceedings{DBLP:conf/infocom/LinDYJY25,
	author = {Huiping Lin and
                  Ruixuan Deng and
                  Chris Z. Yao and
                  Zhengfeng Ji and
                  Mingsheng Ying},
	title = {Control Flow Adaption: An Efficient Simulation Method for Noisy Quantum
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044741},
	doi = {10.1109/INFOCOM55648.2025.11044741},
	timestamp = {Tue, 05 Aug 2025 22:40:40 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LinDYJY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum network research at both the software stack and hardware implementation level has become an exciting area of quantum information science. Although demonstrations of small-scale quantum networks have emerged in the past decade, quantum communication and computation hardware remain scarce resources today. As a result, the evaluation and validation of quantum network protocols primarily rely on classical simulators rather than real quantum networks. This paper introduces a novel quantum network simulation method called control flow adaptation, which enhances standard tensor network simulations. This method enables accurate and efficient simulations of many important quantum network protocols by carefully leveraging the control flow structures of them. Furthermore, we have developed a prototype quantum network simulator, qns-3, as a module for ns-3. This new module implements the control flow adaptation technique and applies it to a wide range of protocols. All related code is open-source [1]. We believe the development of the control flow adaptation method and the qns-3 simulator represents a step forward in quantum network research, offering a versatile and scalable platform for testing and verifying quantum network protocols on classical computers.}
}


@inproceedings{DBLP:conf/infocom/TalukderX25,
	author = {Munmun Talukder and
                  Jiang (Linda) Xie},
	title = {Safeguarding WiFi 7 and Beyond: Tackling Protocol-Aware Jamming in
                  Multi-AP Coordination},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044739},
	doi = {10.1109/INFOCOM55648.2025.11044739},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TalukderX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access point (AP) coordination (MAP-Co) feature of WiFi 7 and beyond has recently gained significant attention due to its capability to facilitate concurrent transmissions of multiple APs by sharing radio resources among them. However, the security implications of MAP-Co are not studied yet. Existing research predominantly focuses on maximizing the number of APs involved in concurrent transmissions. This paper is the first to shed light on the vulnerabilities associated with MAP-Co control frames and demonstrate how a perpetrator can exploit these vulnerabilities to degrade MAP-Co performance and even completely jam the transmissions of a particular AP. We propose two novel intelligent jamming attacks, MTF Turmoil and Precision Jamming attacks, and necessary countermeasures to protect MAP-Co against these attacks. Furthermore, we propose analytical models to analyze the MAP-Co performance in scenarios with no attack, under attack, and under attack with countermeasures, and validate them against simulations. Mathematical analysis and extensive simulation results highlight the severity of our proposed attacks and demonstrate the superior performance of our proposed countermeasures.}
}


@inproceedings{DBLP:conf/infocom/ZengCWZ0025,
	author = {Hao Zeng and
                  Helei Cui and
                  Cong Wang and
                  Bo Zhang and
                  Zhiwen Yu and
                  Bin Guo},
	title = {SenFEED: Dynamic Decentralized Oracle Services for Accurate and Real-Time
                  Sensor Data},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044714},
	doi = {10.1109/INFOCOM55648.2025.11044714},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZengCWZ0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart contracts, as self-executing programs on blockchains, have been applied in many Internet of Things (IoT) systems to achieve decentralized, fair, and auditable transaction services. However, since smart contracts cannot natively fetch real-world data, this limitation hinders the full economic potential of smart contracts in IoT. A promising solution is to leverage oracle services (also known as data feeds), which enable smart contracts to access and interact with external data sources. Nonetheless, these services must contend with the streaming nature of sensor data in IoT and the dynamic joining or leaving of service nodes in the blockchain. To address these challenges, we propose dynamic decentralized oracle services called SenFEED. Specifically, we integrate iterative-based and incremental-based truth discovery algorithms to achieve accurate and real-time truth inference on IoT streaming data. Furthermore, to address the challenge of reaching consensus in a decentralized setting, we design a new Byzantine fault-tolerant protocol that allows service nodes to join or leave without requiring a system reset. Theoretical analysis and extensive experiments confirm the effectiveness of SenFEED, demonstrating significant improvements in scenarios with a larger number of nodes and robust support for data streams in IoT systems.}
}


@inproceedings{DBLP:conf/infocom/Zhu0LLLZLLLJ25,
	author = {Xiaojun Zhu and
                  Jiawei Huang and
                  Haifeng Liu and
                  Zhaoyi Li and
                  Yijun Li and
                  Shengwen Zhou and
                  Hui Li and
                  Weihe Li and
                  Jingling Liu and
                  Wanchun Jiang},
	title = {{DACC:} Data Augmentation for Learning-based Congestion Control},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044755},
	doi = {10.1109/INFOCOM55648.2025.11044755},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhu0LLLZLLLJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) stands as a powerful tool for advancing congestion control algorithms (CCAs). However, the contradiction between the substantial number of samples required for training and the cost of collecting samples from real networks, hurts the effectiveness of training models, especially for deep reinforcement learning (DRL). Through experimental measurements, we reveal that it is difficult for existing DRL-based CCAs to train high-quality models with insufficient samples. To address this problem, we propose a Data Augmentation framework for learning-based Congestion Control (DACC). To augment samples in a lightweight yet effective way, DACC selects high-quality policies to generate actions and employs a backward-forward state model to compute the next state. An extensive set of experiments in both real-world Internet and emulated networks demonstrate that DACC reduces latency by 24.41%, and decreases packet loss rate by 73.29% compared to the state-of-the-art learning-based CCA.}
}


@inproceedings{DBLP:conf/infocom/AslanAGCI25,
	author = {Fatih Aslan and
                  Jose A. Ayala{-}Romero and
                  Andres Garcia{-}Saavedra and
                  Xavier Costa{-}P{\'{e}}rez and
                  George Iosifidis},
	title = {FairRIC: Real-Time Fair Allocation in {O-RAN} with Shared Computing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044643},
	doi = {10.1109/INFOCOM55648.2025.11044643},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/AslanAGCI25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of O-RAN systems on general-purpose computing platforms represents a significant paradigm shift, promising remarkable performance improvements. However, these architectures may potentially increase both the capital and operational expenses of the network. The processor pooling concept is a promising solution to address this problem, consisting of a set of processing units (PUs) in the O-Cloud shared by several virtualized BSs (vBSs). Nevertheless, this strategy requires sophisticated resource assignment mechanisms to provide the expected gains in terms of cost and reliability. This paper proposes a novel online learning framework that assigns computing resources to vBSs in real-time (e.g., every TTI), thus handling the burstiness of real traffic loads. Our algorithm relies on online convex optimization (OCO) theory, extending state-of-the-art approaches in long-term fairness and constrained optimization and allowing discrete decisions. Our method offers an intrinsic closed-form iteration, speeding up the computation process and consequently allowing real-time operation. Moreover, our solution has guarantees in terms of fairness among the vBSs while adhering to long-term energy constraints over the entire operation horizon. We validate our theoretical findings via simulation and evaluate experimentally the algorithms in an O-RAN platform.}
}


@inproceedings{DBLP:conf/infocom/LinLCLCZ25,
	author = {Wei Lin and
                  Zongxiao Li and
                  Binbin Chen and
                  Jianwei Liu and
                  Ray{-}Guang Cheng and
                  Fan Zhang},
	title = {5G-Muffler: Covert DoS Attacks over Open Fronthaul Interface of {O-RAN}
                  5G Network},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044568},
	doi = {10.1109/INFOCOM55648.2025.11044568},
	timestamp = {Tue, 05 Aug 2025 22:40:40 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LinLCLCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The O-RAN ALLIANCE promotes an open Radio Access Network (RAN) vendor ecosystem by defining O-RAN architecture and interfaces. The open fronthaul (O-FH) interface is a crucial interface between the O-RAN Radio Unit (O-RU) and the O-RAN Distributed Unit (O-DU), allowing mobile network operators to select best-of-breed O-RUs among multiple vendors. However, O-FH also introduces new security risk. In this work, we present 5G-Muffler, a set of covert DoS attacks over the O-FH interface. 5G-Muffler disrupts the random access process, the initial step for a user equipment (UE) to connect to the network. By preventing UEs from completing this step, 5G-Muffler makes the 5G network inaccessible. Furthermore, the attack is invisible to anomaly detection mechanisms above PHY layer. Our first variant, 5G-Muffler-1, introduces a man-in-the-middle device between O-RU and O-DU by manipulating an O-FH switch's configurations. The second variant, 5G-Muffler-2, targets the common 5G shared-cell setup, where a cell uses multiple O-RUs to enhance its coverage and signal quality. 5G-Muffler-2 only needs to control a single O-RU and it leverages weakness in the shared-cell signal aggregation process to amplify the attack to the whole cell. We demonstrate 5G-Muffler on commercial O-RAN systems and propose countermeasures to mitigate these attacks.}
}


@inproceedings{DBLP:conf/infocom/ZhangZJCDNY25,
	author = {Xinchen Zhang and
                  Running Zhao and
                  Zhihan Jiang and
                  Handi Chen and
                  Yulong Ding and
                  Edith C. H. Ngai and
                  Shuang{-}Hua Yang},
	title = {Continual Learning with Strategic Selection and Forgetting for Network
                  Intrusion Detection},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044615},
	doi = {10.1109/INFOCOM55648.2025.11044615},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangZJCDNY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intrusion Detection Systems (IDS) are crucial for safeguarding digital infrastructure. In dynamic network environments, both threat landscapes and normal operational behaviors are constantly changing, resulting in concept drift. While continuous learning mitigates the adverse effects of concept drift, insufficient attention to drift patterns and excessive preservation of outdated knowledge can still hinder the IDS's adaptability. In this paper, we propose SSF (Strategic Selection and Forgetting), a novel continual learning method for IDS, providing continuous model updates with a constantly refreshed memory buffer. Our approach features a strategic sample selection algorithm to select representative new samples and a strategic forgetting mechanism to drop outdated samples. The proposed strategic sample selection algorithm prioritizes new samples that cause the ‘drifted’ pattern, enabling the model to better understand the evolving landscape. Additionally, we introduce strategic forgetting upon detecting significant drift by discarding outdated samples to free up memory, allowing the incorporation of more recent data. SSF captures evolving patterns effectively and ensures the model is aligned with the change of data patterns, significantly enhancing the IDS's adaptability to concept drift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15 datasets demonstrates its superior adaptability to concept drift for network intrusion detection.}
}


@inproceedings{DBLP:conf/infocom/GaoZCHQYXH25,
	author = {Ming Gao and
                  Lingfeng Zhang and
                  Yike Chen and
                  Sifeng He and
                  Feng Qian and
                  Lei Yang and
                  Fu Xiao and
                  Jinsong Han},
	title = {Exploring Acoustic Reverse Nonlinearity Against Speech Forgery in
                  Real-Time Voice Applications},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044526},
	doi = {10.1109/INFOCOM55648.2025.11044526},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GaoZCHQYXH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unauthorized editing of speech recordings poses a significant threat to the security and authenticity of speeches, particularly in the forensic and legal fields. Even worse, the speech is increasingly at risk of being tampered with due to the development of AI techniques (e.g., Audio Deepfake). It is difficult for normal users to guarantee what they say has not been illegally changed. Audio watermark techniques are recognized as an active method against speech forgery. However, such techniques suffer from audio quality degradation and non-real-time insertion. Therefore, they cannot be adopted into real-time voice applications against forgery on remote recordings, e.g., phone calls, live broadcasts, and online meetings. Fortunately, high-definition (HD) audio techniques provide ultrasonic bands without distortion. Therefore, ultrasonic creditable factors can be utilized. We propose an audio tamper-proof system, named Aegis. It provides commodity mobile devices (e.g., smartphones) with an effective method of real-time insertion of inaudible creditable factors. Users can claim that audio with no or mismatched ultrasound is invalid and illegal. In particular, we explore the acoustic reverse-nonlinear phenomenon where audible signals can be modulated onto the ultrasonic spectrum. By emphasizing the correlation between speech signals and ultrasound, we realize effective defense against various tampering methods.}
}


@inproceedings{DBLP:conf/infocom/RenLZHGX25,
	author = {Neng Ren and
                  Yanbiao Li and
                  Chunyang Zhang and
                  Jing Hu and
                  Lingbo Guo and
                  Gaogang Xie},
	title = {A Balanced Tuple Partitioning Method for Packet Classification with
                  High-Performance and Scalability},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044630},
	doi = {10.1109/INFOCOM55648.2025.11044630},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/RenLZHGX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The scalability of packet classification and dynamic rule updates in SDNINFV-driven networks remains a critical challenge as network sizes expand. To address this, we propose Balanced Tuple Partitioning (BTP), a novel two-phase optimization framework based on the Tuple Space Search (TSS) algorithm. BTP introduces (1) tuple merging-based partition, which strategically consolidates overlapping tuples to reduce rule-set complexity, and (2) conflict-aware tuple chaining, a lightweight mechanism to resolve post-merging conflicts while ensuring rule integrity. We formally prove the correctness of BTP through a analysis of tuple space efficiency (reducing search iterations) and rule space consistency (maintaining se-mantic equivalence). Experimental results demonstrate that BTP achieves 2.2x, 3.3x, and 1.8x higher classification throughput on average than DynamicTuple, TupleTree, and TupleChain, respectively. Furthermore, BTP sustains high performance under extreme scalability demands, supporting rule-sets of 10 million rules and dozens of match fields.}
}


@inproceedings{DBLP:conf/infocom/Ji00C0W25,
	author = {Kunhong Ji and
                  Chi Lin and
                  Jie Xiong and
                  Liming Chen and
                  Xin Fan and
                  Guowei Wu},
	title = {LiDAR-Track: Multi-Person Positioning and Tracking Using LiDAR},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044580},
	doi = {10.1109/INFOCOM55648.2025.11044580},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Ji00C0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indoor multi-person positioning and trajectory tracking are crucial for applications such as smart homes, smart healthcare, and industrial security. Existing solutions suffer from low accuracy and privacy concerns, particularly in complex scenarios involving occlusions and dynamic interference. In this paper, we develop LiDAR-Track, a positioning and tracking system leveraging 2D LiDAR technology to address these issues. We propose a robust human detection and positioning method to handle occlusions and incomplete raw LiDAR readings. We also develop an efficient trajectory segmentation algorithm that leverages temporal and spatial characteristics for reliable multi-person tracking. Extensive real-world experiments demonstrate the superior performance of LiDAR-Track, achieving average positioning and single-user tracking errors of 3.01 cm and 3.05 cm, respectively. Even when tracking seven users in a complex indoor environment, the error remains within 6.55 cm.}
}


@inproceedings{DBLP:conf/infocom/LiWH0JL25,
	author = {Xin Li and
                  Hongbo Wang and
                  Jingzhi Hu and
                  Zhe Chen and
                  Zhiping Jiang and
                  Jun Luo},
	title = {CCS-Fi: Widening Wi-Fi Sensing Bandwidth via Compressive Channel Sampling},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044653},
	doi = {10.1109/INFOCOM55648.2025.11044653},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiWH0JL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enabling multi-person differentiation is crucial for the wide adoption of Wi-Fi sensing, so it is imperative for Wi-Fi sensing to gain GHz-level bandwidth and thus to achieve sufficient spatial resolution. Whereas stitching wide bandwidth leveraging continuous channel samples appears to be plausible, it is inefficient (if not impossible) in both time and frequency domains. Fortunately, as physical phenomena to be sensed are often sparse, acquiring GHz-bandwidth from sparse channel samples can be feasible. To this end, we propose CCS- Fi as a novel scheme to widen Wi-Fi sensing bandwidth, exploiting sparse and irregular channel samples. We start with establishing a compressive sensing framework to analyze the potential of realizing GHz sensing capability with only sparse channel samples. Then we propose a model-driven deep learning strategy to implement the sparse recovery process, aiming to derive radar-like channel response as direct output while overcoming the impossibility of obtaining labels for training. Through comprehensive experimental evaluations, we demonstrate that CCS- Fi achieves centimeter-level resolution, effectively enabling indoor multi-person sensing.}
}


@inproceedings{DBLP:conf/infocom/FuLLC0ZD25,
	author = {Chengxuan Fu and
                  Jia Liu and
                  Xiaoyu Li and
                  Xingyu Chen and
                  Xuan Liu and
                  Shigeng Zhang and
                  Junzhao Du},
	title = {Exploring the Frontiers of {RFID} Coverage Capacity: Theoretical and
                  Practical Perspectives},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044668},
	doi = {10.1109/INFOCOM55648.2025.11044668},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FuLLC0ZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio Frequency Identification (RFID) technology, pivotal in sectors such as retail, logistics, healthcare, and security, faces a fundamental challenge in maximizing the efficiency of large-scale systems-specifically, the coverage capacity of RFID readers. This paper addresses the under-investigated problem of RFID coverage capacity by merging theoretical analysis and practical implementation to optimize reader deployment. By incorporating tag layout factors along with the link loss budget, we construct a theoretical model of coverage capacity that reflects the maximum number of tags a reader can effectively cover. Additionally, we develop a cutoff-power-based solution to bridge the gap between theoretical limits and practical application, without requiring any predetermined system parameters. A prototype is built upon commercial RFID systems and experimental results demonstrate a significant improvement in coverage capability.}
}


@inproceedings{DBLP:conf/infocom/YangHHYC25,
	author = {Tao Yang and
                  Ling Hu and
                  Bingnan Hou and
                  Zhenzhong Yang and
                  Zhiping Cai},
	title = {Pruning as Scanning: Towards Internet-Wide IPv6 Network Periphery
                  Discovery},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044733},
	doi = {10.1109/INFOCOM55648.2025.11044733},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangHHYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IPv6 network peripheries serve as the last-hop routing devices connecting customers, making the rapid discovery of periphery addresses crucial for Internet security and network measurement. Traditional IPv6 scanning methods, based on target generation algorithms (TGAs), often fall short due to mismatches between the patterns identified by TGAs and those inherent in IPv6 periphery addresses. To address this, we introduce Pruning-as-Scanning (PaS), a novel IPv6 scanning approach designed for fast global IPv6 periphery address discovery. Unlike previous methods, PaS initiates scanning from the entire IPv6 address space without relying on seed datasets. It systematically probes IPv6 Internet prefixes from the shortest to the longest, similar to a hierarchical search, while discarding those irrelevant prefixes (and sub-prefixes), i.e., space pruning. This reduces the complexity of searching the entire IPv6 address space, allowing comprehensive measurement of IPv6 endpoint subnets where periphery devices are installed. We implemented an asynchronous scanner prototype based on this principle. Real-world tests demonstrated that PaS outperformed existing methods in both probing scale and the number of identified periphery addresses. Our approach achieved the first comprehensive, global-scale IPv6 network periphery discovery, uncovering over 712 million IPv6 periphery addresses using 33 billion probe packets, averaging 4 million addresses per hour at an uplink of 50 kpps, significantly expanding the IPv6 address corpus.}
}


@inproceedings{DBLP:conf/infocom/QiuZWSC25,
	author = {Wenqi Qiu and
                  Yipeng Zhou and
                  Jinzhi Wang and
                  Quan Z. Sheng and
                  Laizhong Cui},
	title = {FLM-TopK: Expediting Federated Large Language Model Tuning by Sparsifying
                  Intervalized Gradients},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044514},
	doi = {10.1109/INFOCOM55648.2025.11044514},
	timestamp = {Tue, 22 Jul 2025 13:38:08 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QiuZWSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The past few years have witnessed the unprecedented capability of large language models (LLMs). To adapt LLMs with various downstream tasks, fine-tuning methods, e.g., Low-Rank Adaptation (LoRA), are proposed to efficiently tune LLMs. Meanwhile, federated LLM tuning emerges for refining LLMs with clients owning private data. In the federated tuning process, the server and clients frequently exchange fine-tune gradients via Internet, giving the rise of the communication challenge. To overcome this challenge, most existing works employ quantization methods for compressing gradients because sparsification methods like TopK incur heavy overhead for transmitting position IDs (PIDs) of sparsified gradients. In this work, to expedite federated LLM tuning with a higher compression rate, we design the Federated LLM Tuning with TopK (FLM-TopK) algorithm. Specifically, FLM-TopK intervalizes gradients before compression. Then, TopK is separately applied for gradients in each interval so that the overhead representing PIDs is constrained. To optimize our algorithm, we empirically study the distribution of gradients, which obeys the Gaussian distribution. Based on the Gaussian distribution, we establish an optimization problem to minimize the compression error by jointly optimizing the interval size and the sparsification rate per interval. We prove that the non-convex problem can be approximately solved by alternating optimization. To demonstrate the superiority of FLM-TopK, we conduct extensive experiments on nine public datasets. The results demonstrate that FLM-TopK significantly outperforms SOTA baselines, achieving 6.42%-18.87% improvement in accuracy and 17.07%-44.44% reduction in communication traffic.}
}


@inproceedings{DBLP:conf/infocom/0006WLGZLMLG25,
	author = {Jianfeng Li and
                  Dongliang Wang and
                  Yixuan Liu and
                  Yifei Gao and
                  Xiaorong Zhang and
                  Zheng Lin and
                  Xiaobo Ma and
                  Xiapu Luo and
                  Xiaohong Guan},
	title = {Cross-Environmental Website Fingerprinting},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044569},
	doi = {10.1109/INFOCOM55648.2025.11044569},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0006WLGZLMLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite the widespread adoption of encryption, such as TLS, encrypted proxies, and Tor, website fingerprinting (WF) has long been proven to be able to recognize web sites from encrypted traffic. However, existing WF methods were generally developed and evaluated under the implicit assumption that traffic samples for training and recognition are captured in the same environment. When applied to diverse environments affected by practical factors, such as various browsers and proxy software, they will be hampered by three-fold challenges: i) feature drift, ii) sampling dilemma, and iii) few-shot generalization. None of existing WF methods can fully address them. In this paper, we take the first step to cross-environmental WF and advance a systematic framework, dubbed X-EPRINT, to tackle the above challenges. X - EPRINT generates cross-environmentally invariant features to address feature drift. It mitigates sampling dilemma via potential-aware traffic resampling. X-EPRINT capitalizes on inter-flow data augmentation to solve few-shot generalization. We conduct extensive experiments to evaluate X-EPRINT. The experimental results demonstrate that X - EPRINT achieves a robust performance in zero-shot cross-environmental recognition, with an F1-score of 0.719, which is 58.4% higher than the top-performing baseline method. It also attains an F1-score of 0.925 in 3-shot recognition, fulfilling few-shot environment adaptation.}
}


@inproceedings{DBLP:conf/infocom/YangSYZLCW25,
	author = {Bin Yang and
                  Dian Shen and
                  Hanlin Yang and
                  Lunqi Zhao and
                  Jianrui Liu and
                  Jiantao Cheng and
                  Beilun Wang},
	title = {HyperCom: Enabling High Performance and Composable Data Structures
                  for Software Network Functions with eBPF},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044519},
	doi = {10.1109/INFOCOM55648.2025.11044519},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangSYZLCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The extended Berkeley Packet Filter (eBPF) has become a crucial component for modern network infrastructure, where the data structures, play a fundamental role. However, directly implementing these data structures with eBPF poses two main challenges: (1) unable to implement a full set of required functions due to limited programmability; (2) degraded performance caused by a restricted instruction set and runtime environment. This paper presents HyperCom, a framework that enables high-performance, comprehensive and flexible data structures with eBPF. At its core, HyperCom decouples one single data structure into two separate yet composable parts: base-complex but structurally stable across different network functions, and extension-simple yet functionally dynamic, capturing individual network functions, and applies right techniques to implement each of them. Specifically, HyperCom designs non-invasive and invasive extensions to support a flexible combination of base and extension. This approach achieves high performance by employing advanced hardware instructions for the base part, while providing flexibility by leveraging eBPF to fully implement the simplified extension part despite limited programmability. Furthermore, HyperCom leverages RUST to implement base to ensure safety. Evaluations on real-world network functions show that HyperCom-based ones outperform their eBPF-based counterparts by up to 2.5 ×in terms of packet processing rate, without compromising flexibility.}
}


@inproceedings{DBLP:conf/infocom/YaoL0ZWL25,
	author = {Zhiyi Yao and
                  Zuning Liang and
                  Yuedong Xu and
                  Jin Zhao and
                  Jessie Hui Wang and
                  Tong Li},
	title = {MemFerry: {A} Fast and Memory Efficient Offload Training Framework
                  with Hybrid {GPU} Computation},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044593},
	doi = {10.1109/INFOCOM55648.2025.11044593},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YaoL0ZWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the ever-growing size of deep learning models, GPU memory is prone to be insufficient during training. A prominent approach is ZeRO-Offload which moves the optimizer states to CPU memory and performs parameter update using CPU. However, the deficiencies of ZeRO-Offload include low GPU utilization, imperfect overlapping of communication and computation, and inflexible offloading. In this paper, we leverage Direct Host Access (DHA) in GPU that can compute data on CPU memory to form a novel hybrid on-GPU and DHA. We design and implement MemFerry consisting of an execution scheduler and a shadow model. The scheduler strategically chooses layers of parameters for DHA computation and transmits the remaining parameters to GPU memory simultaneously to shorten forward propagation time, and further loads DHA parameters to GPU memory for reducing backward propagation time. The shadow model presents a unified memory abstraction for the parameter partitions stored separately in GPU and CPU memories. To further reduce GPU memory usage, we present GO-MemFerry along with its dynamic programming algorithm that offloads gradients to CPU memory via DHA. Our experiments show that MemFerry trains up to 1.68x faster and GO-MemFerry could train 1.52 x larger model compared to ZeRO-Offload on a single GPU, and increase training speed by at least 28.1 % when scaling to data parallelism on 8 GPUs.}
}


@inproceedings{DBLP:conf/infocom/Jia0F0LW25,
	author = {Zifan Jia and
                  Qingsong Liu and
                  Haihui Fan and
                  Xiaoyan Gu and
                  Bo Li and
                  Weiping Wang},
	title = {Learning to Optimize Resource Utilization with QoS Guarantees},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044609},
	doi = {10.1109/INFOCOM55648.2025.11044609},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Jia0F0LW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resource allocation optimization is crucial in cloud computing platforms, which need to support a heterogeneous set of users sharing the same physical computing resource. Without prior knowledge of fluctuating user demands, existing methods often result in inefficient resource utilization (low system utility) or fail to meet Quality of Service (QoS) guarantees for users. Motivated by this, we present a feedback-limited online resource allocation model for a divisible resource shared among multiple users, each with specific QoS requirements and fluctuating demands. Using only binary feedback on the queue status of each user, we propose an efficient online algorithm that balances resource utilization and adherence to users' QoS requirements. Our algorithm ensures nearly optimal resource utilization, even when compared to the omniscient offline dynamic optimum. Also, it concurrently meets all individual users' QoS requirements with minimal error. The core algorithmic technique involves a multiplicative weight update strategy and a primal-dual approach to secure these guarantees. Furthermore, we present numerical results to validate the effectiveness of the algorithm.}
}


@inproceedings{DBLP:conf/infocom/Huang0GY25,
	author = {Zhaowu Huang and
                  Fang Dong and
                  Xiaolin Guo and
                  Daheng Yin},
	title = {FaSei: Fast Serverless Edge Inference with Synergistic Lazy Loading
                  and Layer-wise Caching},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044642},
	doi = {10.1109/INFOCOM55648.2025.11044642},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Huang0GY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serverless edge computing (SEC) provides low-latency, resource-efficient deep learning (DL) services but faces a significant cold start time due to the loading of large DL models. Existing methods for cold starts include full and partial container caching. The former may be inefficient because large DL models cannot be cached in the resource-limited SEC; The latter only caches common packages for sharing, while user-specified DL models still need to be loaded before execution. We identify model lazy loading to mitigate cold start, which begins inference with a shallow model while lazily loading deeper layers in a pipeline manner. However, naively using lazy loading can result in significant inference bubbles because model loading time is typically much longer than inference time. To address this, we propose FaSei, a fast serverless edge inference method with synergistic model lazy loading and layer-wise caching, to reduce application completion time (ACT). Considering the impact of heterogeneous model-layer behaviors and SEC resources on ACT, we jointly optimize lazy loading, layer-wise caching, and function placement. We formulate it as an integer nonlinear programming problem and then design an approximation algorithm with a theoretical performance guarantee. Extensive experiments demonstrate that FaSei achieves up to  6.7 × 6.7\\times  speedup in reducing ACT.}
}


@inproceedings{DBLP:conf/infocom/WangSZ025,
	author = {Zheng Wang and
                  Xiaoqi Sun and
                  Yuanqing Zheng and
                  Yanwen Wang},
	title = {Push the Limit of Acoustic Indoor Fire Monitoring},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044751},
	doi = {10.1109/INFOCOM55648.2025.11044751},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangSZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In indoor fire rescue, swift and precise fire source localization and fire severity assessment are pivotal for firefighting strategic planning and casualty evacuation. However, existing solutions primarily focus on detecting fire presence, which do not offer insights into fire's localization and severity. In this paper, we propose UltraFlame, an accurate, user-friendly, and timely system for pinpointing fire sources and assessing fire severity based on acoustic sensing, which bridges significant gaps in fire safety and response. UltraFlame consists of a collocated commodity speaker and microphone pair, sensing fire by emitting inaudible sound waves. We conduct an in-depth investigation of sound propagation impacted by fire combustion, providing physically interpretable data for deep learning framework and enabling fire source localization even without any sound reflection by fire. We dedicatedly establish a correlation between fire severity and sound propagation delays, which serves as an effective indicator for estimating the heated region. Finally, an appropriate deep learning framework is employed to effectively extract temporal and spatial features from channel measurement. Extensive experiments demonstrate that 94% of the localization results have an error of less than 0.8m. Additionally, UltraFlame achieves an accuracy of 96.9% in fire severity assessment across diverse setups, providing real-time and reliable monitoring.}
}


@inproceedings{DBLP:conf/infocom/NarasimhaNSP25,
	author = {Dheeraj Narasimha and
                  Srinivas Nomula and
                  Srinivas Shakkottai and
                  Parimal Parag},
	title = {The Power of Two in Large Service-Marketplaces},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044602},
	doi = {10.1109/INFOCOM55648.2025.11044602},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/NarasimhaNSP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a large-scale service marketplace with numerous servers that scale with the job arrival rate. Jobs arrive with private valuations representing their willingness to pay. In a centralized system, jobs are matched to available servers, and prices are set in a centralized manner to maximize revenue. We investigate whether similar scalability can be achieved in a distributed marketplace where jobs are randomly matched to servers, which set their own prices based on job valuations and system occupancy. Our results show that matching a job to a single randomly selected server leads to increasing prices with the arrival rate, resulting in high blocking probability and reduced revenue. We then examine matching jobs to two servers, which compete to provide service if unoccupied. We demonstrate the existence and convergence to a mean field equilibrium (MFE) in this setup, where servers strategically respond to competitors' prices. We characterize the MFE and show that this two-server choice mitigates price scaling with the arrival rate, ensuring lower blocking probabilities and higher system revenue. Our findings are validated through simulations illustrating a variety of operating scenarios.}
}


@inproceedings{DBLP:conf/infocom/HuDCHH0025,
	author = {Yi Hu and
                  Haonan Ding and
                  Haoxuan Chen and
                  Jianwen He and
                  Menglan Hu and
                  Chao Cai and
                  Kai Peng},
	title = {Collaborative Orchestration with Probabilistic Routing for Dynamic
                  Service Mesh in Clouds},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044542},
	doi = {10.1109/INFOCOM55648.2025.11044542},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuDCHH0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Service mesh architectures decouple Internet applications into lightweight and modular microservices for flexible development. However, in large-scale microservice systems, frequent data communications, intricate call dependencies, and stringent Service Level Objective (SLO) requirements bring severe difficulties to ingenious service mesh orchestration. To adapt to time-varying and stochastic properties of concurrent requests with various SLOs and service multiplexing, the orchestration scheme is dynamically adjusted. In this case, the problems of multi-instance microservice deployment and request routing in service mesh architectures are tightly-coupled, and cannot be well optimized individually, further enlarging the challenge to collaborative orchestration in dynamic scenarios. Nevertheless, most existing work did not propose the fine-grained models and methods for the above challenges. Therefore, this paper studies the dynamic service mesh orchestration with probabilistic routing for tree-like microservice graphs in clouds. First, the joint optimization problem is approximately submodular, and we construct a multi-instance model based on open Jackson queuing networks to accurately capture complex dependencies and analyze full response latency. To improve the overall performance, we propose an efficient cardinal-constrained algorithm to implement topology-aware microservice autoscaling and migration, and adaptive probabilistic routing. Finally, through extensive trace-driven experiments, our method is validated to be superior to other baselines.}
}


@inproceedings{DBLP:conf/infocom/XiePZ25,
	author = {Lingnan Xie and
                  Linning Peng and
                  Junqing Zhang},
	title = {Towards Robust {RF} Fingerprint Identification Using Spectral Regrowth
                  and Carrier Frequency Offset},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044651},
	doi = {10.1109/INFOCOM55648.2025.11044651},
	timestamp = {Tue, 05 Aug 2025 22:40:41 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XiePZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio frequency fingerprint identification (RFFI) is a promising device authentication approach by exploiting the unique hardware impairments as device identifiers. Because the hardware features are extracted from the received waveform, they are twisted with the channel propagation effect. Hence, channel elimination is critical for a robust RFFI system. In this paper, we designed a channel-robust RFFI scheme for IEEE 802.11 devices based on spectral regrowth and proposed a carrier frequency offset (CFO)-assisted collaborative identification mechanism. In particular, the spectral regrowth was utilized as a channel-resilient RFF representation which is rooted in the power amplifier nonlinearity. While CFO is time-varying and cannot be used alone as a reliable feature, we used CFO as an auxiliary feature to adjust the deep learning-based inference. Finally, a collaborative identification was adopted to leverage the diversity in a multi-antenna receiver. Extensive experimental evaluations were performed in practical environments using 10 IEEE 802.11 devices and a universal software radio peripheral (USRP) X310 receiver with 4 antennas. The results demonstrated the effectiveness of the proposed method against diverse channel conditions and CFO drift, where an average classification accuracy of 92.76% was achieved against channel variations and a 5-month time span, significantly outperforming existing methods.}
}


@inproceedings{DBLP:conf/infocom/WangWL25,
	author = {Tianxin Wang and
                  Xudong Wang and
                  Geoffrey Ye Li},
	title = {GraphRx: Graph-Based Collaborative Learning Among Multiple Cells for
                  Uplink Neural Receivers},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044726},
	doi = {10.1109/INFOCOM55648.2025.11044726},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A pre-trained neural receiver does not perform well in all channel environments, so online retraining is necessary. To acquire channel knowledge efficiently, collaborative learning among multiple neural receivers is indispensable. To this end, a graph-based collaborative learning scheme called GraphRx is developed to retrain uplink neural receivers collaboratively among base stations (BSs). First, considering a collaboration graph among BSs, GraphRx is formulated as a personalized federated learning problem, wherein the graph weights and neural receiver models are learned together so that generalization and personalization are jointly optimized. Second, the problem is solved through an alternating approach under the federated learning paradigm. Particularly, an approximate generalization bound is derived to enable graph optimization at the server without accessing local data on BSs. To reduce overhead of training pilots, data augmentation is employed. GraphRx is evaluated via extensive simulation. Key parameters of GraphRx are first found through ablation study. Next, the effectiveness of the approximation in the generation bound is validated. Comparisons with the state-of-the-art schemes are finally conducted. Results show that, given the same coded bit error rate, GraphRx achieves a SNR gain of 0.4 ~ 0.9 dB and 0.5 ~ 2.1 dB for the cases without and with inter-cell interference, respectively.}
}


@inproceedings{DBLP:conf/infocom/AdhikariMMKFVCD25,
	author = {Abhishek Adhikari and
                  Shivan Mukherjee and
                  Aahan Mehta and
                  Manav Kohli and
                  Rodolfo Feick and
                  Reinaldo A. Valenzuela and
                  Dmitry Chizhik and
                  Jinfeng Du and
                  Gil Zussman},
	title = {Around-Corner and Over-Top 28 GHz Measurement in Manhattan: Path Loss
                  and AoA for {MU-MIMO}},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044468},
	doi = {10.1109/INFOCOM55648.2025.11044468},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/AdhikariMMKFVCD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents findings from an extensive 28 GHz mmWave measurement campaign conducted in New York City. The study includes over 20 million power measurements collected from two key scenarios: around-corner (non-line-of-sight due to building blockages) and same-street (nominally line-of-sight without obstructions from street furniture or foliage), covering over 1,300 unique links. For urban macro-cell (UMa) rooftop base stations above local clutter, the dominant angle of arrival (AoA) deviates by only 2 to 3.5 degrees from the direct transmitter/receiver direction. This small deviation allows for effective spatial separation between users, facilitating the future development of Multi-User MIMO algorithms for Beyond-5G networks. In the urban micro-cell (UMi) dataset, with base stations below local clutter, a path gain drop of over 20 dB was observed in around-corner segments just 20 meters into a corner. Our Street-Clutter-NLOS path loss model achieves an RMSE of 6.4 dB, compared to 11.9 dB from NLOS 3GPP models. Using the best path loss model to estimate coverage for 90 % of users traveling around corners, downlink rates could drop by over 10 times after 50 meters, highlighting the challenges in maintaining consistent user experience over mmWave networks in urban street canyons.}
}


@inproceedings{DBLP:conf/infocom/ZhangD0YQ25,
	author = {Shengyu Zhang and
                  Songshi Dou and
                  Zhenglong Li and
                  Kwan L. Yeung and
                  Tony Q. S. Quek},
	title = {Oracle: QoS-Aware Online Service Provisioning in Non-Terrestrial Networks
                  with Safe Transfer Learning},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044634},
	doi = {10.1109/INFOCOM55648.2025.11044634},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangD0YQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging mega-constellations consisting of numerous Low Earth Orbit (LEO) satellites are actively providing pervasive Internet services worldwide, which are usually considered crucial components of Non-Terrestrial Networks (NTNs). However, the high mobility and limited coverage of LEO satellites can introduce frequent handovers, causing network interruptions and degrading Quality of Service (QoS). While many efforts have been made to alleviate the impact of handovers on service provisioning from NTNs, they usually assume channel conditions are pre-determined and remain unchanged as satellites move, which is different from real situations and thus may experience significant performance degradation compared to theoretical analysis. In this paper, we propose Oracle to promise QoS-aware service provisioning in NTNs under dynamic channel conditions. Specifically, we mathematically formulate a channel model to characterize channel conditions in NTNs and develop a QoS maximization problem considering handover frequency and transmission capacity. To accommodate the dynamic nature of NTNs, we introduce a Model Predictive Control (MPC)-based controller to predict future dynamic network status and generate control strategies correspondingly, and leverage Digital Twin (DT) for real-time network status consideration. For higher efficiency, we further employ Generative Artificial Intelligence (GAI) with a safe transfer learning-based framework to enhance model adaptivity to environmental uncertainties and ensure feasible control decisions in real-world NTNs. Extensive simulation results under real-world constellation demonstrate that Oracle can enhance up to  3 × 3\\times  QoS during service provisioning compared with baseline approaches.}
}


@inproceedings{DBLP:conf/infocom/Liu0Z025,
	author = {Haibo Liu and
                  Da Huo and
                  Zhenzhe Zheng and
                  Fan Wu},
	title = {Latency-Aware Online Continual Learning for Non-Stationary Data Streams},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044660},
	doi = {10.1109/INFOCOM55648.2025.11044660},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Liu0Z025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online continual learning (CL) is beneficial for learning incrementally from continuous data streams without forgetting previously learned knowledge. However, current online CL approaches have overlooked the time cost of online data collection and model adaptation, resulting in a high-latency service response, especially in high-velocity non-stationary data streams. In this work, we aim to realize latency-aware online CL for non-stationary data streams, and propose a two-stage time-scale optimization for online data collection and model adaptation. In the first stage with uncertain data arrivals, we propose an optimal stopping algorithm with a logarithmic regret bound to make an irrevocable decision on when to stop data collection. To minimize the training time of model adaptation for stability-plasticity trade-off in the second stage, we introduce a bidirectional data selection algorithm with a logarithmic approximation, to greedily determine which samples to select from both newly collected data and the previous ones. Extensive evaluations demonstrate that our proposed approach consistently outperforms the-state-of-art solutions, improving the accuracy by 16.8% on average and reducing the latency by up to 6.2 times.}
}


@inproceedings{DBLP:conf/infocom/ParkLURP25,
	author = {Hyungseok Park and
                  Sanghoon Lee and
                  Doosik Um and
                  Hyunho Ryu and
                  Kyung{-}Joon Park},
	title = {An Analytical Latency Model of the Data Distribution Service in {ROS}
                  2},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044454},
	doi = {10.1109/INFOCOM55648.2025.11044454},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ParkLURP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {After its initial release in 2007, the robot operating system (ROS) has been widely adopted as an open-source robotics middleware suite. In 2017, ROS 2 is introduced to offer enhanced performance, and it has since become the de facto standard for robot software development. In this paper, we propose an analytical latency model for the data distribution service (DDS) in ROS 2. DDS operates at the application layer on top of the user datagram protocol (UDP). In ROS 2, retransmissions for reliable data delivery are handled at the application layer, resulting in latency characteristics different from those of the transmission control protocol (TCP). We derive a closed-form analytical model to characterize the latency of DDS reliable data delivery in ROS 2, taking into account key parameters such as the packet delivery ratio, the data period, and the heartbeat period in DDS. Our extensive empirical study shows that the proposed model matches well with the empirical data, with an average error of 6.88 % across 35 different scenarios.}
}


@inproceedings{DBLP:conf/infocom/YangSAPBZ0025,
	author = {Yanni Yang and
                  Zheng Shi and
                  Zhenlin An and
                  Runyu Pan and
                  Yanling Bu and
                  Guoming Zhang and
                  Pengfei Hu and
                  Jiannong Cao},
	title = {{RFNOID:} Protecting {RFID} Motion Privacy via Metasurface},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044530},
	doi = {10.1109/INFOCOM55648.2025.11044530},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangSAPBZ0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RFID tags have become prevailing human motion sensors in applications, like smart homes and health monitoring. However, RFID's powerful through-wall sensing capabilities raise significant privacy concerns about leaking human motion information, which the RFID sensing community has overlooked. To fill this gap, we propose RFNOID, the first system that can protect human motion privacy against adversarial through-wall RFID sensing. The key enabler is our design of an RFID metasurface consisting of multiple 1-bit phase shifters that can obfuscate motion information. As a pioneering work, we perform theoretical modeling and exploratory studies of the metasurface effect on RFID signals in temporal and spectral domains. Based on the preliminary analysis, we find it is non-trivial to achieve effective signal obfuscation in both domains due to the trade-off in simultaneously increasing temporal signal chaos and masking the human motion spectrum. To tackle this issue, we judiciously devise a metasurface controlling strategy by jointly optimizing the signal entropy, variance, and spectrum distribution to balance the temporal and spectral motion obfuscation. Extensive experiments demonstrate that RFNOID can significantly decrease the adversarial through-wall motion detection rate to less than 6% and increase the respiration estimation error by over 3×.}
}


@inproceedings{DBLP:conf/infocom/0001GCR025,
	author = {Francesca Meneghello and
                  Francesco Gringoli and
                  Marco Cominelli and
                  Michele Rossi and
                  Francesco Restuccia},
	title = {How to {BREAK} {MU-MIMO} Precoding in {IEEE} 802.11 Wi-Fi Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044744},
	doi = {10.1109/INFOCOM55648.2025.11044744},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001GCR025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work reveals a critical vulnerability of the Wi-Fi standard that if unaddressed, might lead to serious security issues and compromise the performance of several billions of Wi-Fi devices. Specifically, this paper introduces and validates with commercial off-the-shelf Wi-Fi devices a new Beamforming Report Eavesdropping Attack (BREAK), which leverages the MU-MIMO channel estimation procedure used by Wi-Fi to decrease the throughput of the entire network without being detected. Through rigorous mathematical optimization, we compute the poisoned feedback that a BREAK adversary needs to send to the access point to reduce the throughput of legitimate users. Through extensive experimental evaluation with commercial Wi-Fi routers and smartphones in multiple network configurations, we show that through BREAK, an adversary may decrease the throughput at legitimate stations by 65 % modifying only about 17 % of its feedback without being detected. For replicability, we shared the code implementing the attack together with the modified firmware to be used at the adversary node. A video demonstration of BREAK is also available11https://youtu.be/SeVt0PWZZ8o.}
}


@inproceedings{DBLP:conf/infocom/ZhangWC25,
	author = {Yue Zhang and
                  Wei Wang and
                  Nan Cen},
	title = {{OPTICS:} Human Activity-Aware Integrated Optical Wireless Communication
                  and Sensing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044687},
	doi = {10.1109/INFOCOM55648.2025.11044687},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optical Wireless Communication (OWC) is emerging as  a a  promising complementary technology for 6G and beyond, with a significant increase in research focused on optical-based communication and sensing in recent years. However, until now, these two applications are still being investigated separately within the same system, which hinders further performance improvements. Moreover, the high directionality and low penetration characteristics of optical signals can lead to frequent interruptions caused by human activities, posing significant challenges for both communication and sensing tasks. In this paper, we present OPTICS: an Optical-based Integrated Communication and Sensing (OPTICS) system that seamlessly integrates optical wireless communication and sensing to create a synergistic benefit for both functions. To achieve this, OPTICS analyzes the received signal patterns to identify human activities and dynamically modifies the transmitted optical signals to simultaneously preserve communication quality and enhance human activity recognition. We implemented OPTICS using off-the-shelf devices and conducted extensive evaluations under various real-world scenarios. The results demonstrate that OPTICS can effectively improve original OWC reliability and throughput while achieving up to 99.1% accuracy in human activity recognition.}
}


@inproceedings{DBLP:conf/infocom/LiHZ0ZSCG25,
	author = {Yunzhe Li and
                  Facheng Hu and
                  Hongzi Zhu and
                  Quan Liu and
                  Xiaoke Zhao and
                  Jiangang Shen and
                  Shan Chang and
                  Minyi Guo},
	title = {Prism: Mining Task-aware Domains in Non-i.i.d. {IMU} Data for Flexible
                  User Perception},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044768},
	doi = {10.1109/INFOCOM55648.2025.11044768},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiHZ0ZSCG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A wide range of user perception applications leverage inertial measurement unit (IMU) data for online prediction. However, restricted by the non-i.i.d. nature of IMU data collected from mobile devices, most systems work well only in a controlled setting (e.g., for a specific user in particular postures), limiting application scenarios. To achieve uncontrolled online prediction on mobile devices, referred to as the flexible user perception (FUP) problem, is attractive but hard. In this paper, we propose a novel scheme, called Prism, which can obtain high FUP accuracy on mobile devices. The core of Prism is to discover task-aware domains embedded in IMU dataset, and to train a domain-aware model on each identified domain. To this end, we design an expectation-maximization (EM) algorithm to estimate latent domains with respect to the specific downstream perception task. Finally, the best-fit model can be automatically selected for use by comparing the test sample and all identified domains in the feature space. We implement Prism on various mobile devices and conduct extensive experiments. Results demonstrate that Prism can achieve the best FUP performance with a low latency.}
}


@inproceedings{DBLP:conf/infocom/Zhao00Y25,
	author = {Fangming Zhao and
                  Nikolaos Pappas and
                  Meng Zhang and
                  Howard H. Yang},
	title = {Age of Information in Energy-Harvesting-Enabled Random Access Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044661},
	doi = {10.1109/INFOCOM55648.2025.11044661},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhao00Y25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the age of information (AoI) in an energy harvesting-enabled random access network consisting of multiple source-destination dipoles. Every source node transmits a sequence of information packets to its destination over a shared spectrum using only the harvested energy. We consider that each information packet is encoded with finite-length codewords, thereby describing the nature of shortcode transmissions in random access networks. We derive closed-form expressions for the network average AoI in two special cases, i.e., the small and large energy buffer scenarios, respectively. Both results show that the average AoI under the considered network is equivalent to that in a random access network where energy is consistently available for each packet transmission but with a recalibrated update rate and a rectification term. Our analysis reveals that an optimal blocklength exists that minimizes the network average AoI, by balancing between the success decoding rate and interference across the network.}
}


@inproceedings{DBLP:conf/infocom/YangHLD0X25,
	author = {Huanqi Yang and
                  Mingda Han and
                  Xinyue Li and
                  Di Duan and
                  Tianxing Li and
                  Weitao Xu},
	title = {iRadar: Synthesizing Millimeter-Waves from Wearable Inertial Inputs
                  for Human Gesture Sensing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044481},
	doi = {10.1109/INFOCOM55648.2025.11044481},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangHLD0X25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter-wave (mmWave) radar-based gesture recognition is gaining attention as a key technology to enable intuitive human-machine interaction. Nevertheless, the significant challenge lies in obtaining large-scale, high-quality mmWave gesture datasets. To tackle this problem, we present iRadar, a novel cross-modal gesture recognition framework that employs Inertial Measurement Unit (IMU) data to synthesize the radar signals generated by the corresponding gestures. The key idea is to exploit the IMU signals, which are commonly available in contemporary wearable devices, to synthesize the radar signals that would be produced if the same gesture was performed in front of a mmWave radar. However, several technical obstacles must be overcome due to the differences between mmWave and IMU signals, the noisy gesture sensing of mmWave radar, and the dynamics of human gestures. Firstly, we develop a method for processing IMU and mmWave data that can consistently extract critical gesture features. Secondly, we propose a diffusion-based IMU-to-radar translation model that accurately transforms IMU data into mmWave data. Lastly, we devise a novel transformer model to enhance gesture recognition performance. We thoroughly evaluate iRadar, involving 18 gestures and 30 subjects in three scenarios, using five wearable devices. Experimental results demonstrate that iRadar consistently achieves 99.82% Top-3 accuracy across diverse scenarios.}
}


@inproceedings{DBLP:conf/infocom/ZhangCIA0BT25,
	author = {Qiaolun Zhang and
                  Nicola Di Cicco and
                  Memedhe Ibrahimi and
                  Raul C. Almeida and
                  Alberto Gatto and
                  Raouf Boutaba and
                  Massimo Tornatore},
	title = {Link Configuration for Fidelity-Constrained Entanglement Routing in
                  Quantum Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044462},
	doi = {10.1109/INFOCOM55648.2025.11044462},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangCIA0BT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entanglement routing (ER) in quantum networks must guarantee entanglement fidelity, a property that is crucial for applications such as quantum key distribution, quantum computation, and quantum sensing. Conventional ER approaches assume that network links can only generate entanglements with a fixed fidelity, and then they rely on purification to improve end-to-end fidelities. However, recent advances in entanglement generation technologies show that quantum links can be configured by choosing among different fidelity/entanglement-rate combinations (defined in this paper as link configurations), hence enabling a more flexible assignment of quantum-network resources for meeting specific application requirements. To exploit this opportunity, we introduce the problem of link configuration for fidelity-constrained routing and purification (LC-FCRP) in Quantum Networks. We first formulate a simplified FCRP version as a Mixed Integer Linear Programming (MILP) model, where the link fidelity can be adjusted within a finite set. Then, to explore the full space of possible link configurations, we propose a link configuration algorithm based on a novel shortest-path-based fidelity determination (SPFD) algorithm w/o Bayesian Optimization, which can be applied on top of any existing ER algorithm. Numerical results demonstrate that link configuration improves the acceptance ratio of existing ER algorithms by 87%.}
}


@inproceedings{DBLP:conf/infocom/LamDEHA25,
	author = {Maisy Lam and
                  Laura Dodds and
                  Aline Eid and
                  Jimmy G. Hester and
                  Fadel Adib},
	title = {6D Self-Localization of Drones Using a Single Millimeter-Wave Backscatter
                  Anchor},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044655},
	doi = {10.1109/INFOCOM55648.2025.11044655},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LamDEHA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present the design, implementation, and evaluation of MiFly, a self-localization system for autonomous drones that works across indoor and outdoor environments, including low-visibility, dark, and GPS-denied settings. MiFly performs 6DoF self-localization by leveraging a single millimeter-wave (mmWave) anchor in its vicinity - even if that anchor is visually occluded. MiFly's core contribution is in its joint design of a mmWave anchor and localization algorithm. The low-power anchor features a novel dual-polarization dual-modulation architecture, which enables single-shot 3D localization. Mm Wave radars mounted on the drone perform 3D localization relative to the anchor and fuse this data with the drone's internal inertial measurement unit (IMU) to estimate its 6DoF trajectory. We implemented and evaluated MiFly on a DJI drone. We collected over 6,600 localization estimates across different trajectory patterns and demonstrate a median localization error of 7 cm and a 90th percentile less than 15 cm, even in low-light conditions and when the anchor is fully occluded (visually) from the drone. Demo video: voutu.be/LfXfZ26tEok}
}


@inproceedings{DBLP:conf/infocom/AlamosSW25,
	author = {Jos{\'{e}} {\'{A}}lamos and
                  Thomas C. Schmidt and
                  Matthias W{\"{a}}hlisch},
	title = {CoRa: {A} Collision-Resistant LoRa Symbol Detector of Low Complexity},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044528},
	doi = {10.1109/INFOCOM55648.2025.11044528},
	timestamp = {Tue, 05 Aug 2025 22:40:40 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/AlamosSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Long range communication with LoRa has become popular as it avoids the complexity of multi-hop communication at low cost and low energy consumption. LoRa is openly accessible, but its packets are particularly vulnerable to collisions due to long time on air in a shared band. This degrades communication performance. Existing techniques for demodulating LoRa symbols under collisions face challenges such as high computational complexity, reliance on accurate symbol boundary information, or error-prone peak detection methods. In this paper, we introduce CoRa, a symbol detector for demodulating LoRa symbols under severe collisions. CoRa employs a Bayesian classifier to accurately identify the true symbol amidst interference from other LoRa transmissions, leveraging empirically derived features from raw symbol data. Evaluations using real-world and simulated packet traces demonstrate that CoRa clearly outperforms the related state-of-the-art, i.e., up to 29% better decoding performance than TnB and 178% better than CIC. Compared to the LoRa baseline demodulator, CoRa magnifies the packet reception rate by up to 11.53×. CoRa offers a significant reduction in computational complexity compared to existing solutions by only adding a constant overhead to the baseline demodulator, while also eliminating the need for peak detection and accurately identifying colliding frames.}
}


@inproceedings{DBLP:conf/infocom/000100Y25,
	author = {Tao Ren and
                  Zheyuan Hu and
                  Jianwei Niu and
                  Yiming Yao},
	title = {ExplabOff: Towards Explorative and Collaborative Task Offloading via
                  Mutual Information-Enhanced {MARL}},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044758},
	doi = {10.1109/INFOCOM55648.2025.11044758},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/000100Y25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access edge computing provides mobile devices (MDs) with both satisfactory computing resources and task latency, by offloading MDs' tasks to nearby edge servers. There is a popular trend to develop decentralized offloading (dec-offloading) approaches using multi-agent reinforcement learning (MARL), primarily based on centralized-training and decentralized-execution. However, the dec-offloading policies together could also lack exploration and collaboration since each MD is guided by the policy-critic only through offloading costs without explicitly considering the impacts of other MDs' offloading behaviors. Motivated by this, we propose Explorative and collaborative Offloading (ExplabOff) that can achieve superior dec-offloading by consciously exploiting the implicit exploration and collaboration information involved in MDs' states and actions. Specifically, we design two additional policy-learning metrics, the exploration-metric based on the maximum entropy of MDs' joint offloading actions and collaboration-metric based on one MD's belief about others' offloading behaviors. Then, we assemble these metrics into a new criterion defined as the mutual information (MI) between MDs' states and actions, and adopt it as an additive reward except for the vanilla reward during centralized-training. Furthermore, we distinguish MI between superior and inferior offloading, strengthening and weakening them discriminatively. Experiments on both simulation and real-testbed verify the effectiveness of ExplabOff over state-of-the-art dec-offloading.}
}


@inproceedings{DBLP:conf/infocom/TangSX0GWW25,
	author = {Chen Tang and
                  Tuo Shi and
                  Qian Xu and
                  Kui Wu and
                  Nan Guan and
                  Jen{-}Ming Wu and
                  Jianping Wang},
	title = {Designing and Implementing AoI-Optimized Scheduling for Autonomous
                  Driving Systems},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044491},
	doi = {10.1109/INFOCOM55648.2025.11044491},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TangSX0GWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous Driving Systems (ADS) incorporate complex algorithm stacks, including sensing, localization, perception, prediction, planning, and control. To enhance the safety and comfort of ADS, it is crucial to utilize the most recent sensor data and meticulously schedule these algorithm stacks for better system communication. Since the data-flow communication within ADS leverages sensor data for final planning decisions, our objective is to minimize the Age of Information (AoI), a metric that assesses the freshness of the processed information. The optimization of AoI on ADS, however, poses significant challenges due to computing resource limitations and the complicated nature of ADS operations. Moreover, most ADS platforms, such as Autoware, are built on Robot Operating System 2 (ROS 2), whose special execution behavior introduces additional complexities in scheduling algorithm design. To tackle these challenges, we propose an AoI-based scheduling optimization framework specifically for ADS. This involves developing effective algorithms to streamline and enhance the scheduling processes, as well as customizing ROS 2 at the system level to optimize AoI further. Experimental evaluations of our proposed policy against existing state-of-the-art scheduling policies in ROS 2 demonstrate notable improvements in the efficiency and responsiveness of ADS.}
}


@inproceedings{DBLP:conf/infocom/TongZWDAY25,
	author = {Jingyu Tong and
                  Xiaopeng Zhao and
                  Zhicheng Wang and
                  Donghui Dai and
                  Zhenlin An and
                  Lei Yang},
	title = {Commercial RFIDs as Reconfigurable Intelligent Surfaces},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044470},
	doi = {10.1109/INFOCOM55648.2025.11044470},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TongZWDAY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable Intelligent Surfaces (RISs) are emerging as cost-effective solutions for enhancing signal transmission in wireless networks. Traditional RIS designs, however, face challenges such as bulkiness, high production costs, limited scalability, and intricate installation due to their reliance on wired connections. In this work, we innovate by repurposing 920MHz RFID tags into battery-free unit cells, creating an affordable, scalable, and flexible one-bit phase-modulated RIS. This novel system, referred to as MetaMosaic, is engineered for compatibility with 2.4GHz Wi-Fi communications while being controlled at a 920MHz frequency. Our approach is distinguished by two primary innovations: the conversion of RFID tags into functional unit cells, and the development of a bespoke neural radiance field to identify the most effective reconfiguration strategy. We constructed six MetaMosaic surfaces, each comprising 403 cells, using a total of 2,418 repurposed RFID tags at a cost of 50 cents per tag. Our extensive testing in ten varied environments demonstrates that MetaMosaic substantially boosts signal strength, achieving a mean of 19dB gain (i.e., 80 ×) over non-RIS setups. This outperforms current leading RIS systems by a 3-fold improvement. In the context of XR application, the field study showcases a more than doubling of throughput and a halving of latency.}
}


@inproceedings{DBLP:conf/infocom/ZhouF0G025,
	author = {Hansong Zhou and
                  Jingjing Fu and
                  Yukun Yuan and
                  Linke Guo and
                  Xiaonan Zhang},
	title = {Similarity-Guided Rapid Deployment of Federated Intelligence Over
                  Heterogeneous Edge Computing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044586},
	doi = {10.1109/INFOCOM55648.2025.11044586},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouF0G025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing is envisioned to enable rapid federated intelligence on edge devices to satisfy their dynamically changing AI service demands. Semi-Asynchronous FL (Semi-Async FL) enables distributed learning in an asynchronous manner, where the server does not have to wait all local models for improving the global model. Hence, it takes a small time to well-train a global model. However, system heterogeneity in edge computing results in staleness issue, which will deteriorate training accuracy. In this paper, we propose to accelerate Semi-Async FL while ensuring training accuracy by designing a Similarity-Aware Aggregation (SAA) strategy. SAA is able to enhance the aggregation quality and thus decrease the wall-clock time, the training time for a certain accuracy. Particularly, we leverage the global model similarity to describe the local model influence and let those with higher influence contribute more to global aggregation. We further measure the similarity between global model update deviations as directional similarity, which is then used for determining aggregation timing. We theoretically provide a convergence analysis to SAA. Our extensive experimental results empirically show that the proposed SAA strategy reduces up to 53.7% wall-clock time and 59.4% wall-clock round for Semi-Async FL compared with several benchmark schemes.}
}


@inproceedings{DBLP:conf/infocom/GuWLLWLLLL25,
	author = {Chenwei Gu and
                  Qian Wu and
                  Zeqi Lai and
                  Hewu Li and
                  Yuxuan Weng and
                  Weisen Liu and
                  Jihao Li and
                  Jun Liu and
                  Yuanjie Li},
	title = {NovaPlan: An Efficient Plan of Renting Ground Stations for Emerging
                  {LEO} Satellite Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044563},
	doi = {10.1109/INFOCOM55648.2025.11044563},
	timestamp = {Fri, 18 Jul 2025 10:39:50 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GuWLLWLLLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-Earth Orbit (LEO) Satellite Networks (LSNs) are promising to provide wide Internet access and high-quality network services. Such potentials attract more operators to construct their own LSNs, e.g.,Amazon Kuiper and Telesat. Typically, in an LSN the ground stations (GSs) enable the space-to-ground transmissions. However, due to the long construction period, high cost, and geographical restrictions, it is difficult for those operators whose LSNs are under construction to build sufficient geo-distributed GSs in the short term. Therefore, early-stage LSNs may suffer from poor network reachability under the quality of service (QoS) constraints due to the lack of GSs. Based on the emergence of “Ground-Station-as-a-Service (GSaaS)”, we present Novaplan,a novel GS planning mechanism that can guide operators of developing LSNs to rent GSaaS services and schedule their traffic to guarantee high network availability in a cost-effective manner. Specifically, to efficiently solve the renting GS problem (RGP) with performance constraints, Novaplanaggregates adjacent timeslots without drastic topology changes to decrease the GS combinatorial space and determines the renting policy of individual GS to lower the total cost. Moreover, NOVAplanjudiciously schedules traffic to the path within latency constraints. Extensive evaluations based on real-world constellation information show that NOVAPLAN outperforms existing solutions with nearly 100% service rate and the lowest cost.}
}


@inproceedings{DBLP:conf/infocom/HeGLZ025,
	author = {Jintao He and
                  Jie Gui and
                  Tian Lv and
                  Jiaqi Zhu and
                  Qun Huang},
	title = {FD-Filter: {A} Compact Data Structure for Fine-Grained Intra-Flow
                  Packet Delay Monitoring},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044616},
	doi = {10.1109/INFOCOM55648.2025.11044616},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HeGLZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet delay is a key indicator of network health. Traditional packet delays focus on coarse-grained per-hop or end-to-end delays. However, this paper focuses on Intra-Flow Packet Delay (IFPD). It is the arrival time span between two adjacent packets in each flow. IFPD allows detailed traffic arrival pattern analysis, addressing cases such as arrival jitter and APT detection, etc. Existing solutions fail to monitor IFPD with small memory and high accuracy. To this end, we propose FD-Filter, an extension of Bloom Filter, for IFPD. It comprises several window filters and an existence filter. The window filters store approximate timestamps for IFPD estimation. The existence filter selects newly-arrival flows for higher accuracy. FD-Filter employs four novel techniques: per-bit update and query for better accuracy, filter recycle for prolonged monitoring, counter combination for faster query, and lazy aging for hardware compatibility. Our experiments on real-world traces demonstrate the accuracy and efficiency of FD-Filter over existing approaches.}
}


@inproceedings{DBLP:conf/infocom/MisraSA25,
	author = {Ashitabh Misra and
                  Nurani Saoda and
                  Tarek F. Abdelzaher},
	title = {Latency-Constrained Input-Aware Quantization of Time Series Inference
                  Workflows at the Edge},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044709},
	doi = {10.1109/INFOCOM55648.2025.11044709},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MisraSA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantization is a common technique for compressing Deep Neural Networks (DNNs) for edge device deployment, crucial for real-time time-series sensor data classification. While Adaptive Quantization (AQ) techniques allow updating the model's quantization scheme based on resource constraints without re-training, they are input-agnostic. Conversely, Input-aware Quantization (IQ) techniques adjust quantization per input, improving accuracy but ignoring resource constraints. Additionally, most quantization methods are designed for vision-based applications and underutilize time-frequency domain semantics. To this end, we present ReactQuant, an end-to-end framework for input-aware quantization of DNNs with strict resource constraints, specialized for time-series applications. ReactQuant features Partition-Derived Quantization, which segments time-series input based on its time and frequency domain features; a two-staged training process that restricts quantization space to each layer's promising bit-width candidates; and a novel algorithm to perform input-aware quantization within user-defined resource constraints. Extensive experimentation shows that ReactQuant outperforms AQ and IQ techniques in accuracy and ensures strict compliance with user-defined resource constraints. Under similar resource constraints, ReactQuant achieves up to 19% higher accuracy than AQ techniques and 4.6% higher than state-of-the-art IQ technique. It incurs no resource constraint violations, compared to up to 27.63% for state-of-the-art IQ technique.}
}


@inproceedings{DBLP:conf/infocom/0001R0LHL025,
	author = {Tian Pan and
                  Guohao Ruan and
                  Qiang Fu and
                  Zhengjie Luo and
                  Junkai Huang and
                  Xingshuang Luo and
                  Tao Huang},
	title = {StableRoute: When Dijkstra's Algorithm Meets Topology-Varying Satellite
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044485},
	doi = {10.1109/INFOCOM55648.2025.11044485},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001R0LHL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth Orbit (LEO) satellite constellations are becoming a viable means for Internet access. However, their topology changes as satellites move towards or away from orbital intersection points, leading to constant link down or up. This may cause routing table entry updates and thus path changes between satellites. A path change during transmission may lead to out-of-order packet delivery and invalidate the current TCP congestion window. While some path changes are inevitable, some are avoidable. Dijkstra's algorithm is a popular choice among the routing protocols proposed for LEO satellite networks. We observe that many next-hop route updates by Dijkstra's algorithm are avoidable. Motivated by this, we propose StableR-oute, which stabilizes routing paths from different perspectives. StableRoute Local (SR_L) leverages equal-cost shortest paths and stays with the current one if it is still valid. StableRoute K-Short (SR_K) allows a path longer than the shortest path. StableRoute Global (SR_G) leverages the predictable satellite trajectories and topology variations, and thus works out a next-hop route selection sequence that minimizes the number of route updates over a time period. The evaluation shows that SR_L, SR_K and SR_G outperform Dijkstra's algorithm, substantially reducing the number of route updates in changing topologies.}
}


@inproceedings{DBLP:conf/infocom/YanLHLB25,
	author = {Qifan Yan and
                  Andrew Liu and
                  Shiqi He and
                  Mathias L{\'{e}}cuyer and
                  Ivan Beschastnikh},
	title = {FedFetch: Faster Federated Learning with Adaptive Downstream Prefetching},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044717},
	doi = {10.1109/INFOCOM55648.2025.11044717},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YanLHLB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a machine learning paradigm that facilitates massively distributed model training with end-user data on edge devices directed by a central server. However, the large number of heterogeneous clients in FL deployments leads to a communication bottleneck between the server and the clients. This bottleneck is made worse by straggling clients, any one of which will further slow down training. To tackle these challenges, researchers have proposed techniques like client sampling and update compression. These techniques work well in isolation but combine poorly in the downstream, server-to-client direction. This is because unselected clients have outdated local model states and need to synchronize these states with the server first. We introduce FedFetch, a strategy to mitigate the download time overhead caused by combining client sampling and compression techniques. FedFetch achieves this with an efficient prefetch schedule for clients to prefetch model states multiple rounds before a stated training round. We empirically show that adding FedFetch to communication efficient FL techniques reduces end-to-end training time by 1.26 × and download time by 4.49× across compression techniques with heterogeneous client settings.}
}


@inproceedings{DBLP:conf/infocom/ShinLPB25,
	author = {Juhun Shin and
                  Goodsol Lee and
                  Jeongyeup Paek and
                  Saewoong Bahk},
	title = {C{\'{e}}sar: Cellular Resource Scheduling-Aware Congestion Control},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044650},
	doi = {10.1109/INFOCOM55648.2025.11044650},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ShinLPB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Delay-based congestion control algorithms (CCAs) have been proposed to tackle the bufferbloat problem of traditional loss-based CCAs. However, existing delay-based CCAs either fail to adequately consider the non-congestive delay caused by scheduling characteristics of modern cellular networks leading to improper congestion control, or face practical deployment issues, which results in an inability to fully utilize the high bandwidth and low latency that recent cellular systems provide. To resolve this problem, we propose César, a cellular resource scheduling-aware congestion control with only sender-side modification. César estimates scheduling unit through TCP ACK interval patterns to deduce the scheduling characteristics of the current cellular link, and adjusts the congestion window size based on scheduling unit in a step-wise manner to minimize the impact of the scheduling delay on congestion control. Experimental results on 5G and LTE cellular networks of three different mobile carriers show that César outperforms other state-of-the-art CCAs. Results show that throughput-over-latency performance improves by up to 2.89×, 10.09×, 1.39×, and 5.65× compared to ExLL, PropRate, BBR, and Cubic, respectively.}
}


@inproceedings{DBLP:conf/infocom/DangKSRS25,
	author = {Anh{-}Khoa Dang and
                  Hicham Khalif{\'{e}} and
                  Mathias Sintorn and
                  St{\'{e}}phane Rovedakis and
                  Stefano Secci},
	title = {Data-driven Energy Optimization in Mobile Networks with User Experience
                  Guarantees},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044545},
	doi = {10.1109/INFOCOM55648.2025.11044545},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DangKSRS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we model carrier shutdown in multi-carrier mobile networks as a deep reinforcement learning problem. Our model takes energy-saving actions by turning off carriers and reallocating their users while in addition to maintaining connectivity guarantees a novel user experience metric. Leveraging real and recent datasets, we train and evaluate our model over realistic network scenarios. Our results show more than 15% energy saving with the fulfillment of the user experience constraints, outperforming currently deployed solutions and researched approaches in the literature by almost 50%. More interestingly our approach exhibits generalization properties, a very promising characteristic for its adoption in real mobile networks deployment.}
}


@inproceedings{DBLP:conf/infocom/HuHW25,
	author = {Miao Hu and
                  Qi He and
                  Di Wu},
	title = {{QLLMS:} Quantization-Adaptive {LLM} Scheduling for Partially Informed
                  Edge Serving Systems},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044591},
	doi = {10.1109/INFOCOM55648.2025.11044591},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The quantization technologies have enabled the practical deployment of large language models (LLMs) at the edge, however, current edge scheduling and quantization selection are separately designed. Furthermore, partially informed edge processing performance further exacerbates these challenges. To address these issues, we introduce QLLMS, a joint quantization-adaptive scheduling scheme for large language models tailored to partially informed edge serving systems. The primary objective of our approach is to reduce GPU rental costs by strategically orchestrating both quantization options and heterogeneous resources. The QLLMS algorithm first determines the available quantization set (AQS) to optimize the LLM inference performance within limited edge resources. To address the unpredictable nature of edge computing performance, we present a low-rank property-driven recovery approach, which can reconstruct complete AQS matrices using only partial samples. Subsequently, we devise a novel many-to-one matching algorithm that aims to strike a balance between efficient utilization of edge resources and optimal model inference performance, factoring in the available quantization options. We prove that QLLMS yields a stable matching without blocking pairs that could lead to inefficiencies. Experiment results show that QLLMS achieves a reduction of up to 22.36% in rental costs compared to state-of-the-art baselines in partially informed edge serving systems while simultaneously improving the task completion rate on edge servers.}
}


@inproceedings{DBLP:conf/infocom/RaoM25,
	author = {Chirag Rao and
                  Eytan H. Modiano},
	title = {Minimum-hop Constellation Design for Low Earth Orbit Satellite Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044684},
	doi = {10.1109/INFOCOM55648.2025.11044684},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/RaoM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a Low Earth Orbit (LEO) satellite network with each satellite capable of establishing inter-satellite link (ISL) connections for satellite-to-satellite communication. Since ISLs can be reoriented to change the topology, we optimize the topology to minimize the average shortest path length (ASPL). We characterize the optimal ASPL ISL topology in two families of topologies, 1) vertex-symmetric in which the ISL connections at a satellite node represent a motif that is repeated at all other satellite nodes, and 2) general regular topologies in which no such repeating pattern need exist. We establish ASPL lower bounds for both scenarios and show constructions for which they are achievable assuming each satellite makes 3 or 4 ISL connections. For the symmetric case, we show that the mesh grid is suboptimal in both ASPL and diameter. Additionally, we show there are constructions that maintain intra-orbital ISL connections while still achieving near-optimal ASPL performance. For the general case we show it is possible to construct networks with ASPL close to the general lower bound when the network is sufficiently dense. Simulation results show that for both scenarios, one can find topologies that are very close to the lower bounds as the network size scales.}
}


@inproceedings{DBLP:conf/infocom/ChengLXLX25,
	author = {Wenhui Cheng and
                  Han Lu and
                  Chaocan Xiang and
                  Dehua Liu and
                  Tao Xiang},
	title = {Breaking 'Chicken-Egg': Cross-city Battery Swap Demand Prediction
                  via Knowledge-guided Diffusion},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044523},
	doi = {10.1109/INFOCOM55648.2025.11044523},
	timestamp = {Tue, 08 Jul 2025 16:25:57 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChengLXLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The battery swap market has experienced rapid expansion in recent years. The deployment of battery swap stations (BSSs) is a fundamental issue that needs to be addressed during expansion in new cities. Intuitively, deploying BSSs according to user demand distribution is sensible. However, the absence of historical data makes it challenging to predict the demand in new cities before actual deployment, resulting in a ‘Chicken-Egg’ dilemma. To address this dilemma, we propose STKDec, a spatio-temporal knowledge-guided conditional diffusion model for cross-city battery swap demand prediction. STKDec leverages historical data from source cities with deployed BSSs to predict user demands for target cities planning BSS deployment. The core of STKDec is a novel station-level context representation-based condition module. The key idea of this module is to extract shared spatiotemporal context representations of all stations and use these representations to bridge the source and target cities for demand prediction. Specifically, we derive the spatial context representation of stations through an urban knowledge graph construction and a designed multi-relation-aware graph convolutional network while extracting the temporal context representation using a temporal information encoding strategy. Extensive experiments on real-world battery swap datasets demonstrate that STKDec averagely outperforms state-of-the-art methods by 30.7 %.}
}


@inproceedings{DBLP:conf/infocom/LiuWW25,
	author = {Mengfan Liu and
                  Wei Wang and
                  Chuan Wu},
	title = {Optimizing Distributed Deployment of Mixture-of-Experts Model Inference
                  in Serverless Computing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044553},
	doi = {10.1109/INFOCOM55648.2025.11044553},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advancement of serverless computing, running machine learning (ML) inference services over a serverless platform has been advocated, given its labor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models have been a dominant type of model architectures to enable large models nowadays, with parallel expert networks. Serving large MoE models on serverless computing is potentially beneficial, but has been underexplored due to substantial challenges in handling the skewed expert popularity and scatter-gather communication bottleneck in MoE model execution, for cost-efficient serverless MoE deployment and performance guarantee. We study optimized MoE model deployment and distributed inference serving on a serverless platform, that effectively predict expert selection, pipeline communication with model execution, and minimize the overall billed cost of serving MoE models. Especially, we propose a Bayesian optimization framework with multi-dimensional  ϵ − g r e e d y \\epsilon\\mathbf{-greedy}  search to learn expert selection and optimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian decision-making method for predicting expert popularity; 2) flexibly pipelined scatter-gather communication; and 3) an optimal model deployment algorithm for distributed MoE serving. Extensive experiments on AWS Lambda show that our designs reduce the billed cost of all MoE layers by at least 75.67% compared to CPU clusters while maintaining satisfactory inference throughput. As compared to LambdaML in serverless computing, our design achieves 43.41% lower cost with a throughput decrease of no more than 18.76%.}
}


@inproceedings{DBLP:conf/infocom/YunK0GLZHF0W025,
	author = {Tong Yun and
                  Yinxin Kuang and
                  Haoyu Song and
                  Zhongyi Gu and
                  Zhuang Ling and
                  Zhiyu Zhang and
                  Chengkang Huang and
                  Yibo Fan and
                  Yang Xu and
                  Jianping Wang and
                  Bin Liu},
	title = {Hardware-Accelerated Flow Interaction Graph Compression for High-Speed
                  Anomaly Detection},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044441},
	doi = {10.1109/INFOCOM55648.2025.11044441},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YunK0GLZHF0W025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of encrypted Internet traffic fosters new anomaly detection techniques relying on flow interactions, which are resilient and effective in detecting encrypted malicious traffic and zero-day attacks. However, existing software-based solutions fail to achieve high-speed anomaly detection for up to 100Gbps or higher throughput traffic fed by the current NICs. To solve this problem, we propose ICAD to enable hardware-accelerated flow interaction graph compression and high-speed anomaly detection. ICAD integrates the up-to-date high-speed measurement technology and uses the hardware Bloom filter and CM -Sketches for line-speed data processing and flow interaction graph compression, greatly reducing the host processor's computational load. High throughput and accuracy on data analysis are achieved by using a decision tree model. We implement a prototype on a server with an FPGA-based Smart-NIC and show that with the line-speed hardware acceleration, ICAD improves the software throughput of graph construction and anomaly detection by 2x and 134x over the state-of-the-art solution, and achieves higher than 99 % detection accuracy and 99% F1-score. Using 15 processor cores, ICAD can sustain the worst-case 100GE traffic. The FPGA resource and ASIC chip area evaluations prove the ICAD hardware cost is low.}
}


@inproceedings{DBLP:conf/infocom/ButunHG025,
	author = {Beyza B{\"{u}}t{\"{u}}n and
                  David De Andres Hernandez and
                  Michele Gucciardo and
                  Marco Fiore},
	title = {{DUNE:} Distributed Inference in the User Plane},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044678},
	doi = {10.1109/INFOCOM55648.2025.11044678},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ButunHG025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of Machine Learning (ML) models in the user plane enables line-rate in-network inference, significantly reducing latency and improving the scalability of functions like traffic monitoring. Yet, integrating ML models into programmable network devices requires meeting stringent constraints in terms of memory resources and computing capabilities. Previous solutions have focused on implementing monolithic ML models within individual programmable network devices, which are limited by hardware constraints, especially while executing challenging classification use cases. In this paper, we propose DUNE, a novel framework that realizes for the first time a user plane inference that is distributed across the multiple devices that compose the programmable network. DUNE adopts fully automated approaches to ( i i ) breaking large ML models into simpler sub-models that preserve inference accuracy while minimizing resource usage, (ii) designing the sub-models and their sequencing so as to enable an efficient distributed execution of joint packet- and flow-level inference. We implement DUNE using P4, deploy it in an experimental network with multiple industry-grade programmable switches, and run tests with real-world traffic measurements for two complex classification use cases. Our results demonstrate that DUNE not only reduces perswitch resource utilization with respect to legacy monolithic ML designs but also improves their inference accuracy by up to 7.5 %.}
}


@inproceedings{DBLP:conf/infocom/RenGHCLX025,
	author = {Yaochen Ren and
                  Gaopeng Gou and
                  Chengshang Hou and
                  Tianyu Cui and
                  Zhen Li and
                  Gang Xiong and
                  Chang Liu},
	title = {IPv6 Prefix Target Generation through Pattern and Distribution Learning
                  using Vision-Transformer and Guided-Diffusion},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044676},
	doi = {10.1109/INFOCOM55648.2025.11044676},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/RenGHCLX025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IPv6 network scanning is an essential active technology for network management. Existing target generation algorithms enable the usability of IPv6 scanning technology across the whole network. Under prefix target generation, they are affected by non-target prefix data, resulting in severe performance degradation. Prefix target generation mainly faces the challenges of complex and diverse addressing patterns and huge 64-bit prefix space. In this paper, we first propose a prefix target generation algorithm - 6PTG to address these challenges. 6PTG mines the knowledge of addressing patterns in massive unlabeled addresses based on unsupervised Vision Transformer Autoencoder(ViT-AE) and introduces it into the generation stage. Based on Guided-Diffusion, 6PTG captures the mapping between prefixes and active address distributions in a unified model to avoid the need to train the model separately for each prefix. In natural network environments, the 6PTG achieves an average hit rate of 52 % and a generation rate of 24 % under prefixes of varying active scales. Compared to transition-adapted whole-network-level algorithms, it achieves a 2.1x and 7.6x improvement on the existing public dataset IPv6 Hitlist. 6PTG effectively fills the gap in relevant research.}
}


@inproceedings{DBLP:conf/infocom/LiLRG25,
	author = {Yan Li and
                  Lailong Luo and
                  Bangbang Ren and
                  Deke Guo},
	title = {Anchor: {A} Novel Modeling Methodology for Cooperative {UAV-MEC} Based
                  on Stochastic Geometry},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044603},
	doi = {10.1109/INFOCOM55648.2025.11044603},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiLRG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) play a vital role in the air-to-ground integrated network, which can offer a mobile edge computing (MEC) environment closer to terrestrial user equipments (UEs). This integrated network faces the challenges of unstable links and node mobility. It is urgent to rethink the UAV-assisted MEC architecture to satisfy the requirements of reliable and low-latency communication in dynamic scenarios. This paper proposes a novel dynamic cooperative offloading model for UAV swarms based on Delaunay triangulation, where each UE offloads its data to a triangular cell composed of three UAVs for joint processing. Then, we propose a novel and efficient cooperative handoff mechanism and an innovative analysis method to well tackle the mobility characteristics of UAV s. Finally, we design a performance evaluation method for UAV cooperative task offloading by leveraging the stochastic geometry analysis framework. The simulation and numerical results verify the correctness and practicability of the proposed model and analysis method. Compared to conventional cooperative and non-cooperative offloading mechanisms, our approach achieves an average performance improvement of 20% and 30%, respectively.}
}


@inproceedings{DBLP:conf/infocom/GaoHYL0W25,
	author = {Bin Gao and
                  Zhuomin He and
                  Yizhen Yao and
                  Zhanzhi Lou Lou and
                  Zhi Zhou and
                  Weng{-}Fai Wong},
	title = {Online Context Caching for Distributed Large Language Models Serving},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044599},
	doi = {10.1109/INFOCOM55648.2025.11044599},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GaoHYL0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) based on transformer architectures have demonstrated exceptional performance across various generative tasks. However, the significant GPU resources required for LLM inference pose financial challenges for large-scale deployment. Context caching has been proposed to enhance cost-efficiency by storing intermediate key and value (KV) pairs in cost-effective storage mediums, which can be reused to accelerate inference when requests share prefixes. While promising in single-instance applications, context caching in distributed LLM serving systems introduces unique challenges. Firstly, context caching decisions across time slots are inter-dependent, affecting overall system efficiency due to potential cache misses. Secondly, request scheduling complexities arise, leading to load balancing issues among instances. We address these challenges by formulating an online optimization problem that jointly decides KV cache placement and request scheduling to minimize inference costs across time slots. Given the NP-hard nature of this problem, we propose a framework leveraging regularization, linear relaxation, and randomized rounding techniques. Our solution achieves a competitive ratio near the offline optimum. Experimental results in a distributed LLM serving system demonstrate significant performance improvements over baseline methods.}
}


@inproceedings{DBLP:conf/infocom/LiaoYCY25,
	author = {Si Liao and
                  Fengxu Yang and
                  Huangxun Chen and
                  Zhice Yang},
	title = {Enable Autonomous Backscatter in Everyday Devices},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044708},
	doi = {10.1109/INFOCOM55648.2025.11044708},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiaoYCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent research has integrated backscatter mechanisms into existing wireless networks, aiming to enable backscatter transmitters to communicate directly using conventional wireless protocols. This would allow for low-power wireless communication within today's network infrastructure. However, the lack of native support poses a challenge in seamlessly accommodating backscatter transmitters within legacy infrastructure. This paper introduces EMScatter, a backscatter system designed for commodity mobile devices. It eliminates the need for an external excitation source separate from the transceiver peers. The proposed approach utilizes the inherent electromagnetic radiation (EMR) signals emitted by the device as the excitation signal, effectively transforming commodity mobile devices into backscatter readers. Users can utilize their mobile devices to directly communicate with backscatter tags anytime and anywhere.}
}


@inproceedings{DBLP:conf/infocom/Zhao0T0ZXLX25,
	author = {Hairui Zhao and
                  Hongliang Li and
                  Qi Tian and
                  Jie Wu and
                  Meng Zhang and
                  Zhewen Xu and
                  Xiang Li and
                  Haixiao Xu},
	title = {ArrayPipe: Introducing Job-Array Pipeline Parallelism for High Throughput
                  Model Exploration},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044666},
	doi = {10.1109/INFOCOM55648.2025.11044666},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhao0T0ZXLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep Learning (DL) applications have experienced exponential growth in data volume and model complexity, spurring various parallel approaches. Existing solutions mostly focus on accelerating individual training jobs. However, jobs submitted to a cluster may not always be independent. This is due to the distinctive characteristic of DL training that it is an exploratory process. Model developers often launch multiple training instances in a batch with the same model structure but different settings to tune hyper-parameters, which provides an opportunity to regard these jobs as job-arrays. With further support of low-cost job context switching, sharing resources among these jobs is not just feasible but also beneficial to the resource utilization and the throughput of a DL cluster. This paper introduces Job-Array Pipeline Parallelism (JAP) that assembles a batch of sibling DL training jobs into a concurrent job-array. We design ArrayPipe, a framework that supports high throughput model exploration with JAP. A novel scheduling problem in JAP is proposed that seeks to minimize the per-iteration training time for a job-array, along with two scheduling algorithms for different scales of job-arrays. Extensive testbed experiments and trace-driven simulations show that ArrayPipe achieves 1.46× training throughput on average compared with state-of-the-art related works.}
}


@inproceedings{DBLP:conf/infocom/HassanSLGCCALMK25,
	author = {Fahid Hassan and
                  Zhambyl Shaikhanov and
                  Jeffrey Lei and
                  Hichem Guerboukha and
                  Hou{-}Tong Chen and
                  Chun{-}Chieh Chang and
                  Sadhvikas Addamane and
                  Michael P. Lilly and
                  Daniel M. Mittleman and
                  Edward W. Knightly},
	title = {Downlink Multi-User Sub-THz Communication with a Programmable Metasurface},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044710},
	doi = {10.1109/INFOCOM55648.2025.11044710},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HassanSLGCCALMK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Among the many challenges in realizing wireless networks above 100 GHz, multi-user access remains one of the least explored. Traditionally, multi-user multiplexing at lower frequencies requires the implementation of one RF chain per data stream (user). Adapting such technology to higher frequencies above 100 GHz incurs prohibitively complex design and fabrication challenges, especially when scaling to many users. Here, we propose a fundamentally new approach that enables sub-terahertz multi-user access without any RF chains. Our design consists of a programmable, transmissive metasurface and a single monochromatic sub-terahertz source. We use the metasurface to modulate the phase and amplitude of the transmitted sub-THz wave using random voltage patterns, producing a high-entropy wavefront that has unique angular-dependent patterns. Generating these spatially diverse responses enables concurrent transmission of distinct information symbols to multiple users at different angular locations. We demonstrate the feasibility of this approach using both numerical simulations and exper-imental studies, showcasing the ability to serve multiple users simultaneously with distinctive data streams, even in scenarios with minimal angular separation between users. The study paves the way for a new architecture for sub-THz spatial multiplexing with no RF chains.}
}


@inproceedings{DBLP:conf/infocom/ZhangZHW25,
	author = {Bowen Zhang and
                  Junyang Zhang and
                  Jiahui Hou and
                  Yixin Wang},
	title = {TensAllo: Adaptive Deployment of LLMs on Resource-Constrained Heterogeneous
                  Edge Devices},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044447},
	doi = {10.1109/INFOCOM55648.2025.11044447},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangZHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, large language models (LLMs) have demonstrated powerful capabilities in various domains, including intelligent voice. To preserve privacy while offering better services, it is meaningful to run LLMs on heterogeneous smart home devices for home assistants. Current works predominantly center on the inference capabilities of LLMs using GPU clusters. However, few works study efficient inference on edge devices with constrained computational resources, which presents unique challenges. In this paper, we introduce TensAllo, an innovative system that optimizes the inference of LLMs and ensures reasonable workloads deployed on heterogeneous devices for home assistants. TensAllo employs tensor parallelism for distributed inference and utilizes quantization techniques to reduce memory usage. To reduce inference latency and power consumption, TensAllo intelligently selects devices and determines the optimal allocation of tensors under the constraints of the set power consumption. Extensive experimentation on various heterogeneous edge devices confirms TensAllo's superiority. The results demonstrate that TensAllo increases inference speed by 50 % and 37 %, respectively.}
}


@inproceedings{DBLP:conf/infocom/JiangH25,
	author = {Yuhong Jiang and
                  Jia Hu},
	title = {Accelerating Clustered Federated Learning in Dynamic {D2D} Networks
                  with Transferable {GNN}},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044690},
	doi = {10.1109/INFOCOM55648.2025.11044690},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/JiangH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous computation and communication resources across mobile devices drastically degrade the performance of Federated Learning (FL), while clustered FL is recognized as an effective solution to this issue. Traditional clustered FL methods rely on a cluster head for intra-cluster model aggregation, however, such a cluster head that can directly communicate with all other devices may not exist in practical Device-to-Device (D2D) networks. Besides, most methods consider static network conditions and thus cannot adapt to the dynamic topologies and resources in D2D networks. To address these challenges, we propose a Transferable Graph Neural Network (GNN)-based Clustered FL method, which formulates FL clustering in dynamic D2D networks as a graph problem and develops a transferable GNN model using unsupervised training to adaptively solve this problem. Furthermore, to alleviate the impact of data heterogeneity and accelerate FL, we design a D2D connectivity-aware dynamic programming algorithm driven by Mutual Information for selecting participating devices within each cluster. We also provide a convergence bound for the global loss through theoretical analysis. Finally, we conduct extensive experiments with various network and data settings, and the results demonstrate that our method improves FL time efficiency by 24%-78% and reduces communication cost by 30%-88% compared to key baselines.}
}


@inproceedings{DBLP:conf/infocom/Wan0CC0L025,
	author = {Zirui Wan and
                  Jiao Zhang and
                  Shuai Cheng and
                  Ying Chen and
                  Tian Pan and
                  Pingping Lin and
                  Tao Huang},
	title = {{ACC:} Addressing Performance Limitations in Datacenters with Atomic
                  Congestion Control},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044532},
	doi = {10.1109/INFOCOM55648.2025.11044532},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Wan0CC0L025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of new datacenter applications drives the demand for ultra-low latency and high bandwidth underlying networks. However, we investigate that existing congestion control mechanisms suffer from bandwidth under-utilization due to overreaction or slowly ramp up when network perturbations occur. In this paper, we propose a novel window-based mechanism, called Atomic Congestion Control (ACC), to address the problems. During congestion, ACC directly decreases the rate of congested flows to their receiving rate to avoid overreaction and further leverages a corresponding sending window to drain out built-up queues. During the increase process, ACC combines the dynamic increase factor with the receiving rate to promptly reclaim free bandwidth. ACC ensures fairness and is easy to deploy in commodity datacenters. We theoretically analyze the convergence rate, fairness, and stability of ACC and further implement it with DPDK. Extensive testbed experiments and large-scale simulations show that ACC can cut the average queue length by up to 10× while reducing the average FCT by up to 82% compared with state-of-the-art approaches in large-scale simulations.}
}


@inproceedings{DBLP:conf/infocom/MiBHWBL25,
	author = {Yu Mi and
                  Randeep Bhatia and
                  Fang Hao and
                  An Wang and
                  Steven A. Benno and
                  T. V. Lakshman},
	title = {Tree Embedding Based Mapping System for Low-Latency Mobile Applications
                  in Multi-Access Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044665},
	doi = {10.1109/INFOCOM55648.2025.11044665},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MiBHWBL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-latency applications like AR/VR and online gaming need fast, stable connections. New technologies such as V2X, LEO satellites, and 6G bring unique challenges in mobility management. Traditional solutions based on centralized or distributed anchors often fall short in supporting rapid mobility due to inefficient routing, low versatility, and insufficient multi-access support. In this paper, we design a new end-to-end system for tracking multi-connected mobile devices at scale and optimizing performance for latency-sensitive, highly dynamic applications. Our system, based on the locator/ID separation principle, extends to multi-access networks without requiring specialized routers or caching. Using a novel tree embedding-based overlay, we enable fast session setup while allowing endpoints to directly handle mobility between them. Evaluation with real network data shows our solution cuts connection latency to 7.42 % inflation over the shortest path, compared to LISP's 359% due to cache misses. It also significantly reduces location update overhead and disruption time during mobility.}
}


@inproceedings{DBLP:conf/infocom/ZhaoSZLZ0M25,
	author = {Mingyue Zhao and
                  Jiayi Shi and
                  Zhengyuan Zhang and
                  Yue Ling and
                  Guanzhou Zhu and
                  Dong Zhao and
                  Huadong Ma},
	title = {C\({}^{\mbox{2}}\)F: Enabling Context-Aware Edge-Cloud Collaborative
                  Inference for Foundation Models},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044700},
	doi = {10.1109/INFOCOM55648.2025.11044700},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoSZLZ0M25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transformer-based foundation models (FMs) excel in diverse domains but struggle with high computation costs, hin-dering effective inference on resource-constrained edge devices. Edge-cloud collaborative inference offers a promising paradigm, but existing methods either neglect the contexts (computing resources and data environments) of edge devices or incur high transmission overheads when applied to FMs. In contrast, we propose C2F, a novel context-aware edge-cloud collaborative inference method for FMs, enabling open-set learning with low latency and high accuracy. In C2F, a context-aware model customization module is utilized to customize a small model (SM) from the FM based on the context, subsequently deployed on the designated edge device. During inference, an adaptive inference module is employed to determine whether to query the FM according to local results of the SM; if so, it transmits only vital data patches, effectively reducing transmission costs and end-to-end latency. Extensive experimental results demonstrate that C2F achieves efficient inference for FMs, enhancing accuracy by 3.1-30.3% and reducing end-to-end latency by 1.32-8.75× compared to various baselines.}
}


@inproceedings{DBLP:conf/infocom/LuoLL25,
	author = {Bin Luo and
                  Xiaojun Lin and
                  John C. S. Lui},
	title = {Exploring the Structural Property of the Optimal Entanglement Policy
                  for Quantum Switch},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044695},
	doi = {10.1109/INFOCOM55648.2025.11044695},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LuoLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A quantum switch is one of the most fundamental network elements for connecting different quantum devices. In this paper, we explore the “optimal entanglement policy“ of a quantum switch under a scenario where the stored and entangled qubits in the quantum switch may undergo a de-coherence process. Finding an optimal entanglement policy is important as it enables a quantum switch to make a judicious basis measurement based on the number of existing link-level entanglements to maximize the “weighted throughput“. We use a Markov decision process framework to model the dynamics of the quantum switch, and theoretically shows a threshold-based property between bipartite and tripartite policies under a very general class of weight functions with a weight parameter  β \\beta , Empirically, such a threshold-based property also holds for the optimal entanglement policy. In particular, the quantum switch should perform a bipartite or tripartite policy when  β \\beta  is below a threshold  β ∗ B \\beta_{B}^{*}  or above a threshold  β ∗ T \\beta_{T}^{*}  ‘, respectively. When  β ∗ B < β < β ∗ q \\beta_{B}^{*} < \\beta < \\beta_{q}^{*}  “the quantum switch needs to perform a “threshold-based and state-dependent entanglement policy“. We also extend the work to allow a mixture of bipartite and tripartite policies. We theoretically show similar threshold-related relationships between mixed and bipartite/tripartite policies. We carry out extensive numerical experiments to confirm that the optimal entanglement policy has such structural property.}
}


@inproceedings{DBLP:conf/infocom/ZhaoWZW25,
	author = {Tianya Zhao and
                  Ningning Wang and
                  Junqing Zhang and
                  Xuyu Wang},
	title = {Protocol-Agnostic and Data-Free Backdoor Attacks on Pre-Trained Models
                  in {RF} Fingerprinting},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044704},
	doi = {10.1109/INFOCOM55648.2025.11044704},
	timestamp = {Tue, 05 Aug 2025 22:40:41 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoWZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While supervised deep neural networks (DNNs) have proven effective for device authentication via radio frequency (RF) fingerprinting, they are hindered by domain shift issues and the scarcity of labeled data. The success of large language models has led to increased interest in unsupervised pre-trained models (PTMs), which offer better generalization and do not require labeled datasets, potentially addressing the issues mentioned above. However, the inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently explored. In this paper, we thoroughly investigate data-free backdoor attacks on such PTMs in RF fingerprinting, focusing on a practical scenario where attackers lack access to downstream data, label information, and training processes. To realize the backdoor attack, we carefully design a set of triggers and predefined output representations (PORs) for the PTMs. By mapping triggers and PORs through backdoor training, we can implant backdoor behaviors into the PTMs, thereby introducing vulnerabilities across different downstream RF fingerprinting tasks without requiring prior knowledge. Extensive experiments demonstrate the wide applicability of our proposed attack to various input domains, protocols, and PTMs. Furthermore, we explore potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack.}
}


@inproceedings{DBLP:conf/infocom/LiuJYCZKLLC25,
	author = {Yutong Liu and
                  Haiming Jin and
                  Yinjie Wang Yao and
                  Yunxiang Chen and
                  Yimin Zhao and
                  Linghe Kong and
                  Rui Li and
                  Xiaoyang Liu and
                  Guihai Chen},
	title = {Distributed On-Orbit Sparse Coding for Efficient Space Situational
                  Awareness Image Transmission},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044504},
	doi = {10.1109/INFOCOM55648.2025.11044504},
	timestamp = {Tue, 05 Aug 2025 22:40:40 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuJYCZKLLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Space Situational Awareness (SSA) relies on Low Earth Orbit (LEO) satellites to capture continuous, high-resolution imagery critical for identifying space threats. The vast volume of SSA images overwhelms satellite network band-width, hindering timely transmission and processing. This paper presents a novel image compression method based on sparse coding to mitigate this transmission bottleneck. By exploiting the high sparsity and spatial-temporal redundancy of SSA images, we introduce an Aggregated Dictionary Learning (ADL) algorithm and a Context-aware Adaptive Binary Arithmetic Coding (OABAC) algorithm for further reducing dictionary and coefficient sizes. The proposed sparse coding is operated across LEO satellites in a distributed manner. Both overlapping and non-overlapping regions of the image are divided and processed paralleled on different satellites, optimizing resource usage and reducing latency. Evaluations show a 93.78% high compression ratio, surpassing existing methods and ensuring efficient SSA data transmission and processing in constrained satellite networks.}
}


@inproceedings{DBLP:conf/infocom/WangN25,
	author = {Zhenyu Wang and
                  Shahriar Nirjon},
	title = {FedEXT: Differential Federated Learning with Complementary Extension
                  of Edge Models},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044645},
	doi = {10.1109/INFOCOM55648.2025.11044645},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce FedEXT, an innovative federated learning framework designed to address the challenges of platforms with limited communication and computational resources. FedEXT employs a unique factorization-based methodology complemented by a paired neural network extension to improve model transmission and local training efficiency. This approach enables FedEXT to achieve the accuracy of larger models, which would otherwise be infeasible for federated learning on edge devices with stringent communication budgets. A key component of FedEXT is its periodic scheduler, which works in conjunction with local and global controllers to effectively manage model transmission. This scheduler uses a distinct learning and aggregation strategy to update the model parameters on local edge devices and the cloud server. The effectiveness of the framework is validated in both dataset-driven and real-world scenarios, demonstrating significant efficacy. FedEXT achieves improvements in accuracy ranging from 16% to 22% and a reduction in energy consumption by 29% to 34% compared to several state-of-the-art methods.}
}


@inproceedings{DBLP:conf/infocom/MaGZ025,
	author = {Mulei Ma and
                  Chenyu Gong and
                  Liekang Zeng and
                  Yang Yang},
	title = {Multi-Tier Multi-Node Scheduling of {LLM} for Collaborative {AI} Computing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044698},
	doi = {10.1109/INFOCOM55648.2025.11044698},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MaGZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Language Models (LLMs) have attracted growing attention owing to their advanced capability in under-standing and reacting to instructions. While they are experiencing wide deployment in the multi-tier cloud-edge architecture, their performance is severely constrained by the network capacity, and how to schedule efficient data flow for performance maximum poses significant challenges. Towards that, this paper establishes a comprehensive system model and, for the first time, formulates the efficient LLM scheduling problem in the multi-tier cloud-edge network. Given its non-convexness, we propose the Multi-tier Multi-node Scheduling of LLM (MMSL) algorithm for Collabo-rative AI Computing, a two-stage scheduling framework designed to optimize LLM inference in multi-tier cloud-edge networks. Initially, the inter-tier LLM automated decoupling and partitioning phase employs integer linear programming to allocate model size and computing demands efficiently. Subsequently, the intra-tier LLM task scheduling algorithm, leveraging GNN, identifies optimal scheduling nodes within each tier by evaluating resource utilization and network conditions. Extensive evaluations show that our solution significantly outperforms traditional scheduling methods by 9.1 %- 26.3% throughput improvement.}
}


@inproceedings{DBLP:conf/infocom/OuyangHZZWL025,
	author = {Tao Ouyang and
                  Guihang Hong and
                  Kongyange Zhao and
                  Zhi Zhou and
                  Weigang Wu and
                  Zhaobiao Lv and
                  Xu Chen},
	title = {AdaRAG: Adaptive Optimization for Retrieval Augmented Generation with
                  Multilevel Retrievers at the Edge},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044685},
	doi = {10.1109/INFOCOM55648.2025.11044685},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/OuyangHZZWL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Considering privacy concerns and real-time demands of popular large language models (LLMs), a shift towards edge-based LLM inference leverages edge clusters in proximity to provide low latency and secure responsiveness. To enhance the generation quality of LLMs, retrieval-augmented generation (RAG) can seamlessly integrate relevant external knowledge from local databases into LLMs without dedicated fine-tuning. However, this retrieval process can significantly contribute to overall latency, particularly in resource-constrained edge environments. To address this challenge, we introduce AdaRAG, tailored for edge-based RAG, leveraging multilevel (i.e., light and heavy) retrievers to facilitate adaptive retrieval granularity and efficient pipeline parallelism for retrieval and inference processes by fully exploiting heterogeneous edge resources (i.e., CPU and GPU). AdaRAG adaptively manages the heavy retrieval proportion and selected documents in augmented prompts, aiming to balance the long-term trade-off between overall generation quality and latency for dynamic user queries. Due to the inherent randomness of probabilistic LLM inference and highly dynamic queries at the edge, the underlying relations between the above decisions and performance feedback (i.e., end-to-end latency and accuracy) are difficult to obtain accurately a priori. Thus, we adopt bandit convex optimization to design a lightweight online algorithm, which utilizes real-time performance feedback to estimate the gradient information and optimize the retrieval and prompt decisions on the fly. Our rigorous theoretical analysis and extensive evaluations show our AdaRAG's superior performance. These promising results can boost the adoption of AdaRAG in future edge-based LLM applications.}
}


@inproceedings{DBLP:conf/infocom/WangC25,
	author = {Ruiqi Wang and
                  Guohong Cao},
	title = {Cooperative Traffic Map Construction in Vehicular Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044579},
	doi = {10.1109/INFOCOM55648.2025.11044579},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cooperative perception is fundamental to improving road safety and enabling advanced vehicle applications, where vehicles upload their perception data to the edge server, constructing a comprehensive traffic map that includes all road objects. This traffic map can help drivers be aware of traffic conditions beyond their line-of-sight and make appropriate decisions to avoid potential accidents. However, transmitting and processing large amount of perception data pose significant challenges in vehicular networks with limited bandwidth and computational capacity. Moreover, the GPS sensors might be inaccurate, introducing significant location errors to the traffic map. To address these challenges, we leverage depth images to identify the relevant areas so that only part of the perception data are uploaded and processed, reducing bandwidth consumption and computational burden on the edge server. Then, we construct the traffic map based on the detected objects. Since the same object might be perceived by multiple vehicles, resulting in different detected objects, these corresponding objects should be matched to maintain consistency. We design an efficient algorithm for real-time object matching and improve the location accuracy by transforming relative locations based on the matching. Extensive evaluations demonstrate that our system significantly reduce bandwidth consumption while achieving sub-meter level localization accuracy.}
}


@inproceedings{DBLP:conf/infocom/LiZWE25,
	author = {Mengning Li and
                  Haocheng Zhu and
                  Wenye Wang and
                  Eylem Ekici},
	title = {mSAC: Enhancing Localization with mmWave Sensing and Orthogonal Signals},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044619},
	doi = {10.1109/INFOCOM55648.2025.11044619},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiZWE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {mmWave technologies have been adopted into 5G and 6G systems. Current research primarily focuses on enhancing the quality of service and data throughput in mm Wave communications, as well as improving the accuracy of sensing from their respective functional perspectives. Consequently, this emphasis often overlooks the potential benefits of integrating communication and sensing, enhancing performance across both functionalities. This paper introduces mSAC, a novel mmWave-based system that combines sensing with communication via Orthogonal Frequency-Division Multiplexing (OFDM) signals. This approach differs from traditional mmWave sensing methods that rely on Frequency-Modulated Continuous Wave (FMCW) signals, and it substantially enhances both the accuracy and range of sensing. The mSAC system features a Joint Radar-Communication (JRC) transceiver and a communication receiver, incorporating a theoretical model that fuses sensing information from synchronous and asynchronous ends. This model is crucial for smart home applications such as controlling devices, monitoring environmental conditions, and enhancing security. Comprehensive evaluations demonstrate that mSAC achieves a 65.93% improvement in sensing accuracy and a 43.19% increase in sensing range compared to conventional methods, thereby setting a new benchmark for integrated sensing and communication systems in mmWave applications.}
}


@inproceedings{DBLP:conf/infocom/HuangM025,
	author = {Haiyang Huang and
                  Tianhui Meng and
                  Weijia Jia},
	title = {Joint Optimization of Prompt Security and System Performance in Edge-Cloud
                  {LLM} Systems},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044720},
	doi = {10.1109/INFOCOM55648.2025.11044720},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuangM025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have significantly facilitated human life, and prompt engineering has improved the efficiency of these models. However, recent years have witnessed a rise in prompt engineering-empowered attacks, leading to issues such as privacy leaks, increased latency, and system resource wastage. Though safety fine-tuning based methods with Reinforcement Learning from Human Feedback (RLHF) are proposed to align the LLMs, existing security mechanisms fail to cope with fickle prompt attacks, highlighting the necessity of performing security detection on prompts. In this paper, we jointly consider prompt security, service latency, and system resource optimization in Edge-Cloud LLM (EC-LLM) systems under various prompt attacks. To enhance prompt security, a vector-database-enabled lightweight attack detector is proposed. We formalize the problem of joint prompt detection, latency, and resource optimization into a multi-stage dynamic Bayesian game model. The equilibrium strategy is determined by predicting the number of malicious tasks and updating beliefs at each stage through Bayesian updates. The proposed scheme is evaluated on a real implemented EC-LLM system, and the results demonstrate that our approach offers enhanced security, reduces the service latency for benign users, and decreases system resource consumption compared to state-of-the-art algorithms.}
}


@inproceedings{DBLP:conf/infocom/LiuZYZWSL25,
	author = {Suyuan Liu and
                  Jingmiao Zhang and
                  Haikuo Yu and
                  Yan Zhang and
                  Yuetian Wang and
                  Guobin Shen and
                  Xiang{-}Yang Li},
	title = {AMoS: Autonomous Multimodal {POI} Standardization without Extra Annotation},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044731},
	doi = {10.1109/INFOCOM55648.2025.11044731},
	timestamp = {Fri, 25 Jul 2025 00:04:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuZYZWSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Providing persuasive descriptions of points of interest (POI) is crucial for ensuring the quality of location-based services such as food delivery. However, informal textual descriptions by customers and unreliable geographic coordinates from indoor devices make it challenging to standardize both textual and geospatial descriptions of queried POIs. Previous works often take a retrieve-then-rank approach, which has limited feasibility in real-world delivery scenarios due to their dependency on a vast POI database and the extensive labor required for labeling. In this work, we propose the AMoS system, which is based on the observation that records referring to the same POI are either similar in both semantic and geospatial domains, or at least in one of them. The AMoS system leverages inherent semantic and geospatial similarities within historical POI records, combining them in graph-based clustering to retrieve candidates for standardizing a given query. We also propose a standardization paradigm that combines structured formatting rules of POI descriptions with content diversity. We evaluate the performance on 22356 orders from a real-world online delivery platform. Our retrieval performance outperforms baselines by 16% in precision and remains competitive compared to a supervised approach. Our standardization accuracy reaches 90.24%.}
}


@inproceedings{DBLP:conf/infocom/ZhaoHGY0LLW25,
	author = {Zhixin Zhao and
                  Yitao Hu and
                  Ziqi Gong and
                  Guotao Yang and
                  Wenxin Li and
                  Xiulong Liu and
                  Keqiu Li and
                  Hao Wang},
	title = {Harpagon: Minimizing {DNN} Serving Cost via Efficient Dispatching,
                  Scheduling and Splitting},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044536},
	doi = {10.1109/INFOCOM55648.2025.11044536},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoHGY0LLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advances in deep neural networks (DNNs) have significantly contributed to the development of real-time video processing applications. Efficient scheduling of DNN workloads in cloud-hosted inference systems is crucial to minimizing serving costs while meeting application latency constraints. However, existing systems suffer from excessive module latency during request dispatching, low execution throughput during module scheduling, and wasted latency budget during latency splitting for multi-DNN applications, which undermines their capability to minimize the serving cost. In this paper, we design a DNN inference system called Harpagon, which minimizes the serving cost under latency constraints with a three-level design. It first maximizes the batch collection rate with a batch-aware request dispatch policy to minimize the module latency. It then maximizes the module throughput with multi-tuple configurations and proper amount of dummy requests. It also carefully splits the end-to-end latency into per-module latency budget to minimize the total serving cost for multi-DNN applications. Evaluation shows that Harpagon outperforms the state of the art by 1.49 to 2.37 times in serving cost while satisfying the latency objectives. Additionally, compared to the optimal solution using brute force search, Harpagon derives the lower bound of serving cost for 91.5% workloads with millisecond level runtime.}
}


@inproceedings{DBLP:conf/infocom/MishraMZN025,
	author = {Sachit Mishra and
                  Diego Madariaga and
                  Cezary Ziemlicki and
                  Diala Naboulsi and
                  Marco Fiore},
	title = {An Urban Geography of Mobile Application Usage: Connecting Demand
                  Dynamics and Urban Fabrics},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044701},
	doi = {10.1109/INFOCOM55648.2025.11044701},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MishraMZN025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The surge in usage of mobile applications generates a massive volume of traffic data exhibiting unique dynamics that are hard to unravel. In this work, we leverage factor analysis to pin down recurrent patterns of mobile traffic over the three dimensions of space, time and services in multi-city measurements of unprecedented resolution. We link the revealed structures of real-world mobile demands to urban fabrics, i.e., the combination of infrastructures and social characteristics that determine the functionality of an urban territory, hence establishing connections between specific city landscapes and the mobile application consumption they create. Our study provides new understanding about the diversity of mobile service dynamics in metropolitan areas, including insights on how economic status drives adoption of specific applications, how residential versus commercial areas create a dichotomy in applications usage, how private and public transports drive surges in the prevalence of different sets of applications, or how nightlife or university studies stimulate the utilization of specific classes of services.}
}


@inproceedings{DBLP:conf/infocom/XueDLFWZ25,
	author = {Jiawei Xue and
                  Chunhui Duan and
                  Fan Li and
                  Qihua Feng and
                  Ziang Wang and
                  Yinan Zhu},
	title = {Non-Intrusive Item Authentication with High Robustness for RFID-Enabled
                  Logistics},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044445},
	doi = {10.1109/INFOCOM55648.2025.11044445},
	timestamp = {Tue, 05 Aug 2025 22:40:41 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XueDLFWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RFID technology has found extensive applications in the realm of smart logistics, facilitating rapid package sorting and tracking. Harnessing the potential of RFID in addressing concerns related to logistics security holds significant promise and meaning. Past efforts either focused solely on the status of tags or package boxes, or interfered with the normal logistics processes. To detect the status of goods inside the packages and provide timely alerts for instances of loss, replacement, or damage, we propose RF-Express, a non-intrusive and anti-interference item authentication method. RF-Express enables seamless authentication of items during the logistics process, utilizing the pervasive RFIDs with almost no additional costs. Specifically, by tactfully arranging a pair of tags on each package and extracting representative features from the backscatter signals of the tags, we manage to discern the authenticity of the item with high precision across various multipath environments. Besides, a feature matching algorithm based on the triplet network is employed to further reinforce the robustness of the system. We implemented the RF -Express prototype on commercial devices and conducted plenty of experiments. It is demonstrated that RF-Express achieves a true acceptance rate of 90.97% and a true rejection rate of 94.38% on average under common attack scenarios.}
}


@inproceedings{DBLP:conf/infocom/TangZCE25,
	author = {Tao Tang and
                  Chaokun Zhang and
                  Gong Chen and
                  Jinlong E},
	title = {RoCooper: Robust Cooperative Perception Under Vehicle-to-Vehicle Communication
                  Impairments},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044682},
	doi = {10.1109/INFOCOM55648.2025.11044682},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TangZCE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cooperative perception facilitated by vehicle-to-vehicle (V2V) data sharing has emerged as a crucial enabler for safe and efficient autonomous driving. However, the current state-of-the-art algorithms are unable to resolve severe performance degradation caused by communication impairments in realistic V2V scenarios. This paper models the V2V communication quality and confirms their fragile robustness under loss conditions. To this end, we propose RoCooper, a robust cooperative perception framework. It leverages lossless ego feature as an anchoring foundation, utilizing the multi-dimensional correlations of the features and dynamic regional selective cross-learning to perform multi-scale feature recovery and judiciously fuse multi-view features from neighboring vehicles. Specifically, in this process, we create three components: (i) Augmentor, which leverages historical information to perform spatio-temporal feature enhancement to preliminarily restore corrupted received features; (ii) Aggregator, which performs selective dynamic partitioning of features across multiple scales, and utilizes cross-learning and multi-scale merging to capture multi-granularity feature information; (iii) BlockPrioritizer, which dynamically assigns block weights and enables regional selective cross-learning, facilitating regional recovery and multi-view fusion at multiple scales. Extensive evaluations of real-world datasets demonstrate that our method achieves state-of-the-art performance in varying impairment scenarios.}
}


@inproceedings{DBLP:conf/infocom/Huang0SZ000WX25,
	author = {Teng Huang and
                  Han Ding and
                  Wenxin Sun and
                  Cui Zhao and
                  Ge Wang and
                  Fei Wang and
                  Kun Zhao and
                  Zhi Wang and
                  Wei Xi},
	title = {One Snapshot is All You Need: {A} Generalized Method for mmWave Signal
                  Generation},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044473},
	doi = {10.1109/INFOCOM55648.2025.11044473},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Huang0SZ000WX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless sensing systems, particularly those using mm Wave technology, offer distinct advantages over traditional vision-based approaches, such as enhanced privacy and effectiveness in poor lighting conditions. These systems, leveraging FMCW signals, have shown success in human-centric applications like localization, gesture recognition, and so on. However, comprehensive mm Wave datasets for diverse applications are scarce, often constrained by pre-processed signatures (e.g., point clouds or RA heatmaps) and inconsistent annotation formats. To overcome these limitations, we propose mmGen, a novel and generalized framework tailored for full-scene mmWave signal generation. By constructing physical signal transmission models, mmGen synthesizes human-reflected and environment-reflected mm Wave signals from the constructed 3D meshes. Additionally, we incorporate methods to account for material properties, antenna gains, and multipath reflections, enhancing the realism of the synthesized signals. We conduct extensive experiments using a prototype system with commercial mm Wave devices and Kinect sensors. The results show that the average similarity of Range-Angle and micro-Doppler signatures between the synthesized and real-captured signals across three different environments exceeds 0.91 and 0.89, respectively, demonstrating the effectiveness and practical applicability of mmGen.}
}


@inproceedings{DBLP:conf/infocom/ZhengHLLDZ25,
	author = {Jian Zheng and
                  Huawei Huang and
                  Yinqiu Liu and
                  Taotao Li and
                  Hong{-}Ning Dai and
                  Zibin Zheng},
	title = {Justitia: An Incentive Mechanism Towards the Fairness of Cross-Shard
                  Transactions},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044722},
	doi = {10.1109/INFOCOM55648.2025.11044722},
	timestamp = {Tue, 05 Aug 2025 22:40:41 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhengHLLDZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A cross-shard transaction (CTX) is parsed into two sub-transactions, which are then executed in the source and destination shards, respectively. However, the problem is that the client who submits the original transaction only pays one unit of the transaction fee. Thus, sub-transactions will experience much higher queueing delays than regular intra-shard transactions when they wait in shard transaction pools. This is unfair for those original transactions that will be parsed into sub-transactions from the perspective of a sharded blockchain. Therefore, how to ensure fairness for all CTXs while securing the atomicity of any pair of sub-transactions becomes a critical challenge. State-of-the-art solutions addressed the transaction atomicity challenge, but the literature still lacks a dedicated incentive mechanism to ensure the fairness of CTXs. To this end, we propose an incentive mechanism named Justitia, which aims to achieve fairness by motivating blockchain proposers to prioritize the CTXs queueing in transaction pools when they package transactions to generate a new block. We rigorously analyze that Justitia upholds the fundamental properties of a sharded blockchain, including security, atomicity, and fairness. We then implement a prototype of Justitia on an open-source sharding-enabled blockchain testbed. Our experiments using historical Ethereum transactions demonstrate that i) Justitia guarantees fairness while processing CTXs, ii) its token-issuance mechanism does not lead to unstable economic inflation, and iii) Justitia only yields 20%-80% of queueing latency for CTXs upon comparing with Monoxide protocol.}
}


@inproceedings{DBLP:conf/infocom/TashtarianDLMGK25,
	author = {Farzad Tashtarian and
                  Mahdi Dolati and
                  Daniele Lorenzi and
                  Mojtaba Mozhganfar and
                  Sergey Gorinsky and
                  Ahmad Khonsari and
                  Christian Timmerer and
                  Hermann Hellwagner},
	title = {{ALPHAS:} Adaptive Bitrate Ladder Optimization for Multi-Live Video
                  Streaming},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044484},
	doi = {10.1109/INFOCOM55648.2025.11044484},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TashtarianDLMGK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Live streaming routinely relies on the Hypertext Transfer Protocol (HTTP) and content delivery networks (CDNs) to scalably disseminate videos to diverse clients. A bitrate ladder refers to a list of bitrate-resolution pairs, or representations, used for encoding a video. A promising trend in HTTP-based video streaming is to adapt not only the client's representation choice but also the bitrate ladder during the streaming session. This paper examines the problem of multi-live streaming, where an encoding service coordinates CDN-assisted bitrate ladder adaptation for multiple live streams delivered to heterogeneous clients in different zones via CDN edge servers. We design ALPHAS, a practical and scalable system for multi-live streaming that accounts for CDNs' bandwidth constraints and encoders' computational capabilities and also supports stream prioritization. ALPHAS, aware of both video content and streaming context, seamlessly integrates with the end-to-end streaming pipeline and operates in real time transparently to clients and encoding algorithms. We develop a cloud-based ALPHAS implementation and evaluate it through extensive real-world and trace-driven experiments against four prominent baseline approaches that encode each stream independently. The evaluation shows that ALPHAS outperforms the baselines, improving quality of experience, end-to-end latency, and per-stream processing by up to 23%, 21 %, and 49%, respectively.}
}


@inproceedings{DBLP:conf/infocom/CuiDC25,
	author = {Yue Cui and
                  Ningning Ding and
                  Man Hon Cheung},
	title = {AoI-Aware Federated Unlearning for Streaming Data with Online Client
                  Selection and Pricing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044760},
	doi = {10.1109/INFOCOM55648.2025.11044760},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/CuiDC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Models trained on static datasets often fail to adapt to evolving streaming data, leading to significant accuracy degradation. Federated unlearning can address this by removing outdated data and updating the model with fresh data. However, limited bandwidth prevents all clients from acquiring fresh data in a time-varying environment. Thus, the server must optimally select a subset of clients to update their data in an online manner and compensate them for their costs. To address these challenges, we first propose an efficient federated unlearning algorithm for streaming data and theoretically characterize the model optimality gap as a function of client selection with heterogeneous data freshness and criticality. This allows us to formulate a stochastic optimization problem to minimize the unlearned model loss and total payment. Using Lyapunov optimization, we derive an optimal client selection policy with a closed-form threshold that condenses clients' multi-dimensional heterogeneity into a one-dimensional metric. Furthermore, we model clients' asking prices for fresh data collection as a non-cooperative game and derive its closed-form Nash Equilibrium. Experimental results on a real dataset show that our proposed mechanism reduces the server's cost by up to 32.31% compared to two state-of-the-art baselines.}
}


@inproceedings{DBLP:conf/infocom/ChenYWCHS25,
	author = {Jiahui Chen and
                  Yiding Yu and
                  Libo Wang and
                  Ying Chen and
                  Tianchi Huang and
                  Lifeng Sun},
	title = {Enhanced Bandwidth Measurement and Robust Rate Adaptation for Low-Latency
                  Live Streaming},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044628},
	doi = {10.1109/INFOCOM55648.2025.11044628},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenYWCHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low latency live streaming (LLLS) like LL-DASH has significantly reduced the end-to-end latency via chunked transfer encoding (CTE). However, LLLS also comes with more challenges for adaptive bitrate (ABR) algorithms: (1) bandwidth measurement is non-trivial and inaccurate due to the possible idle time between chunks in CTE; (2) the various uncertainty in LLLS such as fluctuating segment size further lead to inaccurate buffer estimation, severely degrading ABR's performance. In this paper,  w w e propose AAR which comprises two modules: (1) accurate bandwidth measurement via server-side Flag parameter to identify the burst chunks within a segment, which allows for more consecutive valid HTTP chunks; (2) an LLLS tailored ABR with a novel robust objective that maximizes the minimum quality of experience (QoE) brought by the uncertainty. To obtain the minimum QoE, we propose a theorem based on the upper bound of download time estimation, which is backed up by theoretical guarantees. To derive the maximum QoE, we propose a new LLLS state evolution mechanism and apply Model Predictive Controller (MPC) to search for optimal bitrates. Extensive real-world experiments demonstrate that AAR outperforms existing baselines with 10%-80% measurement error reduction, and QoE improves by 39%-104% throughout all considered network conditions.}
}


@inproceedings{DBLP:conf/infocom/00040Y25,
	author = {Tingting Li and
                  Ziming Zhao and
                  Jianwei Yin},
	title = {Fortuna: Towards Efficient Selection of High-Fidelity Link for Quantum
                  Network in the Wild},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044624},
	doi = {10.1109/INFOCOM55648.2025.11044624},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/00040Y25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an emerging technology, quantum networks have the potential to revolutionize secure communication and data transmission technologies. In the Noisy Intermediate-Scale Quantum (NISQ) era, quantum noise causing low fidelity remains challenging in quantum networks. Existing quantum link selection schemes make some assumptions on the fidelity distribution, such as arithmetic sequence, which limits their practicality in the real world. In this paper, we propose Fortuna, an efficient selection algorithm for the high-fidelity link in the wild without assumptions about the fidelity distribution. We design the tailored link exploration strategy and link selection probability based on the coefficient of variation and Thompson sampling, to cope with the exploration-exploitation trade-off dilemma in the Multi-Armed Bandit (MAB) problem (for problem modeling). Extensive experiments involve six generated and one real-world (extracted from 10 IBMQ machines) fidelity distributions, we demonstrate Fortuna significantly outperforms three representative methods under four types of quantum noises. For instance, Fortuna reduces > 90 % bounces (among 1500 candidate links) and decreases the fidelity deviation in 1~4 order of magnitude.}
}


@inproceedings{DBLP:conf/infocom/0001J00L00025,
	author = {Zhuo Ma and
                  Jiayu Jin and
                  Yang Liu and
                  Yilong Yang and
                  Xinjing Liu and
                  Teng Li and
                  Junwei Zhang and
                  Jianfeng Ma},
	title = {SafeLead: Detecting and Excluding Random {STS} Attack in {UWB} Ranging
                  System},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044672},
	doi = {10.1109/INFOCOM55648.2025.11044672},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001J00L00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-wideband (UWB) technology employs ex-tremely short pulses in wireless communication, which enhances signal transmission security and accuracy, making it ideal for spatial awareness. However, recent research indicates that, with random scrambling timestamp sequences (STS), attackers can confuse the leading edge detection for 802.1S.4z authentication, and then forge a shorter ranging result than the actual distance. While existing defenses focus on detection, follow-up countermeasures remain blank. Intuitively discarding signals containing intensive attacks may cause a sharply decrease in ranging frequency or further service interruptions inadvertently facilitating unauthorized access. To address this issue, we propose SafeLead, a novel leading edge detection algorithm designed to exclude attacks and revive legitimate leading edges from attacked signals. For attack detection, SafeLead introduces a new thresh-old based on the variance of leading edges to avoid uncertainties from environmental changes. Furthermore, SafeLead leverages correlations between sub-templates to exclude disordered leading edges and revive legitimate timestamps. Simulations demonstrate that SafeLead achieves optimal performance with over 90 % attack detection rate and 90 % ranging accuracy, representing an 87% accuracy improvement compared to state-of-the-art methods.}
}


@inproceedings{DBLP:conf/infocom/DuY25,
	author = {Wei Du and
                  Bo Yang},
	title = {Channel-Adaptive Denoising Diffusion Models for Reliable Semantic
                  Communications},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044531},
	doi = {10.1109/INFOCOM55648.2025.11044531},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DuY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communication (SC) aims to deliver the profound meaning of information, in which deep-learning-based joint source-channel coding (DeepJSCC) is usually utilized to enable end-to-end communications. DeepJSCC-based SC frame-works perform well in wireless image transmission tasks, particularly under channel conditions with low signal-to-noise ratios (SNRs). However, existing methods still cannot adaptively eliminate different levels of channel noises, limiting the adaptability and effectiveness of DeepJSCC-based SC frameworks. To this end, we design a Channel-Adaptive Denoising Diffusion Model in the SC framework (SC-CAD2M), which can adaptively learn the transmitted semantic features under uneven noise distributions. Specifically, we design the Noise-Adaptive Mask (NAM) module to adjustably reconstruct semantic features across various SNR conditions. Furthermore, we incorporate NAM and CAD2M into the DeepJSCC-based SC framework, which are capable of adaptively predicting and eliminating a diverse range of noises from received semantic features across different channel conditions. We conduct extensive simulations to evaluate the algorithm's performance. Results demonstrate the SC-CAD2M framework's superiorities and adaptabilities for image transmission tasks in dynamic and complex communication scenarios.}
}


@inproceedings{DBLP:conf/infocom/00010LXPC25,
	author = {Yanru Chen and
                  Yuanyuan Zhang and
                  Xin Lin and
                  Tongzhou Xu and
                  Zhiwen Pan and
                  Liangyin Chen},
	title = {ZKP-CapBAC: Capability-Based Access Control via On-Chain Zero-Knowledge
                  Proofs for Cross-Domain Hiding Delegation Tree},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044772},
	doi = {10.1109/INFOCOM55648.2025.11044772},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/00010LXPC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Among various access control mechanisms, Capability-based access control (CapBAC) stands out as a promising research direction for enhancing system security through its token-based management of minimal permissions, dynamic access rights adjustments, and granular, flexible control over access permissions. However, as distributed networks grow in popularity, more security challenges appear. To address them and further advance CapBAC mechanism, this work proposes ZKP-CapBAC for cross-domain hiding delegation tree, a novel CapBAC mechanism integrating Zero-knowledge Proof (ZKP) and blockchain (on-chain) technology into CapBAC, tailored for distributed environments. The proposed ZKP-CapBAC not only secures single-domain capabilities (creation, authorization, verification, deletion), but also secures cross-domain capabilities of verification by utilizing ZKP to effectively hide the delegation tree, allowing for secure and private management of capabilities across domains without requiring cross-domain administrators to share the tree's details. This enhances security and efficiency of capability management in decentralized, transparent environments. Experimental results show that ZKP-CapBAC offers superior access control security with minimal time overhead and excels in practical environments. It outperforms existing mechanisms across five key metrics: monotonicity, flexible capability authorization, capability hiding, delegation tree hiding, and joint capability verification. This work advances the robustness and applicability of existing CapBAC mechanisms, marking a substantial advancement in access control technology.}
}


@inproceedings{DBLP:conf/infocom/ZhouWCLZ25,
	author = {Ke Zhou and
                  Shuai Wang and
                  Li Chen and
                  Dan Li and
                  Shuhan Zhang},
	title = {{SAIP:} Accurate Detection of Anycast Servers with the Rise of Regional
                  Anycast},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044640},
	doi = {10.1109/INFOCOM55648.2025.11044640},
	timestamp = {Wed, 23 Jul 2025 07:40:13 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouWCLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IP anycast enables multiple servers to distribute the service load by using a common IP address. Regional anycast is an emerging trend for IP anycast service providers because it allows users to reach geographically close service end-points, improving performance. There has been continuous effort in anycast detection, as anycast service providers usually do not disclose their anycast addresses. However, with the rise of regional anycast, we find that prior methods face severe accuracy challenges due to distance inflation and routing dynamics. In this paper, we propose SAIP, a spoofing-based anycast IP address detection method, to accurately detect anycast servers in the presence of regional anycast servers. SAIP uses IP spoofing to enable cross-catchment communication and identifies different anycast replicas based on indicators such as TCP connection states. Our experimental results on the ground truth dataset show that SAIP achieves 100% recall and accuracy in discovering regional anycast prefixes, while iGreedy only identifies 83.9% of them. And SAIP also achieves a 100 % recall rate for all anycast prefixes, compared to iGreedy's 93.2%. Through Internet-wide measurements, SAIP discovers 13,465 anycast IP/24 prefixes, 11.5% more than the existing largest anycast prefix dataset. We provide public access to this data at https://ki3.org.cn/#/datasetDetail?dataset=anycas.}
}


@inproceedings{DBLP:conf/infocom/0003CZ25,
	author = {Zichen Xu and
                  Xiaoliang Chen and
                  Zuqing Zhu},
	title = {INT-Assisted Adaptive Packet Scheduling in {PDP} Switches for End-to-End
                  Latency Control},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044732},
	doi = {10.1109/INFOCOM55648.2025.11044732},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0003CZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In today's Internet, ensuring the end-to-end (E2E) latency of delay-sensitive flows is becoming increasingly challenging due to the rapid growth of network services, users and traffic volume. In this work, we propose a packet scheduling scheme that leverages in-band network telemetry (INT) to catch packets' forwarding status in realtime and realizes fine-grained and adaptive packet scheduling accordingly for E2E latency control, and implement it in P4-based programmable data plane switches (PDP-SWs), Specifically, we use INT to make a packet convey its own realtime status (e.g., experienced latency) as it traverses the network, and program PDP-SWs in the network to schedule the packet with the weighted round-robin (WRR) approach based on the realtime status, such that the packet's quality-of-service (QoS) demand on E2E latency can be satisfied in the most effective way. We design the packet processing procedure in PDP-SW, propose an effective packet scheduling algorithm, and optimize the algorithm to ensure that it can be realized with P4 programs. The proposed scheme is then implemented and evaluated in a real-world network testbed built with hardware PDP-SWs. Experimental results confirm the effectiveness of our proposal and suggest that it achieves better control of E2E latency than a few existing benchmarks.}
}


@inproceedings{DBLP:conf/infocom/LiLT0WY25,
	author = {Hongjia Li and
                  Leshui Lv and
                  Ding Tang and
                  Yan Zhang and
                  Weiping Wang and
                  Xinghua Yang},
	title = {VaniKG: Vanishing Key Gradient Attack and Defense for Robust Federated
                  Aggregation},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044620},
	doi = {10.1109/INFOCOM55648.2025.11044620},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiLT0WY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The AGgregation Algorithms (AGAs) that combine locally trained models into a single global model in Federated Learning (FL) is becoming a new attack vector for adversaries. Model Poisoning Attacks (MPAs) are the most notorious repre-sentative; it aims to hamper the accuracy of the jointly trained model through manipulating byzantine FL clients' model updates to deviate the aggregated model from the global optimum. To defeat MPAs, the robust AGAs become prevailing in academia. In this paper, we present a new type of MPA against robust AGAs, referred to as Vanishing Key Gradient attack (VaniKG). In VaniKG, byzantine FL clients first formulate the perturbation vector by inactivating key neurons of one/multiple layer(s) through vanishing their gradients, and then confuse the vector to a population of most benign clients' updates. Through extensive experiments, we show that VaniKG can disable 6 state-of-the-art robust AGAs and sabotage the accuracy. To defeat VaniKG and stealthy MPAs, we enhance robust AGAs by proposing the Diverse Client Selection (DCS) scheme, where byzantine clients with overly consistent gradients are avoided from being all selected. Finally, we demonstrate that DCS plus classical AGAs can guarantee the accuracy at a normal level when FL suffers with VaniKG and classical MPAs.}
}


@inproceedings{DBLP:conf/infocom/ChenCWW25,
	author = {Tai{-}You Chen and
                  Wei{-}Neng Chen and
                  Feng{-}Feng Wei and
                  Yang Wang},
	title = {Decentralized Evolutionary Optimization for Multi-Target Tracking
                  and Data Association with Bearing-only Measurements},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044752},
	doi = {10.1109/INFOCOM55648.2025.11044752},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenCWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Measurement-to-measurement (M2M) association is significant for multi-target tracking when the measurement data of a single sensor is incomplete, like bearing measurements. However, most existing methods for M2M association are processed in a central node, which makes it difficult to meet the real-time requirements of dynamic tracking. In this paper, we propose a decentralized evolutionary optimization method (DeEvo) for multi-target tracking with bearing-only measurements. First, considering the difficulty of negotiating mixed-integer variables with constraints in the decentralized environment, DeEvo processes continuous and discrete variables of M2M association in a bi-level optimization manner to reduce the negotiation complexity. To be specific, differential evolution is used to optimize the target locations in the continuous space, while the Kuhn-Munkres algorithm is used to find the optimal data association in the discrete space. Second, to promote the cooperation and consensus of sensors, a momentum-based population consensus strategy is designed for the co-evolution of neighboring sensors. Finally, to improve the real-time efficiency of DeEvo, a population injection strategy is proposed based on trajectory prediction with the Kalman filter. Experiments show that DeEvo achieves higher tracking accuracy and real-time efficiency than the existing centralized and distributed algorithms.}
}


@inproceedings{DBLP:conf/infocom/MeiDTGY25,
	author = {Zhimin Mei and
                  Donghui Dai and
                  Jingyu Tong and
                  Zheng Gong and
                  Lei Yang},
	title = {Repurposing Optical Mice for Acoustic Eavesdropping},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044584},
	doi = {10.1109/INFOCOM55648.2025.11044584},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MeiDTGY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic eavesdropping presents a longstanding challenge in the realm of personal information security and privacy preservation. In this work, we introduce a novel eavesdropping method called JerryAttack, which repurposes an optical mouse as a covert eavesdropping device. Specifically, we transform the mouse's integrated low-resolution but high-frame-rate image sensor into a high-speed camera for visual vibrometry, capable of capturing acoustic vibrations from nearby loudspeakers. Our contributions are threefold: First, we utilize the ‘pixel grabber’ register as a backdoor to extract the pixel stream from the image sensor. Second, we establish an acoustic-optical side channel that enables effective acoustic eavesdropping. Third, we thoroughly explore two attack scenarios: voice profiling and speech reconstruction. Our findings reveal that the sound recovered through our side channel achieves a mean SNR of 7.3dB, comparable to standard microphone recordings in noisy environments like cafes. Additionally, when combined with a classification neural network, JerryAttack identifies individuals with an overall accuracy of 83.27% across six languages. Moreover, when cooperated with joint channel information, JerryAttack consistently achieves good intelligibility, with a median STOI score exceeding 0.7 in reconstructed results.}
}


@inproceedings{DBLP:conf/infocom/SuWYZJZ025,
	author = {Shaojie Su and
                  Jiasheng Wu and
                  Zijie Ying and
                  Zhiyuan Zhao and
                  Xiangyu Jia and
                  Wenjun Zhu and
                  Yue Gao},
	title = {SkyOctopus: Enabling Low-Latency Mobile Satellite Network Through
                  Multiple Anchors},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044594},
	doi = {10.1109/INFOCOM55648.2025.11044594},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SuWYZJZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid deployment of low earth orbit (LEO) satellite constellations has drawn attention to the potential of non-terrestrial networks (NTN) in providing global communication services. Telecom operators are attempting to collaborate with satellite network providers to develop mobile satellite networks, which serve as an effective supplement to terrestrial networks. However, current mobile satellite network architectures still employ the single-anchor design of terrestrial mobile networks, leading to severely circuitous routing for users and significantly impacting their service experience. To reduce unnecessary latency caused by circuitous routing and provide users with low-latency global internet services, this paper presents SkyOctopus, an advanced multi-anchor mobile satellite network architecture. SkyOctopus innovatively deploys traffic classifiers on satellites to enable connections between users and multiple anchor points distributed globally. It guarantees optimal anchor point selection for each user's target server by monitoring multiple end-to-end paths. We build a prototype of SkyOctopus using enhanced Open5GS and UERANSIM, which is driven by actual LEO satellite constellations such as Starlink, Kuiper, and OneWeb. We conduct extensive experiments, and the results demonstrate that, compared to standard 5G NTN and two other existing schemes, SkyOctopus can reduce end-to-end latency by up to 53%.}
}


@inproceedings{DBLP:conf/infocom/YuanCLPZHXL025,
	author = {Haoxuan Yuan and
                  Zhe Chen and
                  Zheng Lin and
                  Jinbo Peng and
                  Yuhang Zhong and
                  Xuanjie Hu and
                  Songyan Xue and
                  Wei Li and
                  Yue Gao},
	title = {Constructing 4D Radio Map in {LEO} Satellite Networks with Limited
                  Samples},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044496},
	doi = {10.1109/INFOCOM55648.2025.11044496},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YuanCLPZHXL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, Low Earth Orbit (LEO) satellite networks (i.e., non-terrestrial network (NTN)), such as Starlink, have been successfully deployed to provide broader coverage than terrestrial networks (TN). Due to limited spectrum resources, TN and NTN may soon share the same spectrum. Therefore, fine-grained spectrum monitoring is crucial for spectrum sharing and interference avoidance. To this end, constructing a 4D radio map (RM) including three spatial dimensions and signal spectra is important. However, this requires the large deployment of sensors, and high-speed analog-to-digital converters for extensive spatial signal collection and wide power spectrum acquisition, respectively. To address these challenges, we propose a deep unsupervised learning framework without ground truths labeling requirement, DeepRM, comprised of neural compressive sensing (CS) and tensor decomposition (TD) algorithms. Firstly, we map the CS process into the optimization of a neural networks-associated loss function, and design a sparsity-performance balance training algorithm to reconstruct a wide power spectrum under limited sub-N quist samples. Secondly, according to the output of neural CS algorithm, we also utilize neural networks to perform TD, and construct the 3D RM for each frequency, even under very sparse sensor deployment. Extensive evaluations show that DeepRM achieves lower error than its corresponding state-of-the-art baselines, especially with limited samples.}
}


@inproceedings{DBLP:conf/infocom/0008D25,
	author = {Hongbo Li and
                  Lingjie Duan},
	title = {Theory of Mixture-of-Experts for Mobile Edge Computing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044567},
	doi = {10.1109/INFOCOM55648.2025.11044567},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0008D25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mobile edge computing (MEC) networks, mobile users generate diverse machine learning tasks dynamically over time. These tasks are typically offloaded to the nearest available edge server, by considering communication and computational efficiency. However, its operation does not ensure that each server specializes in a specific type of tasks and leads to severe overfitting or catastrophic forgetting of previous tasks. To improve the continual learning (CL) performance of online tasks, we are the first to introduce mixture-of-experts (MoE) theory in MEC networks and save MEC operation from the increasing generalization error over time. Our MoE theory treats each MEC server as an expert and dynamically adapts to changes in server availability by considering data transfer and computation time. Unlike existing MoE models designed for offline tasks, ours is designed to handle continuous streams of tasks in the MEC environment. We introduce an adaptive gating network in MEC to adaptively identify and route newly arrived tasks of unknown data distributions to available experts, enabling each expert to specialize in a specific type of task upon convergence. We derived the minimum number of experts required to match each task with a specialized, available expert. Our MoE approach consistently reduces the overall generalization error over time, unlike the traditional MEC approach. Interestingly, when the number of experts is sufficient to ensure convergence, adding more experts delays the convergence time and worsens the generalization error. Finally, we perform extensive experiments on real datasets in deep neural networks (DNNs) to verify our theoretical results.}
}


@inproceedings{DBLP:conf/infocom/0003LLX25,
	author = {Si Wu and
                  Guantian Lin and
                  Patrick P. C. Lee and
                  Yinlong Xu},
	title = {Leveled Product Codes for Optimal Block Repairs in Geo-distributed
                  Storage Systems},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044776},
	doi = {10.1109/INFOCOM55648.2025.11044776},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0003LLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To provide fault tolerance with low storage overhead, modern geo-distributed storage systems use erasure coding to stripe data redundancy across geographical regions. The prevalence of node, rack, and region failures motivates the needs for both single-block and multi-block repairs, yet block repairs trigger substantial cross-rack and cross-region data transfers. We propose a new family of erasure codes, Leveled Product Codes (LPCs), by adapting the classical Product Codes designed for disk arrays into geo-distributed storage systems. LPCs localize single-block repairs within racks and optimize multi-block repairs with the minimum sum of cross-rack and cross-region data transfers, while providing fault tolerance against node, rack, and region failures. We theoretically prove the optimality of LPCs, and further implement LPCs in a distributed storage prototype. Our numerical analysis and testbed evaluation show that LPCs significantly reduce the single-block and multi-block repair times of state-of-the-art hierarchy-aware erasure codes.}
}


@inproceedings{DBLP:conf/infocom/TrombettiAB25,
	author = {Federico Trombetti and
                  Viviana Arrigoni and
                  Novella Bartolini},
	title = {Distributed Network Tomography for Failure Localization},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044548},
	doi = {10.1109/INFOCOM55648.2025.11044548},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TrombettiAB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In today's complex and interconnected networks, efficiently and accurately identifying performance degradation and failures is essential for service management, application performance, and network reconnaissance. Network Tomography can infer the state of the internal elements of the network via end-to-end measurements from the periphery of the network. Existing solutions rely on a centralized approach where the measures taken by the monitors are processed and analyzed by a central unit, or they assume that monitors can share all the information gained by communicating through a different, independent and non-faulty network. In this paper, we present D2NeT (Distributed and Dynamic Network Tomography) an innovative approach to network monitoring that extends the principles of network tomography by enabling a real-time and decentralized monitoring solution. We propose a framework based on distributed interactions among monitoring nodes, incorporating an advanced Bayesian decision-making support system. We validate our solution by testing it on emulated networks showing that it outperforms other baseline and state-of-the-art solutions in static failure scenarios. We then considered D2NeT in a dynamic scenario with ongoing failures and restorations, demonstrating its exceptional ability to promptly detect sudden changes in the nodes' state.}
}


@inproceedings{DBLP:conf/infocom/LiangDM0X25,
	author = {Xiaoyu Liang and
                  Haohua Du and
                  Wen Ma and
                  Ye Tian and
                  Xiaoya Xu},
	title = {CiDer: {A} Black-box Approach to Classify Node with Certified Robustness
                  Guarantees},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044537},
	doi = {10.1109/INFOCOM55648.2025.11044537},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiangDM0X25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the outstanding performance of graph node classification in tasks such as detecting illegal nodes in transaction networks, adversarial attacks aiming to perturb classification results have proliferated. Although current defenses based on randomized smoothing have shown some effectiveness, these approaches still require modifications to the classification model to ensure accuracy. Here, we propose a novel approach - CiDer, that theoretically guarantees the robustness of graph node classification results in a black-box setting, which means no assumptions on the form of attack and the classification model. The key idea behind our approach is to leverage the denoise capability of diffusion models on features to perform adversarial purification on the data. We then prove this stochastic purification method can ensure certified robustness under certain attack budgets. Our extensive experiments corroborate our theory and demonstrate that node classifiers worked with CiDer achieve significantly superior performance compared to state-of-the-art, e.g., the accuracy improves by 7% on Cora and the optimal result improves by 30% on PubMed.}
}


@inproceedings{DBLP:conf/infocom/KimPL25,
	author = {Bom Kim and
                  Hyeonjun Park and
                  Seungsoo Lee},
	title = {{KUBETEUS:} An Intelligent Network Policy Generation Framework for
                  Containers},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044592},
	doi = {10.1109/INFOCOM55648.2025.11044592},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/KimPL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Containers have become the standard for delivering cloud-native services by taking advantage of their scalability, portability, and resource efficiency. However, particularly in network policies, they have also become major targets for various security attacks that exploit misconfigurations and vulnerabilities. Especially in complex cloud-native environments, manually managing network policies is prone to errors, and existing studies that automate policy generation often have limitations in accuracy. In this paper, we present KUBETEUS,a highly automated, intelligent network policy generation framework. Our system operates in an intent-driven manner, enhanced by natural language processing (NLP) and fine-tuned Large Language Models (LLMs), enabling the generation of network policies without needing to understand complex configurations. Furthermore, our system devises a multi-stage validation process to fundamentally prevent misconfigurations in network policy enforcement. The evaluation of KUBETEUS demonstrates its effectiveness, with the most improved fine-tuned LLM achieving a 360% increase in BLEU score and a 233% increase in ROUGE-2 score compared to the baseline model. We believe that the approach presented in this paper is applicable to the wide range of container-native policy platforms in used today, and that its broader adoption will help address more complex security policy generation concerns.}
}


@inproceedings{DBLP:conf/infocom/GurushankarSS25,
	author = {Keerthana Gurushankar and
                  Noah G. Singer and
                  Bernardo Subercaseaux},
	title = {Latency Guarantees for Caching with Delayed Hits},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044511},
	doi = {10.1109/INFOCOM55648.2025.11044511},
	timestamp = {Sat, 09 Aug 2025 12:13:52 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GurushankarSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the classical caching problem, when a requested page is not present in the cache (i.e., a “miss”), it is assumed to travel from the backing store into the cache before the next request arrives. However, in many real-life applications, such as content delivery networks, this assumption is unrealistic. The delayed-hits model for caching, introduced by Atre, Sherry, Wang, and Berger, accounts for the latency between a missed cache request and the corresponding arrival from the backing store. This theoretical model has two parameters: the delay  Z Z , representing the ratio between the retrieval delay and the inter-request delay in an application, and the cache size  k k , as in classical caching. Classical caching corresponds to  Z = 1 Z=1 , whereas larger values of  Z Z  model applications where retrieving missed requests is expensive. Despite the practical relevance of the delayed-hits model, its theoretical underpinnings are still poorly understood. We present the first tight theoretical guarantee for optimizing delayed-hits caching: The “Least Recently Used” algorithm, a natural, deterministic, online algorithm widely used in practice, is  O ( Z k ) O(Zk)  -competitive, meaning it incurs at most  O ( Z k ) O(Zk)  times more latency than the (offline) optimal schedule. Our result extends to any so-called “marking” algorithm.}
}


@inproceedings{DBLP:conf/infocom/ZhaiLL25,
	author = {Yi Zhai and
                  Junzhou Luo and
                  Jianrui Liu},
	title = {{NRCAC:} Non-Intrusive Microservice Root Cause Analysis Framework
                  for Cloud Providers},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044716},
	doi = {10.1109/INFOCOM55648.2025.11044716},
	timestamp = {Tue, 05 Aug 2025 22:40:41 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaiLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In modern containerized cloud environments, even minor performance issues in microservice systems can significantly impact overall functionality, highlighting the critical need for cloud providers to implement efficient Root Cause Analysis (RCA) to meet SLA requirements. Existing RCA frameworks often rely on intrusive data collection techniques and algorithms requiring extensive computation, which are not only privacy-invasive but also induce considerable performance overhead and predominantly focus on tenant-specific issues, making them less suitable for cloud providers. This paper investigates whether it is possible for cloud providers to perform RCA focusing on eliminating intrusiveness and reducing overhead. To this end, we propose NRCAC, a novel RCA framework designed specifically for cloud providers. NRCAC includes eCollection, a non-intrusive data collection method utilizing eBPF, allowing cloud providers to gather data from the host kernel efficiently without directly accessing microservice applications. Additionally, NRCAC features RCD-DK, a high-performance root cause analysis algorithm which reduces the search space of causal graphs by incorporating domain-specific insights. Evaluation with both synthetic data and real-world microservice systems demonstrates the effectiveness of NRCAC in accurately identifying the root causes of anomalies, outperforming existing frameworks by 46% to 172%.}
}


@inproceedings{DBLP:conf/infocom/Luo0LS25,
	author = {Ziyue Luo and
                  Jia Liu and
                  Myungjin Lee and
                  Ness B. Shroff},
	title = {Prediction-Assisted Online Distributed Deep Learning Workload Scheduling
                  in {GPU} Clusters},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044761},
	doi = {10.1109/INFOCOM55648.2025.11044761},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Luo0LS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent explosive growth of deep learning (DL) models has necessitated a compelling need for efficient job scheduling for distributed deep learning training with mixed parallelisms (DDLwMP) in GPU clusters. This paper proposes an adaptive shortest-remaining-processing-time-first (A-SRPT) scheduling algorithm, a novel prediction-assisted online scheduling approach designed to mitigate the challenges associated with DL cluster scheduling. By modeling each job as a graph corresponding to heterogeneous Deep Neural Network (DNN) models and their associated distributed training configurations, A-SRPT strategically assigns jobs to the available GPUs, thereby minimizing inter-server communication overhead. Observing that most DDLwMP jobs recur, A-SRPT incorporates a random forest regression model to predict training iterations. Crucially, A-SRPT maps the complex scheduling problem into a single-machine instance, which is addressed optimally by a preemptive “shortest-remaining-processing-time-first” strategy. This optimized solution serves as a guide for actual job scheduling within the GPU clusters, leading to a theoretically provable competitive scheduling efficiency. We conduct extensive real-world testbed and simulation experiments to verify our proposed algorithms.}
}


@inproceedings{DBLP:conf/infocom/WangZZYJLJS25,
	author = {Xinyu Wang and
                  Wangqiu Zhou and
                  Hao Zhou and
                  Tianjian Yang and
                  Shenyao Jiang and
                  Zhi Liu and
                  Yusheng Ji and
                  Qi Song},
	title = {Fast and Anti-starvation Charging Device Grouping for Magnetic Wireless
                  Power Transfer},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044582},
	doi = {10.1109/INFOCOM55648.2025.11044582},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangZZYJLJS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Magnetic resonant coupling (MRC) enabled wireless power transfer (WPT) can support charging multiple receiver (RX) devices simultaneously without the need for precise alignment. However, the charging performance of MRC-WPT systems is severely limited by impedance mismatching issues. RX grouping has been identified as an effective way to solve this problem, but has not been extensively researched. In this paper, we propose CharGroup, a fast and anti-starvation RX grouping mechanism that maximizes charging power in MRC-WPT systems. We first leverage the group-level impedance decomposability to obtain the equivalent impedance for all the RX groups rapidly by using a limited number of RX-independent measurements, and then filter them via impedance matching. Next, we use a two-level decomposition approach to capture detailed power distribution among RXs inside each group candidate. Finally, we perform time resource allocation among the candidates to ensure proportional fairness charging at the RX-level. We design and implement the CharGroup prototype, and conduct extensive experiments. The results validate the effectiveness of our scheme, i.e., CharGroup can achieve 462.67% and 21.06% average charging performance improvements over two state-of-the-art baselines, respectively, while guaranteeing RX-level proportional fairness.}
}


@inproceedings{DBLP:conf/infocom/Li0WZ0X025,
	author = {Xiaocan Li and
                  Kun Xie and
                  Jigang Wen and
                  Guangxing Zhang and
                  Wei Liang and
                  Gaogang Xie and
                  Kenli Li},
	title = {Joint Neural Matrix Completion for Multi-Attribute Mobile Crowd Sensing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044771},
	doi = {10.1109/INFOCOM55648.2025.11044771},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Li0WZ0X025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowd Sensing (MCS) has recently emerged as a new paradigm for urban sensing systems. However, the incomplete sensory data prevent the large-scale deployment of MCS and compromise the accuracy of sensing systems. To obtain the complete sensory data, recent studies propose matrix completion-based methods to estimate the missing entries based on a small portion of observed sensory data. Although promising, as MCS applications usually require collecting multi-attribute sensory data, simply applying the matrix completion to each attribute individually will result in a low estimation accuracy. We aim to study a novel problem of accurately estimating multi-attribute sensory data even though some attributes have a very low sampling ratio. Firstly, we design a neural spatial-temporal matrix completion (NSTMC) model to exploit both nonlinear spatial and temporal features in sensory data to improve the estimation accuracy of a single attribute. Then, based on NSTMC, to well exploit the correlations among multiple attributes and preserve the specific knowledge within each attribute, we further propose a joint spatial-temporal matrix completion (JSTMC) model, which includes a low-rank representation based temporal feature sharing strategy and a Laplacian regularizer based spatial feature sharing strategy. The extensive experimental results demonstrate that, by exploiting the correlations among multiple attributes, our JSTMC can accurately estimate the missing entries in sensory data even when the sampling ratio is 10%.}
}


@inproceedings{DBLP:conf/infocom/RenLLD025,
	author = {Yidong Ren and
                  Gen Li and
                  Yimeng Liu and
                  Younsuk Dong and
                  Zhichao Cao},
	title = {AeroEcho: Towards Agricultural Low-power Wide-area Backscatter with
                  Aerial Excitation Source},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044614},
	doi = {10.1109/INFOCOM55648.2025.11044614},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/RenLLD025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) plays a pivotal role in advancing smart agriculture. Leveraging LoRa backscatter technology greatly enhances energy efficiency in agricultural IoT. However, cost and scalability issues prevent reliable coverage of extensive agricultural areas. In this paper, we introduce AeroE-cho, a novel system that integrates aerial excitation sources and backscatter tags to address these challenges and enable efficient agricultural IoT. Firstly, we co-design the excitation source and tag with a customized packet format to enable decoding for multiple tags. Secondly, we propose excitation cells to achieve optimal throughput and symbol error rate. Finally, we devise two aerial routing strategies to optimize system energy efficiency and coverage reliability for arbitrary agricultural sensor deployments. AeroEcho is realized using customized low-cost hardware, signal processing via software-defined radio on TV white space spectrum, and evaluated in real-world scenarios. Results demonstrate that AeroEcho enables concurrent trans-mission of 71 tags with less than 1 % bit error rate using the same non-linear chirp in a single channel, achieving a lOx higher transmission concurrency compared to existing methods. Furthermore, AeroEcho enhances the overall throughput of current backscatter transmission by 5.84 x and individual tag data rate by 12 x compared to state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/infocom/HuangP25,
	author = {Zhiming Huang and
                  Jianping Pan},
	title = {Adversarial Semi-Bandits with Moving Arms},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044659},
	doi = {10.1109/INFOCOM55648.2025.11044659},
	timestamp = {Tue, 12 Aug 2025 21:29:13 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuangP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies a novel multi-agent combinatorial bandit problem called moving semi-bandits involving  K K  agents and  N N  arms, extending the problem of semi-bandits with adversar-ial rewards and stochastic arm availabilities (sleeping semi-bandits). The arms move across agents, making each arm available to at most one agent at a time, and the set of available arms for each agent changes over time. In each round, each agent plays up to  m m  arms from their own available arm set simultaneously and observes the random loss for each played arm (i.e., semi-bandit feedback). The loss of each arm has no stochastic assumptions, and different agents may generate different random losses for each arm. The primary goal is to minimize the cumulative loss for all agents through collaboration. This bandit problem is motivated by real-world applications, such as traffic scheduling in wireless networks with multiple access points and task assignment for multiple crowdsourcing platforms. To address this challenge, we propose an efficient framework called Moving-FTPL, which guarantees a regret bound of  O ( N N T K I n T − − − − − − − − √ ) O(N\\sqrt{NTK\\mathrm{I}\\mathrm{n}T})  over  T T  rounds. Moving-Ftplcan reduce the total regret of all K agents by a factor of  K − − √ \\sqrt{K}  compared to scenarios where agents do not collaborate. Additionally, Moving-FTPL takes a step forward for the long-standing problems of a tighter regret bound for sleeping semi-bandits by significantly improving the state-of-the-art regret bound by a factor of  m N − − √ m\\sqrt{N}  and imnroving the bound for sleeping adversarial bandits by a factor of  N − − √ \\sqrt{N} . Furth ermore, we showcase a crowdsourcing application to demonstrate the effectiveness of our proposed algorithm when compared with others.}
}


@inproceedings{DBLP:conf/infocom/LiHZZWL25,
	author = {Yuting Li and
                  Shaoyuan Huang and
                  Tengwen Zhang and
                  Cheng Zhang and
                  Xiaofei Wang and
                  Victor C. M. Leung},
	title = {Sentinel: Scheduling Live Streams with Proactive Anomaly Detection
                  in Crowdsourced Cloud-Edge Platforms},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044648},
	doi = {10.1109/INFOCOM55648.2025.11044648},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiHZZWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of live streaming services, Crowdsourced Cloud-edge service Platforms (CCPs) are playing an increasingly important role in meeting the increasing demand. Although stream scheduling plays a critical role in optimizing CCPs' revenue, most optimization strategies struggle to achieving practical results due to the various anomalies in unstable CCPs. Additionally, the substantial scale of CCPs magnifies the difficulties of anomaly detection in time-sensitive scheduling. To tackle these challenges, this paper proposes Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a strategy pool; in the post-scheduling stage, upon request arrival, it triggers an appropriate scheduling based on pre-strategy to implement the scheduling process. Extensive experiments on realistic datasets show that Sentinel significantly reduces the anomaly frequency by 70 %, improves revenue by 74%, and 2.0× the scheduling speed.}
}


@inproceedings{DBLP:conf/infocom/ChiariottiMBP25,
	author = {Federico Chiariotti and
                  Andrea Munari and
                  Leonardo Badia and
                  Petar Popovski},
	title = {Distributed Optimization of Age of Incorrect Information with Dynamic
                  Epistemic Logic},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044748},
	doi = {10.1109/INFOCOM55648.2025.11044748},
	timestamp = {Tue, 05 Aug 2025 22:40:40 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChiariottiMBP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed medium access schemes have a key advantage in anomaly tracking applications, as individual sensors know their own observations and can exploit them to reduce their Age of Incorrect Information (AoII). However, the risk of collisions has so far limited their performance. We present Dynamic Epistemic Logic for Tracking Anomalies (DELTA), a medium access protocol that limits collisions and minimizes AoII in anomaly reporting over dense networks. This is achieved by a process of inferring AoII from plain Age of Information (AoI). In a network scenario with randomly generated anomalies, the individual AoII for each sensor is known only to itself, but all nodes can infer its  A A oI by simply tracking the transmission process. Thus, we adopt an approach based on dynamic epistemic logic, which allows individual nodes to infer how their  A A oII values rank among the entire network by exploiting public information such as the  A A oI and the identity of transmitting nodes. We analyze the resulting DELTA protocol both from a theoretical standpoint and with Monte Carlo simulation, showing that our approach is significantly more efficient and robust than basic random access, while outperforming state-of-the-art scheduled schemes by at least 30%.}
}


@inproceedings{DBLP:conf/infocom/ZhaoZL25,
	author = {Ding Zhao and
                  Xinyu Zhang and
                  Myungjin Lee},
	title = {SatPipe: Deterministic {TCP} Adaptation for Highly Dynamic {LEO} Satellite
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044600},
	doi = {10.1109/INFOCOM55648.2025.11044600},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-Earth-Orbit (LEO) satellite networks (satnets) hold significant potential for providing global internet access with high throughput and low latency. However, the high mobility of the satellites and the associated handovers cause high dynamics at the link layer, which in turn degrades end-to-end network throughput and stability. In this work, we first present a measurement profiling of the handover behaviors of the Starlink satnet, so as to better understand its dynamics and accurately determine handover timings with millisecond precision. We then introduce Satpipe, a new mechanism to enhance TCP and make it robust against satnet dynamics. Satpipe exposes the link layer handover schedule to the TCP sender, which then adapts to the link interruptions in a deterministic rather than trial-and-error manner. Our implementation and experiments over Starlink indicate that Satpipe delivers an average throughput gain of 9.4% to 38%, achieves up to a 127.8% enhancement in the 10% lower throughput, and exhibits a 24.7% reduction in retransmission ratio, compared to the state-of-the-art TCP BBR. This advantage further propagates to the application layer, leading to a 10.8% increase in bitrate and 33.5% reduction in rebuffering time for video streaming applications.}
}


@inproceedings{DBLP:conf/infocom/GuYLWX25,
	author = {Huayue Gu and
                  Ruozhou Yu and
                  Zhouyu Li and
                  Xiaojian Wang and
                  Guoliang Xue},
	title = {QuESat: Satellite-Assisted Quantum Internet for Global-Scale Entanglement
                  Distribution},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044649},
	doi = {10.1109/INFOCOM55648.2025.11044649},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GuYLWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entanglement distribution across remote distances is critical for many quantum applications. Currently, the de facto approach for remote entanglement distribution relies on optical fiber for on-the-ground entanglement distribution. However, the fiber-based approach is incapable of global-scale entanglement distribution due to intrinsic limitations. This paper investigates a new hybrid ground-satellite quantum network architecture (QuESat) for global-scale entanglement distribution, integrating an on-the-ground fiber network with a global-scale passive optical network built with low-Earth-orbit satellites. The satellite network provides dynamic construction of photon lightpaths based on near-vacuum beam guides constructed via adjustable arrays of lenses, forwarding photons from one ground station to another with very high efficiency over long distances compared to using fiber. To assess the feasibility and effectiveness of QuESat for global communication, we formulate lightpath provisioning and entanglement distribution problems, considering the orbital dynamics of satellites and the time-varying entanglement demands from ground users. A two-stage algorithm is developed to dynamically configure the beam guides and distribute entanglements, respectively. The algorithm combines randomized and deterministic rounding for lightpath provisioning to enable global connectivity, with optimal entanglement swapping for distributing entanglements to meet users' demands. By developing a ground-satellite quantum network simulator, QuESat achieves multi-fold improvements compared to repeater networks.}
}


@inproceedings{DBLP:conf/infocom/Wang0Z025,
	author = {Junkai Wang and
                  Xutong Liu and
                  Jinhang Zuo and
                  Yuedong Xu},
	title = {Robust Contextual Combinatorial Multi-Armed Bandits for Unreliable
                  Network Systems},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044618},
	doi = {10.1109/INFOCOM55648.2025.11044618},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Wang0Z025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Combinatorial multi-armed bandit (CMAB) is a fundamental framework widely used in networked systems to maximize cumulative rewards under uncertainty. Real-world applications such as federated learning and content delivery network often involve feedback that may be corrupted due to adversarial attacks or network disruptions. In this paper, we study contextual CMAB ( C 2 \\mathrm{C}^{2}  MAB) with adversarial corruptions, where feedback for base arms within any selected super arms can be corrupted by an adversary. We focus on  L 1 L_{1}  -norm smooth reward function and both  L 1 L_{1}  and  L ∞ L_{\\infty}  -norm corruption measures, establishing tight regret upper bounds for each scenario. Additionally, we provide the first lower bounds for  C 2 \\mathrm{C}^{2}  MAB under corruptions, confirming the optimality of our proposed algorithm. To broaden the applicability, we further extend our algorithm to a more general  C 2 \\mathrm{C}^{2}  - MAB setting with probabilistically triggered arms. Empirical validation demonstrates significant improvements across synthetic and real-world datasets, with applications in contextual latency-critic federated learning, user-specific online content delivery and 360° VR video streaming.}
}


@inproceedings{DBLP:conf/infocom/CuiQWTY25,
	author = {Hengrui Cui and
                  Zhihao Qu and
                  Xinyu Wang and
                  Bin Tang and
                  Baoliu Ye},
	title = {{LCO-AGQ:} {A} Lightweight Client-Oriented Adaptive Gradient Quantization
                  Algorithm for Federated Learning},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044636},
	doi = {10.1109/INFOCOM55648.2025.11044636},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/CuiQWTY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantization is one common approach to achieve communication-efficient federated learning (FL) via compressing the gradients uploaded by clients. However, most existing approaches use a uniform quantization level, neglecting factors such as data quality and resource heterogeneity of clients. While some studies applied customized quantization strategies, they always introduce significant computation cost and overlook the impact of training convergence. To address these issues, we propose a novel algorithm called LCO-AGQ (Lightweight Client-Oriented Adaptive Gradient Quantization), which enables each client to adaptively select its quantization level based on its data quality and communication capability, without substantially increasing computation cost. The core idea of LCO-AGQ is modeling the relationship between quantization levels and the impact degree of client quality, which remains consistent across clients and adjacent training rounds. This allows the impact of quantization levels on training convergence to be estimated in a lightweight manner, and such impact is also demonstrated in our theoretical analysis. Consequently, we adjust each client's quantization level to achieve the efficiency of the FL system. Compared to state-of-the-art methods, our approach achieves comparable accuracy and overall training time while only using 62.11 % of the communication cost and 64.52% of the computation cost.}
}


@inproceedings{DBLP:conf/infocom/WenTHL025,
	author = {Chaozheng Wen and
                  Jingwen Tong and
                  Yingdong Hu and
                  Zehong Lin and
                  Jun Zhang},
	title = {{WRF-GS:} Wireless Radiation Field Reconstruction with 3D Gaussian
                  Splatting},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044513},
	doi = {10.1109/INFOCOM55648.2025.11044513},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WenTHL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless channel modeling plays a pivotal role in designing, analyzing, and optimizing wireless communication systems. Nevertheless, developing an effective channel modeling approach has been a longstanding challenge. This issue has been escalated due to the denser network deployment, larger antenna arrays, and wider bandwidth in 5G and beyond networks. To address this challenge, we put forth WRF-GS, a novel framework for channel modeling based on wireless radiation field (WRF) reconstruction using 3D Gaussian splatting. WRF-GS employs 3D Gaussian primitives and neural networks to capture the interactions between the environment and radio signals, enabling efficient WRF reconstruction and visualization of the propagation characteristics. The reconstructed WRF can then be used to synthesize the spatial spectrum for comprehensive wireless channel characterization. Notably, with a small number of measurements, WRF-GS can synthesize new spatial spectra within milliseconds for a given scene, thereby enabling latency-sensitive applications. Experimental results demonstrate that WRF-GS outperforms existing methods for spatial spectrum synthesis, such as ray tracing and other deep-learning approaches. Moreover, WRF-GS achieves superior performance in the channel state information prediction task, surpassing existing methods by a significant margin of more than 2.43 dB.}
}


@inproceedings{DBLP:conf/infocom/Chen0LSD25,
	author = {Fahao Chen and
                  Peng Li and
                  Tom H. Luan and
                  Zhou Su and
                  Jing Deng},
	title = {{SPIN:} Accelerating Large Language Model Inference with Heterogeneous
                  Speculative Models},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044522},
	doi = {10.1109/INFOCOM55648.2025.11044522},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Chen0LSD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Speculative decoding has been shown as an effective way to accelerate Large Language Model (LLM) inference by using a Small Speculative Model (SSM) to generate candidate tokens in a so-called speculation phase, which are subsequently verified by the LLM in a verification phase. However, current state-of-the-art speculative decoding approaches have three key limitations: handling requests with varying difficulty using homogeneous SSMs, lack of robust support for batch processing, and insufficient holistic optimization for both speculation and verification phases. In this paper, we introduce Spin, an efficient LLM inference serving system based on speculative decoding, designed to address these challenges through three main innovations. First, SPIN improves token speculation by using multiple heterogeneous SSMs, with a learning-based algorithm for SSM selection that operates without prior knowledge of request difficulty. Second, SPIN employs a request decomposition method to minimize batching overhead during LLM verification. Finally, SPIN orchestrates speculation and verification phases by pipelining their executions on GPUs to achieve further acceleration. Experimental results demonstrate that SPIN significantly outperforms state-of-the-art methods, achieving a performance increase of approximately 2.28×.}
}


@inproceedings{DBLP:conf/infocom/0001GCLY025,
	author = {Minghui Xu and
                  Hechuan Guo and
                  Ye Cheng and
                  Chunchi Liu and
                  Dongxiao Yu and
                  Xiuzhen Cheng},
	title = {EC-Chain: Cost-Effective Storage Solution for Permissionless Blockchains},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044729},
	doi = {10.1109/INFOCOM55648.2025.11044729},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001GCLY025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Permissionless blockchains face considerable challenges due to increasing storage demands, driven by the proliferation of Decentralized Applications (DApps). This paper introduces EC-Chain, a cost-effective storage solution for permissionless blockchains. EC-Chain reduces storage overheads of ledger and state data, which comprise blockchain data. For ledger data, EC-Chain refines existing erasure coding-based storage optimization techniques by incorporating batch encoding and height-based encoding. We also introduce an easy-to-implement dual-trie state management system that enhances state storage and retrieval through state expiry, mining, and creation procedures. To ensure data availability in permissionless environments, EC-Chain introduces a network maintenance scheme tailored for dynamism. Collectively, these contributions allow EC-Chain to provide an effective solution to the storage challenges faced by permissionless blockchains. Our evaluation demonstrates that EC-Chain can achieve a storage reduction of over 90% compared to native Ethereum Geth.}
}


@inproceedings{DBLP:conf/infocom/ZhouXHOLC25,
	author = {Zhi Zhou and
                  Jiajie Xie and
                  Mengke Huang and
                  Tao Ouyang and
                  Fangming Liu and
                  Xu Chen},
	title = {Towards Federated Inference: An Online Model Ensemble Framework for
                  Cooperative Edge {AI}},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044578},
	doi = {10.1109/INFOCOM55648.2025.11044578},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouXHOLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge inference leverages edge computing devices for the last mile delivery of artificial intelligence (AI) services. To meet the latency requirements while overcoming the resource limitations, edge inference systems deploy lightweight - typically compressed - DNN models. However, due to data drift during deployment, these compressed edge models often fail to deliver satisfactory and stable inference accuracy. To address this issue, we propose a novel edge inference serving paradigm called Federated Inference. This approach, based on ensemble learning, groups multiple edge workers to form an ensemble, enhancing inference accuracy. A key challenge in Federated Inference is maximizing ensemble accuracy while adhering to resource budgets and Service Level Objectives (SLOs). The dynamic nature of the environment and the NP-hardness of the optimization problem add to the complexity. To address these challenges, we propose an online model ensemble framework that integrates online learning with approximate optimization, offering a theoretically rigorous and computationally efficient solution. We have implemented a prototype of our framework and, through extensive test-bed evaluations, demonstrate that it improves average inference accuracy by  12 % ∼ 80 % 12\\% \\sim 80\\% .}
}


@inproceedings{DBLP:conf/infocom/WangDYHYY25,
	author = {Shanyue Wang and
                  Yuxin Ding and
                  Yubo Yan and
                  Feiyu Han and
                  Dawei Yan and
                  Panlong Yang},
	title = {MegaScatter: Large-Scale and Ubiquitous Backscatter Network via Multi-Domain
                  Fusion},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044516},
	doi = {10.1109/INFOCOM55648.2025.11044516},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangDYHYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of the Internet of Things (IoT), the demand for low-power and concurrent transmission capabilities has surged. This paper presents MegaScatter, a large-scale and ubiquitous backscatter network that supports concurrent communication for thousands of backscatter tags demodulating with only two receiver antennas. The core idea of MegaScatter is to integrate Orthogonal Frequency-Division Multiple Access (OFDMA) and spatial-domain pseudo-orthogonal techniques to resolve signal collisions in tags' subcarrier transmissions. Additionally, it employs a time-domain training sequence for accurate channel state and frequency offset estimation and uses orthogonal codewords for harmonic interference cancellation. Our experimental prototype effectively decouples signal collisions within the same subcarrier, enabling 5x concurrent data transmission using only two antennas. Simulations show that the system supports concurrent transmission from 9800 tags across 980 subcarriers, achieving a total throughput of 543.9 Mbps with a 2-antenna receiver. Compared to state-of-the-art systems, these advancements represent a 10× increase in concurrency and a 9.3× improvement in throughput.}
}


@inproceedings{DBLP:conf/infocom/ZhangLLQL0Y25,
	author = {Zutao Zhang and
                  Zeyu Luan and
                  Qing Li and
                  Zhuyun Qi and
                  Kejun Li and
                  Yong Jiang and
                  Zhenhui Yuan},
	title = {SentinelX: {A} Lightweight Malicious Traffic Detection System Based
                  on Programmable Switches},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044759},
	doi = {10.1109/INFOCOM55648.2025.11044759},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangLLQL0Y25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, programmable switches have emerged as robust platforms for deploying high-performance network services to detect malicious traffic. However, current researches face several challenges: firstly, the flow tables generated by model deployment are cumbersome; secondly, existing unsupervised methods have difficulty handling repetitive traffic; and thirdly, the flow-level inference is coarse-grained and susceptible to attacks. To address these challenges, we propose SentinelX, which offers several advancements. Initially, we design a space-saving multi-level flow table representation method. We then introduce TreeDivider, an innovative model-splitting algorithm that achieves significant space reductions of up to 63.88% after only two subdivisions. Next, we propose DualTree, a hardware-specific unsupervised decision tree utilizing a dual threshold mode, which enhances detection accuracy by approximately 30.24%. Finally, we design a fine-grained method for determining the inference point, boosting the detection rate of bypass attacks by 30.03%. Extensive experiments on the H3C S9830-32H-H1 switch demonstrate that SentinelX can reach 99.99% of the maximum bandwidth of switch ports with nanosecond-level latency, approximately 1.38 times the delay of L3 (network layer) base forwarding.}
}


@inproceedings{DBLP:conf/infocom/Chou0WW0Z0025,
	author = {Yi Ching Chou and
                  Long Chen and
                  Hengzhi Wang and
                  Feng Wang and
                  Hao Fang and
                  Haoyuan Zhao and
                  Miao Zhang and
                  Xiaoyi Fan},
	title = {Commercial Dishes Can Be My Ladder: Sustainable and Collaborative
                  Data Offloading in {LEO} Satellite Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044527},
	doi = {10.1109/INFOCOM55648.2025.11044527},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Chou0WW0Z0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth Orbit (LEO) satellite networks, characterized by their high data throughput and low latency, have gained significant interest from both industry and academia. Routing data efficiently within these networks is essential for maintaining a high quality of service. However, current routing strategies, such as bent-pipe and inter-satellite link (ISL) routing, have their unique challenges. The bent-pipe strategy requires a dense deployment of dedicated ground stations, while the ISL-based strategy can negatively impact satellite battery lifespan due to increased traffic load, leading to sustainability issues. In this paper, we propose sustainable collaborative offloading, a framework that orchestrates groups of existing commercial resources like ground stations and 5G base stations for data offloading. This orchestration enhances total capacity, overcoming the limitations of a single resource. We propose the collaborator group set construction algorithm to construct candidate groups and the collaborator selection and total payment algorithm to select offloading targets and determine payments no less than the costs. Extensive real-world-based simulations show that our solution significantly improves energy consumption, satellite service life, and end-to-end latency.}
}


@inproceedings{DBLP:conf/infocom/Martinez-Durive25,
	author = {Orlando E. Mart{\'{\i}}nez{-}Durive and
                  Jos{\'{e}} Su{\'{a}}rez{-}Varela and
                  Jes{\'{u}}s Omana Iglesias and
                  Andra Lutu and
                  Marco Fiore},
	title = {An Evaluation of {RAN} Sustainability Strategies in Production Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044549},
	doi = {10.1109/INFOCOM55648.2025.11044549},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Martinez-Durive25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reducing energy consumption is a primary goal for the mobile telecommunication industry, with strong environmental and economic implications. The main target for savings is the Radio Access Network (RAN), which is responsible for more than 70% of the total energy costs incurred by operators. Lowering energy costs at the RAN is possible by reducing the number of active carriers at off-peak locations and times where the demand can be served with a lower capacity than deployed. While the scientific community has been proposing a plethora of complex solutions to switch-off underutilized carriers, production networks largely rely nowadays on threshold-based strategies that run at individual RAN equipment and are typically enabled only overnight. Moreover, there are no real-world evaluations of the effectiveness of carrier switch-off approaches in reducing energy consumption or their impact on the end users. In this paper, we benchmark five fixed threshold-based cell sleep policies deployed in a production network serving large geographical regions. The study provides unprecedented insights on industry-grade RAN sustainability at scale, in terms of actual energy savings and trade-offs with user experience. Our insights suggest that the capability of the tested policies in reducing the energy costs hits a clear barrier if no degradation is admissible for any user, and provides a strong empirical basis in support of more flexible approaches to save energy at the RAN.}
}


@inproceedings{DBLP:conf/infocom/GuiNGZRH25,
	author = {Jiaping Gui and
                  Mingjie Nie and
                  Jinyao Guo and
                  Futai Zou and
                  Mati Ur Rehman and
                  Wajih Ul Hassan},
	title = {A Principled Approach for Detecting APTs in Massive Networks via Multi-Stage
                  Causal Analytics},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044565},
	doi = {10.1109/INFOCOM55648.2025.11044565},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GuiNGZRH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting Advanced Persistent Threats (APTs) in large enterprise networks with conventional Network Intrusion Detection Systems (NIDS) is challenging due to the stealthy, multi-stage, and long-running nature of APTs. This paper introduces Netguardian, a novel NIDS utilizing a comprehensive methodology to correlate anomalies across APT stages. By merging real traffic with simulated APT scenarios, Netguardian creates a detailed training dataset for enhanced anomaly detection. Netguardian implements custom models for each APT stage, extracting specific traffic features, such as periodicity and failed connections, to identify anomalies. These anomalies are then correlated to reconstruct attack paths. Our system leverages these paths to assign threat scores based on interconnected anomalies matching known APT progression, effectively prioritizing suspicious paths. Evaluation on a large dataset of enterprise network traffic merged with simulated APTs along with the DARPA OpTC dataset shows that Netguardian detects various APT stages with high accuracy and low false positives, outperforming state-of-the-art (SOTA) NIDS.}
}


@inproceedings{DBLP:conf/infocom/0002ZJLW25,
	author = {Junqi Ma and
                  Fusang Zhang and
                  Beihong Jin and
                  Siheng Li and
                  Zhi Wang},
	title = {MULoc: Towards Millimeter-Accurate Localization for Unlimited {UWB}
                  Tags via Anchor Overhearing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044652},
	doi = {10.1109/INFOCOM55648.2025.11044652},
	timestamp = {Sat, 16 Aug 2025 16:39:53 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0002ZJLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have seen rapid advancements in ultra-wideband (UWB)-based localization systems. However, most existing solutions offer only centimeter-level accuracy and support a limited number of UWB tags, which fails to meet the growing demands of emerging sensing applications (e.g., virtual reality). This paper presents MULoc, the first system that can localize an unlimited number of UWB tags with millimeter-level accuracy. At the core of MULoc is the innovative use of UWB phase, which can provide finer-grained distance measurement than traditional time-of-f1ight (ToF) estimates. To accurately obtain phase estimates from unsynchronized devices, we introduce a novel localization scheme called anchor overhearing (AO) and eliminate raw signal errors through a signal-difference-based technique. For precise tag localization, we resolve phase ambiguity by combining a fusion-based filtering method and frequency hopping. We implement MULoc on commercial UWB modules. Extensive experiments demonstrate that our system achieves a median localization error of 0.47 mm and 90-th percentile error of 1.02 cm, reducing the error of traditional method by 91.12%.}
}


@inproceedings{DBLP:conf/infocom/LiW00Z25,
	author = {Weiyang Li and
                  Ning Wang and
                  Chuan Ma and
                  Tao Xiang and
                  Kai Zeng},
	title = {DroneMA: Drone Mobility Alignment Countering AI-Based Spoofing Attacks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044631},
	doi = {10.1109/INFOCOM55648.2025.11044631},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiW00Z25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement of generative AI models empowers adversaries to craft sophisticated spoofing signals that closely mimic legitimate transmissions at the physical layer. This development poses a significant challenge for UAV spoofing detection, particularly due to the severe constraints on drone resources. To address this challenge, we propose a novel mobility alignment-based spoofing detection framework for drones, termed DroneMA. This method leverages an observed phenomenon: the relative positions and movements of a drone with respect to an attacker and a legitimate ground control station (GCS) are inherently different. Consequently, the mobility characteristics of spoofing signals differ from those of legitimate signals. In DroneMA, the variation trend in the received signal strength indicator (RSSI) is employed to capture the mobility characteristics of drones, thereby facilitating practical implementation on off-the-shelf devices. To mitigate the inaccuracies inherent in RSSI, we introduce a feature alignment method that utilizes Z-score normalization and a Gated Recurrent Unit (GRU) network. Addressing the challenge of unpredictable attack models, we propose a detection scheme based on the Interquartile Range (IQR) method, enabling the development of an effective identification model using only positive samples. We conducted real-world experiments to evaluate the effectiveness of DroneMA, demonstrating a remarkable average accuracy of 92.78% across three typical flight scenarios.}
}


@inproceedings{DBLP:conf/infocom/Wang025,
	author = {Suyang Wang and
                  Yu Cheng},
	title = {Deep Learning-Augmented {SHS} Model for Accurate AoI Analysis in Heterogeneous
                  Unsaturated {CSMA} Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044711},
	doi = {10.1109/INFOCOM55648.2025.11044711},
	timestamp = {Tue, 05 Aug 2025 22:40:41 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Wang025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate analysis of the age of information (AoI) is crucial for timeliness-critical systems that often rely on carrier sense multiple access (CSMA) networks. Existing models that minimize the tagged node's AoI in CSMA networks, where nodes contend for channel access, perform well with near-saturated background traffic but struggle in heterogeneous unsaturated networks. To address this, we propose a deep learning (DL)-augmented stochastic hybrid systems (SHS) model for fast and precise AoI analysis. Our approach aggregates background nodes into a single virtual saturated group while reflecting their heterogeneous and unsaturated characteristics via the channel access rate. By leveraging DL, the access point (AP) augments the SHS model with locally monitored traffic, enabling the tagged node to achieve precise AoI analysis. This integration significantly improves SHS precision, leading to accurate AoI estimation in heterogeneous and unsaturated settings. Validation through 802.11-based simulations in the ns-3 simulator demonstrates our model's robustness and efficiency in practical CSMA network scenarios. Additionally, we introduce a joint evaluation metric to balance AoI and sampling cost, ensuring an optimal trade-off between information freshness and resource consumption.}
}


@inproceedings{DBLP:conf/infocom/DuZXYM25,
	author = {Caihui Du and
                  Rongrong Zhang and
                  Chaocan Xiang and
                  Jihong Yu and
                  Tao Ma},
	title = {R\({}^{\mbox{2}}\)Scatter: Long-Range Rapid {LTE} Backscatter Communication
                  Using Tunnel Diodes},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044742},
	doi = {10.1109/INFOCOM55648.2025.11044742},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DuZXYM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An LTE backscatter system allows a tag to detect and backscatter LTE signals where the tag data is modulated to the backscatter receiver. The existing LTE backscatter communication suffers from lower throughput and shorter communication range, hindering its application. In this paper, we introduce R2Scatter, the first higher-throughput LTE backscatter system with longer backscatter communication range of 18m. Our key innovations include: 1) A sample-level anchor position modu-lation (APM) scheme that improves the throughput. Unlike the previous systems using classical modulation schemes to convey tag data, we pick one of half-symbol signals from each LTE symbol and shift it to the backscatter channel, referred to as the anchor symbol, and utilize the anchor symbol position to signify more bits of the tag data. 2) A stronger RF front-end hardware design of the backscatter tag with 188µ W power consumption. Our design that uses a tunnel diode provides 18.8dB backscatter gain, enabling the tag to detect and synchronize with an LTE excitation signal under -60dBm. We build the prototype of the tag and conduct comprehensive experiments. Our results demonstrate that R2Scatter provides 2.7× higher throughput, 11.25× longer synchronization distance, and 6.4 × longer backscatter distance than the state-of-the-art LTE backscatter system CABLTE.}
}


@inproceedings{DBLP:conf/infocom/WangCD25,
	author = {Ting Wang and
                  Kai Cheng and
                  Xiao Du},
	title = {Multi-Task Reinforcement Learning for Collaborative Network Optimization
                  in Data Centers},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044699},
	doi = {10.1109/INFOCOM55648.2025.11044699},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangCD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As data center networks increasingly grow in complexity and scale, efficiently managing traffic scheduling and congestion control becomes crucial for optimizing network performance. Traditional single-task optimization strategies often fall short, failing to adequately address the interplay between different tasks and resulting in suboptimal performance with inefficiencies and robustness issues. To tackle these challenges, this paper proposes a novel Multi-Task Reinforcement Learning (MTRL)-based collaborative Network Optimization scheme, termed MTRLNO, which establishes a structured framework with central and edge systems (i.e., hosts and switches). The SDN-enabled central system incorporates an MTRL agent that simultaneously optimizes traffic scheduling and congestion control tasks, leveraging global network state information to formulate instructive optimization policies for edge systems. Switches implement decentralized multi-agent RL agents to facilitate automatic ECN tuning for congestion control, with the ability to handle incast issues. Hosts feature an MTRL-guided Multiple Level Feedback Queue (MLFQ) demotion threshold adjustment scheme for adaptive traffic scheduling. We further develop a Prioritized Experience Replay-based Soft Actor-Critic (PERSAC) algorithm to enhance learning efficiency and a customized multi-task learning algorithm via improved parameter-sharing to effectively adapt across multiple tasks. Experimental results demonstrate that MTRLNO significantly outperforms state-of-the-art approaches in terms of FCT, latency, and robustness across diverse network conditions.}
}


@inproceedings{DBLP:conf/infocom/ZhangA025,
	author = {Zuyuan Zhang and
                  Vaneet Aggarwal and
                  Tian Lan},
	title = {Network Diffuser for Placing-Scheduling Service Function Chains with
                  Inverse Demonstration},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044702},
	doi = {10.1109/INFOCOM55648.2025.11044702},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangA025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network services are increasingly managed by con-sidering chained-up virtual network functions and relevant traffic flows, known as the Service Function Chains (SFCs). To deal with sequential arrivals of SFCs in an online fashion, we must consider two closely-coupled problems - an SFC placement problem that maps SFCs to servers/links in the network and an SFC scheduling problem that determines when each SFC is executed. Solving the whole SFC problem targeting these two optimizations jointly is extremely challenging. In this paper, we propose a novel network diffuser using conditional generative modeling for this SFC placing-scheduling optimization. Recent advances in generative AI and diffusion models have made it possible to generate high-quality images/videos and decision trajectories from language description. We formulate the SFC optimization as a problem of generating a state sequence for planning and perform graph diffusion on the state trajectories to enable extraction of SFC decisions, with SFC optimization constraints and objectives as conditions. To address the lack of demonstration data due to NP-hardness and exponential problem space of the SFC optimization, we also propose a novel and somewhat maverick approach - Rather than solving instances of this difficult optimization, we start with randomly-generated solutions as input, and then determine appropriate SFC optimization problems that render these solutions feasible. This inverse demonstration enables us to obtain sufficient expert demonstrations, i.e., problem-solution pairs, through further optimization. In our numerical evaluations, the proposed network diffuser outperforms learning and heuristic baselines, by ~20% improvement in SFC reward and ~50% reduction in SFC waiting time and blocking rate.}
}


@inproceedings{DBLP:conf/infocom/KimYKS025,
	author = {Jinmyeong Kim and
                  Juheon Yi and
                  Wootack Kim and
                  Seokgyeong Shin and
                  Youngki Lee},
	title = {Combinational Point Sampling for Fast and Accurate On-Device LiDAR
                  3D Object Detection},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044601},
	doi = {10.1109/INFOCOM55648.2025.11044601},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/KimYKS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While recent LiDARs are evolving to higher distance range and angular resolution, the increase in point rate poses significant challenges in achieving low-latency 3D object detection. Although several recent studies aimed at point sampling for efficient point cloud sampling, they are limited in their application to 3D object detection on outdoor LiDAR scenes due to the high sampling overhead and downstream task accuracy drop. We present Cirrus, an end-to-end system for fast and accurate on-device 3D object detection with a novel combinational point sampling. To enable low-overhead and accuracy-preserving point sampling, we design lightweight sampling methods that effectively leverage the sampling opportunities in outdoor LiDAR point clouds (i.e., sparse object occupancy and redundant short-distance points) and synergistically combine them. Extensive evaluation over various edge devices, 3D object detection models, and datasets show that Cirrus effectively reduces the number of input points by 88%, achieving a 40% reduction in end-to-end latency and 35% decrease in energy consumption without accuracy drop.}
}


@inproceedings{DBLP:conf/infocom/LinJ0G25,
	author = {ChunChih Lin and
                  Chenxu Jiang and
                  Xiaonan Zhang and
                  Linke Guo},
	title = {Achieving Robust Resource Orchestration for Highly Dense Heterogeneous
                  IoT Systems},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044551},
	doi = {10.1109/INFOCOM55648.2025.11044551},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LinJ0G25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of the Internet of Things (IoT) has ushered in a wide array of emerging applications. Current mainstream IoT protocols for supporting the above applications, such as Wi-Fi, ZigBee, and Bluetooth, heavily overlap on the 2.4 GHz bands. When deploying those heterogeneous IoT devices with different wireless protocols in a limited geographic area, e.g., manufacturing warehouse and clinic rooms, inevitable packet collisions will occur due to their spectrum overlapping. Those unpredictable collisions will ultimately degrade network performance, mainly due to the lack of coordination across coexisting protocols. This paper revisits the classic resource orchestration problem in a practical wireless coexistence scenario with a dense indoor IoT deployment. We propose to leverage multi-protocol gateways, e.g., Amazon Echo, Google Nest Hub, and Samsung SmartThing Station, to develop a Multi-Agent Reinforcement Learning (MARL) framework, which jointly considers channel status and contextual information for orchestrating limited resources. Based on the protocol heterogeneity and diverse transmission requests, we design a novel resource pool to achieve fine-grained management of available resources, by which gateways can collaboratively decide the system-level optimal strategy. The proposed design will also feature a cascaded RL model to determine a sequential decision for best utilizing available resources. Based on extensive real-world experiments conducted on a Software-Defined Radio (SDR) platform with up to 33 IoT devices, our proposed framework achieves more than 2.19X in throughput. It reduces 69.07% of delay compared with current random-accessed mechanisms.}
}


@inproceedings{DBLP:conf/infocom/ChenYLLGC25,
	author = {Quan Chen and
                  Ming Yi and
                  Jing Li and
                  Ning Li and
                  Hong Gao and
                  Zhipeng Cai},
	title = {Optimal and Approximate Parallelism-Based Computation Offloading Algorithms
                  for Real-Time Multimodal Learning at the Edge},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044544},
	doi = {10.1109/INFOCOM55648.2025.11044544},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenYLLGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multimodal learning has been introduced as a popular learning paradigm that can integrate inputs from multimodal video data. To accelerate video analytics at the edge, video frames are usually scalarized and compressed into various resolutions to offload to the edge server to achieve a balance between accuracy and latency. In this paper, we investigate the problem of the Joint Schedule of Offloading decision and Resolution selection (JSOR) for real-time multimodal learning at the edge. Firstly, the parallelism between the computation and communication between the edge device and server is identified and modeled. Then, the problem of JSOR to maximize the accuracy while minimizing energy consumption under the latency constraints, is formulated and proved to be NP-hard. To the best of our knowledge, this is the first work that takes the parallelism during the offloading process into account for the JSOR problem. An optimal algorithm based on dynamic programming is proposed with a decision graph, which is constructed to integrate the offloading decision and resolution selection together with the processing latency. To further reduce the time complexity, several pruning strategies and an approximate algorithm are also proposed. Additionally, to maximize the long-term average utility, an adaptive online algorithm based on Lyapunov optimization and reinforcement learning is also proposed. Finally, through extensive simulations and real implementations on the NVIDIA Jetson AGX Orin platform, we demonstrated the effectiveness of the proposed algorithms in terms of accuracy and energy consumption.}
}


@inproceedings{DBLP:conf/infocom/0014CLZF025,
	author = {Xiang Liu and
                  Hau Chan and
                  Minming Li and
                  Xianlong Zeng and
                  Chenchen Fu and
                  Weiwei Wu},
	title = {{CARE:} Compatibility-Aware Incentive Mechanisms for Federated Learning
                  with Budgeted Requesters},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044535},
	doi = {10.1109/INFOCOM55648.2025.11044535},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0014CLZF025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a promising approach that allows requesters (e.g., servers) to obtain local training models from workers (e.g., clients). Since workers are typically unwilling to provide training services/models freely and voluntarily, many incentive mechanisms in FL are designed to incentivize participation by offering monetary rewards from requesters. However, existing studies neglect two crucial aspects of real-world FL scenarios. First, workers can possess inherent incompatibility characteristics (e.g., communication channels and data sources), which can lead to degradation of FL efficiency (e.g., low communication efficiency and poor model generalization). Second, the requesters are budgeted, which limits the amount of workers they can hire for their tasks. In this paper, we investigate the scenario in FL where multiple budgeted requesters seek training services from incompatible workers with private training costs. We consider two settings: the cooperative budget setting where requesters cooperate to pool their budgets to improve their overall utility and the non-cooperative budget setting where each requester optimizes their utility within their own budgets. To address efficiency degradation caused by worker incompatibility, we develop novel compatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both settings to elicit true private costs and determine workers to hire for requesters and their rewards while satisfying requester budget constraints. Our mechanisms guarantee individual rationality, truthfulness, budget feasibility, and approximation performance. We conduct extensive experiments using real-world datasets to show that the proposed mechanisms significantly outperform existing baselines.}
}


@inproceedings{DBLP:conf/infocom/RenLDYQN25,
	author = {Xiaoxu Ren and
                  Qixin Li and
                  Hongyang Du and
                  Haipeng Yao and
                  Chao Qiu and
                  Dusit Niyato},
	title = {Tri-Ring: Asynchronous Service Provisioning with Online Learning in
                  Edge Cloud Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044572},
	doi = {10.1109/INFOCOM55648.2025.11044572},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/RenLDYQN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Container-based microservices are ideal for service provisioning due to their lightweight deployment and fast startup, making them well-suited for resource-limited edge cloud networks. Furthermore, the inherent layered structure enables efficient layer scheduling and caching to address computing node resource constraints. However, dynamic and time-varying user service requests, coupled with the diversity in microservice container layers, present significant challenges for layer-aware service provisioning in edge cloud scenarios. These challenges include time-exceeded offline service provisioning, tangled microservice orchestration, and layer cache redundancy. In this paper, we introduce Tri-Ring, an asynchronous service provisioning framework with online learning in edge cloud networks. This framework addresses a joint optimization problem across three time scales, aiming to maximize the utility of edge cloud nodes. We formulate service provisioning as a mixed-integer nonlinear programming (MINLP) problem, which can be transformed into a linear programming (LP) subproblem and submodular optimization subproblem, solved using the estimator-assessor algorithm. The proposed framework is thoroughly evaluated using real-world datasets. The results demonstrate that Tri-Ring outperforms other baselines in service provisioning, increasing utility by 22.31%. Additionally, it reduces microservice startup time by 63.35% and optimizes storage resources by 30.07%.}
}


@inproceedings{DBLP:conf/infocom/Hu0FRD25,
	author = {Yuan Hu and
                  Gang Yang and
                  Songbo Fu and
                  Marco Di Renzo and
                  M{\'{e}}rouane Debbah},
	title = {CellScatter: Efficient Control and Backscatter Communication via Ambient
                  Cellular Signals},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044694},
	doi = {10.1109/INFOCOM55648.2025.11044694},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Hu0FRD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the continuous traffic of ubiquitous cellular networks and the ultra low-power low-cost characteristics of backscatter communication, cellular backscatter communication is a crucial technology for passive Internet of Things. However, existing cellular backscatter systems suffer from the lack of downlink control ability, low-efficiency modulation, and unreliable demodulation for megabit-rate backscatter transmission. This paper proposes CellScatter to overcome such drawbacks. First, we design a synchronization-and-control module for a CellScatter tag, which exploits cellular reference signals to achieve accurate synchronization and reliable multi-tag control without extra time overhead. Then, we design a single-sideband high-order modulation module to focus the backscatter signal power to the desired band and improve the spectrum utilization efficiency. Furthermore, we establish an explicit model to represent the spectrum-expanded backscatter signal without requiring a high-speed analog-to-digital converter, and derive a closed-form solution for the user equipment to recover the data of the tag at low complexity. We prototype the CellScatter system and evaluate its performance via ambient 4G LTE and 5G NR signals. Experimental results show that CellScatter can achieve 2.24 Mbps backscatter communication within a range of 30 meters, 6.7× lower BER and 33% higher rate than state of the art solutions.}
}


@inproceedings{DBLP:conf/infocom/ShisherPSB25,
	author = {Md Kamran Chowdhury Shisher and
                  Adam Piaseczny and
                  Yin Sun and
                  Christopher G. Brinton},
	title = {Computation and Communication Co-Scheduling for Timely Multi-Task
                  Inference at the Wireless Edge},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044625},
	doi = {10.1109/INFOCOM55648.2025.11044625},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ShisherPSB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In multi-task remote inference systems, an intelligent receiver (e.g., command center) performs multiple inference tasks (e.g., target detection) using data features received from several remote sources (e.g., edge sensors). Key challenges to facilitating timely inference in these systems arise from (i) limited computational power of the sources to produce features from their inputs, and (ii) limited communication resources of the channels to carry simultaneous feature transmissions to the receiver. We develop a novel computation and communication co-scheduling methodology which determines feature generation and transmission scheduling to minimize inference errors subject to these resource constraints. Specifically, we formulate the co-scheduling problem as a weakly-coupled Markov decision process with Age of Information (AoI)-based timeliness gauging the inference errors. To overcome its PSPACE-hard complexity, we analyze a Lagrangian relaxation of the problem, which yields gain indices assessing the improvement in inference error for each potential feature generation-transmission scheduling action. Based on this, we develop a maximum gain first (MGF) policy which we show is asymptotically optimal for the original problem as the number of inference tasks increases. Experiments demonstrate that MGF obtains significant improvements over baseline policies for varying tasks, channels, and sources.}
}


@inproceedings{DBLP:conf/infocom/LuoWLCLC25,
	author = {Xuejiao Luo and
                  Pengcheng Wei and
                  Wenyin Liu and
                  Jiahao Cao and
                  Jiang Li and
                  Binbin Chen},
	title = {{GM-BFD:} {A} Generalizable Multi-View Framework for Botnet Flow Detection},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044450},
	doi = {10.1109/INFOCOM55648.2025.11044450},
	timestamp = {Tue, 15 Jul 2025 09:07:01 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LuoWLCLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid rise of botnet attacks has disrupted telecom network services for millions of users. Detecting botnet traffic is therefore important for maintaining the availability and quality of these services. Existing methods primarily focus on developing attack detection techniques but ignore the adverse effects of various real-world factors, such as changes in attack behaviors and network conditions, on model performance. In response to this challenge, we propose a generalizable multi-view framework for botnet flow detection, namely GM-BFD. GM-BFD learns flow representations from intra-flow and inter-flow views, where the intra-flow view extracts the complex relationship between flow features, and the inter-flow view extracts flow behaviors between hosts while avoiding host representations. This multi-view modeling provides an expressive representation of flows and avoids overfitting host information during training. To enhance the model's generalizability, GM-BFD designs an un-supervised domain adaptation model with prediction confidence filtering to learn discriminative and transferable representations of benign and malicious flows across scenarios as accurately as possible. Experimental validation using the CTU dataset with 13 scenarios demonstrates the superior performance and generalizability of GM-BFD, with an  F 1 F_{1} -score improvement of approximately 11 % over the best baseline.}
}


@inproceedings{DBLP:conf/infocom/ZhuWXZY00025,
	author = {Zhui Zhu and
                  Xu Wang and
                  Jingao Xu and
                  Weichen Zhang and
                  Yankun Yuan and
                  Lin Wang and
                  Fan Dang and
                  Yunhao Liu},
	title = {DoMo: Rethinking Downscaling For Mobile Neural-Enhanced Video Streaming},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044486},
	doi = {10.1109/INFOCOM55648.2025.11044486},
	timestamp = {Wed, 06 Aug 2025 17:48:12 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhuWXZY00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the prevalence of 4G/5G infrastructure and mobile devices, mobile video streaming has become an ubiquitous element of daily life. Nevertheless, the online delivery of high-resolution videos, such as 2K and 4K formats, encounters significant challenges due to bandwidth limitations and network fluctuations. Existing neural-enhanced video streaming systems primarily struggle with two issues: the difficulty of recovering intra-frame high-frequency content and reusing the inter-frame content correlation. Addressing these challenges, this paper introduces a novel approach, designated as DoMo, which reconsiders the potential of mobile-side video super-resolution (SR) from a cloud perspective. We implement DoMo for the VP9 codec and test on real on-demand streaming media videos. Empirical results indicate that DoMo not only surpasses current state-of-the-art neural-enhanced solutions by achieving a 3.32 – 4.54 dB improvement in the peak signal-to-noise ratio (PSNR), but also outperforms traditional non-SR decoding methods by 6.80 – 8.89 dB.}
}


@inproceedings{DBLP:conf/infocom/Cheng0ZL25,
	author = {Yunlai Cheng and
                  Rui Han and
                  Qinglong Zhang and
                  Chi Harold Liu},
	title = {ElasticFed: Collaborative Large-Small Transformer Training for Federated
                  Continual Learning at Edge},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044503},
	doi = {10.1109/INFOCOM55648.2025.11044503},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Cheng0ZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Executing transformer-based applications on edge devices encounters challenging scenarios of continually learning new tasks. Federated continual learning (FCL) is a prevalent framework that supports model training using local data across edge devices. However, state-of-the-art FCL techniques either cause high computation and communication costs on large transformer models, or train small/compressed models, limiting both the learning capacity/accuracy on clients' local data and the global knowledge exchange among them. In this paper, we propose ElasticFed, a neuron-grained scaling approach for large-small transformer collaborative training in edge-based FCL. ElasticFed's key design features are (i) a proxy mechanism in local training, which constructs a compact model consisting of the original transformer's most important neurons to the current task, thus collaboratively training both models with small overheads; and (ii) a neuron-grained global aggregator, which se-lectively aggregates different clients' knowledge belonging to the most important neurons, thus maximizing the positive knowledge transfer with small communication costs. The comparison results against state-of-the-art techniques show that ElasticFed improves accuracy by 59.69% under the same training time. Compared to the techniques on original transformers, ElasticFed reduces training time and communication costs by 1.5x and 2.9x with small accuracy losses of 0.88%}
}


@inproceedings{DBLP:conf/infocom/Wang0SZG0C25,
	author = {Zhaojie Wang and
                  He Huang and
                  Yu{-}e Sun and
                  Hanwen Zhang and
                  Guoju Gao and
                  Haibo Wang and
                  Shigang Chen},
	title = {Swing Filter: {A} Low-Overhead Filter with Larger Filtering Range
                  for Network Traffic Measurement},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044730},
	doi = {10.1109/INFOCOM55648.2025.11044730},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Wang0SZG0C25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic measurement provides indispensable information to many applications in improving network performance. However, the limited on-chip resources face great challenges in measuring millions of flows simultaneously with high accuracy, and the highly skewed traffic distribution further worsens the performance. Although filtering the vast majority of small flows in advance can help to improve the estimation performance of large flows, the existing filters have limitations in filtering range and processing overhead. This paper proposes an efficient filter with a flexible and extended filtering range for network traffic measurement. One key to our design is the use of signed counters whose values swing in positive and negative directions to cancel out small flows, thereby enlarging the filtering range. We show that the proposed filter is highly effective in filtering small flows, with lower memory overhead and processing over-head than the existing work. It supports various measurement tasks and provides a guaranteed bound on misreport rate. We implement our filter on P4 and a NetFPGA-equipped prototype, and conduct extensive experiments based on real-world Internet traces. Experimental results show that the proposed filter reduces the flow-size estimation error by an order of magnitude and achieves 1.69 times higher throughput than SOTA.}
}


@inproceedings{DBLP:conf/infocom/LeeSPB25,
	author = {Kanghyun Lee and
                  Youngwook Son and
                  Jongyeon Park and
                  Saewoong Bahk},
	title = {Enriching Multi-User {OFDMA} in Wi-Fi Networks with Frequency-Selective
                  Channel Awareness},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044595},
	doi = {10.1109/INFOCOM55648.2025.11044595},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LeeSPB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent studies on multi-user (MU) orthogonal frequency division multiple access (OFDMA) in IEEE 802.11ax high-light its advantages in terms of network efficiency under low traffic loads by mitigating channel access overheads. They also claim that MU-OFDMA becomes only susceptible to extra overheads under saturated traffic conditions, even leading to throughput loss compared to legacy single-user (SU) transmissions. Against this perspective, we notice that MU-OFDMA potentially enhances overall capacity of Wi-Fi networks by harnessing small resource units (RUs) flexibly over frequency-selective channels. Through measurements, we find that current RU allocation in commercial APs cannot achieve optimal performance due to the lack of consideration for detailed channel characteristics. We conceptualize a resource allocation strategy that accounts for distinct link quality across RUs, with its effectiveness verified through proof-of-concept study as well as extensive performance analysis. Based on these findings, we present ChORUS, a standard-compliant framework to realize optimal RU allocation by fully exploiting frequency-selective channels and their diversity among users. Our trace-driven simulation using standard channel models and real-world traces demonstrates that ChORUS significantly enhances throughput performance over SU, ensuring network capacity elevation through MU-OFDMA.}
}


@inproceedings{DBLP:conf/infocom/Zhang0Z00DL25,
	author = {Mai Zhang and
                  Lin Cui and
                  Xiaoquan Zhang and
                  Fung Po Tso and
                  Zhen Zhang and
                  Yuhui Deng and
                  Zhetao Li},
	title = {Quark: Implementing Convolutional Neural Networks Entirely on Programmable
                  Data Plane},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044654},
	doi = {10.1109/INFOCOM55648.2025.11044654},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhang0Z00DL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of programmable network devices and the widespread use of machine learning (ML) in networking have facilitated efficient research into intelligent data plane (IDP). Offloading ML to programmable data plane (PDP) enables quick analysis and responses to network traffic dynamics, and efficient management of network links. However, PDP hardware pipeline has significant resource limitations. For instance, Intel Tofino ASIC has only 10Mb SRAM in each stage, and lacks support for multiplication, division and floating-point operations. These constraints significantly hinder the development of IDP. This paper presents Quark, a framework that fully offloads convolutional neural network (CNN) inference onto PDP. Quark employs model pruning to simplify the CNN model, and uses quantization to support floating-point operations. Additionally, Quark divides the CNN into smaller units to improve resource utilization on the PDP. We have implemented a testbed prototype of Quark on both P4 hardware switch (Intel Tofino ASIC) and software switch (i.e., BMv2). Extensive evaluation results demonstrate that Quark achieves 97.3% accuracy in anomaly detection task while using only 22.7% of the SRAM resources on the Intel Tofino ASIC switch, completing inference tasks at line rate with an average latency of 42.66µs.}
}


@inproceedings{DBLP:conf/infocom/XuSQHMLZHL25,
	author = {Haitao Xu and
                  Yiwen Sun and
                  Kaleem Ullah Qasim and
                  Shuai Hao and
                  Wenrui Ma and
                  Zhenyuan Li and
                  Fan Zhang and
                  Meng Han and
                  Zhao Li},
	title = {Understanding the Business of Online Affiliate Marketing: An Empirical
                  Study},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044696},
	doi = {10.1109/INFOCOM55648.2025.11044696},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XuSQHMLZHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Affiliate marketing is a revenue-sharing marketing scheme by which an affiliate, such as a blogger or YouTuber, garners commissions for promoting a merchant's goods or services, thereby aiming to foster a mutually beneficial relationship between affiliates and merchants. Despite being a multi-billion-dollar global industry, affiliate marketing remains inadequately explored, and the research community lacks a comprehensive understanding of its intricate ecosystem. In this paper, we present the first comprehensive empirical study of the affiliate marketing ecosystem. We conduct thorough measurements to assess the prevalence of affiliate marketing, estimate the market size, and elucidate the characteristics of affiliates, merchants, and intermediary affiliate networks. Over a continuous span of 13 months, we monitored four of the most prominent affiliate aggregation platforms, yielding a substantial dataset. We observed 467,219 unique offers - tasks to be undertaken by affiliates - involving 37,109 merchants and 556 affiliate networks across the four platforms. Notably, these offers would cost the merchants more than 19 million USD for the completion of all the actions pre-defined in these offers, such as signing up or making a transaction. Additionally, we compiled a large-scale dataset comprising 124,462 affiliate links, enabling us to conduct a comprehensive investigation. Finally, we propose machine learning models incorporating the characteristics of affiliate links to detect real-world affiliate marketing campaigns.}
}


@inproceedings{DBLP:conf/infocom/Almehdhar00BA25,
	author = {Ahmed F. Almehdhar and
                  Min Dong and
                  Ben Liang and
                  Gary Boudreau and
                  Yahia Ahmed},
	title = {Wireless Network Virtualization in Uplink Coordinated Multi-Cell {MIMO}
                  Systems},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044763},
	doi = {10.1109/INFOCOM55648.2025.11044763},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Almehdhar00BA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider wireless network virtualization (WNV) in the uplink of a coordinated multi-cell system, where multiple service providers (SPs) operate in virtually isolated networks managed by an infrastructure provider (InP). The InP provides service isolation among the SPs by exploiting the spatial structure in MIMO communication. We jointly optimize the uplink receive beamforming at the base stations (BSs) and the transmit power of the SPs' subscribing users, by alternating between two subproblems that both admit efficient closed-form solutions. We then propose a distributed implementation that solves each subprobem among the BSs without the need for a central controller. We show that the distributed approach requires significantly less communication overhead compared with the centralized one, especially when the system is not overloaded. Our simulation results under typical wireless networking environments demonstrate that the proposed solution enables effective network virtualization, to support the independent operation of multiple SPs over multiple cells, without losing communication efficiency compared with non-virtualized network operation. Furthermore, it substantially outperforms traditional WNV based on strict resource separation, especially for systems with a large number of antennas or a large number of SPs.}
}


@inproceedings{DBLP:conf/infocom/GaoZG025,
	author = {Zhidong Gao and
                  Zhenxiao Zhang and
                  Yuanxiong Guo and
                  Yanmin Gong},
	title = {Federated Adaptive Fine-Tuning of Large Language Models with Heterogeneous
                  Quantization and LoRA},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044641},
	doi = {10.1109/INFOCOM55648.2025.11044641},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GaoZG025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) with parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), offers a privacy-preserving solution for fine-tuning large language models (LLMs) on edge devices. However, due to the enormous size of LLMs, federated fine-tuning with LoRA still faces significant challenges, including high training latency and substantial memory requirements. To address these challenges, we propose FAH-QLoRA, a novel time- and memory-efficient federated adaptive fine-tuning framework for LLMs, which leverages heterogeneous model quantization and LoRA. The key idea behind FAH-QLoRA is to quantize the base model within LoRA and dynamically adjust the LoRA ranks, enabling efficient fine-tuning on resource-constrained and heterogeneous devices while reducing training latency and maintaining performance. FAH-QLoRA integrates two key techniques: i) dynamically adjusting LoRA ranks across training rounds to promote faster convergence and lower resource usage, and ii) assigning heterogeneous LoRA ranks across devices to mitigate the straggler effect during the FL process, ensuring that heterogeneous resource constraints are met. We analyze the convergence of FAH-QLoRA under general non-convex and non-IID FL settings. Extensive experiments demonstrate that FAH-QLoRA can reduce training time by up to 45.86% and memory usage by up to 44.15% compared to baseline methods.}
}


@inproceedings{DBLP:conf/infocom/LiuHLC025,
	author = {Qianli Liu and
                  Zicong Hong and
                  Peng Li and
                  Fahao Chen and
                  Song Guo},
	title = {Mell: Memory-Efficient Large Language Model Serving via Multi-GPU
                  {KV} Cache Management},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044533},
	doi = {10.1109/INFOCOM55648.2025.11044533},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuHLC025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serving large language models (LLMs) for massive users is challenged by the significant memory footprint of the transient state, known as the key-value (KV) cache, which scales with sequence length and number of requests. Instead of renting or buying more expensive GPUs, the load imbalance of the KV cache across GPUs, coupled with recent advances in inter-GPU communication, provides an opportunity to serve more requests via request migration. However, high migration overhead and unpredictable request patterns make it challenging. Therefore, this paper proposes Mell, a memory-efficient LLM serving system via multi-GPU KV cache management. It saves the number of GPUs needed in the system by considering the dynamic KV cache load and the costly request migration. Specifically, we first develop an adaptive request migration mechanism to balance the computational and communication overheads and adapt to diverse resource conditions. Then, we design an online algorithm tailored to a multi-LLM request and multi-GPU scheduling problem with migration enabled. It aims to minimise the required GPUs while limiting the number of migrations. Finally, we implement a prototype of Mell and demonstrate that it reduces the number of GPUs by 31% and increases the GPU utilization by 43% at most compared to existing LLM serving systems.}
}


@inproceedings{DBLP:conf/infocom/LuJZL0CW25,
	author = {Rongwei Lu and
                  Yutong Jiang and
                  Jinrui Zhang and
                  Chunyang Li and
                  Yifei Zhu and
                  Bin Chen and
                  Zhi Wang},
	title = {{\(\gamma\)}-FedHT: Stepsize-Aware Hard-Threshold Gradient Compression
                  in Federated Learning},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044558},
	doi = {10.1109/INFOCOM55648.2025.11044558},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LuJZL0CW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gradient compression can effectively alleviate communication bottlenecks in Federated Learning (FL). Contemporary state-of-the-art sparse compressors, such as Top- k k , exhibit high computational complexity, up to  O ( d log 2 k ) \\mathcal{O}(d\\log_{2}k) , where  d d  is the number of model parameters. The hard-threshold compressor, which simply transmits elements with absolute values higher than a fixed threshold, is thus proposed to reduce the complexity to  O ( d ) \\mathcal{O}(d) . However, the hard-threshold compression causes accuracy degradation in FL, where the datasets are non-IID and the stepsize  γ \\gamma  is decreasing for model convergence. The decaying stepsize reduces the updates and causes the compression ratio of the hard-threshold compression to drop rapidly to an aggressive ratio. At or below this ratio, the model accuracy has been observed to degrade severely. To address this, we propose  γ \\gamma -FedHT, a stepsize-aware low-cost compressor with Error-Feedback to guarantee convergence. Given that the traditional theoretical framework of FL does not consider Error-Feedback, we introduce the fundamental conversation of Error-Feedback. We prove that  γ \\gamma -FedHT has the convergence rate of  O ( 1 T ) ( T \\mathcal{O}\\left(\\frac{1}{T}\\right)(T  representing total training iterations) under  μ \\mu -strongly convex cases and  O ( 1 T √ ) \\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right)  under non-convex cases, same as FedAVG. Extensive experiments demonstrate that  γ \\gamma -FedHT improves accuracy by up to 7.42% over Top- k k  under equal communication traffic on various non-IID image datasets.}
}


@inproceedings{DBLP:conf/infocom/ShiLW0XML25,
	author = {Gaowei Shi and
                  Xiulong Liu and
                  Huan Wang and
                  Yuhan Li and
                  Hao Xu and
                  Liyuan Ma and
                  Keqiu Li},
	title = {BrokerAS: Towards Fault-tolerant Atomic Cross-chain Swaps},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044581},
	doi = {10.1109/INFOCOM55648.2025.11044581},
	timestamp = {Sat, 23 Aug 2025 07:42:37 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ShiLW0XML25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Atomic Cross-chain Swaps (ACCS) is a significant cornerstone of blockchain interoperability technology. ACCS users are required to manually match swap requests off-chain and continuously monitor the status of other blockchains. Subsequently, the ACCS concludes with either the successful swap of all assets or a complete failure. These strong user assumptions, insufficient on-chain liquidity, and the all or nothing attribute limit its practical deployment and application. To address these problems, we propose BrokerAS, the first practical ACCS system that tolerates failures. We design an incentive-compatible broker mechanism that enables users to efficiently match swap requests on-chain using the Minimum Cost Maximum Flow (MCMF) algorithm. Additionally, BrokerAs designs the interaction between Broker Management Smart Contract (BMSC) and Factory Smart Contract (FSC) to implement a four-phase fault-tolerant ACCS protocol, achieving all or something or nothing swaps by using customized security parameters for asset partitioning. Finally, we conduct experiments on Ethereum Testnet and Binance Smart Chain Testnet. The experimental results demonstrate that BrokerAs outperforms state-of-the-art solutions in trustless scenarios, and achieves an improvement of  1. 8 x 1. 8\\mathrm{x}  to 3.F  9 X 9X  in swap success rate and  1. 6 x 1. 6\\mathrm{x}  to  2. 8 x 2. 8\\mathrm{x}  in the number of served ACCS.}
}


@inproceedings{DBLP:conf/infocom/XueL25,
	author = {Haoqian Xue and
                  Xiaojun Lin},
	title = {Fast Computation of Partial Index and Application to an AoI Minimization
                  Problem},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044677},
	doi = {10.1109/INFOCOM55648.2025.11044677},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XueL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study efficient algorithms for computing the partial index. We focus on an AoI (Age-of-Information) minimization problem under the generate-at-will setting such that multiple sources/agents transmit information updates to the base-station over multiple heterogeneous and unreliable wireless channels. While the partial index has been proposed to solve this otherwise exponential-complexity MDP problem, computing the partial index for each source still incurs significant complexity. Existing fast computation algorithms for Whittle index cannot be applied to this setting due to the multiple heterogeneous channels. Instead, we identify a number of general structural conditions for the per-agent MDP, based on which we develop a fast algorithm that can compute the partial index more efficiently. We then verify that the AoI problem under the generate-at-will setting satisfies these general conditions and our algorithm can compute the partial index for all states and all channels with a complexity of  O ( M 3 K 3 ) \\mathcal{O}(M^{3}K^{3}) , where  K K  denotes the number of per-source states and  M M  denotes the number of channel types. Our numerical results confirm that our proposed algorithm is significantly faster in computing the partial index than standard methods based on binary search.}
}


@inproceedings{DBLP:conf/infocom/ZhouZ0X0025,
	author = {Jinrui Zhou and
                  Yu Zhao and
                  Yin Xu and
                  Mingjun Xiao and
                  Jie Wu and
                  Sheng Zhang},
	title = {{PSFL:} Parallel-Sequential Federated Learning with Convergence Guarantees},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044534},
	doi = {10.1109/INFOCOM55648.2025.11044534},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouZ0X0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a novel distributed learning paradigm which can coordinate multiple clients to jointly train a machine learning model by using their local data samples. Existing FL works can be roughly divided into two categories according to the modes of model training: Parallel FL (PFL) and Sequential FL (SFL). PFL can speed up each round of model training time through parallel training, but it might suffer from the convergence degradation when facing the heterogeneity issue. SFL can deal with the heterogeneity issue well to reduce the number of training rounds, but it will spend more time in each round of local model training due to the sequential mode. In this paper, we propose a novel hybrid Parallel-Sequential Federated Learning (PSFL) framework by integrating the parallel and sequence training modes together. We derive the upper bounds of the model convergence and the expected total training time for the PSFL framework through theoretical analysis. Based on the results, we find out the optimal training structure and design a client sampling strategy, which can balance the two training modes and guarantee the unbiasedness. Extensive experiments validate our theoretical analysis and demonstrate the significant performance of the PSFL framework.}
}


@inproceedings{DBLP:conf/infocom/Zhang25,
	author = {Zhenghao Zhang},
	title = {QCode: Achieving High Capacity in Uncoordinated Access Channels {(UCACH)}
                  of Broadband Low Earth Orbit {(LEO)} Satellite Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044546},
	doi = {10.1109/INFOCOM55648.2025.11044546},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhang25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth Orbit (LEO) satellite networks have emerged as an attractive option to complement the existing wireless infrastructure. One of the main challenges in LEO networks is to achieve low latency for applications such as automatic driving and gaming. A promising direction to reduce the latency is to employ Uncoordinated Access Channels (UCACH) which avoid the overhead of resource allocation prior to data transmissions. In this paper, a novel modulation scheme, referred to as QCode, is proposed to support communications from user terminals to satellites in UCACHs. The key novelty of QCode is to use long symbols while aggressively increasing the number of bits modulated on each symbol. As a result, QCode achieves high network capacity without sacrificing data rates for individual user terminals. Experiments in the POWDER platform confirm that QCode packets can be received correctly in real-world wireless channels without prior synchronization. Simulations with satellite channel models show that QCode achieves significantly higher network throughput than the existing option.}
}


@inproceedings{DBLP:conf/infocom/HaqueS025,
	author = {Md Ashikul Haque and
                  Abusayeed Saifullah and
                  Haibo Zhang},
	title = {Deep Reinforcement Learning Based Coexistence Management in {LPWAN}},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044466},
	doi = {10.1109/INFOCOM55648.2025.11044466},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HaqueS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the coexistence problem in Low-Power Wide-Area Networks (LPWANs), focusing on LoRa, a leading technology in this domain. Current LPWANs lack effective mechanisms to handle coexistence, especially in urban areas where numerous networks and devices may be operating in the limited spectrum. Existing approaches, including collision resolution techniques and adaptations from other wireless technologies like WiFi, are inadequate due to LPWANs' unique characteristics, such as long-range communication and severe energy constraints. Existing learning based approach addresses this issue through embedded Q-learning at low-power for LoRa nodes, imposing computation and energy overhead for them and limiting the network's learning capability by using simple Q-table learning at individual nodes. We propose a novel system design leveraging the computational capabilities of LoRa Network Servers (LNS) for coexistence management. By offloading learning and computation tasks to LNS, the proposed framework employs deep Q-learning, a powerful reinforcement learning technique, to adapt dynamically to complex coexistence scenarios. By exploiting the LNS's global view of the communication channels, our framework enables more effective learning and decision-making compared to decentralized approaches. Furthermore, our system design seamlessly integrates with traditional LoRaWAN infrastructure, imposing minimal overhead on low-power nodes. We evaluate our approach through physical experiments and large-scale simulations in NS-3, considering various coexistence scenarios for a LoRa network. Our results show that, in comparison with the state-of-the-art decentralized learning method, our scheme achieves up to 70.97%, 62.91%, and 47.01% of improvement in packet reception rate, energy per packet, and average transmission attempts per packet, respectively.}
}


@inproceedings{DBLP:conf/infocom/Cai00S25,
	author = {Xuhong Cai and
                  Yi Chen and
                  Shenghao Yang and
                  Xingyan Shi},
	title = {Erasure Coding-Based Non-Conservative Network Communication: {A} Ground
                  Up Approach},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044663},
	doi = {10.1109/INFOCOM55648.2025.11044663},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Cai00S25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Erasure coding has been studied for network communications due to various advantages over retransmission. Many prior works have discussed how to integrate erasure coding into existing communication protocols. However, as the congestion control mechanisms inherent to these established protocols were not natively designed for coded flows, the potential of erasure coding may not be fully realized. In this paper, we demonstrate the benefits of incorporating erasure coding into congestion control design. We explore a communication protocol that is built from the ground up with erasure coding, named Coding-based Non-conservative Communication Protocol (CNCP). CNCP adopts hop-by-hop congestion control and distinguishes itself from traditional conservative flow enforcement by accommodating non-conservative flows. To address potential issues associated with non-conservative flows, such as increased congestion and queue instability, CN CP utilizes a specific queuing design and packet deletion strategy at intermediate network nodes. Our analysis reveals that non-conservative flows do not undermine fairness and may expedite convergence towards optimal network utilization. Furthermore, we develop a hop-by-hop congestion control mechanism based on primal-dual method, specifically tailored for non-conservative flows in CNCP. Through simulations, we show that combining erasure coding and non-conservative flow control can significantly enhance network communication performance under dynamic network conditions.}
}


@inproceedings{DBLP:conf/infocom/ZhouLQ25,
	author = {Longyu Zhou and
                  Supeng Leng and
                  Tony Q. S. Quek},
	title = {HaDT: Hardening Digital Twins for UAVs-Based Industrial Logistics
                  Distribution Systems},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--8},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044738},
	doi = {10.1109/INFOCOM55648.2025.11044738},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouLQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of network autonomy, Unmanned Aerial Vehicles (UAV) applications are attractive to serve intelligent logistics with the advantages of miniaturization and flexibility. It, however, is difficult to implement accurate UAV control in complex distribution scenarios. In this context, Digital Twins (DT) is a potential tool to assist UAVs in acquiring feasible cooperative distribution decisions with the ability of imitation and derivation. Nonetheless, it is challenging to perform real-time DT implementations due to the limited computing resources of UAVs. To address the mentioned problems, we achieve a Hardened DT (HaDT) framework, operating at the edge side, to enable a double DT cooperation manner for efficient resource scheduling and path planning. The resource scheduling model can implement the integration of computing and communication resources among UAVs to acquire feasible cooperative logistics distribution decisions. The decisions can drive the path planning model to derive positions and velocities of UAVs for low-latency logistics distribution performance with energy saving. Experiment results demonstrate the efficiency of our HaDT framework. Compared to state-of-the-art logistics distribution solutions, our solution reduces the distribution latency by 63.9% while improving the successful distribution ratio by 10.9%.}
}


@inproceedings{DBLP:conf/infocom/ZhaoZLKC25,
	author = {Ming Zhao and
                  Yuru Zhang and
                  Qiang Liu and
                  Ahan Kak and
                  Nakjung Choi},
	title = {AdaSlicing: Adaptive Online Network Slicing Under Continual Network
                  Dynamics in Open Radio Access Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044475},
	doi = {10.1109/INFOCOM55648.2025.11044475},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoZLKC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Open radio access networks (e.g., O-RAN) facilitate fine-grained control (e.g., near-RT RIC) in next-generation networks, necessitating advanced AI/ML techniques in handling online resource orchestration in real-time. However, existing approaches can hardly adapt to time-evolving network dynamics in network slicing, leading to significant online performance degradation. In this paper, we propose AdaSlicing, a new adaptive network slicing system, to online learn to orchestrate virtual resources while efficiently adapting to continual network dynamics. The AdaSlicing system includes a new soft-isolated RAN virtualization framework and a novel AdaOrch algorithm. We design the AdaOrch algorithm by integrating AI/ML techniques (i.e., Bayesian learning agents) and optimization methods (i.e., the ADMM coordinator). We design the soft-isolated RAN virtualization to improve the virtual resource utilization of slices while assuring the isolation among virtual resources at runtime. We implement AdaSlicing on an O-RAN compliant network testbed by using OpenAirInterface RAN, Open5GS Core, and FlexRIC near-RT RIC, with Ettus USRP B210 SDR. With extensive network experiments, we demonstrate that AdaSlicing substantially outperforms state-of-the-art works with 64.2% cost reduction and 45.5% normalized performance improvement, which verifies its high adaptability, scalability, and assurance.}
}


@inproceedings{DBLP:conf/infocom/Huang0ZLWSBLRL025,
	author = {Peihao Huang and
                  Guo Chen and
                  Xin Zhang and
                  Can Liu and
                  Hongyu Wang and
                  Huijun Shen and
                  Ying Bian and
                  Yuanwei Lu and
                  Zhenyuan Ruan and
                  Bojie Li and
                  Jiansong Zhang and
                  Yongfeng Liu and
                  Zhigang Chen},
	title = {Fast and Scalable Selective Retransmission for {RDMA}},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044566},
	doi = {10.1109/INFOCOM55648.2025.11044566},
	timestamp = {Sat, 23 Aug 2025 07:42:35 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Huang0ZLWSBLRL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RDMA, with its high throughput, ultra-low latency, and low CPU utilization, has been widely used in large-scale data centers. However, due to the limited RDMA RNIC on-chip memory, commodity RDMA usually implements the simple Go-Back-N (GBN) loss recovery mechanism, which consumes less memory but leads to a significant performance drop when encountering loss. Recent works try to improve RDMA performance under packet loss by introducing selective retransmission (SR) to it. Nevertheless, implementing efficient SR in RDMA remains challenging. Specifically, either it consumes too much memory for maintaining SR states which leads to poor connection scalability, or it incurs high CPU consumption and latency for onloading SR processing back to the CPU software. To this end, we propose FaSR, a fast and scalable RDMA selective retransmission. It is fast by processing the SR with 200Gbps+ line-rate fully on the RDMA NIC chip, and is scalable by introducing novel SR state management schemes thus consuming small memory even under high concurrency. Specifically, FaSR adopts a dynamically sharing SR structure among connections to reduce the memory footprint by orders of magnitude when the concurrency is high. Also, utilizing the loss recovery pattern, FaSR devises several techniques thus it can access the sharing structure with line-rate for different connections. We have implemented FaSR in Xilinx FPGA board with ~4000 lines of verilog code. Testbed evaluation demonstrates that FaSR can maintain 92%+ throughput at a packet loss rate of 1% under more than 5K concurrent connections, which is 16% and 12.6x higher compared to the latest RDMA SR solution and commodity RDMA NICs, respectively.}
}


@inproceedings{DBLP:conf/infocom/LvFFSDJ25,
	author = {Jiaxi Lv and
                  Guiyun Fan and
                  Xinyue Fu and
                  Jiahui Sun and
                  Rong Ding and
                  Haiming Jin},
	title = {mmWave-Based Relay Reflector Reconstruction for LiDAR-Free Around-Corner
                  Human Sensing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044715},
	doi = {10.1109/INFOCOM55648.2025.11044715},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LvFFSDJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {mmWave radar enables mobile platforms (e.g., mobile robots) to perceive conditions of around-corner human via multi-bounce signal reflection through relay reflector. Existing methods rely on LiDAR to reconstruct relay reflector geometry, which helps map mirror radar point cloud back to its actual position. However, LiDAR is not universally available and is unable to reconstruct transparent relay reflectors. This paper presents mmRec, an mmWave-based relay reflector reconstruction method which avoids LiDAR assistance for the first time. LiDAR absence poses two challenges: generating dense radar point cloud for recovering relay reflector geometry, and separating radar point clouds of human and relay reflector solely based on radar point cloud features. mmRec addresses such challenges with two key designs, including (i) a point cloud generation component that generates dense point cloud of relay reflector by leveraging diffuse reflection and performing peak detection in the range-angle domain, and (ii) a semantic segmentation component that performs accurate segmentation of radar point cloud via feature augmentation, and neural network design which enables learnable graph construction and multi-scale feature fusion. Extensive experiments show that mmRec-reconstructed relay reflectors achieve an average post-mapping location error of 6.25cm.}
}


@inproceedings{DBLP:conf/infocom/SunLSWTL25,
	author = {Yu Sun and
                  Xinyu Liu and
                  Qian Sun and
                  Jiaming Wang and
                  Lin Tian and
                  Jianwei Liu},
	title = {5GC-Fuzz: Finding Deep Stateful Vulnerabilities in 5G Core Network
                  with Black-Box Fuzzing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044489},
	doi = {10.1109/INFOCOM55648.2025.11044489},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SunLSWTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given the large-scale deployment of 5G, rigorous testing of its core network (5GC) is essential to ensure security and robustness. Fuzzing is currently one of the most popular vulnerability discovery techniques. However, existing fuzzers suffer from low coverage of 3GPP-specified 5GC states, invalid long signaling sequence generation when exploring deep 5GC states, and coarse-grained feedback of closed-source 5G systems. This paper presents 5GC-Fuzz, a black-box fuzzing framework to detect deep stateful vulnerabilities in 5GC implementations. 5GC-Fuzz integrates three innovative techniques: (1) a systematic construction of a 5GC state machine derived from 3GPP specifications to guide the fuzzing process; (2) a 5G grammar-aware signaling sequence mutation method based on protocol stack interception to generate test cases while maximally guaranteeing the syntactic, semantic, and cryptographic correctness; and (3) a fine-grained state-transition-path feedback mechanism based on 5GC logs to optimize test states and sequences selection. The 5GC-Fuzz was evaluated on three popular 5GC implementations and achieves 152.6% more states and 206.7% more state transition paths than the state-of-the-art fuzzers. Moreover, 5GC-Fuzz exposed 22 security-critical vulnerabilities, with 6 CVEs assigned. In general, 5GC- Fuzz could explore deeper states and uncover more vulnerabilities in 5GC, significantly enhancing the security of mobile communication infrastructures.}
}


@inproceedings{DBLP:conf/infocom/YangZYXLL25,
	author = {Cheng Yang and
                  Xiaoning Zhang and
                  Bodong Yan and
                  Sun Xu and
                  Bingyi Liu and
                  Jianchun Liu},
	title = {Towards Lightweight Traffic Forecasting in {RDMA} Networks: Design
                  and Application},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044588},
	doi = {10.1109/INFOCOM55648.2025.11044588},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangZYXLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Communication becomes the bottleneck of data-parallel computing systems. Although Remote Direct Memory Access (RDMA) was proposed to solve the communication bottleneck at end hosts, network congestion can still prolong data transmission time, thereby degrading application performance. To avoid network congestion, we need to deploy bandwidth provisioning or traffic engineering schemes, both of which require ahead-of-time traffic information as input. In this work, we design ApOLLO to accurately forecast the traffic in RDMA networks. The key idea of ApOLLO is to estimate the traffic amount that will be injected into RDMA networks based on the information recorded in memory or RDMA NICs. To reduce the system overhead and achieve timely forecasting, ApOLLO uses shared memory to transfer forecasting results among its processes. By implementing ApOLLO via modifying RDMA Verbs APIs, ApOLLO is ready-to-deploy and transparent to users. Through extensive experiments on a real testbed, we demonstrate that ApOLLO can achieve accurate traffic forecasting with less than 10% CPU usage and can help reduce the average flow completion time by up to 28.9% when combined with a naive load balancer.}
}


@inproceedings{DBLP:conf/infocom/Wang0Y0CSLHKKA25,
	author = {Tianshi Wang and
                  Jinyang Li and
                  Qikai Yang and
                  Ruijie Wang and
                  Yizhuo Chen and
                  Dachun Sun and
                  Bohan Li and
                  Yigong Hu and
                  Tomoyoshi Kimura and
                  Denizhan Kara and
                  Tarek F. Abdelzaher},
	title = {DynaGen: Conditional Diffusion Models for Enhancing Acoustic and Seismic-Based
                  Vehicle Detection},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044455},
	doi = {10.1109/INFOCOM55648.2025.11044455},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Wang0Y0CSLHKKA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ground vibrations and acoustic signatures associated with driving vehicles exhibit distinct characteristics pivotal for vehicle detection, i.e., inferring vehicle types. This non-visual detection method presents advantages over vision-based techniques, notably in terms of privacy preservation and cost-effectiveness in both hardware requirements and data processing. However, current models struggle with variations in vehicle types, terrains, and vehicle dynamics such as the instant speed and acceleration. The limitation primarily stems from the inherent scarcity of the manually collected dataset, leading to an underrepresentation of the possible vehicle-terrain combinations and the full spectrum of vehicle dynamics. In response, this paper introduces DynaGen, a diffusion-model-based data augmentation framework for enriching the diversity of the original dataset by interpolating the data samples under unobserved conditions. DynaGen integrates the three key factors in vehicle detection-vehicle type, terrain, and vehicle dynamics-as conditions for controlling the generation process of the diffusion model. In the generation stage, DynaGen generalizes and predicts signals under novel conditions, effectively broadening the scope of the original dataset. Empirical evaluations show that generated samples from DynaGen effectively augment the original dataset, enhancing the performance of subsequent vehicle detection models.}
}


@inproceedings{DBLP:conf/infocom/JinSTWYW25,
	author = {Zitong Jin and
                  Xingang Shi and
                  Ying Tian and
                  Zhiliang Wang and
                  Xia Yin and
                  Jianping Wu},
	title = {Affinity-Model: Improving {AS} Routing Models via {AS} Affinity Behavior
                  Inference},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044520},
	doi = {10.1109/INFOCOM55648.2025.11044520},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/JinSTWYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Understanding the routing behavior exhibited by ASes is crucial for us to comprehend the Internet workings, optimize diverse network applications, and facilitate the evolution of the Internet. However, prior research often models AS routing behaviors with oversimplified AS relationships, lacking explanations for complex behaviors, thus hampering their own accuracy. In this paper, we introduce Affinity-Model (AM), a novel approach designed to discover AS affinity behaviors that traditional assumptions cannot explain, thereby enhancing existing routing models. AM first uses the state-of-the-art AS-level path inference algorithm to infer the RIB-in path of each source AS. Discrepancies between the observed routing selection and the optimal path determined by traditional sorting methods then identify the AS affinity behaviors. Second, AM leverages the high-occurrence affinities discovered by full VPs, extracts their key features, and makes extensive inferences about remaining affinities across the entire Internet. Third, AM extends its utility to infer non valley-free policies in the Internet. Findings indicate a strong association between affinities and AS prefix/destination-specific policies, with a higher incidence in European IXP connections. Our experiments show that the inferred AS affinity behaviors align highly with IRR-recorded policies, modeling 85.9% of AS routing behaviors in the actual Internet.}
}


@inproceedings{DBLP:conf/infocom/Hu0SL025,
	author = {Qinnan Hu and
                  Yuntao Wang and
                  Zhou Su and
                  Tom H. Luan and
                  Ruidong Li},
	title = {ConWatcher: Towards Adaptive and Label-Efficient Online Smart Contract
                  Analysis in Blockchains},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044590},
	doi = {10.1109/INFOCOM55648.2025.11044590},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Hu0SL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the immutable nature of smart contracts, online contract analysis is the only viable approach for revealing vulnerabilities in deployed contracts. Existing online approaches face significant challenges in terms of efficiency, adaptability, and reliance on vulnerability labels. This paper proposes ConWatcher, an adaptive and label-efficient online contract analysis framework capable to detect yet-unknown attacks under evolving tactics without reliance on vulnerability labels. ConWatcher simulates the Advanced Persistent Threat (APT) tactics commonly used in yet-unknown attacks by continuously applying minor perturbations to legitimate interaction behaviors. It then reversely learns the denoising process, guided by potential logic vulnerabilities (i.e., functionality dependencies), to adaptively identify stealthy anomalies and detect yet-unknown attacks without needing vulnerability labels. ConWatcher proceeds in four steps. First, interaction behavior modeling. Via bytecode-level, account-level, and revenue-level modeling, we propose behavior-aware multivariate time series model to accurately represent long-term contract interactions with multi-faceted behaviors. Second, APT-like noise adding. We leverage the forward diffusion model to produce minor and stochastic APT-like noises with efficiency. Third, reverse denoising learning. To effectively guide reverse denoising using functionality dependencies, we devise an adaptive contract analysis engine equipped with heterogeneous control flow graph modeling and heterogeneous message passing mechanisms to extract function-level and bytecode-level functionality dependencies. Last, contract anomaly detection. We design a label-efficient attack detector based on reconstruction error for contract anomaly detection. Extensive empirical validations on a manually constructed dataset, covering both mainstream and novel vulnerabilities, demonstrate ConWatcher's effectiveness, adaptability, and label efficiency, with an average F1-score of 0.88 across all types of attacks without prior knowledge of corresponding vulnerabilities.}
}


@inproceedings{DBLP:conf/infocom/HuBWHZ25,
	author = {Bing Hu and
                  Yuanguo Bi and
                  Kui Wu and
                  Zixuan Huang and
                  Rongfei Zeng},
	title = {Achieving Efficient Multipath Validation in Software-Defined Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044662},
	doi = {10.1109/INFOCOM55648.2025.11044662},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuBWHZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The programmability of Software-Defined Networks (SDN) enables multipath routing through dynamic adjustments and optimizations of network resources. However, a compromised switch can violate packet forwarding rules, creating serious security vulnerability. While path validation ensures packets follow designated paths, mainstream methods impose excessive computational burden on the controller and significant storage overhead on switches due to the uncertainty and potentially large number of packet forwarding paths. To address these issues, we propose a Naive Packet-level MultiPath Validation Scheme (NPM-PVS) as the first attempt to verify multiple forwarding paths in SDN. Building on NPM-PVS, we introduce an Enhanced Packet-level MultiPath Validation Scheme (EPM-PVS), which uses a Supplementary Validation Information (SVI) generation method to reduce the controller's load by ensuring consistent validation for packets of a network flow across various forwarding paths. To further improve the efficiency of EPM-PVS, we propose a Flow-level MultiPath Validation Scheme (FM-PVS) and implement a validation information compression method to minimize data plane storage overhead. Additionally, we introduce an anomaly switch identification method to locate compromised switches when path validation fails at the controller. Evaluation results demonstrate that the proposed FM-PVS achieves low switch storage overhead and reduces the computational burden on the controller.}
}


@inproceedings{DBLP:conf/infocom/ZhaoT0AS25,
	author = {Longfei Zhao and
                  Jingbo Tan and
                  Jintao Wang and
                  Ian F. Akyildiz and
                  Zhi Sun},
	title = {Covering Underwater Shadow Zones using Acoustic Reconfigurable Intelligent
                  Surfaces},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044529},
	doi = {10.1109/INFOCOM55648.2025.11044529},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoT0AS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To enhance our exploration and understanding of Earth's oceans, seamless communication coverage across the vast three-dimensional underwater space has long been sought. Unlike terrestrial networks that utilize radio signals, underwater acoustic communications face a significant challenge: nodes located in underwater shadow zones cannot connect to the broader network, even when they are within the Line of Sight (LoS). These shadow zones can extend for tens of kilometers, preventing acoustic signals from propagating and resulting in disconnection of nodes within these areas. Existing solutions primarily focus on deploying nodes outside of shadow zones; however, these approaches fail to guarantee seamless coverage in dynamic ocean environments. This paper aims to address the shadow zone issue fundamentally by leveraging acoustic Reconfigurable Intelligent Surfaces (aRIS) to actively manage the underwater communication channel. Initially, the shadow zones are modeled analytically, and based on this model, optimal aRIS deployment strategies are developed to cover shadow zones in both deep and shallow seas. The initial aRIS design presented in this paper is refined to account for practical engineering constraints, enabling implementation on real-world hardware, with performance validated through pool tests. The shadow zone coverage performance, assessed through Bellhop-based simulations, indicates that without aRIS deployment, coverage is limited to less than 20%, regardless of the increased radiated energy from the initial coverage source. The ocean environment leads to persistent shadow zones, providing coverage only in bending regions. In contrast, optimal aRIS deployment achieves nearly 100% energy coverage across all areas, including previously inaccessible shadow zones, by introducing energy into these regions and utilizing energy-rich areas to enhance coverage in shadow zones.}
}


@inproceedings{DBLP:conf/infocom/FanDWZJ25,
	author = {Guiyun Fan and
                  Rong Ding and
                  Xiaocheng Wang and
                  Yichen Zhu and
                  Haiming Jin},
	title = {m\({}^{\mbox{3}}\)ASL: {ASL} Gesture Recognition with Moving mmWave
                  Radar},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044456},
	doi = {10.1109/INFOCOM55648.2025.11044456},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FanDWZJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper tries to answer the question: “Can we enable moving mmWave radar to recognize ASL gestures?” A positive answer would help facilitate the non-contact interaction between the millions of deaf or hard of hearing ASL users and radar-equipped mobile robots. The challenges, however, include how to deal with the pros and cons of radar motion, and how to exploit the multi-scale nature of radar data. This paper gives an affirmative answer by proposing m3ASL, a method which achieves ASL recognition with moving mmWave radar for the first time. Specifically, m3ASL generates accurate radar point cloud in a motion-ware manner: it constructs extended phase sequences for accurate angle estimation by leveraging radar motion, corrects motion-incurred phase errors via self-supervision, and calibrates motion-incurred point cloud distortion via careful spatial transformation and speed compensation. Furthermore, m3ASL extracts features from multi-scale radar data accordingly with augmented graph and temporal convolution neural network, and complementarily fuses the extracted features with our proposed symmetric double cross attention neural network structure to infer ASL gestures. We conduct extensive experiments via both simulation and real-world system. The experimental results show that m3ASL achieves close to 93% recognition accuracy on 30 representative ASL gestures, surpassing SOTA by at least 30%.}
}


@inproceedings{DBLP:conf/infocom/FengLZDW025,
	author = {Yunyun Feng and
                  Xin Liu and
                  Jia Zhao and
                  Yuan Ding and
                  Gongpu Wang and
                  Wei Gong},
	title = {Energy-Efficient Paging for Duty-Cycled {LTE} Backscatter},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044777},
	doi = {10.1109/INFOCOM55648.2025.11044777},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FengLZDW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benefiting from the continuous and ubiquitous transmission from LTE base stations (eNodeBs), LTE backscatter is more efficient and reliable than traditional backscatter operating in ISM bands. However, ISM bands can utilize intermittent signals to synchronize low-duty cycle backscatter and receivers, a mechanism not available in LTE due to its continuous transmission. Consequently, LTE backscatter and receivers (UE) may not have overlapping operational periods, making it possible that the UE might never receive the backscattered data. To solve this issue, we use LTE-standard paging, one of the LTE power management mechanisms, to wake up the receiver and allow it and the tag to maintain a consistent sleep-wake schedule. Following LTE messages, our tag employs a new low-power state machine to accommodate different phases of the UE. We prototype PScatter using FPGAs, SDR eNodeBs, and UEs. Extensive results show that PScatter supports the paging mechanism similarly to active LTE devices and achieves better energy efficiency and more effective transmission than prior LTE backscatter. It consumes 21x lower power than active LTE radios for paging decoding. For end-to-end, PScatter achieves more than 6.7x better energy efficiency than previously reported LScatter+. Moreover, PScatter delivers an effective transmission ratio of up to 100%.}
}


@inproceedings{DBLP:conf/infocom/GuanZWW25,
	author = {Zhangshuang Guan and
                  Yulin Zhao and
                  Zhiguo Wan and
                  Wei Wang},
	title = {Input Integrity and Authentic Results: Towards Trustworthy Aggregation
                  in Federated Learning},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044719},
	doi = {10.1109/INFOCOM55648.2025.11044719},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GuanZWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a collaborative machine learning approach that allows multiple clients to train a model jointly while keeping their data local, thus enhancing privacy. A critical component of FL is secure aggregation (SA), which protects user privacy during the server-side aggregation of client updates. However, current state-of-the-art (SOTA) SA schemes, such as LERNA and Flamingo, have significant limitations. They require at least 3 round-trip communications to handle client dropouts, making them inefficient. Additionally, they are vulnerable to malicious attacks from both clients (input data poisoning) and the server (aggregation manipulation). To address these issues, we propose the first 2-round-trip trustworthy aggregation scheme, called HyperSA, which guarantees both input integrity and result authenticity. HyperSA achieves its goal with two key techniques: a consistent authenticated decryption mechanism and a three-step chain proof procedure. The former, implemented through consistent threshold key recovery, reduces the communication round trips from 3 to 2. The latter, comprising three zero-knowledge proofs chained together, enables misbehavior detection, ensuring that all clients receive authentic aggregation results. We implemented HyperSA and conducted a comprehensive evaluation. Both theoretical analysis and experimental results demonstrate the practicality and efficiency of our design compared to existing SOTA schemes.}
}


@inproceedings{DBLP:conf/infocom/0001L0025,
	author = {Juncheng Wang and
                  Yituo Liu and
                  Ben Liang and
                  Min Dong},
	title = {Constrained Over-the-Air Model Updating for Wireless Online Federated
                  Learning with Delayed Information},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044570},
	doi = {10.1109/INFOCOM55648.2025.11044570},
	timestamp = {Tue, 05 Aug 2025 22:40:40 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001L0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study online federated learning over a wireless network, where the central server updates an online global model sequence to minimize the time-varying loss of multiple local devices over time. The server updates the global model through over-the-air model-difference aggregation from the local devices over a noisy multiple-access fading channel. We consider the practical scenario where information on both the local loss functions and the channel states is delayed, and each local device is under a time-varying power constraint. We propose Constrained Over-the-air Model Updating with Delayed infOrmation (COMUDO), where a new lower-and-upper-bounded virtual queue is introduced to counter the delayed information and control the hard constraint violation. We show that its local model updates can be efficiently computed in closed-form expressions. Furthermore, through a new Lyapunov drift analysis, we show that COMUDO provides bounds on the dynamic regret, static regret, and hard constraint violation. Simulation results on image classification tasks under practical wireless network settings show substantial accuracy gain of COMUDO over state-of-the-art approaches, especially in the low-power region.}
}


@inproceedings{DBLP:conf/infocom/RahmanH25,
	author = {Md. Rashedur Rahman and
                  Moinul Hossain},
	title = {Channel Access Deterrence Attack: An Attack Against Spectrum Coexistence
                  Between {NR-U} and Wi-Fi in the 5 GHz Band},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044478},
	doi = {10.1109/INFOCOM55648.2025.11044478},
	timestamp = {Tue, 08 Jul 2025 07:46:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/RahmanH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Listen-Before- Talk-based channel access mechanisms have been crucial to enabling coexistence between cellular technologies (e.g., 4G and 5G) and Wi-Fi in the 5 GHz band. Though the coexistence between cellular and Wi-Fi technologies garnered significant research regarding interference and fairness, security issues have long been overlooked. This paper introduces a novel attack strategy, namely the Channel Access Deterrence attack, that takes advantage of an inherent vulnerability in the channel access mechanism of cellular technologies (e.g., 5G NR-U), whereby small interference signals are strategically transmitted to thwart the cellular base station from accessing the channel. This paper introduces a Markov Chain-based model to embody this attack's impact on cellular technologies and Wi-Fi; this model helps to understand and assess the channel access vulnerabilities in such coexistence scenarios. In addition, this paper proposes a resilience mechanism during channel access to mitigate such threats and evaluates the system's performance in various scenarios. To the extent of our knowledge, this research is the first to introduce such a threat and to provide a novel security perspective when designing spectrum coexistence mechanisms.}
}


@inproceedings{DBLP:conf/infocom/YuZHCZZCW25,
	author = {Jiashuo Yu and
                  Longlong Zhu and
                  Long Huang and
                  Xinyang Chen and
                  Linying Zheng and
                  Dong Zhang and
                  Xiang Chen and
                  Chunming Wu},
	title = {Scaling Learning-based Packet Classification Hardware with NeuTree},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044564},
	doi = {10.1109/INFOCOM55648.2025.11044564},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YuZHCZZCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning-based packet classification (LPC) methods tackle the challenge of increased network rules by learned data structures to minimize memory usage. Hence, LPC shows promise in enabling resource-constrained network devices (e.g., switches and NICs) to handle large-scale rules. However, existing LPC methods involve floating-point computations, which ASICs in high-speed network devices cannot support. In this paper, we propose NeuTree, scaling packet classification on hardware by revising the LPC structure with binarized computation. We introduce the recursive binarized model index (RBMI) structure, enabling non-floating computation and low resource consumption. Meanwhile, we design the division scheme for preprocessing rules that enables packets to be matched in multiple RBMIs in parallel to maintain high-speed classification. The experiment shows that the memory consumption of NeuTree is competitive to the state-of-the-art LPC method, with floating-point operations reduced from 81.11% to 0% and less than 1% construction time. Further, we verify NeuTree on NetFPGA SUME with a throughput of 120.48Mpps.}
}


@inproceedings{DBLP:conf/infocom/JiangJ025,
	author = {Changkun Jiang and
                  Bohong Jiang and
                  Jianqiang Li},
	title = {When Labor-Intensive Mobile Crowdsourcing Meets Unobservability: Contextual
                  Bandit Learning with Unobservable Individual Rewards},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044692},
	doi = {10.1109/INFOCOM55648.2025.11044692},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/JiangJ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsourcing (MCS) has emerged as an effective means of leveraging the power of the crowd for large-scale location-related tasks. However, one key challenge of labor-intensive MCS is assigning labor-intensive tasks to suitable workers, as different workers are better suited for different tasks and contribute differently to the overall goal. Moreover, these relationships are often unknown and time-varying, and decision-makers typically focus on the overall task performance rather than individual worker performance. Previous works have not addressed this problem adequately, as they either relied on individual rewards for decision-making or assumed a known relationship between individual rewards and the overall reward. To address this problem, we propose a new approach that models labor-intensive task assignments as a contextual bandit learning problem with unobservable individual rewards (UIR). Our approach employs an improved UCB-UIR algorithm for the known reward relationship scenario, which yields a sublinear regret bound with UIR. For the unknown reward relationship scenario, we propose a generic Transformer-UIR algorithm to learn the relationship between contextual information and then determine optimal assignments with UIR. We demonstrate the effectiveness of our approach using a realistic MCS application, where our algorithms outperform state-of-the-art baselines significantly with both known and unknown reward relationships.}
}


@inproceedings{DBLP:conf/infocom/Liu0ZZQQ025,
	author = {Shuo Liu and
                  Minghui Xu and
                  Yuezhou Zheng and
                  Yifei Zou and
                  Wangjie Qiu and
                  Gang Qu and
                  Xiuzhen Cheng},
	title = {Partially Synchronous {BFT} Consensus Made Practical in Wireless Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044727},
	doi = {10.1109/INFOCOM55648.2025.11044727},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Liu0ZZQQ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consensus is becoming increasingly important in wireless networks. Partially synchronous BFT consensus, a significant branch of consensus, has made considerable progress in wired networks. However, its implementation in wireless networks, especially in dynamic ad hoc wireless networks, remains challenging. Existing wireless synchronous consensus protocols, despite being well-developed, are not readily adaptable to partially synchronous settings. Additionally, reliable communication, a cornerstone of BFT consensus, can lead to high message and time complexity in wireless networks. To address these challenges, we propose a wireless communication protocol called ReduceCatch (Reduce and Catch) that supports reliable 1-to-N, N-to-1, and N-to-N communications. We employ ReduceCatch to tailor three partially synchronous BFT consensus protocols (PBFT, Tendermint, and HotStuff) for seamless adaptation from wired to ad hoc wireless networks. To evaluate the performance of the ReduceCatch-enabled consensus protocols, we develop a three-layer wireless consensus testbed, based on which we implement 20 distinct consensus protocols and measure their latency and throughput. The experimental results demonstrate the superiority of the ReduceCatch-based consensus protocol in terms of latency and throughput.}
}


@inproceedings{DBLP:conf/infocom/DuttaguptaJF0W25,
	author = {Abhishek Duttagupta and
                  MohammadErfan Jabbari and
                  Claudio Fiandrino and
                  Marco Fiore and
                  Joerg Widmer},
	title = {{SYMBXRL:} Symbolic Explainable Deep Reinforcement Learning for Mobile
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044492},
	doi = {10.1109/INFOCOM55648.2025.11044492},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DuttaguptaJF0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of Deep Reinforcement Learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SYMBXRL, a novel technique for EXplainable Reinforcement Learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SYMBXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SYMBXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.}
}


@inproceedings{DBLP:conf/infocom/ZhuangWWZ025,
	author = {Xinyi Zhuang and
                  Jiaqi Wu and
                  Hongjia Wu and
                  Tingting Zhang and
                  Lin Gao},
	title = {Joint Optimization of Model Inferencing and Task Offloading for MEC-Empowered
                  Large Vision Model Services},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044689},
	doi = {10.1109/INFOCOM55648.2025.11044689},
	timestamp = {Tue, 05 Aug 2025 22:40:41 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhuangWWZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid advancement of Large Vision Models (LVMs) such as Sora, the initial comprehension of physical laws by large AI models has garnered significant attention, which enables them to interpret and apply physical principles with increasing accuracy and sophistication. Nevertheless, due to resource limitations and delay constraints, traditional cloud-based LVM services often fail to meet the diverse needs of users, particularly in scenarios requiring real-time responsiveness. In this work, we explore the scenario of Mobile Edge Computing (MEC)-empowered LVM services in wireless networks, where heterogeneous LVMs are deployed on both cloud and edge servers, and LVM Users (LUs) can offload computation task to edge servers to reduce delay and energy consumption. In such a scenario, we focus on the joint optimization of model inferencing and task offloading for LUs, aiming to maximize the total service utility, while minimizing delay and energy consumption. First, to characterize the utility of LVM services, we propose a multi-dimensional video quality metric based on real measurements, which incorporates both the prompt-video alignment and the classic video quality indicators. Then, to solve the problem in a decentralized manner, we propose a two-stage solution based on both learning and optimization techniques. In the first stage, we design a reinforcement learning-based Multi-Agent Proximal Policy Optimization (MAPPO) approach to make the real-time model inferencing and task offloading decisions. In the second stage, we employ the optimization-based Sequential Least Squares Programming (SLSQP) to make the efficient resource allocation decisions. Simulation results show that our proposed solution outperforms other benchmarks, and can reduce delay and energy consumption by up to 17.2% and 21.7%, respectively, while increasing service utility by up to 3%.}
}


@inproceedings{DBLP:conf/infocom/ZhaoZ0S0W25,
	author = {Chonghe Zhao and
                  Yipeng Zhou and
                  Shengli Zhang and
                  Quan Z. Sheng and
                  Yang Zhang and
                  Shiting Wen},
	title = {ExClique: An Express Consensus Algorithm for High-Speed Transaction
                  Process in Blockchains},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044465},
	doi = {10.1109/INFOCOM55648.2025.11044465},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoZ0S0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Proof of Authority (PoA) plays a pivotal role in blockchains for reaching consensus. Clique, which selects consensus nodes to generate blocks with a pre-determined order, is the most popular implementation of PoA due to its low communication overhead and energy consumption. However, our study unveils that the speed to process transactions by Clique is severely restricted by 1) the long communication delay of full blocks (each containing a certain number of transactions) between consensus nodes; and 2) occurrences of no-turn blocks, generated by no-turn nodes if an in-turn block generation fails. Consequently, Clique struggles to support distributed applications requiring a high transaction processing speed, e.g., online gaming. To overcome this deficiency, we propose an Express Clique (ExClique) algorithm by improving Clique from two perspectives: compacting blocks for broadcasting to shorten communication delay and prohibiting the occurrences of no-turn blocks. For performance evaluation, we implement ExClique by modifying Geth of Ethereum, the software implementing Clique, and deploy a permissioned blockchain network by using container technology. The experimental results show that ExClique achieves a substantial enhancement in transactions per second (TPS). Specifically, it boosts TPS by 2.25× in a typical network with 21 consensus nodes and an impressive 7.01× in a large-scale network with 101 consensus nodes when compared to Clique.}
}


@inproceedings{DBLP:conf/infocom/KalntisLIKI25,
	author = {Michail Kalntis and
                  Andra Lutu and
                  Jesus Alberto Oma{\~{n}}a Iglesias and
                  Fernando A. Kuipers and
                  George Iosifidis},
	title = {Smooth Handovers via Smoothed Online Learning},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044691},
	doi = {10.1109/INFOCOM55648.2025.11044691},
	timestamp = {Tue, 08 Jul 2025 07:55:53 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/KalntisLIKI25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With users demanding seamless connectivity, handovers (HOs) have become a fundamental element of cellular networks. However, optimizing HOs is a challenging problem, further exacerbated by the growing complexity of mobile net-works. This paper presents the first countrywide study of HO optimization, through the prism of Smoothed Online Learning (SOL). We first analyze an extensive dataset from a commercial mobile network operator (MNO) in Europe with more than 40M users, to understand and reveal important features and performance impacts on HOs. Our findings highlight a correlation between HO failures/delays, and the characteristics of radio cells and end-user devices, showcasing the impact of heterogeneity in mobile networks nowadays. We subsequently model UE-cell associations as dynamic decisions and propose a realistic system model for smooth and accurate HOs that extends existing approaches by (i) incorporating device and cell features on HO optimization, and (ii) eliminating (prior) strong assumptions about requiring future signal measurements and knowledge of end-user mobility. Our algorithm, aligned with the O-RAN paradigm, provides robust dynamic regret guarantees, even in challenging environments, and shows superior performance in multiple scenarios with real-world and synthetic data.}
}


@inproceedings{DBLP:conf/infocom/0116SHC25,
	author = {Hao Wang and
                  Decang Sun and
                  Jinbin Hu and
                  Kai Chen},
	title = {Enabling In-Network Acceleration Over the Cloud},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044680},
	doi = {10.1109/INFOCOM55648.2025.11044680},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0116SHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Offloading computing and storage to programmable switches, or in-network acceleration (INA), is a recent wisdom to speed up distributed applications. Researchers have proposed a variety of tailored INA solutions for separate applications with different data-plane layouts and network protocols. However, as hardware resource of programmable switches is limited, it is hard to integrate all these INA solutions simultaneously. Consequently, specialized INA techniques cannot realize full potential on the cloud, which needs to support various applications and requires concurrent access by multi-tenants. To enable on-demand INA service on the cloud, we present a generic INA framework called INAaaS (INA as a Service). At its core, INAaaS provides a universal INA interface in network and offers application-specific adapters on end-hosts. It further addresses the isolation problem of different applications, and guarantees the reliability and correctness. Our evaluation shows that INAaaS effectively improves the performance of various cloud applications, competing with the specific solutions.}
}


@inproceedings{DBLP:conf/infocom/YangDW000SD25,
	author = {Lingxiao Yang and
                  Xuewen Dong and
                  Zhiguo Wan and
                  Sheng Gao and
                  Wei Tong and
                  Di Lu and
                  Yulong Shen and
                  Xiaojiang Du},
	title = {AsyncSC: An Asynchronous Sidechain for Multi-Domain Data Exchange
                  in Internet of Things},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044780},
	doi = {10.1109/INFOCOM55648.2025.11044780},
	timestamp = {Sun, 17 Aug 2025 16:45:45 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangDW000SD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sidechain techniques improve blockchain scalability and interoperability, providing decentralized exchange and cross-chain collaboration solutions for Internet of Things (IoT) data across various domains. However, current state-of-the-art (SOTA) schemes for IoT multi-domain data exchange are constrained by the need for synchronous networks, hindering efficient cross-chain interactions in discontinuous networks and leading to suboptimal data exchange. In this paper, we propose AsyncSC, a novel asynchronous sidechain construction. It employs a committee to provide Cross-Blockchain as a Service (C-BaaS) for data exchange in multi-domain IoT. To fulfill the need for asynchronous and efficient data exchange, we combine the ideas of aggregate signatures and verifiable delay functions to devise a novel cryptographic primitive called delayed aggregate signature (DAS), which constructs asynchronous cross-chain proofs (ACPs) that ensure the security of cross-chain interactions. To ensure the consistency of asynchronous transactions, we propose a multilevel buffered transaction pool that guarantees the transaction sequencing. We analyze and prove the security of AsyncSC, simulate an asynchronous communication environment, and conduct a comprehensive evaluation. The results show that AsyncSC outperforms SOTA schemes, improving throughput by an average of 1.21 to 3.96 times, reducing transaction latency by 59.76% to 83.61%, and maintaining comparable resource overhead.}
}


@inproceedings{DBLP:conf/infocom/LiangQZ0H25,
	author = {Xufeng Liang and
                  Zhida Qin and
                  Pengzhan Zhou and
                  Shuang Li and
                  Tianyu Huang},
	title = {Dual-GT: Dual-scale Spatial Dependency for Grid-based Traffic Flow
                  Prediction},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044644},
	doi = {10.1109/INFOCOM55648.2025.11044644},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiangQZ0H25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a crucial component of intelligent transportation system, grid-based traffic flow prediction has gained an extensive application in the smart city. In general, it segments the city into equal regions and aims to accurately predict the flow for each of them. Despite the progress achieved, existing works solely rely on the static spatial connections to capture the grids dependencies, neglecting the dynamic semantic dependencies over different time scales, which inevitably results in a degradation of performance. To this end, we propose a novel Dual-scale spatial dependency for Grid-based Traffic flow prediction model, called Dual-GT. Specifically, we design a dual-scale spatial dependency learning mechanism to capture the grid dependencies from both long-term and short-term time scales. In detail, we learn the long-term dependencies through the Dynamic Time Warping algorithm. Furthermore, we innovatively characterize the short-term semantic dependencies by clustering the flow data series, and design an adaptive transfer method to efficiently integrate the short-term dependencies with the learned long-term ones. Extensive experiments on five real-world public traffic datasets verify the superiority of our approach. Additionally, we visualize the spatial dependency learned from long-term and short-term traffic flows to further show the effectiveness and interpretability of our Dual-GT model.}
}


@inproceedings{DBLP:conf/infocom/ZhaoSWFCCWL25,
	author = {Haoyuan Zhao and
                  Jianxin Shi and
                  Guanzhen Wu and
                  Hao Fang and
                  Yi Ching Chou and
                  Long Chen and
                  Feng Wang and
                  Jiangchuan Liu},
	title = {{BAROC:} Concealing Packet Losses in LSNs with Bimodal Behavior Awareness
                  for Livecast Ingestion},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044617},
	doi = {10.1109/INFOCOM55648.2025.11044617},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoSWFCCWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of Low-Earth Orbit satellite networks (LSNs), exemplified by initiatives like Starlink, One Web and Kuiper, has ushered in a new era of “Internet from Space” global connectivity. Recent studies have shown that LSN s are capable of providing unprecedented download capacity and low latency to support Livecast viewing. However, Livecast ingestion still faces significant challenges, such as limited uplink capacity, bandwidth degradation, and the burst of packet loss due to frequent satellite reallocations, which cause previous recovery and adaptive solutions to be inferior under this new scenario. In this paper, we conduct an in-depth measurement study dedicated to understanding the implications of satellite reallocations, which reveals that the network status during reallocations with network anomalies exhibits a different distribution, leading to bimodal behaviors on the overall network performance. Motivated by this finding, we propose BAROC, a framework that can effectively conceal burst packet losses by combining a novel proposed MTP-Informer with bimodal behavior awareness during satellite reallocation. BAROC enhances video QoE on the server side by addressing the above challenges and jointly determining the optimal video encoding and recovery parameters. Our extensive evaluation shows that BAROC outperforms other video delivery recovery approaches, achieving an average PSNR improvement of 1.95 dB and a maximum of 3.44 dB, along with enhancements in frame rate and parity packet utilization. Additionally, a comprehensive ablation study is conducted to assess the effectiveness of MTP-Informer and components in BAROC.}
}


@inproceedings{DBLP:conf/infocom/DongNZXHZSQ0NSX25,
	author = {Shichen Dong and
                  Zhixiong Niu and
                  Mingchao Zhang and
                  Zhiying Xu and
                  Chuntao Hu and
                  Pengzhi Zhu and
                  Qingchun Song and
                  Lei Qu and
                  Peng Cheng and
                  Cam{-}Tu Nguyen and
                  Shaoling Sun and
                  Xiaohu Xu and
                  Yongqiang Xiong and
                  Wei Wang and
                  Xiaoliang Wang},
	title = {Mina: Fine-Grained In-network Aggregation Resource Scheduling for
                  Machine Learning Service},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044657},
	doi = {10.1109/INFOCOM55648.2025.11044657},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DongNZXHZSQ0NSX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-network aggregation (INA) offloads gradient aggregation onto switches, and thus effectively reduces the aggregation latency and the volume of traffic. However, INA resources are limited due to the high cost of on-chip memory, which imposes distinct challenges to the effective scheduling of these resources in multi-job MLaaS scenarios. In this paper, we explore the scheduling of INA resources in spatial and temporal dimensions, specifically focusing on its impact on the average job completion time (JCT) and the efficiency of INA resources. We propose Mina,an innovative co-design of algorithm and system that intelligently assigns INA resources to each job and effectively schedules these resources among multiple jobs. Our experiments show that Minaattains an INA efficiency score of 0.9998, implying that almost all jobs run nearly as efficiently as they would with exclusive INA acceleration.}
}


@inproceedings{DBLP:conf/infocom/WangSGJ0YW25,
	author = {Zhaozhen Wang and
                  Xingang Shi and
                  Haijun Geng and
                  Zitong Jin and
                  Han Zhang and
                  Xia Yin and
                  Zhiliang Wang},
	title = {On Non-Commutative Routing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044753},
	doi = {10.1109/INFOCOM55648.2025.11044753},
	timestamp = {Fri, 08 Aug 2025 17:28:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangSGJ0YW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The complexity of routing requirements leads to increasingly intricate routing metrics. Existing routing algebra theories have demonstrated that convergent and optimal routing algorithms can be designed only when path metrics satisfy certain properties such as monotonicity and isotonicity. Furthermore, some non-isotonic metrics can be converted into isotonic forms on partial orders through reduction. However, practical scenarios often involve non-commutative algebraic properties, which are overlooked by existing theories. For these problems, there lacks a unified framework to study their solvability, a systematic method for their reduction, and an efficient algorithm to compute optimal routes. In this work, we extend routing algebra to accommodate non-commutative routing problems, propose general reduction methods for them, and explore their solvability. In addition, we design a link state algorithm that converge fast on a reduced partial order. All these discussions are supported by concrete examples, theoretical proofs, and simulations on various network topologies.}
}


@inproceedings{DBLP:conf/infocom/YoshinakaTKH25,
	author = {Yutaro Yoshinaka and
                  Junji Takemasa and
                  Yuki Koizumi and
                  Toru Hasegawa},
	title = {Minimal and Fastest Anonymous Communication against Colluding Passive
                  Adversaries},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044517},
	doi = {10.1109/INFOCOM55648.2025.11044517},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YoshinakaTKH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anonymous communication protocols are crucial in protecting privacy from colluding eavesdroppers on the Internet. Onion Routing protocols provide relationship anonymity-unlinkability of senders and receivers-by decrypting the entire packet at each on-path relay. Although this decryption prevents adversaries from linking inbound and outbound packets at every relay, it bottlenecks forwarding performance. This paper highlights its excessiveness, proposing that packet unlinkability across the end-to-end path suffices, rather than at every individual relay. We introduce Cabbage Routing, where each relay decrypts only a portion of the packet, and consecutive relays collaboratively decrypt the entire packet, defeating passive adversaries. This approach not only improves performance on software platforms but enables operation at Tbps-class speeds on hardware platforms. We present the protocol design, implementation on both software and hardware platforms, formal security validation, and experimental performance analyses. Our evaluations affirm that the Cabbage Routing protocol is secure and provides high forwarding speeds appropriate for wide deployment over the Internet.}
}


@inproceedings{DBLP:conf/infocom/Zhang0LLS0HZL25,
	author = {Song Zhang and
                  Wenxin Li and
                  Yulong Li and
                  Yuan Liu and
                  Lide Suo and
                  Sheng Chen and
                  Yitao Hu and
                  Laiping Zhao and
                  Keqiu Li},
	title = {Lark: {A} Buffer-aware Building Block for Programmable Packet Scheduling
                  in Datacenters},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044448},
	doi = {10.1109/INFOCOM55648.2025.11044448},
	timestamp = {Mon, 18 Aug 2025 21:44:01 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhang0LLS0HZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programmable packet scheduling enables users to customize scheduling algorithms flexibly without designing new ASICs. Existing schemes prefre to approximate optimal Push-In First-Out (PIFO) using First-In First-Out (FIFO) queues in commodity programmable switches. Despite its availability, these schemes suffer performance degradation due to the unawareness of available switch buffer. To be specific, when the port buffer is drained, existing schemes discard all incoming packets, even though these packets have higher priorities than the enqueued packets. In this paper, we reveal that the problem's key culprit is the lack of coordination between buffer management and packet scheduling in the switch. To fill this gap, we present Lark, a buffer-aware building block for programmable scheduling schemes designed to solve the above problem. Its key idea is to proactively drop the low-priority packets when the allocated buffer is to be drained, thereby admitting the later-arriving high-priority packets. Lark contains two modules, a lightweight gradient-based online prediction module and a simple priority-based decision module. Lark relies the former module to identify whether the allocated buffer is to be drained and uses the later to determine whether to drop the incoming packet. We have integrated Lark into two representative schemes, SP-PIFO and AIFO. Our large-scale evaluations over three realistic workloads show that Lark can significantly optimize their key metrics without sacrificing throughnut.}
}


@inproceedings{DBLP:conf/infocom/WangLZSZ025,
	author = {Ne Wang and
                  Wenxiang Lin and
                  Lin Zhang and
                  Shaohuai Shi and
                  Ruiting Zhou and
                  Bo Li},
	title = {SP-MoE: Expediting Mixture-of-Experts Training with Optimized Pipelining
                  Planning},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044508},
	doi = {10.1109/INFOCOM55648.2025.11044508},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangLZSZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparsely activated Mixture-of-Experts (MoE) has emerged as a key technique to expand the size of Transformer-based large language models (LLMs) while maintaining low computational costs. However, MoE layers require to route the input data to distributed devices, incurring significant communication latency. Existing studies have primarily focused on alleviating this problem by overlapping computation and communication tasks within a single MoE layer, which fails to achieve sufficient overlap and results in limited performance gains. In this work, we introduce an orthogonal partitioning dimension from existing task-parallel methods by leveraging the autoregressive nature of causal Transformer-based LLMs, i.e. partitioning tasks along the sequence dimension. This provides more flexible and efficient overlaps among tasks from both non-MoE and MoE layers. To this end, we propose an efficient MoE training approach, SP-MoE, with two innovative designs. 1) It incorporates non-MoE layers into the overlapping with not only the current MoE layer but also the preceding MoE layer, thereby facilitating more efficient training; 2) It identifies the optimal combination of pipeline degrees for non-MoE and MoE layers and devises the best scheduling plans for load-imbalanced non-MoE and uniform MoE layers to achieve the goal of minimizing the total training latency. Extensive experiments conducted on two GPU clusters demonstrate that SP-MoE can effectively identify the optimal combination of pipeline degrees and achieve 16.1% - 34.3% reduction in training latency compared to three state-of-the-art MoE systems.}
}


@inproceedings{DBLP:conf/infocom/LiWCZ25,
	author = {Tingyu Li and
                  Zhen Wei and
                  Xiaoliang Chen and
                  Zuqing Zhu},
	title = {MrgRecmp4: Efficient Stateful {SFC} Recompilation in {PDP} Switches
                  with Flexible Table Merging},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044451},
	doi = {10.1109/INFOCOM55648.2025.11044451},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiWCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The fast development of P4-based programmable data plane (PDP) has motivated the offloading of service function chains (SFCs) to switches. To adapt to dynamic service demands, PDP switches will need to be reprogrammed occasionally to update their packet processing pipelines, especially for stateful SFCs. In this work, we propose a novel stateful SFC recompilation system, namely, MrgRecmp4. It jointly considers the existing and new SFCs to optimize the P4 program recompilation for deploying new SFCs as reconstructed pipelines, such that the complexity of SFC reconfiguration can be minimized together with the hardware resource utilization in PDP switches. We first lay out the system design of MrgRecmp4, and explain how to leverage a flexible and fine-grained table merging scheme to optimize the P4 programs of stateful SFCs. Then, we formulate a bi-level integer linear programming (BILP) model to accomplish network-wide optimization for MrgRecmp4, which jointly considers the existing and new SFCs in a PDP network to determine the PDP switches that need to be reprogrammed and to generate reconstructed pipelines for them with table merging. We also propose a sliding-window-based heuristic to solve the network-wide optimization quickly. Both simulations and real-world experiments confirm that MrgRecmp4 outperforms existing benchmarks.}
}


@inproceedings{DBLP:conf/infocom/Yu0X0M25,
	author = {Fu Yu and
                  Xiaolong Zheng and
                  Dan Xia and
                  Liang Liu and
                  Huadong Ma},
	title = {Enabling Reliable LoRa Decoding under Cross-channel Interference},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044479},
	doi = {10.1109/INFOCOM55648.2025.11044479},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Yu0X0M25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper focuses on cross-channel interference caused by collided chirps with different bandwidths but same slope in time-frequency domain. Existing concurrency decoding methods cannot resolve it because they require the accurate time duration of all chirps. But in limited receiving bandwidth, interfering chirps with asymmetric bandwidths are incomplete. Their time duration is unpredictable due to the randomly modulated symbols. We instead propose Elora to decode target packets under cross-channel interference. Elora leverages the difference in chirps' time duration instead of absolute time duration to identify target chirp because only target chirp fills the decoding window in time and frequency. Elora adopts chirp elimination to test whether each received chirp can be completely eliminated after subtracting the reference chirp that fully occupies the decoding window because only the target chirp is completely eliminated while interfering chirps remain unaffected. However, in practice, unexpected frequency shift and phase rotation prevent the target chirp from being completely eliminated. We propose a linear-fitting method to correct frequency shift and then compensate phase rotation by an energy varying model. We also adopt window dividing to estimate energy intensity of chirp's residue. The experiments show that Elora reduces the symbol error rate by up to 86.3% compared with existing methods.}
}


@inproceedings{DBLP:conf/infocom/Xu0BS25,
	author = {Wen Xu and
                  Ben Liang and
                  Gary Boudreau and
                  Hamza Umit Sokun},
	title = {Client Sampling for Communication-Efficient Distributed Minimax Optimization},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044637},
	doi = {10.1109/INFOCOM55648.2025.11044637},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Xu0BS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed minimax optimization is essential for robust federated learning, offering resiliency against the variability in data distribution. Most previous works focus only on learning guarantees and convergence analysis, without explicit consideration of the communication delay, which can be crucial in practical systems. In this work, we consider the problem of communication-efficient distributed minimax optimization via judicious client sampling, proposing an algorithm termed CE-MINIMAX, which takes into consideration both the training convergence performance and the communication time per training round. We derive convergence bounds for CE − MINIMAX under both convex and non-convex loss functions, which we then use to design the client sampling probabilities in joint consideration of the communication time. We conduct numerical experiments with canonical classification datasets to demonstrate that CE − MINIMAX can achieve higher worst-case test accuracy under substantially reduced communication time, compared with state-of-the-art client sampling schemes for distributed minimax optimization.}
}


@inproceedings{DBLP:conf/infocom/Hou25,
	author = {I{-}Hong Hou},
	title = {Network Optimization in Dynamic Systems: Fast Adaptation via Zero-Shot
                  Lagrangian Update},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044756},
	doi = {10.1109/INFOCOM55648.2025.11044756},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Hou25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses network optimization in dynamic systems, where factors such as user composition, service requirements, system capacity, and channel conditions can change abruptly and unpredictably. Unlike existing studies that focus primarily on optimizing long-term performance in steady states, we develop online learning algorithms that enable rapid adaptation to sudden changes. Recognizing that many current network optimization algorithms rely on dual methods to iteratively learn optimal Lagrange multipliers, we propose zero-shot updates for these multipliers using only information available at the time of abrupt changes. By combining Taylor series analysis with complementary slackness conditions, we theoretically derive zero-shot updates applicable to various abrupt changes in two distinct network optimization problems. These updates can be integrated with existing algorithms to significantly improve performance during transitory phases in terms of total utility, operational cost, and constraint violations. Simulation results demonstrate that our zero-shot updates substantially improve transitory performance, often achieving near-optimal outcomes without additional learning, even under severe system changes.}
}


@inproceedings{DBLP:conf/infocom/OkoroMMDBZ25,
	author = {Blessing Okoro and
                  Maxwell McNeil and
                  Kavya Meka and
                  Karyn Doke and
                  Petko Bogdanov and
                  Mariya Zheleva},
	title = {Sparse Recovery Transmitter Detection},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044608},
	doi = {10.1109/INFOCOM55648.2025.11044608},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/OkoroMMDBZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transmitter detection and separation in radio spec-trum scans is an essential component in emerging spectrum-sharing networks, as it underpins situational awareness for coexistence and enforcement. However, detecting transmitters in noisy real-world traces is challenging and has been tackled with limited practical applicability. Beyond noisy measurements, the challenges stem from the need to simultaneously detect multiple and possibly overlapping transmitter frequency bands and track their transmissions over time. We address these challenges with SCAN (Sparse reCovery trAnsmitter detectioN): an unsupervised approach based on sparse dictionary coding to jointly detect the frequency and temporal behavior of multiple co-occurring transmitters in power spectral density traces. We demonstrate SCAN's applicability to high-noise regimes and across various transmitter co-occurrence scenarios, including when transmitters concurrently overlap in time and frequency (akin to intentional or unintentional inter-ference). We evaluated SCAN's performance with synthetic and real-world traces and in comparison with baselines. We show that SCAN can characterize multiple transmitters even when their power levels are the same. Furthermore, SCAN successfully detects and characterizes 10 simultaneously observed trans-mitters, whereas counterparts fall short even in 3-transmitter scenarios. Finally, we demonstrate that SCAN can discern real-world activity with WiFi, ZigBee, LTE and LoRa transmitters.}
}


@inproceedings{DBLP:conf/infocom/HanW0J0ZZL25,
	author = {Rongxin Han and
                  Jingyu Wang and
                  Haifeng Sun and
                  Zengteng Jiang and
                  Qi Qi and
                  Zirui Zhuang and
                  Yuan Zhang and
                  Jianxin Liao},
	title = {Network CoPilot: Intent-Driven Network Configuration Updating for
                  Service Guarantee},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044495},
	doi = {10.1109/INFOCOM55648.2025.11044495},
	timestamp = {Thu, 17 Jul 2025 16:37:43 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HanW0J0ZZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Updating network configurations not only requires satisfying new forwarding strategies, but also faces critical performance constraints brought on by various services. However, existing methods face significant limitations in interaction form, customization, and performance guarantees, making it challenging to keep pace with the rapid development of enterprise networks. Therefore, we propose a network copilot, a natural language intent-driven model for network configuration updating called NLI2Conj, which integrates graph and language models. We design network multimodal interfaces based on text attribute graphs to input network information in its raw format, including configuration files, topology, and performance metrics. This eliminates the need for additional specialized language design and learning. NLI2Conf understands forwarding strategies and performance constraints in natural language, and updates existing configurations, achieving alignment between intents and device behaviors. We provide a configuration collection process that mirrors the real network, and collect data samples from strategy intent to network configuration to facilitate NLI2Conj's capability of configuration updating. Extensive experiments demonstrate that NLI2Conf can accurately interpret network information and strategy intent in raw format. The consistency between configuration and strategy intent reaches 96.6 %, and the proposal reduces labor costs to 1/26.}
}


@inproceedings{DBLP:conf/infocom/KhanEKK25,
	author = {Akmal Khan and
                  Usama Ejaz and
                  Ted "Taekyoung" Kwon and
                  Hyunchul Kim},
	title = {Exploring the Internet Routing Registries to Augment AS-Level Topology},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044524},
	doi = {10.1109/INFOCOM55648.2025.11044524},
	timestamp = {Tue, 05 Aug 2025 22:40:40 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/KhanEKK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite substantial research efforts over the past decade, it remains a challenge to develop a complete and accurate view of the Internet Autonomous System (AS) level topology. Recent studies highlight that the incompleteness of the AS-level topology is greater than previously recognized. To mitigate the issue, we highlight the usefulness of the Internet Routing Registries (IRR), a set of databases used by ASes to register and share their inter-domain routing policies. We first propose a methodology to extract AS-level links (e.g., bilateral and multilateral peering links) from the IRR, extracting 3.1 M AS-level links; 48.7% of which can be matched with BGP, traceroute, or the cliques of Internet eXchange points (IXPs). Next, we find the active usage of the IRR by the member ASes of the IXPs, which helps us infer the peering matrices of large and small IXPs. Non-matching IRR AS links (51.3%) can be attributed to the limited coverage of BGP route collectors, especially in regions where local interconnections are heavily regulated, and to regional IXP interconnection policy dynamics.}
}


@inproceedings{DBLP:conf/infocom/BazganCNR25,
	author = {Cristina Bazgan and
                  Morgan Chopin and
                  Andr{\'{e}} Nichterlein and
                  Camille Richer},
	title = {Parameterized Complexity of Segment Routing},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044521},
	doi = {10.1109/INFOCOM55648.2025.11044521},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/BazganCNR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Segment Routing is a recent network technology that helps optimizing network throughput by providing finer control over the routing paths. Instead of routing directly from a source to a target, packets are routed via intermediate waypoints. Between consecutive waypoints, the packets are routed according to traditional shortest path routing protocols. Bottlenecks in the network can be avoided by such rerouting, preventing overloading parts of the network. The associated NP-hard computational problem is Segment Routing: Given a network and a set of traffic demands (vertex pairs), the task is to find for each demand pair the placement of a given number of waypoints such that with shortest path routing along these waypoints, all demands are fulfilled without exceeding the capacities of the network. We investigate if special structures of real-world communication networks could be exploited algorithmically. Our results comprise NP-hardness on graphs with constant treewidth even if only one waypoint per demand is allowed. We further exclude (under standard complexity assumptions) the existence of efficient exact algorithms even if we assume a fixed number of waypoints per demand and a “small” amount of traffic demands. We complement these lower bounds with polynomial-time solvable special cases.}
}


@inproceedings{DBLP:conf/infocom/HuangP25a,
	author = {Zhiming Huang and
                  Jianping Pan},
	title = {Faster Convergence for Unknown-Game Bandits},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044606},
	doi = {10.1109/INFOCOM55648.2025.11044606},
	timestamp = {Tue, 12 Aug 2025 21:29:13 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuangP25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study unknown-game bandits, where multiple agents play a general-sum game repeated over  T T  rounds. In each round, each agent independently selects an action and observes the reward for that action. The game is unknown to every agent, meaning each agent has no knowledge about the underlying game structure, the number of other agents, or their actions and rewards. Such unknown-game bandits have wide applications in computer and communication networks, including congestion control and network selection. The goal of each agent is to minimize swap regret, which measures the performance gap from a broader class of competitors than the traditional external regret that only compares against competitors always playing a fixed action. Our main contribution is to bridge the gap in the literature by proving the first swap-regret bound with a time-dependence of  O ~ ( T τ 4 ) \\tilde{O}(T^{\\frac{\\tau}{4}})  if the proposed learning algorithm based on optimistic follow-the-regularized-leader (OFTRL) is played by all agents involved in the game, where  O ~ ( ⋅ ) \\tilde{O}(\\cdot)  hides logarithmic factors. This regret bound demonstrates a faster convergence rate with respect to the number of rounds  τ \\tau  compared to the state-of-the-art swap regret bound of  O ( T 1 3 ) O(T^{\\frac{1}{3}}) . Furthermore, we demonstrate the efficacy of the proposed algorithm through an application in heterogeneous network selection with both numerical and simulation-based experiments.}
}


@inproceedings{DBLP:conf/infocom/ZhouHYSCELZ25,
	author = {Jiasheng Zhou and
                  Lin He and
                  Yifan Yang and
                  Xiaoyi Shi and
                  Daguo Cheng and
                  Jinlong E and
                  Ying Liu and
                  Dong Zhang},
	title = {6Map: Enabling Fast Active IPv6 Address Discovery with Programmable
                  Switches},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044705},
	doi = {10.1109/INFOCOM55648.2025.11044705},
	timestamp = {Thu, 17 Jul 2025 16:37:43 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouHYSCELZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The vast address space of IPv6 makes it impractical to apply exhaustive scanning to survey the entire network. Existing studies that aim to quickly discover active IPv6 addresses by optimizing the scanning space face issues of poor scalability and high time complexity, which hinder a comprehensive under-standing of the IPv6 network and impede the timely provision of security snapshots. Fortunately, the emergence of programmable switches provides an opportunity to address the above issues. To this end, we propose 6Map, a fast active IPv6 address discovery system based on programmable switches. We design a lightweight target generation algorithm running on the control plane of the programmable switch and implement fast scanning address generation on the switch ASIC. The experimental results show that the efficiency of IPv6 active address discovery increases by 40.1 × compared to the state-of-the-art method with a 100M probe budget.}
}


@inproceedings{DBLP:conf/infocom/ChadagaM25,
	author = {Sathwik Chadaga and
                  Eytan H. Modiano},
	title = {Drift Plus Optimistic Penalty - {A} Learning Framework for Stochastic
                  Network Optimization},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044621},
	doi = {10.1109/INFOCOM55648.2025.11044621},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChadagaM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of joint routing and scheduling in queueing networks, where the edge transmission costs are unknown. At each time-slot, the network controller receives noisy observations of transmission costs only for those edges it picks for transmission. The network controller's objective is to take routing and scheduling decisions so that the total expected cost is minimized. This problem exhibits an exploration-exploitation trade-off, however, previous bandit-style solutions cannot be directly applied to this problem due to the queueing dynamics. In order to ensure network stability, the network controller needs to optimize throughput and cost simultaneously. We show that the best achievable cost is lower bounded by the solution to a static optimization problem, and develop a network control policy using techniques from Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show that the policy achieves a sub-linear regret of order  O ( T 2 / 3 ) O(T^{2/3}) , as compared to the best policy that has complete knowledge of arrivals and costs. Finally, we evaluate the proposed policy using simulations and show that its regret is indeed sub-linear.}
}


@inproceedings{DBLP:conf/infocom/DongLLGTSD25,
	author = {Xuewen Dong and
                  Yi Liu and
                  Teng Li and
                  Xiaojie Guo and
                  Youliang Tian and
                  Yulong Shen and
                  Xiaojiang Du},
	title = {{LBFT-DAG:} {A} Swift, Leader-Driven, DAG-Based Consortium Blockchain
                  with Byzantine Fault-Tolerance},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044629},
	doi = {10.1109/INFOCOM55648.2025.11044629},
	timestamp = {Sun, 17 Aug 2025 16:45:45 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DongLLGTSD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to their parallel processing capabilities across different nodes, Directed Acyclic Graph (DAG) based blockchains have emerged as promising alternatives to traditional blockchains with a single-chain structure. However, existing DAG ledgers mostly have complex structures and rely on resource-intensive consensus protocols (e.g., PoW) to establish a stable main chain and determine the total order of all blocks. Moreover, they require a certain number of voting or successive PoW-like confirmations for each new block, resulting in significant delays. To tackle above issues, this paper proposes LBFT-DAG, the first Leader-drtven, Byzantine Fault- Iolerant, DAG-based consortium blockchain. In LBFT-DAG, a concise ledger consists of the leader node's main subchain and several normal nodes' parallel subchains, where each block only refers to the preceding blocks in two subchains. To quickly establish the total order of the ledger, we present a two-stage ordering algorithm following the principle of the main subchain first. Additionally, an asynchronous, Byzantine Fault-tolerant voting process on only leader blocks is designed to smoothly achieve security for all blocks. Extensive analysis confirms the security of LBFT-DAG, and experimental results demonstrate that LBFT-DAG achieves about four times higher transaction throughput on average compared to state-of-the-art solutions.}
}


@inproceedings{DBLP:conf/infocom/TengZZ0S025,
	author = {Yulin Teng and
                  Pinchang Zhang and
                  Shuangrui Zhao and
                  Xiaohong Jiang and
                  Yulong Shen and
                  Fu Xiao},
	title = {Secure Device Authentication for MmWave {MIMO} Systems via Mutual
                  Coupling and Spatial AoA},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044681},
	doi = {10.1109/INFOCOM55648.2025.11044681},
	timestamp = {Sun, 17 Aug 2025 16:45:45 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TengZZ0S025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of multiple-input multiple-output (MIMO) with the millimeter wave (mmWave) technique enables superior beamforming gain with highly directional beams, providing reliable communication services for massive devices. Nevertheless, the super-directive directional beams and the broadcast characteristics of mmWave MIMO systems render communication devices extremely susceptible to identity-based impersonation attacks. To combat such security threats, two-modal features including antenna array-specific mutual coupling (MC) and spatial angle of arrival (AoA) are jointly exploited to design a linear weighted combination (LWC) physical (PHY)-layer authentication scheme. Moreover, analytical expressions for authentication performance metrics like detection and false alarm probabilities are also derived to conduct a theoretical analysis of the proposed scheme. To fully achieve the collaborative authentication capability of adopted features, a sum-weighted approximation technique is utilized to establish the mapping relationship between feature weights and performance metrics. Based on such a relationship, an optimization problem for optimal feature weight values is then formulated to maximize the detection accuracy under a false alarm probability constraint. Finally, simulation results show that our scheme can reach 99.2 % detection accuracy, with performance improvements of 29% and 6 % compared to existing works.}
}


@inproceedings{DBLP:conf/infocom/JiangFZL25,
	author = {Linyi Jiang and
                  Silvery D. Fu and
                  Yifei Zhu and
                  Bo Li},
	title = {Janus: Collaborative Vision Transformer Under Dynamic Network Environment},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044626},
	doi = {10.1109/INFOCOM55648.2025.11044626},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/JiangFZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Network architectures and achieved state-of-the-art results in various computer vision tasks. Since ViTs are computationally expensive, the models either have to be pruned to run on resource-limited edge devices only or have to be executed on remote cloud servers after receiving the raw data transmitted over fluctuating networks. The resulting degraded performance or high latency all hinder their widespread applications. In this paper, we present Janus, the first framework for low-latency cloud-device collaborative Vision Transformer inference over dynamic networks. Janus overcomes the intrinsic model limitations of ViTs and realizes collaboratively executing ViT models on both cloud and edge devices, achieving low latency, high accuracy, and low communication overhead. Specifically, Janus judiciously combines token pruning techniques with a carefully designed fine-to-coarse model splitting policy and non-static mixed pruning policy. It attains a balance between accuracy and latency by dynamically selecting the optimal pruning level and split point. Experimental results across various tasks demonstrate that Janus enhances throughput by up to 5.15x and reduces latency violation ratios by up to 98.7% when compared with baseline approaches under various network environments.}
}


@inproceedings{DBLP:conf/infocom/LinLCLGZLL25,
	author = {Ruxin Lin and
                  Bingyan Liu and
                  Yelin Cui and
                  Duo Liu and
                  Ruipeng Gao and
                  Xiaoqiang Zhu and
                  Jiqiang Liu and
                  Lingkun Li},
	title = {LuminaLink: Enabling Low Cost Secure Visible Light Communication with
                  Birefringence},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044750},
	doi = {10.1109/INFOCOM55648.2025.11044750},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LinLCLGZLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Visible Light Communication (VLC) offers a promising solution for wireless communication due to its large spectrum, interference resistance, and inherent security features. However, the broadcast nature of VLC signals poses significant security and privacy challenges, particularly in the context of the Internet of Things (IoT) where low-cost and resource-constrained devices are prevalent. In this paper, we introduce LuminaLink, a novel position-based VLC system designed to establish a secure communication link between a transmitter and a receiver located at a designated position. Leveraging the principle of birefringence, LuminaLink encodes messages using Color-Shift Keying (CSK), ensuring that only the receiver at the specified position can correctly decode the transmitted information. To achieve this, we design an innovative interference chip using transparent adhesive tape as a birefringence material, making LuminaLink both cost-effective and easy to deploy. Our system eliminates the need for expensive cryptographic algorithms by employing an initialization process that establishes encoding and decoding mappings, significantly reducing computational overhead. Extensive experimental results demonstrate that LuminaLink achieves an average bit-error-rate (BER) of 0.030 at the designated position, while maintaining an average BER of 0.524 at other positions, confirming its effectiveness in securing VLC communications.}
}


@inproceedings{DBLP:conf/infocom/Guan0LWWWQHA025,
	author = {Jingjing Guan and
                  Hui Li and
                  Xiangdong Li and
                  Xiaolei Wang and
                  Binghan Wang and
                  Qiuye Wang and
                  Shengchao Qin and
                  Mengda He and
                  Md. Armanuzzaman and
                  Ziming Zhao},
	title = {Formally Verifying the State Machine of {TLS} 1.3 Handshake in OpenSSL},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044576},
	doi = {10.1109/INFOCOM55648.2025.11044576},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Guan0LWWWQHA025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The TLS handshake state machine manages the handshake messages exchanged in a session based on the parameters negotiated between the client and the server. Although the TLS 1.3 standard has undergone multiple rounds of analysis and revisions before official release to ensure its security, the process from natural language descriptions to implementations still relies on human expertise and is error-prone. In this paper, we propose a systematic method to conduct equivalence verification between the implementation of the TLS state machine and the standard. We also conduct formal verification of OpenSSL, the extensively utilized open-source TLS implementation for secure communications. Using Cryptol, we model the handshake state machine both from the standards and OpenSSL's implementation to perform its formal verification. Our automatic tool E-Verify performs equivalence verification by comparing the state transition sequences produced by the RFC model and OpenSSL model with all combinations of negotiation parameters. Guided by the verification results, we identify 640 mismatches out of a total of 1,536 scenarios and pinpoint the corresponding negotiation parameter combinations.}
}


@inproceedings{DBLP:conf/infocom/ZengZ0Z25,
	author = {Yifan Zeng and
                  Ruiting Zhou and
                  Lei Jiao and
                  Renli Zhang},
	title = {Online Scheduling of Edge Multiple- Model Inference with {DAG} Structure
                  and Retraining},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044490},
	doi = {10.1109/INFOCOM55648.2025.11044490},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZengZ0Z25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge inference applications are becoming increasingly complex and composed of multiple models. The dependency of models is modeled by a Directed Acyclic Graph (DAG). The accuracy of the edge model is easily affected by data drift. Retraining is employed to sustain the inference accuracy of models. But the introduction of retraining complicates the inter-task dependency of the inference request. Moreover, model retraining prolongs the inference completion time. The accuracy improvement and the latency increment under different retraining configurations necessitate a trade-off between inference accu-racy and request completion time. In this paper, we investigate multiple-model inference with retraining, aiming to maximize inference accuracy while minimizing request completion time. Through experimental analysis, we observed that retraining can enhance model inference accuracy in a short time. We represent the retraining tasks of models as nodes in the DAG of the inference request and then construct a unified DAG structure for both retraining and inference tasks. We first propose a Single Request Scheduling Algorithm (SRS) with a theoretical performance guarantee to select the optimal retraining configuration for each model under edge resource constraints and jointly schedule retraining and inference tasks. Subsequently, we extend SRS to a Multiple Requests Scheduling Algorithm (MRS) to address scheduling in a more general online multi-request scenario. The experiments on an edge system indicate that compared to existing methods, MRS can enhance the inference accuracy by 25 % while reducing the request completion time by 45 %.}
}


@inproceedings{DBLP:conf/infocom/LiuGLD025,
	author = {Yimeng Liu and
                  Maolin Gan and
                  Gen Li and
                  Younsuk Dong and
                  Zhichao Cao},
	title = {Adonis: Neural-enhanced Fine-grained Leaf Wetness Sensing with Efficient
                  mmWave Imaging},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044639},
	doi = {10.1109/INFOCOM55648.2025.11044639},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuGLD025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting Leaf Wetness Duration (LWD) is crucial for plant disease control. However, the lack of standardized techniques to measure LWD precisely hampers accurate prediction. While previous works have explored various methods, they fail to quantify the actual water on the leaf, undermining their practical effectiveness and accuracy. This paper presents Adonis, an innovative approach using millimeter-wave (mmWave) radar to address the complexities of leaf wetness detection. It introduces a new metric, Leaf Wetness Level (LWL), for measuring leaf surface water. We employ advanced signal processing on mmWave signals to extract more wetness-related features in dynamic environments. Furthermore, we develop a Contrastive Learning Feature Extraction model to precisely capture wetness features and design a calibration process for the inference stage to detect LWLs accurately in real-world fields. Using a frequency-modulated continuous-wave (FMCW) radar within the 77 to 81 GHz band, Adonis is meticulously evaluated across various plants. Adonis can detect LWLs with the mean absolute error (MAE) of 4.43 in controlled environments and 6.49 in real farm conditions. The performance significantly surpasses traditional Leaf Wetness Sensors, which have an MAE of 11.84 indoors and 14.32 in field conditions. These findings have substantial implications for enhancing disease prediction and crop management.}
}


@inproceedings{DBLP:conf/infocom/ZouSYG25,
	author = {Yu Zou and
                  Muyuan Shen and
                  Hao Yin and
                  Yayu Gao},
	title = {Delay Analysis of Multi-Link Devices Coexisting with Single-Link Devices
                  in Wi-Fi 7},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044683},
	doi = {10.1109/INFOCOM55648.2025.11044683},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZouSYG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-Link Operation (MLO) introduced in the IEEE 802.11be standard represents a significant advancement by facilitating high data rates and supporting latency-sensitive applications in Wi-Fi 7 via simultaneous utilization of multiple wireless interfaces. Nevertheless, newly-introduced multi-link devices (MLDs) will continue to coexist with a large number of legacy single- link devices (SLDs). In this paper, we propose a novel analytical model for multi-link Wi-Fi 7 networks with coexisting MLDs and SLDs, and study the end-to-end delay performance of MLDs under various traffic conditions. The analysis is verified by simulation results using the open-source simulator ns-3, which shows that the impact of SLDs on MLDs' delay depends on whether SLDs are saturated or unsaturated. Moreover, the analysis underscores the pivotal role of the link traffic allocation policy at the upper medium access control layer of MLDs for delay optimization, particularly when the multiple links are asymmetric due to distinct link conditions or different contention levels due to varying coexisting SLDs, which sheds important light for practical network design of multi-link Wi-Fi 7.}
}


@inproceedings{DBLP:conf/infocom/JinSMSWYW25,
	author = {Zitong Jin and
                  Xingang Shi and
                  Qiang Ma and
                  Letong Sun and
                  Zhiliang Wang and
                  Xia Yin and
                  Jianping Wu},
	title = {Which way to go? Inferring Fine-Grained {AS} Paths with PathRadar},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044440},
	doi = {10.1109/INFOCOM55648.2025.11044440},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/JinSMSWYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate comprehension of routing paths, i.e., how packets are routed from a source AS to a destination, is critical for analyzing and understanding the Internet. Previous path inference algorithms often oversimplify the complexity of Internet topology and routing policies, so they often fail to differentiate between paths to different prefixes within the same AS, and tend to be either inaccurate or incomplete. Based on an in-depth anal-ysis of observable routing behaviors, we introduce PathRadar, a novel framework for fine-grained AS path inference. PathRadar designs a Progressive Learning Process (PLP) that employs different inference models for different AS categories, leading to higher accuracy and completeness than existing methods. It also captures the intrinsic features of a large portion of non valley-free paths, and can accurately infer such paths which no existing method can infer. We evaluate PathRadar with comprehensive data collected by multiple measurement platforms. Compared with the latest algorithms, PathRadar can improve the accuracy and completeness of AS-level path inference by up to 6.7x, improve the accuracy for prefix-specific paths by up to 8.9 x, and reach the accuracy of 90% for non valley-free paths.}
}


@inproceedings{DBLP:conf/infocom/TeranishiAA25,
	author = {Yuuichi Teranishi and
                  Toyokazu Akiyama and
                  Kota Abe},
	title = {ByzSkip - {A} Byzantine-Resilient Skip Graph},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044766},
	doi = {10.1109/INFOCOM55648.2025.11044766},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TeranishiAA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain technology has accelerated the advancements of next-generation P2P data networks and decentralized architectures. However, current P2P data networks have significant limitations in handling interrelated data and executing range queries, while ensuring Byzantine fault tolerance. We present ByzSkip, a novel key-order preserving structured overlay network (KOPSON), resilient to Byzantine faults. Preserving the ordering relationship among keys in the structure enables ByzSkip to effectively handle interrelated data and execute range queries. ByzSkip extends the structure of Skip Graph, a well-studied KOPSON, by incorporating multiple neighbors. The search algorithm of ByzSkip provides fully decentralized routing that forwards the same message redundantly from  k k  distinct nodes to  k k  distinct nodes in each hop, which achieves a high search success ratio under the influence of adversarial routing attacks. ByzSkip updates the routing table by redundantly collecting each routing table entry to prevent pollution from adversarial nodes in eclipse attacks. Results of theoretical analysis, simulations, and experiments show the performance, resilience, and practicality of ByzSkip. ByzSkip achieves a small latency and high success ratio in searches under attacks, with small overheads to maintain the overlay network compared to the existing methods.}
}


@inproceedings{DBLP:conf/infocom/TangDJWZ25,
	author = {Huijun Tang and
                  Ming Du and
                  Pengfei Jiao and
                  Huaming Wu and
                  Zhidong Zhao},
	title = {HyperRole: Hyperbolic Graph Transformer for Role Discovery in Online
                  Social Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044749},
	doi = {10.1109/INFOCOM55648.2025.11044749},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TangDJWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Role discovery assist in various applications of online social networks, such as water army detection, shopping recommendation, rumor tracing, etc. However, existing studies often overlook the significance of hierarchical structures in online social networks, which are crucial for understanding the roles played by different users. To address this gap, we propose a novel approach based on hyperbolic graph learning, called HyperRole, which effectively leverages the hierarchical structure of online social networks for role discovery. HyperRole first extracts structural features from users and constructs user sequences based on feature similarity, capturing the relationships between users across different scales. Then, we learn role information from structural features by hyperbolic graph Transformer to embed users into the hyperbolic space, preserving the hierarchical structure between users and enabling interactions between users of the same level that are far away from each other. Additionally, we leverage the hierarchical distance between the target user and other users within the same sequence to guide and modify the role information of the target user. Based on the generated user role embeddings, we train a multi-class classifier to classify roles. Extensive experiments on several real-world network datasets demonstrate that our model outperforms existing baseline methods, showcasing its superior performance.}
}


@inproceedings{DBLP:conf/infocom/Shao0WYWL25,
	author = {Zerui Shao and
                  Beibei Li and
                  Zhibo Wang and
                  Yanbing Yang and
                  Peiran Wang and
                  Jun Luo},
	title = {FedUFD: Personalized Edge Computing Using Federated Uncertainty-Driven
                  Feature Distillation},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044664},
	doi = {10.1109/INFOCOM55648.2025.11044664},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Shao0WYWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, federated learning (FL) has been considered a promising and well-suited technique for edge computing applications, such as intelligent traffic control, autonomous driving, and mobile crowdsensing. However, since each edge device may perform individual-specific tasks, they often have heterogeneous data distributions that impact the performance of collaborative training models. Personalized FL (PFL) has then received considerable attention to tackle this problem. Many existing PFL works often employ knowledge distillation to mitigate the negative effects of data heterogeneity. Nevertheless, these works often neglect the fact that the knowledge transferred from the teacher models is not completely correct, which limits the personalization performance of edge devices. In this work, we leverage the knowledge contained in global features to explore the potential of global models and propose a novel uncertainty-driven feature distillation framework called FedUFD. Specifically, we design an uncertainty estimation module in local models, by estimating the uncertainty of the personalized feature distribution, FedUFD can measure the difficulty of learning different personalized features, and then combine the global features to distill the corresponding personalized features. Extensive experiments show that FedUFD outperforms fourteen state-of-the-art PFL frameworks in edge computing, beating the best-performing traditional and personalized baselines by up to 45.45% and 3.55%, respectively.}
}


@inproceedings{DBLP:conf/infocom/YeOZQ0TC25,
	author = {Shengyuan Ye and
                  Bei Ouyang and
                  Liekang Zeng and
                  Tianyi Qian and
                  Xiaowen Chu and
                  Jian Tang and
                  Xu Chen},
	title = {Jupiter: Fast and Resource-Efficient Collaborative Inference of Generative
                  LLMs on Edge Devices},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044734},
	doi = {10.1109/INFOCOM55648.2025.11044734},
	timestamp = {Tue, 08 Jul 2025 07:36:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YeOZQ0TC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generative large language models (LLMs) have garnered significant attention due to their exceptional capabilities in various AI tasks. Traditionally deployed in cloud datacenters, LLMs are now increasingly moving towards more accessible edge platforms to protect sensitive user data and ensure privacy preservation. The limited computational resources of individual edge devices, however, can result in excessively prolonged in-ference latency and overwhelmed memory usage. While existing research has explored collaborative edge computing to break the resource wall of individual devices, these solutions yet suffer from massive communication overhead and under-utilization of edge resources. Furthermore, they focus exclusively on optimizing the prefill phase, neglecting the crucial autoregressive decoding phase for generative LLMs. To address that, we propose Jupiter, a fast, scalable, and resource-efficient collaborative edge AI system for generative LLM inference. Jupiter introduces a flexible pipelined architecture as a principle and differentiates its system design according to the differentiated characteristics of the prefill and decoding phases. For prefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and develops a meticulous parallelism planning strategy to maximize resource efficiency; For decoding, Jupiter devises an effective outline-based pipeline parallel decoding mechanism combined with speculative decoding, which further magnifies inference acceleration. Extensive evaluation based on realistic implementation demon-strates that Jupiter remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving up to 26.1 × end-to-end latency reduction while rendering on-par generation quality.}
}


@inproceedings{DBLP:conf/infocom/FanLXL25,
	author = {Yu Fan and
                  Jingyao Liu and
                  Pengjin Xie and
                  Liang Liu},
	title = {Aether: Toward Generalized Traffic Engineering with Elastic Multi-agent
                  Graph Transformers},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044607},
	doi = {10.1109/INFOCOM55648.2025.11044607},
	timestamp = {Tue, 08 Jul 2025 07:36:33 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FanLXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent algorithms show deep learning's potential to efficiently allocate traffic flows in wide-area networks (WANs). However, current learning-based methods ignore the importance of edges correlations and traffic differentiation, and have limitations in generalizing to multiple networks, let alone unseen ones, making them impractical. In this paper, we propose a novel traffic engineering algorithm, Aether, which excels in generalizing across different networks and different amount of demands. We propose an elastic multi-agent graph transformer where each agent process a demand, and agents are sequentially modeled by graph transformer to enhance both representation capability and generalization ability. To improve effectiveness, we propose hierarchical graph neural networks which model inter- and intra-relations between edges and paths. And a differentiated traffic strategy is proposed which handles small flows with rules, letting the model focuses on larger flows for better learning. Experiments on real-world data show that our model outperforms state-of-the-art learning-based approaches, achieving 12.1 % to 31.7% better MLU without network-specific training, while demonstrating scalability for large networks. It is worth highlighting that Aether's performance is only 3.9% lower on MLU compared to its network-specific trained version.}
}


@inproceedings{DBLP:conf/infocom/ChenNRC25,
	author = {Yichong Chen and
                  Zifeng Niu and
                  Manuel Roveri and
                  Giuliano Casale},
	title = {{CEED:} Collaborative Early Exit Neural Network Inference at the Edge},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044557},
	doi = {10.1109/INFOCOM55648.2025.11044557},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenNRC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative inference at the edge has gained traction in recent years as one of the main trends within edge computing. The early exit neural network (EENN) architecture supports this by balancing inference time and accuracy with configurable early exit thresholds within the neural network. Such thresholds enable the dynamic tuning of the processing latency of a job based on confidence scores. However, most distributed EENN setups use a preset confidence threshold and assume constant data arrivals. This assumption exposes the system to potential data loss due to finite memory capacity in the edge devices. To address these issues, we propose CEED, an AI-based optimization framework to enable collaborative EENN inference on a multilayer edge infrastructure. CEED integrates an EENN predictor and a Loss ratio predictor to rapidly evaluate confidence threshold configurations and job assignment to devices. Experiments conducted on a physical testbed show that CEED significantly improves existing EENN inference methods by striking a better balance between end-to-end system loss ratio and EENN inference accuracy.}
}


@inproceedings{DBLP:conf/infocom/XuZLCSLLWLRLD25,
	author = {Duling Xu and
                  Dafang Zhang and
                  Tong Li and
                  Yunpeng Chai and
                  Zegang Sun and
                  Weiming Li and
                  Yangfan Liu and
                  Qipeng Wang and
                  Jiaqi Liang and
                  Yang Ren and
                  Wei Lu and
                  Xiaoyong Du},
	title = {GeoLM: Performance-oriented Leader Management for Geo-Distributed
                  Consensus Protocol},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044598},
	doi = {10.1109/INFOCOM55648.2025.11044598},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XuZLCSLLWLRLD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The global business of transnational enterprises demands geo-distributed databases, where the leader-follower-based consensus protocols are the key to guaranteeing consistency of replicas spread across regions. Compared with traditional databases running in a single data center, determining which node is the leader in consensus protocol has a greater per-formance impact in geo-distributed databases running across multiple data centers. However, the performance of legacy leader management is far from satisfactory due to the network and application dynamics (e.g., network delay, node popularity, operation read-write ratio). This paper proposes GeoLM toward performance-oriented leader management for geo-distributed consensus protocols. GeoLM captures the network and application dynamics and proactively conducts seamless leader handovers with bounded switching costs. Our geo-distributed experimental results show that GeoLM improves performance up to 49.75% over the baselines (e.g., Raft and Geo-Raft) and achieves considerably good performance compared to state-of-the-art consensus protocols (e.g., SwiftPaxos, CURP, and EPaxos).}
}


@inproceedings{DBLP:conf/infocom/PanZHWLWL25,
	author = {Yongchen Pan and
                  Jiao Zhang and
                  Yuxuan Hu and
                  Zirui Wan and
                  Baohong Lin and
                  Junliang Wang and
                  Huimin Luo},
	title = {Weir: Delay-based {RNIC} Cache Control Software Middleware for Scalable
                  {RDMA} Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044610},
	doi = {10.1109/INFOCOM55648.2025.11044610},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/PanZHWLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RDMA is widely used in distributed services in DCNs due to its high performance. As DCNs expand in scale, RDMA faces scalability issues. The reason is that the high concurrency Queue Pairs (QPs) lead to cache misses on RNIC and frequent evictions, and the behaviour of fetching the cache via PCIe leads to performance degradation of RDMA. In this paper, we model the behaviour of Work Queue Element (WQE) on RNIC as a producer-consumer model and investigate that the root cause of WQE cache misses is the mismatch between the production rate of the CPU and the consumption rate of the RNIC. We design Weir from the perspective of WQE cache control to avoid cache misses and improve throughput under high concurrent QPs. Weir determines the cache occupancy on the RNIC by monitoring the number of active QPs and the increase/decrease in the life cycle of WQEs, and calculates the production rate and pacing by credit. The implementation of Weir exhibits minimal CPU overhead. Evaluation results show that Weir can maintain 97Gbps throughput without degradation even with up to 16K concurrent QPs, and effectively reduces various observable cache misses by 5x to 10x.}
}


@inproceedings{DBLP:conf/infocom/YangFCGLLJ25,
	author = {Zhaoxing Yang and
                  Guiyun Fan and
                  Anjie Cao and
                  Yuchen Guo and
                  Wenlong Li and
                  Tianyuan Liu and
                  Haiming Jin},
	title = {Learning to Accelerate Traffic Allocation Over Large-Scale Networks},
	booktitle = {{IEEE} {INFOCOM} 2025 - {IEEE} Conference on Computer Communications,
                  London, United Kingdom, May 19-22, 2025},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2025},
	url = {https://doi.org/10.1109/INFOCOM55648.2025.11044775},
	doi = {10.1109/INFOCOM55648.2025.11044775},
	timestamp = {Mon, 07 Jul 2025 17:58:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangFCGLLJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In network systems with large-scale nodes and links, how to allocate traffic in real-time when both data transmission demands and link connections vary dynamically over time, in order to maximize the network's long-term total throughput under link capacity constraints, is a fundamental problem. However, state-of-the-art (SOTA) works lack scalability due to the requirement of solving the time-consuming constrained optimization problems online. Further, their reliance on reinforcement learning algorithms makes them inefficient to explore and train when facing systems with large-scale nodes and complex topologies. In this paper, we formulate such a challenging problem as the Dynamic Traffic Allocation (DTA) problem, which is generic and applicable across systems. Then, we propose the Fast Networked Control (FNC) policy framework for DTA, which removes the requirement of solving constrained optimization problems online, and only utilizes basic operations such as normalizations and comparisons to achieve fast constraints satisfaction. Further, we propose an imitation learning-based algorithm for efficiently optimizing the FNC policy. Experiments in large-scale networks show that our FNC policy achieves a maximal 10% improvement in demands satisfaction and 60x reduction in latency at the same time versus SOTA works.}
}
