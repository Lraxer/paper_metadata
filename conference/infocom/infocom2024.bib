@inproceedings{DBLP:conf/infocom/Xu0LLGL24,
	author = {Yonghuan Xu and
                  Ming Yang and
                  Zhen Ling and
                  Zixia Liu and
                  Xiaodan Gu and
                  Lan Luo},
	title = {A De-anonymization Attack against Downloaders in Freenet},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621209},
	doi = {10.1109/INFOCOM52122.2024.10621209},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Xu0LLGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Freenet is a well-known anonymous communication system that enables file sharing among users. It employs a probabilistic hops-to-live (HTL) decrement approach to hide the originator among nodes in a multi-hop path. Therefore, all nodes shall exhibit identical behaviors to preserve anonymity. However, we discover that the path folding mechanism in Freenet violates this principle due to behavior discrepancy between downloaders and intermediate nodes. The path folding mechanism is designed to optimize the network topology of Freenet. A delayed path folding message by a successor node may incur a timeout event at its predecessor, and an intermediate node reacts differently to such timeout with a downloader. Therefore, malicious nodes can deliberately trigger the timeout event to identify downloaders. The complex implementation of the path folding timeout detection mechanism in Freenet complicates our de-anonymization attack. We thoroughly analyze the underlying cause and develop three strategies to manipulate three types of messages respectively at the malicious node, minimizing the false positive rate. We conduct extensive real-world experiments to verify the feasibility and effectiveness of our attack. They show that our attack achieves a true positive rate of 100% and false positive rate of near 0% under two different Freenet download modes.}
}


@inproceedings{DBLP:conf/infocom/Feng0CTL24,
	author = {Dou Feng and
                  Lin Wang and
                  Shutong Chen and
                  Lingching Tung and
                  Fangming Liu},
	title = {X-Stream: {A} Flexible, Adaptive Video Transformer for Privacy-Preserving
                  Video Stream Analytics},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621341},
	doi = {10.1109/INFOCOM52122.2024.10621341},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Feng0CTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video stream analytics (VSA) systems fuel many exciting applications that facilitate people’s lives, but also raise critical concerns about exposing too much individuals’ privacy. To alleviate these concerns, various frameworks have been presented to enhance the privacy of VSA systems. Yet, existing solutions suffer two limitations: (1) being scenario-customized, thus limiting the generality of adapting to multifarious scenarios, (2) requiring complex, imperative programming, and tedious process, thus largely reducing the usability of such systems. In this paper, we present X-Stream, a privacy-preserving video transformer that achieves flexibility and efficiency for a large variety of VSA tasks. X-Stream features three major novel designs: (1) a declarative query interface that provides a simple yet expressive interface for users to describe both their privacy protection and content exposure requirements, (2) an adaptation mechanism that dynamically selects the most suitable privacy-preserving techniques and their parameters based on the current video context, and (3) an efficient execution engine that incorporates optimizations for multi-task deduplication and inter-frame inference. We implement X-Stream and evaluate it with representative VSA tasks and public video datasets. The results show that X-Stream achieves significantly improved privacy protection quality and performance over the state-of-the-art, while being simple to use.}
}


@inproceedings{DBLP:conf/infocom/XuLZWWL24,
	author = {Hao Xu and
                  Xiulong Liu and
                  Chenyu Zhang and
                  Wenbin Wang and
                  Jianrong Wang and
                  Keqiu Li},
	title = {Crackle: {A} Fast Sector-based {BFT} Consensus with Sublinear Communication
                  Complexity},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1--10},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621123},
	doi = {10.1109/INFOCOM52122.2024.10621123},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XuLZWWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain systems widely employ Byzantine fault-tolerant (BFT) protocols to ensure consistency. Improving BFT protocols’ throughput is crucial for large-scale blockchain systems. Frontier protocols face crucial problems: (i) the binary dilemma between leader bottleneck in star-based linear communication and compromised resilience in tree-based sublinear communication; and (ii) 2- or 3-round protocols restrict the phase number of one proposal, thereby limiting the scalability and parallelism of the pipeline. To overcome the above problems, this paper proposes Crackle, the first sector-based pipelined BFT protocol with a sublinear communication complexity, for a throughput improvement of consensus protocol with max resilience of (N-1)/3. We propose a sector-based communication mode to disseminate messages from the leader to a subset of replicas in each phase to accelerate consensus and split the traditional two-round protocol into 2κ phases to increase the basic pipeline scale. When implementing Crackle, we address two technical challenges: (i) to ensure Quorum Certificate (QC) validation during continuous κ phases, we design a voteMap field within each block, and verify QC by the aggregation of continuous κ voteMaps; and (ii) to achieve pipeline decoupling among shorter phases, we propose a vote-appending mechanism that accelerates the leader’s transition to the next phase. We provide comprehensive theoretical proof of the correctness of Crackle, including safety and liveness. Moreover, we implement Crackle based on a public BFT framework and deploy it on 64 cloud servers. Real experimental results reveal that Crackle achieves up to 10.36x higher throughput compared with state-of-the-art BFT protocols such as Kauri and Hotstuff.}
}


@inproceedings{DBLP:conf/infocom/QianLXWZCF24,
	author = {Xinyuan Qian and
                  Hongwei Li and
                  Guowen Xu and
                  Haoyong Wang and
                  Tianwei Zhang and
                  Xianhao Chen and
                  Yuguang Fang},
	title = {Privacy-Preserving Data Evaluation via Functional Encryption, Revisited},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {11--20},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621262},
	doi = {10.1109/INFOCOM52122.2024.10621262},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QianLXWZCF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In cloud-based data marketplaces, the cardinal objective lies in facilitating interactions between data shoppers and sellers. This engagement allows shoppers to augment their internal datasets with external data, consequently leading to significant enhancements in their machine learning models. Nonetheless, given the potential diversity of data values, it becomes critical for consumers to assess the value of data before cementing any transactions. Recently, Song et al. introduced Primal (publish in ACSAC), the pioneering cloud-assisted privacy-preserving data evaluation (PPDE) strategy. This strategy relies on variants of functional encryption (FE) as the underlying framework, conferring notable performance advantages over alternative cryptographic primitives such as secure multi-party computation and homomorphic encryption. However, in this paper, we regretfully highlight that Primal is susceptible to inadvertent misuse of FE, and leaves much-desired room for performance amelioration. To combat this, we introduce a novel cryptographic primitive known as labeled function-hiding inner-product encrypted. This new primitive serves as a remedy and forms the foundation for designing the concrete framework for PPDE. Furthermore, experiments conducted on real datasets demonstrate that our framework significantly reduces the overall computation cost of the current state-of-the-art secure PPDE scheme by roughly 10× and the communication cost for the data seller by about 2×.}
}


@inproceedings{DBLP:conf/infocom/LiuWZC24,
	author = {Yu Liu and
                  Zibo Wang and
                  Yifei Zhu and
                  Chen Chen},
	title = {DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated
                  Learning as a Service},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {21--30},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621227},
	doi = {10.1109/INFOCOM52122.2024.10621227},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuWZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged as a prevalent distributed machine learning scheme that enables collaborative model training without aggregating raw data. Cloud service providers further embrace Federated Learning as a Service (FLaaS), allowing data analysts to execute their FL training pipelines over differentially-protected data. Due to the intrinsic properties of differential privacy, the enforced privacy level on data blocks can be viewed as a privacy budget that requires careful scheduling to cater to diverse training pipelines. Existing privacy budget scheduling studies prioritize either efficiency or fairness individually. In this paper, we propose DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes both efficiency and fairness. We first develop a comprehensive utility function incorporating data analyst-level dominant shares and FL-specific performance metrics. A sequential allocation mechanism is then designed using the Lagrange multiplier method and effective greedy heuristics. We theoretically prove that DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and Weak Strategy Proofness. We also theoretically prove the existence of a fairness-efficiency tradeoff in privacy budgeting. Extensive experiments demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an average efficiency improvement of 1.44× ~ 3.49×, and an average fairness improvement of 1.37×~24.32×.}
}


@inproceedings{DBLP:conf/infocom/0003PC024,
	author = {Shaowei Wang and
                  Yun Peng and
                  Kongyang Chen and
                  Wei Yang},
	title = {Optimal Locally Private Data Stream Analytics},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {31--40},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621194},
	doi = {10.1109/INFOCOM52122.2024.10621194},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0003PC024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online data analytics with local privacy protection is widely adopted in real-world applications. Despite numerous endeavors in this field, significant gaps in utility and functionality remain when compared to its offline counterpart. This work demonstrates that private data analytics can be conducted online without excess utility loss, even at a constant factor. We present an optimal, streamable mechanism for local differentially private sparse vector estimation. The mechanism enables a range of online analytics on streaming binary vectors, including multi-dimensional binary, categorical, or set-valued data. By leveraging the negative correlation of occurrence events in the sparse vector, we attain an optimal error rate under local privacy constraints, only requiring streamable computations during the input’s data-dependent phase. Through experiments with both synthetic and real-world datasets, our proposals have been shown to reduce error rates by 40% to 60% compared to SOTA approaches.}
}


@inproceedings{DBLP:conf/infocom/ScalingiD0MG24,
	author = {Alessio Scalingi and
                  Salvatore D'Oro and
                  Francesco Restuccia and
                  Tommaso Melodia and
                  Domenico Giustiniano},
	title = {Det-RAN: Data-Driven Cross-Layer Real-Time Attack Detection in 5G
                  Open RANs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {41--50},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621223},
	doi = {10.1109/INFOCOM52122.2024.10621223},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ScalingiD0MG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fifth generation (5G) and beyond cellular networks are vulnerable to security threats, primarily due to the lack of integrity protection in the Radio Resource Control (RRC) layer. In order to address this problem, we propose a real-time anomaly detection framework that leverages the concept of distributed applications in 5G Open RAN networks. Specifically, we identify Physical Layer (PHY) features that can generate a reliable fingerprint, infer in a novel way the time of arrival of uplink packets lacking integrity protection, and handle cross-layer features. By identifying legitimate message sources and detecting suspicious activities through an Artificial Intelligence (AI) design, we demonstrate that Open RAN-based applications that run at the edge can be designed to provide additional security to the network. Our solution is first validated in extensive emulation environments achieving over 85% accuracy in predicting potential attacks on unseen test scenarios. We then integrate our approach into a real-world prototype with a large channel emulator to assess its real-time performance and costs. Our solution meets the low-latency real-time constraints of 2 ms, making it well-suited for real-world deployments.}
}


@inproceedings{DBLP:conf/infocom/LvGDLYY024,
	author = {Jiamei Lv and
                  Yi Gao and
                  Zhi Ding and
                  Yuxiang Lin and
                  Xinyun You and
                  Guang Yang and
                  Wei Dong},
	title = {Providing UE-level QoS Support by Joint Scheduling and Orchestration
                  for 5G vRAN},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {51--60},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621408},
	doi = {10.1109/INFOCOM52122.2024.10621408},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LvGDLYY024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtualized radio access networks (vRAN) enable network operators to run RAN functions on commodity servers instead of proprietary hardware. It has garnered significant interest due to its ability to reduce costs, provide deployment flexibility, and offer other benefits, particularly for operators of 5G private networks. However, the non-deterministic computing platforms pose difficulties to effective quality of service (QoS) provision, especially in the case of hybrid deployment of time-critical and throughput-demanding applications. Existing approaches including network slicing and other resource management schemes fail to provide fine-grained and effective QoS support at the User Equipments level. In this paper, we propose UQ-vRAN, a UE-level QoS provision framework. UQ-vRAN presents the first comprehensive analysis of the complicated impacts among key network parameters, e.g., network function splitting, resource block allocation, and modulation/coding scheme selection and builds an accurate and comprehensive network model. UQ-vRAN also provides a fast network configurator which gives feasible configurations in seconds, making it possible to be practical in actual 5G vRAN. We implement UQ-vRAN on OpenAirInterface and use simulation and testbed-base experiments to evaluate it. Results show that compared with existing works, UQ-vRAN reduces the delay violation rate by 12%–41% under various network settings, while minimizing the total energy consumption.}
}


@inproceedings{DBLP:conf/infocom/Adamuz-Hinojosa24,
	author = {Oscar Adamuz{-}Hinojosa and
                  Lanfranco Zanzi and
                  Vincenzo Sciancalepore and
                  Andres Garcia{-}Saavedra and
                  Xavier Costa{-}P{\'{e}}rez},
	title = {{ORANUS:} Latency-tailored Orchestration via Stochastic Network Calculus
                  in 6G {O-RAN}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {61--70},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621356},
	doi = {10.1109/INFOCOM52122.2024.10621356},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Adamuz-Hinojosa24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Open Radio Access Network (O-RAN)-compliant solutions lack crucial details to perform effective control loops at multiple time scales. In this vein, we propose ORANUS, an O-RAN-compliant mathematical framework to allocate radio resources to multiple ultra Reliable Low Latency Communication (uRLLC) services. In the near-RT control loop, ORANUS relies on a novel Stochastic Network Calculus (SNC)-based model to compute the amount of guaranteed radio resources for each uRLLC service. Unlike traditional approaches as queueing theory, the SNC-based model allows ORANUS to ensure the probability the packet transmission delay exceeds a budget, i.e., the violation probability, is below a target tolerance. ORANUS also utilizes an RT control loop to monitor service transmission queues, dynamically adjusting the guaranteed radio resources based on detected traffic anomalies. To the best of our knowledge, ORANUS is the first O-RAN-compliant solution which benefits from SNC to carry out near-RT and RT control loops. Simulation results show that ORANUS significantly improves over reference solutions, with an average violation probability 10× lower.}
}


@inproceedings{DBLP:conf/infocom/MungariPGC24,
	author = {Federico Mungari and
                  Corrado Puligheddu and
                  Andres Garcia{-}Saavedra and
                  Carla{-}Fabiana Chiasserini},
	title = {{OREO:} {O-RAN} intElligence Orchestration of xApp-based network services},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {71--80},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621166},
	doi = {10.1109/INFOCOM52122.2024.10621166},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MungariPGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Open Radio Access Network (O-RAN) architecture aims to support a plethora of network services, such as beam management and network slicing, through the use of third-party applications called xApps. To efficiently provide network services at the radio interface, it is thus essential that the deployment of the xApps is carefully orchestrated. In this paper, we introduce OREO, an O-RAN xApp orchestrator, designed to maximize the offered services. OREO’s key idea is that services can share xApps whenever they correspond to semantically equivalent functions, and the xApp output is of sufficient quality to fulfill the service requirements. By leveraging a multi-layer graph model that captures all the system components, from services to xApps, OREO implements an algorithmic solution that selects the best service configuration, maximizes the number of shared xApps, and efficiently and dynamically allocates resources to them. Numerical results as well as experimental tests performed using our proof-of-concept implementation, demonstrate that OREO closely matches the optimum, obtained by solving an NP-hard problem. Further, it outperforms the state of the art, deploying up to 35% more services with an average of 30% fewer xApps and a similar reduction in the resource consumption.}
}


@inproceedings{DBLP:conf/infocom/MatsonS24,
	author = {N. Cameron Matson and
                  Karthikeyan Sundaresan},
	title = {Online Radio Environment Map Creation via {UAV} Vision for Aerial
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {81--90},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621219},
	doi = {10.1109/INFOCOM52122.2024.10621219},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MatsonS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio environment maps provide a comprehensive spatial view of the wireless channel and are especially useful in on-demand UAV wireless networks where operators are not afforded the typical time spent planning base station deployments (e.g. emergency response). Equipped with an accurate radio environment map, a mobile UAV can quickly locate to an optimal location to serve users on the ground. Machine learning has recently been proposed as a tool to create radio environment maps from satellite images of the target environment. However, the highly dynamic nature that precipitates most on-demand aerial network deployments likely renders the satellite image data available for the environment to be inaccurate. In this paper we present, OREMAN, a hybrid offline-online system for aerial radio environment map creation which leverages a common sensing modality present on most UAVs: visual cameras. OREMAN combines a suite of off-line trained neural network models with an adaptive trajectory planning algorithm to iteratively predict/refine the radio environment map and estimate the most valuable trajectory locations. By using UAV vision, OREMAN arrives at a highly accurate map much faster and with fewer measurements than other approaches, and is very effective even in scenarios where no prior environmental knowledge is available.}
}


@inproceedings{DBLP:conf/infocom/Sun0HM0024,
	author = {Zemin Sun and
                  Geng Sun and
                  Long He and
                  Fang Mei and
                  Shuang Liang and
                  Yanheng Liu},
	title = {A Two Time-Scale Joint Optimization Approach for UAV-assisted {MEC}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {91--100},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621095},
	doi = {10.1109/INFOCOM52122.2024.10621095},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Sun0HM0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAV)-assisted mobile edge computing (MEC) is emerging as a promising paradigm to provide aerial-terrestrial computing services close to mobile devices (MDs). However, meeting the demands of computation-intensive and delay-sensitive tasks for MDs poses several challenges, including the demand-supply contradiction between MDs and MEC servers, the demand-supply heterogeneity between MDs and MEC servers, the trajectory control requirements on energy efficiency and timeliness, and the different timescale dynamics of the network. To address these issues, we first present a hierarchical architecture by incorporating terrestrial-aerial computing capabilities and leveraging UAV flexibility. Furthermore, we formulate a joint computing resource allocation, computation offloading, and trajectory control problem to maximize the system utility. Since the problem is a non-convex mixed integer nonlinear programming (MINLP), we propose a two timescale joint computing resource allocation, computation offloading, and trajectory control (TJCCT) approach. In the short time scale, we propose a price-incentive method for on-demand computing resource allocation and a matching mechanism-based method for computation offloading. In the long time scale, we propose a convex optimization-based method for UAV trajectory control. Besides, we prove the stability, optimality, and polynomial complexity of TJCCT. Simulation results demonstrate that TJCCT outperforms the comparative algorithms in terms of the utility of the system, the QoE of MDs, and the revenue of MEC servers.}
}


@inproceedings{DBLP:conf/infocom/HeSSW00N24,
	author = {Long He and
                  Geng Sun and
                  Zemin Sun and
                  Pengfei Wang and
                  Jiahui Li and
                  Shuang Liang and
                  Dusit Niyato},
	title = {An Online Joint Optimization Approach for QoE Maximization in UAV-Enabled
                  Mobile Edge Computing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {101--110},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621306},
	doi = {10.1109/INFOCOM52122.2024.10621306},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HeSSW00N24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given flexible mobility, rapid deployment, and low cost, unmanned aerial vehicle (UAV)-enabled mobile edge computing (MEC) shows great potential to compensate for the lack of terrestrial edge computing coverage. However, limited battery capacity, computing and spectrum resources also pose serious challenges for UAV-enabled MEC, which shorten the service time of UAVs and degrade the quality of experience (QoE) of user devices (UDs) without effective control approach. In this work, we consider a UAV-enabled MEC scenario where a UAV serves as an aerial edge server to provide computing services for multiple ground UDs. Then, a joint task offloading, resource allocation, and UAV trajectory planning optimization problem (JTRTOP) is formulated to maximize the QoE of UDs under the UAV energy consumption constraint. To solve the JTRTOP that is proved to be a future-dependent and NP-hard problem, an online joint optimization approach (OJOA) is proposed. Specifically, the JTRTOP is first transformed into a per-slot real-time optimization problem (PROP) by using the Lyapunov optimization framework. Then, a two-stage optimization method based on game theory and convex optimization is proposed to solve the PROP. Simulation results validate that the proposed approach can achieve superior system performance compared to the other benchmark schemes.}
}


@inproceedings{DBLP:conf/infocom/ChangKKCW24,
	author = {Shu{-}Wei Chang and
                  Jian{-}Jhih Kuo and
                  Mong{-}Jen Kao and
                  Bozhong Chen and
                  Qian{-}Jing Wang},
	title = {Near-Optimal {UAV} Deployment for Delay-Bounded Data Collection in
                  IoT Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {111--120},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621135},
	doi = {10.1109/INFOCOM52122.2024.10621135},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChangKKCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid growth of Internet of Things (IoT) applications has spurred the need for efficient data collection mechanisms. Traditional approaches relying on fixed infrastructure have limitations in coverage, scalability, and deployment costs. Unmanned Aerial Vehicles (UAVs) have emerged as a promising alternative due to their mobility and flexibility. In this paper, we aim to minimize the number of UAVs deployed to collect data in IoT networks while considering a delay budget for energy limitation and data freshness. To this end, we propose a novel 3-approximation dynamic-programming-based algorithm called GPUDA to address the challenges of efficient data collection from IoT devices via UAVs for real-world scenarios where the number of UAVs owned by an individual or organization is unlikely to be excessive, improving the best-known approximation ratio of 4. GPUDA is a geometric partition-based method that incorporates data rounding techniques. The experimental results demonstrate that the proposed algorithm requires 35.01% to 58.55% fewer deployed UAVs than the existing algorithms on average.}
}


@inproceedings{DBLP:conf/infocom/Xia0RZWCZ24,
	author = {Tengxi Xia and
                  Ju Ren and
                  Wei Rao and
                  Qin Zu and
                  Wenjie Wang and
                  Shuai Chen and
                  Yaoxue Zhang},
	title = {AeroRec: An Efficient On-Device Recommendation Framework using Federated
                  Self-Supervised Knowledge Distillation},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {121--130},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621240},
	doi = {10.1109/INFOCOM52122.2024.10621240},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Xia0RZWCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern recommendation systems operate entirely on the basis of central servers, requiring users to upload their behavior data from mobile devices to these servers. This practice has raised concerns about data privacy among users. Federated learning (FL), a machine learning technique designed to protect privacy, is becoming the standard solution. By combining federated learning with recommendation systems, Federated Recommendation Systems (FRS) allow users to collaboratively train a shared recommendation model without transmitting their original behavior data. However, existing federated learning solutions disregard the inherent limitations of resource-constrained mobile devices, which include limited storage space, computational overhead, and communication bandwidth. Although deploying a lightweight recommendation model can address the constraints of mobile devices, achieving satisfactory accuracy is difficult even with substantial communication overhead incurred during multiple rounds of federated learning training due to the limitations of the lightweight model’s capabilities and the sparsity of recommendation data. To address this issue, we propose AeroRec, a n e fficient on-device Feder ated Recommendation Framework. In AeroRec, we use federated self-supervised distillation to enhance the global model after each parameter aggregation in each round. This approach not only accelerates the convergence rate but also surpasses the upper limit of the lightweight model’s capability, thereby enabling a higher accuracy. We demonstrate that AeroRec outperforms several state-of-the-art FRS frameworks regarding recommendation accuracy and convergence speed through extensive experiments on three real-world datasets.}
}


@inproceedings{DBLP:conf/infocom/WuSWLGPHJ24,
	author = {Zhiyuan Wu and
                  Sheng Sun and
                  Yuwei Wang and
                  Min Liu and
                  Bo Gao and
                  Quyang Pan and
                  Tianliu He and
                  Xuefeng Jiang},
	title = {Agglomerative Federated Learning: Empowering Larger Model Training
                  via End-Edge-Cloud Collaboration},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {131--140},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621254},
	doi = {10.1109/INFOCOM52122.2024.10621254},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WuSWLGPHJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables training Artificial Intelligence (AI) models over end devices without compromising their privacy. As computing tasks are increasingly performed by a combination of cloud, edge, and end devices, FL can benefit from this End-Edge-Cloud Collaboration (EECC) paradigm to achieve collaborative device-scale expansion with real-time access. Although Hierarchical Federated Learning (HFL) supports multitier model aggregation suitable for EECC, prior works assume the same model structure on all computing nodes, constraining the model scale by the weakest end devices. To address this issue, we propose Agglomerative Federated Learning (FedAgg), which is a novel EECC-empowered FL framework that allows the trained models from end, edge, to cloud to grow larger in size and stronger in generalization ability. FedAgg recursively organizes computing nodes among all tiers based on Bridge Sample Based Online Distillation Protocol (BSBODP), which enables every pair of parent-child computing nodes to mutually transfer and distill knowledge extracted from generated bridge samples. This design enhances the performance by exploiting the potential of larger models, with privacy constraints of FL and flexibility requirements of EECC both satisfied. Experiments under various settings demonstrate that FedAgg outperforms state-of-the-art methods by an average of 4.53% accuracy gains and remarkable improvements in convergence rate. Our code is available at https://github.com/wuzhiyuan2000/FedAgg.}
}


@inproceedings{DBLP:conf/infocom/QiaoZYYCZRY24,
	author = {Jing Qiao and
                  Zuyuan Zhang and
                  Sheng Yue and
                  Yuan Yuan and
                  Zhipeng Cai and
                  Xiao Zhang and
                  Ju Ren and
                  Dongxiao Yu},
	title = {BR-DeFedRL: Byzantine-Robust Decentralized Federated Reinforcement
                  Learning with Fast Convergence and Communication Efficiency},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {141--150},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621347},
	doi = {10.1109/INFOCOM52122.2024.10621347},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QiaoZYYCZRY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose Byzantine-Robust Decentralized Federated Reinforcement Learning (BR-DeFedRL), an innovative framework that effectively combats the harmful influence of Byzantine agents by adaptively adjusting communication weights, thereby significantly enhancing the robustness of the learning system. By leveraging decentralized learning, our approach eliminates the dependence on a central server. Striking a harmonious balance between communication round count and sample complexity, BR-DeFedRL achieves efficient convergence with a rate of \\mathcal{O}\\left( {\\frac{1}{{TN}}} \\right)\n, where T denotes the communication rounds and N represents the local steps related to variance reduction. Notably, each agent attains an ϵ-approximation with a state-of-the-art sample complexity of \\mathcal{O}\\left( {\\frac{1}{{\\varepsilon N}} + \\frac{1}{\\varepsilon }} \\right)\n. Extensive experimental validations further affirm the efficacy of BR-DeFedRL, making it a promising and practical solution for Byzantine-robust decentralized federated reinforcement learning.}
}


@inproceedings{DBLP:conf/infocom/0001CHPDC024,
	author = {Zhibo Wang and
                  Zhiwei Chang and
                  Jiahui Hu and
                  Xiaoyi Pang and
                  Jiacheng Du and
                  Yongle Chen and
                  Kui Ren},
	title = {Breaking Secure Aggregation: Label Leakage from Aggregated Gradients
                  in Federated Learning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {151--160},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621090},
	doi = {10.1109/INFOCOM52122.2024.10621090},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001CHPDC024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) exhibits privacy vulnerabilities under gradient inversion attacks (GIAs), which can extract private information from individual gradients. To enhance privacy, FL incorporates Secure Aggregation (SA) to prevent the server from obtaining individual gradients, thus effectively resisting GIAs. In this paper, we propose a stealthy label inference attack to bypass SA and recover individual clients’ private labels. Specifically, we conduct a theoretical analysis of label inference from the aggregated gradients that are exclusively obtained after implementing SA. The analysis results reveal that the inputs (embeddings) and outputs (logits) of the final fully connected layer (FCL) contribute to gradient disaggregation and label restoration. To preset the embeddings and logits of FCL, we craft a fishing model by solely modifying the parameters of a single batch normalization (BN) layer in the original model. Distributing client-specific fishing models, the server can derive the individual gradients regarding the bias of FCL by resolving a linear system with expected embeddings and the aggregated gradients as coefficients. Then the labels of each client can be precisely computed based on preset logits and gradients of FCL’s bias. Extensive experiments show that our attack achieves large-scale label recovery with 100% accuracy on various datasets and model architectures.}
}


@inproceedings{DBLP:conf/infocom/ZhangWL24,
	author = {Shuhan Zhang and
                  Shuai Wang and
                  Dan Li},
	title = {Robust or Risky: Measurement and Analysis of Domain Resolution Dependency},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {161--170},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621098},
	doi = {10.1109/INFOCOM52122.2024.10621098},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DNS relies on domain delegation for good scalability, where domains delegate their resolution service to authoritative nameservers. However, such delegations lead to complex inter-dependencies between DNS zones. While a complex dependency might improve the robustness of domain resolution, it could also introduce security risks unexpectedly. In this work, we perform a large-scale measurement on nearly 217M domains to analyze their resolution dependencies at both zone level and infrastructure level. According to our analysis, domains under country-code TLDs and new generic TLDs generally present more complicated dependency relationships. For robustness consideration, popular domains prefer to configure more complex dependencies. However, the centralization of nameserver hosting and the silent outsourcing of DNS providers could lead to severe false redundancy at infrastructure level. Worse, considerable domain configurations in the wild are "not robust but risky": a more complex dependency may also indicate more vulnerabilities, e.g., domains with a 2× higher dependency complexity have a 2.87× larger proportion suffering from the hijacking risk brought by lame delegation.}
}


@inproceedings{DBLP:conf/infocom/ChenS0YW00P0Z024,
	author = {Xiang Chen and
                  Xi Sun and
                  Wenbin Zhang and
                  Xin Yao and
                  Zizheng Wang and
                  Hongyan Liu and
                  Qun Huang and
                  Gaoning Pan and
                  Xuan Liu and
                  Haifeng Zhou and
                  Chunming Wu},
	title = {Accelerating Sketch-based End-Host Traffic Measurement with Automatic
                  {DPU} Offloading},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {171--180},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621293},
	doi = {10.1109/INFOCOM52122.2024.10621293},
	timestamp = {Mon, 16 Sep 2024 10:28:05 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenS0YW00P0Z024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sketch-based traffic measurement is a crucial building block for monitoring traffic statistics and ensuring the quality of services of end-host applications. However, existing approaches for building sketches in end-hosts exhibit poor packet processing performance or high CPU consumption. In this paper, we propose MPU, which automatically offloads sketch-based measurement to the emerging hardware, DPU. MPU consists of a sketch analyzer that profiles sketch resource consumption and an optimization framework that formulates the offloading problem and maximizes sketch performance on DPU. We implement MPU on the NVIDIA BlueField DPU. Our testbed results indicate that MPU achieves 85% lower per-packet processing latency and 47% higher traffic measurement accuracy when compared to existing approaches.}
}


@inproceedings{DBLP:conf/infocom/LiGS024,
	author = {Fuliang Li and
                  Kejun Guo and
                  Jiaxing Shen and
                  Xingwei Wang},
	title = {Effective Network-Wide Traffic Measurement: {A} Lightweight Distributed
                  Sketch Deployment},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {181--190},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621384},
	doi = {10.1109/INFOCOM52122.2024.10621384},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiGS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network measurement is critical for various network applications, but scaling measurement techniques to the network-wide level is challenging for existing sketch-based solutions. Centralized sketch deployment provides low resource usage but suffers from poor load balancing. In contrast, collaborative measurement achieves load balancing through flow distribution across switches but requires high resource usage. This paper presents a novel lightweight distributed deployment framework that overcomes the limitations above. First, our framework is lightweight such that it splits sketches into segments and allocates them across forwarding paths to minimize resource usage and achieve load balancing. This also enables per-packet load balancing by distributing computations across switches. Second, our framework is also optimized for load balancing by coordinating between flows and enabling finer-grained flow distribution. We evaluate the proposed framework on various network topologies and different sketch deployments. Results indicate our solution matches the load balancing of collaborative measurement while approaching the low resource usage of centralized deployment. Moreover, it achieves superior performance in per-packet load balancing, which is not considered in previous deployment policies. Our work provides efficient distributed sketch deployment to strike a balance between load balancing and resource usage enabling effective network-wide measurement.}
}


@inproceedings{DBLP:conf/infocom/ZhangCHGWW24,
	author = {Heng Zhang and
                  Zixuan Cui and
                  Shaoyuan Huang and
                  Deke Guo and
                  Xiaofei Wang and
                  Wenyu Wang},
	title = {{QM-RGNN:} An Efficient Online QoS Measurement Framework with Sparse
                  Matrix Imputation for Distributed Edge Clouds},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {191--200},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621145},
	doi = {10.1109/INFOCOM52122.2024.10621145},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangCHGWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Measurements for the quality of end-to-end network services (QoS) are crucial to ensure stability, reliability, and user experience for distributed edge clouds. Measuring all QoS data brings significant costs. Existing QoS measurement methods attempt to use sparse measured QoS data to estimate unmeasured QoS data. But they suffer from limited estimation accuracy when facing QoS data with high sparsity or significant volatility. Moreover, they also consume high sampling and training costs during continuously online measurements. Our preliminary analysis reveals that end-to-end QoS is strongly temporal-spatial related. It inspires us to leverage partially measured QoS data to impute temporal-spatial-related unmeasured QoS data for reducing measurement costs. To predict unmeasured QoS data precisely with low computational costs, we propose a novel QoS Measurement framework based on Residual Graph Neural Network (QM-RGNN), which inputs QoS data as a graph and outputs the prediction of unmeasured QoS data. It consists of three core components: 1) an encoder-decoder model QM-GNN with GCN as the encoder and MLP as the decoder is devised for efficient QoS prediction, and a residual module is introduced in QM-GNN to tackle highly sparse and volatile QoS data; 2) a dynamic adaptive sample ratio is proposed to reduce the sampling costs; 3) an online learning pattern is designed to reduce continuous training costs. Experiments on two real-world industrial edge cloud datasets demonstrate the superiority of QM-RGNN in QoS measurement. It obtains at least a 37.5% reduction of relative RMSE between ground-truth and predicted QoS data with up to 90% training cost reduction and 22.7% sampling cost reduction.}
}


@inproceedings{DBLP:conf/infocom/QiaoWLGYL24,
	author = {Litao Qiao and
                  Bang Wu and
                  Heng Li and
                  Cuiying Gao and
                  Wei Yuan and
                  Xiapu Luo},
	title = {Trace-agnostic and Adversarial Training-resilient Website Fingerprinting
                  Defense},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {211--220},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621205},
	doi = {10.1109/INFOCOM52122.2024.10621205},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QiaoWLGYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural network (DNN) based website fingerprinting (WF) attacks can achieve an attack success rate (ASR) of over 90%, seriously threatening the privacy of Tor users. At present, adversarial example (AE) based defenses have demonstrated great potential to defend against WF attacks. However, existing AE-based defenses require knowing a complete traffic trace for adversarial perturbation calculation, which is unrealistic in practice. Moreover, they may become ineffective once adversarial training (AT) is adopted by attackers. To mitigate these two problems, we propose a defense called ALERT. It generates adversarial perturbations without knowing traffic traces, and can effectively resist AT-aided WF attacks. The key idea of ALERT is to produce universal perturbations that vary from user to user. We conduct extensive experiments to evaluate ALERT. In the closed world, ALERT significantly surpasses four representative WF defenses, including the state-of-the-art (SOTA) defense AWA. Specifically, ALERT reduces the ASR of the SOTA DF attack to 12.68% and uses only 20.13% of communication bandwidth. In the open world, ALERT uses only 19.91% of bandwidth, reduces the True Positive Rate (TPR) of the DF attack to 37.46%, obviously outperforming the other defenses.}
}


@inproceedings{DBLP:conf/infocom/ZhaoWZM24,
	author = {Tianya Zhao and
                  Xuyu Wang and
                  Junqing Zhang and
                  Shiwen Mao},
	title = {Explanation-Guided Backdoor Attacks on Model-Agnostic {RF} Fingerprinting},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {221--230},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621289},
	doi = {10.1109/INFOCOM52122.2024.10621289},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoWZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite the proven capabilities of deep neural networks (DNNs) for radio frequency (RF) fingerprinting, their security vulnerabilities have been largely overlooked. Unlike the extensively studied image domain, few works have explored the threat of backdoor attacks on RF signals. In this paper, we analyze the susceptibility of DNN-based RF fingerprinting to backdoor attacks, focusing on a more practical scenario where attackers lack access to control model gradients and training processes. We propose leveraging explainable machine learning techniques and autoencoders to guide the selection of positions and values, enabling the creation of effective backdoor triggers in a model-agnostic manner. To comprehensively evaluate our backdoor attack, we employ four diverse datasets with two protocols (Wi-Fi and LoRa) across various DNN architectures. Given that RF signals are often transformed into the frequency or time-frequency domains, this study also assesses attack efficacy in the time-frequency domain. Furthermore, we experiment with potential defenses, demonstrating the difficulty of fully safeguarding against our attacks.}
}


@inproceedings{DBLP:conf/infocom/Bremler-BarrCLT24,
	author = {Anat Bremler{-}Barr and
                  Michael Czeizler and
                  Hanoch Levy and
                  Jhonatan Tavori},
	title = {Exploiting Miscoordination of Microservices in Tandem for Effective
                  DDoS Attacks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {231--240},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621335},
	doi = {10.1109/INFOCOM52122.2024.10621335},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Bremler-BarrCLT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today’s software development landscape has witnessed a shift towards microservices based architectures. Using this approach, large software systems are implemented by combining loosely-coupled services, each responsible for specific task and defined with separate scaling properties. Auto-scaling is a primary capability of cloud computing which allows systems to adapt to fluctuating traffic loads by dynamically increasing (scale-up) and decreasing (scale-down) the number of resources used.We observe that when microservices which utilize separate auto-scaling mechanisms operate in tandem to process traffic, they may perform ineffectively, especially under overload conditions, due to DDoS attacks. This can result in throttling (Denial of service - DoS) and over-provisioning of resources (Economic Denial of Sustainability - EDoS).This paper demonstrates how an attacker can exploit the tandem behavior of microservices with different auto-scaling mechanisms to create an attack we denote as the Tandem Attack. We demonstrate the attack on a typical Serverless architecture and analyze its economical and performance damages. One intriguing finding is that some attacks may make a cloud customer paying for service denied requests.We conclude that independent scaling of loosely coupled components might form an inherent difficulty and end-to-end controls might be needed.}
}


@inproceedings{DBLP:conf/infocom/ChenZJH024,
	author = {Zhuo Chen and
                  Liehuang Zhu and
                  Peng Jiang and
                  Jialing He and
                  Zijian Zhang},
	title = {A Generic Blockchain-based Steganography Framework with High Capacity
                  via Reversible {GAN}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {241--250},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621377},
	doi = {10.1109/INFOCOM52122.2024.10621377},
	timestamp = {Wed, 04 Sep 2024 21:09:32 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenZJH024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain-based steganography enables data hiding via encoding the covert data into a specific blockchain transaction field. However, previous works focus on the specific field-embedding methods while lacking a consideration on required field-generation embedding. In this paper, we propose GBSF, a generic framework for blockchain-based steganography. The sender generates the required fields, where the additional covert data is embedded to enhance the channel capacity. Based on GBSF, we design R-GAN that utilizes the generative adversarial network (GAN) with a reversible generator to generate the required fields and encode additional covert data into the input noise of the reversible generator. We then explore the performance flaw of R-GAN and introduce CCR-GAN as an improvement. CCR-GAN employs a counter-intuitive data preprocessing mechanism to reduce decoding errors in covert data. It incurs gradient explosion for model convergence and we design a custom activation function. We conduct experiments using the transaction amount of the Bitcoin mainnet as the required field. The results demonstrate that R-GAN and CCR-GAN allow to embed 11-bit (embedding rate of 17.2%) and 24-bit (embedding rate of 37.5%) covert data within a transaction amount, and enhance the channel capacity of state-of-the-art works by 4.30% to 91.67% and 9.38% to 200.00%, respectively.}
}


@inproceedings{DBLP:conf/infocom/ChenHYYY24,
	author = {Qinde Chen and
                  Huawei Huang and
                  Zhaokang Yin and
                  Guang Ye and
                  Qinglin Yang},
	title = {Broker2Earn: Towards Maximizing Broker Revenue and System Liquidity
                  for Sharded Blockchains},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {251--260},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621431},
	doi = {10.1109/INFOCOM52122.2024.10621431},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenHYYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-shard Transactions (CTXs) widely exist in sharded blockchains. CTXs have to endure large confirmation latency because they need to participate in consensus in both their source and destination shards. To diminish CTXs, plenty of state-of-the-art blockchain protocols have been proposed. For example, in BrokerChain [1], some intermediary broker accounts can help turn CTXs into intra-shard transactions through their voluntary liquidity services. Thereby, the original CTXs can be confirmed in blockchain shards quickly. However, we found that BrokerChain is impractical for a sharded blockchain because it does not consider how to recruit a sufficient number of broker accounts. Thus, blockchain clients do not have the motivation to provide token liquidity for others. To address this challenge, we design Broker2Earn, which is essentially a decentralized finance (DeFi) protocol that works as an incentive mechanism for blockchain users who choose to become brokers. Via participating in Broker2Earn, brokers can earn native revenues when they collateralize their tokens to the protocol. Furthermore, Broker2Earn can also benefit the sharded blockchain since it can efficiently spend each staked liquidity provided by brokers on diminishing CTXs. We formulate the core module of Broker2Earn into a revenue-maximization problem, which is proven NP-hard. To solve this problem, we design an online approximation algorithm using the relax-and-rounding technique. We also rigorously analyze the approximation ratio of our online algorithm. Finally, we conduct extensive experiments using real-world Ethereum transactions on both a transaction-driven simulator and an open-source blockchain testbed. The evaluation results show that the proposed Broker2Earn protocol demonstrates a near-optimal performance that outperforms other baselines, in terms of broker revenues and the usage of system liquidity.}
}


@inproceedings{DBLP:conf/infocom/0001ZG0Y0LW24,
	author = {Minghui Xu and
                  Jiahao Zhang and
                  Hechuan Guo and
                  Xiuzhen Cheng and
                  Dongxiao Yu and
                  Qin Hu and
                  Yijun Li and
                  Yipu Wu},
	title = {FileDES: {A} Secure, Scalable and Succinct Decentralized Encrypted
                  Storage Network},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {261--270},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621113},
	doi = {10.1109/INFOCOM52122.2024.10621113},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001ZG0Y0LW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decentralized Storage Network (DSN) is an emerging technology that challenges traditional cloud-based storage systems by consolidating storage capacities from independent providers and coordinating to provide decentralized storage and retrieval services. However, current DSNs face several challenges associated with data privacy and efficiency of the proof systems. To address these issues, we propose FileDES ( Decentralized Encrypted Storage), which incorporates three essential elements: privacy preservation, scalable storage proof, and batch verification. FileDES provides encrypted data storage while maintaining data availability, with a scalable Proof of Encrypted Storage (PoES) algorithm that is resilient to Sybil and Generation attacks. Additionally, we introduce a rollup-based batch verification approach to simultaneously verify multiple files using publicly verifiable succinct proofs. We conducted a comparative evaluation on FileDES, Filecoin, Storj and Sia under various conditions, including a WAN composed of up to 120 geographically dispersed nodes. Our protocol outperforms the others in terms of proof generation/verification efficiency, storage costs, and scalability.}
}


@inproceedings{DBLP:conf/infocom/HuangLZ24,
	author = {Huawei Huang and
                  Yue Lin and
                  Zibin Zheng},
	title = {Account Migration across Blockchain Shards using Fine-tuned Lock Mechanism},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {271--280},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621244},
	doi = {10.1109/INFOCOM52122.2024.10621244},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuangLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sharding is one of the most promising techniques for improving blockchain scalability. In blockchain state sharding, account migration across shards is crucial to the low ratio of cross-shard transactions and cross-shard workload balance. Through reviewing state-of-the-art protocols proposed to reconfigure blockchain shards via account shuffling, we find that account migration plays a significant role. From the literature, we only find a related work that utilizes the lock mechanism to realize account migration. We call this method the SOTA Lock, in which both the target account’s state and its associated transactions need to be locked when migrating this account between shards. Thereby, SOTA Lock causes a high makespan to the associated transactions. To address these challenges of account migration, we propose a dedicated Fine-tuned Lock protocol. Unlike SOTA Lock, Fine-tuned Lock enables real-time processing of the affected transactions during account migration. Thus, the makespan of associated transactions can be lowered. We implement Fine-tuned Lock protocol using an open-sourced blockchain testbed (i.e., BlockEmulator) and deploy it in Tencent cloud. The experimental results show that the proposed Fine-tuned Lock outperforms the SOTA Lock in terms of transaction makespan. For example, the transaction makespan of Fine-tuned Lock achieves around 30% the makespan of SOTA Lock.}
}


@inproceedings{DBLP:conf/infocom/TruongTTDC24,
	author = {Thanh Phung Truong and
                  Anh{-}Tien Tran and
                  Van{-}Dat Tuong and
                  Nhu{-}Ngoc Dao and
                  Sungrae Cho},
	title = {NOMA-Enhanced Quantized Uplink Multi-user {MIMO} Communications},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {281--290},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621204},
	doi = {10.1109/INFOCOM52122.2024.10621204},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TruongTTDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This research examines quantized uplink multi-user MIMO communication systems with low-resolution quantizers at users and base stations (BS). In such a system, we employ the non-orthogonal multiple access (NOMA) technique for communication between users and the BS to enhance communication performance. To maximize the number of users that satisfy the quality of service (QoS) requirement while minimizing the user’s transmit power, we jointly optimize the transmit power and precoding matrices at the users and the digital beamforming matrix at the BS. Owing to the non-convexity of the objective function, we transform the problem into a reinforcement learning-based problem and propose a deep reinforcement learning (DRL) framework named QNOMA-DRLPA to overcome the challenge. Because the nature of the action decided by the DRL algorithm may not satisfy the problem constraints, we propose a postactor process to redesign the actions to meet all the problem constraints. In the simulation, we assess the proposed framework’s performance in training convergence and demonstrate its superior performance under various environmental parameters compared with other benchmark schemes.}
}


@inproceedings{DBLP:conf/infocom/SongWLL024,
	author = {Qingyu Song and
                  Juncheng Wang and
                  Jingzong Li and
                  Guochen Liu and
                  Hong Xu},
	title = {A Learning-only Method for Multi-Cell Multi-User {MIMO} Sum Rate Maximization},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {291--300},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621282},
	doi = {10.1109/INFOCOM52122.2024.10621282},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SongWLL024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Solving the sum rate maximization problem for interference reduction in multi-cell multi-user multiple-input multiple-output (MIMO) wireless communication systems has been investigated for a decade. Several machine learning-assisted methods have been proposed under conventional sum rate maximization frameworks, such as the Weighted Minimum Mean Square Error (WMMSE) framework. However, existing learning-assisted methods suffer from a deficiency in parallelization, and their performance is intrinsically bounded by WMMSE. In contrast, we propose a structural learning-only framework from the abstraction of WMMSE. Our proposed framework increases the solvability of the original MIMO sum rate maximization problem by dimension expansion via a unitary learnable parameter matrix to create an equivalent problem in a higher dimension. We then propose a structural solution updating method to solve the higher dimensional problem, utilizing neural networks to generate the learnable matrix-multiplication parameters. We show that the proposed structural learning framework achieves lower complexity than WMMSE thanks to its parallel implementation. Simulation results under practical communication network settings demonstrate that our proposed learning-only framework achieves up to 98% optimality over state-of-the-art algorithms while providing up to 47× acceleration in various scenarios.}
}


@inproceedings{DBLP:conf/infocom/GhoshHZ24,
	author = {Debamita Ghosh and
                  Manjesh K. Hanawal and
                  Nikola Zlatanov},
	title = {HoloBeam: Learning Optimal Beamforming in Far-Field Holographic Metasurface
                  Transceivers},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {301--310},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621175},
	doi = {10.1109/INFOCOM52122.2024.10621175},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GhoshHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Holographic Metasurface Transceivers (HMTs) are emerging as cost-effective substitutes to large antenna arrays for beamforming in Millimeter and TeraHertz wave communication. However, to achieve desired channel gains through beamforming in HMT, phase-shifts of a large number of elements need to be appropriately set, which is challenging. Also, these optimal phase-shifts depend on the location of the receivers, which could be unknown. In this work, we develop a learning algorithm using a fixed-budget multi-armed bandit framework to beamform and maximize received signal strength at the receiver for far-field regions. Our algorithm, named Holographic Beam (HoloBeam) exploits the parametric form of channel gains of the beams, which can be expressed in terms of two phase-shifting parameters. Even after parameterization, the problem is still challenging as phase-shifting parameters take continuous values. To overcome this, HoloBeam works with the discrete values of phase-shifting parameters and exploits their unimodal relations with channel gains to learn the optimal values faster. We upper bound the probability of HoloBeam incorrectly identifying the (discrete) optimal phase-shift parameters in terms of the number of pilots used in learning. We show that this probability decays exponentially with the number of pilot signals. We demonstrate that HoloBeam outperforms state-of-the-art algorithms through extensive simulations.}
}


@inproceedings{DBLP:conf/infocom/ChenL0024,
	author = {Wei{-}Han Chen and
                  Xin Liu and
                  Kannan Srinivasan and
                  Srinivasan Parthasarathy},
	title = {{FTP:} Enabling Fast Beam-Training for Optimal mmWave Beamforming},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {311--320},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621112},
	doi = {10.1109/INFOCOM52122.2024.10621112},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenL0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To maximize Signal-to-Noise Ratio (SNR), it is necessary to move beyond selecting beams from a codebook. While the state-of-the-art approaches can significantly improve SNR compared to codebook-based beam selection by exploiting the globally-optimal beam, they incur significant beam-training overhead, which limits the applicability to large-scale antenna arrays and the scalability for multiple users. In this paper, we propose FTP, a highly-scalable beam-training solution that can find the globally-optimal beam with minimal beam-training overhead. FTP works by estimating per-path direction along with its complex gain and synthesizes the globally-optimal beam from these parameters. Our design significantly reduces the search space for finding such path parameters, which enables FTP to scale to large-scale antenna arrays. We implemented and evaluated FTP on a mmWave experimental platform with 32 antenna elements. Our results demonstrate that FTP achieves optimal SNR performance comparable with the state-of-the-art while reducing the beam-training overhead by 3 orders of magnitude. Under simulated settings, we demonstrate that the gain of FTP can be even more significant for larger antenna arrays with up to 1024 elements.}
}


@inproceedings{DBLP:conf/infocom/Chen0HXL24,
	author = {Siyu Chen and
                  Hongbo Jiang and
                  Jingyang Hu and
                  Zhu Xiao and
                  Daibo Liu},
	title = {Silent Thief: Password Eavesdropping Leveraging Wi-Fi Beamforming
                  Feedback from {POS} Terminal},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {321--330},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621321},
	doi = {10.1109/INFOCOM52122.2024.10621321},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Chen0HXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, point-of-sale (POS) terminals are no longer limited to wired connections, and many of them rely on Wi-Fi for data transmission. While Wi-Fi provides the convenience of wireless connectivity, it also introduces significant security risks. Previous research has explored Wi-Fi-based eavesdropping methods. However, these methods often rely on limited environmental robustness of Channel State Information (CSI) and require invasive Wi-Fi hardware, making them impractical in real-world scenarios. In this work, we present SThief, a practical Wi-Fi-based eavesdropping attack that leverages beamforming feedback information (BFI) exchanged between POS terminal and access points (APs) to keystroke inference on POS keypads. By capitalizing on the clear-text transmission characteristics of BFI, this attack demonstrates a more flexible and practical nature, surpassing traditional CSI-based methods. BFI is transmitted in the uplink, carrying downlink channel information that allows the AP to adjust beamforming angles. We exploit this channel information to keystroke inference. To enhance the BFI series, we use maximal ratio combining (MRC), ensuring efficiency across various scenarios. Additionally, we employ the Connectionist Temporal Classification method for keystroke inference, providing exceptional generalization and scalability. Extensive testing validates SThief’s effectiveness, achieving an impressive 81% accuracy rate in inferring 6-digit POS passwords within the top-100 attempts.}
}


@inproceedings{DBLP:conf/infocom/0002000WN24,
	author = {Jiahui Li and
                  Geng Sun and
                  Qingqing Wu and
                  Shuang Liang and
                  Pengfei Wang and
                  Dusit Niyato},
	title = {Two-Way Aerial Secure Communications via Distributed Collaborative
                  Beamforming under Eavesdropper Collusion},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {331--340},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621264},
	doi = {10.1109/INFOCOM52122.2024.10621264},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0002000WN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs)-enabled aerial communication provides a flexible, reliable, and cost-effective solution for a range of wireless applications. However, due to the high line-of-sight (LoS) probability, aerial communications between UAVs are vulnerable to eavesdropping attacks, particularly when multiple eavesdroppers collude. In this work, we aim to introduce distributed collaborative beamforming (DCB) into UAV swarms and handle the eavesdropper collusion by controlling the corresponding signal distributions. Specifically, we consider a two-way DCB-enabled aerial communication between two UAV swarms and construct these swarms as two UAV virtual antenna arrays. Then, we minimize the two-way known secrecy capacity and the maximum sidelobe level to avoid information leakage from the known and unknown eavesdroppers, respectively. Simultaneously, we also minimize the energy consumption of UAVs for constructing virtual antenna arrays. Due to the conflicting relationships between secure performance and energy efficiency, we consider these objectives as a multi-objective optimization problem. Following this, we propose an enhanced multi-objective swarm intelligence algorithm via the characterized properties of the problem. Simulation results show that our proposed algorithm can obtain a set of informative solutions and outperform other state-of-the-art baseline algorithms. Experimental tests demonstrate that our method can be deployed in limited computing power platforms of UAVs and is beneficial for saving computational resources.}
}


@inproceedings{DBLP:conf/infocom/ZhangXFY024,
	author = {Guoming Zhang and
                  Zhijie Xiang and
                  Heqiang Fu and
                  Yanni Yang and
                  Pengfei Hu},
	title = {EchoLight: Sound Eavesdropping based on Ambient Light Reflection},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {341--350},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621338},
	doi = {10.1109/INFOCOM52122.2024.10621338},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangXFY024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sound eavesdropping using light has been an area of considerable interest and concern, as it can be achieved over long distances. However, previous work has often lacked stealth (e.g., active emission of laser beams) or been limited in the range of realistic applications (e.g., using direct light from a device’s indicator LED or a hanging light bulb). In this paper, we present EchoLight, a non-intrusive, passive and long-range sound eavesdropping method that utilizes the extensive reflection of ambient light from vibrating objects to reconstruct sound. We analyze the relationship between reflection light signals and sound signals, particularly in situations where the frequency response of reflective objects and the efficiency of diffuse reflection are suboptimal. Based on this analysis, we have introduced an algorithm based on cGAN to address the issues of nonlinear distortion and spectral absence in the frequency domain of sound. We extensively evaluate EchoLight’s performance in a variety of real-world scenarios. It demonstrates the ability to accurately reconstruct audio from a variety of source distances, attack distances, sound levels, light intensity, light sources, and reflective materials. Our results reveal that the reconstructed audio exhibits a high degree of similarity to the original audio over 40 meters of attack distance.}
}


@inproceedings{DBLP:conf/infocom/XuCLLLF24,
	author = {Xiangyu Xu and
                  Yu Chen and
                  Zhen Ling and
                  Li Lu and
                  Junzhou Luo and
                  Xinwen Fu},
	title = {mmEar: Push the Limit of {COTS} mmWave Eavesdropping on Headphones},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {351--360},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621229},
	doi = {10.1109/INFOCOM52122.2024.10621229},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XuCLLLF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed a surge of headphones (including in-ear headphones) usage in works and communications. Because of the privacy-preserve property, people feel comfortable having confidential communication wearing headphones and pay little attention to speech leakage. In this paper, we present an end-to-end eavesdropping system, mmEar, which shows the feasibility of launching an eavesdropping attack on headphones leveraging a commercial mmWave radar. Different from previous works that realize eavesdropping by sensing speech-induced vibrations with reasonable amplitude, mmEar focuses on capturing the extremely faint vibrations with a low signal-to-noise ratio (SNR) on the surface of headphones. Toward this end, we propose a faint vibration emphasis (FVE) method that models and amplifies the mmWave responses to speech-induced vibrations on the In-phase and Quadrature (IQ) plane, followed by a deep denoising network to further improve the SNR. To achieve practical eavesdropping on various headphones and setups, we propose a cGAN model with a pretrain-finetune scheme, boosting the generalization ability and robustness of the attack by generating high-quality synthesis data. We evaluate mmEar with extensive experiments on different headphones and earphones and find that most of them can be compromised by the proposed attack for speech recovery.}
}


@inproceedings{DBLP:conf/infocom/WuL24,
	author = {Xiaoyi Wu and
                  Bin Li},
	title = {Achieving Regular and Fair Learning in Combinatorial Multi-Armed Bandit},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {361--370},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621191},
	doi = {10.1109/INFOCOM52122.2024.10621191},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WuL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Combinatorial multi-armed bandit refers to the model that aims to maximize cumulative rewards in the presence of uncertainty. Motivated by two important wireless network applications, in addition to maximizing cumulative rewards, it is important to ensure fairness among arms (i.e., the minimum average reward required by each arm) and reward regularity (i.e., how often each arm receives the reward). In this paper, we develop a parameterized regular and fair learning algorithm to achieve these three objectives. In particular, the proposed algorithm linearly combines virtual queue-lengths (tracking the fairness violations), Time-Since-Last-Reward (TSLR) metrics, and Upper Confidence Bound (UCB) estimates in its weight measure. Here, TSLR is similar to age-of-information and measures the elapsed number of rounds since the last time an arm received a reward, capturing the reward regularity performance, and UCB estimates are utilized to balance the tradeoff between exploration and exploitation in online learning. Through capturing a key relationship between virtual queue-lengths and TSLR metrics and utilizing several non-trivial Lyapunov functions, we analytically characterize zero cumulative fairness violation, reward regularity, and cumulative regret performance under our proposed algorithm. These findings are corroborated by our extensive simulations.}
}


@inproceedings{DBLP:conf/infocom/HuangL024,
	author = {Yin Huang and
                  Qingsong Liu and
                  Jie Xu},
	title = {Adversarial Combinatorial Bandits with Switching Cost and Arm Selection
                  Constraints},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {371--380},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621364},
	doi = {10.1109/INFOCOM52122.2024.10621364},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuangL024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The multi-armed bandits (MAB) framework is widely used for sequential decision-making under uncertainty, finding applications in various domains, including computer and communication networks. To address the increasing complexity of real-world systems and their operational requirements, researchers have proposed and studied various extensions to the basic MAB framework. In this paper, we focus on an adversarial MAB problem inspired by real-world systems with combinatorial semi-bandit arms, switching costs, and anytime cumulative arm selection constraints. To tackle this challenging problem, we introduce the Block-structured Follow-the-Regularized-Leader (B-FTRL) algorithm. Our approach employs a hybrid Tsallis-Shannon entropy regularizer in arm selection and incorporates a block structure that divides time into blocks to minimize arm switching costs. The theoretical analysis shows that B-FTRL achieves a reward regret bound of O\\left( {{T^{\\frac{{2a - b + 1}}{{1 + a}}}} + {T^{\\frac{b}{{1 + a}}}}} \\right)\nand a switching regret bound of O\\left( {{T^{\\frac{1}{{1 + a}}}}} \\right)\n, where a and b are tunable algorithm parameters. By carefully selecting the values of a and b, we are able to limit the total regret to O(T 2/3 ) while satisfying the arm selection constraints in expectation. This outperforms the state-of-the-art regret bound of O(T 3/4 ) and expected constraint violation bound o(1), which are derived in less challenging stochastic reward environments. Additionally, we provide a high-probability constraint violation bound of O(\\sqrt T )\n. To validate the effectiveness of the proposed BFTRL algorithm, numerical results are presented to demonstrate its superiority in comparison to other existing methods.}
}


@inproceedings{DBLP:conf/infocom/Steiger0024,
	author = {Juaren Steiger and
                  Bin Li and
                  Ning Lu},
	title = {Backlogged Bandits: Cost-Effective Learning for Utility Maximization
                  in Queueing Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {381--390},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621295},
	doi = {10.1109/INFOCOM52122.2024.10621295},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Steiger0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bipartite queueing networks with unknown statistics, where jobs are routed to and queued at servers and yield server-dependent utilities upon completion, model a wide range of problems in communications and related research areas (e.g., call routing in call centers, task assignment in crowdsourcing, job dispatching to cloud servers). The utility maximization problem in bipartite queueing networks with unknown statistics is a bandit learning problem where the delayed semi-bandit feedback depends on the server queueing delay. In this paper, we propose an efficient algorithm that overcomes the technical shortcomings of the state-of-the-art and achieves square root regret, queue length, and feedback delay. Our approach also accommodates additional constraints, such as quality of service, fairness, and budgeted cost constraints, with constant expected peak violation and zero expected violation after a fixed timeslot. Empirically, our algorithm’s regret is competitive with the state-of-the-art for some problem instances and outperforms it in others, with much lower delay and constraint violation.}
}


@inproceedings{DBLP:conf/infocom/KimZZJ24,
	author = {Taejin Kim and
                  Jinhang Zuo and
                  Xiaoxi Zhang and
                  Carlee Joe{-}Wong},
	title = {Edge-MSL: Split Learning on the Mobile Edge via Multi-Armed Bandits},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {391--400},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621231},
	doi = {10.1109/INFOCOM52122.2024.10621231},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/KimZZJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of 5G technology and edge computing enables the collaborative use of data by mobile users for scalable training of machine learning models. Privacy concerns and communication constraints, however, can prohibit users from offloading their data to a single server for training. Split learning, in which models are split between end users and a central server, somewhat resolves these concerns but requires exchanging information between users and the server in each local training iteration. Thus, splitting models between end users and geographically close edge servers can significantly reduce communication latency and training time. In this setting, users must decide to which edge servers they should offload part of their model to minimize the training latency, a decision that is further complicated by the presence of multiple, mobile users competing for resources. We present Edge-MSL, a novel formulation of the mobile split learning problem as a contextual multi-armed bandits framework. To counter scalability challenges with a centralized Edge-MSL solution, we introduce a distributed solution that minimizes competition between users for edge resources, reducing regret by at least two times compared to a greedy baseline. The distributed Edge-MSL approach improves trained model convergence with a 15% increase in test accuracy.}
}


@inproceedings{DBLP:conf/infocom/ZhouZYY24,
	author = {Mengqiu Zhou and
                  Meng Zhang and
                  Howard H. Yang and
                  Roy D. Yates},
	title = {Age-minimal {CPU} Scheduling},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {401--410},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621420},
	doi = {10.1109/INFOCOM52122.2024.10621420},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouZYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of real-time status updating applications and ubiquitous mobile devices have motivated the analysis and optimization of data freshness in the context of age of information. At the same time, increasing requirements on computer performance have inspired research on CPU scheduling, with a focus on reducing energy consumption. However, since prior CPU scheduling strategies have ignored data freshness, we formulate the first CPU scheduling problem that aims to minimize the long-term average age of information, subject to an average power constraint. In particular, we optimize CPU scheduling strategies that specify when the CPU sleeps and adapt the CPU speed (clock frequency) during the execution of update-processing tasks. We formulate the age-minimal CPU scheduling problem as a constrained semi-Markov decision process (SMDP) problem with uncountable space. We develop a value-iteration-based algorithm and further prove its convergence in infinite space to obtain the optimal policy. Compared with existing benchmarks in terms of long-term average AoI, numerical results show that our proposed scheme can reduce the AoI by up to 53%, and obtains greater benefits when faced with a tighter power constraint. In addition, for a given AoI target, the age-minimal CPU scheduling policy can save more than 50% on energy consumption.}
}


@inproceedings{DBLP:conf/infocom/ZhaoQSWNL24,
	author = {Yunfeng Zhao and
                  Chao Qiu and
                  Xiaoyun Shi and
                  Xiaofei Wang and
                  Dusit Niyato and
                  Victor C. M. Leung},
	title = {Cur-CoEdge: Curiosity-Driven Collaborative Request Scheduling in Edge-Cloud
                  Systems},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {411--420},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621190},
	doi = {10.1109/INFOCOM52122.2024.10621190},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoQSWNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The collaboration between clouds and edges unlocks the full potential of edge-cloud systems. Edge-cloud platform has brought about significant decentralization, heterogeneity, complexity, and instability. These characteristics have posed unprecedented challenges to the optimal scheduling problem in the edge-cloud system, including inaccurate decision-making and slow convergence. In this paper, we propose a curiosity-driven collaborative request scheduling scheme in edge-cloud systems, namely Cur-CoEdge. To tackle the challenge of inaccurate decision-making, we introduce a time-scale and decision-level interaction mechanism. This mechanism employs a small-large-time-scale scheduling learning framework, facilitating mutual learning between different decision levels. To address the challenge of slow convergence, we investigate the underlying reasons, such as the sparse reward-setting in reinforcement learning. In response, we develop a curiosity-driven collaborative exploration approach that fosters intrinsic curiosity in the cloud and simultaneously motivates dispatchers to explore the environment both individually and collectively. The effectiveness of this collaborative exploration is also supported by theoretical proof of convergence. Finally, we implement a prototype system on a network hardware system along with two real-world traces. Evaluations demonstrate significant improvements, with up to a 26% increase in time efficiency, a 40% rise in system throughput, and a 71% enhancement in convergence speed.}
}


@inproceedings{DBLP:conf/infocom/ZhugeCHW0X024,
	author = {Xiangwen Zhuge and
                  Xinjun Cai and
                  Xiaowu He and
                  Zeyu Wang and
                  Fan Dang and
                  Wang Xu and
                  Zheng Yang},
	title = {InNetScheduler: In-network scheduling for time- and event-triggered
                  critical traffic in {TSN}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {421--430},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621265},
	doi = {10.1109/INFOCOM52122.2024.10621265},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhugeCHW0X024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-Sensitive Networking (TSN) is an enabling technology for Industry 4.0. Traffic scheduling plays a key role for TSN to ensure low-latency and deterministic transmission of critical traffic. As industrial network scales, TSN networks are expected to support a rising number of both time-triggered and event-triggered critical traffic (TCT and ECT). In this work, we present InNetScheduler, the first in-network TSN scheduling paradigm that boosts the throughput, i.e., number of scheduled data flows, of both traffic types. Different from existing approaches that conduct entire scheduling on the server, InNetScheduler leverages the computation resources on switches to promptly schedule latency-critical ECT, and delegate the computational-intensive TCT scheduling to server. The key innovation of InNetScheduler includes a Load-Aware Optimizer to mitigate ECT conflicts, a Relaxated ECT Scheduler to accelerate in-network computation, and End-to-End Determinism Guarantee to lower scheduling jitter. We fully implement a suite of InNetScheduler-compatible TSN switches with hardwaresoftware co-design. Extensive experiments are conducted on both simulation and physical testbeds, and the results demonstrate InNetScheduler’s superior performance. By unleashing the power of in-network computation, InNetScheduler points out a direction to extend the capacity of existing industrial networks.}
}


@inproceedings{DBLP:conf/infocom/LiuXF24,
	author = {Qingsong Liu and
                  Weihang Xu and
                  Zhixuan Fang},
	title = {Learning-based Scheduling for Information Gathering with QoS Constraints},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {431--440},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621161},
	doi = {10.1109/INFOCOM52122.2024.10621161},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuXF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of scheduling packets from multiple sources over unreliable channels has attracted much attention due to its great practicability in the Internet of things systems. Most previous work focuses on the throughput/energy consumption/operational cost optimization or the setting that the channel information is known a priori. In this paper, we consider a more generic setting to this problem where packets from different sources have different values, and each heterogeneous source has a distinct Quality of Service (QoS) requirement. The information about packet value and channel reliability is unknown in advance, and the controller schedules sources over time to maximize its collected packet values while providing a QoS guarantee for each source. For the stationary case where packet values are independent and identically distributed (i.i.d.), we propose an efficient learning policy based on linear-programming (LP) methodology. Our proof shows that it meets the QoS constraint of each source and only incurs a logarithmic regret. In the special case that the channel reliability is known a priori, our algorithm can further guarantee a bounded regret. Furthermore, in the case of non-stationary packet values, we apply the sliding window technique to our LP-based algorithm and prove that it still guarantees a sublinear regret while meeting each source’s QoS requirement. Finally, we provide numerical simulations to support our theoretical results.}
}


@inproceedings{DBLP:conf/infocom/LingXLWXL24,
	author = {Zhen Ling and
                  Gui Xiao and
                  Lan Luo and
                  Rong Wang and
                  Xiangyu Xu and
                  Guangchi Liu},
	title = {WFGuard: an Effective Fuzzing-testing-based Traffic Morphing Defense
                  against Website Fingerprinting},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {441--450},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621121},
	doi = {10.1109/INFOCOM52122.2024.10621121},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LingXLWXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Website fingerprinting (WF) attack is a type of traffic analysis attack. It enables a local and passive eavesdropper situated between the Tor client and the Tor entry node to deduce which websites the client is visiting. Currently, deep learning (DL) based WF attacks have overcome a number of proposed WF defenses, demonstrating superior performance compared to traditional machine learning (ML) based WF attacks. To mitigate this threat, we present WFGuard, a fuzzing-testing-based traffic morphing WF defense technique. WFGuard employs fine-grained neuron information within WF classifiers to design a joint optimization function and then applies gradient ascent to maximize both neurons value and misclassification possibility in DL-based WF classifiers. During each traffic mutation cycle, we propose a gradient based dummy traffic injection pattern generation approach, continuously mutating the traffic until a pattern emerges that can successfully deceive the classifier. Finally, the pattern present in successful variant traces are extracted and applied as defense strategies to Tor traffic. Extensive evaluations reveal that WFGuard can effectively decrease the accuracy of DL-based WF classifiers (e.g., DF and Var-CNN) to a mere 4.43%, while only incurring an 11.04% bandwidth overhead. This highlights the potential efficacy of our approach in mitigating WF attacks.}
}


@inproceedings{DBLP:conf/infocom/NgoGN24,
	author = {Huy Quang Ngo and
                  Mingyu Guo and
                  Hung X. Nguyen},
	title = {Catch Me if You Can: Effective Honeypot Placement in Dynamic {AD}
                  Attack Graphs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {451--460},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621210},
	doi = {10.1109/INFOCOM52122.2024.10621210},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/NgoGN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study a Stackelberg game between an attacker and a defender on large Active Directory (AD) attack graphs where the defender employs a set of honeypots to stop the attacker from reaching high-value targets. Contrary to existing works that focus on small and static attack graphs, AD graphs typically contain hundreds of thousands of nodes and edges and constantly change over time. We consider two types of attackers: a simple attacker who cannot observe honeypots and a competent attacker who can. To jointly solve the game, we propose a mixed-integer programming (MIP) formulation. We observed that the optimal blocking plan for static graphs performs poorly in dynamic graphs. To solve the dynamic graph problem, we re-design the mixed-integer programming formulation by combining m MIP (dyMIP(m)) instances to produce a near-optimal blocking plan. Furthermore, to handle a large number of dynamic graph instances, we use a clustering algorithm to efficiently find the m-most representative graph instances for a constant m (dyMIP(m)). We prove a lower bound on the optimal blocking strategy for dynamic graphs and show that our dyMIP(m) algorithms produce close to optimal results for a range of AD graphs under realistic conditions.}
}


@inproceedings{DBLP:conf/infocom/FinkenzellerBRH24,
	author = {Andreas Finkenzeller and
                  Oliver Butowski and
                  Emanuel Regnath and
                  Mohammad Hamad and
                  Sebastian Steinhorst},
	title = {PTPsec: Securing the Precision Time Protocol Against Time Delay Attacks
                  Using Cyclic Path Asymmetry Analysis},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {461--470},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621345},
	doi = {10.1109/INFOCOM52122.2024.10621345},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FinkenzellerBRH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-precision time synchronization is a vital prerequisite for many modern applications and technologies, including Smart Grids, Time-Sensitive Networking (TSN), and 5G networks. Although the Precision Time Protocol (PTP) can accomplish this requirement in trusted environments, it becomes unreliable in the presence of specific cyber attacks. Mainly, time delay attacks pose the highest threat to the protocol, enabling attackers to diverge targeted clocks undetected. With the increasing danger of cyber attacks, especially against critical infrastructure, there is a great demand for effective countermeasures to secure both time synchronization and the applications that depend on it. However, current solutions are not sufficiently capable of mitigating sophisticated delay attacks. For example, they lack proper integration into the PTP protocol, scalability, or sound evaluation with the required microsecond-level accuracy. This work proposes an approach to detect and counteract delay attacks against PTP based on cyclic path asymmetry measurements over redundant paths. For that, we provide a method to find redundant paths in arbitrary networks and show how this redundancy can be exploited to reveal and mitigate undesirable asymmetries on the synchronization path that cause the malicious clock divergence. Furthermore, we propose PTPsec, a secure PTP protocol and its implementation based on the latest IEEE 1588-2019 standard. With PTPsec, we advance the conventional PTP to support reliable delay attack detection and mitigation. We validate our approach on a hardware testbed, which includes an attacker capable of performing static and incremental delay attacks at a microsecond precision. Our experimental results show that all attack scenarios can be reliably detected and mitigated with minimal detection time.}
}


@inproceedings{DBLP:conf/infocom/Ding24,
	author = {Damu Ding},
	title = {{CARBINE:} Exploring Additional Properties of HyperLogLog for Secure
                  and Robust Flow Cardinality Estimation},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {471--480},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621185},
	doi = {10.1109/INFOCOM52122.2024.10621185},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Ding24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Counting distinct elements (also named flow cardinality) of large data streams in the network is of primary importance since it can be used for many practical monitoring applications, including DDoS attack and malware spread detection. However, modern intrusion detection systems are struggling to reduce both memory and computational overhead for such measurements. Many algorithms are designed to estimate flow cardinality, in which HyperLogLog has been proven the most efficient due to its high accuracy and low memory usage. While HyperLogLog provides good performance on flow cardinality estimation, it has inherent algorithmic vulnerabilities that lead to both security and robustness issues. To overcome these issues, we first investigate two possible threats in HyperLogLog, and propose corresponding detection and protection solutions. Lever-aging proposed solutions, we introduce CARBINE, an approach that aims at identifying and eliminating the threats that most probably mislead the output of HyperLogLog. We implement our CARBINE to evaluate the threat detection performance, especially in case of a practical network scenario under volumetric DDoS attack. The results show that our CARBINE can effectively detect different kinds of threats while performing even higher accuracy and update speed than original HyperLogLog.}
}


@inproceedings{DBLP:conf/infocom/WangQWLWZG024,
	author = {Liang Wang and
                  Xiaoyang Qu and
                  Jianzong Wang and
                  Guokuan Li and
                  Jiguang Wan and
                  Nan Zhang and
                  Song Guo and
                  Jing Xiao},
	title = {Gecko: Resource-Efficient and Accurate Queries in Real-Time Video
                  Streams at the Edge},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {481--490},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621399},
	doi = {10.1109/INFOCOM52122.2024.10621399},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangQWLWZG024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Surveillance cameras are ubiquitous nowadays and users’ increasing needs for accessing real-world information (e.g., finding abandoned luggage) have urged object queries in real-time videos. While recent real-time video query processing systems exhibit excellent performance, they lack utility in deployment in practice as they overlook some crucial aspects, including multi-camera exploration, resource contention, and content awareness. Motivated by these issues, we propose a framework Gecko, to provide resource-efficient and accurate real-time object queries of massive videos on edge devices. Gecko (i) obtains optimal models from the model zoo and assigns them to edge devices for executing current queries, (ii) optimizes resource usage of the edge cluster at runtime by dynamically adjusting the frame query interval of each video stream and forking/joining running models on edge devices, and (iii) improves accuracy in changing video scenes by fine-grained stream transfer and continuous learning of models. Our evaluation with real-world video streams and queries shows that Gecko achieves up to 2x more resource efficiency gains and increases overall query accuracy by at least 12% compared with prior work, further delivering excellent scalability for practical deployment.}
}


@inproceedings{DBLP:conf/infocom/ZhangXZD0XC024,
	author = {Xiaoxi Zhang and
                  Haoran Xu and
                  Longhao Zou and
                  Jingpu Duan and
                  Chuan Wu and
                  Yali Xue and
                  Zuozhou Chen and
                  Xu Chen},
	title = {Rosevin: Employing Resource- and Rate-Adaptive Edge Super-Resolution
                  for Video Streaming},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {491--500},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621104},
	doi = {10.1109/INFOCOM52122.2024.10621104},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangXZD0XC024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today’s video streaming service providers have exploited cloud-edge collaborative networks for geo-distributed video delivery. The existing content delivery network (CDN) scheduling and adaptive bitrate algorithms may not fully utilize edge resources or lack a global control to optimize resource sharing. The emerging super-resolution (SR) approach can unleash the potential of leveraging computation resources to compensate for bandwidth consumption, by producing high-quality videos from low-resolution contents. Yet the uncertain SR resource sensitivity and its interplay with bitrate adaptation are underexplored. In this work, we propose Rosevin, the first resource scheduler that jointly decides the bitrates and fine-grained resource allocation to perform SR at the edge, which can learn to optimize the long-term QoE for distributed end users. To handle the time-varying and complex space of decisions as well as a non-smooth objective function, Rosevin realizes a novel online combinatorial learning algorithm, which nicely integrates convex optimization theories and online learning techniques. In addition to theoretically analyzing its performance, we implement an SR-assisted video streaming prototype of Rosevin and demonstrate its advantages over several video delivery benchmarks.}
}


@inproceedings{DBLP:conf/infocom/ZhangZWC24,
	author = {Lei Zhang and
                  Haobin Zhou and
                  Haiyang Wang and
                  Laizhong Cui},
	title = {{TBSR:} Tile-Based 360{\textdegree} Video Streaming with Super-Resolution
                  on Commodity Mobile Devices},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {501--510},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621078},
	doi = {10.1109/INFOCOM52122.2024.10621078},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangZWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Streaming 360° videos demands excessive bandwidth. Tile-based streaming and super-resolution are two widely studied approaches to alleviate bandwidth shortage and enhance user experience in such real-time video streaming systems. The former prioritizes the transmission of a fraction of the 360° video according to the user viewport, while the latter enhances the streamed video in higher resolutions through computations. However, these two approaches bring substantial complexity and computation overhead and thus suffer from resource bottlenecks due to the constrained mobile hardware. This paper proposes TBSR, a practical mobile 360° video streaming system that incorporates in-time super-resolution with tile-based streaming on commodity mobile devices. We present the designs of three key mechanisms, including a rate adaptation method with macro tile grouping to reduce decoding computations, a decoding and SR scheduler for different types of tasks to achieve the best cost efficiency, and the workload adjustment method to control the amount of tasks given the available capabilities. We further implement the TBSR prototype. Our performance evaluation shows that TBSR outperforms the existing methods, improving QoE quality by up to 32% and bandwidth savings by 26%.}
}


@inproceedings{DBLP:conf/infocom/MaLPTYTMMM24,
	author = {Xiaoteng Ma and
                  Qing Li and
                  Junkun Peng and
                  Gareth Tyson and
                  Ziwen Ye and
                  Shisong Tang and
                  Qian Ma and
                  Shengbin Meng and
                  Gabriel{-}Miro Muntean},
	title = {Smart Data-Driven Proactive Push to Edge Network for User-Generated
                  Videos},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {511--520},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621410},
	doi = {10.1109/INFOCOM52122.2024.10621410},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MaLPTYTMMM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To reduce costs and improve performance, video Content Delivery Networks (CDNs) have started to incorporate lightweight edge nodes, e.g., WiFi access points. Because of this, it is necessary for CDNs to intelligently select which video files should be placed at their core data centers vs. these edge nodes. This is more complex than traditional CDN management, as lightweight edge nodes are much more numerous and unstable than data centers. With this in mind, we present SDPush —- a system for managing content placement in edge CDNs. SDPush tackles two problems. First, it is necessary for SDPush to select which files to proactive push. To address this, we build a file popularity prediction model that effectively identifies video files that will receive many views. Second, SDPush should determine how many replicas of each file to push. To address this, we design a model to predict the benefits of pushing particular files (regarding traffic savings) and then formulate the replica decision problem as a lightweight problem, which is solvable within seconds, even for platforms that accommodate millions of daily active users. Through a trace-driven evaluation and a live deployment on a real video platform, we validate SDPush’s effectiveness, offloading peak-period traffic by 12.1% to 23.9% from the data center to edge nodes, thereby reducing the CDN costs.}
}


@inproceedings{DBLP:conf/infocom/LaiWLWZHLL24,
	author = {Zeqi Lai and
                  Yibo Wang and
                  Hewu Li and
                  Qian Wu and
                  Qi Zhang and
                  Yunan Hou and
                  Jun Liu and
                  Yuanjie Li},
	title = {Your Mega-Constellations Can Be Slim: {A} Cost-Effective Approach
                  for Constructing Survivable and Performant {LEO} Satellite Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {521--530},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621083},
	doi = {10.1109/INFOCOM52122.2024.10621083},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LaiWLWZHLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently we have witnessed the active deployment of mega-constellations with hundreds to thousands of low-earth orbit (LEO) satellites, targeting at constructing LEO satellite networks (LSN) to provide ubiquitous Internet services globally. However, while the massive deployment of LEO satellites can improve the network survivability and performance of an LSN, it also involves additional sustainable challenges such as higher deployment cost, risk of satellite conjunction and space debris.In this paper, we investigate an important research problem facing the upcoming satellite Internet: from a network perspective, how many satellites exactly do we need to construct a survivable and performant LSN? To answer this question, we first formulate the survivable and performant LSN design (SPLD) problem, which aims to find the minimum number of needed satellites to construct an LSN that can provide sufficient amount of redundant paths, required link capacity and acceptable latency for traffic carried by the LSN. Second, to efficiently solve the tricky SPLD problem, we propose MegaReduce, a requirement-driven constellation optimization mechanism, which can calculate feasible solutions for SPLD in polynomial time. Finally, we conduct extensive trace-driven simulations to verify MegaReduce’s cost-effectiveness in constructing survivable and performant LSNs on demand, and showcase how MegaReduce can help optimize the incremental deployment and long-term maintenance of future satellite Internet.}
}


@inproceedings{DBLP:conf/infocom/WuSWZ024,
	author = {Jiasheng Wu and
                  Shaojie Su and
                  Xiong Wang and
                  Jingjing Zhang and
                  Yue Gao},
	title = {Accelerating Handover in Mobile Satellite Network},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {531--540},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621115},
	doi = {10.1109/INFOCOM52122.2024.10621115},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WuSWZ024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The construction of Low Earth Orbit (LEO) satellite constellations has recently spurred tremendous attention from academia and industry. 5G and 6G standards have specified LEO satellite network as a key component of 5G and 6G networks. However, ground terminals experience frequent, high-latency handover incurred by satellites’ fast travelling speed, which deteriorates the performance of latency-sensitive applications. To address this challenge, we propose a novel handover flowchart for mobile satellite networks, which can considerably reduce the handover latency. The innovation behind this scheme is to mitigate the interaction between the access and core networks that occupy the majority of time overhead by leveraging the predictable travelling trajectory and spatial distribution inherent in mobile satellite networks. Specifically, we design a fine-grained synchronized algorithm to address the synchronization problem due to the lack of control signalling delivery between the access and core networks. Moreover, we minimize the computational complexity of the core network using information such as the satellite access strategy and unique spatial distribution, which is caused by frequent prediction operations. We have built a prototype for a mobile satellite network using modified Open5GS and UERANSIM, which is driven by actual LEO satellite constellations such as Starlink and Kuiper. We have conducted extensive experiments, and the results demonstrate that our proposed handover scheme can considerably reduce the handover latency compared to the 3GPP Non-terrestrial Networks (NTN) and two other existing handover schemes.}
}


@inproceedings{DBLP:conf/infocom/LiLLWLWLLZ24,
	author = {Jihao Li and
                  Hewu Li and
                  Zeqi Lai and
                  Qian Wu and
                  Weisen Liu and
                  Xiaomo Wang and
                  Yuanjie Li and
                  Jun Liu and
                  Qi Zhang},
	title = {SkyCastle: Taming {LEO} Mobility to Facilitate Seamless and Low-latency
                  Satellite Internet Services},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {541--550},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621390},
	doi = {10.1109/INFOCOM52122.2024.10621390},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiLLWLWLLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging integrated space and terrestrial networks (ISTN) built upon low earth orbit (LEO) satellite constellations aim at providing planet-wide Internet services, not only for residential users, but also for mobile users (e.g., in airplane and cruise scenarios). Efficiently managing global mobility and keeping connections active for mobile users is critical for ISTN operators. However, our quantitative analysis identifies that existing mobility management (MM) schemes suffer from frequent connection interruptions and long latency in ISTN scenarios. The fundamental challenge stems from a unique characteristic of ISTNs: not only users are mobile, but also core network infrastructures (i.e., LEO satellites) are frequently changing their locations in the network.To facilitate seamless and low-latency satellite Internet services, this paper presents SkyCastle, a novel network-based global mobility management mechanism. SkyCastle incorporates two key techniques to address frequent connection interruptions in ISTNs. First, to reduce the interruption time, SkyCastle adopts distributed satellite anchors to track the location changes of mobile nodes, manage handovers and avoid routing convergence. Second, SkyCastle leverages an anchor manager to schedule MM functionalities at satellites to reduce deployment costs while guaranteeing low latency. Extensive evaluations combining real constellation information and mobile user trajectories show that: SkyCastle can improve up to 55.8% uninterrupted time and reduce 47.8% latency as compared to other existing MM solutions.}
}


@inproceedings{DBLP:conf/infocom/ZhangYXZZMXDW24,
	author = {Qiyang Zhang and
                  Xin Yuan and
                  Ruolin Xing and
                  Yiran Zhang and
                  Zimu Zheng and
                  Xiao Ma and
                  Mengwei Xu and
                  Schahram Dustdar and
                  Shangguang Wang},
	title = {Resource-efficient In-orbit Detection of Earth Objects},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {551--560},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621328},
	doi = {10.1109/INFOCOM52122.2024.10621328},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangYXZZMXDW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid proliferation of large Low Earth Orbit (LEO) satellite constellations, a huge amount of in-orbit data is generated and needs to be transmitted to the ground for processing. However, traditional LEO satellite constellations, which downlink raw data to the ground, are significantly restricted in transmission capability. Orbital edge computing (OEC), which exploits the computation capacities of LEO satellites and processes the raw data in orbit, is envisioned as a promising solution to relieve the downlink burden. Yet, with OEC, the bottleneck is shifted to the inelastic computation capacities. The computational bottleneck arises from two primary challenges that existing satellite systems have not adequately addressed: the inability to process all captured images and the limited energy supply available for satellite operations. In this work, we seek to fully exploit the scarce satellite computation and communication resources to achieve satellite-ground collaboration and present a satellite-ground collaborative system named TargetFuse for onboard object detection. TargetFuse incorporates a combination of techniques to minimize detection errors under energy and bandwidth constraints. Extensive experiments show that TargetFuse can reduce detection errors by 3.4× on average, compared to onboard computing. TargetFuse achieves a 9.6× improvement in bandwidth efficiency compared to the vanilla baseline under the limited bandwidth budget constraint.}
}


@inproceedings{DBLP:conf/infocom/LiLZZXJ24,
	author = {Ruoyu Li and
                  Qing Li and
                  Yu Zhang and
                  Dan Zhao and
                  Xi Xiao and
                  Yong Jiang},
	title = {Genos: General In-Network Unsupervised Intrusion Detection by Rule
                  Extraction},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {561--570},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621157},
	doi = {10.1109/INFOCOM52122.2024.10621157},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiLZZXJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly-based network intrusion detection systems (A-NIDS) use unsupervised models to detect unforeseen attacks. However, existing A-NIDS solutions suffer from low throughput, lack of interpretability, and high maintenance costs. Recent in-network intelligence (INI) exploits programmable switches to offer line-rate deployment of NIDS. Nevertheless, current in-network NIDS are either model-specific or only apply to supervised models. In this paper, we propose Genos, a general in-network framework for unsupervised A-NIDS by rule extraction, which consists of a Model Compiler, a Model Interpreter, and a Model Debugger. Specifically, observing benign data are multi-modal and usually located in multiple subspaces in the feature space, we utilize a divide-and-conquer approach for model-agnostic rule extraction. In the Model Compiler, we first propose a tree-based clustering algorithm to partition the feature space into subspaces, then design a decision boundary estimation mechanism to approximate the source model in each subspace. The Model Interpreter interprets predictions by important attributes to aid network operators in understanding the predictions. The Model Debugger conducts incremental updating to rectify errors by only fine-tuning rules on affected subspaces, thus reducing maintenance costs. We implement a prototype using physical hardware, and experiments demonstrate its superior performance of 100 Gbps throughput, great interpretability, and trivial updating overhead.}
}


@inproceedings{DBLP:conf/infocom/AmalapuramTC24,
	author = {Suresh Kumar Amalapuram and
                  Bheemarjuna Reddy Tamma and
                  Sumohana S. Channappayya},
	title = {{SPIDER:} {A} Semi-Supervised Continual Learning-based Network Intrusion
                  Detection System},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {571--580},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621428},
	doi = {10.1109/INFOCOM52122.2024.10621428},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/AmalapuramTC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network intrusion detection (NID) aims to identify unusual network traffic patterns (distribution shifts) that require NID systems to evolve continuously. While prior art emphasizes fully supervised annotated data-intensive continual learning methods for NID, semi-supervised continual learning (SSCL) methods require only limited annotated data. However, the inherent class imbalance (CI) in network traffic can significantly impact the performance of SSCL approaches. Previous approaches to tackle CI issues require storing a subset of labeled training samples from all past tasks in the memory for an extended duration, potentially raising privacy concerns. The proposed Semisupervised Privacy-preserving Intrusion detection with Drift-aware continual LEaRning (SPIDER) is a novel method that combines gradient projection memory (GPM) with SSCL to handle CI effectively without the requirement to store labeled samples from all of the previous tasks. We assess SPIDER’s performance against baselines on six intrusion detection benchmarks formed over a short period and the Anoshift benchmark spanning ten years, which includes natural distribution shifts. Additionally, we validate our approach on standard continual learning image classification benchmarks known for frequent distribution shifts compared to NID benchmarks. SPIDER achieves comparable performance to fully supervised and semisupervised baseline methods, while requiring a maximum of 20% annotated data and reducing the total training time by 2X.}
}


@inproceedings{DBLP:conf/infocom/ZhangZJSDNY24,
	author = {Xinchen Zhang and
                  Running Zhao and
                  Zhihan Jiang and
                  Zhicong Sun and
                  Yulong Ding and
                  Edith C. H. Ngai and
                  Shuang{-}Hua Yang},
	title = {{AOC-IDS:} Autonomous Online Framework with Contrastive Learning for
                  Intrusion Detection},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {581--590},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621346},
	doi = {10.1109/INFOCOM52122.2024.10621346},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangZJSDNY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid expansion of the Internet of Things (IoT) has raised increasing concern about targeted cyber attacks. Previous research primarily focused on static Intrusion Detection Systems (IDSs), which employ offline training to safeguard IoT systems. However, such static IDSs struggle with real-world scenarios where IoT system behaviors and attack strategies can undergo rapid evolution, necessitating dynamic and adaptable IDSs. In response to this challenge, we propose AOC-IDS, a novel online IDS that features an autonomous anomaly detection module (ADM) and a labor-free online framework for continual adaptation. In order to enhance data comprehension, the ADM employs an Autoencoder (AE) with a tailored Cluster Repelling Contrastive (CRC) loss function to generate distinctive representation from limited or incrementally incoming data in the online setting. Moreover, to reduce the burden of manual labeling, our online framework leverages pseudo-labels automatically generated from the decision-making process in the ADM to facilitate periodic updates of the ADM. The elimination of human intervention for labeling and decision-making boosts the system’s compatibility and adaptability in the online setting to remain synchronized with dynamic environments. Experimental validation using the NSL-KDD and UNSW-NB15 datasets demonstrates the superior performance and adaptability of AOC-IDS, surpassing the state-of-the-art solutions. The code is released at https://github.com/xinchen930/AOC-IDS.}
}


@inproceedings{DBLP:conf/infocom/0008LSZ024,
	author = {Ziming Zhao and
                  Zhaoxuan Li and
                  Zhuoxue Song and
                  Fan Zhang and
                  Binbin Chen},
	title = {{RIDS:} Towards Advanced {IDS} via {RNN} Model and Programmable Switches
                  Co-Designed Approaches},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {591--600},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621290},
	doi = {10.1109/INFOCOM52122.2024.10621290},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0008LSZ024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing Deep Learning (DL)-based network Intrusion Detection System (IDS) is able to characterize sequence semantics of traffic and discover malicious behaviors. Yet DL models are often nonlinear and highly non-convex functions that are difficult for in-network deployment. In this paper, we present RIDS, a hardware-friendly Recurrent Neural Network (RNN) model that is co-designed with programmable switches. As its core, RIDS is powered by two tightly-coupled components: (i) rLearner, the RNN learning module with in-network deployability as the first-class requirement; and (ii) rEnforcer, the concrete pipeline design to realize rLearner-generated models inside the network dataplane. We implement a prototype of RIDS and evaluate it on our physical testbed. The experiments show that RIDS could satisfy both detection performance and high-speed bandwidth adaptation simultaneously, when none of the other existing approaches could do so. Inspiringly, RIDS realizes remarkable intrusion/malware detection effect (e.g., ~99% F1 score) and model deployment (e.g., 100 Gbps per port), while only imposing nanoseconds of latency.}
}


@inproceedings{DBLP:conf/infocom/SuZCLL24,
	author = {Xiaoxin Su and
                  Yipeng Zhou and
                  Laizhong Cui and
                  John C. S. Lui and
                  Jiangchuan Liu},
	title = {Fed-CVLC: Compressing Federated Learning Communications with Variable-Length
                  Codes},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {601--610},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621361},
	doi = {10.1109/INFOCOM52122.2024.10621361},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SuZCLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients. FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck. Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates. In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL. We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates. We develop optimal tuning strategy that minimizes the loss function (equivalent to maximizing the model utility) subject to the budget for communication. We further demonstrate that Fed-CVLC is indeed a general compression design that bridges quantization and sparsification, with greater flexibility. Extensive experiments have been conducted with public datasets to demonstrate that Fed-CVLC remarkably outperforms state-of-the-art baselines, improving model utility by 1.50%-5.44%, or shrinking communication traffic by 16.67%-41.61%.}
}


@inproceedings{DBLP:conf/infocom/SuHLL24,
	author = {Ningxin Su and
                  Chenghao Hu and
                  Baochun Li and
                  Bo Li},
	title = {Titanic: Towards Production Federated Learning with Large Language
                  Models},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {611--620},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621164},
	doi = {10.1109/INFOCOM52122.2024.10621164},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SuHLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the recent surge of research interests in Large Language Models (LLMs), a natural question that arises is how pre-trained LLMs can be fine-tuned to tailor to specific needs of enterprises and individual users, while preserving the privacy of data used in the fine-tuning process. On the one hand, sending private data to cloud datacenters for fine-tuning is, without a doubt, unacceptable from a privacy perspective. On the other hand, conventional federated learning requires each client to perform local training, which is not feasible for LLMs with respect to both computation costs and communication overhead, since they involve billions of model parameters. In this paper, we present Titanic, a new distributed training paradigm that allows LLMs to be fine-tuned in a privacy-preserving fashion directly on the client devices where private data is produced, while operating within the resource constraints on computation and communication bandwidth. Titanic first optimally selects a subset of clients with an efficient solution to an integer optimization problem, then partitions an LLM across multiple client devices, and finally fine-tunes the model with no or minimal losses in training performance. A primary focus in the design of Titanic is its feasibility in real-world systems: it is first and foremost designed for production-quality systems, featuring a model-agnostic partitioning mechanism that is fully automated. Our experimental results show that Titanic achieves superior training performance as compared to conventional federated learning, while preserving data privacy and satisfying all constraints on local computation and bandwidth resources.}
}


@inproceedings{DBLP:conf/infocom/LiuCLLW24,
	author = {Yiqi Liu and
                  Shan Chang and
                  Ye Liu and
                  Bo Li and
                  Cong Wang},
	title = {FairFed: Improving Fairness and Efficiency of Contribution Evaluation
                  in Federated Learning via Cooperative Shapley Value},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {621--630},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621438},
	doi = {10.1109/INFOCOM52122.2024.10621438},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuCLLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The quality of federated learning (FL) is highly correlated with the number and quality of the participants involved. It is essential to design proper contribution evaluation mechanisms. Shapley Value (SV)-based techniques have been widely used to provide fair contribution evaluation. Existing approaches, however, do not support dynamic participants (e.g., joining and departure) and incur significant computation costs, making them difficult to apply in practice. Worse, participants may be incorrectly valued as negative contribution under the Non-IID data scenarios, further jeopardizing fairness. In this work, we propose FairFed to address the above challenges. First, given that each iteration is of equal importance, FairFed treats FL as Multiple Single-stage Cooperative Games, and evaluates participants by each iteration for effectively coping with dynamic participants and ensuring fairness across iterations. Second, we introduce Cooperative Shapley Value (CSV) to rectify negative values of participants to improving the fairness while preserving true negative values. Third, we prove if participants are Strategically Equivalent, the number of participant combinations can be sharply reduced from exponential to polynomial, thus significantly reducing the computational complexity of CSV. Experimental results show that FairFed achieves up to 25.3 × speedup and reduces deviations by three orders of magnitude to two state-of-the-art approximation approaches.}
}


@inproceedings{DBLP:conf/infocom/Han0J024,
	author = {Pengchao Han and
                  Shiqiang Wang and
                  Yang Jiao and
                  Jianwei Huang},
	title = {Federated Learning While Providing Model as a Service: Joint Training
                  and Inference Optimization},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {631--640},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621105},
	doi = {10.1109/INFOCOM52122.2024.10621105},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Han0J024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While providing machine learning model as a service to process users’ inference requests, online applications can periodically upgrade the model utilizing newly collected data. Federated learning (FL) is beneficial for enabling the training of models across distributed clients while keeping the data locally. However, existing work has overlooked the coexistence of model training and inference under clients’ limited resources. This paper focuses on the joint optimization of model training and inference to maximize inference performance at clients. Such an optimization faces several challenges. The first challenge is to characterize the clients’ inference performance when clients may partially participate in FL. To resolve this challenge, we introduce a new notion of age of model (AoM) to quantify client-side model freshness, based on which we use FL’s global model convergence error as an approximate measure of inference performance. The second challenge is the tight coupling among clients’ decisions, including participation probability in FL, model download probability, and service rates. Toward the challenges, we propose an online problem approximation to reduce the problem complexity and optimize the resources to balance the needs of model training and inference. Experimental results demonstrate that the proposed algorithm improves the average inference accuracy by up to 12%.}
}


@inproceedings{DBLP:conf/infocom/RamakanthTM24,
	author = {R. Vallabh Ramakanth and
                  Vishrant Tripathi and
                  Eytan H. Modiano},
	title = {Monitoring Correlated Sources: AoI-based Scheduling is Nearly Optimal},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {641--650},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621253},
	doi = {10.1109/INFOCOM52122.2024.10621253},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/RamakanthTM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the design of scheduling policies to minimize monitoring error for a collection of correlated sources, where only one source can be observed at any given time. We model correlated sources as a discrete-time Wiener process, where the increments are multivariate normal random variables, with a general covariance matrix that captures the correlation structure between the sources. Under a Kalman filter based optimal estimation framework, we show that the performance of all scheduling policies oblivious to instantaneous error, can be lower and upper bounded by the weighted sum of Age of Information (AoI) across the sources for appropriately chosen weights. We use this insight to design scheduling policies that are only a constant factor away from optimality, and make the rather surprising observation that AoI-based scheduling that ignores correlation is sufficient to obtain performance guarantees. We also derive scaling results that show that the optimal error scales roughly as the square of the dimensionality of the system, even in the presence of correlation. Finally, we provide simulation results to verify our claims.}
}


@inproceedings{DBLP:conf/infocom/TsanikidisG24,
	author = {Christos Tsanikidis and
                  Javad Ghaderi},
	title = {Scheduling Stochastic Traffic With End-to-End Deadlines in Multi-hop
                  Wireless Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {651--660},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621239},
	doi = {10.1109/INFOCOM52122.2024.10621239},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TsanikidisG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scheduling deadline-constrained packets in multihop networks has received increased attention recently. However, there is very limited work on this problem for wireless networks where links are subject to interference. The existing algorithms either provide approximation ratio guarantees which diminish in quality as parameters of the network scale, or hold in an asymptotic regime when the time horizon, network bandwidth, and packet arrival rates are scaled to infinity, which limits their practicality. While attaining a constant approximation ratio has been shown to be impossible in the worst-case traffic setting, it is unclear if the same holds under stochastic traffic, in a non-asymptotic setting. In this work, we show that, in the stochastic traffic setting, constant approximation ratio or near-optimal algorithms can be achieved. Specifically, we propose algorithms that attain Ω((1 − ϵ)/β) or Ω(1 − ϵ) fraction of the optimal value, when the number of channels is\nC=Ω(\nlog(L/ε)\nε\n2\n)\nor\nC=Ω(\nχ\n⋆\nlog(L/ε)\nε\n3\n)\nrespectively, where L is the maximum route length of packets, χ ⋆ is the fractional chromatic number of the network’s interference graph, and β is its interference degree. This marks the first near-optimal results under nontrivial traffic and bandwidth assumptions in a non-asymptotic regime.}
}


@inproceedings{DBLP:conf/infocom/BlocherNCKEW24,
	author = {Marcel Bl{\"{o}}cher and
                  Nils Nedderhut and
                  Pavel Chuprikov and
                  Ramin Khalili and
                  Patrick Eugster and
                  Lin Wang},
	title = {Train Once Apply Anywhere: Effective Scheduling for Network Function
                  Chains Running on {FUMES}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {661--670},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621125},
	doi = {10.1109/INFOCOM52122.2024.10621125},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/BlocherNCKEW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of network function virtualization has enabled network function chaining as a flexible approach for building complex network services. However, the high degree of flexibility envisioned for orchestrating network function chains introduces several challenges to support dynamism in workloads and the environment necessary for their realization. Existing works mostly consider supporting dynamism by re-adjusting provisioning of network function instances, incurring reaction times that are prohibitively high in practice. Existing solutions to dynamic packet scheduling rely on centralized schedulers and a priori knowledge of traffic characteristics, and cannot handle changes in the environment like link failures.We fill this gap by presenting FUMES, a reinforcement learning based distributed agent design for the runtime scheduling problem of assigning packets undergoing treatment by network function chains to network function instances. Our design consists of multiple distributed agents that cooperatively work on the scheduling problem. A key design choice enables agents, once trained, to be applicable for unknown chains and traffic patterns including branching, and different environments including link failures. The paper presents the system design and shows its suitability for realistic deployments. We empirically compare FUMES with state-of-the-art runtime scheduling solutions showing improved scheduling decisions at lower server capacity.}
}


@inproceedings{DBLP:conf/infocom/HaoYLZWR24,
	author = {Yijun Hao and
                  Shusen Yang and
                  Fang Li and
                  Yifan Zhang and
                  Shibo Wang and
                  Xuebin Ren},
	title = {EdgeTimer: Adaptive Multi-Timescale Scheduling in Mobile Edge Computing
                  with Deep Reinforcement Learning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {671--680},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621305},
	doi = {10.1109/INFOCOM52122.2024.10621305},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HaoYLZWR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mobile edge computing (MEC), resource scheduling is crucial to task requests’ performance and service providers’ cost, involving multi-layer heterogeneous scheduling decisions. Existing schedulers typically adopt static timescales to regularly update scheduling decisions of each layer, without adaptive adjustment of timescales for different layers, resulting in potentially poor performance in practice.We notice that the adaptive timescales would significantly improve the trade-off between the operation cost and delay performance. Based on this insight, we propose EdgeTimer, the first work to automatically generate adaptive timescales to update multi-layer scheduling decisions using deep reinforcement learning (DRL). First, EdgeTimer uses a three-layer hierarchical DRL framework to decouple the multi-layer decision-making task into a hierarchy of independent sub-tasks for improving learning efficiency. Second, to cope with each sub-task, EdgeTimer adopts a safe multi-agent DRL algorithm for decentralized scheduling while ensuring system reliability. We apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns. Extensive trace-driven experiments demonstrate that EdgeTimer can learn adaptive timescales, irrespective of workload patterns and built-in scheduling rules. It obtains up to 9:1 more profit than existing approaches without sacrificing the delay performance.}
}


@inproceedings{DBLP:conf/infocom/00010F24,
	author = {Shuhao Liu and
                  Li Chen and
                  Yuanzhong Fu},
	title = {Periscoping: Private Key Distribution for Large-Scale Mixnets},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {681--690},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621274},
	doi = {10.1109/INFOCOM52122.2024.10621274},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/00010F24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mix networks, or mixnets, are one of the fundamental building blocks of anonymity systems. To defend against epistemic attacks, existing free-route mixnet designs require all clients to maintain a consistent, up-to-date view of the entire key directory. This, however, inevitably raises the performance concern under system scale-out: in a larger mixnet, a client will consume more bandwidth for updating keys in the background.This paper presents Periscoping, a key distribution protocol for mixnets at scale. Periscoping relaxes the download-all requirement for clients. Instead, it allows a client to selectively download a constant number of entries of the key directory, while guaranteeing the privacy of selections. Periscoping achieves this goal via a novel Private Information Retrieval scheme, constructed based on constrained Pseudorandom Functions. Moreover, the protocol is integrated seamlessly into the mixnet operations, readily applicable to existing mixnet systems as an extension at a minimal cost. Our experiments show that, with millions of mixes, it can reduce the traffic load of a mixnet by orders of magnitude, at a minor computational and bandwidth overhead.}
}


@inproceedings{DBLP:conf/infocom/ZhaoLZXLL24,
	author = {Wenwei Zhao and
                  Xiaowen Li and
                  Shangqing Zhao and
                  Jie Xu and
                  Yao Liu and
                  Zhuo Lu},
	title = {Detecting Adversarial Spectrum Attacks via Distance to Decision Boundary
                  Statistics},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {691--700},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621153},
	doi = {10.1109/INFOCOM52122.2024.10621153},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoLZXLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning has been adopted for efficient cooperative spectrum sensing. However, it incurs an additional security risk due to attacks leveraging adversarial machine learning to create malicious spectrum sensing values to deceive the fusion center, called adversarial spectrum attacks. In this paper, we propose an efficient framework for detecting adversarial spectrum attacks. Our design leverages the concept of the distance to the decision boundary (DDB) observed at the fusion center and compares the training and testing DDB distributions to identify adversarial spectrum attacks. We create a computationally efficient way to compute the DDB for machine learning based spectrum sensing systems. Experimental results based on realistic spectrum data show that our method, under typical settings, achieves a high detection rate of up to 99% and maintains a low false alarm rate of less than 1%. In addition, our method to compute the DDB based on spectrum data achieves 54%–64% improvements in computational efficiency over existing distance calculation methods. The proposed DDB-based detection framework offers a practical and efficient solution for identifying malicious sensing values created by adversarial spectrum attacks.}
}


@inproceedings{DBLP:conf/infocom/YangWAZ0024,
	author = {Yanni Yang and
                  Genglin Wang and
                  Zhenlin An and
                  Guoming Zhang and
                  Xiuzhen Cheng and
                  Pengfei Hu},
	title = {RF-Parrot: Wireless Eavesdropping on Wired Audio},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {701--710},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621385},
	doi = {10.1109/INFOCOM52122.2024.10621385},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangWAZ0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent works demonstrated that we can eavesdrop on audio by using radio frequency signals or videos to capture the physical surface vibrations of surrounding objects. They fall short when it comes to intercepting internally transmitted audio through wires. In this work, we first address this gap by proposing a new eavesdropping system, RF-Parrot, that can wirelessly capture the audio signal transmitted in earphone wires. Our system involves embedding a tiny field-effect transistor in the wire to create a battery-free retroreflector, with its reflective efficiency tied to the audio signal’s amplitude. To capture full details of the analog audio signals, we engineered a novel retroreflector using a depletion-mode MOSFET, which can be activated by any voltage of the audio signals, ensuring no information loss. We also developed a theoretical model to demystify the nonlinear transmission of the retroreflector, identifying it as a convolution operation on the audio spectrum. Subsequently, we have designed a novel convolutional neural network-based model to accurately reconstruct the original audio. Our extensive experimental results demonstrate that the reconstructed audio bears a strong resemblance to the original audio, achieving an impressive 95% accuracy in speech command recognition.}
}


@inproceedings{DBLP:conf/infocom/Zheng0YJ0WLC024,
	author = {Yawen Zheng and
                  Fan Dang and
                  Zihao Yang and
                  Jinyan Jiang and
                  Xu Wang and
                  Lin Wang and
                  Kebin Liu and
                  Xinlei Chen and
                  Yunhao Liu},
	title = {BlueKey: Exploiting Bluetooth Low Energy for Enhanced Physical-Layer
                  Key Generation},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {711--720},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621142},
	doi = {10.1109/INFOCOM52122.2024.10621142},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zheng0YJ0WLC024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth Low Energy (BLE) is a prevalent technology in various applications due to its low power consumption and wide device compatibility. Despite its numerous advantages, the encryption methods of BLE often expose devices to potential attacks. To fortify security, we investigate the application of Physical-layer Key Generation (PKG), a promising technology that enables devices to generate a shared secret key from their shared physical environment. We propose a distinctive approach that capitalizes on the inherent characteristics of BLE to facilitate efficient PKG. We harness the constant tone extension within BLE protocols to extract comprehensive physical layer information and introduce an innovative method that employs Legendre polynomial quantization for PKG. This method facilitates the exchange of secret keys with a high key matching rate and a high key generation rate. The efficacy of our approach is validated through extensive experiments on a software-defined radio platform, underscoring its potential to enhance security in the rapidly expanding field of BLE applications.}
}


@inproceedings{DBLP:conf/infocom/SchiavoAGFC24,
	author = {Leonardo Lo Schiavo and
                  Jose A. Ayala{-}Romero and
                  Andres Garcia{-}Saavedra and
                  Marco Fiore and
                  Xavier Costa{-}P{\'{e}}rez},
	title = {YinYangRAN: Resource Multiplexing in GPU-Accelerated Virtualized RANs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {721--730},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621380},
	doi = {10.1109/INFOCOM52122.2024.10621380},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SchiavoAGFC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RAN virtualization is revolutionizing the telco industry, enabling 5G Distributed Units to run using general-purpose platforms equipped with Hardware Accelerators (HAs). Recently, GPUs have been proposed as HAs, hinging on their unique capability to execute 5G PHY operations efficiently while also processing Machine Learning (ML) workloads. While this ambivalence makes GPUs attractive for cost-effective deployments, we experimentally demonstrate that multiplexing 5G and ML workloads in GPUs is in fact challenging, and that using conventional GPU-sharing methods can severely disrupt 5G operations. We then introduce YinYangRAN, an innovative O-RAN-compliant solution that supervises GPU-based HAs so as to ensure reliability in the 5G processing pipeline while maximizing the throughput of concurrent ML services. YinYangRAN performs GPU resource allocation decisions via a computationally-efficient approximate dynamic programming technique, which is informed by a neural network trained on real-world measurements. Using workloads collected in real RANs, we demonstrate that YinYangRAN can achieve over 50% higher 5G processing reliability than conventional GPU sharing models with minimal impact on co-located ML workloads. To our knowledge, this is the first work identifying and addressing the complex problem of HA management in emerging GPU-accelerated vRANs, and represents a promising step towards multiplexing PHY and ML workloads in mobile networks.}
}


@inproceedings{DBLP:conf/infocom/HuBWFH24,
	author = {Bing Hu and
                  Yuanguo Bi and
                  Kui Wu and
                  Rao Fu and
                  Zixuan Huang},
	title = {A Lightweight Path Validation Scheme in Software-Defined Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {731--740},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621099},
	doi = {10.1109/INFOCOM52122.2024.10621099},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuBWFH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Networks (SDN) revolutionize traditional networks by separating control and data planes for enhanced agility and programmability. This separation, however, also opens up vulnerabilities, allowing adversaries to manipulate data plane forwarding and breach security policies. To counter this, we propose a Lightweight Path Validation Scheme (L-PVS) specifically designed for SDN environments. Our approach uses a simple validation scheme for packet forwarding paths that verifies the paths traversed by packets. Then, we further amplify the scheme with a network flow path validation to boost the validation efficiency. To reduce storage demands on switches during flow path validation, we develop a storage optimization method that aligns switch storage overhead with network flows rather than individual packets. Furthermore, we formulate a path partition scheme and present a Greedy-based KeySwitch Node Selection Algorithm (GKSS) to pinpoint optimal switches for path partition, significantly reducing overall data plane storage usage. Lastly, we design a technique using temporary KeySwitch nodes to identify anomaly switches when the controller encounters path validation failure. Evaluation results verify that L-PVS facilitates path validation with a reduced validation header size while minimizing the impact on processing delay and switch storage overhead.}
}


@inproceedings{DBLP:conf/infocom/HeD0ZWWYZSSLLZ24,
	author = {Xin He and
                  Enhuan Dong and
                  Jiahai Yang and
                  Shize Zhang and
                  Zhiliang Wang and
                  Zejie Wang and
                  Ye Yang and
                  Jun Zhou and
                  Xiaoqing Sun and
                  Enge Song and
                  Jianyuan Lu and
                  Biao Lyu and
                  Shunmin Zhu},
	title = {CloudPlanner: Minimizing Upgrade Risk of Virtual Network Devices for
                  Large-Scale Cloud Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {741--750},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621109},
	doi = {10.1109/INFOCOM52122.2024.10621109},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HeD0ZWWYZSSLLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud networks continuously upgrade softwarized virtual network devices (VNDs) to meet evolving tenant demands. However, such upgrades may result in unexpected failures. An intuitive idea to prevent upgrade failures is to resolve all compatibility issues before deployment, but it is impractical to replicate all deployed VND cases and test them with lots of replayed real traffic for the VND developers. As a result, the operations team takes upgrade risk to test upgrades by gradually deploying them. Although careful upgrade schedule planning is the most common method to minimize upgrade risk, to the best of our knowledge, no VND upgrade schedule planning scheme has been adequately studied for large-scale cloud networks. To fill this gap, we propose CloudPlanner, the first VND upgrade schedule planning scheme aiming to minimize the VND upgrade risk for large-scale cloud networks. CloudPlanner prioritizes upgrading VNDs that are more likely to trigger failures based on expert knowledge and historical failure-trigger VND properties and limits the number of tenants associated with simultaneously upgraded VNDs. We also propose a heuristic solver which can quickly and greedily plan schedules. Using real-world data from production environments, we demonstrate the benefits of CloudPlanner through extensive experiments.}
}


@inproceedings{DBLP:conf/infocom/BehraveshBLR24,
	author = {Rasoul Behravesh and
                  David Breitgand and
                  Dean H. Lorenz and
                  Danny Raz},
	title = {A Practical Near Optimal Deployment of Service Function Chains in
                  Edge-to-Cloud Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {751--760},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621162},
	doi = {10.1109/INFOCOM52122.2024.10621162},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/BehraveshBLR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing offers a myriad of opportunities to innovate and introduce novel applications, thereby enhancing user experiences considerably. A critical issue extensively investigated in this domain is efficient deployment of Service Function Chains (SFCs) across the physical network, spanning from the edge to the cloud. This problem is known to be NP-hard. As a result of its practical importance, there is significant interest in the development of high-quality sub-optimal solutions.In this paper, we consider this problem and propose a novel near-optimal heuristic that is extremely efficient and scalable. We compare our solution to the state-of-the-art heuristic and to the theoretical optimum. In our large scale evaluations, we use realistic topologies which were previously reported in the literature. We demonstrate that the execution time offered by our solution grows slowly as the number of Virtual Network Function (VNF) forwarding graph embedding requests grows, and it handles one million requests in slightly more than 20 seconds for 100 nodes and 150 edges physical topology.}
}


@inproceedings{DBLP:conf/infocom/ChenCHJ0C24,
	author = {Siyang Chen and
                  Shuangwu Chen and
                  Huasen He and
                  Xiaofeng Jiang and
                  Jian Yang and
                  Siyu Cheng},
	title = {Causality Correlation and Context Learning Aided Robust Lightweight
                  Multi-Tab Website Fingerprinting Over Encrypted Tunnel},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {761--770},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621235},
	doi = {10.1109/INFOCOM52122.2024.10621235},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenCHJ0C24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Encrypted tunnels are increasingly applied to privacy protection, however, a passive eavesdropper can still infer which website a user is visiting via website fingerprinting (WF). State-of-the-art WF suffers from several critical challenges in a realistic multi-tab web browsing scenario, where the number of concurrent tabs is dynamic and uncertain, training a separate model for each website is too overweight to deploy, and the robustness against the packet loss, duplication and disorder caused by dynamic network conditions is rarely considered. To address these challenges, we propose a robust and lightweight multi-tab WF method over the encrypted tunnel, named RobustWF. Due to the causality relationship between user’s request and website’s response, RobustWF employs causality correlation to associate the interactive packets belonging to the same website together, which form a causality chain. Then, RobustWF utilizes context learning to capture the dependencies between the causality chains. The missing of some specific details does not have a significant impact on the overall structure of target web, thus enhancing the robustness of RobustWF. To make the model lightweight enough, RobustWF trains an integrated model to adapt to the dynamic number of concurrent tabs. The experimental results demonstrate that the accuracy of RobustWF improves 14% in dynamic multi-tab WF scenario compared to the State-of-the-art method.}
}


@inproceedings{DBLP:conf/infocom/WeiYYX24,
	author = {Qiushi Wei and
                  Dejun Yang and
                  Ruozhou Yu and
                  Guoliang Xue},
	title = {Thor: {A} Virtual Payment Channel Network Construction Protocol over
                  Cryptocurrencies},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {771--780},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621092},
	doi = {10.1109/INFOCOM52122.2024.10621092},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WeiYYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Payment Channel Networks (PCNs) have been proposed as a second-layer solution to the scalability issue of blockchain-based cryptocurrencies, most developed systems still lack effective strategies for further scalability solutions. Virtual payment channel (VPC) has been proposed as an off-chain technique that avoids the involvement of intermediaries for payments in a PCN. However, there is no research on how to efficiently construct VPCs while considering the characteristics of the underlying PCN. To fill this void, this paper focuses on the VPC construction in a PCN. More specifically, we propose a metric, Capacity to the Number of Intermediaries Ratio (CNIR), to consider both the capacity of the constructed VPC and the collateral locked by the involved users. We first study the VPC construction problem for a single pair of users and design an efficient algorithm that achieves the optimal CNIR. Based on this, we propose Thor, a protocol that constructs a virtual payment channel network (VPCN) for multiple pairs. Evaluation results show that Thor can efficiently construct a VPCN and outperform baseline algorithms in terms of the CNIR.}
}


@inproceedings{DBLP:conf/infocom/ShiZZZZ0WZSLG24,
	author = {Shuo Shi and
                  Chao Zhang and
                  Zongpu Zhang and
                  Hubin Zhang and
                  Xin Zeng and
                  Weigang Li and
                  Junyuan Wang and
                  Xiantao Zhang and
                  Yibin Shen and
                  Jian Li and
                  Haibing Guan},
	title = {vCrypto: a Unified Para-Virtualization Framework for Heterogeneous
                  Cryptographic Resources},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {781--790},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621287},
	doi = {10.1109/INFOCOM52122.2024.10621287},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ShiZZZZ0WZSLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transport Layer Security (TLS) connections involve costly cryptographic operations which incur significant resource consumption in the cloud. Hardware accelerators are affordable substitutes of expensive CPU cores to accommodate with the constantly increasing security requirements of datacenters. Existing accelerators virtualization mainly relies on passthrough of Single Root I/O Virtualization (SR-IOV) devices. However, deficiency of service accessibility, functionality and availability make device passthrough not an optimal solution for heterogeneous accelerators with different capabilities. To make up the gap, we propose vCrypto, a unified para-virtualization framework for heterogeneous cryptographic resources. vCrypto supports stateful crypto requests offloading and result retrieval with session lifecycle management and event driven notification. vCrypto transparently integrates virtual crypto device capabilities into the OpenSSL framework to benefit existing applications that are based on crypto library APIs without modification. Multiple physical resources can be partitioned flexibly and scheduled cooperatively to enhance the functionality, performance and robustness of virtual crypto service. Finally, vCrypto achieves an optimized performance with two layers polling and memory sharing mechanism. The comprehensive experiments show that with the same cryptographic resources used, vCrypto framework can provide 2.59x to 3.36x higher AES-CBC-HMAC-SHA1 throughput compared to passthrough SR-IOV device.}
}


@inproceedings{DBLP:conf/infocom/YanL0WH0W24,
	author = {Nan Yan and
                  Yuqing Li and
                  Jing Chen and
                  Xiong Wang and
                  Jianan Hong and
                  Kun He and
                  Wei Wang},
	title = {Efficient and Straggler-Resistant Homomorphic Encryption for Heterogeneous
                  Federated Learning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {791--800},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621440},
	doi = {10.1109/INFOCOM52122.2024.10621440},
	timestamp = {Mon, 02 Sep 2024 15:15:07 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YanL0WH0W24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-silo federated learning (FL) enables multiple institutions (clients) to collaboratively build a global model without sharing their private data. To prevent privacy leakage during aggregation, homomorphic encryption (HE) is widely used to encrypt model updates, yet incurs high computation and communication overheads. To reduce these overheads, packed HE (PHE) has been proposed to encrypt multiple plaintexts into a single ciphertext. However, the original design of PHE does not consider the heterogeneity among different clients, an intrinsic problem in cross-silo FL, often resulting in undermined training efficiency with slow convergence and stragglers. In this work, we propose FedPHE, an efficiently packed homomorphically encrypted FL framework with secure weighted aggregation and client selection to tackle the heterogeneity problem. Specifically, using CKKS with sparsification, FedPHE can achieve efficient encrypted weighted aggregation by accounting for contributions of local updates to the global model. To mitigate the straggler effect, we devise a sketching-based client selection scheme to cherry-pick representative clients with heterogeneous models and computing capabilities. We show, through rigorous security analysis and extensive experiments, that FedPHE can efficiently safeguard clients’ privacy, achieve a training speedup of 1.85 − 4.44×, cut the communication overhead by 1.24 − 22.62× , and reduce the straggler effect by up to 1.71 − 2.39×.}
}


@inproceedings{DBLP:conf/infocom/WangZW024,
	author = {Zibo Wang and
                  Yifei Zhu and
                  Dan Wang and
                  Zhu Han},
	title = {Federated Analytics-Empowered Frequent Pattern Mining for Decentralized
                  Web 3.0 Applications},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {801--810},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621294},
	doi = {10.1109/INFOCOM52122.2024.10621294},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangZW024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging Web 3.0 paradigm aims to decentralize existing web services, enabling desirable properties such as transparency, incentives, and privacy preservation. However, current Web 3.0 applications supported by blockchain infrastructure still cannot support complex data analytics tasks in a scalable and privacy-preserving way. This paper introduces the emerging federated analytics (FA) paradigm into the realm of Web 3.0 services, enabling data to stay local while still contributing to complex web analytics tasks in a privacy-preserving way. We propose FedWeb, a tailored FA design for important frequent pattern mining tasks in Web 3.0. FedWeb remarkably reduces the number of required participating data owners to support privacy-preserving Web 3.0 data analytics based on a novel distributed differential privacy technique. The correctness of mining results is guaranteed by a theoretically rigid candidate filtering scheme based on Hoeffding’s inequality and Chebychev’s inequality. Two response budget saving solutions are proposed to further reduce participating data owners. Experiments on three representative Web 3.0 scenarios show that FedWeb can improve data utility by ∼25.3% and reduce the participating data owners by ∼98.4%.}
}


@inproceedings{DBLP:conf/infocom/YueQHD024,
	author = {Sheng Yue and
                  Zerui Qin and
                  Xingyuan Hua and
                  Yongheng Deng and
                  Ju Ren},
	title = {Federated Offline Policy Optimization with Dual Regularization},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {811--820},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621140},
	doi = {10.1109/INFOCOM52122.2024.10621140},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YueQHD024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Reinforcement Learning (FRL) has been deemed as a promising solution for intelligent decision-making in the era of Artificial Internet of Things. However, existing FRL approaches often entail repeated interactions with the environment during local updating, which can be prohibitively expensive or even infeasible in many real-world domains. To overcome this challenge, this paper proposes a novel offline federated policy optimization algorithm, named DRPO, which enables distributed agents to collaboratively learn a decision policy only from private and static data without further environmental interactions. DRPO leverages dual regularization, incorporating both the local behavioral policy and the global aggregated policy, to judiciously cope with the intrinsic two-tier distributional shifts in offline FRL. Theoretical analysis characterizes the impact of the dual regularization on performance, demonstrating that by achieving the right balance thereof, DRPO can effectively counteract distributional shifts and ensure strict policy improvement in each federative learning round. Extensive experiments validate the significant performance gains of DRPO over baseline methods.}
}


@inproceedings{DBLP:conf/infocom/Guan00024,
	author = {Yixuan Guan and
                  Xuefeng Liu and
                  Jianwei Niu and
                  Tao Ren},
	title = {FedTC: Enabling Communication-Efficient Federated Learning via Transform
                  Coding},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {821--830},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621176},
	doi = {10.1109/INFOCOM52122.2024.10621176},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Guan00024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables distributed training via periodically synchronizing model updates among participants. Communication overhead becomes a dominant constraint of FL since participating clients usually suffer from limited bandwidth. To tackle this issue, top-k based gradient compression techniques are broadly explored in FL context, manifesting powerful capabilities in reducing gradient volumes via picking significant entries. However, previous studies are primarily conducted on the raw gradients where massive spatial redundancies exist and positions of non-zero (top-k) entries vary greatly between gradients, which both impede the achievement of deeper compressions. Top-k may also degrade the performance of trained models due to biased gradient estimations. Targeting the above issues, we propose FedTC, a novel transform coding based compression framework. FedTC transforms gradients into a new domain with more compact energy distributions, which facilitates reducing spatial redundancies and biases in subsequent sparsification. Furthermore, non-zero entries across clients from different rounds become highly aligned in the transform domain, motivating us to partition the gradients into smaller entry blocks with various alignment levels to better exploit these alignments. Lastly, positions and values of non-zero entries are independently compressed in a block-wise manner with our customized designs, through which a higher compression ratio is achieved. Theoretical analysis and extensive experiments consistently demonstrate the effectiveness of our approach.}
}


@inproceedings{DBLP:conf/infocom/YanLWXLZ24,
	author = {Jiaming Yan and
                  Jianchun Liu and
                  Shilong Wang and
                  Hongli Xu and
                  Haifeng Liu and
                  Jianhua Zhou},
	title = {Heroes: Lightweight Federated Learning with Neural Composition and
                  Adaptive Local Update in Heterogeneous Edge Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {831--840},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621351},
	doi = {10.1109/INFOCOM52122.2024.10621351},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YanLWXLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables distributed clients to collaboratively train models without exposing their private data. However, it is difficult to implement efficient FL due to limited resources. Most existing works compress the transmitted gradients or prune the global model to reduce the resource cost, but leave the compressed or pruned parameters under-optimized, which degrades the training performance. To address this issue, the neural composition technique constructs size-adjustable models by composing low-rank tensors, allowing every parameter in the global model to learn the knowledge from all clients. Nevertheless, some tensors can only be optimized by a small fraction of clients, thus the global model may get insufficient training, leading to a long completion time, especially in heterogeneous edge scenarios. To this end, we enhance the neural composition technique, enabling all parameters to be fully trained. Further, we propose a lightweight FL framework, called Heroes, with enhanced neural composition and adaptive local update. A greedy-based algorithm is designed to adaptively assign the proper tensors and local update frequencies for participating clients according to their heterogeneous capabilities and resource budgets. Extensive experiments demonstrate that Heroes can reduce traffic consumption by about 72.05% and provide up to 2.97× speedup compared to the baselines.}
}


@inproceedings{DBLP:conf/infocom/WangYWL024,
	author = {Shulin Wang and
                  Qiang Yu and
                  Xiong Wang and
                  Yuqing Li and
                  Hai Jin},
	title = {On Pipelined {GCN} with Communication-Efficient Sampling and Inclusion-Aware
                  Caching},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {841--850},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621386},
	doi = {10.1109/INFOCOM52122.2024.10621386},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangYWL024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph convolutional network (GCN) has achieved enormous success in learning structural information from unstructured data. As graphs become increasingly large, distributed training for GCNs is severely prolonged by frequent cross-worker communications. Existing efforts to improve the training efficiency often come at the expense of GCN performance, while the communication overhead persists. In this paper, we propose PSC-GCN, a holistic pipelined framework for distributed GCN training with communication-efficient sampling and inclusion-aware caching, to address the communication bottleneck while ensuring satisfactory model performance. Specifically, we devise an asynchronous pre-fetching scheme to retrieve stale statistics (features, embedding, gradient) of boundary nodes in advance, such that the embedding aggregation and model update are pipelined with statistics transmission. To alleviate communication volume and staleness effect, we introduce a variance-reduction based sampling policy, which prioritizes inner nodes over boundary ones for reducing the access frequency to remote neighbors, thus mitigating cross-worker statistics exchange. Complementing graph sampling, a feature caching module is co-designed to buffer hot nodes with high inclusion probability, ensuring that frequently sampled nodes will be available in local memory. Extensive evaluations on real-world datasets show the superiority of PSC-GCN over state-of-the-art methods, where we can reduce training time by 72%-80% without sacrificing model accuracy.}
}


@inproceedings{DBLP:conf/infocom/ZuoTL24,
	author = {Tianyu Zuo and
                  Xueyan Tang and
                  Bu{-}Sung Lee},
	title = {A Randomized Caching Algorithm for Distributed Data Access},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {851--860},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621329},
	doi = {10.1109/INFOCOM52122.2024.10621329},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZuoTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study an online cost optimization problem for distributed data access. The goal of this problem is to dynamically create and delete data copies in a multi-server distributed system as time goes, in order to minimize the total storage and network cost of serving access requests. We propose an online algorithm with randomized storage periods of data copies in the servers, and derive an optimal probability density function of storage periods, which makes the algorithm achieve a competitive ratio of 1 + \\frac{{\\sqrt 2 }}{2}\n. An example is presented to show that the competitive analysis of our algorithm is tight. Experimental evaluations using real data access traces demonstrate that our algorithm outperforms the best known deterministic algorithm.}
}


@inproceedings{DBLP:conf/infocom/Xiao0RW024,
	author = {Hengying Xiao and
                  Jingwei Li and
                  Yanjing Ren and
                  Ruijin Wang and
                  Xiaosong Zhang},
	title = {CDCache: Space-Efficient Flash Caching via Compression-before-Deduplication},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {861--870},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621089},
	doi = {10.1109/INFOCOM52122.2024.10621089},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Xiao0RW024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale storage systems boost I/O performance via flash caching, but the underlying storage medium of flash caching incurs significant financial costs and also exhibits low endurance. Previous studies adopt compression-after-deduplication to mitigate writing redundant contents into the flash cache, so as to address the cost and endurance issues. However, deduplication and compression have conflicting preferable cases, and compression-after-deduplication essentially compromises the space-saving benefits of either deduplication or compression. To simultaneously preserve the benefits of both approaches, we explore compression-before-deduplication, which applies compression to eliminate byte-level redundancies across data blocks, followed by deduplication to write only a single copy of duplicate compressed blocks into the flash cache. We present CDCache, a space-efficient flash caching system that realizes compression-before-deduplication. It proposes to dynamically adjust the compression range of data blocks, so as to preserve the effectiveness of deduplication on the compressed blocks. Also, it builds on various design techniques to approximately estimate duplicate data blocks and efficiently manage compressed blocks. Trace-driven experiments show that CDCache improves the read hit ratio and the write reduction ratio of a previous compression-after-deduplication approach by up to 1.3× and 1.6×, respectively, while it only has small memory overhead for index management.}
}


@inproceedings{DBLP:conf/infocom/DallotFP024,
	author = {Julien Dallot and
                  Amirmehdi Jafari Fesharaki and
                  Maciej Pacut and
                  Stefan Schmid},
	title = {Dependency-Aware Online Caching},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {871--880},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621422},
	doi = {10.1109/INFOCOM52122.2024.10621422},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DallotFP024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a variant of the online caching problem where the items exhibit dependencies among each other: an item can reside in the cache only if all its dependent items are also in the cache. The dependency relations can form any directed acyclic graph. These requirements arise in systems such as CacheFlow (SOSR 2016) that cache forwarding rules for packet classification in IP-based communication networks.First, we present an optimal randomized online caching algorithm which accounts for dependencies among the items. Our randomized algorithm is O(log k)-competitive, where k is the size of the cache, meaning that our algorithm never incurs the cost of O(log k) times higher than even an optimal algorithm that knows the future input sequence.Second, we consider the bypassing model, where requests can be served at a fixed price without fetching the item and its dependencies into the cache — a variant of caching with dependencies introduced by Bienkowski et al. at SPAA 2017. For this setting, we give an O\\left( {\\sqrt {k \\cdot \\log k} } \\right)\n-competitive algorithm, which significantly improves the best known competitiveness. We conduct a small case study, to find out that our algorithm incurs on average 2x lower cost.}
}


@inproceedings{DBLP:conf/infocom/LiuLWLZLLL24,
	author = {Weisen Liu and
                  Zeqi Lai and
                  Qian Wu and
                  Hewu Li and
                  Qi Zhang and
                  Zonglun Li and
                  Yuanjie Li and
                  Jun Liu},
	title = {In-Orbit Processing or Not? Sunlight-Aware Task Scheduling for Energy-Efficient
                  Space Edge Computing Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {881--890},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621268},
	doi = {10.1109/INFOCOM52122.2024.10621268},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuLWLZLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid evolution of space-borne capabilities, space edge computing (SEC) is becoming a new computation paradigm for future integrated space and terrestrial networks. Satellite edges adopt advanced on-board hardware, which not only enables new opportunities to perform complex intelligent tasks in orbit, but also involves new challenges due to the additional energy consumption in power-constrained space environment.In this paper, we present Phoenix, an energy-efficient task scheduling framework for emerging SEC networks. Phoenix exploits a key insight that in the SEC network, there always exist a number of sunlit edges which are illuminated during the entire orbital period and have sufficient energy supplement from the sun. Phoenix accomplishes energy-efficient in-orbit computing by judiciously offloading space tasks to "sunlight-sufficient" edges or to the ground. Specifically, Phoenix first formulates the SEC battery energy optimizing (SBEO) problem which aims at minimizing the average battery energy consumption while satisfying various task completion constraints. Then Phoenix incorporates a sunlight-aware scheduling mechanism to solve the SBEO problem and schedule SEC tasks efficiently. Finally, we implement a Phoenix prototype and build an SEC testbed. Extensive data-driven evaluations demonstrate that as compared to other state-of-the-art solutions, Phoenix can effectively reduce up to 54.8% SEC battery energy consumption and prolong battery lifetime to 2.9× while still completing tasks on time.}
}


@inproceedings{DBLP:conf/infocom/MaxentiDBPCM24,
	author = {Stefano Maxenti and
                  Salvatore D'Oro and
                  Leonardo Bonati and
                  Michele Polese and
                  Antonio Capone and
                  Tommaso Melodia},
	title = {ScalO-RAN: Energy-aware Network Intelligence Scaling in Open {RAN}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {891--900},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621159},
	doi = {10.1109/INFOCOM52122.2024.10621159},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MaxentiDBPCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network virtualization, software-defined infrastructure, and orchestration are pivotal elements in contemporary networks, yielding new vectors for optimization and novel capabilities. In line with these principles, O-RAN presents an avenue to bypass vendor lock-in, circumvent vertical configurations, enable network programmability, and facilitate integrated artificial intelligence (AI) support. Moreover, modern container orchestration frameworks (e.g., Kubernetes, Red Hat OpenShift) simplify the way cellular base stations, as well as the newly introduced RAN Intelligent Controllers (RICs), are deployed, managed, and orchestrated. While this enables cost reduction via infrastructure sharing, it also makes it more challenging to meet O-RAN control latency requirements, especially during peak resource utilization. For instance, the Near-real-time RIC is in charge of executing applications (xApps) that must take control decisions within one second, and we show that container platforms available today fail in guaranteeing such timing constraints. To address this problem, we propose ScalO-RAN, a control framework rooted in optimization and designed as an O-RAN rApp that allocates and scales AI-based O-RAN applications (xApps, rApps, dApps) to: (i) abide by application-specific latency requirements, and (ii) monetize the shared infrastructure while reducing energy consumption. We prototype ScalO-RAN on an OpenShift cluster with base stations, RIC, and a set of AI-based xApps deployed as micro-services. We evaluate ScalO-RAN both numerically and experimentally. Our results show that ScalO-RAN can optimally allocate and distribute O-RAN applications within available computing nodes to accommodate even stringent latency requirements. More importantly, we show that scaling O-RAN applications is primarily a time-constrained problem rather than a resource-constrained one, where scaling policies must account for stringent inference time of AI applications, and not only how many resources they consume.}
}


@inproceedings{DBLP:conf/infocom/LinS024,
	author = {Qiulin Lin and
                  Junyan Su and
                  Minghua Chen},
	title = {Competitive Online Age-of-Information Optimization for Energy Harvesting
                  Systems},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {901--910},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621320},
	doi = {10.1109/INFOCOM52122.2024.10621320},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LinS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the scenario where an energy harvesting source sends its updates to a receiver. The source optimizes its energy allocation over a decision period to maximize a sum of time-varying functions of the age of information (AoI), representing the value of providing timely information. In a practical online setting, we need to make irrevocable energy allocation decisions at each time while the time-varying functions and the energy arrivals are only revealed sequentially. The problem is then challenging as 1) we are facing uncertain energy harvesting arrivals and time-varying functions, and 2) the energy allocation decisions and the energy harvesting process are coupled due to the capacity-limited battery. In this paper, we develop an optimal online algorithm CR-Reserve and show it achieves (lnθ + 1)-competitive, where θ is a parameter representing the level of uncertainty of the time-varying functions. It is the optimal competitive ratio among all deterministic and randomized online algorithms. We conduct simulations based on real-world traces and compare our algorithms with conceivable alternatives. The results show that our algorithms achieve 12% performance improvement as compared to the state-of-the-art baseline.}
}


@inproceedings{DBLP:conf/infocom/Ayala-RomeroSGC24,
	author = {Jose A. Ayala{-}Romero and
                  Leonardo Lo Schiavo and
                  Andres Garcia{-}Saavedra and
                  Xavier Costa{-}P{\'{e}}rez},
	title = {Mean-Field Multi-Agent Contextual Bandit for Energy-Efficient Resource
                  Allocation in vRANs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {911--920},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621197},
	doi = {10.1109/INFOCOM52122.2024.10621197},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Ayala-RomeroSGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio Access Network (RAN) virtualization, key for new-generation mobile networks, requires Hardware Accelerators (HAs) that swiftly process wireless signals from Base Stations (BSs) to meet stringent reliability targets. However, HAs are expensive and energy-hungry, which increases costs and has serious environmental implications. To address this problem, we gather data from our experimental platform and compare the performance and energy consumption of a HA (NVIDIA GPU V100) vs. a CPU (Intel Xeon Gold 6240R, 16 cores) for energy-friendly software processing. Based on the insights obtained from this data, we devise a strategy to offload workloads to HAs opportunistically to save energy while preserving reliability. This offloading strategy, however, needs to be configured in near-real-time for every BS sharing common computational resources. This renders a challenging multi-agent collaborative problem in which the number of involved agents (BSs) can be arbitrarily large and can change over time. Thus, we propose an efficient multi-agent contextual bandit algorithm called ECORAN 1 , which applies concepts from mean field theory to be fully scalable. Using a real platform and traces from a production mobile network, we show that ECORAN can provide up to 40% energy savings with respect to the approach used today by the industry.}
}


@inproceedings{DBLP:conf/infocom/ZechariaS24,
	author = {Rami Zecharia and
                  Yuval Shavitt},
	title = {A Parallel Algorithm and Scalable Architecture for Routing in Bene{\v{s}}
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {921--930},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621315},
	doi = {10.1109/INFOCOM52122.2024.10621315},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZechariaS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Beneš/CLOS architectures are common scalable interconnection networks widely used in backbone routers, data centers, on-chip networks, multi-processor systems, and parallel computers. Recent advances in Silicon Photonic technology, especially MZI technology, have made Beneš networks a very attractive scalable architecture for optical circuit switches.Numerous routing algorithms for Beneš networks were developed starting with linear algorithms having time complexity of O(N log 2 N) steps. Parallel routing algorithms were developed to satisfy the stringent timing requirements of high-performance switching networks and have time complexity of O((log 2 N) 2 ).However, their implementation requires O(N 2 log 2 N) wires (termed connectivity complexity), and thus are difficult to scale.We present a new routing algorithm for Beneš networks combined with a scalable hardware architecture that supports full and partial input permutations. The processing time of the algorithm is limited to O((log 2 N) 2 ) steps (iterations) by potentially forfeiting routing of a few input demands; however achieves close to 100% utilization for both full and partial input permutations. The algorithm and architecture allow a reduction of the connectivity complexity to O(N 2 ), a logN improvement over previous solutions.We prove the algorithm correctness, and analyze its performance analytically and with large scale simulations.}
}


@inproceedings{DBLP:conf/infocom/YaoYD24,
	author = {Yibei Yao and
                  Tong Ye and
                  Ning Deng},
	title = {Nonblocking Conditions for Flex-grid OXC-Clos Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {931--940},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621334},
	doi = {10.1109/INFOCOM52122.2024.10621334},
	timestamp = {Fri, 06 Sep 2024 16:14:02 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YaoYD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging high-capacity optical networks makes it urgent to design large-scale flexible mesh optical cross-connects (OXCs). Though Clos network is the theory for building scalable and cost-effective switching fabrics, the nonblocking conditions of flex-grid optical Clos networks without wavelength conversion remain unknown. This paper studies the nonblocking conditions for the flex-grid OXC-Clos network, which is constructed from a number of small-size standard OXCs. We first show that a strictly nonblocking (SNB) OXC-Clos network will incur a high cost, as small-granularity lightpaths may abuse central modules, rendering them unavailable for large-granularity requests due to frequency conflicts. We thus propose a granularity differential routing (GDR) strategy, the idea of which is to restrict the set of CMs that can be used by the lightpaths of each granularity. Under the GDR strategy, we investigate two system models, granularity-port binding and unbinding models, and prove the wide-sense nonblocking (WSNB) conditions for OXC-Clos network. We show that the cost of WSNB network is remarkably smaller than that of SNB network, and find that the second model can lead to more flexible network-bandwidth utilization than the first model only at a small cost of switching fabrics.}
}


@inproceedings{DBLP:conf/infocom/YangC024,
	author = {Pu Yang and
                  Tianfang Chang and
                  Lin Cai},
	title = {{DDR:} {A} Deadline-Driven Routing Protocol for Delay Guaranteed Service},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {941--950},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621415},
	doi = {10.1109/INFOCOM52122.2024.10621415},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangC024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-sensitive applications have become increasingly prevalent in modern networks, necessitating the development of Delay-Guaranteed Routing (DGR) solutions. Finding an optimal DGR solution remains a challenging task due to the NP-hard nature of the problem and the dynamic nature of network traffic. In this paper, we propose Deadline-Driven Routing (DDR), a distributed traffic-aware adaptive routing protocol that addresses the DGR problem. Inspired by online navigation techniques, DDR leverages real-time traffic conditions to optimize routing decisions and ensure on-time packet delivery. By combining network topology-based path generation with real-time traffic knowledge, each router can adjust packet forwarding directions to meet its heterogeneous latency requirements. Comprehensive simulations on real-world network topologies demonstrate that DDR can consistently provide delay-guaranteed service in different network topologies with varying traffic conditions. In addition, DDR ensures backward compatibility with legacy devices and existing routing protocols, making it a viable solution for supporting delay-guaranteed service.}
}


@inproceedings{DBLP:conf/infocom/Berczi-KovacsGV24,
	author = {Erika R. B{\'{e}}rczi{-}Kov{\'{a}}cs and
                  P{\'{e}}ter Gyimesi and
                  Bal{\'{a}}zs Vass and
                  J{\'{a}}nos Tapolcai},
	title = {Efficient Algorithm for Region-Disjoint Survivable Routing in Backbone
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {951--960},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621256},
	doi = {10.1109/INFOCOM52122.2024.10621256},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Berczi-KovacsGV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Survivable routing is crucial in backbone networks to ensure connectivity, even during failures. At network design, groups of network elements prone to potential failure events are identified. These groups are referred to as Shared Risk Link Groups (SRLGs), and if they are a set of links intersected by a connected region of the plane, we call them regional-SRLGs. A recent study has presented a polynomial-time algorithm for finding a maximum number of regional-SRLG-disjoint paths between two given nodes in a planar topology, with the paths being nodedisjoint. However, existing algorithms for this problem are not practical due to their runtime and implementation complexities.This paper investigates a more general model, the maximum number of non-crossing, regional-SRLG-disjoint paths problem. It introduces an efficient and easily implementable algorithmic framework, leveraging an arbitrarily chosen shortest path finding subroutine for graphs with possibly negative weights. Depending on the subroutine chosen, the framework improves the previous worst-case runtime complexity, or can solve the problem w.h.p. in near-linear expected time. The proposed framework enables the first additive approximation for a more general\nNP\n-hard version of the problem, where the objective is to find the maximum number of regional-SRLG-disjoint paths. We validate our findings through extensive simulations.}
}


@inproceedings{DBLP:conf/infocom/Lyu0GZS024,
	author = {Mengxia Lyu and
                  Hao Zhou and
                  Kaiwen Guo and
                  Wangqiu Zhou and
                  Xingfa Shen and
                  Yu Gu},
	title = {MultiHGR: Multi-Task Hand Gesture Recognition with Cross-Modal Wrist-Worn
                  Devices},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {961--970},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621430},
	doi = {10.1109/INFOCOM52122.2024.10621430},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Lyu0GZS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hand gesture recognition (HGR) is essential for human-machine interaction. Although the existing solutions achieve good performance in specific tasks, they still face challenges when users navigate through different application contexts, i.e., demanding multi-task ability to support newly arrived HGR tasks. In this paper, we propose the first IMU-vision based system hosted on wrist-worn devices to support multi-task HGR, denoted as MultiHGR. The system introduces a novel two-stage training strategy, i.e., task-agnostic stage to align cross-modal features from unlabeled arbitrary gesture through contrastive learning, and task-related stage to learn modality contributions with limited labeled data in specific tasks through self-attention mechanism. Since only the second task-related stage should be executed for each new task, MultiHGR could accommodate multiple tasks with significant reduced training cost and storage requirement. The evaluation results on three HGR tasks demonstrates that MultiHGR reduces 64.92% training time, and 24.04% storage as compared with traditional multimodal single-task models, and MultiHGR outperforms unimodal single-task models with 14.37%, 19.28%, and 31% improvements in these three tasks, respectively. As compared with state-of-the-art multimodal single-task model, MultiHGR achieves average 6.35% accuracy improvement, along with 65.74% training time reduction.}
}


@inproceedings{DBLP:conf/infocom/0018Z24,
	author = {Qiang Yang and
                  Yuanqing Zheng},
	title = {Neural Enhanced Underwater {SOS} Detection},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {971--980},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621343},
	doi = {10.1109/INFOCOM52122.2024.10621343},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0018Z24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Every day, one person loses his life due to drowning in swimming pools, even with professional lifeguards present. Contrary to what the public might assume, drowning swimmers can hardly splash or yell for help. This life-threatening situation calls for a robust SOS channel between the swimmers and the lifeguards. This paper proposes Neusos, a neural-enhanced underwater SOS communication system based on commercial wearable devices and low-cost hydrophones deployed in the swimming pool. Specifically, we repurpose popular wearable devices (e.g., smartwatches) as SOS transmitters, which can send a distress signal when the user is in an emergency. In response, an underwater hydrophone in the swimming pool can detect SOS signals and make alerts immediately to facilitate a timely rescue. The main technical challenge lies in reliably detecting weak SOS signals in non-stationary underwater scenarios. To achieve so, we thoroughly characterize the properties of underwater channels and examine the limitations of the traditional correlation-based signal detection method in underwater communication scenarios. Based on our empirical findings, we developed a robust SOS detection method enhanced with deep learning. By fully embedding signal characteristics into networks, Neusos outperforms traditional signal processing-based underwater SOS detection methods. In particular, our experiments in a real swimming pool show that Neusos can detect SOS signals with a detection rate of 98.2% under various underwater conditions. Given the increasing popularity of smartwatches among swimmers, our system holds immense potential to enhance their safety in swimming pools.}
}


@inproceedings{DBLP:conf/infocom/LiW24,
	author = {Mengning Li and
                  Wenye Wang},
	title = {Hybrid Zone: Bridging Acoustic and Wi-Fi for Enhanced Gesture Recognition},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {981--990},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621132},
	doi = {10.1109/INFOCOM52122.2024.10621132},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gesture recognition has significant applications such as assisted living, e-health, and human-device interactions. Moving away from conventional computer vision techniques, recent studies are turning towards ubiquitous methods such as Wi-Fi and acoustic signals, which offer device deployment at minimal costs. In this paper, we explore these two ubiquitous techniques to enhance gesture recognition, addressing challenges related to multi-modal fusion. Due to the inherent differences in signal types, we employ a tailored method to harmonize information from these distinct signals. Traditional multi-modal fusion methods often lack theoretical models due to insufficient analysis of the foundational characteristics of different signals. In particular, we propose a concept, Hybrid Zone, which is a theoretical model illustrating the fusion of acoustic and Wi-Fi sensing. Hybrid zone offers a comprehensive perspective on the fusion of acoustic and Wi-Fi sensing areas. Moreover, it provides an intricate view of the synthesis of acoustic and Wi-Fi velocities at a granular level. Our experimental results have been promising, achieving a high accuracy rate of 93.75% in gesture recognition.}
}


@inproceedings{DBLP:conf/infocom/ZhaoLXXZZW24,
	author = {Zhiyuan Zhao and
                  Fan Li and
                  Yadong Xie and
                  Huanran Xie and
                  Kerui Zhang and
                  Li Zhang and
                  Yu Wang},
	title = {HearBP: Hear Your Blood Pressure via In-ear Acoustic Sensing Based
                  on Heart Sounds},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {991--1000},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621249},
	doi = {10.1109/INFOCOM52122.2024.10621249},
	timestamp = {Thu, 22 Aug 2024 07:32:50 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoLXXZZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continuous blood pressure (BP) monitoring using wearable devices has received increasing attention due to its importance in diagnosing diseases. However, existing methods mainly measure BP intermittently, involve some form of user effort, and suffer from insufficient accuracy due to sensor properties. In order to overcome these limitations, we study the BP measurement technology based on heart sounds, and find that the time interval between the first and second heart sounds (TIFS) of bone-conducted heart sounds collected in the binaural canal is closely related to BP. Motivated by this, we propose HearBP, a novel BP monitoring system that utilizes inear microphones to collect bone-conducted heart sounds in the binaural canal. We first design a noise removing method based on U-net autoencoder-decoder to separate clean heart sounds from background noises. Then, we design a feature extraction method based on shannon energy and energy-entropy ratio to further mine the time domain and frequency domain features of heart sounds. In addition, combined with the principal component analysis algorithm, we achieve feature dimension reduction to extract the main features related to BP. Finally, we propose a network model based on dendritic neural regression to construct a mapping between the extracted features and BP. Extensive experiments with 41 participants show the average estimation error of 0.97mmHg and 1.61mmHg and the standard deviation error of 3.13mmHg and 3.56mmHg for diastolic pressure and systolic pressure, respectively. These errors are within the acceptable range specified by the FDA’s AAMI protocol.}
}


@inproceedings{DBLP:conf/infocom/YeDZO0L024,
	author = {Shengyuan Ye and
                  Jiangsu Du and
                  Liekang Zeng and
                  Wenzhong Ou and
                  Xiaowen Chu and
                  Yutong Lu and
                  Xu Chen},
	title = {Galaxy: {A} Resource-Efficient Collaborative Edge {AI} System for
                  In-situ Transformer Inference},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1001--1010},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621342},
	doi = {10.1109/INFOCOM52122.2024.10621342},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YeDZO0L024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transformer-based models have unlocked a plethora of powerful intelligent applications at the edge, such as voice assistant in smart home. Traditional deployment approaches offload the inference workloads to the remote cloud server, which would induce substantial pressure on the backbone network as well as raise users’ privacy concerns. To address that, in-situ inference has been recently recognized for edge intelligence, but it still confronts significant challenges stemming from the conflict between intensive workloads and limited on-device computing resources. In this paper, we leverage our observation that many edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources and propose Galaxy, a collaborative edge AI system that breaks the resource walls across heterogeneous edge devices for efficient Transformer inference acceleration. Galaxy introduces a novel hybrid model parallelism to orchestrate collaborative inference, along with a heterogeneity-aware parallelism planning for fully exploiting the resource potential. Furthermore, Galaxy devises a tile-based fine-grained overlapping of communication and computation to mitigate the impact of tensor synchronizations on inference latency under bandwidth-constrained edge environments. Extensive evaluation based on prototype implementation demonstrates that Galaxy remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving up to 2.5× end-to-end latency reduction.}
}


@inproceedings{DBLP:conf/infocom/LiuD0XY24,
	author = {Yuhuan Liu and
                  Yulong Ding and
                  Jie Jiang and
                  Bin Xiao and
                  Shuang{-}Hua Yang},
	title = {Industrial Control Protocol Type Inference Using Transformer and Rule-based
                  Re-Clustering},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1011--1020},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621186},
	doi = {10.1109/INFOCOM52122.2024.10621186},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuD0XY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The development of the Industrial Internet of Things (IIoT) is impeded by the lack of unknown protocol specifications. Protocol Reverse Engineering (PRE) plays a crucial role in inferring unpublished protocol specifications by analyzing traffic messages. Since different types within a protocol often have distinct formats, inferring the protocol type is essential for subsequent reverse analysis. Natural Language Processing (NLP) models have demonstrated remarkable capabilities in various sequence tasks, and traffic messages of unknown protocols can be analyzed as sequences. In this paper, we propose a framework for clustering unknown industrial control protocol types. Our framework utilizes a transformer-based auto-encoder network to train corresponding request and response messages, leveraging intermediate layer embedding vectors learned by the network for clustering. The clustering results are employed to extract candidate keywords and establish empirical rules. Subsequently, rule-based re-clustering is performed, and its effectiveness is evaluated based on previous clustering results. Through this re-clustering process, we identify the most effective combination of keywords that define the type. We evaluate the proposed framework using three general protocols that have different type rules and successfully separate the protocol internal types completely.}
}


@inproceedings{DBLP:conf/infocom/Chen0H0WZZ24,
	author = {Jinyu Chen and
                  Wenchao Xu and
                  Zicong Hong and
                  Song Guo and
                  Haozhao Wang and
                  Jie Zhang and
                  Deze Zeng},
	title = {{OTAS:} An Elastic Transformer Serving System via Token Adaptation},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1021--1030},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621087},
	doi = {10.1109/INFOCOM52122.2024.10621087},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Chen0H0WZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transformer model empowered architectures have become a pillar of cloud services that keeps reshaping our society. However, the dynamic query loads and heterogeneous user requirements severely challenge current transformer serving systems, which rely on pre-training multiple variants of a foundation model, i.e., with different sizes, to accommodate varying service demands. Unfortunately, such a mechanism is unsuitable for large transformer models due to the additional training costs and excessive I/O delay. In this paper, we introduce OTAS, the first elastic serving system specially tailored for transformer models by exploring lightweight token management. We develop a novel idea called token adaptation that adds prompting tokens to improve accuracy and removes redundant tokens to accelerate inference. To cope with fluctuating query loads and diverse user requests, we enhance OTAS with application-aware selective batching and online token adaptation. OTAS first batches incoming queries with similar service-level objectives to improve the ingress throughput. Then, to strike a tradeoff between the overhead of token increment and the potentials for accuracy improvement, OTAS adaptively adjusts the token execution strategy by solving an optimization problem. We implement and evaluate a prototype of OTAS with multiple datasets, which show that OTAS improves the system utility by at least 18.2%.}
}


@inproceedings{DBLP:conf/infocom/BelgiovineGSTTI24,
	author = {Mauro Belgiovine and
                  Joshua Groen and
                  Miquel Sirera and
                  Chinenye Tassie and
                  Sage Trudeau and
                  Stratis Ioannidis and
                  Kaushik R. Chowdhury},
	title = {{T-PRIME:} Transformer-based Protocol Identification for Machine-learning
                  at the Edge},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1031--1040},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621284},
	doi = {10.1109/INFOCOM52122.2024.10621284},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/BelgiovineGSTTI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectrum sharing allows different protocols of the same standard (e.g., 802.11 family) or different standards (e.g., LTE and DVB) to coexist in overlapping frequency bands. As this paradigm continues to spread, wireless systems must also evolve to identify active transmitters and unauthorized waveforms in real time under intentional distortion of preambles, extremely low signal-to-noise ratios and challenging channel conditions. We overcome limitations of correlation-based preamble matching methods in such conditions through the design of T-PRIME: a Transformer-based machine learning approach. T-PRIME learns the structural design of transmitted frames through its attention mechanism, looking at sequence patterns that go beyond the preamble alone. The paper makes three contributions: First, it compares Transformer models and demonstrates their superiority over traditional methods and state-of-the-art neural networks. Second, it rigorously analyzes T-PRIME’s real-time feasibility on DeepWave’s AIR-T platform. Third, it utilizes an extensive 66 GB dataset of over-the-air (OTA) WiFi transmissions for training, which is released along with the code for community use. Results reveal nearly perfect (i.e. > 98%) classification accuracy under simulated scenarios, showing 100% detection improvement over legacy methods in low SNR ranges, 97% classification accuracy for OTA single-protocol transmissions and up to 75% double-protocol classification accuracy in interference scenarios.}
}


@inproceedings{DBLP:conf/infocom/ChangM24,
	author = {Hyunseok Chang and
                  Sarit Mukherjee},
	title = {Zeta: Transparent Zero-Trust Security Add-on for {RDMA}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1041--1050},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621259},
	doi = {10.1109/INFOCOM52122.2024.10621259},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChangM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While the fast adoption of RDMA in data centers has been primarily driven by its performance benefits, more and more attention is being paid to its security implication, especially with mounting security risks associated with lateral communication within data centers. However, since RDMA is implemented as NIC’s fixed function, it is challenging to incorporate any new security feature in RDMA. In this paper, we propose Zeta, a zero-trust security addon for RoCEv2, which enables network-independent, fine-grained zero-trust security control for RDMA. It does not require any change in RDMA’s ASIC implementation or application-level interfaces. To this end, Zeta leverages modern SmartNIC’s versatility to perform zero-trust policy control on RDMA packets within a SmartNIC in a cryptographically secure fashion. From its prototype implementation and evaluation based on real-word applications, we show that, while cryptographic verification of Zeta introduces 1.5 ms session startup latency, the overhead of end-to-end application performance is marginal (e.g., less than 1% throughput and 5% latency penalty).}
}


@inproceedings{DBLP:conf/infocom/Li0YDL24,
	author = {Yulong Li and
                  Wenxin Li and
                  Yinan Yao and
                  Yuxuan Du and
                  Keqiu Li},
	title = {Host-driven In-Network Aggregation on {RDMA}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1051--1060},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621230},
	doi = {10.1109/INFOCOM52122.2024.10621230},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Li0YDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale datacenter networks are increasingly using in-network aggregation (INA) and remote direct memory access (RDMA) techniques to accelerate deep neural network (DNN) training. However, existing research trends suggest that these two techniques are on an inevitable collision course. To fill this gap, we present FreeINA, a host-driven in-network aggregation aimed at providing RDMA reliable connection (RC) for multi-tenant learning settings. FreeINA relies on dual transmission paths to support RC compatibility, with one path for INA and another one for aggregation on end-host parameter server. With dynamic control of these two paths, FreeINA can leave the traditional in-server aggregation unaffected while ensuring INA’s reliability without modifying RDMA network interfaces (RNICs). To support multi-tenant learning, FreeINA employs all-reduce-level memory allocation, which can capture the well-known "on and off" DNN training pattern and thus improve switch memory efficiency. We have implemented a FreeINA prototype using P4-programmable switch and commercial RNICs, and evaluated it extensively using 100Gbps testbed. The results show that compared to the state-of-the-art solution—ATP, FreeINA improves single-job training speedup ratio by 1.20×, while improving the aggregation throughput by 2.65× in multi-job scenario.}
}


@inproceedings{DBLP:conf/infocom/ChangHMW24,
	author = {Hyunseok Chang and
                  Walid A. Hanafy and
                  Sarit Mukherjee and
                  Limin Wang},
	title = {{INSERT:} In-Network Stateful End-to-End {RDMA} Telemetry},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1061--1070},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621203},
	doi = {10.1109/INFOCOM52122.2024.10621203},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChangHMW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Remote Direct Memory Access (RDMA) has been widely adopted in modern data centers thanks to its high-throughput, low-latency data transfer capability and reduced CPU overhead. However, traditional network-flow-based monitoring is poor at interpreting RDMA communication and hence inadequate for gaining insights. In this paper, we present INSERT, an end-to-end RDMA telemetry system that enables seamless visibility into RDMA communication from the network layer all the way to the application layer. To this end, INSERT combines (i) eBPF-based transparent RDMA tracing on end-hosts and (ii) stateful RDMA network telemetry on programmable data plane. We implement RDMA network telemetry on programmable SmartNICs, where we address practical challenges for maintaining fine-grained state on massively-parallel packet processing pipelines. We demonstrate that INSERT can perform reasonably accurate telemetry at line-rate for different types of RDMA traffic even in the presence of out-of-order packets, and finally showcase two practical use cases that can benefit from it.}
}


@inproceedings{DBLP:conf/infocom/0001TWZ0Y024,
	author = {Haifeng Sun and
                  Yixuan Tan and
                  Yongtong Wu and
                  Jiaqi Zhu and
                  Qun Huang and
                  Xin Yao and
                  Gong Zhang},
	title = {RB\({}^{\mbox{2}}\): Narrow the Gap between {RDMA} Abstraction and
                  Performance via a Middle Layer},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1071--1080},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621169},
	doi = {10.1109/INFOCOM52122.2024.10621169},
	timestamp = {Fri, 30 Aug 2024 11:50:16 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001TWZ0Y024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although the native RDMA interface allows for high throughput and low latency, its low-level abstraction raises significant programming challenges. Consequently, numerous systems encapsulate the RDMA interface into more user-friendly high-level abstractions such as Socket, MPI, and RPC. However, this ease of development often incurs considerable performance degradation. To address this trade-off, this paper introduces RB 2 , a high-performance RDMA-based Distributed Ring Buffer (DRB). RB 2 serves as a middle layer that effectively conceals the low-level details of the RDMA interface while also facilitating extension to other high-level abstractions.Nonetheless, it is non-trivial for DRBs to preserve the RDMA performance. We optimize the performance of RB 2 in three aspects. First, we perform micro-benchmarks to identify the pointer synchronization methods that are seemingly counter-intuitive but offer optimal performance improvements. Second, we propose an adaptive batching mechanism to alleviate the limitations of conventional fixed batching. Finally, we build an efficient memory subsystem using various optimization techniques. RB 2 outperforms SOTA designs by achieving 2.5 × to 7.5 × throughput while maintaining comparable tail latency for small messages.}
}


@inproceedings{DBLP:conf/infocom/HouW24,
	author = {Boxin Hou and
                  Jiliang Wang},
	title = {LoBaCa: Super-Resolution LoRa Backscatter Localization for Low-Cost
                  Tags},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1081--1090},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621170},
	doi = {10.1109/INFOCOM52122.2024.10621170},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HouW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Long-range(LoRa) backscatter has shown great potential in many applications. However, the narrow bandwidth of LoRa and the unstable backscatter tag make localization challenging in practice. This paper presents LoBaCa, the first super-resolution LoRa backscatter localization system for low-cost tags. To increase the overall bandwidth, LoBaCa utilizes the frequency hopping technique and exploits the phase slope to synchronize multiple frequency bands. We further show that the unstable low-cost backscatter tag causes additional phase error in the weak backscatter signal and thus introduces significant localization error. We use the upper and lower sideband signals to improve the SNR and correct the phase error. Finally, LoBaCa adopts a super-resolution ESPRIT algorithm to solve the complex multipath effect, estimate the angle of arrival (AoA), and localize the backscatter tag. We prototype LoBaCa and conduct extensive experiments to evaluate LoBaCa in both indoor and outdoor scenarios. Our results show that the localization error of LoBaCa is 5.0 cm and 71 cm when the LoBaCa tag is 5m and 40m away, which is 4.3× and 1.7× better than the state-of-the-art.}
}


@inproceedings{DBLP:conf/infocom/LinXXTCDL24,
	author = {Jingkai Lin and
                  Runqun Xiong and
                  Zhuqing Xu and
                  Wei Tian and
                  Ciyuan Chen and
                  Xirui Dong and
                  Junzhou Luo},
	title = {Multi-Node Concurrent Localization in LoRa Networks: Optimizing Accuracy
                  and Efficiency},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1091--1100},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621435},
	doi = {10.1109/INFOCOM52122.2024.10621435},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LinXXTCDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa Localization, a fundamental service in LoRa networks, has garnered significant attention due to its long-range capabilities and low power consumption. However, existing approaches for LoRa localization are either incompatible with commercial devices or highly susceptible to environmental factors. To tackle this challenge, we propose SyncLoc, a TDoA-based LoRa localization framework that integrates a dedicated node for multi-dimensional time-drift correction. Our proposal is built on two key observations: firstly, the nanosecond-level measurement of time differences between gateways, and secondly, the substantial impact of SNR on gateway time drift. To accomplish our objective, we present three progressively enhanced versions of SyncLoc, each intended to comprehensively analyze the factors influencing LoRa time synchronization accuracy across different deployment scenarios involving nodes, carrier frequencies, and spreading factors. In addition to improving accuracy, we identify inefficiencies in LoRa’s multi-node concurrent localization, and introduce SyncLoc-4, a multi-node localization scheduling mechanism that optimizes efficiency with a 2-approximation ratio. Extensive experiments utilizing commercial LoRa devices in real-world demonstrates a 2.44× improvement in accuracy. Furthermore, simulations of large-scale networks exhibit a 2.47× boost in localization scalability (i.e., the number of concurrently located nodes) when employing SyncLoc instead of LoRaWAN.}
}


@inproceedings{DBLP:conf/infocom/WangXZLCC0LC24,
	author = {Haoyang Wang and
                  Jingao Xu and
                  Chenyu Zhao and
                  Zihong Lu and
                  Yuhan Cheng and
                  Xuecheng Chen and
                  Xiao{-}Ping Zhang and
                  Yunhao Liu and
                  Xinlei Chen},
	title = {TransformLoc: Transforming MAVs into Mobile Localization Infrastructures
                  in Heterogeneous Swarms},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1101--1110},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621375},
	doi = {10.1109/INFOCOM52122.2024.10621375},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangXZLCC0LC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A heterogeneous micro aerial vehicles (MAV) swarm consists of resource-intensive but expensive advanced MAVs (AMAVs) and resource-limited but cost-effective basic MAVs (BMAVs), offering opportunities in diverse fields. Accurate and real-time localization is crucial for MAV swarms, but current practices lack a low-cost, high-precision, and real-time solution, especially for lightweight BMAVs. We find an opportunity to accomplish the task by transforming AMAVs into mobile localization infrastructures for BMAVs. However, turning this insight into a practical system is non-trivial due to challenges in location estimation with BMAVs’ unknown and diverse localization errors and resource allocation of AMAVs given coupled influential factors. This study proposes TransformLoc, a new framework that transforms AMAVs into mobile localization infrastructures, specifically designed for low-cost and resource- constrained BMAVs. We first design an error-aware joint location estimation model to perform intermittent joint location estimation for BMAVs and then design a proximity-driven adaptive grouping-scheduling strategy to allocate resources of AMAVs dynamically. TransformLoc achieves a collaborative, adaptive, and cost-effective localization system suitable for large-scale heterogeneous MAV swarms. We implement TransformLoc on industrial drones and validate its performance. Results show that TransformLoc outperforms baselines including SOTA up to 68% in localization performance, motivating up to 60% navigation success rate improvement.}
}


@inproceedings{DBLP:conf/infocom/LiaoT24,
	author = {Qi Liao and
                  Tze{-}Yang Tung},
	title = {AdaSem: Adaptive Goal-Oriented Semantic Communications for End-to-End
                  Camera Relocalization},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1111--1120},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621397},
	doi = {10.1109/INFOCOM52122.2024.10621397},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiaoT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, deep autoencoders have gained traction as a powerful method for implementing goal-oriented semantic communications systems. The idea is to train a mapping from the source domain directly to channel symbols, and vice versa. However, prior studies often focused on rate-distortion tradeoff and transmission delay, at the cost of increasing end-to-end complexity and thus latency. Moreover, the datasets used are often not reflective of real-world environments, and the results were not validated against real-world baseline systems, leading to an unfair comparison. In this paper, we study the problem of remote camera pose estimation and propose AdaSem, an adaptive semantic communications approach that optimizes the tradeoff between inference accuracy and end-to-end latency. We develop an adaptive semantic codec model, which encodes the source data into a dynamic number of symbols, based on the latent space distribution and the channel state feedback. We utilize a lightweight model for both transmitter and receiver to ensure comparable complexity to the baseline implemented in a real- world system. Extensive experiments on real-environment data show the effectiveness of our approach. When compared to a real implementation of a client-server camera relocalization service, AdaSem outperforms the baseline by reducing the end-to-end delay and estimation error by over 75% and 63%, respectively.}
}


@inproceedings{DBLP:conf/infocom/Zhang0WW0Z24,
	author = {Cheng Zhang and
                  Yang Xu and
                  Xiaowei Wu and
                  En Wang and
                  Hongbo Jiang and
                  Yaoxue Zhang},
	title = {A Semi-Asynchronous Decentralized Federated Learning Framework via
                  Tree-Graph Blockchain},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1121--1130},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621425},
	doi = {10.1109/INFOCOM52122.2024.10621425},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhang0WW0Z24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decentralized federated learning (DFL) overcomes the single point of failure issue of centralized federated learning. Building upon DFL, blockchain-based federated learning (BFL) takes further strides in establishing trust, enhancing security, and fault tolerance. However, BFL based on the classical linear blockchain exhibits diminished training efficiency in heterogeneous environments and is limited by the performance bottleneck of blockchain. Recent solutions introduce the directed acyclic graph (DAG) blockchain to address these issues, yet they compromise the verifiability of BFL, struggle with handling outdated models, and have a slow convergence speed. In this paper, we propose TGFL, a decentralized federated learning framework based on the Tree-Graph blockchain. The underlying blockchain structure of TGFL is designed as a block-centered DAG to support verifiable and semi-asynchronous training. To facilitate fast convergence, we design a pivot chain generation algorithm that topologically sorts the semi-asynchronous training process, guiding participants in sampling appropriate models. The consensus mechanism, which is closely integrated with federated learning, ensures that the TGFL can effectively resist attacks on the model and the blockchain system. Extensive experiments in various settings demonstrate that TGFL can achieve better training efficiency and model accuracy compared to three baselines.}
}


@inproceedings{DBLP:conf/infocom/YueHC024,
	author = {Sheng Yue and
                  Xingyuan Hua and
                  Lili Chen and
                  Ju Ren},
	title = {Momentum-Based Federated Reinforcement Learning with Interaction and
                  Communication Efficiency},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1131--1140},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621260},
	doi = {10.1109/INFOCOM52122.2024.10621260},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YueHC024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Reinforcement Learning (FRL) has garnered increasing attention recently. However, due to the intrinsic spatio-temporal non-stationarity of data distributions, the current approaches typically suffer from high interaction and communication costs. In this paper, we introduce a new FRL algorithm, named MFPO, that utilizes momentum, importance sampling, and additional server-side adjustment to control the shift of stochastic policy gradients and enhance the efficiency of data utilization. We prove that by proper selection of momentum parameters and interaction frequency, MFPO can achieve \\widetilde {\\mathcal{O}}\\left({H{N^{ - 1}}{\\varepsilon ^{ - 3/2}}}\\right)\nand \\widetilde {\\mathcal{O}}\\left({{\\varepsilon ^{ - 1}}}\\right)\ninteraction and communication complexities (N represents the number of agents), where the interaction complexity achieves linear speedup with the number of agents, and the communication complexity aligns the best achievable of existing first-order FL algorithms. Extensive experiments corroborate the substantial performance gains of MFPO over existing methods on a suite of complex and high-dimensional benchmarks.}
}


@inproceedings{DBLP:conf/infocom/ZhongPCYMCM24,
	author = {Luying Zhong and
                  Yueyang Pi and
                  Zheyi Chen and
                  Zhengxin Yu and
                  Wang Miao and
                  Xing Chen and
                  Geyong Min},
	title = {SpreadFGL: Edge-Client Collaborative Federated Graph Learning with
                  Adaptive Neighbor Generation},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1141--1150},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621368},
	doi = {10.1109/INFOCOM52122.2024.10621368},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhongPCYMCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Graph Learning (FGL) has garnered widespread attention by enabling collaborative training on multiple clients for semi-supervised classification tasks. However, most existing FGL studies do not well consider the missing inter-client topology information in real-world scenarios, causing insufficient feature aggregation of multi-hop neighbor clients during model training. Moreover, the classic FGL commonly adopts the FedAvg but neglects the high training costs when the number of clients expands, resulting in the overload of a single edge server. To address these important challenges, we propose a novel FGL framework, named SpreadFGL, to promote the information flow in edge-client collaboration and extract more generalized potential relationships between clients. In SpreadFGL, an adaptive graph imputation generator incorporated with a versatile assessor is first designed to exploit the potential links between subgraphs, without sharing raw data. Next, a new negative sampling mechanism is developed to make SpreadFGL concentrate on more refined information in downstream tasks. To facilitate load balancing at the edge layer, SpreadFGL follows a distributed training manner that enables fast model convergence. Using real-world testbed and benchmark graph datasets, extensive experiments demonstrate the effectiveness of the proposed SpreadFGL. The results show that SpreadFGL achieves higher accuracy and faster convergence against state-of-the-art algorithms.}
}


@inproceedings{DBLP:conf/infocom/DingWB24,
	author = {Ningning Ding and
                  Ermin Wei and
                  Randall Berry},
	title = {Strategic Data Revocation in Federated Unlearning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1151--1160},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621201},
	doi = {10.1109/INFOCOM52122.2024.10621201},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DingWB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By allowing users to erase their data’s impact on federated learning models, federated unlearning protects users’ right to be forgotten and data privacy. Despite a burgeoning body of research on federated unlearning’s technical feasibility, there is a paucity of literature investigating the considerations behind users’ requests for data revocation. This paper proposes a non-cooperative game framework to study users’ data revocation strategies in federated unlearning. We prove the existence of a Nash equilibrium. However, users’ best response strategies are coupled via model performance and unlearning costs, which makes the equilibrium computation challenging. We obtain the Nash equilibrium by establishing its equivalence with a much simpler auxiliary optimization problem. We also summarize users’ multi-dimensional attributes into a single-dimensional metric and derive the closed-form characterization of an equilibrium, when users’ unlearning costs are negligible. Moreover, we compare the cases of allowing and forbidding partial data revocation in federated unlearning. Interestingly, the results reveal that allowing partial revocation does not necessarily increase users’ data contributions or payoffs due to the game structure. Additionally, we demonstrate that positive externalities may exist between users’ data revocation decisions when users incur unlearning costs, while this is not the case when their unlearning costs are negligible.}
}


@inproceedings{DBLP:conf/infocom/Zhu0CSQL24,
	author = {Andong Zhu and
                  Sheng Zhang and
                  Ke Cheng and
                  Xiaohang Shi and
                  Zhuzhong Qian and
                  Sanglu Lu},
	title = {AdaStreamer: Machine-Centric High-Accuracy Multi-Video Analytics with
                  Adaptive Neural Codecs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1161--1170},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621074},
	doi = {10.1109/INFOCOM52122.2024.10621074},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhu0CSQL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Increased videos captured by widely deployed cameras are being analyzed by computer vision-based Deep Neural Networks (DNNs) on servers rather than being streamed for humans. Unfortunately, the conventional codecs (e.g., H.26x and MPEG-x) originally designed for video streaming lack content-aware feature extraction and hinder machine-centric video analytics, making it difficult to achieve the required high accuracy with tolerable delay. Neural codecs (e.g., autoencoder) now hold impressive compression performance and have been widely advocated in video streaming. While autoencoder shows transformative potential, the application in video analytics is hampered by low accuracy in detecting small objects of highresolution videos and the serious challenges posed by multivideo streaming. To this end, we propose AdaStreamer with adaptive neural codecs to enable real machine-centric highaccuracy multi-video analytics. We also investigate how to achieve optimal accuracy under delay constraints via careful scheduling in Compression Ratios (CRs, the ratio of the compressed size to the original data size) and bandwidth allocation, and further propose a Markov-based Adaptive Compression and Bandwidth Allocation algorithm (MACBA). We have practically developed a prototype of AdaStreamer, based on which extensive experiments verify its accuracy improvement (up to 15%) compared to stateof-the-art coding and streaming solutions.}
}


@inproceedings{DBLP:conf/infocom/E0ZWCC24,
	author = {Jinlong E and
                  Lin He and
                  Zongyi Zhao and
                  Yachen Wang and
                  Gonglong Chen and
                  Wei Chen},
	title = {AggDeliv: Aggregating Multiple Wireless Links for Efficient Mobile
                  Live Video Delivery},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1173--1180},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621184},
	doi = {10.1109/INFOCOM52122.2024.10621184},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/E0ZWCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile live-streaming applications with stringent latency and bandwidth requirements have gained tremendous attention in recent years. Encountered with bandwidth insufficiency and congestion instability of the wireless uplinks, multi-access networking provides opportunities to achieve fast and robust connectivity. However, the state-of-the-art multi-path transmission solutions are lack of adaptivity to the heterogeneous and dynamic nature of wireless networks. Meanwhile, the indispensable video coding and transformation bring about extra latency and make the video delivery vulnerable to network throughput fluctuation. This paper presents AggDeliv, a framework that provides efficient and robust multi-path transmission for mobile live video delivery. The key idea is to relate multi-path packet scheduling to congestion control optimization over diverse wireless links and adapt it to the mobile video characteristics. This is achieved by probabilistic packet allocation based on links’ congestion windows, wireless-oriented delay and loss aware congestion control, as well as lightweight video frame coding and network-adaptive frame-packet transformation. Real-world evaluations demonstrate that our framework significantly outperforms the state-of-the-art solutions on aggregate goodput and streaming video bitrate.}
}


@inproceedings{DBLP:conf/infocom/SunW0MDL024,
	author = {Lin Sun and
                  Weijun Wang and
                  Tingting Yuan and
                  Liang Mi and
                  Haipeng Dai and
                  Yunxin Liu and
                  Xiaoming Fu},
	title = {BiSwift: Bandwidth Orchestrator for Multi-Stream Video Analytics on
                  Edge},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1181--1190},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621392},
	doi = {10.1109/INFOCOM52122.2024.10621392},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SunW0MDL024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-definition (HD) cameras for surveillance and road traffic have experienced tremendous growth, demanding intensive computation resources for real-time analytics. Recently, offloading frames from the front-end device to the back-end edge server has shown great promise. In multi-stream competitive environments, efficient bandwidth management and proper scheduling are crucial to ensure both high inference accuracy and high throughput. To achieve this goal, we propose BiSwift, a bi-level framework that scales the concurrent real-time video analytics by a novel adaptive hybrid codec integrated with multi-level pipelines, and a global bandwidth controller for multiple video streams. The lower-level front-back-end collaborative mechanism (called adaptive hybrid codec) locally optimizes the accuracy and accelerates end-to-end video analytics for a single stream. The upper-level scheduler aims to accuracy fairness among multiple streams via the global bandwidth controller. The evaluation of BiSwift shows that BiSwift is able to real-time object detection on 9 streams with an edge device only equipped with an NVIDIA RTX3070 (8G) GPU. BiSwift improves 10%∼21% accuracy and presents 1.2∼ 9× throughput compared with the state-of-the-art video analytics pipelines.}
}


@inproceedings{DBLP:conf/infocom/ZhuZSCSL24,
	author = {Andong Zhu and
                  Sheng Zhang and
                  Xiaohang Shi and
                  Ke Cheng and
                  Hesheng Sun and
                  Sanglu Lu},
	title = {Crucio: End-to-End Coordinated Spatio-Temporal Redundancy Elimination
                  for Fast Video Analytics},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1191--1200},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621116},
	doi = {10.1109/INFOCOM52122.2024.10621116},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhuZSCSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video Analytics Pipeline (VAP) usually relies on traditional codecs to stream video content from clients to servers. However, such analytics-agnostic codecs preserve considerable pixels not relevant to achieving high analytics accuracy, incurring a large end-to-end delay. Despite the significant efforts of pioneers, they fall short as they resisted complete redundancy elimination. Achieving such a goal is extremely challenging, and naive design without coordination can result in the benefits of redundancy elimination being counterbalanced by intolerable delays introduced. We present CRUCIO, an end-to-end coordinated spatio-temporal redundancy elimination system for edge video analytics. CRUCIO leverages reshaped asymmetric autoencoders for end-to-end frame filtering (temporally) and coordinated intra-frame (spatially), inter-frame (temporally) compression. Furthermore, CRUCIO can decode the compressed key frames all in one go and support adaptive VAP batch size for delay optimization. Extensive evaluations reveal significant end-to-end delay reductions (at least 31% under an accuracy target of 0.9) in CRUCIO compared to the state-of-the-art VAP redundancy elimination methods (e.g., DDS, Reducto, STAC, etc).}
}


@inproceedings{DBLP:conf/infocom/WangJ0L24,
	author = {Penghao Wang and
                  Ruobing Jiang and
                  Chao Liu and
                  Jun Luo},
	title = {{AGR:} Acoustic Gait Recognition Using Interpretable Micro-Range Profile},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1201--1210},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621283},
	doi = {10.1109/INFOCOM52122.2024.10621283},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangJ0L24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent times, gait recognition, a type of biometric identification, has been widely used for area access control and smart homes. It improves convenience, privacy, and personalized experiences. Contemporary academic inquiry centers on privacy-preserving wireless sensing solutions as substitutes for computer vision. Yet, prevailing strategies heavily lean on abstract features, leading to inherent limitations in interpretability and stability. Fortunately, the widespread utilization of smart speakers has opened up opportunities for acoustic sensing, making it possible to extract more interpretable features. In this paper, we further push the limit of acoustic recognition with visual interpretability by sequentially visualizing fine-grained acoustic human gait features. The construction of initial gait profiles involves matrixing and compressing multipath gait echoes, resulting in imperceptible gait indications. Interpretability is then achieved through novel micro-range profiles, incorporating innovations such as clutter elimination using the Mobile Target Detector (MTD), compensation for farther echo strength, and subtraction of macro torso migration. These interpretable gait profiles offer practical benefits by enhancing data utilization, optimizing abnormal data handling, and improving model stability. Extensive evaluations with an open experimental scenario have been conducted to demonstrate accuracy reaching 97.5% in general, and robust performance against impacts from various practical factors.}
}


@inproceedings{DBLP:conf/infocom/CaoZLCL24,
	author = {Yetong Cao and
                  Shujie Zhang and
                  Fan Li and
                  Zhe Chen and
                  Jun Luo},
	title = {hBP-Fi: Contactless Blood Pressure Monitoring via Deep-Analyzed Hemodynamics},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1211--1220},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621267},
	doi = {10.1109/INFOCOM52122.2024.10621267},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/CaoZLCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blood pressure (BP) measurement is significant to the assessment of many dangerous health conditions. Apart from invasively inserting catheters into arteries, non-invasive approaches typically rely on wearing devices on specific skin areas with consistent pressure. However, this can be uncomfortable and unsuitable for certain individuals, and the accuracy of these methods may significantly decrease due to improper device placements and wearing states. Recently, contactless methods leveraging RF technology have emerged as a potential alternative. However, these methods suffer from the drawback of overfitting deep learning (DL) models without a sound physiological basis, resulting in a lack of clear explanations for their outputs. Consequently, such limitations lead to skepticism and distrust among medical experts. In this paper, we propose hBP-Fi, a contactless BP measurement system driven by hemodynamics acquired via RF sensing. In addition to its contactless convenience, hBP-Fi is superior to other RF sensing approaches in i) grounding on hemodynamics as the key physical process of heart-pulse activities, ii) exploiting beam-steerable RF devices to achieve a super-resolution scan on the fine-grained pulse activities along arm arteries, and iii) ensuring the trustworthiness of system outputs via an explainable (decision-understandable) DL model. Extensive experiments with 35 subjects demonstrate that hBP-Fi can achieve the error of -2.05±6.83 mmHg and 1.99 ± 6.30 mmHg for monitoring systolic and diastolic blood pressures, respectively.}
}


@inproceedings{DBLP:conf/infocom/Hu0ZHWC0L24,
	author = {Jingyang Hu and
                  Hongbo Jiang and
                  Tianyue Zheng and
                  Jingzhi Hu and
                  Hongbo Wang and
                  Hangcheng Cao and
                  Zhe Chen and
                  Jun Luo},
	title = {M\({}^{\mbox{2}}\)-Fi: Multi-person Respiration Monitoring via Handheld
                  WiFi Devices},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1221--1230},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621199},
	doi = {10.1109/INFOCOM52122.2024.10621199},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Hu0ZHWC0L24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wi-Fi signals are commonly used for conventional communication, yet they can also realize low-cost and non-invasive human sensing. However, Wi-Fi sensing in Multi-person scenarios is still a challenging problem. In this paper, we propose M 2 -Fi to achieve multi-person respiration monitoring using a handheld device. M 2 -Fi leverages Wi-Fi BFI (beamforming feedback information) performs respiration monitoring. As a compressed version of the uplink CSI (channel state information), BFI transmission is unencrypted, easily obtained using frame capture, and does not require specific firmware to obtain. M 2 -Fi is based on an interesting experiment phenomenon that when a Wi-Fi device is very close to a subject, near-field channel changes caused by the subject significantly cancel out changes from other subjects. We employed VMD (Variational Mode Decomposition) to eliminate the interference caused by hand movement in the BFI time series. Subsequently, we devised a deep learning architecture based on GAN (Generative Adversarial Networks) to recover fine-grained respiration waveforms from the respiration patterns extracted from the BFI time series. Our experiments on collected 50-hour data from 8 subjects show that M 2 -Fi can accurately recover the respiration waveforms of multiple persons with handheld devices.}
}


@inproceedings{DBLP:conf/infocom/Zhao00H24,
	author = {Leqi Zhao and
                  Rui Xiao and
                  Jianwei Liu and
                  Jinsong Han},
	title = {One is Enough: Enabling One-shot Device-free Gesture Recognition with
                  {COTS} WiFi},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1231--1240},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621091},
	doi = {10.1109/INFOCOM52122.2024.10621091},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhao00H24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, WiFi-based gesture recognition (WGR) has gained popularity due to its privacy-preserving nature and the wide availability of WiFi infrastructure. However, existing WGR systems suffer from scalability issues, i.e., requiring extensive data collection and re-training for each new gesture class. To address these limitations, we propose OneSense, a one-shot WiFi-based gesture recognition system that can efficiently and easily adapt to new gesture classes. Specifically, we first propose a data enrichment approach based on the law of signal propagation in physical world to generate virtual gestures, enhancing the diversity of the training set without extra overhead of real sample collection. Then, we devise an aug-meta learning (AML) framework to enable efficient and scalable few-short learning. This framework leverages two pre-training stages (i.e., aug-training and meta-training) to improve the model’s feature extraction and generalization abilities, and ultimately achieves accurate one-shot gesture recognition through fine-tuning. Experimental results demonstrate that OneSense achieves 93% one-shot gesture recognition accuracy, which outperforms the state-of-the-art approaches. Moreover, it maintains high recognition accuracy when facing new environments, user locations, and user orientations. Furthermore, the proposed AML framework reduces 86%+ pre-training latency compared to conventional meta-learning method.}
}


@inproceedings{DBLP:conf/infocom/YahyaouiBVD24,
	author = {Wassim Yahyaoui and
                  Joachim Bruneau{-}Queyreix and
                  Marcus V{\"{o}}lp and
                  J{\'{e}}r{\'{e}}mie Decouchant},
	title = {Tolerating Disasters with Hierarchical Consensus},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1241--1250},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621371},
	doi = {10.1109/INFOCOM52122.2024.10621371},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YahyaouiBVD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geo-replication provides disaster recovery after catastrophic accidental failures or attacks, such as fires, blackouts or denial-of-service attacks to a data center or region. Naturally distributed data structures, such as Blockchains, when well designed, are immune against such disruptions, but they also benefit from leveraging locality. In this work, we consolidate the performance of geo-replicated consensus by leveraging novel insights about hierarchical consensus and a construction methodology that allows creating novel protocols from existing building blocks. In particular we show that cluster confirmation, paired with subgroup rotation, allows protocols to safely operate through situations where all members of the global consensus group are Byzantine. We demonstrate our compositional construction by combining the recent HotStuff and Damysus protocols into a hierarchical geo-replicated blockchain with global durability guarantees. We present a compositionality proof and demonstrate the correctness of our protocol, including its ability to tolerate cluster crashes. Our protocol — Orion 1 — achieves a 20% higher throughput than GeoBFT, the latest hierarchical Byzantine Fault-Tolerant (BFT) protocol.}
}


@inproceedings{DBLP:conf/infocom/ChenFZCDZ24,
	author = {Wuhui Chen and
                  Yikai Feng and
                  Jianting Zhang and
                  Zhongteng Cai and
                  Hong{-}Ning Dai and
                  Zibin Zheng},
	title = {Auncel: Fair Byzantine Consensus Protocol with High Performance},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1251--1260},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621379},
	doi = {10.1109/INFOCOM52122.2024.10621379},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenFZCDZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the advent of decentralized financial applications based on blockchains, new attacks that take advantage of manipulating the order of transactions have emerged. To this end, order fairness protocols are devised to prevent such order manipulations. However, existing order fairness protocols adopt time-consuming mechanisms that bring huge computation overheads and defer the finalization of transactions to the following rounds, eventually compromising system performance. In this work, we present Auncel, a novel consensus protocol that achieves both order fairness and high performance. Auncel leverages a weight-based strategy to order transactions, enabling all transactions in a block to be committed within one consensus round, without cost computation and further delays. Furthermore, Auncel achieves censorship resistance by integrating the consensus protocol with the fair ordering strategy, ensuring all transactions can be ordered fairly. To reduce the overheads introduced by the fair ordering strategy, we also design optimization mechanisms, including dynamic transaction compression and adjustable replica proposal strategy. We implement a prototype of Auncel based on HotStuff and construct extensive experiments. Experimental results show that Auncel can increase the throughput by 6× and reduce the confirmation latency by 3× compared with state-of-the-art order fairness protocols.}
}


@inproceedings{DBLP:conf/infocom/SuZC024,
	author = {Xiaoxin Su and
                  Yipeng Zhou and
                  Laizhong Cui and
                  Song Guo},
	title = {Expediting In-Network Federated Learning by Voting-Based Consensus
                  Model Compression},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1271--1280},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621376},
	doi = {10.1109/INFOCOM52122.2024.10621376},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SuZC024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy. To conduct model training by FL, multiple clients exchange model updates with a parameter server via Internet. To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients. The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS. To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and model aggregating. In the former phase, clients report their significant model update indices to the PS to estimate global significant model updates. In the latter phase, clients upload global significant model updates to the PS for aggregation. FediAC consumes much less memory space and communication traffic than existing works because the first phase can guarantee consensus compression across clients. The PS easily aligns model update indices to swiftly complete aggregation in the second phase. Finally, we conduct extensive experiments by using public datasets to demonstrate that FediAC remarkably surpasses the state-of-the-art baselines in terms of model accuracy and communication traffic.}
}


@inproceedings{DBLP:conf/infocom/ShenT24,
	author = {Wen{-}Hsuan Shen and
                  Hsin{-}Mu Tsai},
	title = {MatrixLoc: Centimeter-Level Relative Vehicle Positioning with Matrix
                  Headlight},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1281--1290},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621180},
	doi = {10.1109/INFOCOM52122.2024.10621180},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ShenT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Precise vehicle positioning is the fundamental technology in vehicle platooning, for sensing the driving environment, detecting hazards, and determining driving strategies. While radars offer robust performance in extreme weather, the positioning error of up to 1.8 m could be insufficient for platoons with vehicle spacings of a few meters. In contrast, LiDARs provide great accuracy, but its high cost hinders its penetration in the commercial market. To this end, this paper presents MatrixLoc, utilizing a projector as the headlight and a customized light sensor array as the receiver to achieve low-cost and centimeterlevel relative vehicle positioning in both longitudinal and lateral axes, along with bearing information. MatrixLoc leverages differential phase shift keying (DPSK) modulation to create a fringe pattern, enabling single-shot positioning. Accurate positioning is achieved by analyzing the observed space-varying frequency and demodulated phase information. Real-world driving results demonstrate that MatrixLoc achieves centimeter-level positioning accuracy, with a positioning error of 30 cm and a bearing error of 9 degrees at a distance of 20 m.}
}


@inproceedings{DBLP:conf/infocom/WangC24,
	author = {Ruiqi Wang and
                  Guohong Cao},
	title = {Edge-Assisted Camera Selection in Vehicular Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1291--1300},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621152},
	doi = {10.1109/INFOCOM52122.2024.10621152},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Camera sensors have been widely used to perceive the vehicle surrounding environments, understand the traffic condition, and then help avoid traffic accidents. Since most sensors are limited by line of sight, the perception data collected through individual vehicle can be uploaded and shared through the edge server. To reduce the bandwidth, storage and processing cost, we propose an edge-assisted camera selection system that only selects the necessary camera images to upload to the server. The selection is based on the camera metadata which describes the coverage of the cameras represented with GPS locations, orientations, and field of views. Different from existing work, our metadata based approach can detect and locate camera occlusions by leveraging LiDAR sensors, and then precisely and quickly calculate the real camera coverage and identify the coverage overlap. Based on the camera metadata, we study two camera selection problems, the Max-Coverage problem and the Min-Selection problem, and solve them with efficient algorithms. Moreover, we propose similarity based redundancy suppression techniques to further reduce the bandwidth consumption which becomes significant due to vehicle movements. Extensive evaluations demonstrate that the proposed algorithms can effectively select cameras to maximize coverage or minimize bandwidth consumption based on the application requirements.}
}


@inproceedings{DBLP:conf/infocom/PradhanRSC24,
	author = {Suyash Pradhan and
                  Debashri Roy and
                  Batool Salehi and
                  Kaushik R. Chowdhury},
	title = {{COPILOT:} Cooperative Perception using Lidar for Handoffs between
                  Road Side Units},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1301--1310},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621174},
	doi = {10.1109/INFOCOM52122.2024.10621174},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/PradhanRSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents COPILOT, a ML-based approach that allows vehicles requiring ubiquitous high bandwidth connectivity to identify the most suitable road side units (RSUs) through proactive handoffs. By cooperatively exchanging the data obtained from local 3D Lidar point clouds within adjacent vehicles and with coarse knowledge of their relative positions, COPILOT identifies transient blockages to all candidate RSUs along the path under study. Such cooperative perception is critical for choosing RSUs with highly directional links required for mmWave bands, which majorly degrade in the absence of LOS. COPILOT proposes three modules that operate in an inter-connected manner: (i) As an alternative to sending raw Lidar point clouds, it extracts and transmits low-dimensional intermediate features to lower the overhead of inter-vehicle messaging; (ii) It utilizes an attention-mechanism to place greater emphasis on data collected from specific vehicles, as opposed to nearest neighbor and distance-based selection schemes, and (iii) it experimentally validates the outcomes using an outdoor testbed composed of an autonomous car and Talon AD7200 60GHz routers emulating the RSUs, accompanied by the public release of the datasets. Results reveal COPILOT yields upto 69.8% and 20.42% improvement in latency and throughput compared to traditional reactive handoffs for mmWave networks, respectively.}
}


@inproceedings{DBLP:conf/infocom/WangZCLCZ24,
	author = {Zhenxi Wang and
                  Hongzi Zhu and
                  Yunxiang Cai and
                  Quan Liu and
                  Shan Chang and
                  Liang Zhang},
	title = {LoRaPCR: Long Range Point Cloud Registration through Multi-hop Relays
                  in VANETs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1311--1320},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621158},
	doi = {10.1109/INFOCOM52122.2024.10621158},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangZCLCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Point cloud registration (PCR) can significantly extend the visual field and enhance the point density on distant objects, thereby improving driving safety. However, it is very challenging for vehicles to perform online registration between long-range point clouds. In this paper, we propose an online long-range PCR scheme in VANETs, called LoRaPCR, where vehicles achieve long-range registration through multi-hop short-range highly-accurate registrations. Given the NP-hardness of the problem, a heuristic algorithm is developed to determine best registration paths while leveraging the reuse of registration results to reduce computation costs. Moreover, we utilize an optimized dynamic programming algorithm to determine the transmission routes while minimizing the communication overhead. Results of extensive simulations demonstrate that LoRaPCR can achieve high PCR accuracy with low relative translation and rotation errors of 0.55 meters and 1.43°, respectively, at a distance of over 100 meters, and reduce the computation overhead by more than 50% compared to the state-of-the-art method.}
}


@inproceedings{DBLP:conf/infocom/DuLL0Z024,
	author = {Bingqian Du and
                  Jun Liu and
                  Ziyue Luo and
                  Chuan Wu and
                  Qiankun Zhang and
                  Hai Jin},
	title = {Expediting Distributed {GNN} Training with Feature-only Partition
                  and Optimized Communication Planning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1321--1330},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621137},
	doi = {10.1109/INFOCOM52122.2024.10621137},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DuLL0Z024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature-only partition of large graph data in distributed Graph Neural Network (GNN) training offers advantages over commonly adopted graph structure partition, such as minimal graph preprocessing cost and elimination of cross-worker subgraph sampling burdens. Nonetheless, performance bottleneck of GNN training with feature-only partitions still largely lies in the substantial communication overhead due to cross-worker feature fetching. To reduce the communication overhead and expedite distributed training, we first investigate and answer two key questions on convergence behaviors of GNN model in feature-partition based distribute GNN training: 1) As no worker holds a complete copy of each feature, can gradient exchange among workers compensate for the information loss due to incomplete local features? 2) If the answer to the first question is negative, is feature fetching in every training iteration of the GNN model necessary to ensure model convergence? Based on our theoretical findings on these questions, we derive an optimal communication plan that decides the frequency for feature fetching during the training process, taking into account bandwidth levels among workers and striking a balance between model loss and training time. Extensive evaluation demonstrates consistent results with our theoretical analysis, and the effectiveness of our proposed design.}
}


@inproceedings{DBLP:conf/infocom/TiranaTIC24,
	author = {Joana Tirana and
                  Dimitra Tsigkari and
                  George Iosifidis and
                  Dimitris Chatzopoulos},
	title = {Workflow Optimization for Parallel Split Learning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1331--1340},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621348},
	doi = {10.1109/INFOCOM52122.2024.10621348},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TiranaTIC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Split learning (SL) has been recently proposed as a way to enable resource-constrained devices to train multi-parameter neural networks (NNs) and participate in federated learning (FL). In a nutshell, SL splits the NN model into parts, and allows clients (devices) to offload the largest part as a processing task to a computationally powerful helper. In parallel SL, multiple helpers can process model parts of one or more clients, thus, considerably reducing the maximum training time over all clients (makespan). In this paper, we focus on orchestrating the workflow of this operation, which is critical in highly heterogeneous systems, as our experiments show. In particular, we formulate the joint problem of client-helper assignments and scheduling decisions with the goal of minimizing the training makespan, and we prove that it is NPhard. We propose a solution method based on the decomposition of the problem by leveraging its inherent symmetry, and a second one that is fully scalable. A wealth of numerical evaluations using our testbed’s measurements allow us to build a solution strategy comprising these methods. Moreover, we show that this strategy finds a near-optimal solution, and achieves a shorter makespan than the baseline scheme by up to 52.3%.}
}


@inproceedings{DBLP:conf/infocom/ZouAD24,
	author = {Zhibin Zou and
                  Iresha Amarasekara and
                  Aveek Dutta},
	title = {Learning to Decompose Asymmetric Channel Kernels for Generalized Eigenwave
                  Multiplexing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1341--1350},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621411},
	doi = {10.1109/INFOCOM52122.2024.10621411},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZouAD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning the principal eigenfunctions of a kernel is at the core of many machine-learning problems. Common methods usually deal with symmetric kernels based on Mercer’s Theorem. However, in the communication systems, the channel kernel is usually asymmetric due to the inconsistencies between the uplink and the downlink propagation environment. In this paper, we propose an explainable Neural Network for extracting eigenfunctions from generic multi-dimensional asymmetric channel kernels based on a recent method called High Order Generalized Mercer’s Theorem (HOGMT), by decomposing it into jointly orthogonal eigenfunctions. The proposed neural network based approach is efficient and can be easily implemented compared to the conventional SVD based solutions used for eigen decomposition. We also discuss the effect of different hyperparameters on the training time, constraint satisfaction, and overall performance. Finally, we show that multiplexing using these eigenfunctions mitigates interference across all the available Degrees of Freedom (DoF), both mathematically as well as via neural network based system-level simulations.}
}


@inproceedings{DBLP:conf/infocom/TianWLLY24,
	author = {Zijie Tian and
                  En Wang and
                  Wenbin Liu and
                  Baoju Li and
                  Funing Yang},
	title = {{META-MCS:} {A} Meta-knowledge Based Multiple Data Inference Framework},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1351--1360},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621276},
	doi = {10.1109/INFOCOM52122.2024.10621276},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TianWLLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing (MCS) is a paradigm for data collection with the limitation of budgets and worker availability. The central strategy of MCS is recruiting workers to sense a part of data and subsequently infer the unsensed data. To infer unsensed data, prior research has proposed several algorithms that do not require historical data, but their inference accuracy is very limited. More effective works are training a model with sufficient historical data. However, such methods can’t infer data with few to none historical data. A more promising strategy is training models from other similar datasets that have been sensed. However, such datasets are different in terms of sensing locations, numbers of sensed data and data types. Such variance introduces the complex issue of integrating knowledge from these datasets and then training inference models. To solve these, we propose a meta-knowledge based multiple data inference framework named META-MCS. In META-MCS, we propose a similarity evaluation model TMFS. Following this, we cluster similar datasets and train generalized models for each cluster. Finally, META-MCS selects an appropriate model to infer unsensed data. We validate our proposed methods through extensive experiments using ten different datasets, which substantiate the effectiveness of our framework.}
}


@inproceedings{DBLP:conf/infocom/HanXWLSL24,
	author = {Jiangping Han and
                  Kaiping Xue and
                  Wentao Wang and
                  Ruidong Li and
                  Qibin Sun and
                  Jun Lu},
	title = {RateMP: Optimizing Bandwidth Utilization with High Burst Tolerance
                  in Data Center Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1361--1370},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621096},
	doi = {10.1109/INFOCOM52122.2024.10621096},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HanXWLSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Load balancing in data center networks (DCNs) is a crucial and complex undertaking. Multi-path TCP (MPTCP) has been proposed as a cost-effective solution that aims to distribute workloads and improve network resource utilization. However, it can escalate buffer occupancy and undermine burst tolerance, particularly in scenarios involving incast short flows. To address these limitations, we propose a novel multi-path congestion control algorithm, RateMP, to optimize bandwidth utilization efficiency while ensuring burst tolerance in DCNs. RateMP employs a hybrid window and rate control loop with coupled gradient projection adjustment, enabling fast and fine-grained bandwidth allocation and accelerating convergence. Additionally, RateMP eliminates the limitation of cwnd with under-rate pacing to protect incast and busty flows. We prove that RateMP is Lyapunov stable and asymptotically stable, and show the improvement of RateMP through a kernel-based implementation and extended large-scale simulations. RateMP keeps high bandwidth utilization, cuts RTT by 2x and reduces flow completion times (FCT) by 45% in incast scenarios compared to existing algorithms.}
}


@inproceedings{DBLP:conf/infocom/DasSN24,
	author = {Sushovan Das and
                  Arlei Silva and
                  T. S. Eugene Ng},
	title = {Rearchitecting Datacenter Networks: {A} New Paradigm with Optical
                  Core and Optical Edge},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1371--1380},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621224},
	doi = {10.1109/INFOCOM52122.2024.10621224},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DasSN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {All-optical circuit-switching (OCS) technology is the key to design energy-efficient and high-performance datacenter network (DCN) architectures for the future. However, existing round-robin based OCS cores perform poorly under realistic workloads having high traffic skewness and high volume of inter-rack traffic. To address this issue, we propose a novel DCN architecture OSSV: a combination of OCS-based core (between ToR switches) and OCS-based reconfigurable edge (between servers and ToR switches). On one hand, the OCS core is traffic agnostic and realizes reconfigurably non-blocking ToR-level connectivity. On the other hand, OCS-based edge reconfigures itself to reshape the incoming traffic in order to jointly minimize traffic skewness and inter-rack traffic volume. Our novel optimization framework can obtain the right balance between these intertwined objectives. Our extensive simulations and testbed evaluation show that OSSV can achieve high performance under diverse DCN traffic while consuming low power and incurring low cost.}
}


@inproceedings{DBLP:conf/infocom/Wan0YLYZH24,
	author = {Zirui Wan and
                  Jiao Zhang and
                  Mingxuan Yu and
                  Junwei Liu and
                  Jun Yao and
                  Xinghua Zhao and
                  Tao Huang},
	title = {BiCC: Bilateral Congestion Control in Cross-datacenter {RDMA} Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1381--1390},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621412},
	doi = {10.1109/INFOCOM52122.2024.10621412},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Wan0YLYZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of network-intensive applications like machine learning and cloud storage, there are two growing trends: (i) RDMA has been widely deployed to enhance underlying high-speed networks; (ii) applications are deployed on geographically distributed datacenters to meet customer demands (e.g., low access latency to services or regular data backups). To fully utilize the benefits of RDMA, we desire to support long-haul RDMA transport for cross-datacenter applications. Different from common intra-datacenter communications, the hybrid of long-haul and intra-datacenter traffic complicates the congestion state, and the considerably long control loop makes it more severe. We revisit existing congestion control methods and find they are insufficient to address the hybrid traffic congestion.Note that regional datacenters are connected by dedicated long-haul optical fiber and datacenter interconnection (DCI) switches directly. In this paper, we propose Bilateral Congestion Control (BiCC), a novel solution relying on two-side DCI-switches to bilaterally alleviate the hybrid traffic congestion in the sender-side and receiver-side datacenter while serving as a building block for existing host-driven methods. BiCC can shorten the control loop to a single datacenter scale and aggregate congestion information across the whole datacenter. We implement BiCC on commodity P4-based switches and conduct evaluations using both testbed experiments and NS3 simulations. The extensive evaluation results show that BiCC ensures fast congestion avoidance. Thus, BiCC reduces the average FCT for intra-datacenter and inter-datacenter traffic by up to 53% and 51%, respectively, in large-scale simulations.}
}


@inproceedings{DBLP:conf/infocom/MengZHWR24,
	author = {Qingkai Meng and
                  Yiran Zhang and
                  Chaolei Hu and
                  Bo Wang and
                  Fengyuan Ren},
	title = {Explicit Dropping Notification in Data Centers},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1391--1400},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621312},
	doi = {10.1109/INFOCOM52122.2024.10621312},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MengZHWR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Datacenter applications increasingly demand microsecond-scale latency and tight tail latency. Despite recent advances in datacenter transport protocols, we notice that the timeout caused by packet loss is the killer of microsecond-scale latency. Moreover, refining the RTO setting is impractical due to the significant fluctuations in RTT. In this paper, we propose explicit dropping notification (EDN) to avoid timeouts. EDN rekindles ICMP Source Quench, where the switch notifies the source of precise packet loss information. Then the source can rapidly pinpoint dropped packets for fast retransmission instead of waiting for timeouts. More importantly, fast retransmission does not mean immediate retransmission which is prone to aggravate congestion and deteriorate latency. In light of this, we suggest finessing the timing and sending rate of retransmission. Specifically, as a reward of the paradigm shift to explicit notification, the source can pause for the queue draining time piggybacked on EDN messages and estimate connection capacity to figure out a proper sending rate, thus avoiding congestion aggravation. We implement EDN on the P4-programmable switching ASIC and Linux kernel. Evaluations show that, compared with state-of-the-art loss recovery schemes, EDN reduces the latency by up to 4.1× on average and 3.6× at the 99th-percentile.}
}


@inproceedings{DBLP:conf/infocom/LiuLCA0L24,
	author = {Maoli Liu and
                  Zhuohua Li and
                  Kechao Cai and
                  Jonathan Allcock and
                  Shengyu Zhang and
                  John C. S. Lui},
	title = {Quantum {BGP} with Online Path Selection via Network Benchmarking},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1401--1410},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621359},
	doi = {10.1109/INFOCOM52122.2024.10621359},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuLCA0L24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale quantum networks with thousands of nodes require topology-oblivious routing protocols to realize. Most existing quantum network routing protocols only consider the intra-domain scenario, where all nodes belong to a single party with complete topology knowledge. However, like the classical Internet, quantum Internet will likely be provided by multiple quantum Internet Service Providers (qISPs). In this paper, we consider the inter-domain scenario, where the network consists of multiple subnetworks owned by mutually untrusted parties without centralized control. Under this setting, previously proposed quantum entanglement routing policies, which rely on the network topology knowledge, are no longer applicable. We propose a Quantum Border Gateway Protocol (QBGP) for efficiently routing entanglement across qISP boundaries. To guarantee high-quality information transmission, we propose an algorithm named online top-K path selection. This algorithm utilizes the information gain introduced in this paper to adaptively decide on measurement parameters, allowing for the selection of high-fidelity paths and accurate fidelity estimates, while minimizing costs. Additionally, we implement a quantum network simulator and evaluate our protocol and algorithm. Our evaluation shows that QBGP effectively distributes entanglement across different qISPs, and our path selection algorithm increases the network performance by selecting high-fidelity paths with much lower resource consumption than other methods.}
}


@inproceedings{DBLP:conf/infocom/XuZHQ24,
	author = {Sun Xu and
                  Yangming Zhao and
                  Liusheng Huang and
                  Chunming Qiao},
	title = {Routing and Photon Source Provisioning in Quantum Key Distribution
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1411--1420},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621389},
	doi = {10.1109/INFOCOM52122.2024.10621389},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XuZHQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum Key Distribution (QKD) is considered to be an ultimate solution to communication security. However, current QKD devices, especially quantum photon sources, are expensive, and they can generate secret keys only at a low rate. In this paper, we design a system named RPSP for trusted relay-based QKD networks to not only minimize the number of photon sources needed in a network to ensure at least one feasible relay path exists for any potential QKD requests but also save the time to complete a batch of end-to-end QKD requests by jointly optimizing the routing of relay paths and the provisioning of photon sources along each relay path. Compared with existing works, RPSP focuses on a more practical scenario where only some of the nodes are equipped with photon sources and it leverages optical switching to enable dynamic photon source provisioning such that we can utilize such QKD devices in a more efficient way. Extensive simulations show that compared with baseline schemes, RPSP can save up to 87% of the photon sources needed in a trusted relay based QKD network, and 36% of the time to complete a batch of QKD requests.}
}


@inproceedings{DBLP:conf/infocom/LiuLWL24,
	author = {Maoli Liu and
                  Zhuohua Li and
                  Xuchuang Wang and
                  John C. S. Lui},
	title = {LinkSelFiE: Link Selection and Fidelity Estimation in Quantum Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1421--1430},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621263},
	doi = {10.1109/INFOCOM52122.2024.10621263},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reliable transmission of fragile quantum information requires one to efficiently select and utilize high-fidelity links among multiple noisy quantum links. However, the fidelity, a quality metric of quantum links, is unknown a priori. Uniformly estimating the fidelity of all links can be expensive, especially in networks with numerous links. To address this challenge, we formulate the link selection and fidelity estimation problem as a best arm identification problem and propose an algorithm named LinkSelFiE. The algorithm efficiently identifies the optimal link from a set of quantum links and provides an accurate fidelity estimate of that link with low quantum resource consumption. LinkSelFiE estimates link fidelity based on the feedback of a vanilla network benchmarking subroutine, and adaptively eliminates inferior links throughout the whole fidelity estimation process. This elimination leverages a novel confidence interval derived in this paper for the estimates from the subroutine, which theoretically guarantees that LinkSelFiE outputs the optimal link correctly with high confidence. We also establish a provable upper bound of cost complexity for LinkSelFiE. Moreover, we perform extensive simulations under various scenarios to corroborate that LinkSelFiE outperforms other existing methods in terms of both identifying the optimal link and reducing quantum resource consumption.}
}


@inproceedings{DBLP:conf/infocom/WangZHQ24,
	author = {Yangyu Wang and
                  Yangming Zhao and
                  Liusheng Huang and
                  Chunming Qiao},
	title = {Routing and Wavelength Assignment for Entanglement Swapping of Photonic
                  Qubits},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1431--1440},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621354},
	doi = {10.1109/INFOCOM52122.2024.10621354},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangZHQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient entanglement routing in Quantum Data Networks (QDNs) is essential in order to concurrently establish as many Entanglement Connections (ECs) as possible, which in turn maximizes the network throughput. In this work, we consider a new class of QDNs with wavelength division multiplexed (WDM) quantum links where each quantum repeater will perform entanglement swapping by measuring two photonic qubits coming from some entangled photon sources directly on the same wavelength. To address unique challenges in achieving a high network throughput in such QDNs, we propose QuRWA to jointly optimize the entanglement routing and wavelength assignment. To this end, we introduce a key concept named Co-Path to improve fault-tolerance: all ELs in a Co-Path set will be assigned the same wavelength and this may serve as backup for some other ELs in the same Co-Path when establishing ECs. We design efficient algorithms to optimize the Co-Path selection and wavelength assignment to maximize resource utilization and fault tolerance. Extensive simulations demonstrate that compared with the methods without introducing Co-Path, QuRWA improves the network throughput by up to 122%.}
}


@inproceedings{DBLP:conf/infocom/Meng0WTHLR24,
	author = {Qingkai Meng and
                  Shan Zhang and
                  Zhiyuan Wang and
                  Tao Tong and
                  Chaolei Hu and
                  Hongbin Luo and
                  Fengyuan Ren},
	title = {{BCC:} Re-architecting Congestion Control in DCNs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1441--1450},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621082},
	doi = {10.1109/INFOCOM52122.2024.10621082},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Meng0WTHLR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The nature of datacenter traffic is a high volume of bursty tiny flows and standing long flows, which forms the coexistence of transient and persistent congestion. Traditional congestion control (CC) algorithms have inherent limitations in reconciling fast response and high efficiency towards transients with stability and fairness during persistence. In this paper, we provide an insight that re-architects CC with two control laws, tailored to transient and persistent concerns, respectively. Armed with this key insight, we propose bimodal congestion control (BCC), which is founded on two core ideas: (i) Quaternary network state detection, which further distinguishes transient and persistent states in switches, and (ii) Bimodal control law, which is manifested as the transient controller and persistent controller at sources. The transient controller employs a precise control paradigm that pauses flows to drain backlogged packets and ramps down/up flow rates to bottleneck bandwidth directly, striving for high efficiency. The persistent controller grounds itself in traditional CC algorithms, inheriting stability and fairness. We implement BCC in the Linux kernel and P4-programmable switch. In our evaluation, compared to DCQCN, HPCC, PowerTCP, and Swift, BCC reduces flow completion times by 14% ~ 99%.}
}


@inproceedings{DBLP:conf/infocom/GiacomoniP24,
	author = {Luca Giacomoni and
                  George Parisis},
	title = {Reinforcement Learning-based Congestion Control: {A} Systematic Evaluation
                  of Fairness, Efficiency and Responsiveness},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1451--1460},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621288},
	doi = {10.1109/INFOCOM52122.2024.10621288},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GiacomoniP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reinforcement learning (RL)-based congestion control (CC) promises efficient CC in a fast-changing networking landscape, where evolving communication technologies, applications and traffic workloads pose severe challenges to human-derived, static CC algorithms. RL-based CC is in its early days and substantial research is required to understand existing limitations, identify research challenges and, eventually, yield deployable solutions for real-world networks. In this paper we present the first reproducible and systematic study of RL-based CC with the aim to highlight strengths and uncover fundamental limitations of the state-of-the-art. We identify challenges in evaluating RL-based CC, establish a methodology for studying said approaches and perform large-scale experimentation with RL-based CC approaches that are publicly available. We show that existing approaches can acquire all available bandwidth swiftly and are resistant to non-congestive loss, however, this is commonly at the cost of excessive packet loss in normal operation. We show that, as fairness is not embedded directly into reward functions, existing approaches exhibit unfairness in almost all tested network setups. Finally, we provide evidence that existing RL-based CC approaches under-perform when the available bandwidth and end-to-end latency dynamically change. Our experimentation codebase and datasets are publicly available with the aim to galvanise the community towards transparency and reproducibility, which have been recognised as crucial for researching and evaluating machine-generated policies.}
}


@inproceedings{DBLP:conf/infocom/DaiDFL024,
	author = {Wenkai Dai and
                  Michael Dinitz and
                  Klaus{-}Tycho Foerster and
                  Long Luo and
                  Stefan Schmid},
	title = {Approximation Algorithms for Minimizing Congestion in Demand-Aware
                  Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1461--1470},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621340},
	doi = {10.1109/INFOCOM52122.2024.10621340},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DaiDFL024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging reconfigurable optical communication technologies allow to enhance datacenter topologies with demand-aware links optimized towards traffic patterns. This paper studies the algorithmic problem of jointly optimizing topology and routing in such demand-aware networks to minimize congestion, along two dimensions: (1) splittable or unsplittable flows, and (2) whether routing is segregated, i.e., whether routes can or cannot combine both demand-aware and demand-oblivious (static) links.For splittable and segregated routing, we show that the problem is generally 2-approximable, but APX-hard even for uniform demands induced by a bipartite demand graph. For unsplittable and segregated routing, we establish upper and lower bounds of O (log m/ log log m) and Ω (log m/ log log m), respectively, for polynomial-time approximation algorithms, where m is the number of static links. We further reveal that under un-/splittable and non-segregated routing, even for demands of a single source (resp., d estina tion), the problem cannot be approximated better than \\Omega \\left({\\frac{{{c_{\\max }}}}{{{c_{\\min }}}}}\\right)\nunless P=NP, where c max (resp., c min ) denotes the maximum (resp., minimum) capacity. It remains NP-hard for uniform capacities, but is tractable for a single commodity and uniform capacities.Our trace-driven simulations show a significant reduction in network congestion compared to existing solutions.}
}


@inproceedings{DBLP:conf/infocom/ZhangY24,
	author = {Jinkun Zhang and
                  Edmund Yeh},
	title = {Congestion-aware Routing and Content Placement in Elastic Cache Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1471--1480},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621304},
	doi = {10.1109/INFOCOM52122.2024.10621304},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Caching can be leveraged to significantly improve network performance and mitigate congestion. However, characterizing the optimal tradeoff between routing cost and cache deployment cost remains an open problem. In this paper, for a network with arbitrary topology and congestion-dependent nonlinear cost functions, we aim to jointly determine the cache deployment, content placement, and hop-by-hop routing strategies, so that the sum of routing cost and cache deployment cost is minimized. We tackle this mixed-integer nonlinear problem starting with a fixed-routing setting, and then generalize to a dynamic-routing setting. For the fixed-routing setting, a Gradient-combining Frank-Wolfe algorithm with \\left( {\\frac{1}{2},1} \\right)\n-approximation is presented. For the general dynamic-routing setting, we obtain a set of KKT conditions, and devise a distributed and adaptive online algorithm based on these conditions. We demonstrate via extensive simulation that our algorithms significantly outperform a number of baselines.}
}


@inproceedings{DBLP:conf/infocom/LiZXZS024,
	author = {Danyang Li and
                  Yishujie Zhao and
                  Jingao Xu and
                  Shengkai Zhang and
                  Longfei Shangguan and
                  Zheng Yang},
	title = {edgeSLAM2: Rethinking Edge-Assisted Visual {SLAM} with On-Chip Intelligence},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1481--1490},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621278},
	doi = {10.1109/INFOCOM52122.2024.10621278},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiZXZS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge-assisted visual SLAM stands as a pivotal enabler for emerging mobile applications, such as search-and-rescue, smart logistics, and industrial inspection. Limited by the computing capability of lightweight mobile devices like MAVs, current innovations balance system accuracy and efficiency by allocating lightweight and time-sensitive tracking tasks to mobile devices, while offloading the more resource-intensive yet delay-tolerant map optimization tasks to the edge. However, our pilot study in a large-scale oil field reveals several limitations of such a tracking-optimization decoupled paradigm, arising due to the disruption of inter-dependencies between the two tasks concerning data, resources, and threads.In this paper, we design and implement edgeSLAM2, an innovative system that reshapes the edge-assisted visual SLAM paradigm by tightly integrating tracking and partial-yet-crucial optimization on mobile. edgeSLAM2 harnesses the hierarchical and heterogeneous computing units offered by the latest commercial systems-on-chip (SoCs) to enhance the computational capacity of mobile devices, which in turn, allows edgeSLAM2 to design a suit of novel algorithms for map sync, optimization, and tracking that accommodate such architectural upgrade. By fully embracing the on-chip intelligence, edgeSLAM2 simultaneously enhances system accuracy and efficiency through software-hardware co-design. We deploy edgeSLAM2 on an industrial drone and conduct comprehensive experiments in a large-scale oil field over three months. The results show that edgeSLAM2 surpasses comparative methods by achieving an 80% reduction in bandwidth consumption, a 32% improvement in accuracy, and a 26% reduction in tracking delay.}
}


@inproceedings{DBLP:conf/infocom/FengSWXXXW24,
	author = {Yicheng Feng and
                  Shihao Shen and
                  Xiaofei Wang and
                  Qiao Xiang and
                  Hong Xu and
                  Chenren Xu and
                  Wenyu Wang},
	title = {{BREAK:} {A} Holistic Approach for Efficient Container Deployment
                  among Edge Clouds},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1491--1500},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621084},
	doi = {10.1109/INFOCOM52122.2024.10621084},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FengSWXXXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Container technology has revolutionized service deployment, offering streamlined processes and enabling container orchestration platforms to manage a growing number of container clusters. However, the deployment of containers in distributed edge clusters presents challenges due to their unique characteristics, such as bandwidth limitations and resource constraints. Existing approaches designed for cloud environments often fall short in addressing the specific requirements of edge computing. Additionally, very few edge-oriented solutions explore fundamental changes to the container design, resulting in difficulties achieving backward compatibility.In this paper, we reevaluate the fundamental layer-based structure of containers. We identify that the proliferation of redundant files and operations within image layers hinders efficient container deployment. Drawing upon the crucial insight of enhancing layer reuse and extracting benefits from it, we introduce BREAK, a holistic approach centered on layer structure throughout the entire container deployment pipeline, ensuring backward compatibility. BREAK refactors image layers and proposes an edge-oriented cache solution to enable ubiquitous and shared layers. Moreover, it addresses the complete deployment pipeline by introducing a customized scheduler and a tailored storage driver. Our results demonstrate that BREAK accelerates the deployment process by up to 2.1× and reduces redundant image size by up to 3.11× compared to state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/infocom/HeYQ24,
	author = {Xingqiu He and
                  Chaoqun You and
                  Tony Q. S. Quek},
	title = {Exploiting Storage for Computing: Computation Reuse in Collaborative
                  Edge Computing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1501--1510},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621100},
	doi = {10.1109/INFOCOM52122.2024.10621100},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HeYQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative Edge Computing (CEC) is a new edge computing paradigm that enables neighboring edge servers to share computational resources with each other. Although CEC can enhance the utilization of computational resources, it still suffers from resource waste. The primary reason is that end-users from the same area are likely to offload similar tasks to edge servers, thereby leading to duplicate computations. To improve system efficiency, the computation results of previously executed tasks can be cached and then reused by subsequent tasks. However, most existing computation reuse algorithms only consider one edge server, which significantly limits the effectiveness of computation reuse. To address this issue, this paper applies computation reuse in CEC networks to exploit the collaboration among edge servers. We formulate an optimization problem that aims to minimize the overall task response time and decompose it into a caching subproblem and a scheduling subproblem. By analyzing the properties of optimal solutions, we show that the optimal caching decisions can be efficiently searched using the bisection method. For the scheduling subproblem, we utilize projected gradient descent and backtracking to find a local minimum. Numerical results show that our algorithm significantly reduces the response time in various situations.}
}


@inproceedings{DBLP:conf/infocom/Wang0ST24,
	author = {Bin Wang and
                  David E. Irwin and
                  Prashant J. Shenoy and
                  Don Towsley},
	title = {{INVAR:} Inversion Aware Resource Provisioning and Workload Scheduling
                  for Edge Computing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1511--1520},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621417},
	doi = {10.1109/INFOCOM52122.2024.10621417},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Wang0ST24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing is emerging as a complementary architecture to cloud computing to address some of its associated issues. One of the major advantages of edge computing is that edge data centers are usually much closer to users compared to traditional cloud data centers. Therefore, it is commonly believed that for developers of latency-sensitive applications, they can effectively reduce the overall end-to-end latency by simply transitioning from a cloud deployment to an edge deployment. However, as recent work has shown, the performance of an edge deployment is vulnerable to a couple of factors which under many practical scenarios can lead to edge servers providing worse end-to-end response time than cloud servers. This phenomenon is referred to as edge performance inversion. In this paper, we propose resource allocation and workload scheduling algorithms that actively prevent edge performance inversion. Our algorithms, named INVAR, are based on queueing theory results and optimization techniques. Evaluation results show that INVAR can find a near-optimal solution that outperforms the performance of a cloud deployment by an adjustable margin. Simulation results based on production workloads from Akamai data centers show that INVAR can outperform common heuristic-based edge deployment by 11% to 24% in real-world scenarios.}
}


@inproceedings{DBLP:conf/infocom/FiandrinoGPMFW24,
	author = {Claudio Fiandrino and
                  Eloy P{\'{e}}rez G{\'{o}}mez and
                  Pablo Fern{\'{a}}ndez P{\'{e}}rez and
                  Hossein Mohammadalizadeh and
                  Marco Fiore and
                  Joerg Widmer},
	title = {AIChronoLens: Advancing Explainability for Time Series {AI} Forecasting
                  in Mobile Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1521--1530},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621134},
	doi = {10.1109/INFOCOM52122.2024.10621134},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FiandrinoGPMFW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next-generation mobile networks will increasingly rely on the ability to forecast traffic patterns for resource management. Usually, this translates into forecasting diverse objectives like traffic load, bandwidth, or channel spectrum utilization, measured over time. Among the other techniques, Long-Short Term Memory (LSTM) proved very successful for this task. Unfortunately, the inherent complexity of these models makes them hard to interpret and, thus, hampers their deployment in production networks. To make the problem worsen, EXplainable Artificial Intelligence (XAI) techniques, which are primarily conceived for computer vision and natural language processing, fail to provide useful insights: they are blind to the temporal characteristics of the input and only work well with highly rich semantic data like images or text. In this paper, we take the research on XAI for time series forecasting one step further proposing AIChronoLens, a new tool that links legacy XAI explanations with the temporal properties of the input. In such a way, AIChronoLens makes it possible to dive deep into the model behavior and spot, among other aspects, the hidden cause of errors. Extensive evaluations with real-world mobile traffic traces pinpoint model behaviors that would not be possible to spot otherwise and model performance can increase by 32%.}
}


@inproceedings{DBLP:conf/infocom/MishraZMMZF24,
	author = {Sachit Mishra and
                  Andr{\'{e}} Felipe Zanella and
                  Orlando E. Mart{\'{\i}}nez{-}Durive and
                  Diego Madariaga and
                  Cezary Ziemlicki and
                  Marco Fiore},
	title = {Characterizing 5G Adoption and its Impact on Network Traffic and Mobile
                  Service Consumption},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1531--1540},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621344},
	doi = {10.1109/INFOCOM52122.2024.10621344},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MishraZMMZF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The roll out of 5G, coupled with the traffic monitoring capabilities of modern industry-grade networks, offers an unprecedented opportunity to closely observe the impact that the introduction of a new major wireless technology has on the end users. In this paper, we seize such a unique chance, and carry out a first-of-its-kind in-depth analysis of 5G adoption along spatial, temporal and service dimensions. Leveraging massive measurement data about application-level demands collected in a nationwide 4G/5G network, we characterize the impact of the new technology on when, where and how mobile subscribers consume 5G traffic both in aggregate and for individual types of services. This lets us unveil the overall incidence of 5G in the total mobile network traffic, its spatial and temporal fluctuations, its effect on the way 5G services are consumed, the way individual services and geographical locations contribute to fluctuations in the 5G demand, as well as surprising connections between socioeconomic status of local populations and the way the 5G technology is presently consumed.}
}


@inproceedings{DBLP:conf/infocom/Jin0JSG24,
	author = {Lewei Jin and
                  Wei Dong and
                  Bowen Jiang and
                  Tong Sun and
                  Yi Gao},
	title = {Exploiting Multiple Similarity Spaces for Efficient and Flexible Incremental
                  Update of Mobile Apps},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1541--1550},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621326},
	doi = {10.1109/INFOCOM52122.2024.10621326},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Jin0JSG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile application updates occur frequently, and they continue to add considerable traffic over the Internet. Differencing algorithms, which compute a small delta between the new version and the old version, are often employed to reduce the update overhead. Transforming the old and new files into the decoded similarity spaces can drastically reduce the delta size. However, this transformation is often hindered by two practical reasons: (1) insufficient decoding (2) long recompression time. To address this challenge, we have proposed two general approaches to transforming the compressed files (more specifically, deflate stream) into the full decoded similarity space and partial decoded similarity space, with low recompression time. The first approach uses recompression-aware searching mechanism, based on a general full decoding tool to transform deflate stream to the full decoded similarity space with a configurable searching complexity, even when it cannot be recompressed identically. The second approach uses a novel solution to transform a deflate stream into the partial decoded similarity space with differencing-friendly LZ77 token reencoding. We have also proposed an algorithm called MDiffPatch to exploit the full and partial decoded similarity spaces. The algorithm can well balance compression ratio and recompression time by exposing a tunable parameter. Extensive evaluation results show that MDiffPatch achieves lower compression ratio than state-of-the-art algorithms and its tunable parameter allows us to achieve a good tradeoff between compression ratio and recompression time.}
}


@inproceedings{DBLP:conf/infocom/ZhuZ24,
	author = {Yinan Zhu and
                  Qian Zhang},
	title = {LoPrint: Mobile Authentication of RFID-Tagged Items Using {COTS} Orthogonal
                  Antennas},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1551--1560},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621362},
	doi = {10.1109/INFOCOM52122.2024.10621362},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhuZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Authenticating RFID-tagged items during mobile inventory, such as entering or leaving the warehouse, is a critical task for anti-counterfeiting. However, past authentication solutions using commercial off-the-shelf (COTS) devices cannot be applied in mobile scenarios, such as conveyors or tunnels, due to either high latency or non-robustness to tag movement. This paper introduces LoPrint, the first system to effectively authenticate mobile tagged items using the COTS orthogonal antennas existing in most infrastructures. The key insight of LoPrint is to randomly attach multiple tags on each item as a tag group and leverage the stable layout relationships of this tag group as novel fingerprints, including the relative distance matrix (RDM) and relative orientation matrix (ROM). Additionally, a new hardware fingerprint called cross-polarization ratio (CPR) is proposed to help distinguish the tag category. Furthermore, a lightweight approach is designed to robustly extract RDM, ROM, and CPR from RSSI and phase sequences under various environmental factors. LoPrint is prototyped and deployed on a conveyor in a lab environment and a tunnel in a real-world RFID warehouse, where 726 tagged items with random layouts are used for evaluation. Experimental results show that LoPrint can achieve a high authentication accuracy of 82.92% on the fixed conveyor and 79.48% on the random warehouse trolley when the size of tag group is three, outperforming the transferred stateof-the-art solution by over 10×.}
}


@inproceedings{DBLP:conf/infocom/MaG0S024,
	author = {Tianyu Ma and
                  Guoju Gao and
                  He Huang and
                  Yu{-}e Sun and
                  Yang Du},
	title = {Scout Sketch: Finding Promising Items in Data Streams},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1561--1570},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621279},
	doi = {10.1109/INFOCOM52122.2024.10621279},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MaG0S024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies a new but important pattern for items in data streams, called promising items. The promising items mean that the frequencies of an item in multiple continuous time windows show an upward trend overall, while a slight decrease in some of these windows is allowed. Many practical applications can benefit from the property of promising items, e.g., detecting potential hot events or news in social networks, preventing network congestion in communication channels, and monitoring latent attacks in computer networks. To accurately find promising items in data streams in real-time under limited memory space, we propose a novel structure named Scout Sketch, which consists of Filter and Finder. Filter is devised based on the Bloom filter to eliminate the ungratified items with less memory overload; Finder records some necessary information about the potential items and detects the promising items at the end of each time window, where we propose some tailor-made detection operations. We also analyze the theoretical performance of Scout Sketch. Finally, we conducted extensive experiments based on four real-world datasets. The experimental results show that the F1 Score and throughput of Scout Sketch are about 2.02 and 5.61 times that of the compared solutions, respectively.}
}


@inproceedings{DBLP:conf/infocom/ParkHKLHL24,
	author = {Shinik Park and
                  Sanghyun Han and
                  Junseon Kim and
                  Jongyun Lee and
                  Sangtae Ha and
                  Kyunghan Lee},
	title = {Exstream: {A} Delay-minimized Streaming System with Explicit Frame
                  Queueing Delay Measurement},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1571--1580},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621103},
	doi = {10.1109/INFOCOM52122.2024.10621103},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ParkHKLHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network fluctuations can cause unpredictable degradation of the user’s quality of experience (QoE) on real-time video streaming. The intrinsic property of real-time video streaming, which generates delay-sensitive and chunk-based video frames, makes the situation even more complicated. Although previous approaches have tried to alleviate this problem by controlling the video bitrate based on the current network capacity estimate, they do not take into account the explicit queueing delay experienced by the video frame in determining the bitrate of upcoming video frames. To tackle this problem, we propose a new real-time video streaming system, Exstream, that can adapt to dynamic network conditions with the help of video bitrate control method and bandwidth estimation method designed to support real-time video streaming environments. Exstream explicitly estimates the queueing delay experienced by the video frame based on the transmission time budget that each frame can maximally utilize, which depends on the frame generation interval, and adjusts the bitrate of newly generated video frames to suppress the queueing delay level close to zero. Our comprehensive experiments demonstrate that Exstream achieves lower frame delay than four existing systems, Salsify, WebRTC, Skype, and Hangouts without frequent video frame skip.}
}


@inproceedings{DBLP:conf/infocom/DouWM24,
	author = {Rengan Dou and
                  Xin Wang and
                  Richard T. B. Ma},
	title = {Emma: Elastic Multi-Resource Management for Realtime Stream Processing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1581--1590},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621313},
	doi = {10.1109/INFOCOM52122.2024.10621313},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DouWM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In stream processing applications, an operator is often instantiated into multiple parallel execution instances, referred to as executors, to facilitate large-scale data processing. Due to unpredictable changes in executor workloads, data tuples processed by different executors may exhibit varying latency. In particular, within the same operator, the executor with the maximum latency significantly impacts the end-to-end (E2E) latency of the application. Existing solutions, such as load balancing and horizontal scaling, which involve workload migration, often incur substantial time overhead induced by state migration and synchronization. In contrast, elastically scaling up/down resources of executors rather than moving workloads can not only effectively handle workload fluctuations but also offer rapid adjustments; however, prior works only considered CPU scaling with the assumption of sufficient memory.In this paper, we propose Emma, an elastic multi-resource manager. Emma leverages the resource elasticity of lightweight virtualization containers, e.g., Linux containers, to resize the resource of executors at runtime. The core of Emma is a multi-resource provisioning plan that conducts performance analysis and resource adjustment in real-time. We explore the relationship between resources and performance experimentally and theoretically, guiding the plan to adaptively allocate the appropriate combination of resources to each executor to 1) accommodate the dynamic workload; 2) efficiently utilize resources to enhance the performance of as many executors as possible. Additionally, we propose an online learning method that makes the manager seamlessly adapt to diverse stream applications. We integrate Emma with Apache Samza, and our experiments show that compared to existing solutions, Emma can significantly reduce latency by orders of magnitude in real-world applications.}
}


@inproceedings{DBLP:conf/infocom/KhanDANSS24,
	author = {Nouman Khan and
                  Ujwal Dinesha and
                  Subrahmanyam Arunachalam and
                  Dheeraj Narasimha and
                  Vijay G. Subramanian and
                  Srinivas Shakkottai},
	title = {A Multi-Agent View of Wireless Video Streaming with Delayed Client-Feedback},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1591--1600},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621216},
	doi = {10.1109/INFOCOM52122.2024.10621216},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/KhanDANSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the optimal control of multiple video streams over a wireless downlink from a base-transceiver-station (BTS)/access point to N end-devices (EDs). The BTS sends video packets to each ED under a joint transmission energy constraint, the EDs choose when to play out the received packets, and the collective goal is to provide a high Quality-of-Experience (QoE) to the clients/end-users. All EDs send feedback about their states and actions to the BTS which reaches it after a fixed deterministic delay. We analyze this team problem with delayed feedback as a cooperative Multi-Agent Constrained Partially Observable Markov Decision Process (MA-C-POMDP).First, using a recently established strong duality result for MAC-POMDPs, the original problem is decomposed into N independent unconstrained transmitter-receiver (two-agent) problems— all sharing a Lagrange multiplier (that also needs to be optimized for optimal control). Thereafter, the common information (CI) approach and the formalism of approximate information states (AISs) are used to guide the design of a neural-network based architecture for learning-based multi-agent control in a single unconstrained transmitter-receiver problem. Finally, simulations on a single transmitter-receiver pair with a stylized QoE model are performed to highlight the advantage of delay-aware two-agent coordination over the transmitter choosing both transmission and play-out actions (perceiving the delayed state of the receiver as its current state).}
}


@inproceedings{DBLP:conf/infocom/HoqueR24,
	author = {Naureen Hoque and
                  Hanif Rahbari},
	title = {Deep Learning Models as Moving Targets to Counter Modulation Classification
                  Attacks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1601--1610},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621413},
	doi = {10.1109/INFOCOM52122.2024.10621413},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HoqueR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Malicious entities abuse advanced modulation classification (MC) techniques to launch traffic analysis, selective jamming, evasion, and poison attacks. Recent studies show that current defenses against such attacks are static in nature and vulnerable to persistent adversaries who invest time and resources into learning the defenses, thereby being able to design and execute more sophisticated attacks to circumvent them. In this paper, we present a moving-target defense framework to support a novel modulation-masking mechanism we develop against advanced and persistent MC attacks. The modulated symbols are first masked using small perturbations to make them appear to an adversary in a state of ambiguity about the model as if they are from another modulation scheme. By deploying a pool of deep learning models and perturbation-generating techniques, our defense strategy keeps changing (moving) them as needed, making it difficult (cubic time complexity) for adversaries to keep up with the evolving defense system over time. We show that the overall system performance remains unaffected under our technique. We further demonstrate that, over time, a persistent adversary can learn and eventually circumvent our masking technique, along with other existing defenses, unless a moving target defense approach is adopted.}
}


@inproceedings{DBLP:conf/infocom/KimMG24,
	author = {Byungjun Kim and
                  Christoph F. Mecklenbr{\"{a}}uker and
                  Peter Gerstoft},
	title = {Deep Learning-based Modulation Classification of Practical {OFDM}
                  Signals for Spectrum Sensing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1611--1620},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621421},
	doi = {10.1109/INFOCOM52122.2024.10621421},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/KimMG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this study, the modulation of symbols on OFDM subcarriers is classified for transmissions following Wi-Fi 6 and 5G downlink specifications. First, our approach estimates the OFDM symbol duration and cyclic prefix length based on the cyclic autocorrelation function. We propose a feature extraction algorithm characterizing the modulation of OFDM signals, which includes removing the effects of a synchronization error. The obtained feature is converted into a 2D histogram of phase and amplitude and this histogram is taken as input to a convolutional neural network (CNN)-based classifier. The classifier does not require prior knowledge of protocol-specific information such as Wi-Fi preamble or resource allocation of 5G physical channels. The classifier’s performance, evaluated using synthetic and real-world measured over-the-air (OTA) datasets, achieves a minimum accuracy of 97% accuracy with OTA data when SNR is above the value required for data transmission.}
}


@inproceedings{DBLP:conf/infocom/0001WMLC24,
	author = {Chetna Singhal and
                  Y. Wu and
                  Francesco Malandrino and
                  Marco Levorato and
                  Carla{-}Fabiana Chiasserini},
	title = {Resource-aware Deployment of Dynamic DNNs over Multi-tiered Interconnected
                  Systems},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1621--1630},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621218},
	doi = {10.1109/INFOCOM52122.2024.10621218},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001WMLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing pervasiveness of intelligent mobile applications requires to exploit the full range of resources offered by the mobile-edge-cloud network for the execution of inference tasks. However, due to the heterogeneity of such multi-tiered networks, it is essential to make the applications’ demand amenable to the available resources while minimizing energy consumption. Modern dynamic deep neural networks (DNN) achieve this goal by designing multi-branched architectures where early exits enable sample-based adaptation of the model depth. In this paper, we tackle the problem of allocating sections of DNNs with early exits to the nodes of the mobile-edge-cloud system. By envisioning a 3-stage graph-modeling approach, we represent the possible options for splitting the DNN and deploying the DNN blocks on the multi-tiered network, embedding both the system constraints and the application requirements in a convenient and efficient way. Our framework – named Feasible Inference Graph (FIN) – can identify the solution that minimizes the overall inference energy consumption while enabling distributed inference over the multi-tiered network with the target quality and latency. Our results, obtained for DNNs with different levels of complexity, show that FIN matches the optimum and yields over 65% energy savings relative to a state-of-the-art technique for cost minimization.}
}


@inproceedings{DBLP:conf/infocom/AkemBGF24,
	author = {Aristide Tanyi{-}Jong Akem and
                  Beyza B{\"{u}}t{\"{u}}n and
                  Michele Gucciardo and
                  Marco Fiore},
	title = {Jewel: Resource-Efficient Joint Packet and Flow Level Inference in
                  Programmable Switches},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1631--1640},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621365},
	doi = {10.1109/INFOCOM52122.2024.10621365},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/AkemBGF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Embedding machine learning (ML) models in programmable switches realizes the vision of high-throughput and low-latency inference at line rate. Recent works have made breakthroughs in embedding Random Forest (RF) models in switches for either packet-level inference or flow-level inference. The former relies on simple features from packet headers that are simple to implement but limit accuracy in challenging use cases; the latter exploits richer flow features to improve accuracy, but leaves early packets in each flow unclassified. We propose Jewel, an in-switch ML model based on a fully joint packet-and flow-level design, which takes the best of both worlds by classifying early flow packets individually and shifting to flow-level inference when possible. Our proposal involves (i) a single RF model trained to classify both packets and flows, and (ii) hardware-aware model selection and training techniques for resource footprint minimization. We implement Jewel in P4 and deploy it in a testbed with Intel Tofino switches, where we run extensive experiments with a variety of real-world use cases. Results reveal how our solution outperforms four state-of-the-art benchmarks, with accuracy gains in the 2.0%–5.3% range.}
}


@inproceedings{DBLP:conf/infocom/YangC24,
	author = {Tao Yang and
                  Zhiping Cai},
	title = {Efficient IPv6 Router Interface Discovery},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1641--1650},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621168},
	doi = {10.1109/INFOCOM52122.2024.10621168},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient discovery of router interfaces on the IPv6 Internet is critical for network measurement and cybersecurity. However, existing solutions commonly suffer from inefficiencies due to a lack of initial probing targets (seeds), ultimately exhibiting limitations on large-scale IPv6 networks. Therefore, it is imperative to develop a methodology that enables the efficient collection of IPv6 router interfaces with limited resources, considering the impracticality of conducting a brute-force exploration across the extensive IPv6 address space. In this paper, we introduce Treestrace, an innovative asynchronous prober specifically designed for this purpose. Without prior knowledge of the networks, this tool incrementally adjusts search directions, automatically prioritizing the survey of IPv6 address spaces with a higher concentration of IPv6 router interfaces. Furthermore, we have developed a carefully crafted architecture optimized for probing performance, allowing the tool to probe at the highest theoretically possible rate without requiring excessive computational resources. Real-world tests show that Treestrace outperforms state-of-the-art works on both seed-based and seedless tasks, achieving at least a 5.57-fold efficiency improvement on large-scale IPv6 router interface discovery. With Treestrace, we discovered approximately 8 million IPv6 router interface addresses from a single vantage point within several hours.}
}


@inproceedings{DBLP:conf/infocom/LiLMLQLG24,
	author = {Jianfeng Li and
                  Zheng Lin and
                  Xiaobo Ma and
                  Jianhao Li and
                  Jian Qu and
                  Xiapu Luo and
                  Xiaohong Guan},
	title = {DNSScope: Fine-Grained {DNS} Cache Probing for Remote Network Activity
                  Characterization},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1651--1660},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621277},
	doi = {10.1109/INFOCOM52122.2024.10621277},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiLMLQLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The domain name system (DNS) is indispensable to nearly every Internet service. It has been extensively utilized for network activity characterization in passive and active approaches. Compared to the passive approach, active DNS cache probing is privacy-preserving and low-cost, enabling worldwide characterization of remote network activities in different networks. Unfortunately, existing probing-based methods are too coarse-grained to characterize the time-varying features of network activities, substantially limiting their applications in time-sensitive tasks. In this paper, we advance DNSScope, a fine-grained DNS cache probing framework by tackling three challenges: sample sparsity, observational distortion, and cache entanglement. DNSScope synthesizes statistical learning and self-supervised transfer learning to achieve time-varying characterization. Extensive evaluations demonstrate that it can accurately estimate the time-varying DNS query arrival rates on recursive DNS resolvers. Its average mean absolute error is 0.124, as low as one-sixth that of the baseline methods.}
}


@inproceedings{DBLP:conf/infocom/AndersonMBCS24,
	author = {Alex Anderson and
                  Aadi Swadipto Mondal and
                  Paul Barford and
                  Mark Crovella and
                  Joel Sommers},
	title = {An Elemental Decomposition of {DNS} Name-to-IP Graphs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1661--1670},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621147},
	doi = {10.1109/INFOCOM52122.2024.10621147},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/AndersonMBCS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Domain Name System (DNS) is a critical piece of Internet infrastructure with remarkably complex properties and uses, and accordingly has been extensively studied. In this study we contribute to that body of work by organizing and analyzing records maintained within the DNS as a bipartite graph. We find that relating names and addresses in this way uncovers a surprisingly rich structure. In order to characterize that structure, we introduce a new graph decomposition for DNS name-to-IP mappings, which we term elemental decomposition. In particular, we argue that (approximately) decomposing this graph into bicliques — maximally connected components — exposes this rich structure. We utilize large-scale censuses of the DNS to investigate the characteristics of the resulting decomposition, and illustrate how the exposed structure sheds new light on a number of questions about how the DNS is used in practice and suggests several new directions for future research.}
}


@inproceedings{DBLP:conf/infocom/BianJH0C24,
	author = {Rui Bian and
                  Lin Jin and
                  Shuai Hao and
                  Haining Wang and
                  Chase Cotton},
	title = {Silent Observers Make a Difference: {A} Large-scale Analysis of Transparent
                  Proxies on the Internet},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1671--1680},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621441},
	doi = {10.1109/INFOCOM52122.2024.10621441},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/BianJH0C24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transparent web proxies have been widely deployed on the Internet, bridging the communications between clients and servers and providing desirable benefits to both sides, such as load balancing, security monitoring, and privacy enhancement. Meanwhile, they work silently as clients and servers may not be aware of their existence. However, due to their invisibility and stealthiness, transparent proxies remain understudied for their behaviors, suspicious activities, and potential vulnerabilities that could be exploited by attackers. To better understand transparent proxies, we design and develop a framework to systematically investigate them in the wild. We identify two major types of transparent web proxies, named FDR and CPV, respectively. FDR is a type of transparent proxy that independently performs Forced DNS Resolution during interception. CPV is a type of transparent proxy that presents Cache Poisoning Vulnerability. We perform a large-scale measurement to detect each type of transparent web proxy and scrutinize their security implications. In total, we observe 32,246 FDR and 11,286 CPV cases through our acquired vantage points. We confirm that these two types of transparent proxies are distributed globally — FDRs are observed in 98 countries and CPVs are observed in 51 countries. Our work highlights the issues of vulnerable transparent proxies and provides insights for mitigating such problems.}
}


@inproceedings{DBLP:conf/infocom/QiL24,
	author = {Xiaodong Qi and
                  Yi Li},
	title = {LightCross: Sharding with Lightweight Cross-Shard Execution for Smart
                  Contracts},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1681--1690},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621127},
	doi = {10.1109/INFOCOM52122.2024.10621127},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QiL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sharding is a prevailing solution to enhance the scalability of current blockchain systems. However, the cross-shard commit protocols adopted in these systems to commit cross-shard transactions commonly incur multi-round shard-to-shard communication, leading to low performance. Furthermore, most solutions only focus on simple transfer transactions without supporting complex smart contracts, preventing sharding from widespread applications. In this paper, we propose LightCross, a novel sharding blockchain system that enables efficient execution of complex cross-shard smart contracts. First, LightCross offloads the execution of cross-shard transactions into off-chain executors equipped with the TEE hardware, which can accommodate execution for arbitrarily complex contracts. Second, we design a lightweight cross-shard commit protocol to commit cross-shard transactions without multi-round shard-to-shard communication between shards. Last, LightCross lowers the cross-shard transaction ratio by dynamically changing the distribution of contracts according to historical transactions. We implemented the LightCross prototype based on the FISCO-BCOS project and evaluated it in real-world blockchain environments, showing that LightCross can achieve 2.6× more throughput than state-of-the-art sharding systems.}
}


@inproceedings{DBLP:conf/infocom/WongZNLCYLLW24,
	author = {Taiyu Wong and
                  Chao Zhang and
                  Yuandong Ni and
                  Mingsen Luo and
                  Heying Chen and
                  Yufei Yu and
                  Weilin Li and
                  Xiapu Luo and
                  Haoyu Wang},
	title = {ConFuzz: Towards Large Scale Fuzz Testing of Smart Contracts in Ethereum},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1691--1700},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621414},
	doi = {10.1109/INFOCOM52122.2024.10621414},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WongZNLCYLLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fuzzing is effective at finding vulnerabilities in traditional applications and has been adapted to smart contracts. However, existing fuzzing solutions for smart contracts are not smart enough and can hardly be applied to large-scale testing since they heavily rely on source code or ABI. In this paper, we propose a fuzzing solution ConFuzz applicable to large-scale testing, especially for bytecode-only contracts. ConFuzz adopts Adaptive Interface Recovery (AIR) and Function Information Collection (FIC) algorithm to automatically recover the function interfaces and information, supporting fuzzing smart contracts without source code or ABI. Furthermore, ConFuzz employs a Dependence-based Transaction Sequence Generation (DTSG) algorithm to infer dependencies of transactions and generate high-quality sequences to trigger the vulnerabilities. Lastly, ConFuzz utilizes taint analysis and function information to help detect harmful vulnerabilities and reduce false positives. The experiment shows that ConFuzz can accurately recover over 99.7% of function interfaces and reports more vulnerabilities than state-of-the-art solutions with 98.89% precision and 93.69% accuracy. On all 1.4M unique contracts from Ethereum, ConFuzz found over 11.92% vulnerable contracts. To the best of our knowledge, ConFuzz is the first efficient and scalable solution to test all smart contracts deployed in Ethereum.}
}


@inproceedings{DBLP:conf/infocom/WangYDLZF24,
	author = {Shan Wang and
                  Ming Yang and
                  Wenxuan Dai and
                  Yu Liu and
                  Yue Zhang and
                  Xinwen Fu},
	title = {Deanonymizing Ethereum Users behind Third-Party {RPC} Services},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1701--1710},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621236},
	doi = {10.1109/INFOCOM52122.2024.10621236},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangYDLZF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Third-party RPC services have become the mainstream way for users to access Ethereum. In this paper, we present a novel deanonymization attack that can link an Ethereum address to a real-world identity such as IP address of a user who accesses Ethereum via a third-party RPC service. We find that RPC API calls result in distinguishable sizes of encrypted TCP packets. An attacker can then find when a user sends a transaction to an RPC provider and immediately send a beacon transaction after the user transaction. By exploiting the differences in the distributions of inter-arrival time intervals of normal transactions and two simultaneously initiated transactions, the attacker can identify the victim transaction in the Ethereum network. This enables the attacker to correlate the Ethereum address of the victim transaction’s initiator with the source IP address of TCP packets from a victim user. We model the attack through empirical measurements and conduct extensive real-world experiments to validate the effectiveness of our attack. With three optimization strategies, the correlation accuracy can reach to 98.70% and 96.60% respectively in Ethereum testnet and mainnet. We are the first to study the deanonymization of Ethereum users behind third-party RPC services.}
}


@inproceedings{DBLP:conf/infocom/ZhaoZZWS024,
	author = {Chonghe Zhao and
                  Yipeng Zhou and
                  Shengli Zhang and
                  Taotao Wang and
                  Quan Z. Sheng and
                  Song Guo},
	title = {DEthna: Accurate Ethereum Network Topology Discovery with Marked Transactions},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1711--1720},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621281},
	doi = {10.1109/INFOCOM52122.2024.10621281},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoZZWS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Ethereum, the ledger exchanges messages along an underlying Peer-to-Peer (P2P) network to reach consistency. Understanding the underlying network topology of Ethereum is crucial for network optimization, security and scalability. However, the accurate discovery of Ethereum network topology is non-trivial due to its deliberately designed security mechanism. Consequently, existing measuring schemes cannot accurately infer the Ethereum network topology with a low cost. To address this challenge, we propose the Distributed Ethereum Network Analyzer (DEthna) tool, which can accurately and efficiently measure the Ethereum network topology. In DEthna, a novel parallel measurement model is proposed that can generate marked transactions to infer link connections based on the transaction replacement and propagation mechanism in Ethereum. Moreover, a workload offloading scheme is designed so that DEthna can be deployed on multiple distributed probing nodes so as to measure a large-scale Ethereum network at a low cost. We run DEthna on Goerli (the most popular Ethereum test network) to evaluate its capability in discovering network topology. The experimental results demonstrate that DEthna significantly outperforms the state-of-the-art baselines. Based on DEthna, we further analyze characteristics of the Ethereum network revealing that there exist more than 50% low-degree Ethereum nodes that weaken the network robustness.}
}


@inproceedings{DBLP:conf/infocom/WangA024,
	author = {Suyang Wang and
                  Oluwaseun T. Ajayi and
                  Yu Cheng},
	title = {An Analytical Approach for Minimizing the Age of Information in a
                  Practical {CSMA} Network},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1721--1730},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621330},
	doi = {10.1109/INFOCOM52122.2024.10621330},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangA024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Age of information (AoI) is a crucial metric in modern communication systems, quantifying the information freshness at the receiver side. This study proposes a novel and general approach utilizing stochastic hybrid systems (SHS) for AoI analysis and minimization in carrier sense multiple access (CSMA) networks. Specifically, we consider a practical networking scenario where multiple nodes contend for transmission through a standard CSMA-based medium access control (MAC) protocol, and the tagged node under consideration uses a small transmission buffer for a low AoI. We for the first time develop an SHS-based analytical model for this finite-buffer transmission system over the CSMA MAC. Moreover, we develop a creative method to incorporate the collision probability into the SHS model, with background nodes having heterogeneous traffic arrival rates. This new model enables us to analytically find the optimal sampling rate to minimize the AoI of the tagged node in a wide range of practical networking scenarios. Our analysis reveals insights into buffer size impacts when jointly optimizing throughput and AoI. The SHS model is cast over an 802.11-based MAC to examine the performance, with comparison to ns-based simulation results. The accuracy of the modeling and the efficiency of optimal sampling are convincingly demonstrated.}
}


@inproceedings{DBLP:conf/infocom/Li0LLZJ024,
	author = {Yijun Li and
                  Jiawei Huang and
                  Zhaoyi Li and
                  Jingling Liu and
                  Shengwen Zhou and
                  Wanchun Jiang and
                  Jianxin Wang},
	title = {Gsyn: Reducing Staleness and Communication Waiting via Grouping-based
                  Synchronization for Distributed Deep Learning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1731--1740},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621250},
	doi = {10.1109/INFOCOM52122.2024.10621250},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Li0LLZJ024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed deep learning has been widely employed to train deep neural network over large-scale dataset. However, the commonly used parameter server architecture suffers from long synchronization time in data-parallel training. Although the existing solutions are proposed to reduce synchronization overhead by breaking the synchronization barriers or limiting the staleness bound, they inevitably experience low convergence efficiency and long synchronization waiting. To address these problems, we propose Gsyn to reduce both synchronization overhead and staleness. Specifically, Gsyn divides workers into multiple groups. The workers in the same group coordinate with each other using the bulk synchronous parallel scheme to achieve high convergence efficiency, and each group communicates with parameter server asynchronously to reduce the synchronization waiting time, consequently increasing the convergence efficiency. Furthermore, we theoretically analyze the optimal number of groups to achieve a good tradeoff between staleness and synchronization waiting. The evaluation test in the realistic cluster with multiple training tasks demonstrates that Gsyn is beneficial and accelerates distributed training by up to 27% over the state-of-the-art solutions.}
}


@inproceedings{DBLP:conf/infocom/ZhouL24,
	author = {Sixiang Zhou and
                  Xiaojun Lin},
	title = {An Easier-to-Verify Sufficient Condition for Whittle Indexability
                  and Application to AoI Minimization},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1741--1750},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621350},
	doi = {10.1109/INFOCOM52122.2024.10621350},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study a scheduling problem for a base-station transmitting status information to multiple user-equipments (UE) with the goal of minimizing the total expected Age-of-Information (AoI). Such a problem can be formulated as a Restless Multi-Armed Bandit (RMAB) problem and solved asymptotically-optimally by a low-complexity Whittle index policy, if each UE’s sub-problem is Whittle indexable. However, proving Whittle indexability can be highly non-trivial, especially when the value function cannot be derived in closed-form. In particular, this is the case for the AoI minimization problem with stochastic arrivals and unreliable channels, whose Whittle indexability remains an open problem. To overcome this difficulty, we develop a sufficient condition for Whittle indexability based on the notion of active time (AT). Even though the AT condition shares considerable similarity to the Partial Conservation Law (PCL) condition, it is much easier to understand and verify. We then apply our AT condition to the stochastic-arrival unreliable-channel AoI minimization problem and, for the first time in the literature, prove its Whittle indexability. Our proof uses a novel coupling approach to verify the AT condition, which may also be of independent interest to other large-scale RMAB problems.}
}


@inproceedings{DBLP:conf/infocom/LiuLLDL24,
	author = {Haolin Liu and
                  Sirui Liu and
                  Saiqin Long and
                  Qingyong Deng and
                  Zhetao Li},
	title = {Joint Optimization of Model Deployment for Freshness-Sensitive Task
                  Assignment in Edge Intelligence},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1751--1760},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621314},
	doi = {10.1109/INFOCOM52122.2024.10621314},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuLLDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge Intelligence aims to push deep learning (DL) services to network edge to reduce response time and protect privacy. In implementations, proximity deployment of DL models and timely updates can improve the quality of experience (QoE) for users, but increase the operation cost as well as pose a challenge for task assignment. To address the challenge, a joint online optimization problem for DL model deployment (including placement and update) and freshness-sensitive task assignment is formulated to improve QoE and application service provider (ASP) profit. In the problem, we introduce the age of information (AOI) to quantify the freshness of the DL model and represent user QoE as an AOI based utility function. To solve the problem, an online model placement, update, and task assignment (MPUTA) algorithm is proposed. It first converts the time-slot coupled problem into a single time-slot problem using the regularization technique, and decomposes the single time-slot problem into model deployment and task assignment subproblems. Then, using the randomized round technique to deal with the model deployment subproblem and the graph matching technique to solve the task assignment subproblem. In simulation experiments, MPUTA is shown to outperform other benchmark algorithms in terms of both user QoE and ASP profit.}
}


@inproceedings{DBLP:conf/infocom/ZouNGS024,
	author = {Yang Zou and
                  Xin Na and
                  Xiuzhen Guo and
                  Yimiao Sun and
                  Yuan He},
	title = {Trident: Interference Avoidance in Multi-reader Backscatter Network
                  via Frequency-space Division},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1761--1770},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621258},
	doi = {10.1109/INFOCOM52122.2024.10621258},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZouNGS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Backscatter is an enabling technology for battery-free sensing in industrial IoT applications. For the purpose of full coverage of numerous tags in the deployment area, one often needs to deploy multiple readers, each of which is to communicate with tags within its communication range. But the actual backscattered signals from a tag are likely to reach a reader outside its communication range, causing undesired interference. Conventional approaches for interference avoidance, either TDMA or CSMA based, separate the readers’ media accesses in the time dimension and suffer from limited network throughput. In this paper, we propose Trident, a novel backscatter tag design that enables interference avoidance with frequency-space division. By incorporating a tunable bandpass filter and multiple terminal loads, a Trident tag is able to detect its channel condition and adaptively adjust the frequency band and the power of its backscattered signals, so that all the readers in the network can operate concurrently without being interfered. We implement Trident and evaluate its performance under various settings. The results demonstrate that Trident enhances the network throughput by 3.18×, compared to the TDMA based scheme.}
}


@inproceedings{DBLP:conf/infocom/DuYZA24,
	author = {Caihui Du and
                  Jihong Yu and
                  Rongrong Zhang and
                  Jianping An},
	title = {ConcurScatter: Scalable Concurrent {OFDM} Backscatter Using Subcarrier
                  Pattern Diversity},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1771--1780},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621252},
	doi = {10.1109/INFOCOM52122.2024.10621252},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DuYZA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ambient OFDM backscatter communication has attracted considerable research efforts. Yet the prior works focus on point-to-point backscatter from a single tag, leaving behind efficient backscatter networking of multiple tags. In this paper, we design and implement ConcurScatter, the first ambient OFDM backscatter system that scales to concurrent transmission of hundreds of tags. Our key innovation is building and using the subcarrier pattern diversity to distinguish concurrent tags. This would yield linear collision states rather than exponential ones in the prior works based on the IQ domain diversity, supporting more concurrent transmission. We concrete this by designing a suit of techniques including midair frequency synthesis that forms a unique subcarrier pattern for each concurrent tag, non-integer cyclic shift that contributes to support more concurrent tags, and subcarrier pattern reconstruction that creates virtual subcarriers to enable single-symbol parallel decoding. The testbed experiment confirms that ConcurScatter supports seven more concurrent tags with similar BER and 8.4× higher throughput than the point-to-point backscatter RapidRider. The large-scale simulation shows that ConcurScatter supports 200 tags which is 40× more than the state-of-the-art concurrent OFDM backscatter FreeCollision.}
}


@inproceedings{DBLP:conf/infocom/YangF0024,
	author = {Yifan Yang and
                  Yunyun Feng and
                  Wei Gong and
                  Yu Yang},
	title = {Efficient {LTE} Backscatter with Uncontrolled Ambient Traffic},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1781--1790},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621196},
	doi = {10.1109/INFOCOM52122.2024.10621196},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YangF0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ambient LTE backscatter is a promising way to enable ubiquitous wireless communication with ultra-low power and cost. However, modulation in previous LTE backscatter systems relies heavily on the original data (content) of the signals. They either demodulate tag data using an additional receiver to provide the content of the excitation or modulate on a few predefined reference signals in random ambient LTE traffic.This paper presents CABLTE, a content-agnostic backscatter system that efficiently utilizes uncontrolled LTE PHY resources for backscatter communication using a single receiver. Our system is superior to prior work in two aspects: 1) Using one receiver to obtain tag data makes CABLTE more practical in real-world applications, and 2) Efficient modulation on LTE PYH resources improves the data rate of backscatter communication. To obtain the tag data without knowing the ambient content, we design a checksum-based codeword translation method. We also propose a customized channel estimation scheme and a signal identification component in the backscatter system to ensure our accurate modulation and demodulation. Extensive experiments show that our CABLTE provides maximum tag throughput of 22 kbps, which is 3.67x higher than the content-agnostic system CAB and even 1.38x higher than the content-based system SyncLTE.}
}


@inproceedings{DBLP:conf/infocom/JiangLLG24,
	author = {Maoran Jiang and
                  Xin Liu and
                  Dong Li and
                  Wei Gong},
	title = {Efficient Two-Way Edge Backscatter with Commodity Bluetooth},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1791--1800},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621373},
	doi = {10.1109/INFOCOM52122.2024.10621373},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/JiangLLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Two-way backscatter is essential to general-purpose backscatter communication as it provides rich interaction to support diverse applications on commercial devices. However, existing Bluetooth backscatter systems suffer from unstable uplinks due to poor carrier-identification capability and inefficient downlinks caused by packet-length modulation. This paper proposes EffBlue, an efficient two-way backscatter design for commercial Bluetooth devices. EffBlue employs a simple edge backscatter server that alleviates the computational burden on the tag and helps build efficient uplinks and downlinks. Specifically, efficient uplinks are designed by introducing an accurate synchronization scheme, which can effectively eliminate the use of non-compliant packets as carriers. To break the limitation of packet-level modulation, we design a new symbollevel WiFi-ASK downlink where the edge sends ASK-like WiFi signals and the tag can decode such signals using a simple envelope detector. We prototype the edge server using commodity WiFi and Bluetooth chips and build two-way backscatter tags with FPGAs. Experimental results show that EffBlue can identify the target excitations with more than 99% precision. Meanwhile, its WiFi-ASK downlink can achieve up to 124 kbps, which is 25x better than FreeRider.}
}


@inproceedings{DBLP:conf/infocom/HuangWZZWW24,
	author = {Shaoyuan Huang and
                  Zheng Wang and
                  Zhongtian Zhang and
                  Heng Zhang and
                  Xiaofei Wang and
                  Wenyu Wang},
	title = {Seer: Proactive Revenue-Aware Scheduling for Live Streaming Services
                  in Crowdsourced Cloud-Edge Platforms},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1801--1810},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621424},
	doi = {10.1109/INFOCOM52122.2024.10621424},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HuangWZZWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As live streaming services skyrocket, Crowdsourced Cloud-edge service Platforms (CCPs) have surfaced as pivotal intermediaries catering to the mounting demand. Despite the role of stream scheduling to CCPs’ Quality of Service (QoS) and throughput, conventional optimization strategies struggle to enhancing CCPs’ revenue, primarily due to the intricate relationship between resource utilization and revenue. Additionally, the substantial scale of CCPs magnifies the difficulties of time-intensive scheduling. To tackle these challenges, we propose Seer, a proactive revenue-aware scheduling system for live streaming services in CCPs. The design of Seer is motivated by meticulous measurements of real-world CCPs environments, which allows us to achieve accurate revenue modeling and overcome three key obstacles that hinder the integration of prediction and optimal scheduling. Utilizing an innovative Preschedule-Execute-Re-schedule paradigm and flexible scheduling modes, Seer achieves efficient revenue-optimized scheduling in CCPs. Extensive evaluations demonstrate Seer’s superiority over competitors in terms of revenue, utilization, and anomaly penalty mitigation, boosting CCPs revenue by 147% and expediting scheduling 3.4× faster.}
}


@inproceedings{DBLP:conf/infocom/LiMCX00C24,
	author = {Zuxin Li and
                  Fanhang Man and
                  Xuecheng Chen and
                  Susu Xu and
                  Fan Dang and
                  Xiao{-}Ping Zhang and
                  Xinlei Chen},
	title = {{QUEST:} Quality-informed Multi-agent Dispatching System for Optimal
                  Mobile Crowdsensing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1811--1820},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621374},
	doi = {10.1109/INFOCOM52122.2024.10621374},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiMCX00C24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We address the challenges in achieving optimal Quality of Information (QoI) for non-dedicated vehicular Mobile Crowdsensing (MCS) systems, by utilizing vehicles not originally designed for sensing purposes to provide real-time data while moving around the city. These challenges include the coupled sensing coverage and sensing reliability, as well as the uncertainty and time-varying vehicle status. To tackle these issues, we propose QUEST, a QUality-informed multi-agEnt diSpaTching system, that ensures high sensing coverage and sensing reliability in non-dedicated vehicular MCS. QUEST optimizes QoI by introducing a novel metric called ASQ (aggregated sensing quality), which considers both sensing coverage and sensing reliability jointly. Additionally, we design a mutual-aided truth discovery dispatching method to estimate sensing reliability and improve ASQ under uncertain vehicle statuses. Real-world data from our deployed MCS system in a metropolis is used for evaluation, demonstrating that QUEST achieves up to 26% higher ASQ improvement, leading to a reduction of reconstruction map errors by 32-65% for different reconstruction algorithms.}
}


@inproceedings{DBLP:conf/infocom/WangCZSC24,
	author = {Hengzhi Wang and
                  Laizhong Cui and
                  Lei Zhang and
                  Linfeng Shen and
                  Long Chen},
	title = {Combinatorial Incentive Mechanism for Bundling Spatial Crowdsourcing
                  with Unknown Utilities},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1821--1830},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621352},
	doi = {10.1109/INFOCOM52122.2024.10621352},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangCZSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incentive mechanisms in Spatial Crowdsourcing (SC) have been widely studied as they provide an effective way to motivate mobile workers to perform spatial tasks. Yet, most existing mechanisms only involve single tasks, neglecting the presence of complementarity and substitutability among tasks. This limits their effectiveness in practice cases. Motivated by this, we consider task bundles for incentive mechanism design and closely analyze the mutual exclusion effect that arises with task bundles. We then develop a combinatorial incentive mechanism, including three key policies: In the offline case, we propose a combinatorial assignment policy to address the conflict between mutual exclusion and assignment efficiency. We next study the conflict between mutual exclusion and truthfulness, and build a combinatorial pricing policy to pay winners that yields both incentive compatibility and individual rationality. In the online case with unknown workers’ utilities, we present an online combinatorial assignment policy that balances the exploration-exploitation trade-off under the mutual exclusion constraints. Through theoretical analysis and numerical simulations using real-world mobile networking datasets, we demonstrate the effectiveness of the proposed mechanism.}
}


@inproceedings{DBLP:conf/infocom/WangZYXSY24,
	author = {En Wang and
                  Mijia Zhang and
                  Bo Yang and
                  Yang Xu and
                  Zixuan Song and
                  Yongjian Yang},
	title = {Few-Shot Data Completion for New Tasks in Sparse Crowdsensing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1831--1840},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621156},
	doi = {10.1109/INFOCOM52122.2024.10621156},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangZYXSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowdsensing is a type of technology that utilizes mobile devices and volunteers to gather data about specific topics at large scales in real-time. However, in practice, limited participation leads to missing data, i.e., the collected data may be sparse, which makes it difficult to perform accurate analysis. A possible technique called sparse crowdsensing incorporates the sparse case with data completion, where unsensed data could be estimated through inference. However, sparse crowdsensing typically suffers from poor performance during the data completion stage due to various challenges: the sparsity of the sensed data, reliance on numerous timeslots, and uncertain spatiotemporal connections. To resolve such few-shot issues, the proposed solution uses the Correlated Data Fusion for Matrix Completion (CDFMC) approach, which leverages a small amount of objective data to retrain an auxiliary dataset-based pre-trained model that can estimate unsensed data efficiently. CDFMC is trained using a combination of the traditional Deep Matrix Factorization and the Kalman Filtering, which not only enables the efficient representation and comparison of data samples but also fuses the objective data and auxiliary data effectively. Evaluation results show that the proposed CDFMC outperforms baseline techniques, achieving high accuracy in completing unsensed data with minimal training data.}
}


@inproceedings{DBLP:conf/infocom/CaiW24,
	author = {Guanyu Cai and
                  Jiliang Wang},
	title = {{ATP:} Acoustic Tracking and Positioning under Multipath and Doppler
                  Effect},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1841--1850},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621165},
	doi = {10.1109/INFOCOM52122.2024.10621165},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/CaiW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic tracking and positioning technologies using microphones and speakers have gained significant interest for applications like virtual reality, augmented reality, and IoT devices. However, existing methods still face challenges in real-world deployment due to multipath interference, Doppler frequency shift, and sampling frequency offset between devices. We propose a versatile Acoustic Tracking and Positioning (ATP) method to address these challenges. First, we propose an iterative sampling frequency offset calibration method. Next, we propose a Doppler frequency shift estimation and compensation model. Finally, we propose a fast adaptive algorithm to reconstruct the line-of-sight (LOS) signal under multipath 1 . We implement ATP in Android and PC and compare it with eight different methods. Evaluation results show that ATP achieves mean accuracy of 0.66 cm, 0.56 cm, and 1.0 cm in tracking, ranging, and positioning tasks. It is 2×, 6×, and 5.8× better than the state-of-the-art methods. ATP advances acoustic sensing for practical applications by providing a robust solution for real-world environments.}
}


@inproceedings{DBLP:conf/infocom/CaoXLYL24,
	author = {Hao Cao and
                  Jingao Xu and
                  Danyang Li and
                  Zheng Yang and
                  Yunhao Liu},
	title = {EventBoost: Event-based Acceleration Platform for Real-time Drone
                  Localization and Tracking},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1851--1859},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621101},
	doi = {10.1109/INFOCOM52122.2024.10621101},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/CaoXLYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Drones have demonstrated their pivotal role in various applications such as search-and-rescue, smart logistics, and industrial inspection, with accurate localization playing an indispensable part. However, in high dynamic range and rapid motion scenarios, traditional visual sensors often face challenges in pose estimation. Event cameras, with their high temporal resolution, present a fresh opportunity for perception in such challenging environments. Current efforts resort to event-visual fusion to enhance the drone’s sensing capability. Yet, the lack of efficient event-visual fusion algorithms and corresponding acceleration hardware causes the potential of event cameras to remain underutilized. In this paper, we introduce EventBoost, an acceleration platform designed for drone-based applications with event-image fusion. We propose a suit of novel algorithms through software-hardware co-design on Zynq SoC, aimed at enhancing real-time localization precision and speed. EventBoost achieves enhanced visual fusion precision and markedly elevated processing efficiency. The performance comparison with two state-of-the-art systems shows EventBoost achieves 24.33% improvement in accuracy with 30 ms latency on resource-constrained platforms.}
}


@inproceedings{DBLP:conf/infocom/LiLL0024,
	author = {Yeming Li and
                  Hailong Lin and
                  Jiamei Lv and
                  Yi Gao and
                  Wei Dong},
	title = {{BLE} Location Tracking Attacks by Exploiting Frequency Synthesizer
                  Imperfection},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1860--1869},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621247},
	doi = {10.1109/INFOCOM52122.2024.10621247},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiLL0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, Bluetooth Low Energy (BLE) has become one of the most wildly used wireless protocols and it is common that users carry one or more BLE devices. With the extensive deployment of BLE devices, there is a significant privacy risk if these BLE devices can be tracked. However, the common wisdom suggests that the risk of BLE location tracking is negligible. The reason is that researchers believe there are no stable BLE fingerprints that are stable across different scenarios (e.g., temperatures) for different BLE devices with the same model. In this paper, we introduce a novel physical-layer fingerprint named Transient Dynamic Fingerprint (TDF), which originated from the negative feedback control process of the frequency synthesizer. Because of the hardware imperfection, the dynamic features of the frequency synthesizer are different, making TDF unique among different devices, even with the same model. Furthermore, TDF keeps stable under different thermal conditions. Based on TDF, we propose BTrack, a practical BLE device tracking system and evaluate its tracking performance in different environments. The results show BTrack works well once BLE beacons are effectively received. The identification accuracy is 35.38%-57.41% higher than the existing method, and stable over temperatures, distances, and locations.}
}


@inproceedings{DBLP:conf/infocom/LizarribarCSSBG24,
	author = {Yago Lizarribar and
                  Roberto Calvo{-}Palomino and
                  Alessio Scalingi and
                  Giuseppe Santaromita and
                  G{\'{e}}r{\^{o}}me Bovet and
                  Domenico Giustiniano},
	title = {ORAN-Sense: Localizing Non-cooperative Transmitters with Spectrum
                  Sensing and 5G {O-RAN}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1870--1879},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621298},
	doi = {10.1109/INFOCOM52122.2024.10621298},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LizarribarCSSBG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowdsensing networks for the sole purpose of performing spectrum measurements have resulted in prior initiatives that have failed primarily due to their costs for maintenance. In this paper, we take a different view and propose ORAN-Sense, a novel architecture of Internet of Things (IoT) spectrum crowd-sensing devices integrated into the Next Generation of cellular networks. We use this framework to extend the capabilities of 5G networks and localize a transmitter that does not collaborate in the process of positioning. While 5G signals can not be applied to this scenario as the transmitter does not participate in the localization process through dedicated pilot symbols and data, we show how to use Time Difference of Arrival-based positioning using low-cost spectrum sensors, minimizing hardware impairments of low-cost spectrum receivers, introducing methods to address errors caused by over-the-air signal propagation, and proposing a low-cost synchronization technique. We have deployed our localization network in two major cities in Europe. Our experimental results indicate that signal localization of non-collaborative transmitters is feasible even using low-cost radio receivers with median accuracies of tens of meters with just a few sensors spanning cities, which makes it suitable for its integration in the Next Generation of cellular networks.}
}


@inproceedings{DBLP:conf/infocom/PanLS0S024,
	author = {Xinglin Pan and
                  Wenxiang Lin and
                  Shaohuai Shi and
                  Xiaowen Chu and
                  Weinong Sun and
                  Bo Li},
	title = {Parm: Efficient Training of Large Sparsely-Activated Models with Dedicated
                  Schedules},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1880--1889},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621327},
	doi = {10.1109/INFOCOM52122.2024.10621327},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/PanLS0S024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparsely-activated Mixture-of-Expert (MoE) layers have found practical applications in enlarging the model size of large-scale foundation models, with only a sub-linear increase in computation demands. Despite the wide adoption of hybrid parallel paradigms like model parallelism, expert parallelism, and expert-sharding parallelism (i.e., MP+EP+ESP) to support MoE model training on GPU clusters, the training efficiency is hindered by communication costs introduced by these parallel paradigms. To address this limitation, we propose Parm, a system that accelerates MP+EP+ESP training by designing two dedicated schedules for placing communication tasks. The proposed schedules eliminate redundant computations and communications and enable overlaps between intra-node and inter-node communications, ultimately reducing the overall training time. As the two schedules are not mutually exclusive, we provide comprehensive theoretical analyses and derive an automatic and accurate solution to determine which schedule should be applied in different scenarios. Experimental results on an 8-GPU server and a 32-GPU cluster demonstrate that Parm outperforms the state-of-the-art MoE training system, DeepSpeed-MoE, achieving 1.13× to 5.77× speedup on 1296 manually configured MoE layers and approximately 3× improvement on two real-world MoE models based on BERT and GPT-2.}
}


@inproceedings{DBLP:conf/infocom/Wang0WL024,
	author = {Ranran Wang and
                  Yin Zhang and
                  Wenchao Wan and
                  Xiong Li and
                  Min Chen},
	title = {Predicting Multi-Scale Information Diffusion via Minimal Substitution
                  Neural Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1890--1899},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621418},
	doi = {10.1109/INFOCOM52122.2024.10621418},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Wang0WL024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In social media platforms such as Weibo, Twitter, and Facebook, a variety of information is diffused daily. Exploring and exploiting the diffusion patterns in this information play crucial roles in areas such as viral marketing, recommendation systems, and public opinion management. However, the diffusion of this information is not merely sequential propagation among users, as most researchers assume. When we observe the diffusion of information in the entire network from a macroscopic perspective, we discover that these phenomena of information diffusion exhibit a series of interconnected relationships, such as alternation or dependency. In traditional methods of information diffusion prediction (IDP), these aspects are often overlooked. To address this, we introduce a substitution theory of information diffusion, minimal substitution (MS), and we combine it with neural networks to design a network model known as MSNN. First, the incorporation of MS theory enables our model to effectively capture the complex interrelations among pieces of information. Second, we analyze the relationship of the multi-scale IDP task, develop a one-step MS-based microscopic IDP method and a dynamic MS-based macroscopic IDP method, and utilize these two methods for joint training to achieve multi-scale prediction. Finally, we validate the accuracy of the proposed MSNN model through training on two real-world datasets with different growth patterns.}
}


@inproceedings{DBLP:conf/infocom/Cai0H24,
	author = {Huaiguang Cai and
                  Zhi Zhou and
                  Qianyi Huang},
	title = {Online Resource Allocation for Edge Intelligence with Colocated Model
                  Retraining and Inference},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1900--1909},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621206},
	doi = {10.1109/INFOCOM52122.2024.10621206},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Cai0H24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With edge intelligence, AI models are increasingly push to the edge to serve ubiquitous users. However, due to the drift of model, data, and task, AI model deployed at the edge suffers from degraded accuracy in the inference serving phase. Model retraining handles such drifts by periodically retraining the model with newly arrived data. When colocating model retraining and model inference serving for the same model on resource-limited edge servers, a fundamental challenge arises in balancing the resource allocation for model retraining and inference, aiming to maximize long-term inference accuracy. This problem is particularly difficult due to the underlying mathematical formulation being time-coupled, non-convex, and NP-hard. To address these challenges, we introduce a lightweight and explainable online approximation algorithm, named ORRIC, designed to optimize resource allocation for adaptively balancing the accuracy of model training and inference. The competitive ratio of ORRIC outperforms that of the traditional Inference-Only paradigm, especially when data drift persists for a sufficiently lengthy time. This highlights the advantages and applicable scenarios of colocating model retraining and inference. Notably, ORRIC can be translated into several heuristic algorithms for different resource environments. Experiments conducted in real scenarios validate the effectiveness of ORRIC.}
}


@inproceedings{DBLP:conf/infocom/QiZ0X24,
	author = {Tianyu Qi and
                  Yufeng Zhan and
                  Peng Li and
                  Yuanqing Xia},
	title = {Tomtit: Hierarchical Federated Fine-Tuning of Giant Models based on
                  Autonomous Synchronization},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1910--1919},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621369},
	doi = {10.1109/INFOCOM52122.2024.10621369},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QiZ0X24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the quick evolution of giant models, the paradigm of pre-training models and then fine-tuning them for downstream tasks has become increasingly popular. The adapter has been recognized as an efficient fine-tuning technique and attracts much research attention. However, adapter-based fine-tuning still faces the challenge of lacking sufficient data. Federated fine-tuning has been recently proposed to fill this gap, but existing solutions suffer from a serious scalability issue, and they are inflexible in handling dynamic edge environments. In this paper, we propose Tomtit, a hierarchical federated fine-tuning system that can significantly accelerate fine-tuning and improve the energy efficiency of devices. Via extensive empirical study, we find that model synchronization schemes (i.e., when edge servers and devices should synchronize their models) play a critical role in federated fine-tuning. The core of Tomtit is a distributed design that allows each edge and device to have a unique synchronization scheme with respect to their heterogeneity in model structure, data distribution and computing capability. Furthermore, we provide a theoretical guarantee about the convergence of Tomtit. Finally, we develop a prototype of Tomtit and evaluate it on a testbed. Experimental results show that it can significantly outperform the state-of-the-art.}
}


@inproceedings{DBLP:conf/infocom/TrautweinWPSCGT24,
	author = {Dennis Trautwein and
                  Yiluo Wei and
                  Yiannis Psaras and
                  Moritz Schubotz and
                  Ignacio Castro and
                  Bela Gipp and
                  Gareth Tyson},
	title = {{IPFS} in the Fast Lane: Accelerating Record Storage with Optimistic
                  Provide},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1920--1929},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621404},
	doi = {10.1109/INFOCOM52122.2024.10621404},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TrautweinWPSCGT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The centralization of web services has raised concerns about critical single points of failure, such as content hosting, name resolution, and certification. To address these issues, the "Decentralized Web" movement advocates for de-centralized alternatives. Distributed Hash Tables (DHTs) have emerged as a key component facilitating this movement, as they offer efficient key/value indexing. The InterPlanetary File System (IPFS) exemplifies this approach by leveraging DHTs for data indexing and distribution. A critical finding of previous studies is that DHT PUT performance for record storage is unacceptably slow, sometimes taking minutes to complete and hindering the adoption of delay-intolerant applications. To address this challenge, this research paper presents three significant contributions. First, we present the design of Optimistic Provide, an approach to accelerate DHT PUT operations in Kademlia-based IPFS networks while maintaining full backward compatibility. Second, we implement and deploy the mechanism and see its usage in the de-facto IPFS deployment, Kubo. Third, we evaluate its effectiveness in the IPFS and Filecoin DHTs. We confirm that we enable sub-second record storage from North America and Europe for 90% of PUT operations while reducing networking overhead by over 40% and maintaining record availability.}
}


@inproceedings{DBLP:conf/infocom/Racke0V24,
	author = {Harald R{\"{a}}cke and
                  Stefan Schmid and
                  Radu Vintan},
	title = {Fast Algorithms for Loop-Free Network Updates using Linear Programming
                  and Local Search},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1930--1939},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621077},
	doi = {10.1109/INFOCOM52122.2024.10621077},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Racke0V24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To meet stringent performance requirements, communication networks are becoming increasingly programmable and flexible, supporting fast and frequent adjustments. However, reconfiguring networks in a dependable and transiently consistent manner is known to be algorithmically challenging. This paper revisits the fundamental problem of how to update the routes in a network in a (transiently) loop-free manner, considering both the Strong Loop-Freedom (SLF) and the Relaxed Loop-Freedom (RLF) property.We present two fast algorithms to solve the SLF and RLF problem variants exactly, to optimality. Our algorithms are based on a parameterized integer linear program which would be intractable to solve directly by a classic solver. Our main technical contribution is a lazy cycle breaking strategy which, by adding constraints lazily, improves performance dramatically, and outperforms the state-of-the-art exact algorithms by an order of magnitude on realistic medium-sized networks. We further explore approximate algorithms and show that while a relaxation approach is relatively slow, with a local search approach short update schedules can be found, outperforming the state-of-the-art heuristics.On the theoretical front, we also provide an approximation lower bound for the update time of the state-of-the-art algorithm in the literature. As a contribution to the research community, we made all our code and implementations publicly available.}
}


@inproceedings{DBLP:conf/infocom/LiLDCG0C24,
	author = {Meng Li and
                  Wenqi Luo and
                  Haipeng Dai and
                  Huayi Chai and
                  Rong Gu and
                  Xiaoyu Wang and
                  Guihai Chen},
	title = {The Reinforcement Cuckoo Filter},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1940--1949},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621370},
	doi = {10.1109/INFOCOM52122.2024.10621370},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiLDCG0C24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider the problem of approximate membership testing problem on skewed network traffic traces, in which some hot or popular items repeat frequently. Previous solutions suffer from either high false positive rates or low lookup throughput. To address this problem, we propose a variant of the cuckoo filter, enhanced with a hotness-aware suffix cache. We note that a false positive item must have a matched fingerprint in the cuckoo filter, and propose to reduce false positives by memorizing them, but with their suffixes only. For each false positive item, we apply a linear-congruential-based hash function and then divide the hash value into three parts: the bucket index to be accessed in the cuckoo filter, the fingerprint to be stored in the cuckoo filter, and the suffix to be cached. Combining the three parts, we propose RCF that can uniquely identify a hot false positive item, which thus reduces hot false positives. Our evaluation results indicate that RCF significantly outperforms non-adaptive filters on skewed data traces. Given the same memory size, it achieves a much lower false positive ratio without sacrificing its lookup throughput. Compared with adaptive filters, RCF provides a competitive false positive ratio while offering a considerably higher lookup throughput.}
}


@inproceedings{DBLP:conf/infocom/GaoZ24,
	author = {Yang Gao and
                  Hongli Zhang},
	title = {Multi-Order Clustering on Dynamic Networks: On Error Accumulation
                  and Its Elimination},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1950--1959},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621124},
	doi = {10.1109/INFOCOM52122.2024.10621124},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GaoZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Local clustering aims to find a high-quality cluster near a given vertex. Recently, higher-order units are introduced to local clustering, and the underlying information has been verified to be essential. However, original edges are underestimated in these techniques, leading to the degeneration of network information. Moreover, most of the higher-order models are designed for static networks, whereas real-world networks are generally large and evolve rapidly. Repeatedly conducting a static algorithm at each snapshot is usually computationally impractical, and recent approaches instead track a cluster by updating the cluster sequentially. However, errors would accumulate over lengthy evolutions, and the complete cluster needs to be recalculated periodically to maintain the accuracy, which naturally affects the efficiency. To bridge the two gaps, we design a multi-order hypergraph, and present a hybrid model for dynamic clustering. In particular, we propose an incremental method to track a personalized PageRank vector in the evolving hypergraph, which converges to the exact solution at each snapshot when significantly reducing the complexity. We further develop a dynamic sweep to identify a cut in each vector, whereby a cluster can be incrementally updated with no accumulated errors. We provide rigorous theoretical basis and conduct comprehensive experiments, which demonstrate the effectiveness.}
}


@inproceedings{DBLP:conf/infocom/AhmedROC24,
	author = {Imran Ahmed and
                  Roshan Kumar Rai and
                  Eiji Oki and
                  Bijoy Chand Chatterjee},
	title = {AnalyticalDF: Analytical Model for Blocking Probabilities Considering
                  Spectrum Defragmentation in Spectrally-Spatially Elastic Optical Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1960--1969},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621217},
	doi = {10.1109/INFOCOM52122.2024.10621217},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/AhmedROC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, multi-core and multi-mode fibers (MCMMFs) have been considered to overcome physical limitations and increase transport capacity. They are combined with elastic optical networks (EONs) to form spectrally-spatially elastic optical networks (SS-EONs), an emerging technology. Fragmentation and crosstalk (XT) are well-known drawbacks of SS-EONs that increase blocking probability; evaluating blocking probability analytically is difficult due to additional constraints. When calculating blocking probabilities in MCMMFs-based SS-EONs, it is observed that all current studies either employ simulation-based techniques or do not consider defragmentation of their analytical models. This paper proposes an exact analytical continuous-time Markov chain model for blocking probabilities, named AnalyticalDF, in SS-EONs, which considers defragmentation and the XT-avoided approach. AnalyticalDF generates all possible states and transitions while avoiding inter-core and inter-mode XTs for single-class and multi-class requests. Single-class requests utilize the same number of slots, whereas multi-class requests adopt varying numbers of slots to accommodate client needs. We introduce an iterative approximation model for a single-hop link when AnalyticalDF is not tractable due to scalability. We evaluate AnalyticalDF, the iterative approximate model, and simulation studies for a single-hop link. The numerical results indicate that AnalyticalDF outperforms a non-defragmentation-aware benchmark model.}
}


@inproceedings{DBLP:conf/infocom/DozierSR24,
	author = {Kahlil Dozier and
                  Loqman Salamatian and
                  Dan Rubenstein},
	title = {Modeling Average False Positive Rates of Recycling Bloom Filters},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1970--1979},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621226},
	doi = {10.1109/INFOCOM52122.2024.10621226},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DozierSR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bloom Filters are a space-efficient data structure used for the testing of membership in a set that errs only in the False Positive direction. However, the standard analysis that measures this False Positive rate provides a form of worst case bound that is both overly conservative for the majority of network applications that utilize Bloom Filters, and reduces accuracy by not taking into account the actual state (number of bits set) of the Bloom Filter after each arrival. In this paper, we more accurately characterize the False Positive dynamics of Bloom Filters as they are commonly used in networking applications. In particular, network applications often utilize a Bloom Filter that “recycles”: it repeatedly fills, and upon reaching a certain level of saturation, empties and fills again. In this context, it makes more sense to evaluate performance using the average False Positive rate instead of the worst case bound. We show how to efficiently compute the average False Positive rate of recycling Bloom Filter variants via renewal and Markov models. We apply our models to both the standard Bloom Filter and a "two-phase" variant, verify the accuracy of our model with simulations, and find that the previous analysis’ worst-case formulation leads to up to a 30% reduction in the efficiency of Bloom Filter when applied in network applications, while two-phase overhead diminishes as the needed False Positive rate is tightened.}
}


@inproceedings{DBLP:conf/infocom/CiucuMR24,
	author = {Florin Ciucu and
                  Sima Mehri and
                  Amr Rizk},
	title = {On Ultra-Sharp Queueing Bounds},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1980--1988},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621177},
	doi = {10.1109/INFOCOM52122.2024.10621177},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/CiucuMR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a robust method to analyze a broad range of classical queueing models, e.g., the GI/G/1 queue with renewal arrivals, an AR/G/1 queue with alternating renewals (AR), as a special class of Semi-Markovian processes, and Markovian fluids queues. At the core of the method lies a standard change-of-measure argument to reverse the sign of the negative drift in the underlying random walks. Combined with a suitable representation of the overshoot, we obtain exact results in terms of series. Closed-form and computationally fast bounds follow by taking the series’ first terms, which are the dominant ones because of the positive drift under the new probability measure. The obtained bounds generalize the state-of-the-art class of martingale bounds and can be much sharper by orders of magnitude.}
}


@inproceedings{DBLP:conf/infocom/BeyturAVV24,
	author = {Hasan Burhan Beytur and
                  Ahmet Gunhan Aydin and
                  Gustavo de Veciana and
                  Haris Vikalo},
	title = {Optimization of Offloading Policies for Accuracy-Delay Tradeoffs in
                  Hierarchical Inference},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1989--1998},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621325},
	doi = {10.1109/INFOCOM52122.2024.10621325},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/BeyturAVV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a hierarchical inference system with multiple clients connected to a server via a shared communication resource. When necessary, clients with low-accuracy machine learning models can offload classification tasks to a server for processing on a high-accuracy model. We propose a distributed online offloading algorithm which maximizes the accuracy subject to a shared resource utilization constraint thus indirectly realizing accuracy-delay tradeoffs possible given an underlying network scheduler. The proposed algorithm, named Lyapunov-EXP4, introduces a loss structure based on Lyapunov-drift minimization techniques to the bandits with expert advice framework. We prove that the algorithm converges to a near-optimal threshold policy on the confidence of the clients’ local inference without prior knowledge of the system’s statistics and efficiently solves a constrained bandit problem with sublinear regret. We further consider settings where clients may employ multiple thresholds, allowing more aggressive optimization of overall accuracy at a possible loss in fairness. Extensive simulation results on real and synthetic data demonstrate convergence of Lyapunov-EXP4, and show the accuracy-delay-fairness tradeoffs achievable in such systems.}
}


@inproceedings{DBLP:conf/infocom/HanXL24,
	author = {Lixiang Han and
                  Zhen Xiao and
                  Zhenjiang Li},
	title = {{DTMM:} Deploying TinyML Models on Extremely Weak IoT Devices with
                  Pruning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {1999--2008},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621387},
	doi = {10.1109/INFOCOM52122.2024.10621387},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HanXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DTMM is a library designed for efficient deployment and execution of machine learning models on weak IoT devices such as microcontroller units (MCUs). The motivation for designing DTMM comes from the emerging field of tiny machine learning (TinyML), which explores extending the reach of machine learning to many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak capability of embedded devices, it is necessary to compress models by pruning enough weights before deploying. Although pruning has been studied extensively on many computing platforms, two key issues with pruning methods are exacerbated on MCUs: models need to be deeply compressed without significantly compromising accuracy, and they should perform efficiently after pruning. Current solutions only achieve one of these objectives, but not both. In this paper, we find that pruned models have great potential for efficient deployment and execution on MCUs. Therefore, we propose DTMM with pruning unit selection, pre-execution pruning optimizations, runtime acceleration, and post-execution low-cost storage to fill the gap for efficient deployment and execution of pruned models. It can be integrated into commercial ML frameworks for practical deployment, and a prototype system has been developed. Extensive experiments on various models show promising gains compared to state-of-the-art methods.}
}


@inproceedings{DBLP:conf/infocom/XieWJL0X024,
	author = {Xueshuo Xie and
                  Haoxu Wang and
                  Zhaolong Jian and
                  Tao Li and
                  Wei Wang and
                  Zhiwei Xu and
                  Guiling Wang},
	title = {Memory-Efficient and Secure {DNN} Inference on TrustZone-enabled Consumer
                  IoT Devices},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2009--2018},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621088},
	doi = {10.1109/INFOCOM52122.2024.10621088},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/XieWJL0X024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge intelligence enables resource-demanding Deep Neural Network (DNN) inference without transferring original data, addressing concerns about data privacy in consumer Inter-net of Things (IoT) devices. For privacy-sensitive applications, deploying models in hardware-isolated trusted execution environments (TEEs) becomes essential. However, the limited secure memory in TEEs poses challenges for deploying DNN inference, and alternative techniques like model partitioning and offloading introduce performance degradation and security issues. In this paper, we present a novel approach for advanced model deployment in TrustZone that ensures comprehensive privacy preservation during model inference. We design a memory-efficient management method to support memory-demanding inference in TEEs. By adjusting the memory priority, we effectively mitigate memory leakage risks and memory overlap conflicts, resulting in 32 lines of code alterations in the trusted operating system. Additionally, we leverage two tiny libraries: S-Tinylib (2,538 LoCs), a tiny deep learning library, and Tinylibm (827 LoCs), a tiny math library, to support efficient inference in TEEs. We implemented a prototype on Raspberry Pi 3B+ and evaluated it using three well-known lightweight DNN models. The experimental results demonstrate that our design significantly improves inference speed by 3.13 times and reduces power consumption by over 66.5% compared to non-memory optimization method in TEEs.}
}


@inproceedings{DBLP:conf/infocom/YanZWCC0XL24,
	author = {Yuting Yan and
                  Sheng Zhang and
                  Xiaokun Wang and
                  Ning Chen and
                  Yu Chen and
                  Yu Liang and
                  Mingjun Xiao and
                  Sanglu Lu},
	title = {VisFlow: Adaptive Content-Aware Video Analytics on Collaborative Cameras},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2019--2028},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621148},
	doi = {10.1109/INFOCOM52122.2024.10621148},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YanZWCC0XL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is an increasing demand for analyzing live surveillance video streams via large-scale camera networks, particularly for applications in public safety and smart cities. To address the conflict between resource-intensive detection models and limited capabilities of cameras, a detection-with-tracking framework has gained prominence. However, since trackers are vulnerable to occlusions and new object appearances, frequent detections are required to calibrate the results, leading to varying detection demands that depends on video content. Consequently, we propose a mechanism for content-aware analytics on collaborative cameras, denoted as VisFlow, to increase the quality of detections and achieve the latency requirement by fully utilizing camera resources. We formulate such a problem as a non-linear, integer program with a long-term perspective, aimed at maximizing detection accuracy. An online mechanism, underpinned by a queue-based algorithm and randomized rounding, is then devised to dynamically orchestrate detection workloads among cameras, thus adapting to fluctuating detection demands. Via rigorous proof, both dynamic regret regarding overall accuracy and the transmission budget are ensured in the long run. The testbed experiments on Jetson Kits demonstrate that VisFlow improves accuracy by 18.3% over the baselines.}
}


@inproceedings{DBLP:conf/infocom/LiuYLZLLF24,
	author = {Kaizheng Liu and
                  Ming Yang and
                  Zhen Ling and
                  Yuan Zhang and
                  Chongqing Lei and
                  Lan Luo and
                  Xinwen Fu},
	title = {Samba: Detecting {SSL/TLS} {API} Misuses in IoT Binary Applications},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2029--2038},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621138},
	doi = {10.1109/INFOCOM52122.2024.10621138},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuYLZLLF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IoT devices are increasingly adopting Secure Socket Layer (SSL) and Transport Layer Security (TLS) protocols. However, the misuse of SSL/TLS libraries still threatens the communication. Existing tools for detecting SSL/TLS API misuses primarily rely on source code analysis while IoT applications are usually released as binaries with no source code. This paper presents Samba, a novel tool to automatically detect SSL/TLS API misuses in IoT binaries through static analysis. To overcome the path explosion problem and deal with various SSL/TLS implementations, we introduce a three-level reduction method to construct the SSL/TLS API-centric graph (SAG), which has a much smaller size compared with the conventional inter-procedural control flow graph. We propose a formal expression of API misuse signatures, which is capable of capturing different types of misuse, particularly those in the SSL/TLS connection establishment process. We successfully analyze 115 IoT binaries and find that 94 of them have the vulnerability of insecure certificate verification and 112 support deprecated SSL/TLS protocols. Samba is the first IoT binary analysis system for detecting SSL/TLS API misuses.}
}


@inproceedings{DBLP:conf/infocom/LiLLWZYC24,
	author = {Xiaoyu Li and
                  Jia Liu and
                  Xuan Liu and
                  Yanyan Wang and
                  Shigeng Zhang and
                  Baoliu Ye and
                  Lijun Chen},
	title = {RF-Boundary: RFID-Based Virtual Boundary},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2039--2048},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621297},
	doi = {10.1109/INFOCOM52122.2024.10621297},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiLLWZYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A boundary is a physical or virtual line that marks the edge or limit of a specific region, which has been widely used in many applications, such as autonomous driving, virtual wall, and robotic lawn mowers. However, none of existing work can well balance the cost, the deployability, and the scalability of a boundary. In this paper, we propose a new RFID-based boundary scheme together with its detection algorithm called RF-Boundary, which has the competitive advantages of being battery-free, low-cost, and easy-to-maintain. We develop two technologies of phase gradient and dual-antenna DoA to address the key challenges posed by RF-boundary, in terms of lack of calibration information and multi-edge interference. We implement a prototype of RF-Boundary with commercial RFID systems and a mobile robot. Extensive experiments verify the feasibility as well as the good performance of RF-Boundary.}
}


@inproceedings{DBLP:conf/infocom/ZhouWZJ0J24,
	author = {Wangqiu Zhou and
                  Xinyu Wang and
                  Hao Zhou and
                  Shenyao Jiang and
                  Zhi Liu and
                  Yusheng Ji},
	title = {Safety Guaranteed Power-Delivered-to-Load Maximization for Magnetic
                  Wireless Power Transfer},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2049--2058},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621150},
	doi = {10.1109/INFOCOM52122.2024.10621150},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouWZJ0J24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electromagnetic radiation (EMR) safety has always been a critical reason for hindering the development of magneticenabled wireless power transfer technology. People focus on the actual received energy at charging devices while paying attention to their health. Thus, we study this significant problem in this paper, and propose a universal safety guaranteed power-delivered-to-load (PDL) maximization scheme (called SafeGuard). Technically, we first utilize the off-the-shelf electromagnetic simulator to perform the EMR distribution analysis to ensure the universality of the method. Then, we innovatively introduce the concept of multiple importance sampling for achieving efficient EMR safety constraint extraction. Finally, we treat the proposed optimization problem as an optimal boundary point search problem from the perspective of space geometry, and devise a brand-new grid-based multi-constraint parallel processing algorithm to efficiently solve it. We implement a system prototype for SafeGuard, and conduct extensive experiments to evaluate it. The results indicate that our SafeGuard can obviously improve the achieved PDL by up to 1.75× compared with the state-of-the-art baseline while guaranteeing EMR safety. Furthermore, SafeGuard can accelerate the solution process by 29.12× compared with the traditional numerical method to satisfy the fast optimization requirement of wireless charging systems.}
}


@inproceedings{DBLP:conf/infocom/MaWGSY024,
	author = {Yuzhuo Ma and
                  Di{\'{e}} Wu and
                  Jing Gao and
                  Wen Sun and
                  Jilin Yang and
                  Tang Liu},
	title = {Dynamic Power Distribution Controlling for Directional Chargers},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2059--2068},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621233},
	doi = {10.1109/INFOCOM52122.2024.10621233},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MaWGSY024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, deploying static chargers to construct timely and robust Wireless Rechargeable Sensor Networks (WRSNs) has become an important research issue for solving the limited energy problem of wireless sensor networks. However, the established fixed power distribution lacks flexibility in response to dynamic charging requests from sensors and may render some sensors to be continuously impacted by destructive wave interference. This results in a gap between energy supply and practical demand, making the charging process less efficient. In this paper, we focus on the real-time sensor charging requests and formulate a dynamic power disTributIon controlling for Directional chargErs (TIDE) problem to maximize the overall charging utility. To solve the problem, we first build a charging model for directional chargers while considering wave interference and extract the candidate charging orientations from the continuous search space. Then we propose the neighbor set division method to narrow the scope of calculation. Finally, we design a dynamic power distribution controlling algorithm to update the neighbor sets timely and select optimal orientations for chargers. Our experimental results demonstrate the effectiveness and efficiency of the proposed scheme, it outperforms the comparison algorithms by 142.62% on average.}
}


@inproceedings{DBLP:conf/infocom/LiuJW24,
	author = {Yihao Liu and
                  Jinyan Jiang and
                  Jiliang Wang},
	title = {LoMu: Enable Long-Range Multi-Target Backscatter Sensing for Low-Cost
                  Tags},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2069--2078},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621272},
	doi = {10.1109/INFOCOM52122.2024.10621272},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Backscatter sensing has shown great potential in the Internet of Things (IoT) and has attracted substantial research interest. We present LoMu, the first long-range multi-target backscatter sensing system for low-cost tags under ambient LoRa. LoMu analyzes the received low-SNR backscatter signals from different tags and calculates their phases to derive the motion information. The design of LoMu faces practical challenges including near-far interference between multiple tags, phase offsets induced by unsynchronized transceivers, and phase errors due to frequency drift in low-cost tags. We propose a conjugate-based energy concentration method to extract high-quality signals and a Hamming-window-based method to alleviate the near-far problem. We then leverage the relationship between the excitation signal and backscatter signals to synchronize TX and RX. Finally, we combine the double sidebands of backscatter signals to cancel the tag frequency drift. We implement LoMu and conduct extensive experiments to evaluate its performance. The results demonstrate that LoMu can accurately sense 35 tags at the same time. The average frequency sensing error is 0.7% at 400m, which is 4× distance of the state-of-the-art.}
}


@inproceedings{DBLP:conf/infocom/KangHCHZC24,
	author = {Hua Kang and
                  Qingyong Hu and
                  Huangxun Chen and
                  Qianyi Huang and
                  Qian Zhang and
                  Min Cheng},
	title = {Cross-shaped Separated Spatial-Temporal UNet Transformer For Accurate
                  Channel Prediction},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2079--2088},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621187},
	doi = {10.1109/INFOCOM52122.2024.10621187},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/KangHCHZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate channel estimation is crucial for the performance gains of massive multiple-input multiple-output (mMIMO) technologies. However, it is bandwidth-unfriendly to estimate large channel matrix frequently to combat the time-varying wireless channel. Deep learning-based channel prediction has emerged to exploit the temporal relationships between historical and future channels to address the bandwidth-accuracy trade-off. Existing methods with convolutional or recurrent neural networks suffer from their intrinsic limitations, including restricted receptive fields and propagation errors. Therefore, we propose a Transformer-based model, CS3T-UNet tailored for mMIMO channel prediction. Specifically, we combine the cross-shaped spatial attention with a group-wise temporal attention scheme to capture the dependencies across spatial and temporal domains, respectively, and introduce the shortcut paths to well-aggregate multi-resolution representations. Thus, CS3T-UNet can globally capture the complex spatial-temporal relationship and predict multiple steps in parallel, which can meet the requirement of channel coherence time. Extensive experiments demonstrate that the prediction performance of CS3T-UNet surpasses the best baseline by at most 6.86 dB with a smaller computation cost on two channel conditions.}
}


@inproceedings{DBLP:conf/infocom/HeWGC24,
	author = {Wei He and
                  Wenjia Wu and
                  Xiaolin Gu and
                  Zichao Chen},
	title = {Diff-ADF: Differential Adjacent-dual-frame Radio Frequency Fingerprinting
                  for LoRa Devices},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2089--2098},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621079},
	doi = {10.1109/INFOCOM52122.2024.10621079},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/HeWGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, LoRa radio frequency fingerprinting has gained widespread attention due to its lightweight nature and difficulty in being forged. The existing fingerprint extraction methods are mainly divided into two categories, i.e., deep learning-based methods and feature engineering-based methods. Deep learning-based methods have poor robustness and require significant resource costs for model training. Although feature engineering-based methods can overcome these drawbacks, the features it commonly uses, such as carrier frequency offset (CFO) and phase noise, lack sufficient discriminative power. Therefore, it is very challenging to design a radio frequency fingerprinting solution with high-accuracy and stable identification performance. Fortunately, we find that the differential phase noise of adjacent dual frames possesses excellent discriminative power and stability. Then, we design the corresponding radio frequency fingerprinting solution called Diff-ADF, which utilizes a classifier with differential phase noise as the primary feature, complemented by the use of CFO as an auxiliary feature. Finally, we implement the Diff-ADF and conduct experiments in real environments. Experimental results demonstrate that our proposed solution achieves an accuracy of over 90% on training and test data collected from different days, which is significantly superior to deep learning-based methods. Even in non-line-of-sight environments, our identification accuracy can still reach close to 85%.}
}


@inproceedings{DBLP:conf/infocom/ZhaoWM24,
	author = {Tianya Zhao and
                  Xuyu Wang and
                  Shiwen Mao},
	title = {Cross-domain, Scalable, and Interpretable {RF} Device Fingerprinting},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2099--2108},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621423},
	doi = {10.1109/INFOCOM52122.2024.10621423},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaoWM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a cross-domain, scalable, and interpretable radio frequency (RF) fingerprinting system using a modified prototypical network (PTN) and an explanation-guided data augmentation across various domains and datasets with only a few samples. Specifically, a convolutional neural network is employed as the feature extractor of the PTN to extract RF fingerprint features. The predictions are made by comparing the similarity between prototypes and feature embedding vectors. To further improve the system performance, we design a customized loss function and deploy an eXplainable Artificial Intelligence (XAI) method to guide data augmentation during fine-tuning. To evaluate the effectiveness of our system in addressing domain shift and scalability problems, we conducted extensive experiments in both cross-domain and novel-device scenarios. Our study shows that our approach achieves exceptional performance in the cross-domain case, exhibiting an accuracy improvement of approximately 80% compared to convolutional neural networks in the best case. Furthermore, our approach demonstrates promising results in the novel-device case across different datasets. Our customized loss function and XAI-guided data augmentation can further improve authentication accuracy to a certain degree.}
}


@inproceedings{DBLP:conf/infocom/FangSLZHS024,
	author = {Liang Fang and
                  Ruiyuan Song and
                  Zhi Lu and
                  Dongheng Zhang and
                  Yang Hu and
                  Qibin Sun and
                  Yan Chen},
	title = {{PRISM:} Pre-training {RF} Signals in Sparsity-aware Masked Autoencoders},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2109--2118},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621246},
	doi = {10.1109/INFOCOM52122.2024.10621246},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FangSLZHS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces a novel paradigm for learning-based RF sensing, termed Pre-training RF signals In Sparsity-aware Masked autoencoders (PRISM), which shifts the RF sensing paradigm from supervised training on limited annotated datasets to unsupervised pre-training on large-scale unannotated datasets, followed by fine-tuning with a small annotated dataset. PRISM leverages a carefully designed sparsity-aware masking strategy to predict missing contents by masking a portion of RF signals, resulting in an efficient pre-training framework that significantly reduces computation and memory resources. This addresses the major challenges posed by large-scale and high-dimensional RF datasets, where memory consumption and computation speed are critical factors. We demonstrate PRISM’s excellent generalization performance across diverse RF sensing tasks by evaluating it on three typical scenarios: human silhouette segmentation, 3D pose estimation, and gesture recognition, involving two general RF devices, radar and WiFi. The experimental results provide strong evidence for the effectiveness of PRISM as a robust learning-based solution for large-scale RF sensing applications.}
}


@inproceedings{DBLP:conf/infocom/FangDHWHYYXKLS24,
	author = {Xing Fang and
                  Feiyan Ding and
                  Bang Huang and
                  Ziyi Wang and
                  Gao Han and
                  Rulan Yang and
                  Lizhao You and
                  Qiao Xiang and
                  Linghe Kong and
                  Yutong Liu and
                  Jiwu Shu},
	title = {Network Can Help Check Itself: Accelerating SMT-based Network Configuration
                  Verification Using Network Domain Knowledge},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2119--2128},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621215},
	doi = {10.1109/INFOCOM52122.2024.10621215},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FangDHWHYYXKLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satisfiability Modulo Theories (SMT) based network configuration verification tools are powerful tools in preventing network configuration errors. However, their fundamental limitation is efficiency, because they rely on generic SMT solvers to solve SMT problems, which are in general NP-complete. In this paper, we show that by leveraging network domain knowledge, we can substantially accelerate SMT-based network configuration verification. Our key insights are: given a network configuration verification formula, network domain knowledge can (1) guide the search of solutions to the formula by avoiding unnecessary search spaces; and (2) help simplify the formula, reducing the problem scale. We leverage these insights to design a new SMT- based network configuration verification tool called NetSMT. Extensive evaluation using real-world topologies and synthetic network configurations shows that NetSMT achieves orders of magnitude improvements compared to state-of-the-art methods.}
}


@inproceedings{DBLP:conf/infocom/ZhangY024,
	author = {Delong Zhang and
                  Chong Ye and
                  Fei He},
	title = {P4Inv: Inferring Packet Invariants for Verification of Stateful {P4}
                  Programs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2129--2138},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621366},
	doi = {10.1109/INFOCOM52122.2024.10621366},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangY024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {P4 is widely adopted for programming data planes in software-defined networking. Formal verification of P4 programs is essential to ensure network reliability and security. However, existing P4 verifiers overlook the stateful nature of packet processing, rendering them inadequate for verifying complex stateful P4 programs.In this paper, we introduce a novel concept called packet invariants to address the stateful aspects of P4 programs. We present an automated verification tool specifically designed for stateful P4 programs. This algorithm efficiently discovers and validates packet invariants in a data-driven manner, offering a novel and effective verification approach for stateful P4 programs. To the best of our knowledge, this approach represents the first attempt to generate and leverage domain-specific invariants for P4 program verification. We implement our approach in a prototype tool called P4Inv. Experimental results demonstrate its effectiveness in verifying stateful P4 programs.}
}


@inproceedings{DBLP:conf/infocom/QiaoY024,
	author = {Yan Qiao and
                  Xinyu Yuan and
                  Kui Wu},
	title = {Routing-Oblivious Network Tomography with Flow-Based Generative Model},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2139--2148},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621139},
	doi = {10.1109/INFOCOM52122.2024.10621139},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QiaoY024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given the high cost associated with directly measuring the traffic matrix (TM), researchers have dedicated decades to devising methods for estimating the complete TM from low-cost link loads by solving a set of heavily ill-posed linear equations. Today’s increasingly intricate networks present an even greater challenge: the routing matrix within these equations can no longer be deemed reliable. To address this challenge, we, for the first time, employ a flow-based generative model for TM estimation by establishing an invertible correlation between TM and link loads, oblivious of the routing matrix. We demonstrate that the lost information within the ill-posed equations can be independently segregated from the TM. Our model collaboratively learns the invertible correlations between TM and link loads as well as the distribution of the lost information. As a result, our model can unbiasedly reverse-transform the link loads to the true TM. Our model has undergone extensive experiments on two real-world datasets. Surprisingly, even without knowledge of the routing matrix, it significantly outperforms six representative baselines in deterministic and noisy routing scenarios regarding estimation accuracy and distribution similarity. Particularly, if the actual routing matrix is absent, our model can improve the performance of the best baseline by 41% ∼ 58%.}
}


@inproceedings{DBLP:conf/infocom/WangYYGL24,
	author = {Xiaojian Wang and
                  Ruozhou Yu and
                  Dejun Yang and
                  Huayue Gu and
                  Zhouyu Li},
	title = {VeriEdge: Verifying and Enforcing Service Level Agreements for Pervasive
                  Edge Computing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2149--2158},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621128},
	doi = {10.1109/INFOCOM52122.2024.10621128},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangYYGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing gained popularity for its promises of low latency and high-quality computing services to users. However, it has also introduced the challenge of mutual untrust between user and edge devices for service level agreement (SLA) compliance. This obstacle hampers wide adoption of edge computing, especially in pervasive edge computing (PEC) where edge devices can freely enter or exit the market, which makes verifying and enforcing SLAs significantly more challenging. In this paper, we propose a framework for verifying and enforcing SLAs in PEC, allowing a user to assess SLA compliance of an edge service and ensure correctness of the service results. Our solution, called VeriEdge, employs a verifiable delayed sampling approach to sample a small number of computation steps, and relies on randomly selected verifiers to verify correctness of the computation results. To make sure the verification process is non-manipulable, we employ verifiable random functions to post-select the verifier(s). A dispute protocol is designed to resolve disputes for potential misbehavior. Rigorous security analysis demonstrates that VeriEdge achieves a high probability of detecting SLA violation with a minimal overhead. Experimental results indicate that VeriEdge is lightweight, practical, and efficient.}
}


@inproceedings{DBLP:conf/infocom/WenL024,
	author = {Dacheng Wen and
                  Yupeng Li and
                  Francis C. M. Lau},
	title = {Augment Online Linear Optimization with Arbitrarily Bad Machine-Learned
                  Predictions},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2159--2168},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621317},
	doi = {10.1109/INFOCOM52122.2024.10621317},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WenL024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The online linear optimization paradigm is important to many real-world network applications as well as theoretical algorithmic studies. Recent studies have made attempts to augment online linear optimization with machine-learned predictions of the cost function that are meant to improve the performance of the algorithms. However, they fail to address the critical case in practical systems where the predictions can be arbitrarily bad. In this work, we take the first step to study the problem of online linear optimization with a dynamic number of arbitrarily bad machine-learned predictions per round and propose an algorithm termed OLOAP. Our theoretical analysis shows that, when the qualities of the predictions are satisfactory, OLOAP achieves a regret bound of O(logT), which circumvents the tight lower bound of Ω(\nT\n−\n−\n√\n) for the vanilla problem of online linear optimization (i.e., the one without any predictions). Meanwhile, the regret of our algorithm is never worse than O(\nT\n−\n−\n√\n) irrespective of the qualities of predictions. In addition, we further derive a lower bound for the regret of the studied problem, which demonstrates that OLOAP is near-optimal. We consider two important network applications and conduct extensive evaluations. Our results validate the superiority of our algorithm over state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/infocom/Jia0HLS24,
	author = {Lianchen Jia and
                  Chao Zhou and
                  Tianchi Huang and
                  Chaoyang Li and
                  Lifeng Sun},
	title = {Dancing with Shackles, Meet the Challenge of Industrial Adaptive Streaming
                  via Offline Reinforcement Learning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2169--2178},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621126},
	doi = {10.1109/INFOCOM52122.2024.10621126},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Jia0HLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive video streaming has been studied for over 10 years and has demonstrated remarkable performance. However, adaptive video streaming is not an independent algorithm but relies on other components of the video system. Consequently, as other components undergo optimization, the gap between the traditional simulator and the real-world system continues to grow which makes the adaptive video streaming algorithm must adapt to these variations.In order to address the challenges facing industrial adaptive video streaming, we introduce a novel offline reinforcement learning framework called Backwave. This framework leverages history logs to reduce the sim-real gap. We propose new metrics based on counterfactual reasoning to evaluate its performance and we integrate expert knowledge to generate valuable data to mitigate the issue of data override. Furthermore, we employ curriculum learning to minimize additional errors.We deployed Backwave on a mainstream commercial short video platform, Kuaishou. In a series of A/B tests conducted nearly one month with over 400M daily watch times, Backwave consistently outperforms prior algorithms. Specifically, Backwave reduces stall time by 0.45% to 8.52% while maintaining comparable video quality and Backwave demonstrates improvements in average play duration by 0.12% to 0.16%, and overall play duration by 0.12% to 0.26%.}
}


@inproceedings{DBLP:conf/infocom/WangZWY0XY24,
	author = {Junyang Wang and
                  Lan Zhang and
                  Junhao Wang and
                  Mu Yuan and
                  Yihang Cheng and
                  Qian Xu and
                  Bo Yu},
	title = {GraphProxy: Communication-Efficient Federated Graph Learning with
                  Adaptive Proxy},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2179--2188},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621171},
	doi = {10.1109/INFOCOM52122.2024.10621171},
	timestamp = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangZWY0XY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated graph learning (FGL) enables multiple participants with distributed but connected graph data to collaboratively train a model in a privacy-preserving way. However, the high communication cost hinder the adoption of FGL in many resource-limited or delay-sensitive applications. In this work, we focus on reducing the communication cost incurred by the transmission of neighborhood information in FGL. We propose to search for local proxies that can play a substitute role as the external neighbors, and develop a novel federated graph learning framework named GraphProxy. GraphProxy utilizes representation similarity and class correlation to select local proxies for external neighbors. And we propose to dynamically adjust the proxy strategy according to the changing representation of nodes during the iterative training process. We also perform a theoretical analysis and show that using a proxy node has a similar influence on training when it is sufficiently similar to the external one. Extensive evaluations show the effectiveness of our design, e.g., GraphProxy can achieve 8× communication efficiency with only 0.14% performance degradation.}
}


@inproceedings{DBLP:conf/infocom/0002ZWWXL24,
	author = {Xutong Liu and
                  Jinhang Zuo and
                  Junkai Wang and
                  Zhiyong Wang and
                  Yuedong Xu and
                  John C. S. Lui},
	title = {Learning Context-Aware Probabilistic Maximum Coverage Bandits: {A}
                  Variance-Adaptive Approach},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2189--2198},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621257},
	doi = {10.1109/INFOCOM52122.2024.10621257},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0002ZWWXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Probabilistic maximum coverage (PMC) is an important framework that can model many network applications, including mobile crowdsensing, content delivery, and task repli¬cation. In PMC, an operator chooses nodes in a graph that can probabilistically cover other nodes, aiming to maximize the total rewards from the covered nodes. To tackle the challenge of unknown parameters in network environments, PMC are studied under the online learning context, i.e., the PMC bandit. However, existing PMC bandits lack context-awareness and fail to exploit valuable contextual information, limiting their efficiency and adaptability in dynamic environments. To address this limitation, we propose a novel context-aware PMC bandit model (C-PMC). C-PMC employs a linear structure to model the mean outcome of each arm, effectively incorporating contextual information and enhancing its applicability to large-scale network systems. Then we design a variance-adaptive contextual combinatorial upper confidence bound algorithm (VAC 2 UCB), which utilizes second-order statistics, specifically variance, to re-weight feedback data and estimate unknown parameters. Our theoretical analysis shows that C-PMC achieves a regret of\nO\n~\n(d\n|V|T\n−\n−\n−\n−\n√\n)\n, independent of the number of edges\n|E|\nand action size K. Finally, we conduct experiments on synthetic and real-world datasets, showing the superior performance of VAC 2 UCB in context-aware mobile crowdsensing and user-targeted content delivery applications.}
}


@inproceedings{DBLP:conf/infocom/ZouW24,
	author = {Rui Zou and
                  Wenye Wang},
	title = {Effi-Ace: Efficient and Accurate Prediction for High-Resolution Spectrum
                  Tenancy},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2199--2208},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621271},
	doi = {10.1109/INFOCOM52122.2024.10621271},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZouW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectrum prediction is a key enabler for the forthcoming coexistence paradigm where various Radio Access Technologies share overlapping radio spectrum, to substantially improve spectrum efficiency in 5G and beyond systems. Although this fundamental issue has received tremendous research attention, existing algorithms are designed for and validated against spectrum usage data in low time-frequency granularities, which causes inevitable errors when applied to spectrum prediction in realistic resolutions. Therefore, in this paper, we redesign three key components along the spectrum prediction pipeline to propose Effi-Ace, an efficient and accurate prediction for high-resolution spectrum tenancy. First, we obtain raw spectrum data in the same resolutions as scheduling, which reflects the actual dynamics of the subject to be predicted. We improve the Deep Q-Network (DQN) prediction algorithm with enhanced experience replay to reduce sample complexity, so that the proposed DQN is more efficient in terms of sample quantities. New prediction features are extracted from high-resolution measurement data to improve prediction accuracy. According to our detailed experiments, the proposed prediction algorithm substantially reduces sample complexity by 88. 9%, and the improvements in prediction accuracy are up to 14%, when compared with various state-of-the-art counterparts.}
}


@inproceedings{DBLP:conf/infocom/MadnaikMS24,
	author = {Aadesh Madnaik and
                  N. Cameron Matson and
                  Karthikeyan Sundaresan},
	title = {Scalable Network Tomography for Dynamic Spectrum Access},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2209--2218},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621172},
	doi = {10.1109/INFOCOM52122.2024.10621172},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/MadnaikMS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile networks have increased spectral efficiency through advanced multiplexing strategies that are coordinated by base stations (BS) in licensed spectrum. However, external interference on clients leads to significant performance degradation during dynamic (unlicensed) spectrum access (DSA). We introduce the notion of network tomography for DSA, whereby clients are transformed into spectrum sensors, whose joint access statistics are measured and used to account for interfering sources. Albeit promising, performing such tomography naively incurs an impractical overhead that scales exponentially with the multiplexing order of the strategies deployed – which will only continue to grow with 5G/6G technologies.To this end, we propose a novel, scalable network tomography framework called NeTo-X that estimates joint client access statistics with just linear overhead, and forms a blue-print of the interference, thus enabling efficient DSA for future networks. NeTo-X’s design incorporates intelligent algorithms that leverage multi-channel diversity and the spatial locality of interference impact on clients to accurately estimate the desired interference statistics from just pair-wise measurements of its clients. The merits of its framework are showcased in the context of resource management and jammer localization applications, where its performance significantly outperforms baseline approaches and closely approximates optimal performance at a scalable overhead.}
}


@inproceedings{DBLP:conf/infocom/Uvaydov0RDM024,
	author = {Daniel Uvaydov and
                  Milin Zhang and
                  Clifton Paul Robinson and
                  Salvatore D'Oro and
                  Tommaso Melodia and
                  Francesco Restuccia},
	title = {Stitching the Spectrum: Semantic Spectrum Segmentation with Wideband
                  Signal Stitching},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2219--2228},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621332},
	doi = {10.1109/INFOCOM52122.2024.10621332},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Uvaydov0RDM024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectrum has become an extremely scarce and congested resource. As a consequence, spectrum sensing enables the coexistence of different wireless technologies in shared spectrum bands. Most existing work requires spectrograms to classify signals. Ultimately, this implies that images need to be continuously created from I/Q samples, thus creating unacceptable latency for real-time operations. In addition, spectrogram-based approaches do not achieve sufficient granularity level as they are based on object detection performed on pixels and are based on rectangular bounding boxes. For this reason, we propose a completely novel approach based on semantic spectrum segmentation, where multiple signals are simultaneously classified and localized in both time and frequency at the I/Q level. Conversely from the state-of-the-art computer vision algorithm, we add non-local blocks to combine the spatial features of signals, and thus achieve better performance. In addition, we propose a novel data generation approach where a limited set of easy-to-collect real-world wireless signals are "stitched together" to generate large-scale, wideband, and diverse datasets. Experimental results obtained on multiple testbeds (including the Arena testbed) using multiple antennas, multiple sampling frequencies, and multiple radios over the course of 3 days show that our approach classifies and localizes signals with a mean intersection over union (IOU) of 96.70% across 5 wireless protocols while performing in real-time with a latency of 2.6 ms. Moreover, we demonstrate that our approach based on non-local blocks achieves 7% more accuracy when segmenting the most challenging signals with respect to the state-of-the-art U-Net algorithm. We will release our 17 GB dataset and code.}
}


@inproceedings{DBLP:conf/infocom/DokeOZZ24,
	author = {Karyn Doke and
                  Blessing Okoro and
                  Amin Zare and
                  Mariya Zheleva},
	title = {{VIA:} Establishing the link between spectrum sensor capabilities
                  and data analytics performance},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2229--2238},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621266},
	doi = {10.1109/INFOCOM52122.2024.10621266},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/DokeOZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automated spectrum analytics inform critical decisions in dynamic spectrum access networks such as (i) how to allocate network resources to clients, (ii) when to enforce penalties due to malicious or disruptive activity, and (iii) how to chart policies for future regulations. The insights gleaned from a spectrum trace, however, are as objective as the trace itself, and artifacts introduced by sensor imperfections or improper configuration will inevitably affect analysis outcomes. Yet, spectrum analytics have been largely developed in isolation from the underlying data collection and are oblivious to sensor-induced artifacts.To address this challenge, we develop VIA, a framework that attributes sensor properties and configuration to spectrum data fidelity, and models the relationship between spectrum analytics performance and data quality. VIA does not require expert input or intervention and can be used to profile the fidelity of unknown sensors. VIA takes as an input a spectrum trace and the sensor configuration, and benchmarks data quality along three dimensions: (i) Veracity, or how truthfully a scan captures spectrum activity, (ii) Intermittency, characterizing the temporal persistence of spectrum scans and (iii) Ambiguity quantifying the likelihood of false detection. We employ VIA to measure the data fidelity of five common sensor platforms. We then predict the outcome of several spectrum analysis tasks including occupancy and transmitter detection, and modulation recognition using both controlled and real-world measurements. We demonstrate high prediction performance with an average mean squared error of 0.0013 across all tasks using both regression and neural network models.}
}


@inproceedings{DBLP:conf/infocom/WangSLWLLS24,
	author = {Tao Wang and
                  Tuo Shi and
                  Xiulong Liu and
                  Jianping Wang and
                  Bin Liu and
                  Yingshu Li and
                  Yechao She},
	title = {Minimizing Latency for Multi-DNN Inference on Resource-Limited CPU-Only
                  Edge Devices},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2239--2248},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621120},
	doi = {10.1109/INFOCOM52122.2024.10621120},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WangSLWLLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite considerable advancements in specialized hardware, the majority of IoT edge devices still rely on CPUs. The burgeoning number of IoT users amplifies the challenges associated with performing multiple Deep Neural Network inferences on these resource-limited, CPU-only edge devices. Existing strategies, including model compression, hardware acceleration, and model partitioning, often involve a trade-off in inference accuracy, are unsuitable due to hardware specificity, or lead to inefficient resource utilization. In response to these challenges, this paper introduces L-PIC (Latency Minimized Parallel Inference on CPU)—a framework expressly devised to optimize resource allocation, decrease inference latency, and maintain result accuracy on CPU-only edge devices. A series of comprehensive experiments have verified the superior efficiency and effectiveness of the L-PIC framework in comparison to the state-of-the-art method. Remarkably, compared to the state-of-the-art method, L-PIC can reduce the inference latency of multi-DNN by an average of approximately 30% across all tested scenarios.}
}


@inproceedings{DBLP:conf/infocom/0001H0FH24,
	author = {Tao Ren and
                  Zheyuan Hu and
                  Jianwei Niu and
                  Weikun Feng and
                  Hang He},
	title = {M\({}^{\mbox{3}}\)OFF: Module-Compositional Model-Free Computation
                  Offloading in Multi-Environment {MEC}},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2249--2258},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621299},
	doi = {10.1109/INFOCOM52122.2024.10621299},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001H0FH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computation offloading is one of the key issues in mobile edge computing (MEC) that alleviates the tension between user equipment's limited capabilities and mobile application's high requirements. To achieve model-free computation offloading when reliable MEC dynamics are unavailable, deep reinforcement learning (DRL) has become a popular methodology. However, most existing DRL-based offloading approaches are developed for a single MEC environment, with invariant system bandwidth, edge capability, task types, etc., while realistic MEC scenarios tend to be of high diversity. Unfortunately, in multi-MEC environments, DRL-based offloading faces at least two challenges, learning inefficiency and interference of offloading experiences. To address the challenges, we propose a DRL-based Multi-environmental Module-compositional Modelfree computation OFFloading (M 3 OFF) framework. M 3 OFF generates offloading policies using module composition instead of a single DRL network so that learning efficiency could be improved by reusing the same modules and learning interference could be reduced by composing different modules. Furthermore, we design multiple module composition-specific training methods for M 3 OFF, including alternate modules-and-composer updates to improve training stability, loss-regularization to avoid module degeneration, and module-dropout to mitigate overfitting. Extensive experimental results on both simulation and testbed demonstrate that M 3 OFF outperforms the performances of most state-of-the-arts in multi-MEC and reaches close to single-MEC.}
}


@inproceedings{DBLP:conf/infocom/Li0QTZ24,
	author = {Yuepeng Li and
                  Lin Gu and
                  Zhihao Qu and
                  Lifeng Tian and
                  Deze Zeng},
	title = {On Efficient Zygote Container Planning and Task Scheduling for Edge
                  Native Application Acceleration},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2259--2268},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621106},
	doi = {10.1109/INFOCOM52122.2024.10621106},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Li0QTZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge native applications usually consist of several dependent tasks encapsulated in containers and started on-demand in the edge cloud. Unfortunately, the application performance is deeply affected by the notorious cold startup problem of containers. Pre-warming Zygote container pre-imported certain common packages has been proven as an effective startup acceleration solution. Since a Zygote can be shared among colocated tasks that require identical common packages, not only the Zygote planning but also the task scheduling decisions shall be carefully made to maximize the benefit of the Zygotes pre-warmed in limited memory. Additionally, task dependency necessitates co-locating highly dependent tasks on the same server, naturally raising a dilemma in task scheduling. To this end, in this paper, we investigate the problem of how to plan Zygote and schedule tasks for application completion time minimization, which is proved to be NP-hard. We further propose a Priority and Popularity (P&P) based edge native application acceleration algorithm. Both theoretical analysis and extensive experiments demonstrate the effectiveness of our proposed algorithm. The experiment results show that P&P can reduce the application completion time by 11.7%.}
}


@inproceedings{DBLP:conf/infocom/LiuC024,
	author = {Chang Liu and
                  Terence Jie Chua and
                  Jun Zhao},
	title = {Optimization for the Metaverse over Mobile Edge Computing with Play
                  to Earn},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2269--2278},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621355},
	doi = {10.1109/INFOCOM52122.2024.10621355},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuC024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The concept of the Metaverse has garnered growing interest from both academic and industry circles. The decentralization of both the integrity and security of digital items has spurred the popularity of play-to-earn (P2E) games, where players are entitled to earn and own digital assets which they may trade for physical-world currencies. However, these computationally-intensive games are hardly playable on resource-limited mobile devices and the computational tasks have to be offloaded to an edge server. Through mobile edge computing (MEC), users can upload data to the Metaverse Service Provider (MSP) edge servers for computing. Nevertheless, there is a trade-off between user-perceived in-game latency and user visual experience. The downlink transmission of lower-resolution videos lowers user-perceived latency while lowering the visual fidelity and consequently, earnings of users. In this paper, we design a method to enhance the Metaverse-based mobile augmented reality (MAR) in-game user experience. Specifically, we formulate and solve a multi-objective optimization problem. Given the inherent NP-hardness of the problem, we present a low-complexity algorithm to address it, mitigating the trade-off between delay and earnings. The experiment results show that our method can effectively balance the user-perceived latency and profitability, thus improving the performance of Metaverse-based MAR systems.}
}


@inproceedings{DBLP:conf/infocom/LiSHQKT24,
	author = {Chengqi Li and
                  Sarah Simionescu and
                  Wenbo He and
                  Sanzheng Qiao and
                  Nadjia Kara and
                  Chamseddine Talhi},
	title = {Utility-Preserving Face Anonymization via Differentially Private Feature
                  Operations},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2279--2288},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621407},
	doi = {10.1109/INFOCOM52122.2024.10621407},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiSHQKT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Facial images play a crucial role in many web and security applications, but their uses come with notable privacy risks. Despite the availability of various face anonymization algorithms, they often fail to withstand advanced attacks while struggling to maintain utility for subsequent applications. We present two novel face anonymization algorithms that utilize feature operations to overcome these limitations. The first algorithm utilizes perturbation and matching of high-level features, whereas the second algorithm enhances this approach by also incorporating perturbation of low-level features along with regularization. These algorithms significantly enhance the utility of anonymized images while ensuring differential privacy. Additionally, we introduce a task-based benchmark to enable fair and comprehensive evaluations of privacy and utility across different algorithms. Through experiments, we demonstrate that our algorithms outperform others in preserving the utility of anonymized facial images in classification tasks while effectively protecting against a wide range of attacks.}
}


@inproceedings{DBLP:conf/infocom/Wang00LH24,
	author = {Mengyuan Wang and
                  Hongbo Jiang and
                  Peng Peng and
                  Youhuan Li and
                  Wenbin Huang},
	title = {Toward Accurate Butterfly Counting with Edge Privacy Preserving in
                  Bipartite Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2289--2298},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621436},
	doi = {10.1109/INFOCOM52122.2024.10621436},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Wang00LH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Butterfly counting is widely used to analyze bipartite networks, but counting butterflies in original bipartite networks can reveal sensitive data and pose a risk of individual privacy, specifically edge privacy. Current privacy notions do not fully address the needs of both user-user and user-item bipartite networks. In this paper, we propose a novel privacy notion, edge decentralized differential privacy (edge DDP), which preserves edge privacy in any bipartite network. We also design the randomized edge protocol (REP) to perturb real edges in bipartite networks. However, a significant amount of noise in perturbed bipartite networks often leads to an overcount of butterflies. To achieve accurate butterfly counting, we design the randomized group protocol (RGP) to reduce noise. By combining REP and RGP, we propose a two-phase framework called butterfly counting in limitedly synthesized bipartite networks (BC-LimBN) to synthesize networks for accurate butterfly counting. BC-LimBN has been rigorously proven to satisfy edge DDP. Our experiments on various datasets confirm the high accuracy of BC-LimBN in butterfly counting and its superiority over competitors, with a mean relative error of less than 10% at most. Furthermore, our experiments show that BC-LimBN has a low time cost, requiring only a few seconds on our datasets.}
}


@inproceedings{DBLP:conf/infocom/ZhangYM24,
	author = {Linxi Zhang and
                  Xuke Yan and
                  Di Ma},
	title = {Efficient and Effective In-Vehicle Intrusion Detection System using
                  Binarized Convolutional Neural Network},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2299--2307},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621400},
	doi = {10.1109/INFOCOM52122.2024.10621400},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhangYM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern vehicles are equipped with multiple Electronic Control Units (ECUs) communicating over in-vehicle networks such as Controller Area Network (CAN). Inherent security limitations in CAN necessitate the use of Intrusion Detection Systems (IDSs) for protection against potential threats. While some IDSs leverage advanced deep learning to improve accuracy, issues such as long processing time and large memory size remain. Existing Binarized Neural Network (BNN)-based IDSs, proposed as a solution for efficiency, often compromise on accuracy. To this end, we introduce a novel Binarized Convolutional Neural Network (BCNN)-based IDS, designed to exploit the temporal and spatial characteristics of CAN messages to achieve both efficiency and detection accuracy. In particular, our approach includes a novel input generator capturing temporal and spatial correlations of messages, aiding model learning and ensuring high-accuracy performance. Experimental results suggest our IDS effectively reduces memory utilization and detection latency while maintaining high detection rates. Our IDS runs 4 times faster and utilizes only 3.3% of the memory space required by a full-precision CNN-based IDS. Meanwhile, our proposed system demonstrates a detection accuracy between 94.19% and 96.82% relative to the CNN-based IDS across different attack scenarios. This performance marks a noteworthy improvement over existing state-of-the-art BNN-based IDS designs.}
}


@inproceedings{DBLP:conf/infocom/SharmaAGSS024,
	author = {Pragya Sharma and
                  Tolga O. Atalay and
                  Hans{-}Andrew Gibbs and
                  Dragoslav Stojadinovic and
                  Angelos Stavrou and
                  Haining Wang},
	title = {5G-WAVE: {A} Core Network Framework with Decentralized Authorization
                  for Network Slices},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2308--2317},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621131},
	doi = {10.1109/INFOCOM52122.2024.10621131},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SharmaAGSS024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G mobile networks leverage Network Function Virtualization (NFV) to offer services in the form of network slices. Each network slice is a logically isolated fragment constructed by service chaining a set of Virtual Network Functions (VNFs). The Network Repository Function (NRF) acts as a central OpenAuthorization (OAuth) 2.0 server to secure inter-VNF communications resulting in a single point of failure. Thus, we propose 5G-WAVE, a decentralized authorization framework for the 5G core by leveraging the WAVE framework and integrating it into the OpenAirInterface (OAI) 5G core. Our design relies on Side-Car Proxies (SCPs) deployed alongside individual VNFs, allowing point-to-point authorization. Each SCP acts as a WAVE engine to create entities and attestations and verify incoming service requests. We measure the authorization latency overhead for VNF registration, 5G Authentication and Key Agreement (AKA), and data session setup and observe that WAVE verification introduces 155ms overhead to HTTP transactions for decentralizing authorization. Additionally, we evaluate the scalability of 5G-WAVE by instantiating more network slices to observe 1.4x increase in latency with 10x growth in network size. We also discuss how 5G-WAVE can significantly reduce the 5G attack surface without using OAuth 2.0 while addressing several key issues of 5G standardization.}
}


@inproceedings{DBLP:conf/infocom/TanakaU0AKS24,
	author = {Kenji Tanaka and
                  Takashi Uchida and
                  Yuki Matsuda and
                  Yuki Arikawa and
                  Shinya Kaji and
                  Takeshi Sakamoto},
	title = {Transparent Broadband {VPN} Gateway: Achieving 0.39 Tbps per Tunnel
                  with Bump-in-the-Wire},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2318--2327},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621086},
	doi = {10.1109/INFOCOM52122.2024.10621086},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/TanakaU0AKS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The demand for virtual private networks (VPNs) that provide confidentiality, integrity, and authenticity of communications is growing every year. IPsec is one of the oldest and most widely used VPN protocols, implemented between the internet protocol (IP) layer and the data link layer of the Linux kernel. This implementation method, known as bump-in-the-stack, has the advantage of being able to transparently apply IPsec to traffic without changing the application. However, its throughput efficiency (Gbps/core) is worse than regular Linux communication. Therefore, we chose the bump-in-the-wire (BITW) architecture, which handles IPsec in hardware separate from the host. Our proposed BITW architecture consists of inline cryptographic accelerators implemented in field-programmable gate arrays and a programmable switch that connects multiple such accelerators. A VPN gateway implemented with our architecture is transparent and improves the throughput efficiency by 3.51 times and power efficiency by 3.40 times over a VPN gateway implemented in the Linux kernel. It also demonstrates excellent scalability, and has been confirmed to scale to a maximum of 386.24 Gbps per tunnel, exceeding state-of-the-art technology in maximum throughput and efficiency per tunnel. In multiple-tunnels use cases, the proposed architecture improves the energy efficiency by 2.49 times.}
}


@inproceedings{DBLP:conf/infocom/LiuZL24,
	author = {Qiong Liu and
                  Tianzhu Zhang and
                  Leonardo Linguaglossa},
	title = {Non-invasive performance prediction of high-speed softwarized network
                  services with limited knowledge},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2328--2337},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621097},
	doi = {10.1109/INFOCOM52122.2024.10621097},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern telco networks have experienced a significant paradigm shift in the past decade, thanks to the proliferation of network softwarization. Despite the benefits of softwarized networks, the constituent software data planes cannot always guarantee predictable performance due to resource contentions in the underlying shared infrastructure. Performance predictions are thus paramount for network operators to fulfill Service-Level Agreements (SLAs), especially in high-speed regimes (e.g., Gigabit or Terabit Ethernet). Existing solutions heavily rely on in-band feature collection, which imposes non-trivial engineering and data-path overhead. This paper proposes a non-invasive performance prediction approach, which complements state-of-the-art solutions by measuring and analyzing low-level features ubiquitously available in the network infrastructure. Accessing these features does not hamper the packet data path. Our approach does not rely on prior knowledge of the input traffic, VNFs’ internals, and system details. We show that (i) low-level hardware features exposed by the NFV infrastructure can be collected and interpreted for performance issues, (ii) predictive models can be derived with classical ML algorithms, (iii) and can be used to predict performance impairments in real NFV systems accurately. Our code and datasets are publicly available 1 .}
}


@inproceedings{DBLP:conf/infocom/ChengG0S0024,
	author = {Zhongyi Cheng and
                  Guoju Gao and
                  He Huang and
                  Yu{-}e Sun and
                  Yang Du and
                  Haibo Wang},
	title = {BurstDetector: Real-Time and Accurate Across-Period Burst Detection
                  in High-Speed Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2338--2347},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621114},
	doi = {10.1109/INFOCOM52122.2024.10621114},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChengG0S0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic measurement provides essential information for various network services. Burst is a common phenomenon in high-speed network streams, which manifests as a surge in the number of incoming packets in a flow. We propose a new definition named across-period burst, considering the change not in two adjacent time windows but in two groups of windows with time continuity. The across-period burst definition can better capture the continuous changes of flows in high-speed networks. To achieve real-time burst detection with high accuracy and low memory consumption, we propose a novel sketch named BurstDetector, which consists of two stages. Stage 1 excludes those flows that will not become burst flows, while Stage 2 accurately records the information of the potential burst flows and carries out across-period burst detections at the end of every time window. We further propose an optimization called Hierarchical Cell, which can improve the memory utilization of BurstDetector. In addition, we analyze the estimation accuracy and time complexity of BurstDetector. Extensive experiments based on real-world datasets show that our BurstDetector can achieve at least 2.8 times as much detection accuracy and processing throughput as some existing algorithms.}
}


@inproceedings{DBLP:conf/infocom/QiaoZ024,
	author = {Yi Qiao and
                  Han Zhang and
                  Jilong Wang},
	title = {NetFEC: In-network {FEC} Encoding Acceleration for Latency-sensitive
                  Multimedia Applications},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2348--2357},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621183},
	doi = {10.1109/INFOCOM52122.2024.10621183},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/QiaoZ024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In face of packet loss, latency-sensitive multimedia applications cannot afford re-transmission because loss detection and re-transmission could lead to extra latency or otherwise compromised media quality. Alternatively, forward error correction (FEC) ensures reliability by adding redundancy and it is able to achieve lower latency at the cost of bandwidth and computational overheads. We propose to re-locate FEC encoding to hardware that better suits the computational pattern of FEC encoding than CPUs. In this paper, we present NetFEC, an in-network acceleration system that offloads the entire FEC encoding process on the emergent programmable switching ASICs, eliminating all CPU involvement. We design the ghost packet mechanism so that NetFEC can be compatible with important media transport functionalities, including congestion control, pacing and statistics. We integrate NetFEC with WebRTC and conduct extensive experiments with real hardwares. Our evaluations demonstrate that NetFEC is able to eliminate server CPU burden and adds negligible overheads.}
}


@inproceedings{DBLP:conf/infocom/CuiSZ0H24,
	author = {Kaiyan Cui and
                  Leming Shen and
                  Yuanqing Zheng and
                  Fu Xiao and
                  Jinsong Han},
	title = {Talk2Radar: Talking to mmWave Radars via Smartphone Speaker},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2358--2367},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621296},
	doi = {10.1109/INFOCOM52122.2024.10621296},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/CuiSZ0H24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrated Sensing and Communication (ISAC) is gaining a tremendous amount of attention from both academia and industry. Recent work has brought communication capability to sensing-oriented mmWave radars, enabling more innovative applications. These solutions, however, either require hardware modifications or suffer from limited data rates. This paper presents Talk2Radar, which builds a faster communication channel between smartphone speakers and mmWave radars, without any hardware modification to either commodity smartphones or off-the-shelf radars. In Talk2Radar, a smartphone speaker sends messages by playing carefully designed sounds. A mmWave radar acting as a data receiver captures the emitted sounds by detecting the sound-induced smartphone vibrations, and then decodes the messages. Talk2Radar characterizes smartphone speakers for speaker-to-mmWave radar communication and addresses a series of technical challenges, including modulation and demodulation of extremely weak sound-induced vibrations, multi-speaker concurrent communication and human motion suppression. We implement and evaluate Talk2Radar in various practical settings. Experimental results show that Talk2Radar can achieve a data rate of up to 400bps with an average BER of less than 5%, outperforming the state-of-the-art by approximately 33×.}
}


@inproceedings{DBLP:conf/infocom/LiSJYI24,
	author = {Yuanyuan Li and
                  Lili Su and
                  Carlee Joe{-}Wong and
                  Edmund Yeh and
                  Stratis Ioannidis},
	title = {Distributed Experimental Design Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2368--2377},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621241},
	doi = {10.1109/INFOCOM52122.2024.10621241},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiSJYI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As edge computing capabilities increase, model learning deployments in diverse edge environments have emerged. In experimental design networks, introduced recently, network routing and rate allocation are designed to aid the transfer of data from sensors to heterogeneous learners. We design efficient experimental design network algorithms that are (a) distributed and (b) use multicast transmissions. This setting poses significant challenges as classic decentralization approaches often operate on (strictly) concave objectives under differentiable constraints. In contrast, the problem we study here has a non-convex, continuous DR-submodular objective, while multicast transmissions naturally result in non-differentiable constraints. From a technical standpoint, we propose a distributed Frank-Wolfe and a distributed projected gradient ascent algorithm that, coupled with a relaxation of non-differentiable constraints, yield allocations within a 1 − 1/e factor from the optimal. Numerical evaluations show that our proposed algorithms outperform competitors with respect to model learning quality.}
}


@inproceedings{DBLP:conf/infocom/FezeuFRCCT0WZ24,
	author = {Rostand A. K. Fezeu and
                  Claudio Fiandrino and
                  Eman Ramadan and
                  Jason Carpenter and
                  Daqing Chen and
                  Yiling Tan and
                  Feng Qian and
                  Joerg Widmer and
                  Zhi{-}Li Zhang},
	title = {Roaming across the European Union in the 5G Era: Performance, Challenges,
                  and Opportunities},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2378--2387},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621234},
	doi = {10.1109/INFOCOM52122.2024.10621234},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/FezeuFRCCT0WZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Roaming provides users with voice and data connectivity when traveling abroad. This is particularly the case in Europe where the "Roam like Home" policy established by the European Union in 2017 has made roaming affordable. Nonetheless, due to various policies employed by operators, roaming can incur considerable performance penalties as shown in past studies of 3G/4G networks. As 5G provides significantly higher bandwidth, how does roaming affect user-perceived performance? We present, to the best of our knowledge, the first comprehensive and comparative measurement study of commercial 5G in four European countries.Our measurement study is unique in the way it makes it possible to link key 5G mid-band channels and configuration parameters ("policies") used by various operators in these countries with their effect on the observed 5G performance from the network (in particular, the physical and MAC layers) and applications perspectives. Our measurement study not only portrays users’ observed quality of experience when roaming, but also provides guidance to optimize the network configuration and to users and application developers in choosing mobile operators. Moreover, our contribution provides the research community with the largest cross-country roaming 5G dataset to stimulate further research.}
}


@inproceedings{DBLP:conf/infocom/ChengNN24,
	author = {Jiaming Cheng and
                  Duong Thuy Anh Nguyen and
                  Duong Tung Nguyen},
	title = {Two-Stage Distributionally Robust Edge Node Placement Under Endogenous
                  Demand Uncertainty},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2388--2397},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621372},
	doi = {10.1109/INFOCOM52122.2024.10621372},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChengNN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing (EC) promises to deliver low-latency and ubiquitous computation to numerous devices at the network edge. This paper aims to jointly optimize edge node (EN) placement and resource allocation for an EC platform, considering demand uncertainty. Diverging from existing approaches treating uncertainties as exogenous, we propose a novel two-stage decision-dependent distributionally robust optimization (DRO) framework to effectively capture the interdependence between EN placement decisions and uncertain demands. The first stage involves making EN placement decisions, while the second stage optimizes resource allocation after uncertainty revelation. We present an exact mixed-integer linear program reformulation for solving the underlying "min-max-min" two-stage model. We further introduce a valid inequality method to enhance computational efficiency, especially for large-scale networks. Extensive numerical experiments demonstrate the benefits of considering endogenous uncertainties and the advantages of the proposed model and approach.}
}


@inproceedings{DBLP:conf/infocom/GyorgyiL0S24,
	author = {Csaba Gy{\"{o}}rgyi and
                  Kim G. Larsen and
                  Stefan Schmid and
                  Jir{\'{\i}} Srba},
	title = {SyPer: Synthesis of Perfectly Resilient Local Fast Re-Routing Rules
                  for Highly Dependable Networks},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2398--2407},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621323},
	doi = {10.1109/INFOCOM52122.2024.10621323},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/GyorgyiL0S24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern communication networks support local fast re-routing (FRR) to quickly react to link failures. However, configuring such FRR mechanisms is challenging as the rules have to be defined ahead of time, without knowledge of the failures, and can depend only on local decisions made by the nodes incident to a failed link. Designing failover protection against multiple link failures is particularly difficult. We present a novel synthesis approach which addresses this challenge by generating FRR rules in an automated and provably correct manner. Our network model assumes that each node maintains a prioritised list of backup links (a.k.a. skipping forwarding)—an FRR method that allows for a memory-efficient deployment. We study the theoretical properties of the model and implement a synthesis method in our tool SyPer that aims to provide perfect resilience: if there are up to k link failures, we can always route traffic between any two nodes as long as they are still connected in the underlying physical network. To this end, SyPer focuses on the synthesis of efficient forwarding rules using the BDD (binary decision diagram) methodology and our empirical evaluation shows that SyPer is feasible, and can synthesize robust network configuration in realistic settings.}
}


@inproceedings{DBLP:conf/infocom/Qin0GCWZ0024,
	author = {Zhen Qin and
                  Zeyu Yang and
                  Yangyang Geng and
                  Xin Che and
                  Tianyi Wang and
                  Hengye Zhu and
                  Peng Cheng and
                  Jiming Chen},
	title = {Reverse Engineering Industrial Protocols Driven By Control Fields},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2408--2417},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621405},
	doi = {10.1109/INFOCOM52122.2024.10621405},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Qin0GCWZ0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Industrial protocols are widely used in Industrial Control Systems (ICSs) to network physical devices, thus playing a crucial role in securing ICSs. However, most commercial industrial protocols are proprietary and owned by their vendors, which impedes the implementation of protections against cyber threats. In this paper, we design REInPro to Reverse Engineer Industrial Protocols. REInPro is inspired by the fact that the structure of industrial protocols can be determined by a particular field referred to control field. By applying a probabilistic model of network traffic behavior, REInPro automatically identifies the control field and groups the associated network traffic into clusters. REInPro then infers critical semantics of industrial protocols by differentiating the features of corresponding protocol fields. We have experimentally implemented and evaluated REInPro using 8 different industrial protocols across 6 Programmable Logic Controllers (PLCs) belonging to 5 original equipment manufacturers. The experimental results show REInPro to reverse-engineer the formats and semantics of industrial protocols with an average correctness/perfection of 0.70/0.58 and 0.96/0.39.}
}


@inproceedings{DBLP:conf/infocom/00050TW024,
	author = {Shan Jiang and
                  Jiannong Cao and
                  Cheung Leong Tung and
                  Yuqin Wang and
                  Shan Wang},
	title = {Sharon: Secure and Efficient Cross-shard Transaction Processing via
                  Shard Rotation},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2418--2427},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621394},
	doi = {10.1109/INFOCOM52122.2024.10621394},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/00050TW024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, sharding has become a popular direction to scale out blockchain systems by dividing the network into shards that process transactions in parallel. However, secure and efficient cross-shard transaction processing remains a vital and unaddressed challenge. Existing work handles a cross-shard transaction via transaction division: dividing it into sub-transactions, processing them separately, and combing the processing results. Such an approach is unfavorable for decentralized blockchain due to its reliance on trustworthy parties, e.g., the client or a reference node, to perform the transaction division and result combination. Furthermore, the processing result of one transaction can affect another, violating the important property of transaction isolation. In this work, we propose Sharon, a novel sharding protocol that processes cross-shard transactions via shard rotation rather than transaction division. In Sharon, shards rotate to merge pairwisely and process cross-shard transactions when merged. Sharon eliminates reliance on trustworthy parties and provides transaction isolation in nature because transactions are no longer divided. Nevertheless, it poses a scientific question of when and how to merge the shards to improve system performance. To answer the question, we formally define the shard scheduling problem to minimize transaction confirmation latency and propose a novel construction algorithm. The proposed algorithm is proven optimal and runs in polynomial time. We conduct extensive experiments on Amazon EC2 instances using Bitcoin and Ethereum data. The results indicate that Sharon achieves nearly linear scalability, improves the system throughput by 139%, and saves the transaction processing latency by 72.4% compared with state-of-the-art approaches.}
}


@inproceedings{DBLP:conf/infocom/Ding024,
	author = {Wenlong Ding and
                  Hong Xu},
	title = {Dynamic Learning-based Link Restoration in Traffic Engineering with
                  Archie},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2428--2437},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621357},
	doi = {10.1109/INFOCOM52122.2024.10621357},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Ding024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fiber cuts reduce network capacity and take a long time to fix in optical wide-area networks. It is important to select the best restoration plan that minimizes throughput loss by reconfiguring wavelengths on remaining healthy fibers for affected IP links. Recent work studies optimal restoration plan or ticket selection problem in traffic engineering (TE) in a one-shot setting of only one TE interval (5 minutes). Since fiber repair often takes hours, in this work, we extend to consider restoration ticket selection with traffic dynamics over multiple intervals.To balance restoration performance with reconfiguration overhead, we perform dynamic ticket selection every T time steps. We propose an end-to-end learning approach to solve this T-step ticket selection problem as a classification task, combining traffic trend extraction and ticket selection in the same learning model. It uses convolution LSTM network to extract temporal and spatial features from past demand matrices to determine the ticket most likely to perform well T steps down the road, without predicting future traffic or solving any TE optimization. Trace-driven simulation shows that our new TE system, Archie, reduces over 25% throughput loss and is over 3500x faster than conventional demand prediction approach, which requires solving TE many times.}
}


@inproceedings{DBLP:conf/infocom/Chen0B24,
	author = {Evan Chen and
                  Shiqiang Wang and
                  Christopher G. Brinton},
	title = {Taming Subnet-Drift in D2D-Enabled Fog Learning: {A} Hierarchical
                  Gradient Tracking Approach},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2438--2447},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621133},
	doi = {10.1109/INFOCOM52122.2024.10621133},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Chen0B24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) encounters scalability challenges when implemented over fog networks. Semi-decentralized FL (SD-FL) proposes a solution that divides model cooperation into two stages: at the lower stage, device-to-device (D2D) communications is employed for local model aggregations within subnetworks (subnets), while the upper stage handles device-server (DS) communications for global model aggregations. However, existing SD-FL schemes are based on gradient diversity assumptions that become performance bottlenecks as data distributions become more heterogeneous. In this work, we develop semi-decentralized gradient tracking (SD-GT), the first SD-FL methodology that removes the need for such assumptions by incorporating tracking terms into device updates for each communication layer. Analytical characterization of SD-GT reveals convergence upper bounds for both non-convex and strongly-convex problems, for a suitable choice of step size. We employ the resulting bounds in the development of a co-optimization algorithm for optimizing subnet sampling rates and D2D rounds according to a performance-efficiency trade-off. Our subsequent numerical evaluations demonstrate that SD-GT obtains substantial improvements in trained model quality and communication cost relative to baselines in SD-FL and gradient tracking on several datasets.}
}


@inproceedings{DBLP:conf/infocom/ZhouPWH0024,
	author = {Yajie Zhou and
                  Xiaoyi Pang and
                  Zhibo Wang and
                  Jiahui Hu and
                  Peng Sun and
                  Kui Ren},
	title = {Towards Efficient Asynchronous Federated Learning in Heterogeneous
                  Edge Environments},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2448--2457},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621333},
	doi = {10.1109/INFOCOM52122.2024.10621333},
	timestamp = {Wed, 21 Aug 2024 07:35:23 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhouPWH0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is widely used in edge environments as a privacy-preserving collaborative learning paradigm. However, edge devices often have heterogeneous computation capabilities and data distributions, hampering the efficiency of co-training. Existing works develop staleness-aware semi-asynchronous FL that reduces the contribution of slow devices to the global model to mitigate their negative impacts. But this makes data on slow devices unable to be fully leveraged in global model updating, exacerbating the effects of data heterogeneity. In this paper, to cope with both system and data heterogeneity, we propose a clustering and two-stage aggregation-based Efficient Asynchronous Federated Learning (EAFL) framework, which can achieve better learning performance with higher efficiency in heterogeneous edge environments. In EAFL, we first propose a gradient similarity-based dynamic clustering mechanism to cluster devices with similar system and data characteristics together dynamically during the training process. Then, we develop a novel two-stage aggregation strategy consisting of staleness-aware semi-asynchronous intra-cluster aggregation and data size-aware synchronous inter-cluster aggregation to efficiently and comprehensively aggregate training updates across heterogeneous clusters. With that, the negative impacts of slow devices and Non-IID data can be simultaneously alleviated, thus achieving efficient collaborative learning. Extensive experiments demonstrate that EAFL is superior to state-of-the-art methods.}
}


@inproceedings{DBLP:conf/infocom/YuL24,
	author = {Haoran Yu and
                  Fan Li},
	title = {Personalized Prediction of Bounded-Rational Bargaining Behavior in
                  Network Resource Sharing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2458--2467},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621429},
	doi = {10.1109/INFOCOM52122.2024.10621429},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/YuL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There have been many studies leveraging bargaining to incentivize the sharing of network resources between resource owners and seekers. They predicted bargaining behavior and outcomes mainly by assuming that bargainers are fully rational and possess sufficient knowledge about their opponents. Our work addresses the prediction of bargaining behavior in network resource sharing scenarios where these assumptions do not hold, i.e., bargainers are bounded-rational and have heterogeneous knowledge. Our first key idea is using a multi-output Long Short-Term Memory (LSTM) neural network to learn bargainers’ behavior patterns and predict both their discrete and continuous decisions. Our second key idea is assigning a unique latent vector to each bargainer, characterizing the heterogeneity among bargainers. We propose a scheme to jointly learn the LSTM weights and latent vectors from real bargaining data, and utilize them to achieve a personalized behavior prediction. We prove that estimating our LSTM weights corresponds to a special design of LSTM training, and also theoretically characterize the performance of our scheme. To deal with large-scale datasets in practice, we further propose a variant of our scheme to accelerate the LSTM training. Experiments on a large real-world bargaining dataset demonstrate that our schemes achieve more accurate personalized predictions than baselines.}
}


@inproceedings{DBLP:conf/infocom/LiuLCZ0024,
	author = {Xiaochen Liu and
                  Fan Li and
                  Yetong Cao and
                  Shengchun Zhai and
                  Song Yang and
                  Yu Wang},
	title = {PPGSpotter: Personalized Free Weight Training Monitoring Using Wearable
                  {PPG} Sensor},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2468--2477},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621212},
	doi = {10.1109/INFOCOM52122.2024.10621212},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/LiuLCZ0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Free weight training (FWT) is of utmost importance for physical well-being. However, the success of FWT depends on choosing the suitable workload, as improper selections can lead to suboptimal outcomes or injury. Current workload estimation approaches rely on manual recording and specialized equipment with limited feedback. Therefore, we introduce PPGSpotter, a novel PPG-based system for FWT monitoring in a convenient, low-cost, and fine-grained manner. By characterizing the arterial geometry compressions caused by the deformation of distinct muscle groups during various exercises and workloads in PPG signals, PPGSpotter can infer essential FWT factors such as workload, repetitions, and exercise type. To remove pulse-related interference that heavily contaminates PPG signals, we develop an arterial interference elimination approach based on adaptive filtering, effectively extracting the pure motion-derived signal (MDS). Furthermore, we explore 2D representations within the phase space of MDS to extract spatiotemporal information, enabling PPGSpotter to address the challenge of resisting sensor shifts. Finally, we leverage a multi-task CNN-based model with workload adjustment guidance to achieve personalized FWT monitoring. Extensive experiments with 15 participants confirm that PPGSpotter can achieve workload estimation (0.59 kg RMSE), repetitions estimation (0.96 reps RMSE), and exercise type recognition (91.57% F1-score) while providing valid workload adjustment recommendations.}
}


@inproceedings{DBLP:conf/infocom/0001LCC24,
	author = {Seonghoon Park and
                  Jeho Lee and
                  Yonghun Choi and
                  Hojung Cha},
	title = {Vulture: Cross-Device Web Experience with Fine-Grained Graphical User
                  Interface Distribution},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2478--2487},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621433},
	doi = {10.1109/INFOCOM52122.2024.10621433},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/0001LCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a cross-device web solution, called Vulture, which distributes graphical user interface (GUI) elements of apps across multiple devices without requiring modifications of web apps or browsers. Several challenges should be resolved to achieve the goals. First, the peer–server configuration should be efficiently established to distribute web resources in cross-device web environments. Vulture exploits an in-browser virtual proxy that runs the web server’s functionality in web browsers using a virtual HTTP scheme and a relevant API. Second, the functional consistency of web apps must be ensured in GUI-distributed environments. Vulture solves this challenge by providing a single-browser illusion with a two-tier document object models (DOM) architecture, which handles view state changes and user input seamlessly in cross-device environments. We implemented Vulture and extensively evaluated the system under various combinations of operating platforms, devices, and network capabilities while running 50 real web apps. The experiment results show that the proposed scheme provides functionally consistent cross-device web experiences by allowing fine-grained GUI distribution. We also confirmed that the in-browser virtual proxy reduces the GUI distribution time and the view change reproduction time by averages of 38.47% and 20.46%, respectively.}
}


@inproceedings{DBLP:conf/infocom/CaiLSZZCZ0024,
	author = {Jiayi Cai and
                  Hang Lin and
                  Tingxin Sun and
                  Zhengyan Zhou and
                  Longlong Zhu and
                  Haodong Chen and
                  Jiajia Zhou and
                  Dong Zhang and
                  Chunming Wu},
	title = {OpenINT: Dynamic In-band Network Telemetry with Lightweight Deployment
                  and Flexible Planning},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2488--2497},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621221},
	doi = {10.1109/INFOCOM52122.2024.10621221},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/CaiLSZZCZ0024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The normal operation of data center network management tasks relies on accurate measurement of the network status. In-band Network Telemetry (INT) leverages programmable data planes to provide fine-grained and accurate network status. However, existing INT-related works have not considered the telemetry data required for dynamic adjustments of INT under uninterrupted conditions, including additions, deletions, and modifications. To address this issue, this paper proposes OpenINT, a lightweight and flexible In-band Network Telemetry system. The key innovation of OpenINT lies in decoupling telemetry operations in the data plane, using three generic sub-modules to achieve lightweight telemetry. Meanwhile, the control plane utilizes heuristic algorithms for dynamic planning to achieve near-optimal telemetry paths. Additionally, OpenINT provides primitives for defining network measurement tasks, which abstract the underlying telemetry architecture’s details, enabling network operator to conveniently access network status. A prototype of OpenINT is implemented on a programmable switch equipped with the Tofino chip. Experimental results demonstrate that OpenINT achieves highly flexible dynamic telemetry and significantly reduces network overhead.}
}


@inproceedings{DBLP:conf/infocom/Yue0ZT024,
	author = {Xiaofei Yue and
                  Song Yang and
                  Liehuang Zhu and
                  Stojan Trajanovski and
                  Xiaoming Fu},
	title = {Demeter: Fine-grained Function Orchestration for Geo-distributed Serverless
                  Analytics},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2498--2507},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621303},
	doi = {10.1109/INFOCOM52122.2024.10621303},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Yue0ZT024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the era of global services, low-latency analytics on large-volume geo-distributed data has been a regular demand for application decision-making. Serverless computing facilitates fast function start-up and deployment, making it an attractive way for geo-distributed analytics. We argue that the serverless paradigm holds the potential to breach current performance bottlenecks via fine-grained function orchestration. However, how to configure it for geo-distributed analytics remains ambiguous. To fill this gap, we present Demeter, a scalable fine-grained function orchestrator for geo-distributed serverless analytics systems. Demeter aims to minimize the composite cost of co-existing jobs while meeting the user-specific Service Level Objectives (SLO). To handle the volatile environments and learn the diverse function demands, a Multi-Agent Reinforcement Learning (MARL) solution is used to co-optimize the per-function placement and resource allocation. The MARL extracts holistic and compact states via hierarchical graph neural networks, and then designs a novel actor network to shrink the huge decision space and model complexity. Finally, we implement Demeter and evaluate it using realistic workloads. The experimental results reveal that Demeter significantly saves costs by 23.3%∼32.7%, while reducing SLO violations by over 27.4%, surpassing state-of-the-art solutions.}
}


@inproceedings{DBLP:conf/infocom/WeiZLXPPLHL24,
	author = {Dehui Wei and
                  Jiao Zhang and
                  Haozhe Li and
                  Zhichen Xue and
                  Yajie Peng and
                  Xiaofei Pang and
                  Yuanjie Liu and
                  Rui Han and
                  Jialin Li},
	title = {Pscheduler: QoE-Enhanced MultiPath Scheduler for Video Services in
                  Large-scale Peer-to-Peer CDNs},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2508--2517},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621211},
	doi = {10.1109/INFOCOM52122.2024.10621211},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/WeiZLXPPLHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video content providers such as Douyin implement Peer-to-Peer Content Delivery Networks (PCDNs) to reduce the costs associated with Content Delivery Networks (CDNs) while still maintaining optimal user-perceived quality of experience (QoE). PCDNs rely on the remaining resources of edge devices, such as edge access devices and hosts, to store and distribute data with a Multiple-Server-to-One-Client (MS2OC) communication pattern. MS2OC parallel transmission pattern suffers from severe data out-of-order issues. However, direct applying existing schedulers designed for MPTCP to PCDN fails to meet the two goals of high aggregate bandwidth and low end-to-end delivery latency.To address this, we present the comprehensive detail of the Douyin self-developed PCDN video transmission system and propose the first QoE-enhanced packet-level scheduler for PCDN systems, called Pscheduler. Pscheduler estimates path quality using a congestion-control-decoupled algorithm and distributes data by the proposed path-pick-packet method to ensure smooth video playback. Additionally, a redundant transmission algorithm is proposed to improve the task download speed for segmented video transmission. Our large-scale online A/B tests, comprising 100,000 Douyin users that generate tens of millions of videos data, show that Pscheduler achieves an average improvement of 60% in goodput, 20% reduction in data delivery waiting time, and 30% reduction in rebuffering rate.}
}


@inproceedings{DBLP:conf/infocom/SalehGSBK24,
	author = {Saad Saleh and
                  Anouk S. Goossens and
                  Sunny Shu and
                  Tamalika Banerjee and
                  Boris Koldehofe},
	title = {Analog In-Network Computing through Memristor-based Match-Compute
                  Processing},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2518--2527},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621228},
	doi = {10.1109/INFOCOM52122.2024.10621228},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/SalehGSBK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Current network functions consume a significant amount of energy and lack the capacity to support more expressive learning models like neuromorphic functions. The major reason is the underlying transistor-based components that require continuous energy-intensive data movements between the storage and computational units. In this research, we propose the use of a novel component, called Memristor, which can colocalize computation and storage, and provide computational capabilities. Building on memristors, we propose the concept of match-compute processing for supporting energy efficient network functions. Considering the analog processing of memristors, we propose a Probabilistic Content Addressable Memory (pCAM) abstraction which can provide analog match functions. pCAM provides deterministic and probabilistic outputs depending upon the closeness of match of an incoming query with the specified network policy. pCAM uses a crossbar array for line rate matrix multiplications on the match outputs. We proposed a match-compute packet processing architecture and developed the programming abstractions for a baseline network function, i.e., Active Queue Management, which drops packets based upon the higher-order derivatives of sojourn times and buffer sizes. The analysis of match-compute processing over a physically fabricated memristor chip showed only 0.01 fJ/bit/cell of energy consumption, which is 50 times less than the traditional match-action processing.}
}


@inproceedings{DBLP:conf/infocom/Zhang0L0D024,
	author = {Xiaoquan Zhang and
                  Lin Cui and
                  Waiming Lau and
                  Fung Po Tso and
                  Yuhui Deng and
                  Weijia Jia},
	title = {Carlo: Cross-Plane Collaboration for Multiple In-network Computing
                  Applications},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2528--2537},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621111},
	doi = {10.1109/INFOCOM52122.2024.10621111},
	timestamp = {Wed, 21 Aug 2024 09:25:42 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/Zhang0L0D024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-network computing (INC) is a new paradigm that allows applications to be executed within the network, rather than on dedicated servers. Conventionally, INC applications have been exclusively deployed on the data plane (e.g., programmable ASICs), offering impressive performance capabilities. However, the data plane’s efficiency is hindered by limited resources, which can prevent a comprehensive deployment of applications. On the other hand, offloading compute tasks to the control plane, which is underpinned by general-purpose servers with ample resources, provides greater flexibility. However, this approach comes with the tradeoff of significantly reduced efficiency, especially when the system operates under heavy load. To simultaneously exploit the efficiency of data plane and the flexibility of control plane, we propose Carlo, a cross-plane collaborative optimization framework to support the network-wide deployment of multiple INC applications across both the control and data plane. Carlo first analyzes resource requirements of various INC applications across different planes. It then establishes mathematical models for resource allocation in cross-plane and automatically generates solutions using proposed algorithms. We have implemented the prototype of Carlo on Intel Tofino ASIC switches and DPDK. Experimental results demonstrate that Carlo can compute solutions in a short time while avoiding performance degradation caused by the deployment scheme.}
}


@inproceedings{DBLP:conf/infocom/ChenZLW0YQL24,
	author = {Ning Chen and
                  Sheng Zhang and
                  Yu Liang and
                  Jie Wu and
                  Yu Chen and
                  Yuting Yan and
                  Zhuzhong Qian and
                  Sanglu Lu},
	title = {TileSR: Accelerate On-Device Super-Resolution with Parallel Offloading
                  in Tile Granularity},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2538--2547},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621208},
	doi = {10.1109/INFOCOM52122.2024.10621208},
	timestamp = {Wed, 21 Aug 2024 07:35:25 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ChenZLW0YQL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the unprecedented performance of convolutional networks in image super-resolution (SR). SR involves upscaling a single low-resolution image to meet application-specific image quality demands, making it vital for mobile devices. However, the excessive computational and memory requirements of SR tasks pose a challenge in mapping SR networks on a single resource-constrained mobile device, especially for an ultra-high target resolution. This work presents TileSR, a novel framework for efficient image SR through tile-granular parallel offloading upon multiple collaborative mobile devices. In particular, for an incoming image, TileSR first uniformly divides it into multiple tiles and selects the top-K tiles with the highest upscaling difficulty (quantified by mPV). Then, we propose a tile scheduling algorithm based on multi-agent multiarmed bandit, which attains the accurate offload reward through the exploration phase, derives the tile packing decision based on the reward estimates, and exploits this decision to schedule the selected tiles. We have implemented TileSR fully based on COTS hardware, and the experimental results demonstrate that TileSR reduces the response latency by 17.77-82.2% while improving the image quality by 2.38-10.57% compared to other alternatives.}
}


@inproceedings{DBLP:conf/infocom/ZhaiZO0H024,
	author = {Zhiwei Zhai and
                  Liekang Zeng and
                  Tao Ouyang and
                  Shuai Yu and
                  Qianyi Huang and
                  Xu Chen},
	title = {{SECO:} Multi-Satellite Edge Computing Enabled Wide-Area and Real-Time
                  Earth Observation Missions},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} Conference on Computer Communications,
                  Vancouver, BC, Canada, May 20-23, 2024},
	pages = {2548--2557},
	publisher = {{IEEE}},
	year = {2024},
	url = {https://doi.org/10.1109/INFOCOM52122.2024.10621270},
	doi = {10.1109/INFOCOM52122.2024.10621270},
	timestamp = {Tue, 20 Aug 2024 13:54:36 +0200},
	biburl = {https://dblp.org/rec/conf/infocom/ZhaiZO0H024.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rapid advances in low Earth orbit (LEO) satellite technology and satellite edge computing (SEC) have facilitated a key role for LEO satellites in enhanced Earth observation missions (EOM). These missions (e.g., remote object detection) typically require multi-satellite cooperative observations of a large region of interest (RoI) area, as well as the observation image routing and computation processing, enabling accurate and real-time responsiveness. However, optimizing the resources of LEO satellite networks is nontrivial in the presence of its dynamic and heterogeneous properties. To this end, we propose SECO, a SEC-enabled framework that jointly optimizes multi-satellite observation scheduling, routing and computation node selection for enhanced EOM. Specifically, in the observation phase, we leverage the orbital motion and the rotatable onboard cameras of satellites, and propose a distributed game-based scheduling strategy to minimize the overall size of captured images while ensuring full (observation) coverage. In the sequent routing and computation phase, we first adopt image splitting technology to achieve parallel transmission and computation. Then, we propose an efficient iterative algorithm to jointly optimize image splitting, routing and computation node selection for each captured image. On this basis, we propose a theoretically guaranteed systemwide greedy-based strategy to reduce the total time cost (i.e., transmission, computation and queuing delay) over simultaneous processing for multiple images. Extensive experiments based on real-world datasets demonstrate that SECO can achieve up to a 60.7% reduction in overall time cost compared to baselines.}
}
