@inproceedings{DBLP:conf/iccS/BaileyMC21,
	author = {Jos{\'{e}} Monreal Bailey and
                  Hadi Tabatabaee Malazi and
                  Siobh{\'{a}}n Clarke},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Smoothing Speed Variability in Age-Friendly Urban Traffic Management},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {3--16},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_1},
	doi = {10.1007/978-3-030-77961-0\_1},
	timestamp = {Tue, 13 Jul 2021 13:27:57 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/BaileyMC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic congestion has a negative impact on vehicular mobility, especially for senior drivers. Current approaches to urban traffic management focus on adaptive routing for the reduction of fuel consumption and travel time. Most of these approaches do not consider age-friendliness, in particular that speed variability is difficult for senior drivers. Frequent stop and go situations around congested areas are tiresome for senior drivers and make them prone to accidents. Moreover, senior drivers’ mobility is affected by factors such as travel time, surrounding vehicles’ speed, and hectic traffic. Age-friendly traffic management needs to consider speed variability in addition to drivers’ waiting time (which impacts fuel consumption and travel time). This paper introduces a multi-agent pheromone-based vehicle routing algorithm that smooths speed variability while also considering senior drivers during traffic light control. Simulation results demonstrate 17.6% improvement in speed variability as well as reducing travel time and fuel consumption by 11.6% and 19.8% respectively compared to the state of the art.}
}


@inproceedings{DBLP:conf/iccS/DhouC21,
	author = {Khaldoon Dhou and
                  Christopher Cruzen},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {An Innovative Employment of the NetLogo {AIDS} Model in Developing
                  a New Chain Code for Compression},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {17--25},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_2},
	doi = {10.1007/978-3-030-77961-0\_2},
	timestamp = {Tue, 15 Jun 2021 17:23:53 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/DhouC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we utilize the NetLogo HIV model in constructing an environment for bi-level image encoding and employ it in compression. Our model considers converting an image into a virtual environment that comprises female agents testing positive and negative for HIV. Female agents are scattered according to the allocation of the pixels in the original images to be tested. The simulation considers introducing male agents that test positive for HIV, the purpose of which is to track their movements while infecting other HIV- female agents. The progressions of the HIV+ male agents within the simulation take advantage of the relative encoding approach previously used by other image processing and agent-based modeling researchers. That is to say, the simulation allows generating a high proportion of similar movement forms that are similarly encoded regardless of the movements of agents. This is followed up by applying Huffman coding to the obtained chains of movement strings for further reduction. The ultimate results reveal that our product could outperform existing benchmarks using all the images we employed in testing.}
}


@inproceedings{DBLP:conf/iccS/AntczakSSWZ21,
	author = {Tomasz Antczak and
                  Bartosz Skorupa and
                  Mikolaj Szurlej and
                  Rafal Weron and
                  Jacek Zabawa},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Simulation Modeling of Epidemic Risk in Supermarkets: Investigating
                  the Impact of Social Distancing and Checkout Zone Design},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {26--33},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_3},
	doi = {10.1007/978-3-030-77961-0\_3},
	timestamp = {Tue, 15 Jun 2021 17:23:47 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/AntczakSSWZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We build an agent-based model for evaluating the spatial and functional design of supermarket checkout zones and the effectiveness of safety regulations related to distancing that have been introduced after the COVID-19 outbreak. The model is implemented in the NetLogo simulation platform and calibrated to actual point of sale data from one of major European retail chains. It enables realistic modeling of the checkout operations as well as of the airborne diffusion of SARS-CoV-2 particles. We find that opening checkouts in a specific order can reduce epidemic risk, but only under low and moderate traffic conditions. Hence, redesigning supermarket layouts to increase distances between the queues can reduce risk only if the number of open checkouts is sufficient to serve customers during peak hours.}
}


@inproceedings{DBLP:conf/iccS/MaleckiKW21,
	author = {Krzysztof Malecki and
                  Marek Kaminski and
                  Jaroslaw Was},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {A Multi-cell Cellular Automata Model of Traffic Flow with Emergency
                  Vehicles: Effect of a Corridor of Life},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {34--40},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_4},
	doi = {10.1007/978-3-030-77961-0\_4},
	timestamp = {Tue, 15 Jun 2021 17:23:53 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/MaleckiKW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There are various macroscopic and microscopic road traffic models that allow traffic flow analysis. However, it should be emphasized that standard traffic flow models do not include emergency vehicle traffic. We propose a multi-agent microscopic model for analyzing traffic flow of emergency vehicles with some limitations to the distance between vehicles and their proper distribution (corridor of life) to leave free passage for a privileged vehicle. Real data was used to calibrate and validate the model. Our simulation studies show the importance of certain aspects of road traffic (distance between vehicles, corridor of life, size and type of roadside, friction conflict, etc.) in order to increase/decrease the traffic flow in the aspect of an approaching of emergency vehicle.}
}


@inproceedings{DBLP:conf/iccS/TangWLBLW21,
	author = {Zixian Tang and
                  Qiang Wang and
                  Wenhao Li and
                  Huaifeng Bao and
                  Feng Liu and
                  Wen Wang},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {{HSLF:} {HTTP} Header Sequence Based {LSH} Fingerprints for Application
                  Traffic Classification},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {41--54},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_5},
	doi = {10.1007/978-3-030-77961-0\_5},
	timestamp = {Fri, 23 Jun 2023 22:30:47 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/TangWLBLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distinguishing the prosperous network application is a challenging task in network management that has been extensively studied for many years. Unfortunately, previous work on HTTP traffic classification rely heavily on prior knowledge with coarse grained thus are limited in detecting the evolution of new emerging application and network behaviors. In this paper, we propose HSLF, a hierarchical system that employs application fingerprint to classify HTTP traffic. Specifically, we employ local-sensitive hashing algorithm to obtain the importance of each field in HTTP header, from which a rational weight allocation scheme and fingerprint of each HTTP session are generated. Then, similarities of fingerprints among each application are calculated to classify the unknown HTTP traffic. Performance on a real-world dataset of HSLF achieves an accuracy of 96.6%, which outperforms classic machine learning methods and state-of-the-art models.}
}


@inproceedings{DBLP:conf/iccS/KostrzewaKB21,
	author = {Daniel Kostrzewa and
                  Piotr Kaminski and
                  Robert Brzeski},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Music Genre Classification: Looking for the Perfect Network},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {55--67},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_6},
	doi = {10.1007/978-3-030-77961-0\_6},
	timestamp = {Tue, 15 Jun 2021 17:23:46 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/KostrzewaKB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents research on music genre recognition. It is a crucial task because there are millions of songs in the online databases. Classifying them by a human being is impossible or extremely expensive. As a result, it is desirable to create methods that can assign a given track to a music genre. Here, the classification of music tracks is carried out by deep learning models. The Free Music Archive dataset was used to perform experiments. The tests were executed with the usage of Convolutional Neural Network, Convolutional Recurrent Neural Networks with 1D and 2D convolutions, and Recurrent Neural Network with Long Short-Term Memory cells. In order to combine the advantages of different deep neural network architectures, a few types of ensembles were proposed with two types of results mixing methods. The best results obtained in this paper, which are equal to state-of-the-art methods, were achieved by one of the proposed ensembles. The solution described in the paper can help to make the auto-tagging of songs much faster and more accurate in the context of assigning them to particular musical genres.}
}


@inproceedings{DBLP:conf/iccS/CardenasOIK021,
	author = {Pedro C{\'{a}}rdenas and
                  Boguslaw Obara and
                  Ioannis P. Ivrissimtzis and
                  Ibad Kureshi and
                  Georgios Theodoropoulos},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Big Data for National Security in the Era of {COVID-19}},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {68--82},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_7},
	doi = {10.1007/978-3-030-77961-0\_7},
	timestamp = {Fri, 16 Jul 2021 14:32:43 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/CardenasOIK021.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The COVID-19 epidemic has changed the world dramatically as societies adjust their behaviour to meet the challenges and uncertainties of the new normal. These uncertainties have led to instabilities in several facets of society, most notably health, economy and public order. Increasing discontent within societies in response to government mandated measures to contain the pandemic have triggered social unrest, imposing serious threats to national security. Big Data Analytics can provide a powerful force multiplier to support policy and decision makers to contain the virus while at the same time dealing with such threats to national security. This paper presents the utilisation of a big data forecasting and analytics framework to deal with COVID-19 triggered social unrest. The framework is applied and demonstrated in two different disruptive incidents in the United States of America.}
}


@inproceedings{DBLP:conf/iccS/BalcerzakNN21,
	author = {Bartlomiej Balcerzak and
                  Radoslaw Nielek and
                  Jerzy Pawel Nowacki},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Efficient Prediction of Spatio-Temporal Events on the Example of the
                  Availability of Vehicles Rented per Minute},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {83--89},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_8},
	doi = {10.1007/978-3-030-77961-0\_8},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/BalcerzakNN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article shows a solution to the problem of predicting the availability of vehicles rented per minute in a city. A grid-based spatial model with use of LSTM network augmented with Time Distribution Layer was developed and tested against actual vehicle availability dataset. The dataset was also made publicly available for researchers as a part of this study. The predictive model developed in the study is used in a multi-modal trip planner.}
}


@inproceedings{DBLP:conf/iccS/WcisloC21,
	author = {Robert Wcislo and
                  Wojciech Czech},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Grouped Multi-Layer Echo State Networks with Self-Normalizing Activations},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {90--97},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_9},
	doi = {10.1007/978-3-030-77961-0\_9},
	timestamp = {Tue, 15 Jun 2021 17:23:44 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/WcisloC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study prediction performance of Echo State Networks with multiple reservoirs built based on stacking and grouping. Grouping allows for developing independent subreservoir dynamics, which improves linear separability on readout layer. At the same time, stacking enables to capture multiple time-scales of an input signal by the hierarchy of non-linear mappings. Combining those two effects, together with a proper selection of model hyperparameters can boost ESN capabilities for benchmark time-series such as Mackey Glass System. Different strategies for determining subreservoir structure are compared along with the influence of activation function. In particular, we show that recently proposed non-linear self-normalizing activation function together with grouped deep reservoirs provide superior prediction performance on artificial and real-world datasets. Moreover, comparing to standard tangent hyperbolic models, the new models built using self-normalizing activation function are more feasible in terms of hyperparameter selection.}
}


@inproceedings{DBLP:conf/iccS/NevesNP21,
	author = {Diogo Telmo Neves and
                  Marcel Ganesh Naik and
                  Alberto Proen{\c{c}}a},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {SGAIN, {WSGAIN-CP} and {WSGAIN-GP:} Novel {GAN} Methods for Missing
                  Data Imputation},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {98--113},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_10},
	doi = {10.1007/978-3-030-77961-0\_10},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/NevesNP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-world datasets often have missing values, which hinders the use of a large number of machine learning (ML) estimators. To overcome this limitation in a data analysis pipeline, data points may be deleted in a data preprocessing stage. However, an alternative better solution is data imputation. Several methods based on Artificial Neural Networks (ANN) have been recently proposed as successful alternatives to classical discriminative imputation methods. Amongst those ANN imputation methods are the ones that rely on Generative Adversarial Networks (GAN). This paper presents three data imputation methods based on GAN: SGAIN, WSGAIN-CP and WSGAIN-GP. These methods were tested on datasets with different settings of missing values probabilities, where the values are missing completely at random (MCAR). The evaluation of the newly developed methods shows that they are equivalent or outperform competitive state-of-the-art imputation methods in different ways, either in terms of response time, the data imputation quality, or the accuracy of post-imputation tasks (e.g., prediction or classification).}
}


@inproceedings{DBLP:conf/iccS/PaszynskiGPD21,
	author = {Maciej Paszynski and
                  Rafal Wojciech Grzeszczuk and
                  David Pardo and
                  Leszek F. Demkowicz},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Deep Learning Driven Self-adaptive Hp Finite Element Method},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {114--121},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_11},
	doi = {10.1007/978-3-030-77961-0\_11},
	timestamp = {Thu, 23 Jun 2022 19:55:22 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/PaszynskiGPD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The finite element method (FEM) is a popular tool for solving engineering problems governed by Partial Differential Equations (PDEs). The accuracy of the numerical solution depends on the quality of the computational mesh. We consider the self-adaptive hp-FEM, which generates optimal mesh refinements and delivers exponential convergence of the numerical error with respect to the mesh size. Thus, it enables solving difficult engineering problems with the highest possible numerical accuracy. We replace the computationally expensive kernel of the refinement algorithm with a deep neural network in this work. The network learns how to optimally refine the elements and modify the orders of the polynomials. In this way, the deterministic algorithm is replaced by a neural network that selects similar quality refinements in a fraction of the time needed by the original algorithm.}
}


@inproceedings{DBLP:conf/iccS/KnapinskaLW21,
	author = {Aleksandra Knapinska and
                  Piotr Lechowicz and
                  Krzysztof Walkowiak},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Machine-Learning Based Prediction of Multiple Types of Network Traffic},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {122--136},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_12},
	doi = {10.1007/978-3-030-77961-0\_12},
	timestamp = {Tue, 15 Jun 2021 17:23:47 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/KnapinskaLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Prior knowledge regarding approximated future traffic requirements allows adjusting suitable network parameters to improve the network’s performance. To this end, various analyses and traffic prediction methods assisted with machine learning techniques are developed. In this paper, we study on-line multiple time series prediction for traffic of various frame sizes. Firstly, we describe the gathered real network traffic data and study their seasonality and correlations between traffic types. Secondly, we propose three machine learning algorithms, namely, linear regression, k nearest neighbours, and random forest, to predict the network data which are compared under various models and input features. To evaluate the prediction quality, we use the root mean squared percentage error (RMSPE). We define three machine learning models, where traffic related to particular frame sizes is predicted based on the historical data of corresponding frame sizes solely, several frame sizes, and all frame sizes. According to the performed numerical experiments on four different datasets, linear regression yields the highest accuracy when compared to the other two algorithms. As the results indicate, the inclusion of historical data regarding all frame sizes to predict summary traffic of a certain frame size increases the algorithm’s accuracy at the cost of longer execution times. However, by appropriate input features selection based on seasonality, it is possible to decrease this time overhead at the almost unnoticeable accuracy decrease.}
}


@inproceedings{DBLP:conf/iccS/IdziakS0LBE21,
	author = {Jan Idziak and
                  Artjoms Sela and
                  Michal Wozniak and
                  Albert Lesniak and
                  Joanna Byszuk and
                  Maciej Eder},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Scalable Handwritten Text Recognition System for Lexicographic Sources
                  of Under-Resourced Languages and Alphabets},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {137--150},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_13},
	doi = {10.1007/978-3-030-77961-0\_13},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/IdziakS0LBE21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The paper discusses an approach to decipher large collections of handwritten index cards of historical dictionaries. Our study provides a working solution that reads the cards, and links their lemmas to a searchable list of dictionary entries, for a large historical dictionary entitled the Dictionary of the 17\\(^{th}\\)- and 18\\(^{th}\\)-century Polish, which comprizes 2.8 million index cards. We apply a tailored handwritten text recognition (HTR) solution that involves (1) an optimized detection model; (2) a recognition model to decipher the handwritten content, designed as a spatial transformer network (STN) followed by convolutional neural network (RCNN) with a connectionist temporal classification layer (CTC), trained using a synthetic set of 500,000 generated Polish words of different length; (3) a post-processing step using constrained Word Beam Search (WBC): the predictions were matched against a list of dictionary entries known in advance. Our model achieved the accuracy of 0.881 on the word level, which outperforms the base RCNN model. Within this study we produced a set of 20,000 manually annotated index cards that can be used for future benchmarks and transfer learning HTR applications.}
}


@inproceedings{DBLP:conf/iccS/BocewiczNSB21,
	author = {Grzegorz Bocewicz and
                  Izabela Nielsen and
                  Czeslaw Smutnicki and
                  Zbigniew Banaszak},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Out-Plant Milk-Run-Driven Mission Planning Subject to Dynamic Changes
                  of Date and Place Delivery},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {151--167},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_14},
	doi = {10.1007/978-3-030-77961-0\_14},
	timestamp = {Tue, 15 Jun 2021 17:23:54 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/BocewiczNSB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a dynamic vehicle routing problem in which a fleet of vehicles delivers ordered services or goods to spatially distributed customers while moving along separate milk-run routes over a given periodically repeating time horizon. Customer orders and the feasible time windows for the execution of those orders can be dynamically revealed over time. The problem essentially entails the rerouting of routes determined in the course of their proactive planning. Rerouting takes into account current order changes, while proactive route planning takes into account anticipated (previously assumed) changes in customer orders. Changes to planned orders may apply to both changes in the date of services provided and emerging notifications of additional customers. The considered problem is formulated as a constraint satisfaction problem using the ordered fuzzy number (OFN) formalism, which allows us to handle the fuzzy nature of the variables involved, e.g., the timeliness of the deliveries performed, through an algebraic approach. The computational results show that the proposed solution outperforms the commonly used computer simulation methods.}
}


@inproceedings{DBLP:conf/iccS/CardelliniMVBO21,
	author = {Matteo Cardellini and
                  Marco Maratea and
                  Mauro Vallati and
                  Gianluca Boleto and
                  Luca Oneto},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {An Efficient Hybrid Planning Framework for In-Station Train Dispatching},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {168--182},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_15},
	doi = {10.1007/978-3-030-77961-0\_15},
	timestamp = {Tue, 15 Jun 2021 17:23:54 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/CardelliniMVBO21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-station train dispatching is the problem of optimising the effective utilisation of available railway infrastructures for mitigating incidents and delays. This is a fundamental problem for the whole railway network efficiency, and in turn for the transportation of goods and passengers, given that stations are among the most critical points in networks since a high number of interconnections of trains’ routes holds therein\n. Despite such importance, nowadays in-station train dispatching is mainly managed manually by human operators. In this paper we present a framework for solving in-station train dispatching problems, to support human operators in dealing with such task. We employ automated planning languages and tools for solving the task: PDDL+ for the specification of the problem, and the ENHSP planning engine, enhanced by domain-specific techniques, for solving the problem. We carry out a in-depth analysis using real data of a station of the North West of Italy, that shows the effectiveness of our approach and the contribution that domain-specific techniques may have in efficiently solving the various instances of the problem. Finally, we also present a visualisation tool for graphically inspecting the generated plans.}
}


@inproceedings{DBLP:conf/iccS/ColemanCGS21,
	author = {Tain{\~{a}} Coleman and
                  Henri Casanova and
                  Ty Gwartney and
                  Rafael Ferreira da Silva},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Evaluating Energy-Aware Scheduling Algorithms for I/O-Intensive Scientific
                  Workflows},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {183--197},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_16},
	doi = {10.1007/978-3-030-77961-0\_16},
	timestamp = {Mon, 03 Jan 2022 22:23:25 +0100},
	biburl = {https://dblp.org/rec/conf/iccS/ColemanCGS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Improving energy efficiency has become necessary to enable sustainable computational science. At the same time, scientific workflows are key in facilitating distributed computing in virtually all domain sciences. As data and computational requirements increase, I/O-intensive workflows have become prevalent. In this work, we evaluate the ability of twopopular energy-aware workflow scheduling algorithms to provide effective schedules for this class of workflow applications, that is, schedules that strike a good compromise between workflow execution time and energy consumption. These two algorithms make decisions based on a widely used power consumption model that simply assumes linear correlation to CPU usage. Previous work has shown this model to be inaccurate, in particular for modeling power consumption of I/O-intensive workflow executions, and has proposed an accurate model. We evaluate the effectiveness of the two aforementioned algorithms based on this accurate model. We find that, when making their decisions, these algorithms can underestimate power consumption by up\xa0to 360%, which makes it unclear how well these algorithm would fare in practice. To evaluate the benefit of using the more accurate power consumption model in practice, we propose a simple scheduling algorithm that relies on this model to balance the I/O load across the available compute resources. Experimental results show that this algorithm achieves more desirable compromises between energy consumption and workflow execution time than the two popular algorithms.}
}


@inproceedings{DBLP:conf/iccS/BozejkoRUW21,
	author = {Wojciech Bozejko and
                  Pawel Rajba and
                  Mariusz Uchronski and
                  Mieczyslaw Wodecki},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {A Job Shop Scheduling Problem with Due Dates Under Conditions of Uncertainty},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {198--205},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_17},
	doi = {10.1007/978-3-030-77961-0\_17},
	timestamp = {Tue, 15 Jun 2021 17:23:45 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/BozejkoRUW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {\nIn the work we consider a job shop problem with due dates under conditions of uncertainty. Uncertainty is considered for operation execution times and job completion dates. It is modeled by normal and Erlang random variables. We present algorithms whose constructions are based on the tabu search method. Due to the application of the probabilistic model, it was possible to obtain solutions more resistant to data disturbances than in the classical approach.}
}


@inproceedings{DBLP:conf/iccS/ZurekPPK21,
	author = {Dominik Zurek and
                  Kamil Pietak and
                  Marcin Pietron and
                  Marek Kisiel{-}Dorohinicki},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {New Variants of {SDLS} Algorithm for {LABS} Problem Dedicated to {GPGPU}
                  Architectures},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {206--212},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_18},
	doi = {10.1007/978-3-030-77961-0\_18},
	timestamp = {Thu, 23 Jun 2022 19:55:21 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/ZurekPPK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low autocorrelation binary sequence (LABS) remains an open hard optimisation problem that has many applications. One of the promising directions for solving the problem is designing advanced solvers based on local search heuristics. The paper proposes two new heuristics developed from the steepest-descent local search algorithm (SDLS), implemented on the GPGPU architectures. The introduced algorithms utilise the parallel nature of the GPU and provide an effective method of solving the LABS problem. As a means for comparison, the efficiency between SDSL and the new algorithms is presented, showing that exploring the wider neighbourhood improves the results.}
}


@inproceedings{DBLP:conf/iccS/PuchalaS21,
	author = {Dariusz Puchala and
                  Kamil Stokfiszewski},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Highly Effective {GPU} Realization of Discrete Wavelet Transform for
                  Big-Data Problems},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {213--227},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_19},
	doi = {10.1007/978-3-030-77961-0\_19},
	timestamp = {Tue, 15 Jun 2021 17:23:46 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/PuchalaS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Discrete wavelet transform (DWT) is widely used in the tasks of signal processing, analysis and recognition. Moreover it’s practical applications are not limited to the case of one-dimensional signals but also apply to images and multidimensional data. From the moment of introduction of the dedicated libraries that enable to use graphics processing units (GPUs) for mass-parallel general purpose calculations the development of effective GPU based implementations of one-dimensional DWT is an important field of scientific research. It is also important because with use of one-dimensional procedure we can calculate DWT in multidimensional case if only the transform’s separability is assumed. In this paper the authors propose a novel approach to calculation of one-dimensional DWT based on lattice structure which takes advantage of shared memory and registers in order to implement necessary inter-thread communication. The experimental analysis reveals high time-effectiveness of the proposed approach which can be even 5 times higher for Maxwell architecture, and up\xa0to 2 times for Turing family GPU cards, than the one characteristic for the convolution based approach in computational tasks that can be classified as big-data problems.}
}


@inproceedings{DBLP:conf/iccS/BleileBOC21,
	author = {Ryan Bleile and
                  Patrick S. Brantley and
                  Matthew O'Brien and
                  Hank Childs},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {A Dynamic Replication Approach for Monte Carlo Photon Transport on
                  Heterogeneous Architectures},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {228--242},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_20},
	doi = {10.1007/978-3-030-77961-0\_20},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/BleileBOC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers Monte Carlo photon transport applications on heterogenous compute architectures with both CPUs and GPUs. Previous work on this problem has considered only meshes that can fully fit within the memory of a GPU, which is a significant limitation: many important problems require meshes that exceed memory size. We address this gap by introducing a new dynamic replication algorithm that adapts assignments based on the computational ability of a resource. We then demonstrate our algorithm’s efficacy on a variety of workloads, and find that incorporating the CPUs provides speedups of up\xa0to 20% over the GPUs alone. Further, these speedups are well beyond the FLOPS contribution from the CPUs, which provide further justification for continuing to include CPUs even when powerful GPUs are available. In all, the contribution of this work is an algorithm that can be applied in real-world settings to make more efficient use of heterogeneous architectures.}
}


@inproceedings{DBLP:conf/iccS/BalisODSK21,
	author = {Bartosz Balis and
                  Michal Orzechowski and
                  Lukasz Dutka and
                  Renata G. Slota and
                  Jacek Kitowski},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Scientific Workflow Management on Hybrid Clouds with Cloud Bursting
                  and Transparent Data Access},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {243--255},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_21},
	doi = {10.1007/978-3-030-77961-0\_21},
	timestamp = {Sat, 30 Sep 2023 09:43:59 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/BalisODSK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud bursting is an application deployment model wherein additional computing resources are provisioned from public clouds in cases where local resources are not sufficient, e.g. during peak demand periods. We propose and experimentally evaluate a\xa0cloud-bursting solution for scientific workflows. Our solution is portable thanks to using Kubernetes for deployment of the workflow management system and computing clusters in multiple clouds. We also introduce transparent data access by employing a\xa0virtual distributed file system across the clouds, allowing jobs to use a\xa0POSIX file system interface, while hiding data transfer between clouds. To balance load distribution and minimize the communication volume between clouds, we leverage graph partitioning, while ensuring that the algorithm distributes the load equally at each parallel execution stage of a\xa0workflow. The solution is experimentally evaluated using the HyperFlow workflow management system integrated with the Onedata data management platform, deployed in our on-premise cloud in Cyfronet AGH and in the Google Cloud.}
}


@inproceedings{DBLP:conf/iccS/NajdekXT21,
	author = {Mateusz Najdek and
                  Hairuo Xie and
                  Wojciech Turek},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Scaling Simulation of Continuous Urban Traffic Model for High Performance
                  Computing System},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {256--263},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_22},
	doi = {10.1007/978-3-030-77961-0\_22},
	timestamp = {Sun, 02 Oct 2022 16:03:53 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/NajdekXT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Urban traffic simulation of extensive areas with complex driver models poses a significant computational challenge. Developing highly scalable parallel simulation algorithms is the only feasible way to provide useful results in this case. In this paper, we present extensions of the SMARTS system, a traffic simulation tool, which provides efficient scalability with a large number of parallel processes. The presented extensions enabled its scalability for HPC-grade systems. The extended version has been thoroughly tested in strong and weak scalability scenarios for up\xa0to 2400 computing cores of a supercomputer. The satisfactory scalability has been achieved by introducing several significant improvements, which have been discussed in details.}
}


@inproceedings{DBLP:conf/iccS/ErtlSD0S21,
	author = {Benjamin Ertl and
                  Matthias Schneider and
                  Christopher Diekmann and
                  J{\"{o}}rg Meyer and
                  Achim Streit},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {A Semi-supervised Approach for Trajectory Segmentation to Identify
                  Different Moisture Processes in the Atmosphere},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {264--277},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_23},
	doi = {10.1007/978-3-030-77961-0\_23},
	timestamp = {Tue, 13 Jul 2021 13:27:57 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/ErtlSD0S21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Different moisture processes in the atmosphere leave distinctive isotopologue fingerprints. Therefore, the paired analysis of water vapour and the ratio between different isotopologues, for example \\(\\{H_2O,\\delta D\\}\\) with \\(\\delta D\\) as the standardized \\(HDO/H_2O\\) isotopologue ratio, can be used to investigate these processes. In this paper, we propose a novel semi-supervised approach for trajectory segmentation to extract information that enables us to identify atmospheric moisture processes. While our approach can be transferred to a variety of domains as well, we focus our evaluation on Lagrangian air parcel trajectories and modelled \\(\\{H_2O,\\delta D\\}\\) fields. Our final aim is to understand the free tropospheric \\(\\{H_2O,\\delta D\\}\\) pair distribution that is observable by satellite sensors of the latest generation. Our method adopts a recently developed density-based clustering algorithm with constrained expansion, CoExDBSCAN, which identifies clusters of temporal neighbourhoods that are only expanded with regards to a priori constraints in defined subspaces. By formulating a constraint for the correlation of \\(\\{H_2O,\\delta D\\}\\), we can segment trajectories into multiple phases and extract the regression coefficients for each phase. Grouping segments with similar coefficients and comparing them to theoretical values allows us to find interpretable structures that correspond to atmospheric moisture processes. The experimental evaluation demonstrates that our method facilitates an efficient, data-driven analysis of large-scale climate data and multivariate time series in general.\n}
}


@inproceedings{DBLP:conf/iccS/AkhterSASS21,
	author = {Suravi Akhter and
                  Sadia Sharmin and
                  Sumon Ahmed and
                  Abu Ashfaqur Sajib and
                  Mohammad Shoyaib},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {mRelief: {A} Reward Penalty Based Feature Subset Selection Considering
                  Data Overlapping Problem},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {278--292},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_24},
	doi = {10.1007/978-3-030-77961-0\_24},
	timestamp = {Sat, 30 Sep 2023 09:43:58 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/AkhterSASS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature selection plays a vital role in machine learning and data mining by eliminating noisy and irrelevant attributes without compromising the classification performance. To select the best subset of features, we need to consider several issues such as the relationship among the features (interaction) and their relationship with the classes. Even though the state-of-the-art, Relief based feature selection methods can handle feature interactions, they often fail to capture the relationship of features with different classes. That is, a feature that can provide a clear boundary between two classes with a small average distance may be mistakenly ranked low compared to a feature that has a higher average distance with no clear boundary (data overlapping). Moreover, most of the existing methods provide a ranking of the given features rather than selecting a proper subset of the features. To address these issues, we propose a feature subset selection method namely modified Relief (mRelief) that can handle both feature interactions and data overlapping problems. Experimental results over twenty-seven benchmark datasets taken from different application areas demonstrate the superiority of mRelief over the state-of-the-art methods in terms of accuracies, number of the selected features, and the ability to identify the features (gene) to characterize a class (disease).}
}


@inproceedings{DBLP:conf/iccS/GolaszewskiKSL21,
	author = {Grzegorz Golaszewski and
                  Piotr Kulczycki and
                  Tomasz Szumlak and
                  Szymon Lukasik},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Reconstruction of Long-Lived Particles in LHCb {CERN} Project by Data
                  Analysis and Computational Intelligence Methods},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {293--300},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_25},
	doi = {10.1007/978-3-030-77961-0\_25},
	timestamp = {Sun, 02 Oct 2022 16:03:52 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/GolaszewskiKSL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LHCb at CERN, Geneva is a world-leading high energy physics experiment dedicated to searching for New Physics phenomena. The experiment is undergoing a major upgrade and will rely entirely on a flexible software trigger to process the data in real-time. In this paper a novel approach to reconstructing (detecting) long-lived particles using a new pattern matching procedure is presented. A large simulated data sample is applied to build an initial track pattern by an unsupervised approach. The pattern is then updated and verified by real collision data. As a performance index, the difference between density estimated by nonparametric methods using experimental streaming data and the one based on theoretical premises is used. Fuzzy clustering methods are applied for a pattern size reduction. A final decision is made in a real-time regime with rigorous time boundaries.}
}


@inproceedings{DBLP:conf/iccS/Kocon21,
	author = {Maja Kocon},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Motion Trajectory Grouping for Human Head Gestures Related to Facial
                  Expressions},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {301--315},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_26},
	doi = {10.1007/978-3-030-77961-0\_26},
	timestamp = {Tue, 15 Jun 2021 17:23:46 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/Kocon21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The paper focuses on human head motion in connection with facial expressions for virtual-based interaction systems. Nowadays, the virtual representation of a human, with human-like social behaviour and mechanism of movements, can realize the user-machine interaction. The presented method includes the head motion because head gestures transmit additional information about the interaction’s situational context. This paper presents head motion analysis based on the rotation of rigid objects technique for virtual-based interaction systems. First, we captured the head gestures of a human subject, expressing three basic facial expressions. The proposed motion model was described using three non-deformable objects, which reflect the neck and head skeleton movement’s character. Based on the captured actions, the motion trajectories were analyzed, and their characteristic features were distinguished. The obtained dependencies were used to created new trajectories using piecewise cubic Hermite interpolating polynomial (PCHIP). Furthermore, the trajectories assigned to the rigid model have been grouped according to their similarities for a given emotional state. This way, using a single master trajectory and a set of coefficients, we were able to generate the whole set of trajectories for joint rotations of the head for the target emotional state. The resulting rotation trajectories were used to create movements on the three-dimensional human head.}
}


@inproceedings{DBLP:conf/iccS/RadulescuBTAMR21,
	author = {Iulia{-}Maria Radulescu and
                  Alexandru Boicea and
                  Ciprian{-}Octavian Truica and
                  Elena Simona Apostol and
                  Mariana Mocanu and
                  Florin Radulescu},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {DenLAC: Density Levels Aggregation Clustering - {A} Flexible Clustering
                  Method -},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {316--329},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_27},
	doi = {10.1007/978-3-030-77961-0\_27},
	timestamp = {Tue, 15 Jun 2021 17:23:44 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/RadulescuBTAMR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces DenLAC (Density Levels Aggregation Clustering), an adaptable clustering algorithm which achieves high accuracy independent of the input’s shape and distribution. While most clustering algorithms are specialized on particular input types, DenLAC obtains correct results for spherical, elongated and different density clusters. We also incorporate a simple procedure for outlier identification and displacement. Our method relies on defining clusters as density intervals comprised of connected components which we call density bins, through assembling several popular notions in data mining and statistics such as Kernel Density Estimation, the density attraction and density levels theoretical concepts. To build the final clusters, we extract the connected components from each density bin and we merge adjacent connected components using a slightly modified agglomerative clustering algorithm.}
}


@inproceedings{DBLP:conf/iccS/GdawiecKL21,
	author = {Krzysztof Gdawiec and
                  Wieslaw Kotarski and
                  Agnieszka Lisowska},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Acceleration of the Robust Newton Method by the Use of the S-iteration},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {330--337},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_28},
	doi = {10.1007/978-3-030-77961-0\_28},
	timestamp = {Tue, 15 Jun 2021 17:23:47 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/GdawiecKL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {\nIn this paper, we propose an improvement of the Robust Newton’s Method (RNM). The RNM is a generalisation of the known Newton’s root finding method restricted to polynomials. Unfortunately, the RNM is slow. Thus, in this paper, we propose the acceleration of this method by replacing the standard Picard iteration in the RNM by the S-iteration. This leads to an essential acceleration of the modified method. We present the advantages of the proposed algorithm over the RNM using polynomiagraphs and some numerical measures. Moreover, we present its possible application to the generation of artistic patterns.}
}


@inproceedings{DBLP:conf/iccS/KizielewiczSS21,
	author = {Bartlomiej Kizielewicz and
                  Andrii Shekhovtsov and
                  Wojciech Salabun},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {A New Approach to Eliminate Rank Reversal in the {MCDA} Problems},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {338--351},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_29},
	doi = {10.1007/978-3-030-77961-0\_29},
	timestamp = {Tue, 15 Jun 2021 17:23:47 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/KizielewiczSS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the multi-criteria decision analysis (MCDA) domain, one of the most important challenges of today is Rank Reversal. In short, it is a paradox that the order of alternatives belonging to a certain set is changed when a new alternative is added to that set or one of the current ones is removed. It may undermine the credibility of ratings and rankings, which are returned by methods exposed to the Rank Reversal phenomenon. In this paper, we propose to use the Characteristic Objects method (COMET), which is resistant to the Rank Reversal phenomenon and combining it with the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) and Preference Ranking Organization Method for Enrichment Evaluations II (PROMETHEE II) methods. The COMET method requires a very large number of pair comparisons, which depends exponentially on the number of criteria used. Therefore, the task of pair comparisons will be performed using the PROMETHEE II and TOPSIS methods. In order to compare the quality of both proposed approaches, simple comparative experiments will be presented. Both proposed methods have high accuracy and are resistant to the Rank Reveral phenomenon.}
}


@inproceedings{DBLP:conf/iccS/EmuCMC21,
	author = {Mahzabeen Emu and
                  Dhivya Chandrasekaran and
                  Vijay Mago and
                  Salimur Choudhury},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Validating Optimal {COVID-19} Vaccine Distribution Models},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {352--366},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_30},
	doi = {10.1007/978-3-030-77961-0\_30},
	timestamp = {Tue, 15 Jun 2021 17:23:45 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/EmuCMC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the approval of vaccines for the coronavirus disease by many countries worldwide, most developed nations have begun, and developing nations are gearing up for the vaccination process. This has created an urgent need to provide a solution to optimally distribute the available vaccines once they are received by the authorities. In this paper, we propose a clustering-based solution to select optimal distribution centers and a Constraint Satisfaction Problem framework to optimally distribute the vaccines taking into consideration two factors namely priority and distance. We demonstrate the efficiency of the proposed models using real-world data obtained from the district of Chennai, India. The model provides the decision making authorities with optimal distribution centers across the district and the optimal allocation of individuals across these distribution centers with the flexibility to accommodate a wide range of demographics.}
}


@inproceedings{DBLP:conf/iccS/CascittiNMS21,
	author = {Julian Cascitti and
                  Stefan Niebler and
                  Andr{\'{e}} M{\"{u}}ller and
                  Bertil Schmidt},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {RNACache: Fast Mapping of RNA-Seq Reads to Transcriptomes Using MinHashing},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {367--381},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_31},
	doi = {10.1007/978-3-030-77961-0\_31},
	timestamp = {Mon, 26 Jun 2023 20:40:15 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/CascittiNMS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The alignment of reads to a transcriptome is an important initial step in a variety of bioinformatics RNA-seq pipelines. As traditional alignment-based tools suffer from high runtimes, alternative, alignment-free methods have recently gained increasing importance. We present a novel approach to the detection of local similarities between transcriptomes and RNA-seq reads based on context-aware minhashing. We introduce RNACache, a three-step processing pipeline consisting of minhashing of k-mers, match-based (online) filtering, and coverage-based filtering in order to identify truly expressed transcript isoforms. Our performance evaluation shows that RNACache produces transcriptomic mappings of high accuracy that include significantly fewer erroneous matches compared to the state-of-the-art tools RapMap, Salmon, and Kallisto. Furthermore, it offers scalable and highly competitive runtime performance at low memory consumption on common multi-core workstations. RNACache is publicly available at: https://github.com/jcasc/rnacache.}
}


@inproceedings{DBLP:conf/iccS/ProkhorovLB21,
	author = {Dmitriy Prokhorov and
                  Vadim V. Lisitsa and
                  Yaroslav Bazaikin},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Digital Image Reduction for Analysis of Topological Changes in Pore
                  Space During Chemical Dissolution},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {382--393},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_32},
	doi = {10.1007/978-3-030-77961-0\_32},
	timestamp = {Tue, 15 Jun 2021 17:23:54 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/ProkhorovLB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The paper presents an original algorithm for reducing three-dimensional digital images to improve persistence diagrams computing performance. These diagrams represent topology changes in digital rocks pore space. The algorithm has linear complexity because removing the voxel is based on the structure of its neighborhood. We illustrate that the algorithm’s efficiency depends heavily on the pore space’s complexity and the size of the filtration steps.}
}


@inproceedings{DBLP:conf/iccS/DeevaBAVBNK21,
	author = {Irina Deeva and
                  Anna Bubnova and
                  Petr D. Andriushchenko and
                  Anton Voskresenskiy and
                  Nikita V. Bukhanov and
                  Nikolay O. Nikitin and
                  Anna V. Kalyuzhnaya},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Oil and Gas Reservoirs Parameters Analysis Using Mixed Learning of
                  Bayesian Networks},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {394--407},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_33},
	doi = {10.1007/978-3-030-77961-0\_33},
	timestamp = {Sun, 02 Oct 2022 16:03:52 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/DeevaBAVBNK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a multipurpose Bayesian-based method for data analysis, causal inference and prediction in the sphere of oil and gas reservoir development is considered. This allows analysing parameters of a reservoir, discovery dependencies among parameters (including cause and effects relations), checking for anomalies, prediction of expected values of missing parameters, looking for the closest analogues, and much more. The method is based on extended algorithm MixLearn@BN for structural learning of Bayesian networks. Key ideas of MixLearn@BN are following: (1) learning the network structure on homogeneous data subsets, (2) assigning a part of the structure by an expert, and (3) learning the distribution parameters on mixed data (discrete and continuous). Homogeneous data subsets are identified as various groups of reservoirs with similar features (analogues), where similarity measure may be based on several types of distances. The aim of the described technique of Bayesian network learning is to improve the quality of predictions and causal inference on such networks. Experimental studies prove that the suggested method gives a significant advantage in missing values prediction and anomalies detection accuracy. Moreover, the method was applied to the database of more than a thousand petroleum reservoirs across the globe and allowed to discover novel insights in geological parameters relationships.}
}


@inproceedings{DBLP:conf/iccS/SinghM21,
	author = {Abhishek Kumar Singh and
                  Mani Mehra},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Analytic and Numerical Solutions of Space-Time Fractional Diffusion
                  Wave Equations with Different Fractional Order},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {408--421},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_34},
	doi = {10.1007/978-3-030-77961-0\_34},
	timestamp = {Tue, 15 Jun 2021 17:23:53 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/SinghM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The aim of this manuscript is to investigate analytic and numerical solutions of space–time fractional diffusion wave equations with different fractional order (\\(\\alpha \\) and \\(\\beta \\)). After deriving analytic solution, an implicit unconditional stable finite difference method for solving space-time fractional diffusion wave equations is proposed. The Gerschgorin theorem is used to study the stability and convergence of the method. Furthermore, the behavior of the error is examined to verify the order of convergence by numerical example.\n}
}


@inproceedings{DBLP:conf/iccS/Lytaev21,
	author = {Mikhail S. Lytaev},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Chebyshev-Type Rational Approximations of the One-Way Helmholtz Equation
                  for Solving a Class of Wave Propagation Problems},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {422--435},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_35},
	doi = {10.1007/978-3-030-77961-0\_35},
	timestamp = {Tue, 15 Jun 2021 17:23:53 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/Lytaev21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study is devoted to improving the efficiency of the numerical methods for solving the pseudo-differential parabolic equation of diffraction theory. A rational approximation on an interval is used instead of the Padé approximation in a vicinity of a point. The relationship between the pseudo-differential propagation operator, variations of the refractive index, and the maximum propagation angle is established. It is shown that using the approximation on an interval is more natural for this problem and allows using a more sparse computational grid than when using the local Padé approximation. The proposed method differs from the existing ones only in the coefficients of the numerical scheme and does not require significant changes in the implementations of the existing numerical schemas. The application of the proposed approach to the tropospheric radio-wave propagation and underwater acoustics is provided. Numerical examples quantitatively demonstrate the advantages of the proposed approach.}
}


@inproceedings{DBLP:conf/iccS/SaneJC21,
	author = {Sudhanshu Sane and
                  Chris R. Johnson and
                  Hank Childs},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Investigating In Situ Reduction via Lagrangian Representations for
                  Cosmology and Seismology Applications},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {436--450},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_36},
	doi = {10.1007/978-3-030-77961-0\_36},
	timestamp = {Tue, 15 Jun 2021 13:49:14 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/SaneJC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although many types of computational simulations produce time-varying vector fields, subsequent analysis is often limited to single time slices due to excessive costs. Fortunately, a new approach using a Lagrangian representation can enable time-varying vector field analysis while mitigating these costs. With this approach, a Lagrangian representation is calculated while the simulation code is running, and the result is explored after the simulation. Importantly, the effectiveness of this approach varies based on the nature of the vector field, requiring in-depth investigation for each application area. With this study, we evaluate the effectiveness for previously unexplored cosmology and seismology applications. We do this by considering encumbrance (on the simulation) and accuracy (of the reconstructed result). To inform encumbrance, we integrated in situ infrastructure with two simulation codes, and evaluated on representative HPC environments, performing Lagrangian in situ reduction using GPUs as well as CPUs. To inform accuracy, our study conducted a statistical analysis across a range of spatiotemporal configurations as well as a qualitative evaluation. In all, we demonstrate effectiveness for both cosmology and seismology—time-varying vector fields from these domains can be reduced to less than 1% of the total data via Lagrangian representations, while maintaining accurate reconstruction and requiring under 10% of total execution time in over 80% of our experiments.}
}


@inproceedings{DBLP:conf/iccS/0006C21,
	author = {Hong Zhang and
                  Emil M. Constantinescu},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Revolve-Based Adjoint Checkpointing for Multistage Time Integration},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {451--464},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_37},
	doi = {10.1007/978-3-030-77961-0\_37},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/0006C21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider adjoint checkpointing strategies that minimize the number of recomputations needed when using multistage timestepping. We demonstrate that we can improve on the seminal work based on the Revolve algorithm. The new approach provides better performance for a small number of time steps or checkpointing storage. Numerical results illustrate that the proposed algorithm can deliver up\xa0to two times speedup compared with that of Revolve and avoid recomputation completely when there is sufficient memory for checkpointing. Moreover, we discuss a tailored implementation that is arguably better suited for mature scientific computing libraries by avoiding central control assumed in the original checkpointing strategy. The proposed algorithm has been included in the PETSc library.}
}


@inproceedings{DBLP:conf/iccS/SzerszenZBK21,
	author = {Krzysztof Szerszen and
                  Eugeniusz Zieniuk and
                  Agnieszka Boltuc and
                  Andrzej Kuzelewski},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Comprehensive Regularization of {PIES} for Problems Modeled by 2D
                  Laplace's Equation},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {465--479},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_38},
	doi = {10.1007/978-3-030-77961-0\_38},
	timestamp = {Tue, 15 Jun 2021 17:23:43 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/SzerszenZBK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The paper proposes the concept of eliminating the explicit computation of singular integrals appearing in the parametric integral equation system (PIES) used to simulate the steady-state temperature field distribution. These singularities can be eliminated by regularizing the PIES formula with the auxiliary regularization function. Contrary to existing regularization methods that only eliminate strong singularities, the proposed approach is definitely more comprehensive due to the fact that it eliminates all strong and weak singularities. As a result, all singularities associated with PIES's integral functions can be removed. A practical aspect of the proposed regularization is the fact that all integrals appearing in the resulting formula can be evaluated numerically with a standard Gauss-Legendre quadrature rule. Simulation results indicate the high accuracy of the proposed algorithm.}
}


@inproceedings{DBLP:conf/iccS/LochabK21,
	author = {Ruchika Lochab and
                  Vivek Kumar},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {High Resolution {TVD} Scheme Based on Fuzzy Modifiers for Shallow-Water
                  Equations},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {480--492},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_39},
	doi = {10.1007/978-3-030-77961-0\_39},
	timestamp = {Tue, 15 Jun 2021 17:23:44 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/LochabK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work proposes a new fuzzy logic based high resolution (HR), total variation diminishing (TVD) scheme in finite volume frameworks to compute an approximate solution of the shallow water equations (SWEs). Fuzzy logic enhances the execution of classical numerical algorithms. To test the effectiveness and accuracy of the proposed scheme, the dam-break problem is considered. A comparison of the numerical results by implementing some classical flux limiting methods is provided. The proposed scheme is able to capture both smooth and discontinuous profiles, leading to better oscillation-free results.}
}


@inproceedings{DBLP:conf/iccS/BoltucZ21,
	author = {Agnieszka Boltuc and
                  Eugeniusz Zieniuk},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {{PIES} for Viscoelastic Analysis},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {493--499},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_40},
	doi = {10.1007/978-3-030-77961-0\_40},
	timestamp = {Tue, 15 Jun 2021 17:23:53 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/BoltucZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The paper presents the approach for solving 2D viscoelastic problems using the parametric integral equation system (PIES). On the basis of Kelvin model the PIES formula in time differential form is obtained. As solving procedure the time marching is adopted, by introducing a linear approximation of displacements. The proposed approach, unlike other numerical methods, does not require discretization even the boundary. It uses curves as a tool for global modeling of boundary segments: curves of the first degree for linear segments and of the third degree for curvilinear segments. The accuracy is steered by the approximation series with Lagrange basis functions. Some test are made and shown in order to validate the proposed approach.}
}


@inproceedings{DBLP:conf/iccS/Sinkovits21,
	author = {Robert S. Sinkovits},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Fast and Accurate Determination of Graph Node Connectivity Leveraging
                  Approximate Methods},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {500--513},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_41},
	doi = {10.1007/978-3-030-77961-0\_41},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/Sinkovits21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For an undirected graph G, the node connectivity K is defined as the minimum number of nodes that must be removed to make the graph disconnected. The determination of K is a computationally demanding task for large graphs since even the most efficient algorithms require many evaluations of an expensive max flow function. Approximation methods for determining K replace the max flow function with a much faster algorithm that gives a lower bound on the number of node independent paths, but this frequently leads to an underestimate of K. We show here that with minor changes, the approximate method can be adapted to retain most of the performance benefits while still guaranteeing an accurate result.}
}


@inproceedings{DBLP:conf/iccS/Skubalska-Rafajlowicz21,
	author = {Ewa Skubalska{-}Rafajlowicz and
                  Wojciech Rafajlowicz},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {An Exact Algorithm for Finite Metric Space Embedding into a Euclidean
                  Space When the Dimension of the Space Is Not Known},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {514--524},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_42},
	doi = {10.1007/978-3-030-77961-0\_42},
	timestamp = {Tue, 15 Jun 2021 17:23:46 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/Skubalska-Rafajlowicz21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a \\(O(n^3)\\) algorithm for solving the Distance Geometry Problem for a complete graph (a simple undirected graph in which every pair of distinct vertices is connected by a unique edge) consisting of \\(n+1\\) vertices and non-negatively weighted edges. It is known that when the solution of the problem exists, the dimension of the Euclidean embedding is at most n. The algorithm provides the smallest possible dimension of the Euclidean space for which the exact embedding of the graph exists. Alternatively, when the distance matrix under consideration is non-Euclidean, the algorithm determines a subset of graph vertices whose mutual distances form the Euclidean matrix. The proposed algorithm is an exact algorithm. If the distance matrix is a Euclidean matrix, the algorithm provides a geometrically unambiguous solution for the location of the graph vertices. The presented embedding method was illustrated using examples of the metric traveling salesman problem that allowed us in some cases to obtain high dimensional partial immersions.}
}


@inproceedings{DBLP:conf/iccS/GuoSYLGL21,
	author = {Yunchuan Guo and
                  Xiyang Sun and
                  Mingjie Yu and
                  Fenghua Li and
                  Kui Geng and
                  Zifu Li},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Resolving Policy Conflicts for Cross-Domain Access Control: {A} Double
                  Auction Approach},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {525--539},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_43},
	doi = {10.1007/978-3-030-77961-0\_43},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/GuoSYLGL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Policy-mapping mechanisms can efficiently help to realize the exchange and the sharing of cross-domain information at low cost. However, due to concerns over policy conflicts, if not sufficient incentives, most selfish domains are often disinterested in helping others to implement policy mapping cooperatively. Thus an appropriate incentive mechanism is required. In this paper, we propose an incentive mechanism to encourage selfish domains to take part in policy mapping and resolve policy conflicts. Formulating conflict resolution as a double auction and solving Bayesian Nash equilibrium, we design the optimal asking/bidding price scheme to maximize the benefits of the domains involved. Simulations demonstrate that our approach can efficiently incentivize selfish domains to take part in cooperation.}
}


@inproceedings{DBLP:conf/iccS/MoulieBT21,
	author = {Hildebert Moulie and
                  Robin van den Berg and
                  Jan Treur},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {An Adaptive Network Model for Procrastination Behaviour Including
                  Self-regulation and Emotion Regulation},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {540--554},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_44},
	doi = {10.1007/978-3-030-77961-0\_44},
	timestamp = {Thu, 14 Oct 2021 10:08:01 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/MoulieBT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, the goal is to model both the self-control and the emotion regulation dynamics involved in the process of procrastination. This is done by means of a temporal-causal network, incorporating learning and control of the learning. Additionally, the effect of stress regulation-therapy on the process of procrastination was investigated. The model’s base level implementation was verified by making sure the aggregated impact matches the node values for certain stationary points and the model’s Hebbian learning behaviour was also mathematically shown to be correctly implemented. The results proved this model’s ability to model different types of individuals, all with different stress sensitivities. Therapy was also shown to be greatly beneficial.}
}


@inproceedings{DBLP:conf/iccS/DepresFM21,
	author = {Hugues D{\'{e}}pr{\'{e}}s and
                  Guillaume Fertin and
                  {\'{E}}ric Monfroy},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Improved Lower Bounds for the Cyclic Bandwidth Problem},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {555--569},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_45},
	doi = {10.1007/978-3-030-77961-0\_45},
	timestamp = {Tue, 15 Jun 2021 17:23:45 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/DepresFM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the classical Cyclic Bandwidth problem, an optimization problem which takes as input an undirected graph \\(G=(V,E)\\) with \\(|V|=n\\), and asks for a labeling \\(\\varphi \\) of V in which every vertex v takes a unique value \\(\\varphi (v)\\in [1;n]\\), in such a way that \\(B_c(G,\\varphi )=\\max \\{\\min _{uv\\in E(G)}\\{|\\varphi (u)-\\varphi (v)|,n-|\\varphi (u)-\\varphi (v)|\\}\\}\\), called the cyclic bandwidth of G, is minimized.\n We provide three new and improved lower bounds for the Cyclic Bandwidth problem, applicable to any graph G: two are based on the neighborhood vertex density of G, the other one on the length of a longest cycle in a cycle basis of G. We also show that our results improve the best known lower bounds for a large proportion of a set of instances taken from a frequently used benchmark, the Harwell-Boeing sparse matrix collection. Our third proof provides additional elements: first, an improved sufficient condition yielding \\(B_c(G)=B(G)\\) (where \\(B(G)=\\min _{\\varphi }\\{\\max _{uv\\in E(G)}\\{|\\varphi (u)-\\varphi (v)|\\}\\}\\) denotes the bandwidth of G)\xa0; second, an algorithm that, under some conditions, computes a labeling reaching B(G) from a labeling reaching \\(B_c(G)\\).}
}


@inproceedings{DBLP:conf/iccS/Zia0AS21,
	author = {Kashif Zia and
                  Umar Farooq and
                  Sanad Al{-}Maskari and
                  Muhammad Shafi},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Co-evolution of Knowledge Diffusion and Absorption: {A} Simulation-Based
                  Analysis},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {570--584},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_46},
	doi = {10.1007/978-3-030-77961-0\_46},
	timestamp = {Sat, 30 Sep 2023 09:44:02 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/Zia0AS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The paper utilizes agent-based simulations to study diffusion and absorption of knowledge. The causal relation of diffusion on absorption is established in order. The process of diffusion and absorption of knowledge is governed by network structure and the dynamics of the recurring influence, conceptualized and modeled as legitimacy, credibility, and strategic complementarity; again a causal relation between the three in order. If not stationary, the agents can also move to acquire either random walk or profile-based mobility modes. Therefore, the co-evolution of network structure due to the mobility of the agents and the dynamics of the recurring influence of ever-changing neighborhood is also modeled. The simulation results reveal that – (i) higher thresholds for legitimacy and credibility determine slower, (ii) higher number of early adopters results into faster, and (iii) a scheduled and repeated mobility (the profile-based mobility) results in faster – absorption of knowledge.}
}


@inproceedings{DBLP:conf/iccS/ErnstK21,
	author = {Sebastian Ernst and
                  Leszek Kotulski},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Estimation of Road Lighting Power Efficiency Using Graph-Controlled
                  Spatial Data Interpretation},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {585--598},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_47},
	doi = {10.1007/978-3-030-77961-0\_47},
	timestamp = {Tue, 15 Jun 2021 17:23:44 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/ErnstK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimation of street lighting energy requirements is a task crucial for both investment planning and efficiency evaluation of retrofit projects. However, this task is time-consuming and infeasible when performed by hand. This paper proposes an approach based on analysis of the publicly available map data. To assure the integrity of this process and automate it, a new type of graph transformations (Spatially Triggered Graph Transformations) is defined. The result is a semantic description of each lighting situation. The descriptions, in turn, are used to estimate the power necessary to fulfil the European lighting standard requirements, using pre-computed configurations stored in a ‘big data’ structure.}
}


@inproceedings{DBLP:conf/iccS/TagowskiBK21,
	author = {Kamil Tagowski and
                  Piotr Bielak and
                  Tomasz Kajdanowicz},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Embedding Alignment Methods in Dynamic Networks},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {599--613},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_48},
	doi = {10.1007/978-3-030-77961-0\_48},
	timestamp = {Tue, 15 Jun 2021 17:23:47 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/TagowskiBK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, dynamic graph embedding has attracted a lot of attention due to its usefulness in real-world scenarios. In this paper, we consider discrete-time dynamic graph representation learning, where embeddings are computed for each time window, and then are aggregated to represent the dynamics of a graph. However, independently computed embeddings in consecutive windows suffer from the stochastic nature of representation learning algorithms and are algebraically incomparable. We underline the need for embedding alignment process and provide nine alignment techniques evaluated on real-world datasets in link prediction and graph reconstruction tasks. Our experiments show that alignment of Node2vec embeddings improves the performance of downstream tasks up\xa0to 11 pp compared to the not aligned scenario.}
}


@inproceedings{DBLP:conf/iccS/KhouzamiSIKSCS21,
	author = {Nesrine Khouzami and
                  Lars Sch{\"{u}}tze and
                  Pietro Incardona and
                  Landfried Kraatz and
                  Tina Subic and
                  Jer{\'{o}}nimo Castrill{\'{o}}n and
                  Ivo F. Sbalzarini},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {The OpenPME Problem Solving Environment for Numerical Simulations},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {614--627},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_49},
	doi = {10.1007/978-3-030-77961-0\_49},
	timestamp = {Sat, 30 Sep 2023 09:44:00 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/KhouzamiSIKSCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce OpenPME, the Open Particle-Mesh Environment, a problem solving environment that provides a Domain Specific Language (DSL) for numerical simulations in scientific computing. It is built atop a domain metamodel that is general enough to cover the main types of numerical simulations: simulations using particles, meshes, and hybrid combinations of particles and meshes. Using model-to-model transformations, OpenPME generates code against the state-of-the-art C++ parallel computing library OpenFPM. This effectively lowers the programming barrier and enables users to implement scalable simulation codes for high-performance computing (HPC) systems using high-level abstractions. Plenty of recent research has shown that higher-level abstractions and problem solving environments are well suited to alleviate low-level implementation overhead. We demonstrate this for OpenPME and its compiler on three different test cases—particle-based, mesh-based, and hybrid particle-mesh—showing up\xa0to 7-fold reduction in the number of lines of code compared to a direct OpenFPM implementation in C++.\n}
}


@inproceedings{DBLP:conf/iccS/GarridoJS21,
	author = {Daniel Garrido and
                  Jo{\~{a}}o Jacob and
                  Daniel Castro Silva},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Building a Prototype for Easy to Use Collaborative Immersive Analytics},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {628--641},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_50},
	doi = {10.1007/978-3-030-77961-0\_50},
	timestamp = {Tue, 21 Mar 2023 20:57:05 +0100},
	biburl = {https://dblp.org/rec/conf/iccS/GarridoJS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increase in the size and complexity of today’s datasets creates a need to develop and experiment with novel data visualization methods. One of these innovations is immersive analytics, in which extended reality technologies such as virtual reality headsets are used to present and study data in virtual worlds. But while the use of immersive analytics dates back to the end of the 20th century, it wasn’t until recently that collaboration in these data visualization environments was taken in consideration. One of the problems currently surrounding this field is the lack of availability of easy to use cooperative data visualization tools that take advantage of the modern, easily attainable head mounted display virtual reality solutions. This work proposes to create an accessible collaborative immersive analytics framework that users with low virtual reality background can master, and share, regardless of platform. With this in mind, a prototype of a visualization platform was developed in Unity3D that allows users to create their own visualizations and collaborate with other users from around the world. Additional features such as avatars, resizable visualizations and data highlighters were implemented to increase immersion and collaborative thinking. The end result shows promising qualities, as it is platform versatile, simple to setup and use and is capable of rapidly enabling groups to meet and analyse data in an immersive environment, even across the world.}
}


@inproceedings{DBLP:conf/iccS/PawlakP21,
	author = {Michal Pawlak and
                  Aneta Poniszewska{-}Maranda},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Implementation of Auditable Blockchain Voting System with Hyperledger
                  Fabric},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {642--655},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_51},
	doi = {10.1007/978-3-030-77961-0\_51},
	timestamp = {Tue, 15 Jun 2021 17:23:47 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/PawlakP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An efficient democratic process requires a quick, fair and fraud-free election process. Many electronic-based voting systems have been developed to fulfil these requirements but there are still unsolved issues with transparency, privacy and data integrity. The development of distributed ledger technology called blockchain creates the potential to solve these issues. This technology’s rapid advancement resulted in numerous implementations, one of which is Hyperledger Fabric, a secure enterprise permissioned blockchain platform. In this paper, the implementation of an Auditable Blockchain Voting System in Hyperledger Fabric is presented to showcase how various platform components can be used to facilitate electronic voting and improve the election process.}
}


@inproceedings{DBLP:conf/iccS/PurawatDBZWGA21,
	author = {Shweta Purawat and
                  Subhasis Dasgupta and
                  Luke Burbidge and
                  Julia L. Zuo and
                  Stephen D. Wilson and
                  Amarnath Gupta and
                  Ilkay Altintas},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Quantum Data Hub: {A} Collaborative Data and Analysis Platform for
                  Quantum Material Science},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {656--670},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_52},
	doi = {10.1007/978-3-030-77961-0\_52},
	timestamp = {Tue, 15 Jun 2021 17:23:45 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/PurawatDBZWGA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum materials research is a rapidly growing domain of materials research, seeking novel compounds whose electronic properties are born from the uniquely quantum aspects of their constituent electrons. The data from this rapidly evolving area of quantum materials requires a new community-driven approach for collaboration and sharing the data from the end-to-end quantum material process. This paper describes the quantum material science process in the NSF Quantum Foundry with an overarching example, and introduces the Quantum Data Hub, a platform to amplify the value of the Foundry data through data science and facilitation of: (i) storing and parsing the metadata that exposes programmatic access to the quantum material research lifecycle; (ii) FAIR data search and access interfaces; (iii) collaborative analysis using Jupyter Hub on top of scalable cyberinfrastructure resources; and (iv) web-based workflow management to log the metadata for the material synthesis and experimentation process.}
}


@inproceedings{DBLP:conf/iccS/DiRYW21,
	author = {Zichao (Wendy) Di and
                  Esteban Rangel and
                  Shinjae Yoo and
                  Stefan M. Wild},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Hierarchical Analysis of Halo Center in Cosmology},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {671--684},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_53},
	doi = {10.1007/978-3-030-77961-0\_53},
	timestamp = {Tue, 15 Jun 2021 17:23:52 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/DiRYW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ever-increasing data size raises many challenges for scientific data analysis. Particularly in cosmological N-body simulation, finding the center of a dark matter halo suffers heavily from the large computational cost associated with the large number of particles (up\xa0to 20 million). In this work, we exploit the latent structure embed in a halo, and we propose a hierarchical approach to approximate the exact gravitational potential calculation for each particle in order to more efficiently find the halo center. Tests of our method on data from N-body simulations show that in many cases the hierarchical algorithm performs significantly faster than existing methods with a desirable accuracy.}
}


@inproceedings{DBLP:conf/iccS/WiatrSK21,
	author = {Roman Wiatr and
                  Renata G. Slota and
                  Jacek Kitowski},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Fast Click-Through Rate Estimation Using Data Aggregates},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {685--698},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_54},
	doi = {10.1007/978-3-030-77961-0\_54},
	timestamp = {Thu, 23 Jun 2022 19:55:21 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/WiatrSK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Click-Through Rate estimation is a crucial prediction task in Real-Time Bidding environments prevalent in display advertising. The estimation provides information on how to trade user visits in various systems. Logistic Regression is a popular choice as the model for this task. Due to the amount, dimensionality and sparsity of data, it is challenging to train and evaluate the model. One of the techniques to reduce the training and evaluation cost is dimensionality reduction. In this work, we present Aggregate Encoding, a technique for dimensionality reduction using data aggregates. Our approach is to build aggregate-based estimators and use them as an ensemble of models weighted by logistic regression. The novelty of our work is the separation of feature values according to the value frequency, to better utilise regularization. For our experiments, we use the iPinYou data set, but this approach is universal and can be applied to other problems requiring dimensionality reduction of sparse categorical data.}
}


@inproceedings{DBLP:conf/iccS/SilvaC21,
	author = {Joaquim F. Silva and
                  Jos{\'{e}} C. Cunha},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {A Model for Predicting n-gram Frequency Distribution in Large Corpora},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {699--706},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_55},
	doi = {10.1007/978-3-030-77961-0\_55},
	timestamp = {Tue, 15 Jun 2021 17:23:47 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/SilvaC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The statistical extraction of multiwords (n-grams) from natural language corpora is challenged by computationally heavy searching and indexing, which can be improved by low error prediction of the n-gram frequency distributions. For different n-gram sizes (\\(n\\!\\ge \\!1\\)), we model the sizes of groups of equal-frequency n-grams, for the low frequencies, \\(k=1, 2,\\ldots \\), by predicting the influence of the corpus size upon the Zipf’s law exponent and the n-gram group size. The average relative errors of the model predictions, from 1-grams up\xa0to 6-grams, are near \\(4\\%\\), for English and French corpora from 62 Million to 8.6 Billion words.}
}


@inproceedings{DBLP:conf/iccS/WangWZW21,
	author = {Zechen Wang and
                  Shupeng Wang and
                  Lei Zhang and
                  Yong Wang},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Exploiting Extensive External Information for Event Detection Through
                  Semantic Networks Word Representation and Attention Map},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {707--714},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_56},
	doi = {10.1007/978-3-030-77961-0\_56},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/WangWZW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Event detection is one of the key tasks to construct knowledge graph and reason graph, also a hot and difficult problem in information extraction. Automatic event detection from unstructured natural language text has far-reaching significance for human cognition and intelligent analysis. However, limited by the source and genre, corpora for event detection can not provide enough information to solve the problems of polysemy, synonym association and lack of information. To solve these problems, this paper proposes a brand new Event Detection model based on Extensive External Information (EDEEI). The model employs external corpus, semantic network, part of speech and attention map to extract complete and accurate triggers. Experiments on ACE 2005 benchmark dataset show that the model effectively uses the external knowledge to detect events, and is significantly superior to the state-of-the-art event detection methods.}
}


@inproceedings{DBLP:conf/iccS/SalabunSK21,
	author = {Wojciech Salabun and
                  Andrii Shekhovtsov and
                  Bartlomiej Kizielewicz},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {A New Consistency Coefficient in the Multi-criteria Decision Analysis
                  Domain},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {715--727},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_57},
	doi = {10.1007/978-3-030-77961-0\_57},
	timestamp = {Tue, 15 Jun 2021 17:23:45 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/SalabunSK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The logical consistency of decision making matrices is an important topic in developing each multi-criteria decision analysis (MCDA) method. For instance, many published papers are addressed to the decisional matrix’s consistency in the Analytic Hierarchy Process method (AHP), which uses Saaty’s seventeen-values scale. This work proposes a new approach to measuring consistency for using a simple three-value scale (binary with a tie). The paper’s main contribution is a proposal of a new consistency coefficient for a decision matrix containing judgments from an expert. We show this consistency coefficient based on an effective MCDA method called the Characteristic Objects METhod (COMET). The new coefficient is explained based on the Matrix of Expert Judgment (MEJ), which is the critical step of the COMET method. The proposed coefficient is based on analysing the relationship between judgments from the MEJ matrix and transitive principles (triads analysis). Four triads classes have been identified and discussed. The proposed coefficient makes it easy to determine the logical consistency and, thus, the expert responses’ quality is essential in the reliable decision-making process. Results are presented in some short study cases.}
}


@inproceedings{DBLP:conf/iccS/SavovJN21,
	author = {Pavel Savov and
                  Adam Jatowt and
                  Radoslaw Nielek},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Predicting the Age of Scientific Papers},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {728--735},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_58},
	doi = {10.1007/978-3-030-77961-0\_58},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/SavovJN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we show how the age of scientific papers can be predicted given a diachronic corpus of papers from a particular domain published over a certain time period. We first train ordinal regression models for the task of predicting the age of individual sentences by fine-tuning series of BERT models for binary classification. We then aggregate the prediction results on individual sentences into a final result for entire papers. Using two corpora of publications from the International World Wide Web Conference and the Journal of Artificial Societies and Social Simulation, we compare various result aggregation methods, and show that the sentence-based approach produces better results than the direct document-level method.}
}


@inproceedings{DBLP:conf/iccS/SongZH21,
	author = {Xiaohui Song and
                  Liangjun Zang and
                  Songlin Hu},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Data Augmentation for Copy-Mechanism in Dialogue State Tracking},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {736--749},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_59},
	doi = {10.1007/978-3-030-77961-0\_59},
	timestamp = {Fri, 11 Jun 2021 16:21:31 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/SongZH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional dialogue state tracking (DST) approaches need a predefined ontology to provide candidate values for each slot. To handle unseen slot values, the copy-mechanism has been widely used in DST models recently, which copies slot values from user utterance directly. Even though the state-of-the-art approaches have shown a promising performance on several benchmarks, there is still a significant gap between seen slot values (values that occur in both training set and test set) and unseen ones (values that only occur in the test set). In this paper, we aim to find out the factors that influence the generalization capability of the copy-mechanism based DST model. Our key observations include two points: 1) performance on unseen values is positively related to the diversity of slot values in the training set; 2) randomly generated strings can enhance the diversity of slot values as well as real values. Based on these observations, an interactive data augmentation algorithm is proposed to train copy-mechanism models, which augments the input dataset by duplicating user utterances and replacing the real slot values with randomly generated strings. Experimental results on three widely used datasets: WoZ 2.0, DSTC2 and Multi-WoZ demonstrate the effectiveness of our approach.}
}


@inproceedings{DBLP:conf/iccS/MurphyRFRT21,
	author = {Erin Murphy and
                  Alexander Rasin and
                  Jacob Furst and
                  Daniela Raicu and
                  Roselyne Tchoua},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Ensemble Labeling Towards Scientific Information Extraction {(ELSIE)}},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {750--764},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_60},
	doi = {10.1007/978-3-030-77961-0\_60},
	timestamp = {Wed, 10 Jan 2024 22:27:39 +0100},
	biburl = {https://dblp.org/rec/conf/iccS/MurphyRFRT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extracting scientific facts from unstructured text is difficult due to challenges specific to the complexity of the scientific named entities and relations to be extracted. This problem is well illustrated through the extraction of polymer names and their properties. Even in the cases where the property is a temperature, identifying the polymer name associated with the temperature may require expertise due to the use of complicated naming conventions and by the fact that new polymer names are being “introduced” into the lexicon as polymer science advances. While domain-specific machine learning toolkits exist that address these challenges, perhaps the greatest challenge is the lack of—time-consuming, error-prone and costly—labeled data to train these machine learning models. This work repurposes Snorkel, a data programming tool, in a novel approach as a way to identify sentences that contain the relation of interest in order to generate training data, and as a first step towards extracting the entities themselves. By achieving 94% recall and an F1 score of 0.92, compared to human experts who achieve 77% recall and an F1 score of 0.87, we show that our system captures sentences missed by both a state-of-the-art domain-aware natural language processing toolkit and human expert labelers. We also demonstrate the importance of identifying the complex sentences prior to extraction by comparing our application to the natural language processing toolkit.}
}


@inproceedings{DBLP:conf/iccS/HovlandH21,
	author = {Paul D. Hovland and
                  Jan H{\"{u}}ckelheim},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Error Estimation and Correction Using the Forward {CENA} Method},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {765--778},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_61},
	doi = {10.1007/978-3-030-77961-0\_61},
	timestamp = {Tue, 13 Jul 2021 13:27:57 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/HovlandH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing use of heterogeneous and more energy-efficient computing systems has led to a renewed demand for reduced- or mixed-precision floating-point arithmetic. In light of this, we present the forward CENA method as an efficient roundoff error estimator and corrector. Unlike the previously published CENA method, our forward variant can be easily used in parallel high-performance computing applications. Just like the original variant, its error estimation capabilities can point out code regions where reduced or mixed precision still achieves sufficient accuracy, while the error correction capabilities can increase precision over what is natively supported on a given hardware platform, whenever higher accuracy is needed. CENA methods can also be used to increase the reproducibility of parallel sum reductions.}
}


@inproceedings{DBLP:conf/iccS/KashanskyRP21,
	author = {Vladislav Kashansky and
                  Gleb I. Radchenko and
                  Radu Prodan},
	editor = {Maciej Paszynski and
                  Dieter Kranzlm{\"{u}}ller and
                  Valeria V. Krzhizhanovskaya and
                  Jack J. Dongarra and
                  Peter M. A. Sloot},
	title = {Monte Carlo Approach to the Computational Capacities Analysis of the
                  Computing Continuum},
	booktitle = {Computational Science - {ICCS} 2021 - 21st International Conference,
                  Krakow, Poland, June 16-18, 2021, Proceedings, Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {12742},
	pages = {779--793},
	publisher = {Springer},
	year = {2021},
	url = {https://doi.org/10.1007/978-3-030-77961-0\_62},
	doi = {10.1007/978-3-030-77961-0\_62},
	timestamp = {Tue, 15 Jun 2021 17:23:53 +0200},
	biburl = {https://dblp.org/rec/conf/iccS/KashanskyRP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article proposes an approach to the problem of computational capacities analysis of the computing continuum via theoretical framework of equilibrium phase-transitions and numerical simulations. We introduce the concept of phase transitions in computing continuum and show how this phenomena can be explored in the context of workflow makespan, which we treat as an order parameter. We simulate the behavior of the computational network in the equilibrium regime within the framework of the XY-model defined over complex agent network with Barabasi-Albert topology. More specifically, we define Hamiltonian over complex network topology and sample the resulting spin-orientation distribution with the Metropolis-Hastings technique. The key aspect of the paper is derivation of the bandwidth matrix, as the emergent effect of the “low-level” collective spin interaction. This allows us to study the first order approximation to the makespan of the “high-level” system-wide workflow model in the presence of data-flow anisotropy and phase transitions of the bandwidth matrix controlled by the means of “noise regime” parameter \\(\\eta \\). For this purpose, we have built a simulation engine in Python 3.6. Simulation results confirm existence of the phase transition, revealing complex transformations in the computational abilities of the agents. Notable feature is that bandwidth distribution undergoes a critical transition from single to multi-mode case. Our simulations generally open new perspectives for reproducible comparative performance analysis of the novel and classic scheduling algorithms.}
}
