@inproceedings{DBLP:conf/esorics/AbadPU25,
	author = {Gorka Abad and
                  Stjepan Picek and
                  Aitor Urbieta},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Time-Distributed Backdoor Attacks on Federated Spiking Learning},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {1--20},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_1},
	doi = {10.1007/978-3-032-07884-1\_1},
	timestamp = {Sun, 09 Nov 2025 16:31:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/AbadPU25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the vulnerability of federated learning (FL) with spiking neural networks (SNNs) to backdoor attacks using neuromorphic data. Despite the efficiency of SNNs\xa0and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks. We first assess the viability of using FL with SNNs\xa0using neuromorphic data, showing its potential usage. Then, we evaluate the transferability of known FL attack methods to SNNs, finding\xa0that these lead to sub-optimal attack performance. Consequently,\xa0we explore backdoor attacks involving single and multiple attackers\xa0to improve the attack performance. Our main contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious clients, enhancing the attack’s effectiveness and stealthiness. In the\xa0best case, we achieve a 100% attack success rate, 0.13 MSE, and 98.9 SSIM. Moreover, we adapt and evaluate existing defenses against backdoor attacks, revealing their inadequacy in protecting SNNs.\xa0Our code is publicly available. (https://github.com/GorkaAbad/Time-Bandits).}
}


@inproceedings{DBLP:conf/esorics/AnserFCK25,
	author = {Omar Anser and
                  J{\'{e}}r{\^{o}}me Fran{\c{c}}ois and
                  Isabelle Chrisment and
                  Daishi Kondo},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {{TATA:} Benchmark {NIDS} Test Sets Assessment and Targeted Augmentation},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {21--41},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_2},
	doi = {10.1007/978-3-032-07884-1\_2},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/AnserFCK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Research works on Network Intrusion Detection Systems (NIDSs) using Machine Learning (ML) usually reports very high detection rate, often well above 90%. However, these results typically originate from overly simplistic NIDS datasets, where the test set, often just a subset of the overall dataset, mirrors the training set distribution, failing to rigorously assess the NIDS’s robustness under more varied conditions. To address this shortcoming, we propose a method for Test\xa0sets Assessment and Targeted Augmentation (TATA). TATA is a model-agnostic approach that assesses and augments the quality of benchmark ML–based NIDS test sets. First, TATA encodes both training and\xa0test sets in a structured latent space via a contrastive autoencoder, defining three quality metrics (diversity, proximity, and scarcity) to identify test set gaps where the ML-based classification is harder. Next, TATA employs a reinforcement learning (RL) approach guided by these metrics, configuring a testbed that produces realistic data specifically targeting these gaps, creating\xa0a more robust test set. Using CIC-IDS2017 and CSE-CIC-IDS2018, we observe a positive correlation between higher metric values and increased detection difficulty, confirming their utility as meaningful indicators of test set robustness. With the same datasets, TATA’s RL-based augmentation significantly raises detection difficulty for multiple NIDS models, revealing previously overlooked weaknesses.}
}


@inproceedings{DBLP:conf/esorics/AttrapadungHHKMNSSY25,
	author = {Nuttapong Attrapadung and
                  Goichiro Hanaoaka and
                  Ryo Hiromasa and
                  Yoshihiro Koseki and
                  Takahiro Matsuda and
                  Yutaro Nishida and
                  Yusuke Sakai and
                  Jacob C. N. Schuldt and
                  Satoshi Yasuda},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Abuse-Resistant Evaluation of AI-as-a-Service via Function-Hiding
                  Homomorphic Signatures},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {42--61},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_3},
	doi = {10.1007/978-3-032-07884-1\_3},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/AttrapadungHHKMNSSY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Artificial intelligence (AI) has emerged as an incredibly useful tool with wide-ranging applications. This has given rise to numerous companies specializing in the development and training of AI models. Access to these models is often provided via cloud services—an approach referred to as “AI as a service” (AIaaS). An AIaaS provider typically offers limited free access to the service to entice users to buy full access, e.g., a bounded number of queries per month. Such free trial access is useful for users who want to determine whether the functionality and performance of the AI are sufficient for their use case. However, free trial access may also be abused by malicious users who might try to circumvent the trial restrictions e.g., by posing as many different users, in an attempt to gain full access to the AIaaS or potentially to try to mount a model extraction attack. As a first attempt to address this issue, we construct a cryptographic mechanism that enables users to test the performance of an AI while simultaneously preventing abuse caused by granting users full model access. Specifically, using our mechanism, users will be restricted to a pre-specified category of inputs or modifications thereof which prevents users from freely specifying the input while still ensuring input diversity. From a technical perspective, we achieve this by formalizing a new cryptographic primitive, function-hiding homomorphic signatures, and instantiating this via a construction based on non-interactive zero-knowledge proofs.}
}


@inproceedings{DBLP:conf/esorics/BarbatoCVFS25,
	author = {Michele Barbato and
                  Alberto Ceselli and
                  Sabrina De Capitani di Vimercati and
                  Sara Foresti and
                  Pierangela Samarati},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {PriSM: {A} Privacy-Friendly Support Vector Machine},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {62--82},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_4},
	doi = {10.1007/978-3-032-07884-1\_4},
	timestamp = {Sun, 09 Nov 2025 16:31:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/BarbatoCVFS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today’s society is witnessing not only an evergrowing dependency on data, but also an increasingly pervasiveness of related analytics and machine learning applications. From business to leisure, the availability of services providing answers to questions brings great benefits in diverse domains. On the other side of the coin, the need to provide input data that the services need to compute a response. However, some data may be considered sensitive or confidential and users would legitimately be reluctant to release them to third parties. Considering classification tasks in machine learning applications, we introduce our PriSM (Privacy-friendly Support vector Machine) approach for computing a privacy-friendly model. PriSM anticipates the training phase of the classifier with a phase for discovering correlations among attributes that can indirectly expose sensitive information. It then trains the classifier excluding from consideration not only sensitive attributes but also other sets of attributes that have been learned as correlated to them. The result is a privacy-friendly classifier that does not require any of such information as input from the users. Our experimental evaluation on both synthetic and real-world datasets confirms the effectiveness of PriSM in protecting privacy while maintaining classification accuracy.}
}


@inproceedings{DBLP:conf/esorics/BreniauxM25,
	author = {Hugo Breniaux and
                  Djedjiga Mouheb},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Towards Context-Aware Log Anomaly Detection Using Fine-Tuned Large
                  Language Models},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {83--102},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_5},
	doi = {10.1007/978-3-032-07884-1\_5},
	timestamp = {Sun, 09 Nov 2025 16:31:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/BreniauxM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the expansion of complex IT systems, the volume\xa0of generated log data continues to escalate, intensifying\xa0the challenges of monitoring and securing these systems. Recent advances in log-based anomaly detection demonstrate effectiveness\xa0in leveraging deep learning techniques to answer these challenges. However, most approaches remain limited in their ability to extract complex relationships and understand contextual patterns from\xa0log data. In this work, we present a log-based anomaly detection approach based on fine-tuned large language models (LLMs), designed to improve context-aware and intelligent detection methods without using degrading parsing techniques or log templates. The\xa0model learns normal behaviors through self-supervised fine-tuning\xa0on normal system and network log data, aiming to complete a next\xa0log prediction task from a sequence of raw logs. The predicted log\xa0is then compared to the ground truth using cosine similarity to assess the deviation from expected behavior and identify anomalies.\xa0The experiments showcase notable results on system logs, exceeding state-of-the-art F1 scores with 0.945 on BGL, 0.926 on Thunderbird, and 0.920 on Spirit datasets. Furthermore, despite using a language model, our approach unveils promising results over network logs, mainly composed of numerical values, with an F1 score of 0.957\xa0on NLS-KDD.}
}


@inproceedings{DBLP:conf/esorics/ChennoufiHBCK25,
	author = {Sara Chennoufi and
                  Yufei Han and
                  Gregory Blanc and
                  Emiliano De Cristofaro and
                  Christophe Kiennert},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {{PROTEAN:} Federated Intrusion Detection in Non-IID Environments Through
                  Prototype-Based Knowledge Sharing},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {103--125},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_6},
	doi = {10.1007/978-3-032-07884-1\_6},
	timestamp = {Sat, 15 Nov 2025 13:45:37 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/ChennoufiHBCK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In distributed networks, participants often face diverse and fast-evolving cyberattacks. This makes techniques based\xa0on Federated Learning (FL) a promising mitigation strategy. By\xa0only exchanging model updates, FL participants can collaboratively\xa0build detection models without revealing sensitive information, e.g., network structures or security postures. However, the effectiveness of FL solutions is often hindered by significant data heterogeneity, as attack patterns often differ drastically across organizations\xa0due to varying security policies. To address these challenges,\xa0we introduce PROTEAN, a Prototype Learning-based framework geared\xa0to facilitate collaborative and privacy-preserving intrusion detection. PROTEAN enables accurate detection in environments with highly non-IID attack distributions and promotes direct knowledge sharing by exchanging class prototypes of different attack types\xa0among participants. This allows organizations to better understand attack techniques not present in their data collections. We instantiate PROTEAN on two cyber intrusion datasets collected from IIoT\xa0and 5G-connected participants and evaluate its performance in terms\xa0of utility and privacy, demonstrating its effectiveness in addressing data heterogeneity while improving cyber attack understanding\xa0in federated intrusion detection systems (IDSs).}
}


@inproceedings{DBLP:conf/esorics/GangwalCP25,
	author = {Ankit Gangwal and
                  Mauro Conti and
                  Tommaso Pauselli},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {KeTS: Kernel-Based Trust Segmentation Against Model Poisoning Attacks},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {126--146},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_7},
	doi = {10.1007/978-3-032-07884-1\_7},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/GangwalCP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning\xa0(FL) enables multiple users\xa0to collaboratively train a global model in a distributed manner without revealing their personal data. However, FL remains vulnerable\xa0to model poisoning attacks, where malicious actors inject crafted updates to compromise the global model’s accuracy. We propose\xa0a novel defense mechanism, Kernel-based\xa0Trust Segmentation\xa0(KeTS), to counter model poisoning attacks. Unlike existing approaches, KeTS analyzes the evolution\xa0of each client’s updates and effectively segments malicious clients using Kernel Density Estimation\xa0(KDE), even in the presence\xa0of benign outliers. We thoroughly evaluate KeTS’s performance against the six most effective model poisoning attacks\xa0(i.e., Trim-Attack, Krum-Attack, Min-Max attack, Min-Sum attack, and their variants) on four different datasets\xa0(i.e., MNIST, Fashion-MNIST, CIFAR-10, and KDD-CUP-1999) and compare its performance with three classical robust schemes\xa0(i.e., Krum, Trim-Mean,\xa0and Median) and a state-of-the-art defense\xa0(i.e., FLTrust). Our results show that KeTS outperforms the existing defenses in every attack setting; beating\xa0the best-performing defense by an overall average of \\(>24\\)%\xa0(on MNIST), \\(>14\\)%\xa0(on Fashion-MNIST), \\(>9\\)%\xa0(on CIFAR-10), \\(>11\\)%\xa0(on KDD-CUP-1999). A series of further experiments\xa0(varying poisoning approaches, attacker population, etc.) reveal the consistent\xa0and superior performance of KeTS under diverse conditions. KeTS is a practical solution as it satisfies all\xa0three defense objectives\xa0(i.e., fidelity, robustness, and efficiency) without imposing additional overhead on the clients. Finally,\xa0we also discuss a simple, yet effective extension to KeTS to handle consistent-untargeted\xa0(e.g., sign-flipping) attacks as\xa0well as targeted attacks\xa0(e.g., label-flipping).}
}


@inproceedings{DBLP:conf/esorics/GhorbelCLION25,
	author = {Mahmoud Ghorbel and
                  Selina Cheggour and
                  Valeria Loscr{\`{\i}} and
                  Youcef Imine and
                  Hamza Ouarnoughi and
                  Sma{\"{\i}}l Niar},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Machine Learning Vulnerabilities in 6G: Adversarial Attacks and Their
                  Impact on Channel Gain Prediction and Resource Allocation in UC-CFmMIMO},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {147--165},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_8},
	doi = {10.1007/978-3-032-07884-1\_8},
	timestamp = {Thu, 25 Dec 2025 12:47:18 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/GhorbelCLION25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) models integrated into physical-layer functions in wireless systems are increasingly vulnerable to adversarial attacks. Although prior research has investigated such threats in conventional massive MIMO architectures, the security risks in future 6G topologies, particularly user-centric cell-free massive MIMO (UC-CFmMIMO) deployed in vehicular environments, remain largely unexplored. These architectures depend heavily on frequency-domain channel gain estimation, which opens new attack surfaces. In this work, we present a black-box adversarial framework tailored to UC-CFmMIMO networks operating in dynamic vehicular environments. The attacker passively collects RF data to train a surrogate model and crafts perturbations using the FGSM attack. A local anomaly detector is integrated to assess stealth prior to uplink injection via pilot contamination. Our method significantly disrupts channel gain estimation and subband allocation, while requiring no access to the target model’s internals. These results underscore emerging vulnerabilities in ML-enabled wireless systems and highlight the need for robust, context-aware defenses.}
}


@inproceedings{DBLP:conf/esorics/HalderAC25,
	author = {Sajal Halder and
                  Muhammad Ejaz Ahmed and
                  Seyit Camtepe},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {FuncVul: An Effective Function Level Vulnerability Detection Model
                  Using {LLM} and Code Chunk},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {166--185},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_9},
	doi = {10.1007/978-3-032-07884-1\_9},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/HalderAC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software supply chain vulnerabilities arise when attackers exploit weaknesses by injecting vulnerable code into widely used packages or libraries within software repositories. While most existing approaches focus on identifying vulnerable packages or libraries, they often overlook the specific functions responsible for these vulnerabilities. Pinpointing vulnerable functions within packages or libraries is critical, as it can significantly reduce the risks associated with using open-source software. Identifying vulnerable patches is challenging because developers often submit code changes that are unrelated to vulnerability fixes. To address this issue, this paper introduces FuncVul, an innovative code chunk-based model for function-level vulnerability detection in C/C++ and Python, designed to identify multiple vulnerabilities within a function by focusing on smaller, critical code segments. To assess the model’s effectiveness, we construct six code and generic code chunk based datasets using two approaches: (1) integrating patch information with large language models to label vulnerable samples and (2) leveraging large language models alone to detect vulnerabilities in function-level code. To design FuncVul vulnerability model, we utilise GraphCodeBERT fine tune model that captures both the syntactic and semantic aspects of code. Experimental results show that FuncVul outperforms existing state-of-the-art models, achieving an average accuracy of 87–92% and an F1 score of 86–92% across all datasets. Furthermore, we have demonstrated that our code-chunk-based FuncVul model improves 53.9% accuracy and 42.0% F1-score than the full function-based vulnerability prediction. The model code and datasets are publicly available on GitHub (https://github.com/sajalhalder/FuncVul).}
}


@inproceedings{DBLP:conf/esorics/IbanezLissenGFAG25,
	author = {Luis Iba{\~{n}}ez{-}Lissen and
                  Lorena Gonz{\'{a}}lez{-}Manzano and
                  Jos{\'{e}} Mar{\'{\i}}a de Fuentes and
                  Nicolas Anciaux and
                  Joaqu{\'{\i}}n Garc{\'{\i}}a},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {{LUMIA:} Linear Probing for Unimodal and MultiModal Membership Inference
                  Attacks Leveraging Internal {LLM} States},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {186--206},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_10},
	doi = {10.1007/978-3-032-07884-1\_10},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/IbanezLissenGFAG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Language Models (LLMs) are increasingly used in a variety of applications. Concerns around inferring whether data samples belong to the LLM training dataset have grown in parallel. Previous efforts focus on black-to-grey-box models, thus neglecting the potential benefit from internal LLM information. To address this problem, we propose the use of Linear Probes (LPs) as a method to assess Membership Inference Attacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed LUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner workings. We test this method across several model architectures, sizes and datasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA achieves an average gain of 14.90% in Area Under the Curve (AUC) over previous techniques. Remarkably, LUMIA reaches AUC > 60% in 65.33% of cases—an increase of 46.80% against the state of the art. Furthermore, our approach reveals key insights, such as the model layers where MIAs are most detectable. In multimodal models, LPs indicate that visual inputs significantly contribute to MIAs—AUC > 60% is reached in 85.90% of the experiments.}
}


@inproceedings{DBLP:conf/esorics/LiAPC25,
	author = {Jiaxin Li and
                  Gorka Abad and
                  Stjepan Picek and
                  Mauro Conti},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Membership Privacy Evaluation in Deep Spiking Neural Networks},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {207--227},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_11},
	doi = {10.1007/978-3-032-07884-1\_11},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/LiAPC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although Artificial Neural Networks (ANNs) have achieved remarkable success in multiple tasks, e.g., face recognition and object detection, Spiking Neural Networks (SNNs) have recently attracted attention due to their low power consumption, fast inference, and event-driven properties. It is well-known that ANNs are vulnerable to the Membership Inference Attack (MIA), but whether the same applies to SNNs has not been explored. In this paper, we evaluate the membership privacy of SNNs by considering eight MIAs, seven of which are inspired by MIAs against ANNs. Our evaluation results show that SNNs are more vulnerable (maximum 10% higher in terms of balanced attack accuracy) than ANNs when both are trained with neuromorphic datasets (with time dimension). On the other hand, when training ANNs or SNNs with static datasets, the vulnerability depends on the dataset used. If we convert ANNs trained with static datasets to SNNs, the accuracy of MIAs drops (maximum 11.5% with a reduction of 7.6% on the test accuracy of the target model). Next, we explore the impact factors of MIAs on SNNs by conducting a hyperparameter study. Finally, we show that the basic data augmentation method for static datasets and two recent data augmentation methods for neuromorphic datasets can considerably (maximum reduction of 25.7%) decrease MIAs’ performance on SNNs. Regardless, the accuracy of MIAs could still be between 51.7% and 66.4% with data augmentation, indicating data augmentation cannot fully prevent MIAs on SNNs.}
}


@inproceedings{DBLP:conf/esorics/MarchioriAPC25,
	author = {Francesco Marchiori and
                  Marco Alecci and
                  Luca Pajola and
                  Mauro Conti},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {{DUMB} and DUMBer: Is Adversarial Training Worth It in the Real World?},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {228--248},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_12},
	doi = {10.1007/978-3-032-07884-1\_12},
	timestamp = {Sun, 09 Nov 2025 16:31:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/MarchioriAPC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adversarial examples are small and often imperceptible perturbations crafted to fool machine learning models. These attacks seriously threaten the reliability of deep neural networks, especially in security-sensitive domains. Evasion attacks, a form of adversarial attack where input is modified at test time to cause misclassification, are particularly insidious due to their transferability: adversarial examples crafted against one model often fool other models as well. This property, known as adversarial transferability, complicates defense strategies since it enables black-box attacks to succeed without direct access to the victim model. While adversarial training is one of the most widely adopted defense mechanisms, its effectiveness is typically evaluated on a narrow and homogeneous population of models. This limitation hinders the generalizability of empirical findings and restricts practical adoption. In this work, we introduce DUMBer, an attack framework built on the foundation of the DUMB (Dataset soUrces, Model architecture, and Balance) methodology, to systematically evaluate the resilience of adversarially trained models. Our testbed spans multiple adversarial training techniques evaluated across three diverse computer vision tasks, using a heterogeneous population of uniquely trained models to reflect real-world deployment variability. Our experimental pipeline comprises over 130k evaluations spanning 13 state-of-the-art attack algorithms, allowing us to capture nuanced behaviors of adversarial training under varying threat models and dataset conditions. Our findings offer practical, actionable insights for AI practitioners, identifying which defenses are most effective based on the model, dataset, and attacker setup.}
}


@inproceedings{DBLP:conf/esorics/NaKYC25,
	author = {Hyunsik Na and
                  Hajun Kim and
                  Dooshik Yoon and
                  Daeseon Choi},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Countering Jailbreak Attacks with Two-Axis Pre-detection and Conditional
                  Warning Wrappers},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {249--268},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_13},
	doi = {10.1007/978-3-032-07884-1\_13},
	timestamp = {Sun, 09 Nov 2025 16:31:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/NaKYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ensuring the security and ethical alignment of large language models (LLMs) is critical as adversarial attacks, such as prompt injections and jailbreak exploits, continue to evolve. Pre-detection mechanisms have emerged as a promising defense, filtering adversarial prompts before they reach the LLM. However, existing pre-detectors exhibit limitations in distinguishing genuinely harmful queries from legitimate prompts that resemble adversarial inputs, leading to high false positive rates (FPR). To address this, we propose a Two-Axis Pre-Detector (TAPD) that independently classifies harmfulness and jailbreakness, enhancing detection granularity. Furthermore, we introduce a conditional Warning Wrapper mechanism (CWW), a conditional self-reminder that mitigates false positives while maintaining LLM alignment. Our empirical evaluation demonstrates that TAPD significantly reduces FPR while preserving robust security measures, improving both pre-detection reliability and usability in real-world AI applications.}
}


@inproceedings{DBLP:conf/esorics/NougnankeBR25,
	author = {B{\'{e}}no{\^{\i}}t Nougnanke and
                  Gregory Blanc and
                  Thomas Robert},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {How Dataset Diversity Affects Generalization in ML-Based {NIDS}},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {269--288},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_14},
	doi = {10.1007/978-3-032-07884-1\_14},
	timestamp = {Sun, 09 Nov 2025 16:31:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/NougnankeBR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine Learning-based Network Intrusion Detection Systems (ML-based NIDS) rely heavily on the quality of the datasets used for training and evaluation. However, widely used NIDS benchmarks often suffer from poor data diversity, which limits model generalization and undermines the reliability of evaluation protocols. While prior work has acknowledged this limitation, a systematic framework to quantify dataset diversity and analyze its relationship with performance is still missing. To address this gap, we introduce a structured approach for characterizing dataset diversity in ML-based NIDS, grounded in measurement theory. We distinguish three types of diversity—intra-class, inter-class, and domain-shift—and operationalize their measurement using established metrics such as the Vendi Score and the Jensen-Shannon divergence. Our empirical analysis on the CIC-IDS2018 dataset, spanning sixty diversity-controlled train–test experiments, provides new insights into the relationship between diversity and generalization and demonstrates the value of diversity-aware data sampling for improving evaluation reliability.}
}


@inproceedings{DBLP:conf/esorics/OuchebaraD25,
	author = {Dyna Soumhane Ouchebara and
                  St{\'{e}}phane Dupont},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Llama-Based Source Code Vulnerability Detection: Prompt Engineering
                  vs Fine Tuning},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {289--308},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_15},
	doi = {10.1007/978-3-032-07884-1\_15},
	timestamp = {Sat, 15 Nov 2025 13:45:37 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/OuchebaraD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.}
}


@inproceedings{DBLP:conf/esorics/QiGLHZL25,
	author = {Fuqi Qi and
                  Haichang Gao and
                  Boling Li and
                  Guangyu He and
                  Yuhong Zhang and
                  Jiacheng Luo},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {{DBBA:} Diffusion-Based Backdoor Attacks on Open-Set Face Recognition
                  Models},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {309--327},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_16},
	doi = {10.1007/978-3-032-07884-1\_16},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/QiGLHZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural network-based face recognition models are widely deployed in authentication systems but remain vulnerable to backdoor attacks. Existing methods face critical limitations: (1) label-poisoning attacks are easily detectable, while clean-label attacks often rely on adversarial perturbations that degrade image quality; (2) triggers lacking semantic information are conspicuous and impractical in physical settings; and (3) attacker-victim identity selection is often restricted, limiting applicability in open-set scenarios. To address these issues, we propose DBBA, a diffusion-based backdoor attack framework that operates under clean-label constraints in open-set face recognition. DBBA leverages the high-fidelity generative power of diffusion models and their multi-modal capabilities to synthesize visually plausible, semantically meaningful poisoned faces. By incorporating trigger optimization, a multi-objective loss, and an adaptive identity selection strategy, our method achieves a good balance between poisoning success and clean accuracy. Extensive experiments validate the stealth, effectiveness, and real-world applicability of DBBA, which can inspire and promote the security enhancement of the application of face recognition models in the future.}
}


@inproceedings{DBLP:conf/esorics/ReaneyMS25,
	author = {Matthew Reaney and
                  Kieran McLaughlin and
                  Sandra Scott{-}Hayward},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Evaluation of Autonomous Intrusion Response Agents in Adversarial
                  and Normal Scenarios},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {328--345},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_17},
	doi = {10.1007/978-3-032-07884-1\_17},
	timestamp = {Sat, 15 Nov 2025 13:45:37 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/ReaneyMS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of threats to networked cyber-physical systems (CPS) such as FrostyGoop (2024), Pipedream (2022) and Industroyer2 (2022), motivates a desire for improved automated defences against such threats. Research has found deep reinforcement learning (DRL) can enable autonomous intrusion response systems (IRS) to learn optimal response policies from experience, without relying on static pre-configured rules or explicit system models. The goal of autonomous defence is protecting against multi-stage attacks by learning to minimise disruption to the normal operation of the system and restore CPS functionality. However, this approach is dependent on the training environment being representative of a real system while being conducive to learning. Previous approaches focus on designing agents to adapt to adverse training conditions and neglect evaluation in scenarios absent of the adversary and/or defence agent. In contrast, we focus on improving the design of the training environment proposing both adversarial and normal scenarios for evaluation. Our analysis reveals several novel observations linked to suboptimal training conditions. For example, through evaluation of normal scenarios, it was revealed that security alerts were still present in the absence of the adversary. These observations challenge the assumptions made about the environment implementation in previous work. Our contributions support improved agent training for effective autonomous IRS.}
}


@inproceedings{DBLP:conf/esorics/RobinetteNSJ25,
	author = {Preston K. Robinette and
                  Thuy Dung Nguyen and
                  Samuel Sasaki and
                  Taylor T. Johnson},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Trigger-Based Fragile Model Watermarking for Image Transformation
                  Networks},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {346--365},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_18},
	doi = {10.1007/978-3-032-07884-1\_18},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/RobinetteNSJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In fragile watermarking, a sensitive watermark is embedded in an object in a manner such that the watermark breaks upon tampering. This fragile process can be used to ensure the integrity and source of watermarked objects. While fragile watermarking for model integrity has been studied in classification models, image transformation/generation models have yet to be explored. We introduce a novel, trigger-based fragile model watermarking system for image transformation/generation networks that takes advantage of properties inherent to image outputs. For example, manifesting watermarks as specific visual patterns, styles, or anomalies in the generated content when particular trigger inputs are used. Our approach, distinct from robust watermarking, effectively verifies the model’s source and integrity across various datasets and attacks, outperforming baselines by 99%. We conduct additional experiments to analyze the security of this approach, the flexibility of the trigger and resulting watermark, and the sensitivity of the watermarking loss on performance. We also demonstrate the applicability of this approach on two different tasks (1 immediate task and 1 downstream task). This is the first work to consider trigger-based fragile model watermarking for image transformation/generation networks. The code for this project is available here: https://github.com/pkrobinette/img_trans_watermark.}
}


@inproceedings{DBLP:conf/esorics/ShahriarWRHL25,
	author = {Md Hasan Shahriar and
                  Ning Wang and
                  Naren Ramakrishnan and
                  Y. Thomas Hou and
                  Wenjing Lou},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Let the Noise Speak: Harnessing Noise for a Unified Defense Against
                  Adversarial and Backdoor Attacks},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {366--387},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_19},
	doi = {10.1007/978-3-032-07884-1\_19},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/ShahriarWRHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The exponential adoption of machine learning (ML) is propelling the world into a future of distributed and intelligent automation and data-driven solutions. However, the proliferation of malicious data manipulation attacks against ML, namely adversarial and backdoor attacks, jeopardizes its reliability in safety-critical applications. The existing detection methods are attack-specific and built upon some strong assumptions, limiting them in diverse practical scenarios. Thus, motivated by the need for a more robust, unified, and attack-agnostic defense mechanism, we first investigate the shared traits of adversarial and backdoor attacks. Based on our observation, we propose NoiSec, a reconstruction-based intrusion detection system that brings a novel perspective by shifting focus from the reconstructed input to the reconstruction noise itself, which is the foundational root cause of such malicious data alterations. NoiSec disentangles the noise from the test input, extracts the underlying features from the noise, and leverages them to recognize systematic malicious manipulation. Our comprehensive evaluation of NoiSec demonstrates its high effectiveness across various datasets, including basic objects, natural scenes, traffic signs, medical images, spectrogram-based audio data, and wireless sensing against five state-of-the-art adversarial attacks and three backdoor attacks under challenging evaluation conditions. NoiSec demonstrates strong detection performance in both white-box and black-box adversarial attack scenarios, significantly outperforming the closest baseline models, particularly in an adaptive attack setting. We will provide the code for future baseline comparison. Our code and artifacts are publicly available at https://github.com/shahriar0651/NoiSec.}
}


@inproceedings{DBLP:conf/esorics/WuCWY25,
	author = {Kerui Wu and
                  Ka{-}Ho Chow and
                  Wenqi Wei and
                  Lei Yu},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {On the Adversarial Robustness of Graph Neural Networks with Graph
                  Reduction},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {388--409},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_20},
	doi = {10.1007/978-3-032-07884-1\_20},
	timestamp = {Sat, 15 Nov 2025 13:45:37 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/WuCWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Graph Neural Networks (GNNs) become increasingly popular for learning from large-scale graph data across various domains, their susceptibility to adversarial attacks when using graph reduction techniques for scalability remains underexplored. In this paper, we present an extensive empirical study to investigate the impact of graph reduction techniques—specifically graph coarsening and sparsification—on the robustness of GNNs against adversarial attacks. Through extensive experiments involving multiple datasets and GNN architectures, we examine the effects of four sparsification and six coarsening methods on the poisoning attacks. Our results indicate that, while graph sparsification can mitigate the effectiveness of certain poisoning attacks, such as Mettack, it has limited impact on others, like PGD. Conversely, graph coarsening tends to amplify the adversarial impact, significantly reducing classification accuracy as the reduction ratio decreases. Additionally, we provide a novel analysis of the causes driving these effects and examine how defensive GNN models perform under graph reduction, offering practical insights for designing robust GNNs within graph acceleration systems.}
}


@inproceedings{DBLP:conf/esorics/WuLLNL25,
	author = {Xiaodong Wu and
                  Xiangman Li and
                  Qi Li and
                  Jianbing Ni and
                  Rongxing Lu},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {SecureT2I: No More Unauthorized Manipulation on {AI} Generated Images
                  from Prompts},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {410--429},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_21},
	doi = {10.1007/978-3-032-07884-1\_21},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/WuLLNL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Text-guided image manipulation with diffusion models enables flexible and precise editing based on prompts, but raises ethical and copyright concerns due to potential unauthorized modifications. To address this, we propose SecureT2I, a secure framework designed to prevent unauthorized editing in diffusion-based generative models. SecureT2I is compatible with both general-purpose and domain-specific models and can be integrated via lightweight fine-tuning without architectural changes. We categorize images into a permit set and a forbid set based on editing permissions. For the permit set, the model learns to perform high-quality manipulations as usual. For the forbid set, we introduce training objectives that encourage vague or semantically ambiguous outputs (e.g., blurred images), thereby suppressing meaningful edits. The core challenge is to block unauthorized editing while preserving editing quality for permitted inputs. To this end, we design separate loss functions that guide selective editing behavior. Extensive experiments across multiple datasets and models show that SecureT2I effectively degrades manipulation quality on forbidden images while maintaining performance on permitted ones. We also evaluate generalization to unseen inputs and find that SecureT2I consistently outperforms baselines. Additionally, we analyze different vagueness strategies and find that resize-based degradation offers the best trade-off for secure manipulation control.}
}


@inproceedings{DBLP:conf/esorics/XuWLBCR25,
	author = {Jiali Xu and
                  Shuo Wang and
                  Valeria Loscr{\`{\i}} and
                  Alessandro Brighente and
                  Mauro Conti and
                  Romain Rouvoy},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {GANSec: Enhancing Supervised Wireless Anomaly Detection Robustness
                  Through Tailored Conditional {GAN} Augmentation},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {430--449},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_22},
	doi = {10.1007/978-3-032-07884-1\_22},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/XuWLBCR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data augmentation techniques show potential in various domains, yet their application to enhance robustness in wireless anomaly detection remains underexplored. Wireless datasets often suffer from anomaly scarcity and class imbalance, hindering the training of reliable detection models. This work introduces GANSec, a novel conditional Generative Adversarial Networks (GAN) framework specifically designed to augment wireless time-series data. We investigate different neural network architectures (MLP, LSTM, CNN) and two conditional training objectives (Embedded Conditional, Classification Oriented) within GANSec, evaluating the framework using real-world 5G measurements for jamming anomaly detection. For evaluation, we train the downstream anomaly detector exclusively on GANSec-generated data and test its performance in a cross-scenario setting. Our evaluation demonstrates that models trained this way significantly outperform those trained on original or baseline augmentation data when tested under unseen network conditions. Specifically, our approach achieved up to 92.13% accuracy on the unseen dataset (i.e., data collected from a different distribution reflecting network conditions distinct from the training set), compared to 78% for models trained on raw data and 83.33% for the best-performing baseline, exhibiting substantially enhanced robustness and generalization.}
}


@inproceedings{DBLP:conf/esorics/YaguchiK25,
	author = {Terumi Yaguchi and
                  Hiroaki Kikuchi},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {Fine-Grained Data Poisoning Attack to Local Differential Privacy Protocols
                  for Key-Value Data},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {450--468},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_23},
	doi = {10.1007/978-3-032-07884-1\_23},
	timestamp = {Sun, 09 Nov 2025 16:31:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/YaguchiK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the spread of smart devices, companies improve their services by collecting and utilizing users’ behavioral data. However, the collected data from the user’s device can create issues concerning the identification of individuals, and hence privacy protection is required. Local Differential Privacy (LDP) is a technique that perturbs the user’s data before sending to the server so that the server is not able to have access to private data. Unfortunately, LDP is vulnerable to a poisoning attack in which a set of malicious users disrupt the estimated statistics by sending crafted data. In 2024, Li et al. showed that fine-grained manipulation of the estimated means is feasible. In this work, we study a new fine-grained attack to a multidimensional data with LDP known as Locally Differentially Private Correlated Key-Value (PCKV) for key-value data. We evaluate the proposed fine-grained PCKV attack from both theoretical and empirical viewpoints.\n}
}


@inproceedings{DBLP:conf/esorics/YaoKGM25,
	author = {Zexi Yao and
                  Natasa Krco and
                  Georgi Ganev and
                  Yves{-}Alexandre de Montjoye},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {The {DCR} Delusion: Measuring the Privacy Risk of Synthetic Data},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {469--487},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_24},
	doi = {10.1007/978-3-032-07884-1\_24},
	timestamp = {Thu, 30 Oct 2025 10:40:55 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/YaoKGM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Synthetic data has become an increasingly popular means of sharing data without revealing sensitive information. Though Membership Inference Attacks (MIAs) are widely considered the gold standard for empirically assessing the privacy of a synthetic dataset, for simplicity, practitioners and researchers often rely on proxy metrics such as Distance to Closest Record (DCR). These metrics estimate a synthetic dataset’s privacy by measuring the similarity between the training data and generated synthetic data. This similarity can also be compared against the similarity between the training data and a disjoint holdout set of real records to construct a binary privacy test. If the synthetic data is not more similar to the training data than the holdout set is, the synthetic dataset passes the privacy test and is considered private. In this work, we show that, while computationally inexpensive, DCR and other distance-based metrics fail to identify privacy leakage. Across multiple datasets and both classical models, such as Baynet and CTGAN, and more recent diffusion models, we show that datasets deemed private by proxy metrics are highly vulnerable to MIAs. We similarly find both the binary privacy test and the continuous measure based on these metrics to be uninformative of actual membership inference risk. We further show that these failures are consistent across different metric hyperparameter settings and record selection methods. Finally, we argue that DCR and other distance-based metrics are flawed by design and show an example of a simple leakage they miss in practice. With this work, we hope to motivate practitioners to move away from proxy metrics to MIAs as the rigorous, comprehensive standard of evaluating privacy of synthetic data, in particular to make claims of datasets being legally anonymous.}
}


@inproceedings{DBLP:conf/esorics/YoosufALAK25,
	author = {Shehel Yoosuf and
                  Temoor Ali and
                  Ahmed Lekssays and
                  Mashael AlSabah and
                  Issa Khalil},
	editor = {Vincent Nicomette and
                  Abdelmalek Benzekri and
                  Nora Boulahia{-}Cuppens and
                  Jaideep Vaidya},
	title = {StructTransform: {A} Scalable Attack Surface for Safety-Aligned Large
                  Language Models},
	booktitle = {Computer Security - {ESORICS} 2025 - 30th European Symposium on Research
                  in Computer Security, Toulouse, France, September 22-24, 2025, Proceedings,
                  Part {I}},
	series = {Lecture Notes in Computer Science},
	volume = {16053},
	pages = {488--507},
	publisher = {Springer},
	year = {2025},
	url = {https://doi.org/10.1007/978-3-032-07884-1\_25},
	doi = {10.1007/978-3-032-07884-1\_25},
	timestamp = {Thu, 25 Dec 2025 12:47:18 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/YoosufALAK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Safety alignment and adversarial attack research for Large Language Models (LLMs) predominantly focuses on natural language inputs and outputs. This work introduces StructTransform, a blackbox attack against alignment where malicious prompts are encoded into diverse structure transformations. These range from standard formats (e.g., SQL, JSON) to novel syntaxes generated entirely by LLMs. By shifting harmful prompts Out-Of-Distribution (OOD) relative to typical natural language, these transformations effectively circumvent existing safety alignment mechanisms. Our extensive evaluations show that simple StructTransform attacks achieve high Attack Success Rates (ASR), nearing 90% even against state-of-the-art models like Claude 3.5 Sonnet. Combining structural and content transformations further increases ASR to over 96% without any refusals. We demonstrate the ease with which LLMs can generate novel syntaxes and their effectiveness in bypassing defenses, creating a vast attack surface. Using a new benchmark, we show that current alignment techniques and defences largely fail against these structure-based attacks. This failure strongly suggests a reliance on token-level patterns within natural language, rather than a robust, structure-aware conceptual understanding of harmful requests, exposing a critical need for generalized safety mechanisms robust to variations in input structure.\n}
}
