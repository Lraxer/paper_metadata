@inproceedings{DBLP:conf/esorics/PaudelA23,
	author = {Bipin Paudel and
                  George T. Amariucai},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Reinforcement Learning Approach to Generate Zero-Dynamics Attacks
                  on Control Systems Without State Space Models},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {3--22},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_1},
	doi = {10.1007/978-3-031-51482-1\_1},
	timestamp = {Sat, 10 Feb 2024 18:04:44 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/PaudelA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stealthy attacks on control systems are bound to go unnoticed, which makes them a severe threat to critical infrastructure such as power systems, smart grids, and vehicular networks. This paper investigates a subset of stealthy attacks known as zero-dynamics-based stealthy attacks. While previous works on zero-dynamics attacks have highlighted the necessity of highly accurate knowledge of the system’s state space for generating attack signals, our study requires none. We propose a deep reinforcement learning based attacker to generate attack signals without prior knowledge of the system’s state space. We develop several attackers and detectors iteratively until the attacker and detectors no longer improve. In addition, we also show that the reinforcement learning based attacker successfully executes an attack in the same manner as the theoretical attacker described in previous literature.}
}


@inproceedings{DBLP:conf/esorics/MaoXLHYZ23,
	author = {Yunlong Mao and
                  Zexi Xin and
                  Zhenyu Li and
                  Jue Hong and
                  Qingyou Yang and
                  Sheng Zhong},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Secure Split Learning Against Property Inference, Data Reconstruction,
                  and Feature Space Hijacking Attacks},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {23--43},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_2},
	doi = {10.1007/978-3-031-51482-1\_2},
	timestamp = {Thu, 18 Jan 2024 08:27:16 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/MaoXLHYZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Split learning of deep neural networks (SplitNN) has provided a promising solution to learning jointly for the mutual interest of a guest and a host, which may come from different backgrounds, holding features partitioned vertically. However, SplitNN creates a new attack surface for the adversarial participant. By investigating the adversarial effects of highly threatening attacks, including property inference, data reconstruction, and feature hijacking attacks, we identify the underlying vulnerability of SplitNN. To protect SplitNN, we design a privacy-preserving tunnel for information exchange. The intuition is to perturb the propagation of knowledge in each direction with a controllable unified solution. To this end, we propose a new activation function named R3eLU, transferring private smashed data and partial loss into randomized responses. We give the first attempt to secure split learning against three threatening attacks and present a fine-grained privacy budget allocation scheme. The analysis proves that our privacy-preserving SplitNN solution provides a tight privacy budget, while the experimental results show that our solution performs better than existing solutions in most cases and achieves a good tradeoff between defense and model usability.}
}


@inproceedings{DBLP:conf/esorics/RigakiG23,
	author = {Maria Rigaki and
                  Sebastian Garc{\'{\i}}a},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {The Power of {MEME:} Adversarial Malware Creation with Model-Based
                  Reinforcement Learning},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {44--64},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_3},
	doi = {10.1007/978-3-031-51482-1\_3},
	timestamp = {Fri, 26 Jan 2024 07:55:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/RigakiG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection toolchain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of evasion capabilities in almost all cases, producing evasive malware with an evasion rate in the range of 32–73%. It also produces surrogate models with a prediction label agreement with the respective target models between 97–99%. The surrogate could be used to fine-tune and improve the evasion rate in the future.}
}


@inproceedings{DBLP:conf/esorics/LeeCHBP23,
	author = {Younghan Lee and
                  Yungi Cho and
                  Woorim Han and
                  Ho Bae and
                  Yunheung Paek},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive
                  Models},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {65--84},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_4},
	doi = {10.1007/978-3-031-51482-1\_4},
	timestamp = {Tue, 07 May 2024 20:10:33 +0200},
	biburl = {https://dblp.org/rec/conf/esorics/LeeCHBP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets. Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance. However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients. Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system. However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID). In this work, we propose FLGuard, a novel byzantine-robust FL method that detects malicious clients and discards malicious local updates by utilizing the contrastive learning technique, which showed a tremendous improvement as a self-supervised learning method. With contrastive models, we design FLGuard as an ensemble scheme to maximize the defensive capability. We evaluate FLGuard extensively under various poisoning attacks and compare the accuracy of the global model with existing byzantine-robust FL methods. FLGuard outperforms the state-of-the-art defense methods in most cases and shows drastic improvement, especially in non-IID settings. https://github.com/201younghanlee/FLGuard.}
}


@inproceedings{DBLP:conf/esorics/HutherSBRE23,
	author = {Lorenz H{\"{u}}ther and
                  Karsten Sohr and
                  Bernhard J. Berger and
                  Hendrik Rothe and
                  Stefan Edelkamp},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Machine Learning for {SAST:} {A} Lightweight and Adaptable Approach},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {85--104},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_5},
	doi = {10.1007/978-3-031-51482-1\_5},
	timestamp = {Fri, 26 Jan 2024 07:55:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/HutherSBRE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we summarize a novel method for machine learning-based static application security testing (SAST), which was devised as part of a larger study funded by Germany’s Federal Office for Information Security (BSI). SAST describes the practice of applying static analysis techniques to program code on the premise of detecting security-critical software defects early during the development process. In the past, this was done by using rule-based approaches, where the program code is checked against a set of rules that define some pattern, representative of a defect. Recently, an increasing influx of publications can be observed that discuss the application of machine learning methods to this problem. Our method poses a lightweight approach to this concept, comprising two main contributions: Firstly, we present a novel control-flow based embedding method for program code. Embedding the code into a metric space is a necessity in order to apply machine learning techniques to the problem of SAST. Secondly, we describe how this method can be applied to generate expressive, yet simple, models of some unwanted behavior. We have implemented these methods in a prototype for the C and C++ programming languages. Using tenfold cross-validation, we show that our prototype is capable of effectively predicting the location and type of software defects in previously unseen code.}
}


@inproceedings{DBLP:conf/esorics/SulimanL23,
	author = {Mohamed Suliman and
                  Douglas J. Leith},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Two Models are Better Than One: Federated Learning is Not Private
                  for Google GBoard Next Word Prediction},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {105--122},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_6},
	doi = {10.1007/978-3-031-51482-1\_6},
	timestamp = {Fri, 26 Jan 2024 07:55:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/SulimanL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we present new attacks against federated learning when used to train natural language text models. We illustrate the effectiveness of the attacks against the next word prediction model used in Google’s GBoard app, a widely used mobile keyboard app that has been an early adopter of federated learning for production use. We demonstrate that the words a user types on their mobile handset, e.g. when sending text messages, can be recovered with high accuracy under a wide range of conditions and that counter-measures such a use of mini-batches and adding local noise are ineffective. We also show that the word order (and so the actual sentences typed) can be reconstructed with high fidelity. This raises obvious privacy concerns, particularly since GBoard is in production use.}
}


@inproceedings{DBLP:conf/esorics/YuXWL23,
	author = {Xi Yu and
                  Liyao Xiang and
                  Shiming Wang and
                  Chengnian Long},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Privacy-Preserving Split Learning via Pareto Optimal Search},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {123--142},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_7},
	doi = {10.1007/978-3-031-51482-1\_7},
	timestamp = {Fri, 26 Jan 2024 07:55:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/YuXWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of deep learning, it has become a trend for clients to perform split learning with an untrusted cloud server. The models are split into the client-end and server-end with features transmitted in between. However, features are typically vulnerable to attribute inference attacks to the input data. Most existing schemes target protecting data privacy at the inference, but not at the training stage. It remains a significant challenge to remove private information from the features while accomplishing the learning task with high utility. We found the fundamental issue is that utility and privacy are mostly conflicting tasks, which are hardly handled by the linear scalarization commonly used in previous works. Thus we resort to the multi-objective optimization (MOO) paradigm, seeking a Pareto optimal solution according to the utility and privacy objectives. The privacy objective is formulated by the mutual information between feature and sensitive attributes and is approximated by Gaussian models. In each training iteration, we select a direction that balances the dual goal of moving toward the Pareto Front and toward the users’ preference while keeping the privacy loss under the preset threshold. With a theoretical guarantee, the privacy of sensitive attributes is well preserved throughout training and at convergence. Experimental results on image and tabular datasets reveal our method is superior to the state-of-the-art in terms of utility and privacy.}
}


@inproceedings{DBLP:conf/esorics/KraussGD23,
	author = {Torsten Krau{\ss} and
                  Raphael G{\"{o}}tz and
                  Alexandra Dmitrienko},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Security of NVMe Offloaded Data in Large-Scale Machine Learning},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {143--163},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_8},
	doi = {10.1007/978-3-031-51482-1\_8},
	timestamp = {Sat, 10 Feb 2024 18:04:44 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/KraussGD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale machine learning (LSML) models, such as the GPT-3.5 that powers the well-known ChatGPT chatbot, have revolutionized our perception of AI by enabling more natural, context-aware, and interactive experiences. Yet, training such large models nowadays requires multiple months of computation on expensive hardware, including GPUs, orchestrated by specialized software, so-called LSML frameworks. Due to the model size, neither the on-device memory of GPUs nor the RAM is capable of holding all parameters simultaneously during training. Therefore, LSML frameworks dynamically offload data to NVMe storage and reload the information just in time. In this paper, we investigate the security of NVMe offloaded data in LSML against poisoning attacks and present NVMevade, the first untargeted poisoning attack on NVMe offloads. NVMevade allows the attacker to reduce the model performance, as well as slow down or even stall the training process. For instance, we demonstrate that an attacker can achieve a stealthy increase of 182% in training time, thus, inflating costs for model training. To address this vulnerability, we develop NVMensure, the first defense that guarantees the integrity and freshness of NVMe offloaded data in LSML. By conducting a large-scale study, we demonstrate the robustness of NVMensure against poisoning attacks and explore runtime efficiency and security trade-offs it can provide. We tested 22 different NVMensure configurations and report an overhead between 9.8% and 64.2%, depending on the selected security level. We also note that NVMensure is going to be effective against targeted poisoning attacks which do not exist yet but might be developed in the future.}
}


@inproceedings{DBLP:conf/esorics/RandoPH23,
	author = {Javier Rando and
                  Fernando P{\'{e}}rez{-}Cruz and
                  Briland Hitaj},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {PassGPT: Password Modeling and (Guided) Generation with Large Language
                  Models},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {164--183},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_9},
	doi = {10.1007/978-3-031-51482-1\_9},
	timestamp = {Fri, 26 Jan 2024 07:55:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/RandoPH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) successfully model natural language from vast amounts of text without the need for explicit supervision. In this paper, we investigate the efficacy of LLMs in modeling passwords. We present PassGPT, an LLM trained on password leaks for password generation. PassGPT outperforms existing methods based on generative adversarial networks (GAN) by guessing twice as many previously unseen passwords. Furthermore, we introduce the concept of guided password generation, where we leverage PassGPT sampling procedure to generate passwords matching arbitrary constraints, a feat lacking in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the entropy and probability distribution that PassGPT defines over passwords and discuss their use in enhancing existing password strength estimators.}
}


@inproceedings{DBLP:conf/esorics/SeidelEPRMY23,
	author = {Lukas Seidel and
                  Sedick David Baker Effendi and
                  Xavier Pinho and
                  Konrad Rieck and
                  Brink van der Merwe and
                  Fabian Yamaguchi},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Learning Type Inference for Enhanced Dataflow Analysis},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {184--203},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_10},
	doi = {10.1007/978-3-031-51482-1\_10},
	timestamp = {Sat, 10 Feb 2024 18:04:44 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/SeidelEPRMY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Statically analyzing dynamically-typed code is a challenging endeavor, as even seemingly trivial tasks such as determining the targets of procedure calls are non-trivial without knowing the types of objects at compile time. Addressing this challenge, gradual typing is increasingly added to dynamically-typed languages, a prominent example being TypeScript that introduces static typing to JavaScript. Gradual typing improves the developer’s ability to verify program behavior, contributing to robust, secure and debuggable programs. In practice, however, users only sparsely annotate types directly. At the same time, conventional type inference faces performance-related challenges as program size grows. Statistical techniques based on machine learning offer faster inference, but although recent approaches demonstrate overall improved accuracy, they still perform significantly worse on user-defined types than on the most common built-in types. Limiting their real-world usefulness even more, they rarely integrate with user-facing applications. We propose CodeTIDAL5, a Transformer-based model trained to reliably predict type annotations. For effective result retrieval and re-integration, we extract usage slices from a program’s code property graph. Comparing our approach against recent neural type inference systems, our model outperforms the current state-of-the-art by 7.85% on the ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore, we present JoernTI, an integration of our approach into Joern, an open source static analysis tool, and demonstrate that the analysis benefits from the additional type information. As our model allows for fast inference times even on commodity CPUs, making our system available through Joern leads to high accessibility and facilitates security research.}
}


@inproceedings{DBLP:conf/esorics/AharoniBBBDPPSSSV23,
	author = {Ehud Aharoni and
                  Moran Baruch and
                  Pradip Bose and
                  Alper Buyuktosunoglu and
                  Nir Drucker and
                  Subhankar Pal and
                  Tomer Pelleg and
                  Kanthi K. Sarpatwar and
                  Hayim Shaul and
                  Omri Soceanu and
                  Roman Vacul{\'{\i}}n},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Efficient Pruning for Machine Learning Under Homomorphic Encryption},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {204--225},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_11},
	doi = {10.1007/978-3-031-51482-1\_11},
	timestamp = {Thu, 18 Jan 2024 08:27:16 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/AharoniBBBDPPSSSV23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy-preserving machine learning (PPML) solutions are gaining widespread popularity. Among these, many rely on homomorphic encryption (HE) that offers confidentiality of the model and the data, but at the cost of large latency and memory requirements. Pruning neural network (NN) parameters improves latency and memory in plaintext ML but has little impact if directly applied to HE-based PPML. We introduce a framework called HE-PEx that comprises new pruning methods, on top of a packing technique called tile tensors, for reducing the latency and memory of PPML inference. HE-PEx uses permutations to prune additional ciphertexts, and expansion to recover inference loss. We demonstrate the effectiveness of our methods for pruning fully-connected and convolutional layers in NNs on PPML tasks, namely, image compression, denoising, and classification, with autoencoders, multilayer perceptrons (MLPs) and convolutional neural networks (CNNs). We implement and deploy our networks atop a framework called HElayers, which shows a 10–35% improvement in inference speed and a 17–35% decrease in memory requirement over the unpruned network, corresponding to 33–65% fewer ciphertexts, within a 2.5% degradation in inference accuracy over the unpruned network. Compared to the state-of-the-art pruning technique for PPML, our techniques generate networks with 70% fewer ciphertexts, on average, for the same degradation limit.\n}
}


@inproceedings{DBLP:conf/esorics/AliMKNHHNSSZYGR23,
	author = {Muaz Ali and
                  Muhammad Muzammil and
                  Faraz Karim and
                  Ayesha Naeem and
                  Rukhshan Haroon and
                  Muhammad Haris and
                  Huzaifah Nadeem and
                  Waseem Sabir and
                  Fahad Shaon and
                  Fareed Zaffar and
                  Vinod Yegneswaran and
                  Ashish Gehani and
                  Sazzadur Rahaman},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {SoK: {A} Tale of Reduction, Security, and Correctness - Evaluating
                  Program Debloating Paradigms and Their Compositions},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {229--249},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_12},
	doi = {10.1007/978-3-031-51482-1\_12},
	timestamp = {Thu, 18 Jan 2024 08:27:16 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/AliMKNHHNSSZYGR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automated software debloating of program source or binary code has tremendous potential to improve both application performance and security. Unfortunately, measuring and comparing the effectiveness of various debloating methods is challenging due to the absence of a universal benchmarking platform that can accommodate diverse approaches. In this paper, we first present \\({\\textsc {DebloatBench}}_{\\textrm{A}}\\) (Debloating benchmark for applications), an extensible and sustainable benchmarking platform that enables comparison of different research techniques. Then, we perform a holistic comparison of the techniques to assess the current progress. In the current version, we integrated four software debloating research tools: Chisel, Occam, Razor, and Piece-wise. Each tool is representative of a different class of debloaters: program source, compiler intermediate representation, executable binary, and external library. Our evaluation revealed interesting insights (i.e., hidden and explicit tradeoffs) about existing techniques, which might inspire future research. For example, all the binaries produced by Occam and Piece-Wise were correct, while Chisel significantly outperformed others in binary size and Gadget class reductions. In a first-of-its-kind composition, we also combined multiple debloaters to debloat a single binary. Our performance evaluation showed that, in both ASLR-proof and Turing-complete gadget expressively cases, several compositions (e.g., Chisel-Occam, Chisel-Occam-Razor) significantly outperformed the best-performing single tool (i.e., Chisel).}
}


@inproceedings{DBLP:conf/esorics/ZhouXHHG23,
	author = {Ziyi Zhou and
                  Xuangan Xiao and
                  Tianxiao Hou and
                  Yikun Hu and
                  Dawu Gu},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {On the (In)Security of Manufacturer-Provided Remote Attestation Frameworks
                  in Android},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {250--270},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_13},
	doi = {10.1007/978-3-031-51482-1\_13},
	timestamp = {Thu, 18 Jan 2024 08:27:16 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/ZhouXHHG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To provide a tamper-proof mechanism for mobile apps to check the integrity of the device and their own code/data, Android phone manufacturers have introduced Manufacturer-provided Android Remote Attestation (MARA) frameworks. The MARA framework helps an app conduct a series of integrity checks, signs the check results, and sends them to remote servers for a remote attestation. Nonetheless, we observe that real-world MARA frameworks often adopt two implementations of integrity check (hardware-based and software-based) for compatibility consideration, and this allows an attacker to easily conduct a downgrade attack to force the app to utilize the software-based integrity check and forge checking results, even if the Android device is able to employ hardware-supported remote attestation securely. We demonstrate our MARA bypass approach against MARA frameworks (i.e., Google SafetyNet and Huawei SafetyDetect) on real Android devices, and design an automated measurement pipeline to analyze 35,245 popular Android apps, successfully attacking all 104 apps that use these MARA services, including well-known apps and games such as TikTok Lite, Huawei Wallet, and Pokémon GO. Our study reveals the significant risks against MARA frameworks in use.}
}


@inproceedings{DBLP:conf/esorics/ChenQD23,
	author = {Zhe Chen and
                  Haiqing Qiu and
                  Xuhua Ding},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {DScope: To Reliably and Securely Acquire Live Data from Kernel-Compromised
                  {ARM} Devices},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {271--289},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_14},
	doi = {10.1007/978-3-031-51482-1\_14},
	timestamp = {Thu, 18 Jan 2024 08:27:16 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/ChenQD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Live data acquisition from a mobile device controlled by a corrupted kernel is challenging as the adversary can block data reporting from the inside and also sabotage external I/O interactions. This paper proposes DScope as a reliable live data acquisition system for ARM devices without trusting their kernels. It ensures that a device user can always launch DScope to securely extract the needed virtual memory data when the device is under attack. Besides its reliability, DScope also preserves kernel semantic and support user-customized acquisition routines. We have built a prototype of DScope on a Raspberry Pi 4 development board and have also tested DScope ’s reliability against various forms of denial of service attacks. Our experiments show that a user can dynamically import data acquisition routines to the device to extract kernel objects and runtime stacks from an attack scene or a kernel crashing site.}
}


@inproceedings{DBLP:conf/esorics/FarrellyQKCR23,
	author = {Guy Farrelly and
                  Paul Quirk and
                  Salil S. Kanhere and
                  Seyit Camtepe and
                  Damith C. Ranasinghe},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {SplITS: Split Input-to-State Mapping for Effective Firmware Fuzzing},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {290--310},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_15},
	doi = {10.1007/978-3-031-51482-1\_15},
	timestamp = {Thu, 18 Jan 2024 08:27:16 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/FarrellyQKCR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ability to test firmware on embedded devices is critical to discovering vulnerabilities prior to their adversarial exploitation. State-of-the-art automated testing methods rehost firmware in emulators and attempt to facilitate inputs from a diversity of methods (interrupt driven, status polling) and a plethora of devices (such as modems and GPS units). Despite recent progress to tackle peripheral input generation challenges in rehosting, a firmware’s expectation of multi-byte magic values supplied from peripheral inputs for string operations still pose a significant roadblock. We solve the impediment posed by multi-byte magic strings in monolithic firmware. We propose feedback mechanisms for input-to-state mapping and retaining seeds for targeted replacement mutations with an efficient method to solve multi-byte comparisons. The feedback allows an efficient search over a combinatorial solution-space. We evaluate our prototype implementation, SplITS, with a diverse set of 21 real-world monolithic firmware binaries used in prior works, and 3 new binaries from popular open source projects. SplITS automatically solves 497% more multi-byte magic strings guarding further execution to uncover new code and bugs compared to state-of-the-art. In 11 of the 12 real-world firmware binaries with string comparisons, including those extensively analyzed by prior works, SplITS outperformed, statistically significantly. We observed up to 161% increase in blocks covered and discovered 6 new bugs that remained guarded by string comparisons. Significantly, deep and difficult to reproduce bugs guarded by comparisons, identified in prior work, were found consistently.}
}


@inproceedings{DBLP:conf/esorics/DeganiSMC23,
	author = {Luca Degani and
                  Majid Salehi and
                  Fabio Martinelli and
                  Bruno Crispo},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {{\(\mu\)}IPS: Software-Based Intrusion Prevention for Bare-Metal Embedded
                  Systems},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {311--331},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_16},
	doi = {10.1007/978-3-031-51482-1\_16},
	timestamp = {Thu, 18 Jan 2024 08:27:16 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/DeganiSMC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many embedded systems are low-cost bare-metal systems where the firmware executes directly on hardware without an OS. Bare-metal systems typically lack many security primitives, including the well-known Address Space Layout Randomization (ASLR) and Data Execution Prevention (DEP), and their integrity can be compromised using a single vulnerability. Proposed defenses have not yet been deployed due to their requirements for firmware source code availability or hardware modifications. We present \\(\\mu \\)IPS, the first Intrusion Prevention System (IPS) for bare-metal systems that requires no modification to the hardware and can be applied to stripped binaries without access to the source code. \\(\\mu \\)IPS enforces fine-grained control-flow protection targeting both forward and backward edges. To achieve that, \\(\\mu \\)IPS introduces a novel Trusted Execution Environment (TEE) to provide memory isolation at runtime while handling the hardware limitations of bare-metal systems. \\(\\mu \\)IPS also provides Remote Integrity Check (RIC) mechanism to validate the integrity of control-flow protection policies and the TEE code, and secure Over-The-Air (OTA) update mechanism to deploy the updated policies. We evaluate \\(\\mu \\)IPS against ten real-world representative firmware. \\(\\mu \\)IPS imposes a \\(31\\%\\) execution overhead on average on binary instrumented firmware. \\(\\mu \\)IPS reduces exposure to Return-Oriented Programming (ROP) attacks by \\(99\\%\\).}
}


@inproceedings{DBLP:conf/esorics/ZarbafianG23,
	author = {Pouriya Zarbafian and
                  Vincent Gramoli},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Aion: Secure Transaction Ordering Using TEEs},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {332--350},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_17},
	doi = {10.1007/978-3-031-51482-1\_17},
	timestamp = {Thu, 18 Jan 2024 08:27:16 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/ZarbafianG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In state machine replication (SMR), preventing reordering attacks by ensuring a high degree of fairness when ordering commands requires that clients broadcast their commands to all processes. This is impractical due to the impact on scalability, and thus it discourages the adoption of a fair ordering of commands. Alternative approaches to order-fairness allow clients do send their commands to only one process, but provide a weaker notion of order-fairness. In particular, they disadvantage isolated processes. In this paper, we introduce Aion, a set of order-fair protocols for SMR. We first leverage trusted execution environments (TEEs) to enable processes to compute the times when commands are broadcast by their issuers. We then integrate this information into existing consensus protocols to devise order-fair SMR protocols that are both leader-based and leaderless. To realize order-fairness, Aion only requires that a client sends its commands to a single process, while at the same time enabling precise ordering during synchronous periods.}
}


@inproceedings{DBLP:conf/esorics/ArfaouiJLOR23,
	author = {Ghada Arfaoui and
                  Thibaut Jacques and
                  Marc Lacoste and
                  Cristina Onete and
                  L{\'{e}}o Robert},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Towards a Privacy-Preserving Attestation for Virtualized Networks},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {351--370},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_18},
	doi = {10.1007/978-3-031-51482-1\_18},
	timestamp = {Thu, 18 Jan 2024 08:27:16 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/ArfaouiJLOR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {TPM remote attestation allows to verify the integrity of the boot sequence of a remote device. Deep Attestation extends that concept to virtualized platforms by allowing to attest virtual components, the hypervisor, and the link between them. In multi-tenant environments, deep attestation solution offer security and/or efficiency, but no privacy. In this paper, we propose a privacy preserving TPM-based deep attestation solution in multi-tenant environments, which provably guarantees: (i) Inter-tenant privacy: a tenant is cannot know whether other VMs outside its own are hosted on the same machine; (ii) Configuration hiding: the hypervisor’s configuration, used during attestation, remains hidden from the tenants; and (iii) Layer linking: tenants can link hypervisors with the VMs, thus obtaining a guarantee that the VMs are running on specific hardware. We also implement our scheme and show that it is efficient despite the use of complex cryptographic tools.}
}


@inproceedings{DBLP:conf/esorics/GuoHTT23,
	author = {Yuejun Guo and
                  Qiang Hu and
                  Qiang Tang and
                  Yves Le Traon},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {An Empirical Study of the Imbalance Issue in Software Vulnerability
                  Detection},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {371--390},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_19},
	doi = {10.1007/978-3-031-51482-1\_19},
	timestamp = {Fri, 26 Jan 2024 07:55:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/GuoHTT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.}
}


@inproceedings{DBLP:conf/esorics/SunGWZ23,
	author = {Rui Sun and
                  Yinggang Guo and
                  Zicheng Wang and
                  Qingkai Zeng},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {AttnCall: Refining Indirect Call Targets in Binaries with Attention},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {391--409},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_20},
	doi = {10.1007/978-3-031-51482-1\_20},
	timestamp = {Sat, 10 Feb 2024 18:04:44 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/SunGWZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate Control Flow Graphs are crucial for effective binary program analysis, while solving indirect function call targets is its major challenge. Existing static analysis methods heavily rely on domain-specific patterns, resulting in an abundance of false positive edges due to limited expert knowledge. Concurrently, learning-based approaches often depend on heuristic analysis during the code representation stage, which prevents the model from fully comprehending program semantics. To address these limitations, this paper presents AttnCall, a novel neural network learning framework that leverages the attention mechanism to automatically learn the matching relationship between function callsites and callees’ context semantics. AttnCall refines the identification of indirect call targets through the learned matching patterns, eliminating the drawbacks of existing techniques. Additionally, we propose an end-to-end code representation scheme that effectively embeds the semantics of callsites and callees without relying on heuristic rules. The evaluation of AttnCall focuses on the task of predicting indirect function call targets. The results demonstrate that AttnCall surpasses state-of-the-art approaches, achieving 31.4% higher precision and 5% higher recall. Moreover, AttnCall enhances model interpretability, allowing for a better understanding of the underlying analysis process.}
}


@inproceedings{DBLP:conf/esorics/KwashieKKJCN23,
	author = {Selasi Kwashie and
                  Wei Kang and
                  Sandeep Santhosh Kumar and
                  Geoff Jarrad and
                  Seyit Camtepe and
                  Surya Nepal},
	editor = {Gene Tsudik and
                  Mauro Conti and
                  Kaitai Liang and
                  Georgios Smaragdakis},
	title = {Acumen: Analysing the Impact of Organisational Change on Users'
                  Access Entitlements},
	booktitle = {Computer Security - {ESORICS} 2023 - 28th European Symposium on Research
                  in Computer Security, The Hague, The Netherlands, September 25-29,
                  2023, Proceedings, Part {IV}},
	series = {Lecture Notes in Computer Science},
	volume = {14347},
	pages = {410--430},
	publisher = {Springer},
	year = {2023},
	url = {https://doi.org/10.1007/978-3-031-51482-1\_21},
	doi = {10.1007/978-3-031-51482-1\_21},
	timestamp = {Fri, 26 Jan 2024 07:55:28 +0100},
	biburl = {https://dblp.org/rec/conf/esorics/KwashieKKJCN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Planned organisational changes are frequent occurrences in large enterprises due to the dynamicity of employees’ roles, evolution of teams, units and divisions as a result of mergers, demergers, and general restructuring. To safeguard system security and employees’ productivity, it is paramount for system administrators to keep track and remediate all users’ changing access needs. This paper studies the impact of (planned) organisational changes on the access privileges of employees in line with access control policies. Our solution, Acumen, uses binary decision diagrams (BDDs) to encode XACML policies via a Boolean function conversion, and performs semantic interpretation of organisational changes for analysis over the BDDs. The BDD structure is versatile, enabling succinct representation as well as effective and efficient symbolic operations and visualisation. We demonstrate the efficacy of Acumen with two data sets via a series of case studies on: a) a commonly used benchmark access control policy data in the literature; and b) a proprietary data set containing planned organisational changes in a large real-world financial institution with a dynamic business environment. The empirically results show Acumen to be effective and efficient.}
}
