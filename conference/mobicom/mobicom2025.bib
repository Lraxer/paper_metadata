@inproceedings{DBLP:conf/mobicom/0001SK25,
	author = {Junbo Zhang and
                  Yiwen Song and
                  Swarun Kumar},
	title = {PolarVisor: Clutter-free, Electronics-free Fiducial Markers for mmWave
                  Radars Printed on Paper},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1--16},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723456},
	doi = {10.1145/3680207.3723456},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0001SK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we design and fabricate cost-effective, electronics-free millimeter-wave (mmWave) fiducial markers that support clutter-free detection on radars. Fiducial markers are widely used in camera-based systems to provide spatial information in robotic navigation applications. QR-code-like fiducial markers can be readily printed on paper - low-cost, easy to produce, and electronics-free. Yet, an analogous mmWave solution is yet to appear, which stems from the unique challenge in the mmWave context: its vulnerability to clutter. Existing solutions either trade hardware simplicity for clutter resilience or stay simple but remain vulnerable to environmental multipath. This paper seeks to solve this dilemma. We introduce PolarVisor, where we imprint mmWave fiducial markers with paper and foils. Its key insight is manipulating signal polarization to enable clutter-free detection on its markers. Meanwhile, it builds on recent advances in passive metasurfaces where metallic patterns are readily hot-stamped on paper with foils. PolarVisor adopts an innovative design where the radar wears a visor (also paper-based) to eliminate clutter, leaving the markers readily visible. We include an in-depth analysis of its design and fabrication and we show that: (1) PolarVisor can support clutter-free detection at up to 25 m. (2) PolarVisor reports a self-localization accuracy of 0.0297 m (mean) and 0.0232 m (median) across multiple real-world, multipath-rich indoor environments.}
}


@inproceedings{DBLP:conf/mobicom/SongZM025,
	author = {Denghui Song and
                  Anfu Zhou and
                  Huadong Ma and
                  Jie Xiong},
	title = {MoleSen: From Macro Sensing to Micro Molecular-level Taste Sensing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {17--32},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723457},
	doi = {10.1145/3680207.3723457},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/SongZM025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Taste perception plays an essential role in promoting human health and maintaining nutritional balance. Current taste perception techniques usually require expensive equipment and delicate storage conditions, which restrict their use primarily to laboratory settings. In this paper, we show that terahertz (THz) signals generate unique fingerprint spectra when interacting with different taste molecules in aqueous solutions. Building on this finding, we propose Molecular-level Taste Sensing ( MoleSen ), a contact-free gustatory sensing method aimed at achieving wireless human-like perception. Specifically,  MoleSen  emits terahertz signals towards the aqueous solution, captures the reflected signals, and then determines the type and concentration of the tastes by analyzing the unique fingerprint spectra of the reflected signals influenced by the taste molecules. In  MoleSen , we design a bio-inspired deep learning model ( DTB , Digital Taste Bud) to identify the subtle taste features diluted by water molecules. Additionally, we incorporate domain adaptive learning to address the issue of feature distribution shifts when multiple tastes are mixed. Through extensive experiments involving over 247,000 samples, we demonstrate that  MoleSen  can accurately differentiate the five basic tastes—sour, bitter, salty, sweet, and umami—with an accuracy of 98.5% for taste type determination and 96.9% for concentration detection. Moreover,  MoleSen  outperforms the human's taste sensitivity and achieves a highly accurate perception even for mixed tastes.}
}


@inproceedings{DBLP:conf/mobicom/0001BHX25,
	author = {Xuan Huang and
                  Chen Bian and
                  Jun Huang and
                  Guoliang Xing},
	title = {Towards Intelligent LiDAR with Adaptive Focus},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {33--46},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723458},
	doi = {10.1145/3680207.3723458},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0001BHX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the adoption of LiDAR expands across various fields such as autonomous driving, robotics, and smart cities, the demand for adaptive scanning capabilities to better capture dynamic and complex scenes becomes paramount. Current LiDAR technologies, limited by fixed uniform scan patterns, struggle to prioritize critical areas, resulting in reduced perception accuracy and performance inefficiencies. This paper introduces SmartLiDAR, an advanced LiDAR system that enhances scanning efficiency and performance by adaptively optimizing scan focus through an intelligent, software-defined micro-mirror controller. Unlike traditional systems, SmartLiDAR dynamically adjusts its scan pattern based on environmental characteristics and application-specific requirements, concentrating sample points on key objects without increasing power consumption or scan time. SmartLiDAR achieves this by integrating a novel quadratic micro-mirror controller, an adaptive algorithm for generating fine-grained attention map with prioritized scan focus, and a carefully designed optimization algorithm that maps attention maps to practical scanning patterns. We prototype SmartLiDAR by building a software-defined LiDAR using commercially available optical components and FPGA. Our experimental results demonstrate that SmartLiDAR significantly enhances resolution in regions of interest by 3x and increases average object detection precision by up to 16.11%. Additionally, SmartLiDAR maintains negligible extra energy consumption and processing latency, making it suitable for real-time applications, such as autonomous vehicles.}
}


@inproceedings{DBLP:conf/mobicom/Hu0G025,
	author = {Jingzhi Hu and
                  Xin Li and
                  Jin Gan and
                  Jun Luo},
	title = {Poison to Cure: Privacy-preserving Wi-Fi Multi-User Sensing via Data
                  Poisoning},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {47--62},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723459},
	doi = {10.1145/3680207.3723459},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Hu0G025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wi-Fi human sensing, boosted by latest progress in both system innovation and deep analytics, has demonstrated ever-increasing resolution of users' activities. Nonetheless, it may become a spy on users' private activities such as password entry or intimate social interactions. Existing countermeasures include signal obfuscation and adversarial perturbations to hamper and confuse Wi-Fi sensing, yet they both require substantial changes in Wi-Fi hardware/firmware, and they at most stay at  user level  in protection granularity. This paper presents  Poison2Cure , the first  semantic-level  privacy-preserving framework for Wi-Fi human sensing systems, with full compatibility to any underlying hardware. The innovation behind Poison2Cure lies in feeding poisoned training data from (privacy-sensitive) users to the neural model for Wi-Fi sensing, degrading only the sensing for  private  activities while retaining that for  regular  ones. Moreover, we tackle the harsh conditions where the neural model is kept confidential and/or preceded by data cleansing. Our extensive evaluations demonstrate that Poison2Cure reduces over 76% of the accuracy for the private activities while keeping the accuracy for regular activities largely intact.}
}


@inproceedings{DBLP:conf/mobicom/LiTLLSZ025,
	author = {Shengyu Li and
                  Mengchen Teng and
                  Boyu Li and
                  Songfan Li and
                  Xiandong Shao and
                  Chong Zhang and
                  Li Lu},
	title = {Hedgehog: Pushing the Range Limits of Ultrasonic Microphone Jammers},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {63--77},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723460},
	doi = {10.1145/3680207.3723460},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiTLLSZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultrasonic microphone jammers (UMJs) use ultrasonic waves to interfere with concealed microphone recorders, offering significant values in confidential meetings and secret talks. Existing UMJs, however, performs in a short range of typically 2 m, which remains a huge gap to practical applications. The key challenge lies in the fact that the ultrasonic signal will produce audible sounds during its transmission, caused by nonlinear distortion of power amplifiers and loudspeakers in the transmission chain of the UMJ. In this paper, we propose Hedgehog, a room-scale ultrasonic jammer design, which enhances the jamming range through two key methods. First, it corrects nonlinear distortion by modeling the transmission chain and applying digital pre-distortion. Furthermore, it redesigns the jamming signal to improve its effectiveness by not only reducing the signal-to-noise ratio (SNR) but also by suppressing the semantic information in human speech. The experimental results show that Hedgehog achieved a word error rate (WER) of over 95% at 8 meters and over 80% at 10 meters, which is a 4 times increase in jamming distance compared to existing solutions.}
}


@inproceedings{DBLP:conf/mobicom/LiSSCZR025,
	author = {Yilong Li and
                  Ramanujan K. Sheshadri and
                  Karthik Sundaresan and
                  Eugene Chai and
                  Yijing Zeng and
                  Jayaram Raghuram and
                  Suman Banerjee},
	title = {Medusa: Scalable Multi-View Biometric Sensing in the Wild with Distributed
                  {MIMO} Radars},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {78--92},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723461},
	doi = {10.1145/3680207.3723461},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiSSCZR025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio frequency (RF) techniques have shown promise for continuous contactless healthcare applications. However, real-world indoor environments pose challenges for existing systems, which may struggle to detect subtle physiological signals. This paper proposes Medusa, a novel wireless vital-sign sensing system designed for multi-view setups. It enables users to deploy distributed Multiple Input Multiple Output (MIMO) arrays into their daily living environments, facilitating vital-sign sensing in real-world settings. Unlike most existing single Commercial Off-The-Shelf (COTS) radar-based systems that operate under controlled settings Medusa's primary novelty lies in the design of a first-of-its-kind flexible  multi-view  vital sign sensing system that is view-agnostic, pose-agnostic, contactless, and can sense basic human vitals with good accuracy. Through our well-engineered hardware and software co-design, Medusa enables real-time processing of large distributed MIMO arrays, while balancing the tradeoff between Signal-to-Noise Ratio (SNR) and spatial diversity gain across each of its four distributed 4 × 4 sub-arrays for increased robustness. This is achieved using our  novel unsupervised learning  model which effectively recovers vital sign waveforms by decomposing the received signals. Extensive evaluations with 21 participants demonstrate Medusa's spatial diversity gain for real-world vital-sign monitoring, enabling free movement and orientation of subjects in both familiar and unfamiliar indoor environments.}
}


@inproceedings{DBLP:conf/mobicom/Na0ZGZ0S0025,
	author = {Xin Na and
                  Jia Zhang and
                  Jiacheng Zhang and
                  Xiuzhen Guo and
                  Yang Zou and
                  Meng Jin and
                  Yimiao Sun and
                  Yunhao Liu and
                  Yuan He},
	title = {QuinID: Enabling FDMA-Based Fully Parallel {RFID} with Frequency-Selective
                  Antenna},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {93--107},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723462},
	doi = {10.1145/3680207.3723462},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Na0ZGZ0S0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parallelizing passive Radio Frequency Identification (RFID) reading is an arguably crucial, yet unsolved challenge in modern IoT applications. Existing approaches remain limited to time-division operations and fail to read multiple tags simultaneously. In this paper, we introduce QuinID, the first frequency-division multiple access (FDMA) RFID system to achieve fully parallel reading. We innovatively exploit the frequency selectivity of the tag antenna rather than a conventional digital FDMA, bypassing the power and circuitry constraint of RFID tags. Specifically, we delicately design the frequency-selective antenna based on surface acoustic wave (SAW) components to achieve extreme narrow-band response, so that QuinID tags (i.e., QuinTags) operate exclusively within their designated frequency bands. By carefully designing the matching network and canceling various interference, a customized QuinReader communicates simultaneously with multiple QuinTags across distinct bands. QuinID maintains high compatibility with commercial RFID systems and presents a tag cost of less than 10 cents. We implement a 5-band QuinID system and evaluate its performance under various settings. The results demonstrate a fivefold increase in read rate, reaching up to 5000 reads per second.}
}


@inproceedings{DBLP:conf/mobicom/ZhaoW0KY0LHC25,
	author = {Yimin Zhao and
                  Weibo Wang and
                  Xiong Wang and
                  Linghe Kong and
                  Jiadi Yu and
                  Yifei Zhu and
                  Shiyuan Li and
                  Chong He and
                  Guihai Chen},
	title = {B2LoRa: Boosting LoRa Transmission for Satellite-IoT Systems with
                  Blind Coherent Combining},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {108--122},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723463},
	doi = {10.1145/3680207.3723463},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhaoW0KY0LHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of Low Earth Orbit (LEO) satellite networks, satellite-IoT systems using the LoRa technique have been increasingly deployed to provide widespread Internet services to low-power and low-cost ground devices. However, the long transmission distance and adverse environments from IoT satellites to ground devices pose a huge challenge to link reliability, as evidenced by the measurement results based on our real-world setup. In this paper, we propose a blind coherent combining design named B 2 LoRa to boost LoRa transmission performance. The intuition behind B 2 LoRa is to leverage the repeated broadcasting mechanism inherent in satellite-IoT systems to achieve coherent combining under the low-power and low-cost constraints, where each re-transmission at different times is regarded as the same packet transmitted from different antenna elements within an antenna array. Then, the problem is translated into aligning these packets at a fine granularity despite the time, frequency, and phase offsets between packets in the case of frequent packet loss. To overcome this challenge, we present three designs — joint packet sniffing, frequency shift alignment, and phase drift mitigation to deal with ultra-low SNRs and Doppler shifts featured in satellite-IoT systems, respectively. Finally, experiment results based on our real-world deployments demonstrate the high efficiency of B 2 LoRa.}
}


@inproceedings{DBLP:conf/mobicom/KhooiKC25,
	author = {Xin Zhe Khooi and
                  Anuj Kalia and
                  Mun Choon Chan},
	title = {How to Update Your 5G vRAN},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {123--138},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723464},
	doi = {10.1145/3680207.3723464},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/KhooiKC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ongoing virtualization of Radio Access Networks (vRANs) promises increased velocity for updating RAN software with new features and bug fixes. To support this, we developed SwapRAN, a live update system for both vRAN software components: the Distributed Unit (DU) and the Centralized Unit (CU). Unlike previous systems, SwapRAN operates in-place without requiring additional hardware like staging servers or a programmable switch. For DU updates, SwapRAN presents two techniques: (1) using OS thread priorities to safely initialize the new DU while overlapping with the old DU which is active, and (2) using the network interface card's embedded switch to redirect fronthaul traffic to the new DU. For CU updates, SwapRAN is the first working live update system, which we achieve by (1) decoupling the stateful CU-DU connection and transparently rerouting DU messages to the new CU, and (2) repurposing existing midhaul control plane messages to move users to the new CU. We evaluate SwapRAN on real 5G testbeds and demonstrate its practical deployability via integration with Kubernetes. Our evaluations show that SwapRAN completes DU or CU updates with just 1–2 seconds of user downtime.}
}


@inproceedings{DBLP:conf/mobicom/DongWLKHZ025,
	author = {Huixin Dong and
                  Yijie Wu and
                  Feiyu Li and
                  Wei Kuang and
                  Yuan He and
                  Qian Zhang and
                  Wei Wang},
	title = {PassiveBLE: Towards Fully Commodity Compatible {BLE} Backscatter},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {139--153},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723465},
	doi = {10.1145/3680207.3723465},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/DongWLKHZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth Low Energy (BLE) backscatter is a promising candidate for battery-free Internet of Things (IoT) applications. We propose PassiveBLE, a backscatter system that can establish authentic and fully compatible BLE connections on data channels. The key enabling techniques include (i) a synchronization circuit that can wake up tags and activate backscatter communications with symbol-level accuracy to facilitate BLE data packet generation; (ii) a distributed coding scheme that offloads the major encoding and processing burdens from tags to the excitation source while achieving high throughput; (iii) a BLE connection scheduler to enable fully compatible BLE connection interactions, including connection establishment, maintenance and termination for multiple backscatter tags. We prototype PassiveBLE tags with off-the-shelf components and also conduct comprehensive experiments to evaluate the performance. Results demonstrate that PassiveBLE achieves a success rate of over 99.9% in establishing commodity BLE connections. PassiveBLE also achieves commodity-compatible BLE communication with a high goodput of up to 974 kbps in  LE 2M PHY  mode and 532 kbps in LE  1M PHY  mode, which is about 63.3× higher than existing commodity-level BLE backscatter system in the same mode.}
}


@inproceedings{DBLP:conf/mobicom/Zhang0D00CWYMX25,
	author = {Runting Zhang and
                  Yijie Li and
                  Dian Ding and
                  Yi{-}Chao Chen and
                  Yida Wang and
                  Dongyao Chen and
                  Jingxian Wang and
                  Jiadi Yu and
                  Ling Ma and
                  Guangtao Xue},
	title = {Bridge: Enabling {BLE} Direction Finding Feature Compatible with All
                  Bluetooth Devices},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {154--169},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723466},
	doi = {10.1145/3680207.3723466},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Zhang0D00CWYMX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth-based location services have experienced significant growth over the past decades. RSSI-based techniques using beacons only provide meters-level accuracy. Angular-based approaches rely on customized antenna arrays, introducing high costs and limited usability. In 2020, Bluetooth Special Interest Group (Bluetooth SIG) released version 5.1, integrating Angle of Arrival (AoA) estimation to enable direction finding capabilities, which has the potential to improve localization across various fields, including logistics and industry. However, more than 4.1 billion devices (68% of the total) still do not support the direction finding feature. To address this issue and ensure backward compatibility, we proposed Bridge, a solution that leverages an additional trigger node (referred to as Trigger) to make the direction finding feature compatible with all Bluetooth devices without requiring modifications to existing hardware or firmware. The Trigger mimics communication behaviors with both locators and targets simultaneously by sending a nesting packet. Subsequently, processes and algorithms are delicately designed to estimate AoA. Bridge also supports large-scale deployment through dynamic packet flow switching, enabling it to handle concurrent targets and manage handover with a consistent operation pattern. We implemented and evaluated Bridge in real-world scenarios. The system achieved an average localization error of 33.4 cm  while extending the direction-finding feature to 10 target devices of different Bluetooth versions, indicating the effectiveness of Bridge.}
}


@inproceedings{DBLP:conf/mobicom/LaiZZ25,
	author = {Haowen Lai and
                  Zhiwei Zheng and
                  Mingmin Zhao},
	title = {RF-Based 3D {SLAM} Rivaling Vision Approaches},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {170--185},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723467},
	doi = {10.1145/3680207.3723467},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LaiZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents CartoRadar, a novel RF-based SLAM system that delivers high-fidelity 3D mapping with centimeter-level accuracy. CartoRadar builds on top of the advancements in learning-based RF imaging. However, learning-based systems often exhibit variation in prediction accuracy during inference. To address this challenge and enable robust RF sensing, CartoRadar introduces a novel, training-free uncertainty quantification method tailored to RF signals. Additionally, CartoRadar features an efficient SLAM algorithm that incorporates this uncertainty into the mapping process. We deploy CartoRadar on a mobile robot and conduct extensive evaluations across 14 floors in 5 buildings. Results show that CartoRadar achieves a trajectory error of 14.1 cm, outperforming camera-based baselines by 72.1%. For mapping, CartoRadar achieves an accuracy of 7.4 cm and a completion of 8.1 cm, improving over vision methods by 46.2% and 67.6%, respectively. Code, datasets, and demo videos are available on our website.}
}


@inproceedings{DBLP:conf/mobicom/DingJZCFZFTK25,
	author = {Rong Ding and
                  Haiming Jin and
                  Ningzhi Zhu and
                  Zijie Chen and
                  Yi Fu and
                  Fengyuan Zhu and
                  Guiyun Fan and
                  Xiaohua Tian and
                  Linghe Kong},
	title = {Bluetooth-Enabled Transparent {RF} Sensing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {186--200},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723468},
	doi = {10.1145/3680207.3723468},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/DingJZCFZFTK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents Serafin, the first full-stack, sub- m W, and versatile Bluetooth-enabled RF sensor that brings transparent RF sensing to any mobile and IoT device: it independently conducts the whole RF sensing process from RF signal reception to sensing result computation in a wide variety of sensing tasks with only negligible power consumption. At the core of Serafin are our two designs that address the challenge posed by the stringent sub-mW power constraint to jointly achieving versatility and full stackness. Specifically, (i) we utilize the ambient Bluetooth advertising signal as the signal for sensing, and extract the phase difference of the sensing signals received by each antenna pair from the amplitude of their sum signal, which avoids power-hungry hardware components and intensive computation, and (ii) we employ low-power MCU as the computation hardware, and suppress its power consumption by activating it adaptively only when necessary and customizing a light-weight neural network model that still ensures satisfactory inference accuracy. Our extensive experiments on 6 representative sensing tasks show that Serafin achieves  competitive sensing performance , but consumes only around  500–900 μ W  power, which is  3–4 orders of magnitude lower  than those of existing full-stack and versatile counterparts.}
}


@inproceedings{DBLP:conf/mobicom/TanL0PT25,
	author = {Sixu Tan and
                  Zeyu Li and
                  Zhutian Liu and
                  Harsh Patel and
                  Zhaowei Tan},
	title = {Automated Model-Based Fuzzing for 5G {O-RAN}},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {201--215},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723469},
	doi = {10.1145/3680207.3723469},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/TanL0PT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The evolution of 5G and future-G technologies has led to the development of an open, distributed, and multi-vendor architecture known as Open Radio Access Network (O-RAN). While O-RAN offers greater flexibility and stimulates innovation, it also introduces potential issues such as bugs, inconsistencies, and vulnerabilities. In this paper, we present a novel model-based fuzzing system, ARCANE, to address these challenges. Unlike existing fuzzing techniques that struggle with the complexity and dynamism of O-RAN, ARCANE distinguishes itself by employing a smart, model-based approach. It first analyzes O-RAN specifications using a Large Language Model (LLM) and incorporates a unique method that integrates passive model learning to refine the model. With this refined model, ARCANE designs an O-RAN-aware, model-based fuzzing scheme that maintains high efficiency by adhering to O-RAN's specific semantics and syntax. We implement ARCANE and evaluate it on the OAI5G. It finds 9 root bugs from three categories, all having serious security implications. ARCANE has shown superior efficiency and coverage compared to state-of-the-arts.}
}


@inproceedings{DBLP:conf/mobicom/LiLLL0025,
	author = {Yeming Li and
                  Hailong Lin and
                  Renjie Li and
                  Jiamei Lv and
                  Yi Gao and
                  Wei Dong},
	title = {BoRa: LoRa over {BLE}},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {216--230},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723470},
	doi = {10.1145/3680207.3723470},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiLLL0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth Low Energy (BLE) and LoRa are two dominant wireless protocols for the Internet of Things (IoT), each built with specific design goals, rendering them non-interoperable. Cross-Technology Communication (CTC) enables heterogeneous communication between BLE and LoRa. Despite these efforts, existing works suffer from low throughput and short communication range. In this paper, we present a novel system named  BoRa , which enables bi-directional communication between the COTS BLE and COTS LoRa chips. The key idea of  BoRa  to emulate the linear frequency changes of the LoRa chirp is continuously tuning its Modulation Offset (MO), which is originally designed to compensate for the Carrier Frequency Offset (CFO) in BLE chips.  BoRa  can be readily run on COTS chips without any hardware modifications. We implement and evaluate  BoRa  with COTS BLE and LoRa chips. The results show that  BoRa  enables bi-directional cross-technology communication between COTS BLE and COTS LoRa chips, and long-range communication between two COTS BLE chips. It achieves up to 1500m communication range while ensuring 10 Kb/s throughput in outdoor Line-Of-Sight (LOS) scenarios. Besides,  BoRa  has up to 18.54 Kb/s higher throughput and up to 98.45% lower energy consumption compared with state-of-the-art methods.}
}


@inproceedings{DBLP:conf/mobicom/GongZX00F025,
	author = {Danei Gong and
                  Naiyu Zheng and
                  Binbin Xie and
                  Jie Xiong and
                  Shuai Wang and
                  Yuguang Fang and
                  Zhimeng Yin},
	title = {SeRadar: Embracing Secondary Reflections for Human Sensing with mmWave
                  Radar},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {231--246},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723471},
	doi = {10.1145/3680207.3723471},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/GongZX00F025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter-wave (mmWave) has emerged as a promising solution for contact-free sensing due to its high resolution. Although promising, it faces several critical issues, including occlusion from the surrounding environment, unstable orientation-dependent sensing performance, and significant interference when multiple targets are in close proximity. These fundamental issues hinder the widespread adoption of mmWave sensing in the real world. In this paper, we propose SeRadar, the first systematic framework that leverages all useful secondary reflections to significantly enhance reliability and bring mmWave sensing one step closer to real-world adoption. Unlike primary reflections commonly used in wireless sensing, secondary reflections—typically much weaker due to being reflected multiple times—are generally ignored in existing literature. However, we observe that secondary reflections are common in various scenarios and carry valuable information about target movements, which could also contribute to sensing. To effectively utilize secondary reflections for sensing, SeRadar addresses several challenges associated with secondary reflections. Specifically, it boosts weak secondary reflections to improve their sensing capability, identifies useful ones from a large number of secondary reflections captured in the environment, and mitigates primary-secondary interference in multi-target scenarios. We evaluate the performance of SeRadar in various environments, including offices, apartments, and vehicle cabins. Extensive experiments demonstrate SeRadar can enhance accuracy and reliability in diverse sensing scenarios.}
}


@inproceedings{DBLP:conf/mobicom/SharmaBNH25,
	author = {Neha Sharma and
                  Mariam Bebawy and
                  Yik Yu Ng and
                  Mohamed Hefeeda},
	title = {GlucoSense: Non-Invasive Glucose Monitoring using Mobile Devices},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {247--266},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723472},
	doi = {10.1145/3680207.3723472},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/SharmaBNH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Regular glucose monitoring is crucial for diabetic patients to avoid the risk of health complications such as stroke, kidney failure, heart disease, and even death. Most current devices for measuring glucose are costly and painful. We propose GlucoSense, a non-invasive glucose sensing solution on mobile devices. GlucoSense builds on the fact that glucose is an optically active molecule, which interacts with various wavelengths. We first conduct spectral analysis to demonstrate the feasibility of measuring glucose in the visible and near-infrared range (400–1000 nm), which is the range available on mobile devices. We also identify the relative importance of various spectral bands in this range. We further propose multiple practical designs for obtaining the required spectral bands for measuring glucose. We then design GlucoSense exploiting the sensing capabilities of modern smartphones combined with machine learning models. We conduct an ethics-approved user study with a diverse set of participants in terms of age, sex, ethnicity, and body mass index (BMI). We compare GlucoSense against a widely-used, FDA-approved glucose measuring device. Our results show that 80.4% of GlucoSense predictions are within Zone A (clinically accurate), and the remaining 19.3% are in Zone B (clinically acceptable) of the Clarke Error Grid (CEG). In addition, 99.7% of the predictions are within the None and Slight risk zones of the Surveillance Error Grid (SEG), indicating their high accuracy. Both CEG and SEG are standard metrics for assessing glucose-measuring devices. These results were obtained by GlucoSense running on unmodified phones in realistic environments with diverse illuminations.}
}


@inproceedings{DBLP:conf/mobicom/00120CN025,
	author = {Nan Wu and
                  Bo Chen and
                  Ruizhi Cheng and
                  Klara Nahrstedt and
                  Bo Han},
	title = {NeVo: Advancing Volumetric Video Streaming with Neural Content Representation},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {267--282},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723473},
	doi = {10.1145/3680207.3723473},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/00120CN025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Offering high-quality immersive content is the ultimate goal of volumetric video streaming. Although point clouds and meshes are dominant volumetric representations, their limitations in depicting photo-realistic content often undermine user experience. The recent advent of neural radiance fields (NeRF) offers a promising alternative content representation with superior photo-realism. However, streaming NeRF-based volumetric videos over wireless networks to mobile headsets faces significant challenges, including substantial bandwidth usage because of the large frame size, degraded visual quality due to even a low packet loss rate, and content artifacts caused by performance optimizations ( e.g. , remote rendering at the network edge). To address these challenges, in this paper, we introduce NeVo, a next-generation volumetric video streaming system for efficient delivery of neural content such as NeRF. NeVo incorporates the following innovations into a holistic system: (1) a novel method to model visibility of implicitly encoded neural content, thereby avoiding non-essential transmission to drastically reduce network data usage, (2) a lightweight, learning-based model for real-time content reconstruction after packet loss with carefully chosen data, and (3) judicious identification and selective delivery of intermediate data in edge-based NeRF rendering to effectively mitigate artifacts. Our extensive experiments indicate that compared with the state-of-the-art, NeVo saves up to 68.3% of bandwidth usage, maintains high visual quality despite packet loss, and enhances user experience by reducing artifacts.}
}


@inproceedings{DBLP:conf/mobicom/0003WLMQ0X025,
	author = {Hao Pan and
                  Yezhou Wang and
                  Jiting Liu and
                  Ruichun Ma and
                  Lili Qiu and
                  Yi{-}Chao Chen and
                  Guangtao Xue and
                  Ju Ren},
	title = {{CGMM:} Non-Invasive Continuous Glucose Monitoring in Wearables Using
                  Metasurfaces},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {283--298},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723474},
	doi = {10.1145/3680207.3723474},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0003WLMQ0X025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-invasive continuous glucose monitoring for diabetes patients remains challenging despite ongoing interest. This paper presents CGMM, a novel non-invasive wireless glucose monitoring system integrated into wearable devices. It features a specially designed metasurface that couples with the wearable's antenna and tissue fluid beneath the skin, amplifying frequency response changes caused by subtle glucose concentration variations. To address individual tissue variability and optimize the passive metasurface design, we develop a tunable metasurface and a one-shot calibration method to obtain the impedance for optimal resonance in glucose sensing environments with unknown parameters. The calibrated impedance is then used for the inverse design and fabrication of an economical passive metasurface. We implement prototypes of CGMM and conduct extensive experimental evaluations. In human experiments involving ten participants using the prototype with LibreVNA, the overall performance is quantified with relative errors ranging from -5.02% to 6.93% and an RMSE of 9.65 mg/dL.}
}


@inproceedings{DBLP:conf/mobicom/WangYLZZLL0T25,
	author = {Bingbing Wang and
                  Zeming Yang and
                  Wenhui Li and
                  Linling Zhong and
                  Fengyuan Zhu and
                  Jiazhen Lei and
                  Jianyu Luo and
                  Meng Jin and
                  Xiaohua Tian},
	title = {Wook: Enabling High-Throughput Wi-Fi Downlink with Ultra-Low Power},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {299--313},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723475},
	doi = {10.1145/3680207.3723475},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/WangYLZZLL0T25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Wi-Fi-enabled ultra-low power communication system exhibits high asymmetry between uplink and downlink speeds. The uplink can reach up to 1 Mbps, while the downlink throughput is around 100 Kbps. In this paper, we present  Wook , a novel high throughput downlink system to empower Commercial Off-The-Shelf (COTS)  W i-Fi devices to transmit high-speed  OOK  messages. The key innovation underpinning  Wook  is its ability to achieve sub-symbol level modulation, allowing a single OFDM symbol to carry multiple OOK bits. This is done by profoundly exploring the Wi-Fi PHY layer and identifying optimal input payload to achieve fine-grained Wi-Fi waveform manipulation. We fabricate a PCB prototype and employ the COTS Wi-Fi router to implement the entire system. Experimental results show that with a simulated IC power consumption 76.6 μW, Wook  achieves a data rate of up to 1.1 Mbps, an 8.9X improvement over state-of-the-art systems. Moreover, even at a communication distance of 95 m,  Wook  maintains a throughput of 82.9 Kbps.}
}


@inproceedings{DBLP:conf/mobicom/ZhuSLL0ZWT25,
	author = {Fengyuan Zhu and
                  Jiaqi Shen and
                  Wenhui Li and
                  Jianyu Luo and
                  Renjie Zhao and
                  Linling Zhong and
                  Bingbing Wang and
                  Xiaohua Tian},
	title = {NanoScatter: Towards Ambient IoT},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {314--328},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723476},
	doi = {10.1145/3680207.3723476},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhuSLL0ZWT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ambient IoT (A-IoT) aims to connect hundreds of billions of ultra-low-power and battery-free devices, which has been included in the agenda for 6G standardization by 3GPP. Backscatter communication is considered the mainstream enabling technique for A-IoT; however, current state-of-the-art can hardly meet A-IoT's main technical requirements simultaneously: power consumption below 100 μW, communication ranges up to 100 m, and 100+ concurrency. This paper presents NanoScatter, the first backscatter network with each tag implemented using our customized backscatter communication ASIC. We propose a nanowatt wake-up receiver design and a sensitivity-driven downlink/uplink modulation mechanism to carry out the ASIC, which enables minimizing the tag's power consumption and long-range communication. NanoScatter supports concurrent communication of 6 IC-based tags with a subcarrier capacity of 512, achieving communication distances of 66 m indoors and 100 m outdoors. The tag consumes 1 μW in idle listening, with the core circuit using 58 nW and 43 μW during communication.}
}


@inproceedings{DBLP:conf/mobicom/CaoM00025,
	author = {Yetong Cao and
                  Dong Ma and
                  Wentao Xie and
                  Qian Zhang and
                  Jun Luo},
	title = {{ESPIRO:} Natural Pulmonary Function Monitoring via Earphone-Acquired
                  Speech},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {329--344},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723477},
	doi = {10.1145/3680207.3723477},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/CaoM00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a crucial tool for assessing health,  spirometry  provides valuable insights into pulmonary functions. Recent advancements have enabled more convenient measurements by shifting spirometry solutions from cumbersome clinical devices to portable devices. However, the forced maneuvers and burdensome procedures, which necessitate repeated maximal forced breathing, often lead to dizziness and discomfort, rendering them unsuitable for vulnerable populations. In this paper, we present ESPIRO (Earphone-enabled Speech sPIROmetry) system to furnish user-friendly pulmonary function monitoring for diverse populations. Basically, ESPIRO records normal speech using microphone-embedded earphones and characterizes pulmonary function-related glottal flow during speech production. ESPIRO advances existing spirometry solutions in i) leveraging  phonetics  to associate pulmonary function with glottal flow in normal speech, thereby eliminating the need for forced breathing; ii) identifying effective speech features according to physiological basis, ensuring reliable spirometry measurements; and iii) effectively addressing ambient noise, making it suitable for various real-world settings. Extensive experiments with 38 subjects on 18 commodity earphones confirm that ESPIRO accurately estimates pulmonary function indices in practice.}
}


@inproceedings{DBLP:conf/mobicom/XieCG025,
	author = {Binbin Xie and
                  Minhao Cui and
                  Deepak Ganesan and
                  Jie Xiong},
	title = {Making LoRa Sensing Coexist with Communication},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {345--359},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723478},
	doi = {10.1145/3680207.3723478},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/XieCG025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa-based contact-free wireless sensing has attracted a lot of attention owing to its long sensing range, enabling wide-area sensing for the first time. While promising, existing LoRa sensing assumes there is no communication going on which is usually not true. We observe a severe degradation of sensing performance in real-world settings in the presence of communication. This issue hinders LoRa sensing from being adopted in real life and being integrated into the already established LoRa networking infrastructure. In this paper, we propose  LSencom  which takes the first step toward making LoRa-based wireless sensing work in the presence of communication. The key design is to employ the reversed chirp, i.e., downchirp, for sensing while keeping the original upchirp for communication. This design smartly leverages the orthogonality between downchirp and upchirp to mitigate the interference between communication and sensing. While the upchirp-downchirp design can remove most of the interference, we further adopt a novel chirp rotation method to deal with the remaining power leakage interference from upchirp to downchirp, enhancing the sensing performance. We implement  LSencom  on commodity LoRa nodes. Real-world experiments demonstrate that  LSencom  can reduce the communication-induced interference on sensing by 22.3 dB, and enable LoRa sensing even in the presence of multiple communication links.}
}


@inproceedings{DBLP:conf/mobicom/TangHOPS25,
	author = {Mingyue Tang and
                  Jizheng He and
                  Ryu Okubo and
                  Dhruv Panchmia and
                  Elahe Soltanaghai},
	title = {ChirpEye: Passive Sensing and Profiling of {FMCW} Radars with a Resource-constrained
                  Tag},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {360--374},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723479},
	doi = {10.1145/3680207.3723479},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/TangHOPS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Frequency-Modulated Continuous Wave (FMCW) radar systems become increasingly prevalent across various sensing applications, detecting their presence is crucial to mitigate interference and address potential security risks. Existing methods for spectrum sensing or detecting unintended Radio Frequency (RF) transmissions rely on expensive specialized hardware because these radars typically operate in the GHz frequency range and utilize large bandwidths. To overcome these limitations, we present ChirpEye, a simple but effective tag design that is capable of identifying FMCW radar waveforms without requiring prior knowledge of radar parameters such as chirp slope, operating frequency, or bandwidth. In addition, ChirpEye can identify the direction of incident signal, hinting at the potential location of the radar. The key innovation of ChirpEye lies in its novel tag design, which uses multiple antennas and delay lines to process GHz-level radar signals with only kHz sampling rates. The tag structure generates unique baseband frequencies that are proportional to FMCW waveform parameters. We also propose a new super-resolution algorithm, called Spectra-MUSIC, which can accurately estimate these beat frequencies from noisy data. Our extensive evaluations demonstrate that ChirpEye achieves 99% accuracy in detecting FMCW radars at distances up to 15 meters with less than 5% median error in estimating the radar chirp slope and less than 15 degrees median error in estimating the direction of the radar.}
}


@inproceedings{DBLP:conf/mobicom/LiWKSXXCX25,
	author = {Liyao Li and
                  Yun Wu and
                  Minsung Kim and
                  Bozhao Shang and
                  Jie Xiong and
                  Wenyao Xu and
                  Xiaojiang Chen and
                  Yaxiong Xie},
	title = {From Signal-based to Impedance-based Sensing: {A} paradigm Shift for
                  Plug-and-Play, Mobile, and Sensitive Battery-free Sensing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {375--390},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723480},
	doi = {10.1145/3680207.3723480},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiWKSXXCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Battery-free sensing has revolutionized IoT applications, but current solutions relying on signal variations between transmitted and backscattered signals remain vulnerable to environmental dynamics and deployment variations. This paper promotes a paradigm shift: inferring targets through antenna impedance variations instead of signal fluctuations, thereby eliminating the impact of unpredictable wireless communication. We demonstrate the effectiveness of this paradigm by reimplementing three existing applications: RIO [1], Keystub [2], and RF-EATS [3]. Compared to original signal-based implementations, our approach shows significant improvements in accuracy and robustness across diverse environments. Furthermore, By integrating antenna engineering with advanced materials science, we also transform antennas into innovative sensors for pressure, temperature, and UV light sensing. This interdisciplinary methodology pushes the boundaries of battery-free sensing, opening new avenues for IoT applications.}
}


@inproceedings{DBLP:conf/mobicom/0002025,
	author = {Haoming Wang and
                  Wei Gao},
	title = {When Device Delays Meet Data Heterogeneity in Federated AIoT Applications},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {391--406},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723481},
	doi = {10.1145/3680207.3723481},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0002025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated AIoT uses distributed data on IoT devices to train AI models. However, in practical AIoT systems, heterogeneous devices cause data heterogeneity and varying amounts of device staleness, which can reduce model performance or increase federated training time. When addressing the impact of device delays, existing FL frameworks improperly consider it as independent from data heterogeneity. In this paper, we explore a scenario where device delays and data heterogeneity are closely correlated, and propose FedDC, a new technique to mitigate the impact of device delays in such cases. Our basic idea is to use gradient inversion to learn knowledge about device's local data distribution and use such knowledge to compensate the impact of device delays on devices' model updates. Experiment results on heterogeneous IoT devices show that FedDC can improve the FL performance by 34% with high amounts of device delays, without impairing the devices' local data privacy.}
}


@inproceedings{DBLP:conf/mobicom/LiYWZ0B025,
	author = {Zeng Li and
                  Chuan Yan and
                  Liuhuo Wan and
                  Hui Zhuang and
                  Pengfei Hu and
                  Guangdong Bai and
                  Yiran Shen},
	title = {WinSpy: Cross-window Side-channel Attacks on Android's Multi-window
                  Mode},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {407--421},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723482},
	doi = {10.1145/3680207.3723482},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiYWZ0B025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of the Android system and increasing screen size, the use of  multi-window mode  has become prevalent among users. However, the security and privacy implications associated with this mode have not been thoroughly investigated. This paper uncovers severe and unique security vulnerabilities in Android's multi-window mode, revealing several high-risk side-channels that facilitate diverse cross-window attacks, leading to significant breaches of user privacy. In detail, our research introduces  WinSpy , a framework leveraging a newly discovered resource contention side-channel in multi-window mode to fingerprint app launches, web pages, and in-app activities, all without violating Android's permission framework. Our extensive evaluations demonstrate that  WinSpy  achieves high accuracy (from 70 to 80% detecting website and app launches to over 97% recognizing critical in-app activities). Additionally, we reveal that due to Android's lenient permission management for this mode, window apps can also use Inertial Measurement Unit sensors to launch attacks, such as inferring the user's touch positions outside the window with high precision. Furthermore, we propose systematic mitigations against these vulnerabilities.}
}


@inproceedings{DBLP:conf/mobicom/LiZXHCYZ025,
	author = {Ruonan Li and
                  Ziyue Zhang and
                  Xianjin Xia and
                  Ningning Hou and
                  Wenchang Chai and
                  Shiming Yu and
                  Yuanqing Zheng and
                  Tao Gu},
	title = {From Interference Mitigation to Toleration: Pathway to Practical Spatial
                  Reuse in LPWANs},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {422--437},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723483},
	doi = {10.1145/3680207.3723483},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiZXHCYZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the interference challenges, aiming to improve spatial reuse and optimize spectrum efficiency in LPWANs. We reveal that existing strategies such as interference cancellation and MIMO are ill-suited to the low-cost low-rate characteristics of LPWANs. Our work introduces a novel framework,  HydraNet , which leverages the capture effect of LPWAN radios to enable robust concurrent transmissions.  HydraNet  exempts from strict clock synchronization or accurate channel estimation as required by conventional spatial reuse strategies for interference nulling. We conduct in-depth studies with LoRa radios to uncover their underlying packet reception mechanisms and for the first time characterize their unique capture effect. Based on the new findings, we devise novel strategies to jointly control the timing and power of concurrent LPWAN transmissions. These strategies ensure sufficient power differences between packets and interference at their intended receivers. We prototype  HydraNet  and integrate with operational LoRaWANs and comprehensively evaluate its performance. Results show that  HydraNet  achieves higher spectrum utilization with up to 3.6 × throughput improvements over the state-of-the-art.}
}


@inproceedings{DBLP:conf/mobicom/HouZCZWHGX025,
	author = {Haozheng Hou and
                  Bowen Zheng and
                  Sitong Cheng and
                  Xiaoguang Zhao and
                  Peiheng Wu and
                  Lixing He and
                  Yunqi Guo and
                  Guoliang Xing and
                  Zhenyu Yan},
	title = {AquaScan: {A} Sonar-based Underwater Sensing System for Human Activity
                  Monitoring},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {438--452},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723484},
	doi = {10.1145/3680207.3723484},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HouZCZWHGX025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human activity monitoring in the water is essential for pool management and drowning prevention. Existing camera-based solutions pose significant concerns about privacy and extra installation costs. Although sonars have been widely used for underwater sensing in open aquatic environments such as oceans and lakes, monitoring human activities with sonars in a pool setup is still challenging. In this work, we propose AquaScan, the first scanning sonar-based underwater sensing system for human activity monitoring. To overcome the low frame rate due to the sonar's physical limitation, we propose a novel scanning strategy and apply an image reconstruction method to accelerate the scanning speed without compromising the performance of motion detection. To overcome the dynamic interferences in the underwater scenario, we develop a novel signal processing pipeline based on a physical model to remove noises and localize human subjects. We further extract features like motion, time, and spatial information from sonar images and develop a state-transfer-based activity recognition system to recognize five common water activities, i.e., swimming, motionless, splashing, struggling, and drowning. We have deployed AquaScan on three public swimming pools for a total period of 94 hours. The evaluation results show that AquaScan can successfully recognize the five activities in the water with around 91.5%.}
}


@inproceedings{DBLP:conf/mobicom/Tsai0LC25,
	author = {Yi{-}Zhen Tsai and
                  Xuechen Zhang and
                  Zheng Li and
                  Jiasi Chen},
	title = {{L3GS:} Layered 3D Gaussian Splats for Efficient 3D Scene Delivery},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {453--467},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723485},
	doi = {10.1145/3680207.3723485},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Tsai0LC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional 3D content representations include dense point clouds that consume large amounts of data and hence network bandwidth, while newer representations such as neural radiance fields suffer from poor frame rates due to their nonstandard volumetric rendering pipeline. 3D Gaussian splats (3DGS) can be seen as a generalization of point clouds that meet the best of both worlds, with high visual quality and efficient rendering for real-time frame rates. However, delivering 3DGS scenes from a hosting server to client devices is still challenging due to high network data consumption (e.g., 1.5 GB for a single scene). The goal of this work is to create an efficient 3D content delivery framework that allows users to view high quality 3D scenes with 3DGS as the underlying data representation. The main contributions of the paper are: (1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling algorithms to choose what splats to download at what time, and (3) Trace-driven experiments from users wearing virtual reality headsets to evaluate the visual quality and latency. Our system for Layered 3D Gaussian Splats delivery (L3GS) demonstrates high visual quality, achieving 16.9% higher average SSIM compared to baselines, and also works with other compressed 3DGS representations. The code is available at https://github.com/mavenslab/layered_3d_gaussian_splats.}
}


@inproceedings{DBLP:conf/mobicom/Shen0Z025,
	author = {Leming Shen and
                  Qiang Yang and
                  Yuanqing Zheng and
                  Mo Li},
	title = {AutoIOT: LLM-Driven Automated Natural Language Programming for AIoT
                  Applications},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {468--482},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723486},
	doi = {10.1145/3680207.3723486},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Shen0Z025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of Large Language Models (LLMs) has profoundly transformed our lives, revolutionizing interactions with AI and lowering the barrier to AI usage. While LLMs are primarily designed for natural language interaction, the extensive embedded knowledge empowers them to comprehend digital sensor data. This capability enables LLMs to engage with the physical world through IoT sensors and actuators, performing a myriad of AIoT tasks. Consequently, this evolution triggers a paradigm shift in conventional AIoT application development, democratizing its accessibility to all by facilitating the design and development of AIoT applications via natural language. However, some limitations need to be addressed to unlock the full potential of LLMs in AIoT application development. First, existing solutions often require transferring raw sensor data to LLM servers, which raises privacy concerns, incurs high query fees, and is limited by token size. Moreover, the reasoning processes of LLMs are opaque to users, making it difficult to verify the robustness and correctness of inference results. This paper introduces  AutoIOT , an LLM-based automated program generator for AIoT applications.  AutoIOT  enables users to specify their requirements using natural language (input) and automatically synthesizes interpretable programs with documentation (output).  AutoIOT  automates the iterative optimization to enhance the quality of generated code with minimum user involvement.  AutoIOT  not only makes the execution of AIoT tasks more explainable but also mitigates privacy concerns and reduces token costs with local execution of synthesized programs. Extensive experiments and user studies demonstrate  AutoIOT 's remarkable capability in program synthesis for various AIoT tasks. The synthesized programs can match and even outperform some representative baselines.}
}


@inproceedings{DBLP:conf/mobicom/0005YGQZ0HS0S25,
	author = {Yuhao Chen and
                  Yuxuan Yan and
                  Shuowei Ge and
                  Yuyang Qin and
                  Yue Zheng and
                  Qianqian Yang and
                  Shibo He and
                  Zhiguo Shi and
                  Jiming Chen and
                  Yuanchao Shu},
	title = {Confidant: Customizing Transformer-based LLMs via Collaborative Training
                  on Mobile Devices},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {483--497},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723487},
	doi = {10.1145/3680207.3723487},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0005YGQZ0HS0S25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have emerged as a cornerstone for advancing AI technologies. It revolutionizes the way we interact with devices, websites, and information, and paves the way for the development of highly intuitive and capable virtual assistants. Training of today's LLMs happens in cloud data centers due to the requirement of enormous data and a significant amount of computing power. Despite extensive research in mobile edge computing, fine-tuning pre-trained LLMs using resource-constrained devices like commodity smartphones remains highly under-explored. In this paper, we propose Confidant, a practical collaborative training framework that allows modern LLMs to be fine-tuned across multiple off-the-shelf mobile devices. To this end, Confidant partitions an LLM into several sub-models, allowing each of them to fit in the memory of a mobile device. Multiple mobile devices then collaborate to train the LLM by employing a novel pipeline parallel training approach. In specific, Confidant encompasses a memory-aware dynamic model partitioning and intra-device multi-processor scheduler to minimize the training time across heterogeneous platforms. To ensure resilient distributed training, a hybrid fault tolerance mechanism is devised to proactively manage potential device and network failures. We fully implemented Confidant in C++/Python, and built a cross-framework adapter, enabling collaborative training on a variety of mobile platforms. Experimental results show that Confidant excels in achieving computation-, memory-efficient, and robust customization of LLMs - it manages to train state-of-the-art billion-sized LLMs including BERT, GPT-2, Phi2, and LLaMA3, and fine-tunes Phi2-2.7B on Alpaca in just 40.1 hours using three consumer-grade mobile devices.}
}


@inproceedings{DBLP:conf/mobicom/HuZWHLXZ25,
	author = {Qingyong Hu and
                  Yuxuan Zhou and
                  Jinjian Wang and
                  Zirui Huang and
                  Guihua Li and
                  Qianhui Xu and
                  Qian Zhang},
	title = {mmTremor: Practical Tremor Monitoring for Parkinson's Disease and
                  Essential Tremor in Daily Life},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {498--512},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723488},
	doi = {10.1145/3680207.3723488},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HuZWHLXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tremor, a prevalent symptom in various neurological disorders, significantly impacts patients' quality of life. Regular and precise tremor monitoring is essential for optimizing treatment effectiveness. Existing at-home monitoring solutions have limitations in daily life due to low adherence, privacy concerns, or inevitable interference from confounding body components and overlapped motions. In this paper, we propose mmTremor, the first privacy-preserving contactless system to achieve practical tremor detection during activities of daily living (ADL) in real-world settings. We design a mmWave-depth fusion tracking algorithm to handle the high interference from confounding body components and a multimodal spatiotemporal deep learning pipeline to effectively exploit latent tremor information. Additionally, a spatial contrastive unsupervised adaptation is proposed for better discriminability and adaptation capability to unseen domains. To fully assess mmTremor in real-world scenarios, we collect a diverse dataset of 28 patients and 9 healthy subject simulations in over 20 distinct environments, including offices, hospitals, and homes. Extensive evaluations demonstrate that mmTremor achieves a high macro-F1 of 0.877 in tremor detection, showcasing the potential as a transformative contactless solution for tremor monitoring in daily life. The dataset will be open-source to facilitate future research.}
}


@inproceedings{DBLP:conf/mobicom/Xu0YZGZLY025,
	author = {Chi Xu and
                  Wentao Xie and
                  Baichen Yang and
                  Yizhen Zhang and
                  Yanbin Gong and
                  Jin Zhang and
                  Wei Li and
                  Shifang Yang and
                  Qian Zhang},
	title = {EasySpiro: Assessing Lung Function via Arbitrary Exhalations on Commodity
                  Earphones},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {513--528},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723489},
	doi = {10.1145/3680207.3723489},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Xu0YZGZLY025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conventional pulmonary function tests (PFTs) are important but costly. Hence, prior research has proposed IoT sensor-based solutions to facilitate cost-efficient, at-home PFT. However, these solutions require the subject to perform maximal exhalations, a task often challenging without supervision, compromising test accuracy. In response to this challenge, this study introduces EasySpiro that, for the first time, uses non-maximal exhalations to measure PFT indicators. This is challenging since PFT indicators are only defined for maximal exhalations, and there are no guidelines to derive them from submaximal exhalations. To address that, we observe that pulmonary deficiencies affect all types of breathing, where the underlying pulmonary deficiency should be the same under different breathing efforts. Leveraging this insight, we design a reconstruction model to predict the ideal maximal breathing patterns based on submaximal ones and utilize these reconstructions for PFT. Furthermore, since the body dynamics reflect the exhalation effort, we use self-supervised learning techniques to encode body dynamics into breathing effort representations to guide the reconstruction process. We integrate these designs into earphones with microphones to measure breathing patterns and IMUs to measure body dynamics. We collaborate with a hospital and develop a dataset from 50 patients with various diseases to evaluate EasySpiro's performance, which shows an accurate prediction of PFT indicators based on non-maximal exhalations with an error rate of 7%. In addition, we open-source the collected dataset to encourage future research.}
}


@inproceedings{DBLP:conf/mobicom/Lee0FRAK25,
	author = {Chae Young Lee and
                  Pu (Luke) Yi and
                  Maxwell Fite and
                  Tejus Rao and
                  Sara Achour and
                  Zerina Kapetanovic},
	title = {HyperCam: Low-Power Onboard Computer Vision for IoT Cameras},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {529--542},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723490},
	doi = {10.1145/3680207.3723490},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Lee0FRAK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present HyperCam, an energy-efficient image classification pipeline that enables computer vision tasks onboard low-power IoT camera systems. HyperCam leverages hyper-dimensional computing to perform training and inference efficiently on low-power microcontrollers. We implement a low-power wireless camera platform using off-the-shelf hardware and demonstrate that HyperCam can achieve an accuracy of 93.60%, 84.06%, 92.98%, and 72.79% for MNIST, Fashion-MNIST, Face Detection, and Face Identification tasks, respectively, while significantly outperforming other classifiers in resource efficiency. Specifically, it delivers inference latency of 0.08–0.27s while using 42.91–63.00KB flash memory and 22.25KB RAM at peak. Among other machine learning classifiers such as SVM, xgBoost, MicroNets, MobileNetV3, and MCUNetV3, HyperCam is the only classifier that achieves competitive accuracy while maintaining competitive memory footprint and inference latency that meets the resource requirements of low-power camera systems.}
}


@inproceedings{DBLP:conf/mobicom/Huang0H025,
	author = {Kai Huang and
                  Xiangyu Yin and
                  Heng Huang and
                  Wei Gao},
	title = {Modality Plug-and-Play: Runtime Modality Adaptation in LLM-Driven
                  Autonomous Mobile Systems},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {543--558},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723491},
	doi = {10.1145/3680207.3723491},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Huang0H025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multimodal reasoning by LLMs is critical to autonomous mobile systems, but the growing diversity of input data modalities prevents incorporating all modalities into LLMs. Instead, only the useful modalities should be adaptively involved at runtime, based on the current environmental contexts and task requirements. Existing work on runtime modality adaptation uses fixed connections between data encoders and LLM's input layer, but results in high training costs and ineffective cross-modal interaction. In this paper, we present MPnP, a new modality adaptation technique that connects data encoders to a flexible set of last LLM blocks and makes such latent connections fully trainable at runtime. Evaluation results show that MPnP has high compute and data efficiency, with 3.7× FLOPs reduction and 30% memory usage reduction compared to best baselines. It requires only few hundreds of training samples at runtime, and completes modality adaptation within few minutes on weak devices.}
}


@inproceedings{DBLP:conf/mobicom/AtalaySFS025,
	author = {Tolga O. Atalay and
                  Dragoslav Stojadinovic and
                  Alireza Famili and
                  Angelos Stavrou and
                  Haining Wang},
	title = {5G-MAP: Demystifying the Performance Implications of Cloud-Based 5G
                  Core Deployments},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {559--573},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723492},
	doi = {10.1145/3680207.3723492},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/AtalaySFS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Fifth Generation (5G) core network is designed as a set of Virtual Network Functions (VNFs) hosted on Commercial-Off-the-Shelf (COTS) hardware. This creates a growing demand for general-purpose computing resources. Given their elastic infrastructure, cloud services like Amazon Web Services (AWS) are attractive platforms to address this need. Therefore, it is crucial to understand the Quality of Service (QoS) requirements associated with deploying the 5G core in the cloud. We developed the 5G-MAP (5G Measurement and Assessment Platform) to understand the trade-offs between different deployment strategies. Our framework facilitates detailed control and user plane performance assessments in varied deployment scenarios. We integrated 5G-MAP with the OpenAirInterface (OAI) 5G core and utilized it in a series of deployments across seven countries, leveraging eight AWS regions and eighteen edge zones. Our evaluations cover from HTTP transactions to user plane throughput and packet loss. We identify topologies that can considerably lower the 5G core service chain latencies due to a significant reduction in the number of inter-site hops. Such actionable performance improvements illustrate how operators can leverage 5G-MAP to optimize their cloud-based 5G deployments.}
}


@inproceedings{DBLP:conf/mobicom/WangZH025,
	author = {Haodong Wang and
                  Qihua Zhou and
                  Zicong Hong and
                  Song Guo},
	title = {D2MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device
                  MoE-based {LLM} Serving},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {574--588},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723493},
	doi = {10.1145/3680207.3723493},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/WangZH025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The mixture of experts (MoE) model is a sparse variant of large language models (LLMs), designed to hold a better balance between intelligent capability and computational overhead. Despite its benefits, MoE is still too expensive to deploy on resource-constrained edge devices, especially with the demands of on-device inference services. Recent research efforts often apply model compression techniques, such as quantization, pruning and merging, to restrict MoE complexity. Unfortunately, due to their predefined static model optimization strategies, they cannot always achieve the desired quality-overhead trade-off when handling multiple requests, finally degrading the on-device quality of service. These limitations motivate us to propose the D 2 MoE, an algorithm-system co-design framework that matches diverse task requirements by dynamically allocating the most proper bit-width to each expert. Specifically, inspired by the nested structure of matryoshka dolls, we propose the  matryoshka weight quantization  (MWQ) to progressively compress expert weights in a bit-nested manner and reduce the required runtime memory. On top of it, we further optimize the I/O-computation pipeline and design a heuristic scheduling algorithm following our  hottest-expert-bit-first  (HEBF) principle, which maximizes the expert parallelism between I/O and computation queue under constrained memory budgets, thus significantly reducing the idle temporal bubbles waiting for the experts to load. Evaluations on real edge devices show that D 2 MoE improves the overall inference throughput by up to 1.39× and reduces the peak memory footprint by up to 53% over the latest on-device inference frameworks, while still preserving comparable serving accuracy as its INT8 counterparts.}
}


@inproceedings{DBLP:conf/mobicom/ZhaoW000YO025,
	author = {Shanhui Zhao and
                  Hao Wen and
                  Wenjie Du and
                  Cheng Liang and
                  Yunxin Liu and
                  Xiaozhou Ye and
                  Ye Ouyang and
                  Yuanchun Li},
	title = {LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration
                  for Mobile Apps},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {589--603},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723494},
	doi = {10.1145/3680207.3723494},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhaoW000YO025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have opened new opportunities for automated mobile app exploration, an important and challenging problem that used to suffer from the difficulty of generating meaningful UI interactions. However, existing LLM-based exploration approaches rely heavily on LLMs to generate actions in almost every step, leading to a huge cost of token fees and computational resources. We argue that such extensive usage of LLMs is neither necessary nor effective, since many actions during exploration do not require, or may even be biased by the abilities of LLMs. Further, based on the insight that a precise and compact knowledge plays the central role for effective exploration, we introduce LLM-Explorer, a new exploration agent designed for efficiency and affordability. LLM-Explorer uses LLMs primarily for maintaining the knowledge instead of generating actions, and knowledge is used to guide action generation in a LLM-less manner. Based on a comparison with 5 strong baselines on 20 typical apps, LLM-Explorer was able to achieve the fastest and highest coverage among all automated app explorers, with over 148x lower cost than the state-of-the-art LLM-based approach.}
}


@inproceedings{DBLP:conf/mobicom/Kim0VKJ25,
	author = {Minsung Kim and
                  Abhishek Kumar Singh and
                  Davide Venturelli and
                  John Kaewell and
                  Kyle Jamieson},
	title = {X-ResQ: Parallel Reverse Annealing for Quantum Maximum-Likelihood
                  {MIMO} Detection with Flexible Parallelism},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {604--619},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723495},
	doi = {10.1145/3680207.3723495},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Kim0VKJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum Annealing (QA)-accelerated MIMO detection is an emerging research approach in the context of NextG wireless networks. The opportunity is to enable large MIMO systems and thus improve wireless performance. The approach aims to leverage QA to expedite the computation required for theoretically optimal but computationally-demanding Maximum Likelihood detection to overcome the limitations of the currently deployed linear detectors. This paper presents  X-ResQ , a QA-based MIMO detector system featuring flexible parallelism that is uniquely enabled by quantum  Reverse Annealing  (RA). Unlike prior designs, X-ResQ has many desirable parallel QA system properties and has effectively improved detection performance as more qubits are assigned. In our evaluations on a state-of-the-art quantum annealer, fully parallel X-ResQ achieves near-optimal throughput for 4 × 6 MIMO with 16-QAM using approx. 240 qubits achieving 2.5–5× gains compared against other classical and quantum detectors. We also implement and evaluate X-ResQ in the non-quantum digital setting for more comprehensive evaluations. This classical X-ResQ showcases the potential to realize ultra-large 1024 × 1024 MIMO, significantly outperforming other MIMO detectors, including the state-of-the-art RA detector classically implemented in the same way.}
}


@inproceedings{DBLP:conf/mobicom/YeK025,
	author = {Hanting Ye and
                  Niels van der Kolk and
                  Qing Wang},
	title = {{NIRF:} Detecting Cameras That Hide Behind Screen},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {620--634},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3723496},
	doi = {10.1145/3680207.3723496},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/YeK025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hidden spy cameras are a growing global threat to personal privacy. With the emergence of translucent screen technology, a new security risk has arisen: cameras can now hide  behind  devices' screens like TVs and monitors that are common in private places, e.g., hotel rooms. The screen's  covering  over the hidden camera not only makes the cameras behind it unnoticeable to human eyes but also makes existing camera detection methods less effective. Inspired by recent advances in representing real-world scenes accurately using neural networks, we propose  Neural Infrared Reflectance Field (NIRF)  to learn the intricate optical properties of the screen and the cameras hidden behind it. Through NIRF, we design a new camera detection system by leveraging the unique reflective properties of behind-screen cameras and screens. We evaluate NIRF with thorough experiments on five smartphones. Our NIRF archives over 90% detection rate and is robust to different conditions, including varied backgrounds, ambient light levels, screen protectors, and screen contents. Besides, we conduct a field study by deploying 18 common spy cameras behind a 65-inch translucent TV and recruiting 27 people to compare NIRF with commercial hidden camera detectors. NIRF achieves an 89.5% detection rate, significantly outperforming the best commercial hidden camera detector that only has a 14.4% detection rate of behind-screen cameras.}
}


@inproceedings{DBLP:conf/mobicom/DongLCZQ0025,
	author = {Huixin Dong and
                  Jingqi Lin and
                  Minhao Cui and
                  Serene Zhang and
                  Lili Qiu and
                  Jie Xiong and
                  Wei Wang},
	title = {GPSoil: Towards low-cost soil moisture sensing using {GNSS} signals},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {635--649},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765236},
	doi = {10.1145/3680207.3765236},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/DongLCZQ0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With global population growth, sustainable agriculture requires efficient soil moisture sensing for precise irrigation. While commercial soil moisture sensors are often limited by cost and durability, state-of-the-art RF-based sensing solutions require additional signal transmitter infrastructure, hindering widespread adoption. To fill this gap, we introduce GPSoil—a novel soil moisture sensing system that leverages pervasive Global Navigation Satellite System (GNSS) signals. On the hardware side, we employ two antennas along with a low-cost RF switch, enabling error mitigation for long propagation distances by comparing the signals captured from both antennas. On the software side, we leverage the inherent clock drift errors in commercial GNSS sensors and turn them into tools for fine-grained value extraction, enabling high-resolution sensing from otherwise coarse data. By integrating hardware and software innovations, we successfully achieve practical soil moisture sensing using GNSS signals. Built with off-the-shelf components, GPSoil achieves a soil moisture accuracy of 5.2% at a material cost of only $10.56, making it far more affordable than existing systems. GPSoil can sense moisture up to one meter underground, far surpassing other RF-based sensing systems and meeting the needs of most crops and irrigation systems. This work pioneers the use of GNSS signals in agriculture, offering a scalable, low-cost solution for advancing precision irrigation and promoting sustainable agriculture.}
}


@inproceedings{DBLP:conf/mobicom/HeXMFWRJZ25,
	author = {Chenming He and
                  Rui Xia and
                  Chengzhen Meng and
                  Xiaoran Fan and
                  Dequan Wang and
                  Haojie Ren and
                  Jianmin Ji and
                  Yanyong Zhang},
	title = {Ghost Points Matter: Far-Range Vehicle Detection with a Single mmWave
                  Radar in Tunnel},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {650--666},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765237},
	doi = {10.1145/3680207.3765237},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HeXMFWRJZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle detection in tunnels is crucial for traffic monitoring and accident response, yet remains underexplored. In this paper, we develop mmTunnel, a millimeter-wave radar system that achieves far-range vehicle detection in tunnels. The main challenge here is coping with ghost points caused by multi-path reflections, which lead to severe localization errors and false alarms. Instead of merely removing ghost points, we propose correcting them to true vehicle positions by recovering their signal reflection paths, thus reserving more data points and improving detection performance, even in occlusion scenarios. However, recovering complex 3D reflection paths from limited 2D radar points is highly challenging. To address this problem, we develop a multi-path ray tracing algorithm that leverages the ground plane constraint and identifies the most probable reflection path based on signal path loss and spatial distance. We also introduce a curve-to-plane segmentation method to simplify tunnel surface modeling such that we can significantly reduce the computational delay and achieve real-time processing. We have evaluated mmTunnel with comprehensive experiments. In two test tunnels, we conducted controlled experiments in various scenarios with cars and trucks. Our system achieves an average F1 score of 93.7% for vehicle detection while maintaining real-time processing. Even in the challenging occlusion scenarios, the F1 score remains above 91%. Moreover, we collected extensive data from a public tunnel with heavy traffic at times and show our method could achieve an F1 score of 91.5% in real-world traffic conditions.}
}


@inproceedings{DBLP:conf/mobicom/ZhangLLZGYZ025,
	author = {Jinrui Zhang and
                  Pengkai Li and
                  Fengzu Li and
                  Deyu Zhang and
                  Wei Gao and
                  Sheng Yue and
                  Yaoxue Zhang and
                  Ju Ren},
	title = {FocusX: All-in-Focus Image Synthesis for Dynamic Scenes on Mobile
                  Devices},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {667--681},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765238},
	doi = {10.1145/3680207.3765238},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhangLLZGYZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose  FocusX , the first mobile-deployable system achieving artifact-free all-in-focus synthesis in dynamic scenes. Our approach introduces three key innovations: 1) For focal stack acquisition, our depth prior-based dynamic focusing method that adaptively selects focus distances using real-time scene depth distribution analysis and depth-of-field constrained spatial clustering, reducing redundant captures while ensuring full depth coverage; 2) To reduce pixel misalignment caused by lens breathing, we adopt a one-time offline calibration to map the relationship between field-of-view and focus distance, aligning the images by cropping accordingly; 3) We design the  Diff-MotionAIFNet , a conditional diffusion-based model that decouples moving-static components for artifact-free AIF reconstruction in dynamic scene while preserving scene fidelity. We further contribute  DynaAIFSet , containing 5,500 dynamic scenes (120K images) for training and evaluation. Experiments show  FocusX  achieves state-of-the-art performance, outperforming baselines up by 59.6% in  SSIM  and 49.1% in  PSNR , respectively. The deployment latency of  FocusX  is 4.8s on Honor Magic7 Pro. This work bridges computational photography theory with mobile implementation constraints, delivering practical AIF enhancement for user-generated content.}
}


@inproceedings{DBLP:conf/mobicom/HongWYZ000025,
	author = {Zhiqing Hong and
                  Weibing Wang and
                  Anlan Yu and
                  Shuxin Zhong and
                  Haotian Wang and
                  Yi Ding and
                  Tian He and
                  Desheng Zhang},
	title = {Experience Paper: Nationwide Human Behavior Sensing in Last-mile Delivery},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {682--696},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765239},
	doi = {10.1145/3680207.3765239},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HongWYZ000025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human behavior sensing has been receiving growing attention from both academia and industry in recent years. We report - to the best of our knowledge - the first AI-driven nationwide human behavior sensing system called SMILE in urban last-mile delivery. SMILE detects real-time human behaviors with self-supervised sensor data pretraining and uploads detection results to cloud servers with mobile networks. During its full deployment phase at JD Logistics, SMILE is deployed on over 500,000 mobile devices carried by over 300,000 delivery couriers who travel more than 10 million KM every day in 366 Chinese cities. SMILE serves the delivery of 7 billion E-commerce orders every year for more than 500 million customers. SMILE detects  walking, upstairs, downstairs, still , and  driving  behaviors. SMILE has been fully deployed at JD Logistics to support two real-world applications that benefit both the delivery couriers and the logistics platform: (1) workload measurement to improve couriers' welfare; (2) large-scale delivery map data generation to improve the logistics delivery efficiency.}
}


@inproceedings{DBLP:conf/mobicom/Wang00HQ0X025,
	author = {Yezhou Wang and
                  Yongjian Fu and
                  Hao Pan and
                  Qinyun Hu and
                  Lili Qiu and
                  Yi{-}Chao Chen and
                  Guangtao Xue and
                  Ju Ren},
	title = {{WDNN:} Weighted Diffractive Neural Network for Physical-layer {RF}
                  Signal Processing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {697--711},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765240},
	doi = {10.1145/3680207.3765240},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Wang00HQ0X025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Diffractive neural networks (NNs) have garnered attention for directly implementing wireless signal processing at the physical layer. However, they are limited by a constrained weight learning space and activation functions, which restricts their data processing capabilities. To address this, we propose an RF circuit-based weighted diffraction NN (WDNN) that rivals digital NNs in processing ability. We design a weighted asymmetric RF coupler unit that, when stacked into a network, enables diffractive propagation with arbitrary connection weights. Additionally, an activation module is introduced that utilizes RF amplifiers operating in their nonlinear regions. We validate the effectiveness of the proposed WDNN through three tasks: 32-level amplitude modulated (AM) signal decoding, 31-class angle of arrival (AoA) estimation, and 2-class Wi-Fi based fall detection. After training, WDNN achieves the accuracy of 98.5%, 93.7%, and 90.8% in the AM decoding, AoA estimation, and fall detection tasks, respectively; while the diffractive NN SOTA achieves only 21.6%, 16.9%, and 63.3%. We also implement the prototypes of WDNN and SOTA, and real-world experimental results demonstrate that our method achieves an average accuracy improvement of up to 76.85% across various tasks compared to SOTA.}
}


@inproceedings{DBLP:conf/mobicom/NguyenRDLGC00025,
	author = {Khang Nguyen and
                  Yidong Ren and
                  Jialuo Du and
                  Jingkai Lin and
                  Maolin Gan and
                  Shigang Chen and
                  Mi Zhang and
                  Chunyi Peng and
                  Zhichao Cao},
	title = {LoRaSeek: Boosting Denoising Ability in Neural-enhanced LoRa Decoder
                  via Hierarchical Feature Extraction},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {712--726},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765241},
	doi = {10.1145/3680207.3765241},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/NguyenRDLGC00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose LoRaSeek, a lightweight and reliable LoRa denoising framework that enhances signal quality and robustness for neural-enhanced LoRa decoding. LoRaSeek integrates a hybrid architecture combining Convolutional Neural Networks (CNNs), Transformers, and a hierarchical U-Net to effectively capture multi-scale, multidimensional features of LoRa chirp signals. To maintain efficiency, we integrate a lightweight Transformer block that supports various LoRa configurations while keeping computational overhead low. Additionally, we incorporate dual attention-based skip connections to preserve chirp signal properties across different scales. Experiments across diverse LoRa configurations show that LoRaSeek achieves 2.04–3.86 dB signal-to-noise ratio (SNR) gains over standard decoding methods and up to 3.03 dB improvement over state-of-the-art neural-enhanced LoRa decoding methods while reducing model storage by up to 7.4× and inference time by up to 1.6×.}
}


@inproceedings{DBLP:conf/mobicom/TongCW25,
	author = {Shuai Tong and
                  Qian Chen and
                  Jiliang Wang},
	title = {OptSamp: Optimizing the Sampling Rate for LoRa Energy Efficiency Enhancement},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {727--741},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765242},
	doi = {10.1145/3680207.3765242},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/TongCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa, as a widely used Low-Power Wide-Area Network (LP-WAN) technology, is designed for long-term use. However, in practice, the battery life of LoRa devices often falls far short of the expected decade-long duration. We propose OptSamp, a software-based approach that reduces the energy consumption of LoRa devices by lowering their physical-layer sampling rate, thereby extending battery life. To address the challenge of frequency aliasing caused by down-sampling, we embed a specially designed feature into each modulated symbol, enabling the OptSamp receiver to accurately recover the distorted signal. In addition, we design an adaptive sampling-rate selection mechanism to balance link reliability and energy efficiency. We further propose OptSamp+, which compresses symbol duration to shorten uplink transmission time, thereby reducing transmitter energy consumption and enhancing spectral efficiency. We prototype OptSamp and OptSamp+ on software-defined LoRa platforms and evaluate them in both indoor and outdoor environments. Results show that OptSamp reduces the receiver-side sampling rate to 1/16 of the Nyquist rate, cutting downlink energy consumption by 68%, and OptSamp+ reduces uplink transmission time by up to 1/32 compared to traditional LoRa.}
}


@inproceedings{DBLP:conf/mobicom/He0YYJ0YX025,
	author = {Yuting He and
                  Xinyan Wang and
                  Mu Yuan and
                  Bufang Yang and
                  Siyang Jiang and
                  Yihua Huang and
                  Doris Sau{-}Fung Yu and
                  Guoliang Xing and
                  Hongkai Chen},
	title = {Myo-Trainer: {A} Vision-based Muscle-Aware Motion Feedback System
                  for In-Home Resistance Training},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {742--757},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765243},
	doi = {10.1145/3680207.3765243},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/He0YYJ0YX025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-home resistance training (RT) is a convenient and effective way to maintain health and well-being. However, incorrect exercise execution can result in unintended muscle engagement and an increased risk of injury. Without access to professional coaching, an accurate muscle-aware motion feedback system becomes essential for safe and effective training. However, existing visual language models (VLMs) struggle to provide accurate and effective muscle-aware movement guidance due to their limited understanding of RT motion and the absence of related expert knowledge. In this work, we introduce Myo-Trainer, the first vision-based muscle-aware motion feedback system that uses explicit muscle-aware motion analysis and domain-specific expert knowledge to provide corrective guidance on muscle engagement and movement execution. Also, we propose a novel DAGCN-Former network that integrates both spatial and temporal modeling capabilities to capture the complex dynamics of human RT motion. Experiments involving 26 subjects and 1000+ minutes of RT demonstrate that Myo-Trainer improves the accuracy of motion analysis by 17.22%, achieves a 2.5x reduced inference latency and a BertScore of 85.88% of generated feedback compared to those provided by experienced certified trainers, outperforming existing solutions. Additionally, Myo-Trainer received higher satisfaction ratings from participants compared to other AI trainers and video tutorials, highlighting its potential for real-world applications.}
}


@inproceedings{DBLP:conf/mobicom/0001HLDWZ0LC25,
	author = {Feng Lyu and
                  Lijuan He and
                  Mingliu Liu and
                  Sijing Duan and
                  Hao Wu and
                  Jieyu Zhou and
                  Yi Ding and
                  Zaixun Ling and
                  Yibo Cui},
	title = {Auto-UIT: Automating {UAV} Inspection Trajectory by Recognizing Pylon
                  Structure from 3D Point Cloud},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {758--772},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765244},
	doi = {10.1145/3680207.3765244},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0001HLDWZ0LC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {UAV-assisted inspection is critical for modern power grid maintenance, enhancing efficiency and safety in remote areas. However, automatically designing UAV inspection trajectories is challenging due to the cluttered inspection environments, small inspection targets, and pervasive obstacles. We propose  Auto-UIT , a novel method for generating inspection trajectories in noisy, sparse, and complex 3D point cloud.  Auto-UIT  has three core techniques: (1) A  local structure-enhanced pylon segmentation , which accurately segments pylons, power lines, and surroundings in noisy point cloud for effective inspection target identification and trajectory planning. (2)  A 3D fingerprint-based pylon type recognition  that compensates for point cloud sparsity to complete missing inspection targets based on the pylon type. (3)  An adaptive trajectory generation  that samples positions in response to diverse pylon orientations and pervasive environmental obstacles, ensuring UAV operational safety. Our experiments on a real-world dataset across four distinct areas demonstrate that  Auto-UIT  outperforms existing baseline methods in all three tasks. Furthermore, a four-month deployment in a power grid inspection system—covering a 270 km 2  primary mountainous area—yielded an expert first-review acceptance rate of 91.86% for the generated trajectories, and reduced design time by an average of 88.19% compared to manual methods, significantly improving inspection efficiency.}
}


@inproceedings{DBLP:conf/mobicom/Ke0LP025,
	author = {Zhihui Ke and
                  Xiaobo Zhou and
                  Yuyang Liu and
                  Zhizhuo Pang and
                  Tie Qiu},
	title = {EdgeGaussian: Real-time Free-Viewpoint Video for Mobile {VR} via Edge-Client
                  Collaborative Neural Rendering},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {773--787},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765245},
	doi = {10.1145/3680207.3765245},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Ke0LP025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Free-Viewpoint Videos (FVVs) enable immersive viewing of a scene from any position and angle using virtual reality (VR) head-mounted displays (HMDs), thus have great potential in various applications such as telepresence, gaming, and education. Recently, 3D Gaussian Splatting (3DGS) has emerged as a promising method for FVV construction due to its superior reconstruction quality. However, its real-time rendering on untethered HMDs remains challenging due to high computational demands. To address this challenge, we propose EdgeGaussian, a novel edge-client collaborative framework for real-time FVV rendering. Our approach employs decomposed static-dynamic 4D Gaussian splatting (SD-4DGS) to separately reconstruct static and dynamic components of a scene. We further introduce a hybrid mesh-4DGS neural representation, where static components are modeled as textured meshes for local rendering, while dynamic components are offloaded to edge servers as 4DGS. This decomposition significantly reduces the computational burden on the client device while maintaining high rendering quality. Our testbed experiments demonstrate that Edge-Gaussian achieves up to 128 FPS, outperforming state-of-the-art local rendering methods by 4x and edge rendering methods by 5x.}
}


@inproceedings{DBLP:conf/mobicom/Ning0T25,
	author = {Zhiyuan Ning and
                  Zheng Wang and
                  Zhanyong Tang},
	title = {MetaGuardian: Enhancing Voice Assistant Security through Advanced
                  Acoustic Metamaterials},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {788--801},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765246},
	doi = {10.1145/3680207.3765246},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Ning0T25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Voice assistants (VAs) have become integral to daily life, yet their always-on microphones make them attractive targets for attacks that threaten user privacy and safety. We present MetaGuardian, the first system to leverage acoustic metamaterials to defend against three major classes of attacks for VAs - inaudible, adversarial, and laser-based - within a single, portable design. Unlike prior defenses, MetaGuardian can be seamlessly integrated into the enclosures of commercial smart devices, providing strong protection without requiring software modification, hardware redesign, or costly machine learning models. MetaGuardian leverages mutual impedance effects between metamaterial units to extend the protection range to 16–40 kHz, effectively blocking wideband inaudible attacks. It also employs a carefully designed coiled space structure to disrupt adversarial signals while preserving normal VA operations. Its universal design allows flexible adaptation to different devices, striking a balance between portability and protection effectiveness. In controlled evaluations, MetaGuardian achieves a high defense success rate across all attack types, offering a practical and reliable foundation for securing VAs on smart devices.}
}


@inproceedings{DBLP:conf/mobicom/SunHKC25,
	author = {Yansong Sun and
                  Jialuo He and
                  Dirk Kutscher and
                  Huangxun Chen},
	title = {AdaptQNet: Optimizing Quantized {DNN} on Microcontrollers via Adaptive
                  Heterogeneous Processing Unit Utilization},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {802--816},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765247},
	doi = {10.1145/3680207.3765247},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/SunHKC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is a growing trend in deploying DNNs on tiny microcontroller (MCUs) to provide inference capabilities in the IoT. While prior research has explored many lightweight techniques to compress DNN models, achieving overall efficiency in model inference requires not only model optimization but also careful system resource utilization for execution. Existing studies primarily leverage arithmetic logic units (ALUs) for integer-only computations on a single CPU core. Floating-point units (FPU) and multi-core capabilities available in many existing MCUs remain underutilized. To fill this gap, we propose AdaptQNet, a novel MCU neural network system that can determine the optimal precision assignment for different layers of a DNN model. AdaptQNet models the latency of various operators in DNN models across different precisions on heterogeneous processing units. This facilitates the discovery of models that utilize FPU and multi-core capabilities to enhance capacity while adhering to stringent memory constraints. Our implementation and experiments demonstrate that AdaptQNet enables the deployment of models with better accuracy-efficiency trade-off on MCUs.}
}


@inproceedings{DBLP:conf/mobicom/LeeLCIWHOLS25,
	author = {Jungjae Lee and
                  Dongjae Lee and
                  Chihun Choi and
                  Youngmin Im and
                  Jaeyoung Wi and
                  Kihong Heo and
                  Sangeun Oh and
                  Sunjae Lee and
                  Insik Shin},
	title = {VeriSafe Agent: Safeguarding Mobile {GUI} Agent via Logic-based Action
                  Verification},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {817--831},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765248},
	doi = {10.1145/3680207.3765248},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LeeLCIWHOLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Foundation Models (LFMs) have unlocked new possibilities in human-computer interaction, particularly with the rise of mobile Graphical User Interface (GUI) Agents capable of interacting with mobile GUIs. These agents allow users to automate complex mobile tasks through simple natural language instructions. However, the inherent probabilistic nature of LFMs, coupled with the ambiguity and context-dependence of mobile tasks, makes LFM-based automation unreliable and prone to errors. To address this critical challenge, we introduce VeriSafe Agent (VSA) 1 : a formal verification system that serves as a logically grounded safeguard for Mobile GUI Agents. VSA deterministically ensures that an agent's actions strictly align with user intent before executing the action. At its core, VSA introduces a novel  autoformalization  technique that translates natural language user instructions into a formally verifiable specification. This enables runtime, rule-based verification of agent's actions, detecting erroneous actions even before they take effect. To the best of our knowledge, VSA is the first attempt to bring the rigor of formal verification to GUI agents, bridging the gap between LFM-driven actions and formal software verification. We implement VSA using off-the-shelf LFM services (GPT-4o) and evaluate its performance on 300 user instructions across 18 widely used mobile apps. The results demonstrate that VSA achieves 94.33%–98.33% accuracy in verifying agent actions, outperforming existing LFM-based verification methods by 30.00%–16.33%, and increases the GUI agent's task completion rate by 90%–130%.}
}


@inproceedings{DBLP:conf/mobicom/Sun000X25,
	author = {Zehua Sun and
                  Tao Ni and
                  Pengfei Hu and
                  Tao Gu and
                  Weitao Xu},
	title = {SpaceSched: {A} Constellation-Wide Scheduling System for Resolving
                  Ground Track Congestion in Remote Sensing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {832--847},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765249},
	doi = {10.1145/3680207.3765249},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Sun000X25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent proliferation of spacecraft in Earth's orbits has ushered in the rise of large-scale satellite constellations. However, this unprecedented growth of constellations has introduced a previously unforeseen challenge: ground track congestion. Specifically, the increasing density of orbital slots forces satellites to share similar orbit planes, causing their nadir-point projections on Earth's surface (i.e., ground tracks) to overlap or remain in close proximity within short time intervals. Such orbit-endowed ground track congestion can degrade constellation performance in remote sensing operations, specified by limited constellation coverage, redundant satellite count, and delayed data delivery. To address this issue, we propose  SpaceSched , a hierarchical scheduling framework designed to resolve ground track congestion in satellite constellations.  SpaceSched  consists of two key components: an on-ground constellation scheduling pipeline, comprising a coverage distributor and a satellite selector, and an in-space downlink scheduling pipeline, featuring a queue regulator. The coverage distributor assigns attitude profiles over time to satellites, ensuring non-overlapping imagery capture regions. The satellite selector optimizes the constellation by strategically selecting a subset of satellites while maintaining coverage efficiency. During in-space downlink scheduling, the queue regulator manages the downlink traffic queue to minimize the delay of high-value data transmission. We evaluate  SpaceSched  on two operational modes (i.e., stripmap and spotlight) across three well-established satellite constellation systems: SKYSAT, LEMUR, and FLOCK, with 17, 50, and 126 evaluated satellites, respectively. Experimental results demonstrate that  SpaceSched  improves coverage by up to 1.84×, reduces satellite counts by up to 2.38×, and decreases downlink queue load by up to 36.46×, compared to the plain satellite constellation systems. Furthermore, our case study highlights  SpaceSched 's potential to meet diverse task demands.}
}


@inproceedings{DBLP:conf/mobicom/HeZ0SJ0JQ025,
	author = {Zhaodian He and
                  Fusang Zhang and
                  Junqi Ma and
                  Yuqi Su and
                  Beihong Jin and
                  Daqing Zhang and
                  Yuechun Jiao and
                  Lili Qiu and
                  Jie Xiong},
	title = {Multi-Antenna Quantum Receiver: {A} Leap Beyond Angle Estimation Constraints},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {848--862},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765250},
	doi = {10.1145/3680207.3765250},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HeZ0SJ0JQ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Beyond communication, wireless signals have been extensively utilized for localization, tracking, and sensing in recent years. The key information extracted for these purposes includes distance and angle. While distance measurement accuracy is mainly limited by signal bandwidth, angle accuracy depends on the number of antennas and phase noise. Conventional approaches typically improve angle estimation by boosting signal strength and increasing the number of antennas. In this paper, we propose employing a quantum receiver to substantially improve angle estimation performance. Rather than amplifying signal strength, the quantum receiver reduces the inherent hardware noise. Furthermore, we exploit the unique properties of a quantum RF receiver to construct a multi-antenna quantum system. Using only two physical quantum antennas, we generate virtual antennas by leveraging the receiver's broad frequency range, effectively increasing the number of antennas and significantly improving angle measurement performance. Our experimental results demonstrate that, with only two quantum antennas, we achieve angle estimation performance surpassing that of a conventional RF receiver equipped with 40 antennas. Furthermore, quantum antennas are not constrained by the coupling effects that typically limit the spacing between conventional RF antennas, allowing for much closer placement. This represents a significant step toward reducing the size of antenna arrays while preserving localization and tracking performance.}
}


@inproceedings{DBLP:conf/mobicom/ItaniCRKG25,
	author = {Malek Itani and
                  Tuochao Chen and
                  Arun Raghavan and
                  Gavriel Kohlberg and
                  Shyamnath Gollakota},
	title = {Wireless Hearables With Programmable Speech {AI} Accelerators},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {863--877},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765251},
	doi = {10.1145/3680207.3765251},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ItaniCRKG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The conventional wisdom has been that designing ultra-compact, battery-constrained wireless hearables with on-device speech AI models is challenging due to the high computational demands of streaming deep learning models. Speech AI models require continuous, real-time audio processing, imposing strict computational and I/O constraints. We present  NeuralAids , a fully on-device speech AI system for wireless hearables, enabling real-time speech enhancement and denoising on compact, battery-constrained devices. Our system bridges the gap between state-of-the-art deep learning for speech enhancement and low-power AI hardware by making three key technical contributions: 1) a wireless hearable platform integrating a speech AI accelerator for efficient on-device streaming inference, 2) an optimized dual-path neural network designed for low-latency, high-quality speech enhancement, and 3) a hardware-software co-design that uses mixed-precision quantization and quantization-aware training to achieve real-time performance under strict power constraints. Our system processes 6 ms audio chunks in real-time, achieving an inference time of 5.54 ms while consuming 71.6 mW. In real-world evaluations, including a user study with 28 participants, our system outperforms prior on-device models in speech quality and noise suppression, paving the way for next-generation intelligent wireless hearables that can enhance hearing entirely on-device.}
}


@inproceedings{DBLP:conf/mobicom/0001KPLKC25,
	author = {Seonghoon Park and
                  Minchan Kim and
                  Hyejin Park and
                  Jeho Lee and
                  Jiwon Kim and
                  Hojung Cha},
	title = {{EOS:} Energy-Optimized Super-Resolution on Mobile Devices for Live
                  360-Degree Videos},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {878--893},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765252},
	doi = {10.1145/3680207.3765252},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0001KPLKC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although on-device video super-resolution enables high-quality live 360-degree streaming on mobile devices, existing methods often waste energy by overlooking perceived visual quality. In this paper, we present EOS, an energy-efficient on-device super-resolution system for mobile omnidirectional video (ODV) live streaming. EOS reduces energy waste by dynamically adjusting super-resolution complexity based on the predicted visual quality of super-resolved frames. This approach raises two challenges: (1) designing an adaptive inference policy that maximizes energy savings while minimizing degradation in Quality-of-Experience (QoE), and (2) developing a method to predict visual quality under the constraints of mobile ODV live streaming. To tackle these challenges, EOS introduces EOS SR and a No-Reference Up-scaling Quality Prediction scheme. EOS SR employs a device-agnostic, scalable deep neural network optimized for mobile devices, with an energy-aware scheduler that jointly selects the optimal super-resolution model and GPU frequency. The No-Reference Upscaling Quality Prediction scheme estimates visual quality across arbitrary viewpoints in real time without requiring high-resolution reference videos. Experiments on commodity smartphones show that EOS reduces average power consumption by 34.6%–49.9% compared to baseline methods, while preserving high visual quality and frame rates.}
}


@inproceedings{DBLP:conf/mobicom/Xie0GQ025,
	author = {Binbin Xie and
                  Weizheng Wang and
                  Deepak Ganesan and
                  Lili Qiu and
                  Jie Xiong},
	title = {Cross-Technology Sensing: Leveraging LoRa Signals to Empower WiFi
                  Sensing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {894--908},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765253},
	doi = {10.1145/3680207.3765253},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Xie0GQ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Various wireless technologies have been utilized for sensing. Although promising, these wireless sensing technologies have inherent limitations. Prior research mainly focuses on overcoming the limitations of an individual wireless sensing technology, and little attention has been paid to the potential benefits of sensing with more than one wireless technology. In this paper, we introduce the concept of cross-technology sensing for the first time, and propose  LoFiSen  to enable LoRa-to-WiFi sensing.  LoFiSen  leverages the strengths of both LoRa and WiFi—combining LoRa's long-range capability with WiFi's pervasiveness. The chirp characteristic of LoRa signal significantly improves the sensing range of WiFi, and the widespread availability of WiFi devices makes LoRa sensing more pervasive.  LoFiSen  is fully compatible with LoRa and WiFi protocols, and can work on commodity LoRa and WiFi hardware. The key component of our design is enabling the WiFi receiver to capture fine-grained LoRa signal variations for sensing. Real-world experiments demonstrate that  LoFiSen  improves the WiFi sensing range for respiration monitoring from 8 m to 41 m, and pushes the walking sensing range from 16 m to 73.5 m. Through-wall passive respiration monitoring, previously infeasible with state-of-the-art WiFi sensing, is now possible with  LoFiSen.}
}


@inproceedings{DBLP:conf/mobicom/LeTW0K25,
	author = {Tan Khang Le and
                  Mohammad Omidvar Tehrani and
                  Yuepeng Wang and
                  Jianliang Wu and
                  Steven Y. Ko},
	title = {Formalization, Implementation, and Verification of the Bluetooth {L2CAP}
                  State Machine},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {909--922},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765254},
	doi = {10.1145/3680207.3765254},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LeTW0K25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Logical Link Control and Adaptation Protocol (L2CAP) is a core Bluetooth component, and verifying its correctness is crucial for reliable and secure connectivity. However, verification can be challenging due to the complexity and ambiguities in its natural language (English) specification. In this paper, we present a  formally verified implementation  of the L2CAP state machine. Our approach introduces the Specification State Machine (SSM) to formalize the L2CAP state machine in the specification and the Operational State Machine (OSM) as an abstraction of the implementation. We then formally prove that (i) OSM refines SSM, and (ii) our implementation semantically conforms to OSM. By combining these two proofs, we verify that our implementation complies with our formalization of the specification. Furthermore, we define critical safety and liveness properties and formally prove that our implementation satisfies these guarantees. To ensure practicality, we implement the L2CAP state machine in Dafny and integrate it into Android's Fluoride Bluetooth stack. Our evaluation demonstrates that our formally verified implementation maintains competitive performance while ensuring formal correctness.}
}


@inproceedings{DBLP:conf/mobicom/LuWD0LW025,
	author = {Yang Lu and
                  Jie Wang and
                  Xiaoyun Dong and
                  Ziyao Huang and
                  Bingyi Liu and
                  Jen{-}Ming Wu and
                  Jianping Wang},
	title = {VI-Planning: Infrastructure-Assisted Real-Time Planning Optimization
                  for Autonomous Driving},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {923--937},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765255},
	doi = {10.1145/3680207.3765255},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LuWD0LW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Infrastructure-assisted autonomous driving has emerged as a pivotal technology to overcome the challenges posed by occlusions and limited fields of view for individual vehicles. Vehicles can fuse perception information from the infrastructure with their own in real-time, thereby enhancing their perception ability. However, our real-world experiments demonstrate that such an approach could introduce artifacts such as ghost objects, resulting in unsafe and unreliable planning outcomes. Besides, the system integration complexity and communication overhead are typically considerable, posing challenges to practical deployment. Therefore, we propose VI-Planning, an innovative infrastructure-assisted system that effectively optimizes autonomous vehicle planning in real time. The core idea of VI-Planning is to leverage the scene-level future occupancy grid maps constructed by the infrastructure as future drivable area references to directly optimize planning outcomes of autonomous vehicles. Since VI-Planning operates only at the autonomous vehicle's final output stage, without modifying the vehicle's underlying system architecture, it can be plug-and-play for most autonomous driving systems, whether they are modular or end-to-end architectures. Moreover, VI-Planning employs a novel bitwise encoding mechanism to efficiently compress these maps, enabling practical transmission. We implement VI-Planning end-to-end on a real-world testbed. The results of closed-loop and open-loop experiments indicate that VI-Planning can achieve real-time planning optimization (62.54 ms on average) and 817 × data transmission efficiency compared to the state-of-the-art baseline. A video demo of VI-Planning on our real-world testbed is available at: https://youtu.be/DXl5BhDEvFQ.}
}


@inproceedings{DBLP:conf/mobicom/ZhuZZTL0JZ25,
	author = {Hanqi Zhu and
                  Wuyang Zhang and
                  Xinran Zhang and
                  Ziyang Tao and
                  Xinrui Lin and
                  Yu Zhang and
                  Jianmin Ji and
                  Yanyong Zhang},
	title = {UrgenGo: Urgency-Aware Transparent {GPU} Kernel Launching for Autonomous
                  Driving},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {938--952},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765256},
	doi = {10.1145/3680207.3765256},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhuZZTL0JZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancements in autonomous driving have introduced increasingly complex, real-time GPU-bound tasks critical for reliable vehicle operation. However, the proprietary nature of these autonomous systems and closed-source GPU drivers hinder fine-grained control over GPU executions, often resulting in missed deadlines that compromise vehicle performance. To address this, we present UrgenGo, a non-intrusive, urgency-aware GPU scheduling system that operates without access to application source code. UrgenGo implicitly prioritizes GPU executions through transparent kernel launch manipulation, employing task-level stream binding, delayed kernel launching, and batched kernel launch synchronization. We conducted extensive real-world evaluations in collaboration with a self-driving startup, developing 11 GPU-bound task chains for a realistic autonomous navigation application and implementing our system on a self-driving bus. Our results show a significant 61% reduction in the overall deadline miss ratio, compared to the state-of-the-art GPU scheduler that requires source code modifications.}
}


@inproceedings{DBLP:conf/mobicom/MaCL025,
	author = {Xiaoyue Ma and
                  Junming Chen and
                  Lannan Luo and
                  Qiang Zeng},
	title = {LLM-Assisted IoT Testing: Finding Conformance Bugs in Matter SDKs},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {953--968},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765257},
	doi = {10.1145/3680207.3765257},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/MaCL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Matter is an IoT standard endorsed by hundreds of companies, designed to ensure interoperability between devices from various vendors. The Matter Software Development Kit (SDK) serves as the foundation for developing Matter devices, making bug discovery in Matter SDKs crucial. Given the extensive specification and the rapid evolution of Matter—five versions released in just two and a half years—the need for automated solutions is increasingly urgent. In this paper, we present MatterGuard, the first automated system for identifying bugs in Matter SDKs that violate the specification. Unlike traditional SDK testing approaches, which typically integrate testing code with the SDK code, Matter-Guard decouples the two, allowing the testing code to be reused across SDK versions. Furthermore, MatterGuard leverages a large language model to analyze the Matter specification and uses the extracted knowledge to guide the bug discovery process. In our evaluation across all five SDK versions, MatterGuard uncovers 109 bugs, demonstrating the effectiveness and scalability of our approach.}
}


@inproceedings{DBLP:conf/mobicom/LiLZPL0L25,
	author = {Lingang Li and
                  Zilong Liu and
                  Yihao Zhang and
                  Siyuan Peng and
                  Zhirong Liu and
                  Yongrui Chen and
                  Zhijun Li},
	title = {WiLE-Audio: Wide-Coverage Low-Energy Audio via WiFi-BLE Cross-Technology
                  Communication},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {969--983},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765258},
	doi = {10.1145/3680207.3765258},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiLZPL0L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth audio, as a common application in our daily life, faces a major challenge due to its limited transmission range in meeting users' demands. Traditional solutions, such as using high-power Bluetooth transmitters, require hardware upgrades that are neither cost-effective nor energy-efficient. This work introduces WiLE-Audio, a novel approach that extends Low Energy Audio (LE Audio) coverage through physical-layer cross-technology communication (CTC) from WiFi to Bluetooth Low Energy (BLE). We first present a novel symbol mapping technique from WiFi DQPSK to BLE GFSK symbols, which enables all-channel and reliable CTC to support Bluetooth channel hopping. Then, to implement CTC to commodity WiFi Network Interface Card (NIC), we present a real-time reverse scrambling method that dynamically calculates the payload of WiFi packets at the NIC driver. Finally, to align with the strict time window requirements of the BLE receiver, we design a precise timing strategy and a priority scheduling mechanism at the WiFi transmitter, effectively mitigating timing offsets due to Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) and queue management. These systematic innovations allow WiLE-Audio to be easily implemented into existing commercial WiFi and BLE devices with only a simple software upgrade on the WiFi side. Furthermore, using existing WiFi infrastructures, WiLE-Audio enables the relaying of Bluetooth Low Energy Audio (LE-Audio) and the roaming of BLE receiver at any WiFi-covered location. We implement WiLE-Audio on commercial WiFi and BLE devices, and conduct extensive evaluations across various scenarios. Experimental results demonstrate that WiLE-Audio extends the transmission distance of LE Audio by 2× in single-hop mode and at least 2.6× in two-hop roaming mode. This work provides a cost-effective and scalable solution for enhancing Bluetooth audio coverage, providing a promising prospect for whole-house or even whole-building LE Audio listening.}
}


@inproceedings{DBLP:conf/mobicom/YinYX00L25,
	author = {Wangsong Yin and
                  Rongjie Yi and
                  Daliang Xu and
                  Gang Huang and
                  Mengwei Xu and
                  Xuanzhe Liu},
	title = {Elastic On-Device {LLM} Service},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {984--999},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765259},
	doi = {10.1145/3680207.3765259},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/YinYX00L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {On-device Large Language Models (LLMs) are transforming mobile AI, catalyzing applications like UI automation without privacy concerns. Nowadays the common practice is to deploy a single yet powerful LLM as a general task solver for multiple requests. We identify a key system challenge in this paradigm: current LLMs lack the elasticity to serve requests that have diversified Service-Level Objectives (SLOs) on inference latency. To tackle this, we present ElastiLM, an on-device LLM service that elasticizes both the model and the prompt dimension of a full LLM. It incorporates (1) a one-shot neuron-reordering method, which leverages the intrinsic permutation consistency in transformer models to generate high-quality elasticized sub-models with minimal runtime switching overhead; (2) a dual-head tiny language model, which efficiently and effectively refines the prompt and orchestrates the elastification between model and prompt. We implement such an elastic on-device LLM service on multiple COTS smartphones, and evaluate ElastiLM on both standalone NLP/mobile-agent datasets and end-to-end synthesized traces. On diverse SLOs, ElastiLM outperforms 7 strong baselines in (absolute) accuracy by up to 14.83% and 10.45% on average, with <1% TTFT switching overhead, on-par memory consumption and <100 offline GPU hours.}
}


@inproceedings{DBLP:conf/mobicom/ApostoloBNB025,
	author = {Guilherme Henrique Apostolo and
                  Pablo Bauszat and
                  Vinod Nigade and
                  Henri E. Bal and
                  Lin Wang},
	title = {Uirapuru: Timely Video Analytics for High-Resolution Steerable Cameras
                  on Edge Devices},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1000--1014},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765260},
	doi = {10.1145/3680207.3765260},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ApostoloBNB025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time video analytics on high-resolution cameras has become a popular technology for various intelligent services like traffic control and crowd monitoring. While extensive work has been done on improving analytics accuracy with timing guarantees, virtually all of them target static viewpoint cameras. In this paper, we present Uirapuru, a novel framework for real-time, edge-based video analytics on high-resolution  steerable  cameras. The actuation performed by those cameras brings significant dynamism to the scene, presenting a critical challenge to existing popular approaches such as frame tiling. To address this problem, Uirapuru incorporates a comprehensive understanding of camera actuation into the system design paired with fast adaptive tiling at a per-frame level. We evaluate Uirapuru on a high-resolution video dataset, augmented by pan-tilt-zoom (PTZ) movements typical for steerable cameras and on real-world videos collected from an actual PTZ camera. Our experimental results show that Uirapuru provides up to 1.45× improvement in accuracy while respecting specified latency budgets or reaches up to 4.53× inference speedup with on-par accuracy compared to state-of-the-art static camera approaches.}
}


@inproceedings{DBLP:conf/mobicom/XuZGS025,
	author = {Huatao Xu and
                  Yan Zhang and
                  Wei Gao and
                  Guobin Shen and
                  Mo Li},
	title = {Experience Paper: Adopting Activity Recognition in On-demand Food
                  Delivery Business},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1015--1028},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765261},
	doi = {10.1145/3680207.3765261},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/XuZGS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the first nationwide deployment of human activity recognition (HAR) technology in the on-demand food delivery industry. We successfully adapted the state-of-the-art LIMU-BERT foundation model to the delivery platform. Spanning three phases over two years, the deployment progresses from a feasibility study in Yangzhou City to nationwide adoption involving 500,000 couriers across 367 cities in China. The adoption enables a series of downstream applications, and large-scale tests demonstrate its significant operational and economic benefits, showcasing the transformative potential of HAR technology in real-world applications. Additionally, we share lessons learned from this deployment and open-source our LIMU-BERT pretrained with millions of hours of sensor data.}
}


@inproceedings{DBLP:conf/mobicom/Gong0W00X25,
	author = {Chen Gong and
                  Bo Liang and
                  Purui Wang and
                  Xiaoyu Ji and
                  Yin Chen and
                  Chenren Xu},
	title = {RF-Rock: An Intermodulation-based {RFID} Unauthorized Identification
                  Attack without Tag Activation},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1029--1044},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765262},
	doi = {10.1145/3680207.3765262},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Gong0W00X25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Following the broad prospect of Radio Frequency Identification (RFID) technology is the security concern of unauthorized tag identification, which poses threats to the privacy of both objects and users. In this paper, we propose RF-Rock, the first RFID unauthorized identification attack that operates without tag activation, thereby evading almost all existing defenses. This attack exposes the vulnerabilities of current RFID networks in identification legitimacy and privacy. It is based on the intermodulation effect originating from intrinsic nonlinearity within tag circuits. To this end, we explore the distinctness and consistency of the intermodulation-based physical layer fingerprint of RFID tags with theoretical analysis and empirical validation, and optimize the attack accuracy and efficiency with delicate excitation plan. Real-world experiments show that RF-Rock achieves an attack success rate of 93.2% on average under various conditions. The entropy of our proposed fingerprint is 15.5 bits and implies sufficient capacity in practical attacks.}
}


@inproceedings{DBLP:conf/mobicom/0070H025,
	author = {Xin Li and
                  Yinghui He and
                  Jun Luo},
	title = {{\(\mu\)}Ceiver-Fi: Exploiting Spectrum Resources of Multi-Link Receiver
                  for Fine-Granularity Wi-Fi Sensing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1045--1059},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765263},
	doi = {10.1145/3680207.3765263},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0070H025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wi-Fi is deemed as a promising sensing media due to its ubiquity, yet Wi-Fi sensing is known to be confined by its limited bandwidth that leads to insufficient range resolution. Though sampling a wider spectrum multiple times can enable wideband sensing, its practicality is still hampered by the need for accessing Wi-Fi firmware. In this paper, we propose  μ Ceiver-Fi to exploit spectrum resources for fine-granularity Wi-Fi sensing; it relies solely on a commodity  multi-link receiver.  Since the channel samples from multiple links under the same receiver can still be misaligned, we first innovate in a comprehensive calibration process to align these samples. This is followed by a novel optimization framework to extend effective sensing bandwidth to GHz-level using only a few channel samples. Finally, we specifically design a spectral representation for sensing information in order to bridge between wideband signals and diversified downstream applications. Through comprehensive evaluations in Wi-Fi pose estimation task, we demonstrate the promising performance of  μ Ceiver-Fi in fine-granularity sensing.}
}


@inproceedings{DBLP:conf/mobicom/MillarHSHM25,
	author = {Josh Millar and
                  Yushan Huang and
                  Sarab Sethi and
                  Hamed Haddadi and
                  Anil Madhavapeddy},
	title = {Benchmarking Ultra-Low-Power {\(\mu\)}NPUs},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1060--1074},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765264},
	doi = {10.1145/3680207.3765264},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/MillarHSHM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient on-device neural network (NN) inference offers predictable latency, improved privacy and reliability, and lower operating costs for vendors than cloud-based inference. This has sparked recent development of microcontroller-scale NN accelerators, also known as neural processing units ( μ NPUs), designed specifically for ultra-low-power applications. We present the first comparative evaluation of a number of commercially-available  μ NPUs, including the first independent benchmarks for multiple platforms. To ensure fairness, we develop and open-source a model compilation pipeline supporting consistent benchmarking of quantized models across diverse microcontroller hardware. Our resulting analysis uncovers both expected performance trends as well as surprising disparities between hardware specifications and actual performance, including certain  μ NPUs exhibiting unexpected scaling behaviors with model complexity. This work provides a foundation for ongoing evaluation of  μ NPU platforms, alongside offering practical insights for both hardware and software developers in this rapidly evolving space.}
}


@inproceedings{DBLP:conf/mobicom/AfzalRCWA25,
	author = {Sayed Saad Afzal and
                  Jack Rademacher and
                  Weitung Chen and
                  Purui Wang and
                  Fadel Adib},
	title = {Scalable and Low Power Localization for Underwater Robots},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1075--1090},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765265},
	doi = {10.1145/3680207.3765265},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/AfzalRCWA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Localization is a critical task for underwater robots, yet today's underwater localization systems are limited by their accuracy, scalability, and/or energy consumption (i.e., longevity). We present the design, implementation, and evaluation of EchoBLUE- an accurate, scalable, and low-power localization system for underwater robots. In EchoBLUE, an underwater robot transmits SONAR-style (FMCW) signals, and leverages ultra-low power underwater backscatter nodes as location anchors. EchoBLUE's design introduces two key innovations. The first is a novel doppler compensation mechanism that enables it to accurately self-localize under mobility: the technique employs a  cross-chirp  mechanism that exploits the  quad-band nature  of the resulting backscatter response to overcome the range-doppler ambiguity. Second, it introduces the first  semi-active retrodirective underwater backscatter  design and uses it for location anchors; this design achieves wide bandwidth to backscatter the full FMCW signal, enabling fine-grained localization. We implemented a proof of concept prototype of EchoBLUE by building a base station mounted on a BlueROV2 underwater robot and custom-designed low-power retrodirective location anchors deployed in a pool. Our evaluation across 700 real-world trials demonstrates that EchoBLUE achieves a median 3D localization accuracy of 28 cm and 90th percentile of 48 cm. Moreover, these anchors consume only 740  μW  for semi-active backscatter, paving the way for truly low-power and scalable underwater localization.}
}


@inproceedings{DBLP:conf/mobicom/Zhu0HZF025,
	author = {Yinan Zhu and
                  Meng Xue and
                  Haiyan Hu and
                  Cong Zhang and
                  Xiaoyi Fan and
                  Qian Zhang},
	title = {{DD-LIVM:} Pioneering Cross-Domain Photovoltaic Defect Detection Using
                  Large Infrared-Visible Model},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1091--1105},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765266},
	doi = {10.1145/3680207.3765266},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Zhu0HZF025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Photovoltaic (PV) defect detection is crucial for preventing power efficiency loss and fire hazards. The industry primarily relies on the fusion of infrared and visible images for defect localization and diagnosis. However, current detection methods exhibit poor generalizability in new site environments or with altered imaging setups. While recent infrared and vision foundation models (FM) facilitate domain-invariant feature maps extraction, directly concatenating them and fine-tuning achieves limited generalizability gain to PV defect detection, due to the asymmetric dual-modal semantics of defects. In this paper, we present the first large infrared-visible model  DD-LIVM  to enable cross-domain defect detection. The key innovation of DD-LIVM lies in its defect-specific three-step fine-tuning strategy, which utilizes alternating modality masking. Prior to feature fusion and joint fine-tuning, the infrared and visible FM encoders are alternately masked and optimized to enhance their individual semantic utility for defect localization visibility and classification granularity, with feature distances among different defect types regulated through contrastive learning. This approach allows for the extraction of generalizable and defect-specific feature maps. Moreover, for practical employment of DD-LIVM, we propose a domain-agnostic spatial alignment algorithm for infrared-visible images before dual-modal fusion, and develop source data augmentation and adaptive detection head selection schemes based on defects' infrared characteristics to further enhance the generalizability. Extensive experiments on 7,078 dual-modal images from 9 real-world scenarios across 4 cities' PV stations demonstrate that DD-LIVM achieves an accuracy of 87.7% for cross-domain defect detection, surpassing state-of-the-art methods by 17.3%.}
}


@inproceedings{DBLP:conf/mobicom/Kim0LJJC25,
	author = {Gunjoong Kim and
                  Seonghoon Park and
                  Jeho Lee and
                  Chanyoung Jung and
                  Hyungchol Jun and
                  Hojung Cha},
	title = {Vega: Fully Immersive Mobile Volumetric Video Streaming with 3D Gaussian
                  Splatting},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1106--1120},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765267},
	doi = {10.1145/3680207.3765267},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Kim0LJJC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For highly immersive mobile volumetric video streaming, it is essential to deliver photo-realistic full-scene content with smooth playback. Unlike traditional representations such as point clouds, 3D Gaussian Splatting (3DGS) has gained attention for its ability to represent high-quality full-scene 3D content. However, our preliminary experiments show that existing methods for 3DGS-based videos fail to achieve smooth playback on mobile devices. In this paper, we propose Vega, a 3DGS-based photo-realistic full-scene volumetric video streaming system that ensures real-time playback on mobile devices. The core idea behind Vega's real-time rendering is object-level selective computation, which allocates computational resources to visually important objects to meet strict rendering deadlines. To enable mobile streaming based on the selective computation, Vega addresses two challenges: (1) designing an encoding scheme that optimizes the data size of videos while being compatible with object-level prioritization, and (2) developing a rendering pipeline that efficiently operates on resource-constrained mobile devices. We implemented an end-to-end Vega system, consisting of a streaming server and an Android application. Experimental results on commodity smartphones show that Vega achieves 30 frames per second (FPS) for full-scene volumetric video streaming while maintaining competitive data size and visual quality compared to existing baselines.}
}


@inproceedings{DBLP:conf/mobicom/00010YZWZ0C25,
	author = {Meng Xue and
                  Wentao Xie and
                  Zuohuizi Yi and
                  Zhilong Zhang and
                  Shumao Wu and
                  Yinan Zhu and
                  Qian Zhang and
                  Changzheng Chen},
	title = {Home-based Dry Eye Assessment via Blink Kinematics Using mmWave and
                  Clinical Knowledge Distillation},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1121--1135},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765268},
	doi = {10.1145/3680207.3765268},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/00010YZWZ0C25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tear Film Break-Up Time (TBUT) is a critical clinical parameter in the management of dry eye disease (DED). However, traditional TBUT assessments rely on costly and time-consuming clinical procedures, while existing home-based solutions fail to provide precise TBUT values. In this work, we present  Blinic , a contactless system leveraging commercial millimeter-wave (mmWave) radar to predict precise TBUT values and assess DED severity grades at home.  Blinic  incorporates detailed blink kinematics that are closely linked to TBUT. To address the challenge of predicting TBUT directly from radar data, we propose a teacher-student learning framework. The teacher model, trained on electronic health records (EHRs) including image-based diagnostic tests, transfers medical insights to the student model, which uses radar-captured blink dynamics. This knowledge transfer is further enhanced by a fine-tuned large language model, DryEye-LLM, which is based on clinical diagnostic reports and employs unsupervised domain adaptation to align EHRs with radar data. To ensure accurate blink motion capture,  Blinic  employs an antenna-coded MIMO mmWave radar design. Additionally, a query-based multitask learning module simultaneously predicts TBUT and DED severity grades, addressing potential conflicts in feature representation. Evaluated on 192 participants in collaboration with an eye clinic,  Blinic  demonstrates achieving a mean absolute error of 2.73 seconds for TBUT with an average accuracy of 90.54% for DED grading in real-world settings, providing a practical solution for home-based DED management.}
}


@inproceedings{DBLP:conf/mobicom/HarishaHE25,
	author = {Skanda Harisha and
                  Jimmy G. D. Hester and
                  Aline Eid},
	title = {DragonFly: Single mmWave Radar 3D Localization of Highly Dynamic Tags
                  in GPS-Denied Environments},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1136--1150},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765269},
	doi = {10.1145/3680207.3765269},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HarishaHE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The accurate localization and tracking of dynamic targets, such as equipment, people, vehicles, drones, robots, and the assets that they interact with in GPS-denied indoor environments is critical to enabling safe and efficient operations in the next generation of spatially-aware industrial facilities. This paper presents DragonFly, a 3D localization system of highly dynamic backscatter tags using a single MIMO mmWave radar. The system delivers the first demonstration of a mmWave backscatter system capable of exploiting the capabilities of MIMO radars for the 3D localization of mmID tags moving at high speeds and accelerations at long ranges by introducing a critical Doppler disambiguation algorithm and a fully-integrated cross-polarized dielectric-lens-based mmID tag consuming a mere 68 μW. DragonFly was extensively evaluated in static and dynamic configurations, including on a flying quadcopter, and benchmarked against multiple baselines, demonstrating its ability to track the positions of multiple tags with a median 3D accuracy of 12 cm at speeds and acceleration on the order of 10 m s –1  and 4 m s –2  and at ranges of up to 50 m.}
}


@inproceedings{DBLP:conf/mobicom/0433CAKZWSLR025,
	author = {Wei Wang and
                  Ellie C. Chen and
                  Naveed H. Ahmed and
                  Wonjune Kim and
                  Yiwei Zou and
                  Joshua E. Woods and
                  Yumin Su and
                  Huan{-}Cheng Liao and
                  Jacob T. Robinson and
                  Kaiyuan Yang},
	title = {Dual-Resonance Magnetoelectric Power and Data Links for Miniaturized
                  Wireless Bio-Implants},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1151--1165},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765271},
	doi = {10.1145/3680207.3765271},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0433CAKZWSLR025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Miniature, battery-free implants promise transformative bio-electronic therapies by enabling minimally invasive implantation procedures, reducing risk, and extending device lifetime. Among all wireless power and data transfer (WPDT) modalities, magnetoelectrics (ME) has emerged as a particularly promising solution for millimeter-scale implants, boasting lower tissue attenuation and higher power transfer efficiency over conventional inductive and ultrasonic methods. However, as an acoustic resonator, ME devices face an inherent tradeoff between Q-factor and bandwidth, limiting their ability to simultaneously achieve high-speed communication and efficient wireless power transfer (WPT). To fundamentally circumvent the challenge, this paper presents dual-resonance ME WPDT that exploits the unique multimode resonances of ME transducers to realize WPT and communication at distinct frequencies. Based on this dual-resonance principle, we demonstrate reconfigurable active and passive schemes for different biomedical applications, with a proof-of-concept system including a miniature implant and an external transceiver. The active scheme achieves 60 kbps at operational distances of 6 cm with 2.5 mW implant power, while the passive backscatter offers 20 kbps continuous streaming at 4 cm with negligible power, demonstrating the first reported non-interrupted ME WPDT system and more than twice the data rate of previous ME backscatter methods. Both schemes further support on-off keying (OOK) and binary phase shift keying (BPSK) modulations, providing additional flexibility to tailor communication needs between power efficiency and robustness. The complete prototype system was validated through comprehensive  in-vitro  experiments and  in-vivo  EMG streaming in a rodent model.}
}


@inproceedings{DBLP:conf/mobicom/NguyenMHN025,
	author = {Chi{-}Hieu Nguyen and
                  Bui Duc Manh and
                  Dinh Thai Hoang and
                  Diep N. Nguyen and
                  Lin Wang},
	title = {Demo: PP-AICloud for Edge-Assisted Privacy-Preserving {AI} Inference
                  with Homomorphic Encryption in Cloud-Based Mobile Services},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1166--1168},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765578},
	doi = {10.1145/3680207.3765578},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/NguyenMHN025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents PP-AICloud, a practical system prototype designed to enable privacy-preserving AI inference for cloud-based mobile services. Motivated by recent industry efforts, such as Apple's integration of Homomorphic Encryption (HE) for on-device intelligence, our work addresses the key limitations of existing privacy-preserving machine learning (PPML) solutions, i.e., high latency, bandwidth inefficiencies, and excessive on-device computation. PP-AICloud leverages edge nodes as an intermediate computing layer between mobile devices and centralized AICloud infrastructures, distributing the HE workflow across edge and cloud resources. By integrating HE with deep convolutional neural networks (CNNs), the system enables efficient and secure inference on encrypted user data without requiring decryption. Experimental results demonstrate that PP-AICloud achieves more than 91% accuracy on a real-world landmark recognition task with real-time latency of under 0.7 seconds. We demonstrate the capabilities of PP-AICloud through demo videos available at:  Demo link.}
}


@inproceedings{DBLP:conf/mobicom/HuHPYYZ25,
	author = {Haiyan Hu and
                  Yandao Huang and
                  Junyao Peng and
                  Liu Yang and
                  Shuangshuo Yang and
                  Qian Zhang},
	title = {Demo: Intelligent Nutrition Monitoring Pump System for Nasogastric
                  Tube Patients},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1169--1171},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765579},
	doi = {10.1145/3680207.3765579},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HuHPYYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nasogastric tube (NGT) feeding supports over 10 million patients globally, particularly those with stroke-induced dysphagia, Parkinson's disease, and cognitive impairments. However, current practices face three fundamental limitations. First, caregivers often lack knowledge of the nutritional content of homemade blended meals, leading to high malnutrition rates among long-term NGT patients. Second, commercial solutions struggle to analyze blended meals in large containers due to light path distortion. Third, gastric residual volume (GRV) assessments rely on subjective nurse evaluations, resulting in incomplete records and increased feeding intolerance. To address these challenges, this demo introduces NutriBump, an innovative system designed to enhance NGT feeding through closed-loop control. Utilizing advanced spectral analysis technology, the system accurately assesses food nutrients and employs multi-modal sensors for automatic gastric fluid aspiration and analysis. By integrating long-term nutritional intake, digestion data, and health status, NutriBump leverages large language models and AI agents to generate personalized nutrition reports automatically. This closed-loop nutrition management solution improves the accuracy of blended meal analysis and reduces reliance on subjective digestion assessments.}
}


@inproceedings{DBLP:conf/mobicom/McDonaldNGKC25,
	author = {Brandon McDonald and
                  Michael Nebeling and
                  Roland Graf and
                  Hun{-}Seok Kim and
                  Jiasi Chen},
	title = {Demo: Networked iGYM for {AR} Exergames},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1172--1174},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765580},
	doi = {10.1145/3680207.3765580},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/McDonaldNGKC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {iGYM is an augmented reality exercise game for inclusive play that allows people with and without wheelchairs to participate equally in a soccer game with a projected virtual field and ball. However, currently iGYM requires all players to be co-located and lacks capabilities for remote play. In this work, we describe a networked iGYM implementation that allows teams of players to play with each other remotely. The key networking challenge is meeting tight end-to-end latency requirements for interactive play over the Internet. We demonstrate a portable tabletop version of iGYM, implemented in Unity and ROS2, using a distributed authority model to keep track of ownership and propagate state updates about players, game objects, and scores. Each client receives local player tracking updates and spawns player peripersonal circles under its own authority; a shared ball object is synchronized under server ownership with clientside interpolation. The demo GUI will let attendees inject artificial network delay to explore its impact on interactivity.}
}


@inproceedings{DBLP:conf/mobicom/DingJZCFZFTK25a,
	author = {Rong Ding and
                  Haiming Jin and
                  Ningzhi Zhu and
                  Zijie Chen and
                  Yi Fu and
                  Fengyuan Zhu and
                  Guiyun Fan and
                  Xiaohua Tian and
                  Linghe Kong},
	title = {Demo: Bluetooth-Enabled Transparent {RF} Sensing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1175--1177},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765581},
	doi = {10.1145/3680207.3765581},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/DingJZCFZFTK25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper demonstrates Serafin, the first full-stack, sub- m W, and versatile Bluetooth-enabled RF sensor that brings transparent RF sensing to mobile and IoT device: it independently conducts the whole RF sensing process from RF signal reception to sensing result computation in a wide variety of sensing tasks with only negligible power consumption. At the core of Serafin are our two designs that address the challenge posed by the stringent sub-mW power constraint to jointly achieving versatility and full stackness. Specifically, (i) we utilize the ambient Bluetooth advertising signal as the signal for sensing, and extract the phase difference of the sensing signals received by each antenna pair from the amplitude of their sum signal, which avoids power-hungry hardware components and intensive computation, and (ii) we employ low-power MCU as the computation hardware, and suppress its power consumption by activating it only when necessary and customizing a light-weight yet versatile neural network model.}
}


@inproceedings{DBLP:conf/mobicom/He0LWDZDLC25,
	author = {Lijuan He and
                  Feng Lyu and
                  Mingliu Liu and
                  Hao Wu and
                  Sijing Duan and
                  Jieyu Zhou and
                  Yi Ding and
                  Zaixun Ling and
                  Yibo Cui},
	title = {Demo: {UAV} Trajectory Generation from Sparse and Noisy 3D Point Clouds},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1178--1180},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765582},
	doi = {10.1145/3680207.3765582},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/He0LWDZDLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {UAV-assisted inspection is critical for modern power grid maintenance, enhancing efficiency and safety in remote areas. However, automatically designing UAV inspection trajectories is challenging due to the cluttered inspection environments, small inspection targets, and pervasive obstacles. We propose a novel method for generating inspection trajectories in noisy, sparse, and complex 3D point cloud. It has three core techniques: (1)  A local structure-enhanced pylon segmentation , which accurately segments pylons, power lines, and surroundings in noisy point cloud for effective inspection target identification and trajectory planning. (2)  A 3D fingerprint-based pylon type recognition  that compensates for point cloud sparsity to complete missing inspection targets based on the pylon type. (3)  An adaptive trajectory generation  that samples positions in response to diverse pylon orientations and pervasive environmental obstacles, ensuring UAV operational safety. A four-month deployment in a power grid inspection system—covering a 270 km 2  primary mountainous area—yielded an expert first-review acceptance rate of 91.86% for the generated trajectories, and reduced design time by an average of 88.19% compared to manual methods, significantly improving inspection efficiency. Demo video and dataset are available at https://ljhe006.github.io/autouit/.}
}


@inproceedings{DBLP:conf/mobicom/SongMK25,
	author = {Yiwen Song and
                  Carmel Majidi and
                  Swarun Kumar},
	title = {Demo: Frequency-Selective Microwave Actuation of Liquid Crystalline
                  Elastomer Soft Robots},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1181--1183},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765583},
	doi = {10.1145/3680207.3765583},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/SongMK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless research has advanced in utilizing channel diversity and beamforming for more efficient communication, sensing, and harvesting ambient energy. We demonstrate our wireless robotic platform that utilizes radio-frequency beamforming for robot actuation. The platform delivers a maximum of 60 watts of power accurately towards soft robotic actuators by efficient frequency-aware beamforming. We also engineer soft actuators to absorb microwaves of specific frequencies to enable selective actuation. In this demonstration, we show a simplified version of our system that achieves frequency-selective actuation of two actuators to enable simple robot locomotion.}
}


@inproceedings{DBLP:conf/mobicom/SunBZNCG025,
	author = {Yutong Sun and
                  Rui Ban and
                  Zhigang Zhong and
                  Zuoyong Nan and
                  Xinmiao Chen and
                  Haoyuan Guo and
                  Jialin Wang},
	title = {Demo: Immersive Digital Twin Networks for Industrial Internet of Things},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1184--1185},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765584},
	doi = {10.1145/3680207.3765584},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/SunBZNCG025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid advancement of the Industrial Internet of Things (IIoT), Digital Twin Networks (DTNs) have emerged as critical enablers for industrial network optimization. However, current full-space industrial DTNs face persistent challenges: neglect of propagation-impacting industrial factors, computational inefficiency, and poor generalization in Artificial Intelligence (AI)-based prediction method. This demo establishes an immersive IIoT-DTNs framework for real logistics scenarios, where cargo stacking rates are considered as the primary industrial dynamic factor. Hence, the 3D scenes reflecting varying stacking rates are generated for Ray Tracing (RT)-based channel modeling. The proposed Adaptive Sampling-based Sparse RT Calculation (AdaSpRT) method strategically selects critical propagation areas for RT calculation, replacing random sampling. Then, AI-based channel prediction can be trained and fine-tuned with the RT data for remaining sample prediction. Evaluations across two distinct environments, high-density shelving vs. open layout, demonstrate 22.0% and 12.4% efficiency improvements over conventional RT while maintaining Mean Absolute Errors (MAE) of 3.44 dB and 1.56 dB respectively.}
}


@inproceedings{DBLP:conf/mobicom/WuLYFW025,
	author = {Chun Ming Wu and
                  Songfan Li and
                  Zhaoteng Ye and
                  Tsz Ho Fan and
                  Lai Yin Garmisch Wong and
                  Mo Li},
	title = {Demo: AeroRelief: UAV-based Emergency Rescue for Time-Critical Missions},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1186--1188},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765585},
	doi = {10.1145/3680207.3765585},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/WuLYFW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present AeroRelief, an autonomous UAV first-responder system for rapid delivery of critical medical supplies in hard-to-reach areas. Integrating AI-assisted dispatch, real-time path planning, and a winch-based payload mechanism, AeroRelief responds swiftly to emergencies with minimal human intervention. The system uses a long-range LoRa command link for robust communication and delivers supplies mid-air without landing. Our demonstration showcases the complete workflow—from emergency identification to UAV deployment—highlighting AeroRelief's potential to accelerate rescue missions and enhance survival outcomes in remote settings.}
}


@inproceedings{DBLP:conf/mobicom/LiuJO25,
	author = {Yejia Liu and
                  Hengle Jiang and
                  Xiaomin Ouyang},
	title = {Demo: FreePose: Real-Time View-Invariant 3D Human Pose Estimation
                  via Motion-View Disentanglement},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1189--1191},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765586},
	doi = {10.1145/3680207.3765586},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiuJO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {3D human pose estimation is a key technology for applications like healthcare and robotics, but its performance in real-world deployments is often compromised by viewpoint variations. We propose FreePose, a novel framework that achieves viewpoint invariance by explicitly disentangling motion and view features. The core of FreePose is a lightweight view estimator that predicts camera viewpoint from intermediate pose features. This information is then used to guide robust feature alignment and enable a view-aware inference pipeline that adaptively optimizes for latency on edge devices. We will demonstrate our system using a single Intel RealSense D435 camera, capturing from varying viewpoints throughout the demo, with real-time pose inference performed on a PC and an NVIDIA Jetson Orin NX. By leveraging FreePose, our framework achieves consistent accuracy and high frame rates across varying camera angles and positions. A video demonstration of FreePose's performance is available at https://youtu.be/tDIpCcbaRXc.}
}


@inproceedings{DBLP:conf/mobicom/ChenC25,
	author = {Xiaomeng Chen and
                  Dongyao Chen},
	title = {Demo: Hijacking Joystick with Mobile Magnetic Injection},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1192--1194},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765587},
	doi = {10.1145/3680207.3765587},
	timestamp = {Tue, 20 Jan 2026 18:49:23 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ChenC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Joystick has been a major interactive controller for a wide range of devices, e.g., entertainment systems and drones. Recently, the Hall-effect joystick has been gaining traction because of its unique advantages in fine-grained control and durability. Despite the popularity of the Hall-effect joystick, we discovered that this emerging technique is susceptible to controllable magnetic field injection. In this demo, we present MagneCon, a mobile, programmable magnetic injection system capable of hijacking commodity Hall-effect joysticks by injecting carefully crafted magnetic fields. We validate our system on a real-world setup and demonstrate two representative attack scenarios: pre-loaded trajectory injection and real-time untethered hijack of the victim's joystick.}
}


@inproceedings{DBLP:conf/mobicom/ChenYZZSC25,
	author = {Jinbo Chen and
                  Yuqin Yuan and
                  Dongheng Zhang and
                  Dong Zhang and
                  Qibin Sun and
                  Yan Chen},
	title = {Demo: All in One RadioCardiogram: Towards Practical and Clinically
                  Reliable Contactless Cardiac Monitoring},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1195--1197},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765588},
	doi = {10.1145/3680207.3765588},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ChenYZZSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio sensing has emerged as a promising contactless revolution for cardiac monitoring. However, considering the complexity of radio propagation, extracting stable and clinically meaningful features remains challenging, posing a barrier to scaling this technology for practical and clinical reliable deployment. In this demo, we present RadioCardiogram, a system that leverages AI-powered knowledge transfer from well-established ECG diagnostic paradigms to accurately interpret complex radio signals. It enables all-in-one cardiac function monitoring including heart rate variability analysis, arrhythmia detection, and ECG-aligned waveform reconstruction. The system is implemented in a mobile phone-sized prototype and validated in a large-scale, clinically oriented cohort involving 6,258 outpatient visitors. Results demonstrate performance approaching the gold standard in both HRV monitoring and arrhythmia detection, highlighting a pathway toward effortless continuous, and reliable cardiac health coverage in real-world usage. The demo video is available at the following link.}
}


@inproceedings{DBLP:conf/mobicom/ChenFJ25,
	author = {Zijie Chen and
                  Guiyun Fan and
                  Haiming Jin},
	title = {Demo: Full-stack On-device Learning for Heterogeneous Tiny Cameras},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1198--1200},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765589},
	doi = {10.1145/3680207.3765589},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ChenFJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tiny cameras are ubiquitous in embedded devices like smart glasses and drones. On-device learning technique empowers these resource-constrained devices to adapt to changing environments locally. However, existing camera modules rely on diverse programming environments and APIs. This heterogeneity impedes the validation and deployment of practical on-device learning algorithms. In this work, we introduce CamOL, the first full-stack on-device learning scheme for heterogeneous tiny cameras. CamOL integrates a complete workflow encompassing preprocessing, learning, inference, and visualization. For learning, CamOL adopts a library-free code design for maximum portability and programmability, and incorporates a proposed staged training method for efficient full-parameter fine-tuning. For inferring, we have developed a suite of operators and achieved operator fusion for minimal latency. We demonstrate CamOL with a compact, low-cost (<$10) prototype that features an engaging and interactive visualization. This allows participants to intuitively experience the entire on-device learning pipeline for different applications in real-time. Moreover, CamOL can also serve as a handy testbed for embedded vision intelligence.}
}


@inproceedings{DBLP:conf/mobicom/ZhaoLWWLLHL025,
	author = {Lingzi Zhao and
                  Feng Lyu and
                  Hao Wu and
                  Huaqing Wu and
                  Huali Lu and
                  Shucheng Li and
                  Tao He and
                  Wenlong Liao and
                  Sheng Zhong},
	title = {Demo: Task Cooperation for Urban Unmanned Sanitation Vehicles},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1201--1203},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765590},
	doi = {10.1145/3680207.3765590},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhaoLWWLLHL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned sanitation vehicles (USVs) promise cleaner cities, yet efficiently coordinating multiple USVs in large urban areas remains challenging due to constraints such as limited waste capacity and battery life. In this demo, we present MRTC, a multi-robot task cooperation system. First,  Dynamic Task Assignment  employs an Actor-Critic policy within a Markov decision framework to allocate cleaning tasks and decide the required number of USVs. Second,  Single-USV Path Planning  refines each route via a fast two-layer iterative search. Over an  eight-month  real-world deployment in three urban testbeds, our MRTC system markedly improved cleaning efficiency while lowering operating costs. Operating over a combined  10,775 km  of routes per month, the system achieved average monthly savings of  20,575 kWh  of energy and  2,744  labour hours. A demonstration video is available at https://llq978.github.io/Demo/.}
}


@inproceedings{DBLP:conf/mobicom/MenonRA25,
	author = {Rohan Menon and
                  Jack Rademacher and
                  Fadel Adib},
	title = {Demo: Leveraging Underwater Backscatter for Long-Term Environmental
                  Sensing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1204--1206},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765591},
	doi = {10.1145/3680207.3765591},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/MenonRA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This demo presents BlueTag, a permanently deployed underwater sensor system based on backscatter communication. BlueTag is a battery-powered CTD (conductivity, temperature, depth) sensor that transmits measurements every 15 minutes to a remote base station via underwater backscatter. The base station archives these measurements and publishes them online. Unlike prior underwater backscatter systems limited to short-term laboratory experiments, BlueTag has been deployed in the Charles River in Boston, MA since July 9th, 2025, marking the first long-term underwater backscatter deployment of its kind to sense meaningful environmental data. Live data from this deployment is available publicly at https://sk-exp-server.mit.edu/.}
}


@inproceedings{DBLP:conf/mobicom/LiO0ZWS25,
	author = {Zuguang Li and
                  Dongyuan Ou and
                  Wen Wu and
                  Songge Zhang and
                  Shaohua Wu and
                  Xuemin (Sherman) Shen},
	title = {Demo: Split-and-Pipeline: Collaborative Large Model Inference on Edge
                  Devices},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1207--1209},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765592},
	doi = {10.1145/3680207.3765592},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiO0ZWS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deploying and executing large model inference on edge devices is challenging due to their limited computational power and memory resources. To address this challenge, we present a novel Split-and-Pipeline, a collaborative inference scheme that partitions a large model into multiple submodels and executes them across distributed edge devices in a pipelined manner. The scheme parallelizes data transfer across multiple CPU cores to avoid transmission bottlenecks. We build a real-world testbed using NVIDIA Jetson series edge devices to demonstrate the proposed scheme, achieving 1.2×–3.0× throughput improvement over state-of-the-art baselines.}
}


@inproceedings{DBLP:conf/mobicom/WangZC25,
	author = {Rouyi Wang and
                  Zhi Zhou and
                  Xu Chen},
	title = {Demo: WasmSD-Edge: {A} Lightweight Edge Stable Diffusion Image Generation
                  Framework Based on WebAssembly},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1210--1212},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765593},
	doi = {10.1145/3680207.3765593},
	timestamp = {Tue, 20 Jan 2026 18:49:23 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/WangZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing demand for deploying Artificial Intelligence Generated Content (AIGC) models like Stable Diffusion on resource-constrained edge devices challenges balancing quality, lightweight implementation, and portability. The emergence of WebAssembly (WASM) offers a compactcross-platform, and isolated runtime environment, making it a promising solution for efficient edge AIGC inference. However, current WASM based AI inference solutions are restricted to text interactions, offering limited support for image generation. To solve the challenges, we propose WebAssembly-Rust based WasmSD-Edge, a lightweight, edge-oriented AI image generation framework for high performance on-device Stable Diffusion inference on various edge devices. WasmSD-Edge employs a plugin-based architecture by integrating stable-diffusion.cpp as a WASM backend plugin for the WasmEdge runtime. It exposes a set of WebAssembly System Interfaces (WASI) to support text-to-image, image-to-image, and model convertion. Additionally, a Rust Crate SDK further enables developers to parametrically control inference process and output generation. To evaluate usability and portability of WasmSD-Edge on heterogeneous devices, we deployed it on heterogeneous devices. It achieves high inference speed and image quality with low resource consumption, offering a practical and efficient solution for deploying edge AIGC workflow. The implementation has been merged into WasmEdge — one of the largest WASM community, and source code are available at: https://github.com/WasmEdge/wasmedge-stable-diffusion.}
}


@inproceedings{DBLP:conf/mobicom/DingZXWZJZXHC25,
	author = {Wenhua Ding and
                  Zhengli Zhang and
                  Xin Xu and
                  Haoyang Wang and
                  Yinan Zhu and
                  Shilong Ji and
                  Xin Zhou and
                  Jingao Xu and
                  Dongyue Huang and
                  Xinlei Chen},
	title = {Demo: HawkEye: Practical In-Flight Obstacle Avoidance with Event Camera
                  and LiDAR Fusion},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1213--1215},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765594},
	doi = {10.1145/3680207.3765594},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/DingZXWZJZXHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Drones are increasingly used in applications such as last-mile delivery and infrastructure inspection, but their safe operation, especially in high-speed scenarios, remains a critical challenge. Existing vision- and LiDAR-based obstacle localization methods suffer from motion blur, latency, and low spatio-temporal resolution, making them inadequate for detecting and tracking fast-moving objects. In this work, we present HawkEye, a drone obstacle avoidance system that fuses event cameras and LiDAR to achieve high-frequency, accurate 3D tracking of dynamic objects. By leveraging the complementary strengths of both sensors, Hawkeye enables robust real-time sensing and safe evasive maneuvers, addressing a key requirement for the large-scale deployment of autonomous drones.  Demo: https://wenhua00.github.io/HawkEye/.}
}


@inproceedings{DBLP:conf/mobicom/KhooiKC25a,
	author = {Xin Zhe Khooi and
                  Anuj Kalia and
                  Mun Choon Chan},
	title = {Demo: Towards Seamless 5G vRAN Software Updates},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1216--1218},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765595},
	doi = {10.1145/3680207.3765595},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/KhooiKC25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present SwapRAN, a live update system that brings Continuous Integration/Continuous Deployment (CI/CD) to virtualized RANs (vRANs), which includes both the Distributed Unit (DU) and the Centralized Unit (CU). In contrast to prior solutions, SwapRAN performs in-place software updates without relying on additional infrastructure such as staging servers or programmable switches. For DU updates, SwapRAN introduces two techniques: (1) leveraging OS thread priorities to safely bring up the new DU while the old DU remains active, and (2) redirecting fronthaul traffic to the new DU using the embedded switch found on modern network interface cards. For CU updates, SwapRAN is the first system to enable live updates, made possible by (1) decoupling the stateful connection between the CU and DU and transparently rerouting DU messages to the new CU, and (2) repurposing existing midhaul control plane messages to transfer users to the new CU. We demonstrate SwapRAN on our O-RAN testbed equipped with a commercial O-RU, showing that it can perform DU or CU updates with significantly reduced downtime, as low as 1–2 seconds, compared to existing update strategies in Kubernetes.}
}


@inproceedings{DBLP:conf/mobicom/DingWJMJWZZW00025,
	author = {Xin Ding and
                  Jianyu Wei and
                  Fucheng Jia and
                  Liang Mi and
                  Ruofei Ju and
                  Xianye Wang and
                  Yikai Zheng and
                  Ziming Zhang and
                  Weijun Wang and
                  Shiqi Jiang and
                  Yunxin Liu and
                  Ting Cao},
	title = {Demo: EdgeMind-OS: {A} Plug-and-Play Embodied Intelligence System
                  for Real-Time On-Device Deployment},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1219--1221},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765596},
	doi = {10.1145/3680207.3765596},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/DingWJMJWZZW00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Building an always-on, contextual AI assistant that proactively supports humans remains a central goal in Embodied AI—yet cloud-based pipelines struggle to meet due to delay, bandwidth, and privacy constraints. This demo presents EdgeMind-OS, a fully on-device intelligence system designed for embodied agents operating in real-world scenarios. Edge-Mind-OS features a hierarchical architecture combining a real-time StreamBrain, modular skill experts, and a dynamic scene-episode memory. Achieving up to 7.3× faster local processing, it enables low-latency, privacy-preserving, and plug-and-play deployment across tasks such as semantic navigation, spatial memory recall, and multimodal interaction. We demonstrate how EdgeMind-OS empowers a mobile robot with only basic locomotion capabilities to perform realtime, free-form user-robot interaction through autonomous perception, reasoning and action —without reliance on external cloud infrastructure.}
}


@inproceedings{DBLP:conf/mobicom/Zhang0ZDS25,
	author = {Sutong Zhang and
                  Haobo Zhang and
                  Shuhao Zeng and
                  Boya Di and
                  Lingyang Song},
	title = {Demo: Amodal Instance Segmentation Using MmWave Radar},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1222--1224},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765597},
	doi = {10.1145/3680207.3765597},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Zhang0ZDS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Amodal sensing enables the shape reconstruction of occluded objects, facilitating a wide range of sensing applications in complex environments. However, traditional amodal sensing methods based on cameras or LiDAR suffer from privacy issues and performance degradation under poor weather conditions. In this demo, we present a wireless amodal sensing system that leverages mmWave signals to improve robustness and protect privacy. The system first segments the obtained mmWave point clouds into individual object instances, and then reconstructs their complete shapes. Unlike camera and LiDAR-based methods, it is challenging to realize wireless amodal sensing due to the measurement errors caused by wireless channel noise and the data sparsity. To address these challenges, we first design an RCS-enhanced error suppression module to mitigate measurement errors by leveraging the negative correlation between radar cross-section (RCS) values and noise. For the data sparsity, we utilize a modified Transformer architecture to extract diverse geometric features at multiple scales, and incorporate a fine-tuned vision-language model (VLM) to generate semantic features that describe object classes. The geometric and semantic features are finally fused to reconstruct complete object shapes using pre-trained generative models. The effectiveness of the proposed system is demonstrated through extensive experiments in real-world scenarios.}
}


@inproceedings{DBLP:conf/mobicom/LiY0TC25,
	author = {Zheng Li and
                  Bernard Yap and
                  Xuechen Zhang and
                  Yi{-}Zhen Tsai and
                  Jiasi Chen},
	title = {Demo: {L3GS:} Layered 3D Gaussian Splats for Efficient 3D Scene Delivery},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1225--1227},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765598},
	doi = {10.1145/3680207.3765598},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiY0TC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we present L3GS, a framework for efficient 3D scene delivery with 3D Gaussian splats [5]. While 3D Gaussian splats achieve a balance between visual fidelity and rendering efficiency, their massive data size still limits realtime deployment. L3GS addresses this by leveraging scene layering, predictive viewport and bandwidth estimation, and priority-based scheduling to progressively deliver the most essential splats first. This demo will showcase L3GS's 3D scene delivery performance under controlled network conditions and compare it with various baselines.}
}


@inproceedings{DBLP:conf/mobicom/0005ZYGL0H00S25,
	author = {Yuhao Chen and
                  Yue Zheng and
                  Yuxuan Yan and
                  Shuowei Ge and
                  Shun Li and
                  Qianqian Yang and
                  Shibo He and
                  Zhiguo Shi and
                  Jiming Chen and
                  Yuanchao Shu},
	title = {Demo: Customizing Transformer-based LLMs via Collaborative Training
                  on Mobile Devices},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1228--1230},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765599},
	doi = {10.1145/3680207.3765599},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0005ZYGL0H00S25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite large language models (LLMs) being an essential part of our lives, training of LLMs still needs to be done in cloud data centers due to the large requirements of data and computing power, leaving fine-tuning pre-trained LLMs on resource-constrained mobile devices remains highly under-explored. In this demo, we present Confidant, a practical collaborative training system that allows modern LLMs to be fine-tuned across multiple off-the-shelf mobile devices. Confidant partitions an LLM into several sub-models, deploying each of them to a mobile device. Multiple mobile devices then collaborate to train the LLM by employing a novel pipeline parallel training approach. Specifically, Confidant encompasses a memory-aware dynamic model partitioning and intra-device multi-processor scheduler to minimize the training time across heterogeneous platforms. A hybrid fault tolerance mechanism is also devised to proactively manage potential device and network failures. By building a cross-framework adapter and fully implementing Confidant on smartphones and laptops, we present the demo of collaborative training on a variety of mobile platforms.}
}


@inproceedings{DBLP:conf/mobicom/LiLZPL0L25a,
	author = {Lingang Li and
                  ZiLong Liu and
                  Yihao Zhang and
                  Siyuan Peng and
                  Zhirong Liu and
                  Yongrui Chen and
                  Zhijun Li},
	title = {Demo: Wide-coverage {LE} Audio via WiFi-BLE Cross-Technology Communication},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1231--1233},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765600},
	doi = {10.1145/3680207.3765600},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiLZPL0L25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth audio faces a major challenge due to its limited transmission range in meeting users' demands. This work introduces WiLE-Audio, a novel approach that extends Low Energy Audio (LE Audio) coverage through cross-technology communication (CTC) from WiFi to BLE. We first present a novel symbol mapping technique to enable all-channel and reliable CTC. Then, we present a real-time reverse scrambling method to implement CTC to commodity WiFi devices. Finally, we design a precise timing strategy and a priority scheduling to align with the strict time window requirements of the BLE receiver. These systematic innovations allow WiLE-Audio to be easily implemented into existing commercial devices with only a simple software upgrade on the WiFi side. Furthermore, we achieve seamless switching between Bluetooth classic audio and WiLE-Audio, thus supporting whole-house audio roaming. We implement our work on commercial WiFi and BLE devices and demonstrate that WiLE-Audio extends the transmission distance of LE Audio more than 2×.}
}


@inproceedings{DBLP:conf/mobicom/RameshN25,
	author = {Eshan Ramesh and
                  Takayuki Nishio},
	title = {LatentCSI: Real-Time Reconstruction of Physical Scenes from WiFi {CSI}
                  via Latent Diffusion},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1234--1236},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765601},
	doi = {10.1145/3680207.3765601},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/RameshN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We demonstrate real-time high-resolution generation of images of the physical environment from WiFi CSI. Our demo is based on LatentCSI, our novel CSI-to-image generation framework that encodes CSI samples into the latent space of a pretrained latent diffusion model (LDM) to produce high-quality images with optional editing & reconstruction capability by text prompts. Our prototype consists of sensor nodes to collect ground truth CSI and camera images, an edge and training server to host and update LatentCSI components, and a client server that serves predicted images to clients. Our use of latent space offers faster training and inference at better quality. This allows for online model training at ~30 samples/sec, and inference with a latency of ~60ms. To the best of our knowledge, this work presents the first real-time demonstration of image generation from WiFi CSI.}
}


@inproceedings{DBLP:conf/mobicom/0047KM25,
	author = {Yang Liu and
                  Fahim Kawsar and
                  Alessandro Montanari},
	title = {Demo: {A} Real-Time Multimodal Sensing and Feedback System for Closed-Loop
                  Wearable Interaction Using OmniBuds},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1237--1238},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765602},
	doi = {10.1145/3680207.3765602},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0047KM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a real-time multimodal sensing and feedback system that enables closed-loop interaction using wearable devices. Our implementation leverages OmniBuds—a pair of true wireless stereo (TWS) earbuds equipped with dual 9-axis inertial measurement units (IMUs), optical heart rate sensors, and skin temperature sensors, one set in each ear. These sensors continuously stream motion and physiological data via Bluetooth Low Energy (BLE) to a mobile computing platform, where lightweight inference models classify user actions and assess physiological states. Based on this analysis, the system provides real-time auditory feedback through the same earbuds, enabling responsive, human-in-the-loop interaction. As a demonstration, we apply this system to an interactive control scenario based on the T-Rex Chrome Dino game, where users control the avatar using head and body motion captured by the earbuds. Jumping and ducking are recognized in real time through IMU signals, while heart rate and skin temperature dynamically modulates the game speed. The system delivers auditory guidance via the OmniBuds' speakers to help users adapt their actions during game-play. This framework demonstrates the potential of multimodal wearable sensing and closed-loop feedback for embodied interaction, real-time behavioral adaptation, and health-aware interactive systems.}
}


@inproceedings{DBLP:conf/mobicom/0011L0WLX25,
	author = {Liying Wang and
                  Qirui Liu and
                  Qing Li and
                  Shangguang Wang and
                  Xuanzhe Liu and
                  Chenren Xu},
	title = {Demo: Emulating Space Computing Networks with {RHONE}},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1239--1241},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765603},
	doi = {10.1145/3680207.3765603},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0011L0WLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement in satellite technology with the adoption of commercial off-the-shelf (COTS) devices and satellite constellation networking has given rise to Space Computing Networks (SCNs). While SCN research is typically conducted on experimental platforms due to high operational costs, the unique challenges of SCNs require special consideration. In this demo, we introduce Rhone, an emulator that bridges these gaps by achieving both satellite- and constellation-level fidelity (the accurate replication of satellite and constellation states, including power, thermal, and network conditions, as well as application performance) while ensuring usability. Rhone adopts a two-phase approach:  i)  an offline phase builds power, thermal, orbit, network, and computation models using real satellite telemetries and hardware-in-the-loop chip mirroring, and  ii)  an online phase executes container-based emulation integrated with these models. Evaluation shows Rhone's power and computation model errors under 5% and thermal model errors within 1.3 – 2.5°C.}
}


@inproceedings{DBLP:conf/mobicom/ZhuSLL0ZWT25a,
	author = {Fengyuan Zhu and
                  Jiaqi Shen and
                  Wenhui Li and
                  Jianyu Luo and
                  Renjie Zhao and
                  Linling Zhong and
                  Bingbing Wang and
                  Xiaohua Tian},
	title = {Demo: ASIC-based Concurrent Backscatter Networks},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1242--1244},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765604},
	doi = {10.1145/3680207.3765604},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhuSLL0ZWT25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ambient IoT (A-IoT) targets battery-free, ultra-low-power connectivity for massive devices, which has been a key focus in 6G standardization by 3GPP. While backscatter communication enables A-IoT, existing solutions struggle to meet its core demands simultaneously: power consumption below 100 μW, communication ranges up to 100 m, and 100+ concurrency. In this demo, we present NanoScatter, the first backscatter network with each tag implemented using our customized backscatter communication ASIC. We propose a nanowatt wake-up receiver design and a sensitivity-driven downlink/uplink modulation mechanism to carry out the ASIC, which enables minimizing the tag's power consumption and long-range communication. NanoScatter supports concurrent communication of 6 IC-based tags with a subcarrier capacity of 512, achieving communication distances of 66 m indoors and 100 m outdoors. The tag consumes 1 μW in idle listening, with the core circuit using 58 nW and 43 μW during communication.}
}


@inproceedings{DBLP:conf/mobicom/LuWD0LW025a,
	author = {Yang Lu and
                  Jie Wang and
                  Xiaoyun Dong and
                  Ziyao Huang and
                  Bingyi Liu and
                  Jen{-}Ming Wu and
                  Jianping Wang},
	title = {Demo: VI-Planning: Infrastructure-Assisted Real-Time Planning Optimization
                  for Autonomous Driving},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1245--1247},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765605},
	doi = {10.1145/3680207.3765605},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LuWD0LW025a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose VI-Planning, an innovative system that leverages the scene-level future occupancy grid maps predicted by the infrastructure as future drivable area references to directly optimize the planning trajectories of autonomous vehicles in real time. Since VI-Planning operates only at the autonomous vehicle's final output stage, without modifying the underlying system architecture, it can be plug-and-play for most autonomous driving systems. Moreover, VI-Planning employs a novel bitwise encoding mechanism to efficiently compress these maps, enabling practical transmission. The experimental results demonstrate that VI-Planning can achieve real-time planning optimization with extremely low bandwidth consumption, significantly enhancing the driving safety of autonomous systems. The source code and video demonstration of VI-Planning are available on GitHub: https://github.com/YANG-Deep/VI-Planning.}
}


@inproceedings{DBLP:conf/mobicom/FergusonPWM25,
	author = {Andrew E. Ferguson and
                  Ujjwal Pawar and
                  Tianxin Wang and
                  Mahesh K. Marina},
	title = {Demo: {A} Campus Scale Private 5G Open {RAN} Testbed},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1248--1250},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765606},
	doi = {10.1145/3680207.3765606},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/FergusonPWM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next generation of mobile networks are embracing disaggregation, reflected by the industry trend towards Open RAN. Private 5G networks are viewed as particularly suitable contenders for adopting Open RAN, owing to their setting, high degree of control, and opportunity for innovation. Motivated by this, we have recently deployed the first of its kind campus-wide, O-RAN-compliant private 5G testbed across the central campus of the University of Edinburgh. We first present the rationale behind our testbed along with an overview of its make-up. Then, we outline our plan to showcase the coverage, flexibility, and the operational view of the testbed from both network side and user perspectives.}
}


@inproceedings{DBLP:conf/mobicom/XuanSV25,
	author = {Celes Chai Jia Xuan and
                  Dhairya Shah and
                  Ambuj Varshney},
	title = {Demo: VisibleBits: Illuminating Mixed Reality with Li-Fi Information
                  Spotlights},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1251--1253},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765607},
	doi = {10.1145/3680207.3765607},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/XuanSV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Light Fidelity utilizes light to enable high-bandwidth, low-interference networking and complements traditional radio-frequency links. We demonstrate that Light Fidelity\'s precise spatial data confinement makes it a natural fit for mixed reality: By exploiting line-of-sight propagation with mixed-reality headset spatial awareness, we create "information spotlights" - which are localized optical zones that provide context-specific data, such as museum exhibit details or retail product information. This avoids radio frequency based localization, which is both computationally intensive and not supported in today\'s commercial headsets. We present VisibleBits, a proof-of-concept Light Fidelity transceiver integrated with a Meta Quest 3 that converts optical signals into a visual serial protocol, overcoming hardware limitations. In our demo, users receive exhibit-specific real-time information, which is then overlaid onto their mixed reality headsets as they enter a Light Fidelity illumination zone.}
}


@inproceedings{DBLP:conf/mobicom/Chen0000H25,
	author = {Jiazheng Chen and
                  Ge Wang and
                  Zhe Chen and
                  Fei Wang and
                  Wei Xi and
                  Jinsong Han},
	title = {Poster: Zero-effort Cross-domain Wireless Respiration Monitoring under
                  Free Body Movement},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1254--1256},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765651},
	doi = {10.1145/3680207.3765651},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Chen0000H25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless respiratory monitoring has garnered significant attention for its potential in various applications. However, existing systems face practical challenges in adapting to new data domains without substantial customization efforts. Current solutions attempt to address this limitation through domain-independent feature extraction or cross-domain feature translation, employing either knowledge-based sensing models or data-driven neural networks. However, these approaches typically require additional data collection or model retraining for new domains, significantly hindering their practical deployment. This paper proposes RF-Carer, a fully zero-effort cross-domain respiration monitoring system. Our key innovation lies in building an explainable propagation model to transform any heterogeneous signals under unknown domains into a unified form in the signal processing layer. To further address accidental irrelevant factors, we propose to align the feature spaces while suppressing the noisy ones with contrastive learning. On this basis, we develop a one-fits-all model that requires only one-time training but can adapt to unknown scenarios with unconstrained user movements, postures, positions,  etc.  To the best of our knowledge, RF-Carer is the first zero-effort cross-domain respiration monitoring work with wireless RF signals and would be a fundamental step toward real-world deployments.Chen}
}


@inproceedings{DBLP:conf/mobicom/GaoJFC025,
	author = {Ge Gao and
                  Yiming Jin and
                  Hao Fu and
                  Kaiyan Cui and
                  Qiang Yang},
	title = {Poster: ChronoBite: Diet Meets Cellular Aging},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1257--1259},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765652},
	doi = {10.1145/3680207.3765652},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/GaoJFC025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Daily eating habits shape our long-term health, but most diet apps focus only on calories or macronutrients and overlook deeper issues like chronic inflammation and its effects on cellular aging. Prior medical literature has demonstrated a significant inverse relationship between Dietary Inflammatory Index (DII) and telomere length (TL), a key marker of cellular age. Inspired by this finding, we design ChronoBite, a closed-loop feedback system that pairs real-time inflammation scores with periodic cellular aging insights. In addition to regular calorie tracking, it uses fast-changing DII signals and slow-moving cellular aging markers to guide users toward age-aware eating habits. ChronoBite is a mobile-based prototype that combines food recognition, DII analysis, and cellular aging insights to deliver age-aware dietary feedback. Powered by large language models (LLMs), it offers real-time recommendations while supporting long-term tracking of inflammation patterns and telomere dynamics.}
}


@inproceedings{DBLP:conf/mobicom/DengZC25,
	author = {Hongyu Deng and
                  Jiangyou Zhu and
                  He Chen},
	title = {Poster: Radar-Enhanced Robotic Material Perception with Vision Language
                  Models},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1260--1262},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765653},
	doi = {10.1145/3680207.3765653},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/DengZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Robotic perception has been significantly advanced by integrating vision with additional sensing modalities such as acoustic and tactile sensors. However, existing methods largely emphasize external object properties, including appearance and geometry, while neglecting internal material attributes (e.g., composition) that are crucial for robust and reliable robotic manipulation. In this work, we propose augmenting robotic systems with radar sensing and introduce CRMaterial, a new camera-radar fusion framework powered by vision language models (VLMs) for accurate object material identification. Preliminary experiments demonstrate that our system improves material identification accuracy by 2.5× compared to a camera-only baseline.}
}


@inproceedings{DBLP:conf/mobicom/Zhang0D025,
	author = {Runting Zhang and
                  Yijie Li and
                  Dian Ding and
                  Yi{-}Chao Chen},
	title = {Poster: Enabling {BLE} Direction Finding Feature Compatible with All
                  Bluetooth Devices},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1263--1265},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765654},
	doi = {10.1145/3680207.3765654},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Zhang0D025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {BLE direction finding provides high-accuracy localization based on Angle-of-Arrival (AoA), but this feature is only available on BLE 5.1+ devices. Billions of existing Bluetooth devices are excluded from direction finding indoor localization systems. We present Bridge that enables direction finding for all Bluetooth versions without any hardware or firmware modifications. Bridge introduces a novel Trigger that mimics communication behaviors of both locators and targets, allowing the locator to extract AoA information from originally unsupported devices. We implement Bridge on COTS direction finding system and evaluate it on 10+ BLE devices, achieving a median localization error of 33.4 cm.}
}


@inproceedings{DBLP:conf/mobicom/Zheng0WC25,
	author = {Peirong Zheng and
                  Wenchao Xu and
                  Haozhao Wang and
                  Jinyu Chen},
	title = {Poster: Coda: Context-aware Acceleration for Distributed {LLM} Decoding
                  in Edge},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1266--1268},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765655},
	doi = {10.1145/3680207.3765655},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Zheng0WC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The inference of large language models (LLMs) on distributed edge devices is crucial for privacy-preserving applications. However, its performance is severely degraded in a practically lossy edge network due to frequent synchronization. In this paper, we propose Coda, a novel  co ntext-aware  d istributed  a cceleration framework tailored for packet loss scenarios. We observe the consistency of sparse patterns of neuron group activations in the same context. By identifying sparsity patterns during the prefill phase and updating neuron groups mapping strategically, we enable a relaxed synchronization for decoding, without slowing down, while preserving accuracy. Our approach offers up to 5.62x speedup, significantly outperforming the state-of-the-art in various scenarios.}
}


@inproceedings{DBLP:conf/mobicom/Xie025,
	author = {Liang Xie and
                  Wenke Huang},
	title = {Poster: Automatically Generating High-Precision Simulated Road Networking
                  in Traffic Scenario},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1269--1271},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765656},
	doi = {10.1145/3680207.3765656},
	timestamp = {Tue, 20 Jan 2026 18:49:23 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Xie025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing lane-level simulation road network generation is labor-intensive, resource-demanding, and costly due to the need for large-scale data collection and manual post-editing. To overcome these limitations, we propose automatically generating high-precision simulated road networks in traffic scenario, an efficient and fully automated solution. Initially, real-world road street view data is collected through open-source street view map platforms, and a large-scale lane line dataset is constructed to provide a robust foundation for subsequent analysis. Next, an end-to-end lane line detection approach based on deep learning is designed, where a neural network model is trained to accurately detect the number and spatial distribution of lane lines in street view images, enabling automated extraction of lane information. Subsequently, by integrating coordinate transformation and map matching algorithms, the extracted lane information from street views is fused with the foundational road topology obtained from open-source map service platforms, resulting in the generation of a high-precision lane-level simulation road network. This method significantly reduces the costs associated with data collection and manual editing while enhancing the efficiency and accuracy of simulation road network generation. It provides reliable data support for urban traffic simulation, autonomous driving navigation, and the development of intelligent transportation systems, offering a novel technical pathway for the automated modeling of large-scale urban road networks.}
}


@inproceedings{DBLP:conf/mobicom/Wang0WF25,
	author = {Hengtao Wang and
                  Huan Zhou and
                  Zhenning Wang and
                  Xinggang Fan},
	title = {Poster: Diffusion-Driven Stackelberg Games for Semantic Information
                  Trading in Metaverse Systems},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1272--1274},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765657},
	doi = {10.1145/3680207.3765657},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Wang0WF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of 6G and the Metaverse has created a need for efficient real-time data processing and low-overhead communication. To address this challenge, we propose SemCom-MN, a semantic communication-enhanced Metaverse framework integrating an Edge Service Provider (ESP), Edge Sensing Units (ESUs), and Virtual Service Providers (VSPs). ESUs capture physical-world data, ESP manages semantic information, and VSPs create immersive virtual environments. To improve utility under heterogeneous information and computational requirements, we model semantic information trading as a three-stage Stackelberg game and prove the existence of a Nash equilibrium. Furthermore, to overcome high-dimensional dynamics and slow convergence in semantic trading, we develop a Diffusion Game Algorithm (DGA) combining strategic exploration with a game-theoretic denoising mechanism, achieving robust convergence. Simulation results show DGA increases system utility by 8.49%–33.94%.}
}


@inproceedings{DBLP:conf/mobicom/0013K25,
	author = {Jiaqi Liu and
                  Noriaki Kamiyama},
	title = {Poster: Optimized Cache Pollution Attack: {A} DDoS Vector in Content
                  Delivery Networks},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1275--1277},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765658},
	doi = {10.1145/3680207.3765658},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0013K25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Content Delivery Network (CDN) enhances Internet scalability and resilience against traditional Distributed Denial-of-Service (DDoS) attacks by deploying cached edge servers close to end-users. However, CDNs remain vulnerable to unique attack vectors, such as cache pollution attack (CPA). While existing research focuses on attack detection and mitigation, this paper investigates optimized CPA strategies from an adversarial perspective, specifically targeting CDN origin servers to bypass edge server defenses and amplify DDoS impacts. We analyze the attack mechanisms, evaluate performance degradation under varying CPA parameters, and quantify the scalability of this DDoS vector. Experimental results demonstrate that optimized CPA can exhaust origin server resources with fewer malicious requests, highlighting a pressing need for robust CDN architectural countermeasures.}
}


@inproceedings{DBLP:conf/mobicom/XieLT025,
	author = {Liang Xie and
                  Yanting Li and
                  Luyang Tang and
                  Wei Gao},
	title = {Poster: Efficient Geometry Compression and Communication for 3D Gaussian
                  Splatting Point Clouds},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1278--1280},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765659},
	doi = {10.1145/3680207.3765659},
	timestamp = {Tue, 20 Jan 2026 18:49:23 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/XieLT025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As dynamic 3D scene representations grow increasingly complex, the exponential expansion of 3D Gaussian data creates significant storage and transmission bottlenecks, resulting in excessive memory demands. To address this issue, we propose adopting the AVS PCRM reference software for efficient compression of Gaussian point cloud geometry data. The strategy deeply integrates the advanced encoding capabilities of AVS PCRM into the i3DV platform, forming technical complementarity with the original rate-distortion optimization mechanism based on binary hash tables. On one hand, the hash table efficiently caches inter-frame Gaussian point transformation relationships, which allows for high-fidelity transmission within a 40 Mbps bandwidth constraint. On the other hand, AVS PCRM performs precise compression on geometry data. Experiment demonstrate that the framework maintains the advantages of fast rendering and high-quality synthesis in 3D Gaussian technology while achieving significant 10%-25% bitrate savings on universal test dataset. It provides a superior rate-distortion tradeoff solution for the transmission and interaction of volumetric video.}
}


@inproceedings{DBLP:conf/mobicom/0002LZK0T25,
	author = {Boyi Liu and
                  Shuyuan Li and
                  Zimu Zhou and
                  Shuo Kang and
                  Yiming Ma and
                  Yongxin Tong},
	title = {Poster: Asynchronous Federated Learning Library and Benchmark with
                  AFL-Lib},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1281--1283},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765660},
	doi = {10.1145/3680207.3765660},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0002LZK0T25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Asynchronous Federated Learning (AFL) emerges as a practical paradigm for collaborative model training across IoT devices with heterogeneous compute capabilities, network bandwidth, and online availability. Yet AFL research still lacks a unified, easy-to-use platform for reproducible experimentation under such system configurations. We present AFL-Lib, the first open-source library and benchmark de-signed for AFL. It allows researchers to flexibly configure device types, network conditions, and availability patterns to emulate the system heterogeneity that gives rise to model staleness, a key factor affecting AFL algorithm design. Beyond system-level settings, AFL-Lib supports plug-in modules for personalized and multi-modal federated training, enabling exploration of the interplay between system and data heterogeneity. AFL-Lib implements 10 state-of-the-art AFL algorithms and 4 synchronous baselines, and integrates 12 datasets spanning image, text, and sensor. We will continue to expand AFL-Lib with new algorithms, datasets, and features to support ongoing AFL research. All code and data are publicly available at https://github.com/boyi-liu/AFL-Lib.}
}


@inproceedings{DBLP:conf/mobicom/XingD0H25,
	author = {Ke Xing and
                  Yanjie Dong and
                  Xiaoyi Fan and
                  Xiping Hu},
	title = {Poster: Learning to Personalize in Federated Networks with Contribution-Aware
                  Aggregation},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1284--1286},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765661},
	doi = {10.1145/3680207.3765661},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/XingD0H25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized Federated Learning (PFL) targets client-specific models under heterogeneous and limited data. However, conventional methods often use heuristic or data-size-based averaging and overlook the true contributions of client updates. We propose a contribution-oriented PFL framework that quantifies client contributions via gradient alignment and prediction discrepancy for informed aggregation. We further develop a parameter-wise personalization mechanism for adaptive local updates and a mask-aware momentum optimizer for stable training. Preliminary results on CIFAR10 validate its effectiveness.}
}


@inproceedings{DBLP:conf/mobicom/WangZDY25,
	author = {Zhicheng Wang and
                  Shihan Zhao and
                  Donghui Dai and
                  Lei Yang},
	title = {Poster: Bridging Autonomy and Connectivity: Enabling Smart Vehicle
                  Communication via Full Self-Driving Models},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1287--1289},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765662},
	doi = {10.1145/3680207.3765662},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/WangZDY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous vehicles rely on Full Self-Driving (FSD) models for perception and trajectory planning. However, their wireless communication systems face significant challenges in dynamic environments, particularly evident in beamforming dependent vehicle-to-satellite links. Motivated by the powerful sensing capabilities of autonomous vehicles, we introduce Full Smart-Communication (FSC), a unified framework that reuses intermediate outputs from FSD pipelines—such as planned trajectories, occupancy maps, and 3D posture estimates—to model the spatio-temporal radio environment. To avoid accessing raw sensor data directly, FSC leverages processed semantic and kinematic information to anticipate channel conditions and proactively compute beamforming parameters ahead of time. Tested in realistic driving datasets, FSC improves signal strength by up to 15 dB. FSC bridges the gap between autonomous perception and robust communication, enabling proactive, low-latency, and energy-efficient connectivity.}
}


@inproceedings{DBLP:conf/mobicom/KlosowskiRNKS25,
	author = {Grzegorz Klosowski and
                  Tomasz Rymarczyk and
                  Konrad Niderla and
                  Marcin Kowalski and
                  Manuchehr Soleimani},
	title = {Poster: Smart {ECG} Classification with Wearable Sensing and Cloud
                  {AI:} {A} Mobile Health Approach Using Multi-Feature Time Series},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1290--1292},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765663},
	doi = {10.1145/3680207.3765663},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/KlosowskiRNKS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce a wearable-based system for real-time ECG anomaly detection and contextual interpretation within a mobile-health framework. Twenty-four-hour Holter ECG data are synchronized over wireless/mobile networks with e.g. Apple Health streams (iPhone + iWatch), including activity states (walking, running, resting, sleeping) and heart rate history. A hybrid preprocessing pipeline extracts instantaneous frequency (Hilbert), spectral entropy, and RMS energy, concatenated into fixed-length multichannel tensors for deep-learning models deployed via edge or cloud SaaS. The model detects critical cardiac anomalies correlating each with user activity and exertion context. This multimodal approach distinguishes physiological deviations during motion from pathological events at rest or sleep and suppresses motion artifacts. Experiments with subjects wearing both Holter and Apple devices demonstrate improved sensitivity and specificity versus ECG-only baselines. Our system exemplifies wearable computing, mobile health, ML-enabled mobile systems, and edge/cloud mobile analytics. Fig. 1 shows a complete system for recording and classifying ECG signals, including a Holter ECG with electrodes, a smartphone and a smartwatch [1].}
}


@inproceedings{DBLP:conf/mobicom/ZhuangDH25,
	author = {Yilun Zhuang and
                  Tianwei Deng and
                  Xiaoxia Huang},
	title = {Poster: PanoGlass: Optically Transparent Panoramic Metasurface for
                  Sub-6 GHz Network},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1293--1295},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765664},
	doi = {10.1145/3680207.3765664},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhuangDH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compared with conventional metasurfaces, optically transparent metasurfaces can be coated on glass facades so that the impinging signal is steered to the desired direction. These characteristics make them well-suited for simultaneous transmission and reflection beamforming, breaking the constraint of transmit/reflect-only beamforming that limits service to single side of metasurface. In this paper, we propose Pano-Glass, an optically transparent metasurface enables seamless panoramic coverage in both signal propagation and visual experience. By configuring the reflection/transmission states and phase responses of the unit cells designed based on extended Huygens' metasurface principle, PanoGlass achieves simultaneous control of reflective/transmissive signals. Assembling metal mesh patterns onto a glass substrate, PanoGlass attains an optical transparency of 89.5%, which satisfies the demand of a variety of applications. We develop a 0.472×0.472 m 2  prototype with 24×24 unit cells. Our experimental evaluation demonstrates that PanoGlass achieves received signal strength enhancements of 13.9 dB and bit error rate decrease of 98% compared with ordinary glass.}
}


@inproceedings{DBLP:conf/mobicom/KikuchiSYZ025,
	author = {Takamasa Kikuchi and
                  Koki Shibata and
                  Keiichi Yasumoto and
                  Jinxiao Zhu and
                  Yin Chen},
	title = {Poster: Evaluating Effectiveness of Temporal Features and {DTW} Distance
                  for Radio Frequency Fingerprinting},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1296--1298},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765665},
	doi = {10.1145/3680207.3765665},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/KikuchiSYZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio frequency fingerprinting (RFF) is a technique that identifies wireless devices by exploiting unique hardware-induced imperfections measured in their transmitted RF signals. We propose a lightweight RFF method by leveraging temporal variations in RF signals. By combining multi-scale feature extraction through coarse and fine segmentation with DTW-based similarity features, our approach achieves an accuracy of 0.9882 and a Macro-F1 score of 0.988 in our experiment using the Wi-Fi RF data collected from 120 devices.}
}


@inproceedings{DBLP:conf/mobicom/ZhengXL25,
	author = {Haotian Zheng and
                  Xiangyu Xu and
                  Zhen Ling},
	title = {Poster: Object-Aware Vibration Fusion: Leveraging Frequency Response
                  Diversity for Through-Wall Eavesdropping},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1299--1301},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765666},
	doi = {10.1145/3680207.3765666},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhengXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional mmWave-based acoustic eavesdropping systems primarily focus on sensing techniques, overlooking the physical characteristics of the objects being sensed. In this work, we propose Object-Aware Vibration Fusion, a through-wall eavesdropping system that leverages the frequency response diversity of everyday objects to enhance speech reconstruction. Instead of treating environmental surfaces as generic reflectors, we model and exploit their distinct resonance patterns, which respond selectively to different frequency bands of speech. By capturing and fusing vibrations from multiple objects through a frequency-aware fusion framework, our system constructs a richer and more intelligible representation of the original audio. Experimental results show that this object-centric approach significantly improves speech intelligibility and quality across varying conditions, highlighting the power of frequency response diversity in passive acoustic sensing.}
}


@inproceedings{DBLP:conf/mobicom/HuangLLCP0W25,
	author = {Yandao Huang and
                  Cong Li and
                  Chenggao Li and
                  Lin Chen and
                  Junyao Peng and
                  Qian Zhang and
                  Kaishun Wu},
	title = {Poster: Contactless Cardiovascular Hemodynamics Inference and Hypertension
                  Detection via Ballistocardiogram},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1302--1304},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765667},
	doi = {10.1145/3680207.3765667},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HuangLLCP0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hypertension affects 1.28 billion adults, but blood pressure alone is insufficient for accurate hypertension detection. In this paper, we identified a range of multi-dimensional hemodynamics as novel biomarkers that can offer more comprehensive cardiovascular insights for detecting hypertension. We design a novel system that collects ballistocardiogram (BCG) signals from an optic fiber sensor mat. It features a multi-task, multi-branch, unsupervised domain adaptation learning framework, enabling simultaneous prediction of five hemodynamic biomarkers for hypertension detection. In a 4-month trial with 85 subjects, Hyde achieved 97.65% average accuracy for hypertension detection.}
}


@inproceedings{DBLP:conf/mobicom/AsadiBWWK25,
	author = {Navidreza Asadi and
                  Halil Ibrahim Bengu and
                  Lars Wulfert and
                  Hendrik W{\"{o}}hrle and
                  Wolfgang Kellerer},
	title = {Poster: Road to Tiny Reality: Digital Twins for Decentralized {AI}
                  on Microcontrollers},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1305--1307},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765668},
	doi = {10.1145/3680207.3765668},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/AsadiBWWK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work presents a two-stage digital twin methodology for developing and validating DFL algorithms on resource-constrained microcontrollers. The first stage, our simulation-based twin, enables rapid prototyping and algorithm exploration without hardware constraints, while the second stage, based on leveraging several hardware emulation instances in a containerized environment, provides hardware-aware validation under realistic conditions including network delays, resource limitations, and communication protocols. This approach bridges the critical gap between research and deployment, enabling performance analysis at a pace impractical with physical hardware alone. We demonstrate how this digital twin pipeline is essential for robust Machine Learning Operations (MLOps) in IoT environments, allowing for scalable, cost-effective testing of decentralized tiny ML. Our results across simulation, emulation, and a cluster of real ESP32-S3 microcontrollers show that our twins faithfully reproduce physical device behavior, making it a valuable framework for advancing tiny, decentralized AI.}
}


@inproceedings{DBLP:conf/mobicom/Zhang0L25,
	author = {Haixin Zhang and
                  Xiangyu Xu and
                  Zhen Ling},
	title = {Poster: KeyRadar: Contactless Touchscreen Keystroke Inference via
                  mmWave Sensing and Language Models},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1308--1310},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765669},
	doi = {10.1145/3680207.3765669},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Zhang0L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present KeyRadar, a contactless keystroke inference system that leverages mmWave radar to capture fine-grained 2D motion signals from virtual keypresses on touchscreen devices. Unlike existing visual or acoustic side-channel methods, KeyRadar uses a MIMO radar array to sense subtle back-surface vibrations and employs a hybrid CNN-Transformer model for accurate single-key recognition. To reconstruct full text input, it combines a Tire-based decoding algorithm with a large language model for semantic correction. Evaluated on a nine-key touchscreen in various user conditions, KeyRadar achieves more than 77% single-key precision and 0.8 + semantic similarity, demonstrating a practical and stealthy threat to input privacy.}
}


@inproceedings{DBLP:conf/mobicom/Zhang0SWHW25,
	author = {Pinpin Zhang and
                  Yanbing Yang and
                  Yimao Sun and
                  Di{\'{e}} Wu and
                  Yiling Huang and
                  Jiacheng Wu},
	title = {Poster: Towards Backbone-Free {VLC} Networking via {NLOS} Optical
                  Channels},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1311--1313},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765670},
	doi = {10.1145/3680207.3765670},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Zhang0SWHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing VLC backhaul solutions typically rely on rigid wired ( e.g. , Ethernet or power-line) or alignment-sensitive LOS links for inter-attocell connectivity, which renders the overall network vulnerable to backbone link failures. In this poster, we introduce a novel network architecture that exploits the inherent non-line-of-sight (NLOS) optical channels between adjacent attocells to enable inter-attocell communication. In particular, we design a chirp signal based on chirp spread spectrum (CSS) modulation to enhance the robustness of communication under low signal-to-noise ratio (SNR) conditions, along with a two-stage window alignment approach to achieve precise synchronization while reducing real-time decoding latency. The potential and feasibility of the newly suggested approach are demonstrated through implementations on both ESP32 and FPGA platforms.}
}


@inproceedings{DBLP:conf/mobicom/ZhangLSDHC25,
	author = {Zhengli Zhang and
                  Xinyu Luo and
                  Yucheng Sun and
                  Wenhua Ding and
                  Dongyue Huang and
                  Xinlei Chen},
	title = {Poster: Skyshield: Event-Driven Submillimeter Thin Obstacle Detection
                  for Drone Flight Safety},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1314--1316},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765671},
	doi = {10.1145/3680207.3765671},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhangLSDHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Drones operating in complex environments face a significant threat from thin obstacles, such as steel wires and kite strings at the submillimeter level, which are notoriously difficult for conventional sensors like RGB cameras, LiDAR, and depth cameras to detect. This paper introduces SkyShield, an event-driven, end-to-end framework designed for the perception of submillimeter scale obstacles. Drawing upon the unique features that thin obstacles present in the event stream, our method employs a lightweight U-Net architecture and an innovative Dice-Contour Regularization Loss to ensure precise detection. Experimental results demonstrate that our event-based approach achieves mean F1 Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment on edge and mobile platforms.}
}


@inproceedings{DBLP:conf/mobicom/WangC25,
	author = {Shengqian Wang and
                  He Chen},
	title = {Poster: Fast and Precise Compression of LiDAR Range Images for Real-Time
                  Streaming},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1317--1319},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765672},
	doi = {10.1145/3680207.3765672},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/WangC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The range image has emerged as a dominant representation of 3D LiDAR data, enabling the direct application of well-established image and video compression techniques. However, existing compression methods, primarily optimized for human visual perception, often compromise the fidelity of physical distance information embedded in range images, which is critical for downstream robotic tasks. Additionally, rate-distortion optimization (RDO)-based rate control remains largely unexplored in range image-based LPCC. To address these limitations, we introduce D-Compress, a new framework for detail-preserving, fast, and precise compression of LiDAR range images tailored for real-time streaming. D-Compress focuses on preserving fine-grained range image details while achieving both high compression speed and geometric accuracy. Compared to state-of-the-art (SOTA) codecs, our approach delivers superior geometric precision, high compression ratios, and robust rate control.}
}


@inproceedings{DBLP:conf/mobicom/ErgencAAD25,
	author = {Doganalp Ergen{\c{c}} and
                  Ahmed Abdulfattah and
                  Ahmed Hasan Ansari and
                  Falko Dressler},
	title = {Poster: Towards Open Wireless Time-sensitive Networking in Linux},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1320--1322},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765673},
	doi = {10.1145/3680207.3765673},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ErgencAAD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of IEEE 802.1 Time-sensitive Networking (TSN) with wireless technologies promises to support realtime applications over hybrid networks. Although TSN over Ethernet is well established, extending its capabilities to WiFi remains a technical challenge due to hardware and protocol differences. In this work, we integrate a core TSN scheduling mechanism, IEEE 802.1Qbv Time-aware Shaper (TAS), with WiFi interfaces in the Linux kernel. Our approach enables scheduling for mixed-criticality traffic over wired and wireless links using standard Linux tools. We provide our implementation and evaluation scenarios as open source to support further research in TSN-WiFi integration.}
}


@inproceedings{DBLP:conf/mobicom/LiLXTND25,
	author = {Zhidu Li and
                  Dongsheng Luo and
                  Qing Xue and
                  Yue Tian and
                  Zhengwei Ni and
                  Mingliang Deng},
	title = {Poster: FedFNS: {A} Robust Federated Learning Method for Data Shift
                  over Unreliable Communication Links},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1323--1325},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765674},
	doi = {10.1145/3680207.3765674},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiLXTND25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a novel FL method called FedFNS, which integrates feature norm regularization and statistical aggregation. Specifically, a local model correction strategy is proposed to reduce client bias through incorporating a regularization term into the loss function. To mitigate the negative effects of unreliable communications on global model performance, a statistical weighted aggregation strategy is proposed, which leverages the transmission success probability of each client. The effectiveness of FedFNS is validated through extensive experiments, demonstrating its superiority over several classic FL methods in terms of accuracy.}
}


@inproceedings{DBLP:conf/mobicom/XuL0LW25,
	author = {Xiaowen Xu and
                  Xiaosi Liu and
                  Zhidan Liu and
                  Zhenjiang Li and
                  Kaishun Wu},
	title = {Poster: Diffusion-Driven Spatio-Temporal Modeling of Cellular Traffic
                  Generation},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1326--1328},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765675},
	doi = {10.1145/3680207.3765675},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/XuL0LW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate modeling of traffic demand through cellular traffic generation is crucial for optimizing base station deployment. We thus present STOUTER, a  S patio -T emporal diffusi O n model for cell U lar  T raffic gen ER ation. To effectively capture spatial and temporal dynamics, we pretrain both a temporal graph and a base-station graph, and introduce a Spatio-Temporal Feature Fusion Module (STFFM). On five datasets from two regions, STOUTER reduces Jensen-Shannon Divergence by 52.8% over prior methods, generating distributions that closely match real traffic and aiding downstream planning tasks.}
}


@inproceedings{DBLP:conf/mobicom/XueML0BM25,
	author = {Leyang Xue and
                  Meghana Madhyastha and
                  Myungjin Lee and
                  Amos Storkey and
                  Randal C. Burns and
                  Mahesh K. Marina},
	title = {Poster: On Harnessing Idle Compute at the Edge for Foundation Model
                  Training},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1329--1331},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765676},
	doi = {10.1145/3680207.3765676},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/XueML0BM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The significant computational demands for foundation model training have led to a growing concern on the centralization of the ecosystem behind their development. Although training with volunteered edge devices can democratize the process, existing approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. We introduce a new communication-efficient paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Our evaluations show that we can match cloud-based GPU training using a large collection of edge devices, achieving 4–10x speedup over edge methods.}
}


@inproceedings{DBLP:conf/mobicom/HuangSMZ25,
	author = {Xinyu Huang and
                  Leming Shen and
                  Zijing Ma and
                  Yuanqing Zheng},
	title = {Poster: Towards Privacy-Preserving and Personalized Smart Homes via
                  Tailored Small Language Models},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1332--1334},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765677},
	doi = {10.1145/3680207.3765677},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HuangSMZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Language Models (LLMs) exhibit remarkable language comprehension to revolutionize smart homes. Existing LLM-based smart home assistants typically transmit user commands, along with user profiles and home configurations, to remote servers to obtain personalized services. However, users are increasingly concerned about potential privacy leakage. To address this, we develop  HomeLLaMA , an on-device assistant for privacy-preserving personalized smart homes with a tailored small language model (SLM).  HomeLLaMA  learns from cloud LLMs to deliver satisfactory responses and enable user-friendly interactions. Once deployed,  HomeLLaMA  facilitates proactive interactions by continuously updating local SLMs and user profiles. To further enhance user interaction while protecting privacy, we develop  PrivShield  to offer an optional privacy-preserving serving for those users who are unsatisfied with local responses and willing to send less-sensitive queries to remote servers. Experiments demonstrate  HomeLLaMA  provides satisfactory services while significantly enhancing user privacy.}
}


@inproceedings{DBLP:conf/mobicom/PardoARJFPC25,
	author = {Enric Pardo and
                  Giuseppe Avino and
                  Uwe Roth and
                  Mahmoud Jaziri and
                  S{\'{e}}bastien Faye and
                  Aldo Pallotta and
                  Jean{-}Paul Casel},
	title = {Poster: 5G-CriCo Vision for MCx on 5G Standalone\}},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1335--1337},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765678},
	doi = {10.1145/3680207.3765678},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/PardoARJFPC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the vision, technical approach, and experimentation plan of the 5G-CriCo project, which aims to design, implement, and validate Mission-Critical Communications (MCx) services over 5G Standalone (5G SA) public networks in Luxembourg. The project leverages 3GPP-compliant features such as network slicing, Ultra-Reliable Low-Latency Communication (URLLC), and QoS management to meet the stringent requirements of public safety and industrial operations. The work presented include open challenges and an approach to address them, to be shared and discussed with the community.}
}


@inproceedings{DBLP:conf/mobicom/ChenWY25,
	author = {Lien{-}Wu Chen and
                  Yu{-}Jie Wang and
                  Hao{-}Hsiang Yang},
	title = {Poster: iHead: {A} Head-Mounted Individual Monitoring and Finding
                  System Based on Internet of Things Technologies},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1338--1340},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765679},
	doi = {10.1145/3680207.3765679},
	timestamp = {Tue, 20 Jan 2026 18:49:23 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ChenWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper designs and implements a head-mounted individual monitoring and finding system, called iHead, to achieve proactive individual safety and assistance services through Internet of Things (IoT) technologies. The iHead system utilizes inertial data to monitor individual behavior and detects incidents (e.g., falls, crashing accidents, head hitting, etc.) based on deep learning. The individual activity recognition model is developed based on heterogeneous learning attention of long short-term memory, feedforward neural networks, and densely connected neural networks to accurately identify individual status. In addition, in daily use, iHead can recognize bad sitting posture and provide suggestions for correcting the sitting posture. Furthermore, in hazardous situations, iHead can automatically issue alerts and actively track the target individual in a crowdsourced sensing manner. To validate the feasibility of iHead, an Android-based prototype with the head wearable device for individual monitoring and finding has been implemented.}
}


@inproceedings{DBLP:conf/mobicom/FanSCV25,
	author = {Enguang Fan and
                  Emerson Sie and
                  Federico Cifuentes{-}Urtubey and
                  Deepak Vasisht},
	title = {Poster: Scalable Indoor Localization with Non-Cooperative Wi-Fi Ranging},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1341--1343},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765680},
	doi = {10.1145/3680207.3765680},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/FanSCV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate, ubiquitous indoor localization has long been a central goal in wireless systems, yet most proposed methods remain impractical for large-scale deployment. We present PeepLoc, a scalable Wi-Fi-based system that leverages existing infrastructure and unmodified mobile devices. PeepLoc operates in any indoor space with standards-compliant Wi-Fi APs and regular pedestrian traffic. It combines (a) extracting non-cooperative time-of-flight (ToF) from any AP, and (b) a crowdsourced bootstrapping approach using pedestrian dead reckoning (PDR) to localize APs as anchors. Implemented on commodity hardware, PeepLoc is evaluated across four buildings, achieving 3.41m mean and 3.06m median error, outperforming commercial indoor localization systems and approaching GPS-level accuracy outdoors.}
}


@inproceedings{DBLP:conf/mobicom/NgLPSDL25,
	author = {Calvin Alexander Ng and
                  Sage Lyon and
                  Sheree Pagsuyoin and
                  Paul J. Schofield and
                  Arun K. Dhar and
                  Yan Luo},
	title = {Poster: Networked Multimodal Sensor Framework for Shrimp Health and
                  Behavior Analysis},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1344--1346},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765681},
	doi = {10.1145/3680207.3765681},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/NgLPSDL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study presents a multimodal sensing and deep learning framework to enhance monitoring shrimp health and behavior. By integrating real-time water quality sensors, acoustic monitoring, and computer vision, we track key parameters closely tied to feed consumption in healthy and diseased conditions in shrimp. Over a 5-week growth period, we analyzed shrimp feeding behavior through acoustic signals, and performed a controlled disease challenge to identify unique patterns associated with healthy and diseased condition in shrimp. The resulting data can help offer actionable insights for farm operators to develop cost effective feeding strategies and reduce cost associated with aquafeed use for both commercial applications and small individual farm owners.}
}


@inproceedings{DBLP:conf/mobicom/HarishaHE25a,
	author = {Skanda Harisha and
                  Jimmy G. D. Hester and
                  Aline Eid},
	title = {Poster: Vibration-Tolerant Doppler Disambiguation Algorithm for {MIMO}
                  Backscatter Localization Systems},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1347--1349},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765682},
	doi = {10.1145/3680207.3765682},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/HarishaHE25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reliable 3D localization in dynamic and cluttered environments remains a significant challenge, particularly in indoor scenarios. DragonFly demonstrated the feasibility of accurate 3D localization using mmWave tags and a single commercial MIMO radar. However, in dynamic conditions, elevation estimates are often corrupted by outliers, especially during rapid accelerations(mostly due to vibrations) that exceed the system's maximum unambiguous radial acceleration threshold. To address this issue, we propose a novel correction framework that leverages a hypothesis testing procedure to detect and correct elevation outliers. The method tracks multiple candidate elevation trajectories over time, detects inconsistencies using a zig-zag pattern analysis, and resolves ambiguity through a likelihood ratio test based on elevation velocities modeled as a Gaussian distribution. Experiments conducted in highly cluttered environments with drones and mobile tags demonstrate that our method reliably aligns radar estimates with ground truth, successfully correcting 100% of outliers in the evaluated datasets.}
}


@inproceedings{DBLP:conf/mobicom/LiXDL0025,
	author = {Jiarong Li and
                  Chihan Xu and
                  Wenfeng Deng and
                  Xiaojun Liang and
                  Wenbo Ding and
                  Weihua Gui},
	title = {Poster: LightWalk: Passive Gait Recognition via Reflected {VLC} Signals},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1350--1352},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765683},
	doi = {10.1145/3680207.3765683},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiXDL0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A visible light communication (VLC)-based sensing system is proposed for gait recognition and classification. Human-induced reflections are modeled using a time-varying channel representation, enabling the capture of gait dynamics without requiring wearable devices. A low-cost sensing module with embedded processing and multi-channel photodetectors is implemented. Filtered signals are transformed into spectral-spatial features and analyzed using deep learning models. Experiments involving ten participants across eight gait types demonstrate that contrastive and multi-scale models achieve over 98% accuracy, highlighting the potential of VLC-based sensing for unobtrusive, privacy-preserving, and real-time human gait recognition.}
}


@inproceedings{DBLP:conf/mobicom/AngjoTDC25,
	author = {Joana Angjo and
                  Elena Tonini and
                  Falko Dressler and
                  Renato Lo Cigno},
	title = {Poster: mmWave CSI-based Sensing: a Feasibility Study},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1353--1355},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765684},
	doi = {10.1145/3680207.3765684},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/AngjoTDC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Channel state information (CSI) is widely used for joint communication and sensing in sub-6 GHz wireless networks, but its application at mmWave frequencies remains under-explored. This work applies a quantized amplitude-based CSI similarity metric to data from a 28 GHz indoor testbed, showing the feasibility of CSI-based ambient sensing in next-generation wireless networks.}
}


@inproceedings{DBLP:conf/mobicom/MajerekRWK25,
	author = {Dariusz Majerek and
                  Tomasz Rymarczyk and
                  Dariusz W{\'{o}}jcik and
                  Marcin Kowalski},
	title = {Poster: Electrical impedance tomography using a hybrid PINN-ViT model
                  for low-power healthcare systems},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1356--1358},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765685},
	doi = {10.1145/3680207.3765685},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/MajerekRWK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electrical impedance tomography (EIT) is a non-invasive imaging technique with promising applications in mobile and embedded healthcare systems. It reconstructs internal conductivity distributions from boundary voltage measurements but suffers from ill-posedness, sensitivity to noise and limited spatial resolution when solved using traditional iterative methods. This work proposes a hybrid reconstruction architecture that combines a Vision Transformer (ViT) with a Physics-Informed Neural Network (PINN). The transformer extracts global contextual features from voltage measurements, while the PINN enforces the quasistatic conduction equation with Neumann boundary conditions through a physics-informed loss function. Although trained and evaluated on synthetic data, the proposed PINN-ViT model demonstrates improved reconstruction accuracy and noise robustness over classical algorithms and purely data-driven networks. These results indicate its potential for future deployment in energy-efficient, real-time EIT systems for mobile healthcare applications.}
}


@inproceedings{DBLP:conf/mobicom/FanX025,
	author = {Yuang Fan and
                  Xuhai Xu and
                  Xiaofan Jiang},
	title = {Poster: Split-and-Combine Rectification of Ultra-Wide Fisheye Images
                  into Cubemaps},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1359--1361},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765686},
	doi = {10.1145/3680207.3765686},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/FanX025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-wide fisheye cameras (FoV > 180°) offer unmatched scene coverage but introduce severe geometric distortions that degrade vision system performance. Existing rectification methods struggle with such extreme FoVs due to the inherent limitations of single-perspective projections and the scarcity of ground truth data. We propose a novel split-and-combine framework that rectifies ultra-wide fisheye images into 5-face cubemaps. Our approach begins with a lightweight CNN estimating geometric parameters to guide a structured decomposition of the fisheye image into directional regions. Each region is independently corrected using a two-stage pipeline: a flow-based pre-corrector for geometric warping and a diffusion-based enhancer for detail restoration. We train on a synthetic dataset derived from real-world perspective images, enabling partial supervision. Preliminary results demonstrate strong quantitative and qualitative performance, validating our modular architecture as an effective solution for high-fidelity rectification in ultra-wide fisheye imagery.}
}


@inproceedings{DBLP:conf/mobicom/Pal0DK25,
	author = {Shantanu Pal and
                  Saifur Rahman and
                  Robin Doss and
                  Chandan K. Karmakar},
	title = {Poster: BlockFL-Med: Blockchain-Enabled and Lightweight Federated
                  Learning for Smart Medical Spaces},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1362--1364},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765687},
	doi = {10.1145/3680207.3765687},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Pal0DK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a blockchain-enabled lightweight federated learning (BlockFL-Med) framework tailored for smart medical spaces, e.g., the Internet of Medical Things (IoMT), addressing key challenges, e.g., privacy preservation, trust management, and scalability. The framework ensures the privacy of sensitive patient data by employing federated learning, where only model updates are shared instead of raw data. To enhance trust, the framework integrates blockchain technology, creating a decentralized and tamper-proof network that verifies client contributions and mitigates risks from malicious participants. Experimental results demonstrate the scalability and efficiency issues by optimizing communication costs, e.g., transmitting lightweight kilobyte-sized model updates instead of larger megabyte-sized models, making it well-suited for heterogeneous and resource-constrained IoMT environments.}
}


@inproceedings{DBLP:conf/mobicom/LuWD0LWW25,
	author = {Yang Lu and
                  Jie Wang and
                  Xiaoyun Dong and
                  Ziyao Huang and
                  Bingyi Liu and
                  Jen{-}Ming Wu and
                  Jianping Wang},
	title = {Poster: VI-Planning: Infrastructure-Assisted Real-Time Planning Optimization
                  for Autonomous Driving},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1365--1367},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765688},
	doi = {10.1145/3680207.3765688},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LuWD0LWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose VI-Planning, an innovative system that leverages the scene-level future occupancy grid maps predicted by the infrastructure as future drivable area references to directly optimize the planning trajectories of autonomous vehicles in real time. Since VI-Planning operates only at the autonomous vehicle's final output stage, without modifying the underlying system architecture, it can be plug-and-play for most autonomous driving systems. Moreover, VI-Planning employs a novel bitwise encoding mechanism to efficiently compress these maps, enabling practical transmission. The experimental results demonstrate that VI-Planning can achieve real-time planning optimization with extremely low bandwidth consumption, significantly enhancing the driving safety of autonomous systems. The source code and video demonstration of VI-Planning are available on GitHub: https://github.com/YANG-Deep/VI-Planning.}
}


@inproceedings{DBLP:conf/mobicom/ZhangXGD25,
	author = {Yang Zhang and
                  Kaijie Xiao and
                  Yi Gao and
                  Wei Dong},
	title = {Poster: An LLM-Assisted IoT Agent System for Heterogeneous User Tasks},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1368--1370},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765689},
	doi = {10.1145/3680207.3765689},
	timestamp = {Sun, 01 Feb 2026 13:31:38 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ZhangXGD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) systems fundamentally rely on sensing, planning, and execution capabilities. Although existing systems excel at sensing, they often lack advanced abilities of reasoning and planning to generalize across user's diverse and open-ended requests. To address this, we propose IoTAgent, a hierarchical multi-agent system empowered by large language models (LLMs) to enhance intelligence in IoT environments. IoTAgent interprets user's natural language requests to perform heterogeneous tasks such as complex question answering and real-time event triggering, by coordinating high-level task planning and low-level execution and memory management. It integrates sensor data and domain-specific expert knowledge to achieve deeper understanding of the IoT environment. Evaluations on real-world datasets demonstrate that IoTAgent enhances the quality of question answering, and reduces the latency of rule execution by up to 84.3% while balancing energy consumption.}
}


@inproceedings{DBLP:conf/mobicom/ChiaM25,
	author = {Lih Wei Chia and
                  Mehul Motani},
	title = {Poster: High-Performance Optical Camera Communications for {ISAC}
                  Localization and Sensing},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1371--1373},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765690},
	doi = {10.1145/3680207.3765690},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ChiaM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-Performance Optical Camera Communications (HP-OCC) extends conventional Optical Camera Communication (OCC) by combining single-photon avalanche diode (SPAD) sensors with on-sensor edge processing. This enables highspeed optical communication and centimetre-level localization within a single platform, offering a compelling optical-domain alternative for Integrated Sensing and Communication (ISAC) in RF-restricted or congested environments. In collaboration with Kuehne+Nagel, HP-OCC was deployed in an operational warehouse to evaluate two logistics applications: inventory tracking using passive optical tags and warehouse automation via an AGV equipped with HP-OCC for simultaneous video streaming and localization. Passive tags, operating without dedicated power, were reliably identified despite interference from ambient LED lighting, while AGV trials demonstrated stable 5Mbps video transmission and 10cm localization accuracy over 25m. Higher data rates are possible with advancements in SPAD technology.}
}


@inproceedings{DBLP:conf/mobicom/0002LLSGS25,
	author = {Barbara Morawska and
                  Piotr Lipinski and
                  Krzysztof Lichy and
                  Maciej Stepien and
                  Bartlomiej Gryglak and
                  Adam Sztamborski},
	title = {Poster: {A} Real-Time Localization System for Visible and Invisible
                  Fiducial Markers Under Variable Lighting Conditions},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1374--1376},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765691},
	doi = {10.1145/3680207.3765691},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/0002LLSGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fiducial markers play a crucial role in localization, navigation, and tracking systems, particularly in robotics. However, widely used marker detection methods relying on RGB cameras often fail in low-light or no-light environments, where traditional black-and-white markers become ineffective. To address this challenge, we propose a Variable Lighting Conditions Fiducial Marker (VLCFM) system that enhances detection accuracy across diverse lighting conditions, ensuring reliable performance in complete darkness while remaining undetectable to the human eye.}
}


@inproceedings{DBLP:conf/mobicom/LiuZD25,
	author = {Yilai Liu and
                  Shiyuan Zhang and
                  Hongyang Du},
	title = {Poster: Enhancing Mobile Traffic Data Generation through Spatio-temporal
                  Correlation Imaging},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1377--1379},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765692},
	doi = {10.1145/3680207.3765692},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiuZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User-level mobile traffic data is essential for fine-grained network planning, but it is difficult to collect due to privacy concerns and deployment costs. A promising solution is to generate synthetic traffic data, however, existing generative methods fail to recover realistic distributions under extreme sparsity. To address this limitation, we propose Multivariate-Imaged Diffusion (MIDiff), which encodes multivariate mobile-usage and user trajectories data as phase relationships and transforms them into two-dimensional images. By adapting the Gramian Angular Summation Field into a cross-relation computation, MIDiff highlights sparse but important data points as salient image features for diffusion. Experimental results demonstrate that MIDiff achieves higher similarity to real data and reduces temporal consistency error by 72% compared to TTS-GAN.}
}


@inproceedings{DBLP:conf/mobicom/JangCPSK25,
	author = {Kunmin Jang and
                  You Rim Choi and
                  Dongik Park and
                  Hyunwoo Shin and
                  Hyung{-}Sin Kim},
	title = {Poster: Home-based, On-Device, Non-contact Sleep Staging with Infrared
                  Video},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1380--1382},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765693},
	doi = {10.1145/3680207.3765693},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/JangCPSK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sleep is essential for health and well-being, yet the gold-standard method for sleep stage assessment, polysomnography (PSG), requires overnight monitoring with numerous wired sensors and manual scoring, limiting its scalability and comfort in real-world settings. We present  ViSS , the first end-to-end deep learning framework for fully non-contact sleep staging directly from raw infrared video. To capture the dual temporal structure of sleep,  ViSS  integrates per-epoch motion encoding and long-range sequence modeling in a compact hierarchical architecture, without relying on intermediate physiological signals. Evaluated on the largest infrared video dataset to date, including both healthy individuals and patients,  ViSS  achieves  80%  accuracy and  0.78  macro F1-score using fewer than  7 million  parameters. These results establish infrared video as a viable modality for automated sleep staging and highlight the potential of  ViSS  for scalable, privacy-preserving home-based assessment.}
}


@inproceedings{DBLP:conf/mobicom/Dai0ZZC025,
	author = {Yichen Dai and
                  Ming Gao and
                  Ruixuan Zhang and
                  Yuefan Zhai and
                  Kaiyan Cui and
                  Fu Xiao},
	title = {Poster: IMU-Aided Speech Enhancement for {COTS} Earphones},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1383--1385},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765694},
	doi = {10.1145/3680207.3765694},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/Dai0ZZC025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern communication tools often suffer quality issues from background noise and other speakers. Existing solutions either face mask blockages or need specialized gear, limiting use on standard earphones. Vibration-based methods, focusing on below 8 kHz speech, lack high frequencies, reducing naturalness. To solve these, we present an innovative multi-sensory speech enhancement framework for commercial earphones. It uses built-in inertial measurement units (IMUs) even with ultra-low 25 Hz sampling as extra input. We developed a mathematical framework linking IMU movement data and vocal signals, incorporating unsupervised domain adaptation to reduce individual differences and guiding intermediate-data integration to connect limited-frequency IMU info with 22 kHz full-range speech.}
}


@inproceedings{DBLP:conf/mobicom/TangGHS25,
	author = {Mingyue Tang and
                  Qinglin Ge and
                  Jizheng He and
                  Elahe Soltanaghai},
	title = {Poster: Passive {FMCW} Radar Detection and Profiling under Multipath
                  and Mobility Conditions},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1386--1388},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765695},
	doi = {10.1145/3680207.3765695},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/TangGHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As radars become more prevalent as a sensing solution, detecting and profiling unknown radars become crucial for privacy or interference avoidance purposes. ChirpEye [3] is a radar detection system capable of identifying the parameters and angle of arrival (AoA) of Frequency-Modulated Continuous-Wave (FMCW) radars without prior knowledge of radar configuration or location. In this paper, we provide and extended analysis on robustness of ChirpEye under varying multipath conditions or highly mobile scenarios. First, we prove that Chirpeye can preserve its accurate radar detection and characterization even under high mobility due to the unique tag structure that cancels out Doppler shifts. Furthermore, through controlled simulations, we show that multipath interference could cause chirp slope and AoA estimation errors only under low SINR conditions and within a specific multipath delay range.}
}


@inproceedings{DBLP:conf/mobicom/JinZLWYW025,
	author = {Lyudong Jin and
                  Yanning Zhang and
                  Yanhan Li and
                  Shurong Wang and
                  Howard H. Yang and
                  Jian Wu and
                  Meng Zhang},
	title = {Poster: MoE\({}^{\mbox{2}}\): Optimizing Collaborative Inference for
                  Edge Large Language Models},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1389--1391},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765696},
	doi = {10.1145/3680207.3765696},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/JinZLWYW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. Exploiting the heterogeneous capabilities of edge LLMs is crucial for emerging applications, enabling greater cost-effectiveness and reduced latency. In this work, we introduce  Mixture-of-Edge-Experts (MoE 2 ), a collaborative inference framework for edge LLMs. We formulate the joint gating and expert selection problem to optimize inference under energy and latency constraints. Unlike conventional MoE problems, expert selection here is more challenging due to the combinatorial nature and heterogeneity of edge LLMs. To address this, we propose a two-level expert selection mechanism and uncover an optimality-preserving property of gating parameters that decouples training and selection, reducing complexity. We further leverage the objective's monotonicity and design a discrete monotonic optimization algorithm. Implemented on Jetson Orin and RTX 4090 platforms, MoE 2  achieves optimal trade-offs across delay and energy budgets, outperforming baselines under various resource constraints.}
}


@inproceedings{DBLP:conf/mobicom/LiuC25,
	author = {Siyuan Liu and
                  Huangxun Chen},
	title = {Poster: MemAura: Persistent Personalized Context Memory for {LLM}
                  Services in Smart Environments},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1392--1394},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765697},
	doi = {10.1145/3680207.3765697},
	timestamp = {Sun, 01 Feb 2026 13:31:37 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/LiuC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this poster, we present our efforts to enable personalized LLM services in smart environments through persistent context memory. By systematically organizing long-term sensor logs into a structured format that captures user's behavioral patterns/preference in the environment, our solution MemAura empowers LLMs to better understand user intents and deliver tailored services. With its effective memory management, MemAura holds promise to make smart living spaces more efficient, context-aware and user-centric.}
}


@inproceedings{DBLP:conf/mobicom/ChenZWC25,
	author = {Jinhao Chen and
                  Zhi Zhou and
                  Rouyi Wang and
                  Xu Chen},
	title = {Poster: {A} Unified Framework for Simultaneous Video Analytics and
                  Streaming on UAVs},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1395--1397},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765698},
	doi = {10.1145/3680207.3765698},
	timestamp = {Tue, 20 Jan 2026 18:49:23 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ChenZWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing adoption of unmanned aerial vehicles (UAVs) in critical applications such as infrastructure inspection and emergency response, efficient on-site recognition via live video analytics and streaming has become essential. However, the inherent resource limitation poses significant challenges for performing simultaneous and real-time video analytics and streaming on UAVs. To address this issue, we propose a unified framework that orchestrate the Neural Processing Unit (NPU) and Graph Processing Unit (GPU) of the Systems-on-Chip (SoC) processor to accelerate and carefully schedule the pipeline of video analytics and streaming on UAVs. Additionally, our system incorporates frame interpolation to enable real-time streaming of video analytics results, providing immediate visual feedback to on-site operators. Empirical results on a commercial UAV equipped with Snapdragon 865 SoC platform show that our system reduces per-frame inference latency from 163ms (GPU) to 63ms (NPU), achieving a 2.6× speedup. Combined with optimized pre-processing and frame interpolation, our system increases effective streaming throughput from 2 to 30 FPS, enabling smooth and simultaneous real-time video analytics and streaming.}
}


@inproceedings{DBLP:conf/mobicom/GuptaV25,
	author = {Chandranshu Gupta and
                  Gaurav Varshney},
	title = {Poster: Lightweight PUF-based Authentication and Key-exchange for
                  Offline Smart Home IoT Devices},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1398--1400},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765699},
	doi = {10.1145/3680207.3765699},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/GuptaV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IoT-enabled smart home devices communicating through protocols like BLE or Zigbee, often constrained by limited computational resources and lack of direct internet connectivity, face significant authentication challenges. Traditional cryptographic methods such as Public-Key Infrastructure (PKI) or Identity-Based Encryption (IBE) impose heavy computational overheads, making them unsuitable for resource-constrained IoT environments. To address these issues, we propose a novel lightweight authentication protocol leveraging Physical Unclonable Functions (PUFs), utilizing smart-phones as intermediaries to facilitate secure communication with a trusted server. Our scheme avoids the overhead of maintaining large Challenge-Response Pair (CRP) databases and eliminates complex operations like Elliptic Curve cryptography and digital certificates (X.509). Experiments on an ESP32-based testbed demonstrate significant efficiency improvements over existing protocols, confirming the practicality of our lightweight approach for resource-constrained IoT environments.}
}


@inproceedings{DBLP:conf/mobicom/ChavoshianGS25,
	author = {Parham Chavoshian and
                  Hanbo Guo and
                  Elahe Soltanaghai},
	title = {Poster: Feasibility of Bistatic Millimeter-Wave Radar Sensing under
                  Loose Synchronization},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1401--1403},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765700},
	doi = {10.1145/3680207.3765700},
	timestamp = {Sun, 01 Feb 2026 13:31:35 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/ChavoshianGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-cost FMCW millimeter-wave (mmWave) radars are increasingly used beyond automotive applications due to their robustness to occlusion and lighting changes. However, the monostatic configuration of off-the-shelf radars limits spatial coverage and often misses reflections that scatter away from the receiver. Bistatic radars, with spatially separated transmitter and receiver, can capture these additional scattering paths, providing bistatic radar cross-section (RCS) information. However, a key challenge is the need for continuous phase synchronization between transceivers. Traditional solutions rely on costly ultra-stable oscillators, fiber links, or shared external clocks. In this paper, we investigate the feasibility of bistatic FMCW sensing with only frame-level synchronization. We demonstrate the feasibility of bistatic tracking of a single object movement and range estimation using two TI AWR1843 radars at 77 GHz with frame-level synchronization through a shared hardware trigger.}
}


@inproceedings{DBLP:conf/mobicom/KovalenkoAKMZM25,
	author = {Aleksandr Kovalenko and
                  Ahmad Ahmad and
                  Alexander Karandeev and
                  Alexey Maslov and
                  Dmitry Zhevnenko and
                  Ilya Makarov},
	title = {Poster: Car2Vec: Task-Agnostic Latent Embeddings from {CAN} Bus for
                  Efficient {V2X} Communication},
	booktitle = {Proceedings of the 31st Annual International Conference on Mobile
                  Computing and Networking, {ACM} {MOBICOM} 2025, Hong Kong, November
                  4-8, 2025},
	pages = {1404--1406},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3680207.3765701},
	doi = {10.1145/3680207.3765701},
	timestamp = {Sun, 01 Feb 2026 13:31:36 +0100},
	biburl = {https://dblp.org/rec/conf/mobicom/KovalenkoAKMZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The high bandwidth demand of raw sensor data transmission remains a critical bottleneck for scalable Vehicle-to-Everything (V2X) networks. We propose Car2Vec - a self-supervised contrastive learning framework that generates compact latent embeddings from vehicle CAN bus data. Our initial experiments demonstrate the framework's ability to create visually distinguishable embeddings in latent space, successfully separating clusters for fuel from different suppliers and identifying varying levels of motor oil viscosity. These early results suggest the framework's potential to encode semantic state information including vehicle dynamics and driver context, and serve as efficient communication primitives for V2V/V2I applications, potentially reducing wireless overhead by orders of magnitude compared to raw telemetry. By moving computation to vehicle edge devices, Car2Vec aligns with the AI-RAN co-design vision, offering a promising direction for semantic-aware resource optimization in next-gen transportation networks.}
}
