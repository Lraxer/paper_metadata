@inproceedings{DBLP:conf/kdd/AiW00ZVRTTH25,
	author = {Mengting Ai and
                  Tianxin Wei and
                  Yifan Chen and
                  Zhichen Zeng and
                  Ritchie Zhao and
                  Girish Varatkar and
                  Bita Darvish Rouhani and
                  Xianfeng Tang and
                  Hanghang Tong and
                  Jingrui He},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {ResMoE: Space-efficient Compression of Mixture of Experts LLMs via
                  Residual Restoration},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1--12},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709196},
	doi = {10.1145/3690624.3709196},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/AiW00ZVRTTH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mixture-of-Experts (MoE) Transformer, the backbone architecture of multiple phenomenal language models, leverages sparsity by activating only a fraction of model parameters for each input token. The sparse structure, while allowing constant time costs, results in space inefficiency: we still need to load all the model parameters during inference. We introduce ResMoE, an innovative MoE approximation framework that utilizes Wasserstein barycenter to extract a common expert (barycenter expert) and approximate the residuals between this barycenter expert and the original ones. ResMoE enhances the space efficiency for inference of large-scale MoE Transformers in a one-shot and data-agnostic manner without retraining while maintaining minimal accuracy loss, thereby paving the way for broader accessibility to large language models. We demonstrate the effectiveness of ResMoE through extensive experiments on Switch Transformer, Mixtral, and DeepSeekMoE models. The results show that ResMoE can reduce the number of parameters in an expert by up to 75% while maintaining comparable performance. The code is available at https://github.com/iDEA-iSAIL-Lab-UIUC/ResMoE, and the supplementary appendix is available at https://famous-blue-raincoat.github.io/mengtingai/files/ResMoE_Appendix.pdf.}
}


@inproceedings{DBLP:conf/kdd/AntelmiCVPPS25,
	author = {Alessia Antelmi and
                  Gennaro Cordasco and
                  Daniele De Vinco and
                  Valerio Di Pasquale and
                  Mirko Polato and
                  Carmine Spagnuolo},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Hypergraph Motif Representation Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {13--24},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709274},
	doi = {10.1145/3690624.3709274},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/AntelmiCVPPS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hypergraphs have emerged as a powerful tool for representing high-order connections in real-world complex systems. Similar to graphs, local structural patterns in hypergraphs, known as high-order motifs (h-motifs), play a crucial role in network dynamics and serve as fundamental building blocks across various domains. For this reason, predicting h-motifs can be highly beneficial in different fields. In this paper, we aim to advance our understanding of such complex high-order dynamics by introducing and formalizing the problem of h-motifs prediction. To address this task, we propose a novel solution that leverages both high-order and pairwise information by combining hypergraph and graph convolutions to capture hyperedges correlation within h-motifs, along with an innovative negative sampling approach designed to generate close-to-positive negative samples. To evaluate the effectiveness of our approach, we defined several baselines inspired by existing literature on hyperedge prediction methods. Our extensive experimental assessments demonstrate that our approach consistently outperforms all the considered baselines, showcasing its superior performance and robustness in predicting h-motifs.}
}


@inproceedings{DBLP:conf/kdd/AzadCKA25,
	author = {Poupak Azad and
                  Baris Coskunuzer and
                  Murat Kantarcioglu and
                  Cuneyt Gurcan Akcora},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Chainlet Orbits: Topological Address Embedding for Blockchain},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {25--36},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709322},
	doi = {10.1145/3690624.3709322},
	timestamp = {Thu, 01 May 2025 20:24:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/AzadCKA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of cryptocurrencies like Bitcoin has not only increased trade volumes but also broadened the use of graph machine learning techniques, such as address embeddings, to analyze transactions and decipher user patterns. Traditional analysis methods rely on simple heuristics and extensive data gathering, while more advanced Graph Neural Networks encounter challenges such as scalability, poor interpretability, and label scarcity in massive blockchain transaction networks. To overcome existing techniques' computational and interpretability limitations, we introduce a topological approach,  Chainlet Orbits,  which embeds blockchain addresses by leveraging their topological characteristics in temporal transactions. We employ our innovative address embeddings to investigate financial behavior and e-crime in the Bitcoin and Ethereum networks, focusing on distinctive substructures that arise from user behavior. Our model demonstrates exceptional performance in node classification experiments compared to GNN-based approaches. Furthermore, our approach embeds all daily nodes of the largest blockchain transaction network, Bitcoin, and creates explainable machine learning models in less than 17 minutes which takes days for GNN-based approaches.}
}


@inproceedings{DBLP:conf/kdd/BeiCCZYFHB25,
	author = {Yuanchen Bei and
                  Weizhi Chen and
                  Hao Chen and
                  Sheng Zhou and
                  Carl Ji Yang and
                  Jiapei Fan and
                  Longtao Huang and
                  Jiajun Bu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Correlation-Aware Graph Convolutional Networks for Multi-Label Node
                  Classification},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {37--48},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709197},
	doi = {10.1145/3690624.3709197},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/BeiCCZYFHB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-label node classification is an important yet under-explored domain in graph mining as many real-world nodes belong to multiple categories rather than just a single one. Although a few efforts have been made by utilizing Graph Convolution Networks (GCNs) to learn node representations and model correlations between multiple labels in the embedding space, they still suffer from the ambiguous feature and ambiguous topology induced by multiple labels, which reduces the credibility of the messages delivered in graphs and overlooks the label correlations on graph data. Therefore, it is crucial to reduce the ambiguity and empower the GCNs for accurate classification. However, this is quite challenging due to the requirement of retaining the distinctiveness of each label while fully harnessing the correlation between labels simultaneously. To address these issues, in this paper, we propose a  Cor relation-aware  G raph  C onvolutional  N etwork ( CorGCN ) for multi-label node classification. By introducing a novel Correlation-Aware Graph Decomposition module, CorGCN can learn a graph that contains rich label-correlated information for each label. It then employs a Correlation-Enhanced Graph Convolution to model the relationships between labels during message passing to further bolster the classification process. Extensive experiments on five datasets demonstrate the effectiveness of our proposed CorGCN.}
}


@inproceedings{DBLP:conf/kdd/BonchiGNPV25,
	author = {Francesco Bonchi and
                  Claudio Gentile and
                  Francesco Paolo Nerini and
                  Andr{\'{e}} Panisson and
                  Fabio Vitale},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Fast and Effective {GNN} Training through Sequences of Random Path
                  Graphs},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {49--60},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709301},
	doi = {10.1145/3690624.3709301},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/BonchiGNPV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present GERN, a novel scalable framework for training GNNs in node classification tasks, based on  effective resistance,  a standard tool in spectral graph theory. Our method progressively refines the GNN weights on a sequence of random spanning trees suitably transformed into  path graphs  which, despite their simplicity, are shown to retain essential topological and node information of the original input graph. The sparse nature of these path graphs substantially lightens the computational burden of GNN training. This not only enhances scalability but also improves accuracy in subsequent test phases, especially under  small training set  regimes, which are of great practical importance, as in many real-world scenarios labels may be hard to obtain. In these settings, our framework yields very good results as it effectively counters the training deterioration caused by overfitting when the training set is small. Our method also addresses common issues like over-squashing and over-smoothing while avoiding under-reaching phenomena. Although our framework is flexible and can be deployed in several types of GNNs, in this paper we focus on graph convolutional networks and carry out an extensive experimental investigation on a number of real-world graph benchmarks, where we achieve simultaneous improvement of training speed and test accuracy over a wide pool of representative baselines.}
}


@inproceedings{DBLP:conf/kdd/CaoXZWYW025,
	author = {Yuxuan Cao and
                  Jiarong Xu and
                  Chen Zhao and
                  Jiaan Wang and
                  Carl Ji Yang and
                  Chunping Wang and
                  Yang Yang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {How to use Graph Data in the Wild to Help Graph Anomaly Detection?},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {61--72},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709320},
	doi = {10.1145/3690624.3709320},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/CaoXZWYW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, graph anomaly detection has gained considerable attention and has found extensive applications in various domains such as social, financial, and communication networks. However, anomalies in graph-structured data present unique challenges, including  label scarcity, ill-defined anomalies , and  varying anomaly types , making supervised or semi-supervised methods unreliable. Researchers often adopt unsupervised approaches to address these challenges, assuming that anomalies deviate significantly from the normal data distribution. Yet, when the available data is insufficient, capturing the normal distribution accurately and comprehensively becomes difficult. To overcome this limitation, we propose to utilize external graph data ( i.e. , graph data in the wild) to help anomaly detection tasks. This naturally raises the question:  How can we use external data to help graph anomaly detection task ? To answer this question, we propose a novel framework  Wild-GAD . Our framework is built upon a unified database, UniWildGraph, which comprises a large and diverse collection of graph data with broad domain coverage, ample data volume, and a unified feature space. We further develop selection criteria based on  representativity  and  diversity  to identify the most suitable external data for each anomaly detection task. Extensive experiments on six real-world test datasets demonstrate the effectiveness of Wild-GAD. Compared to the baseline methods, our framework has an average 18% AUCROC and 32% AUCPR improvement over the best-competing methods.}
}


@inproceedings{DBLP:conf/kdd/CastiglioniNRST25,
	author = {Matteo Castiglioni and
                  Alessandro Nuara and
                  Giulia Romano and
                  Giorgio Spadaro and
                  Francesco Trov{\`{o}} and
                  Nicola Gatti},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Safe Online Bid Optimization with Return on Investment and Budget
                  Constraints},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {73--81},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709288},
	doi = {10.1145/3690624.3709288},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/CastiglioniNRST25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In online marketing, the advertisers aim to balance achieving high volumes and  high profitability . The companies' business units address this tradeoff by maximizing the volumes while guaranteeing a minimum Return On Investment (ROI) level. Such a task can be naturally modeled as a combinatorial optimization problem subject to ROI and budget constraints that can be solved online. In this picture, the learner's uncertainty over the constraints' parameters plays a crucial role since the algorithms' exploration choices might lead to their violation during the entire learning process. Such violations represent a major obstacle to adopting online techniques in real-world applications. Thus, controlling the algorithms' exploration during learning is paramount to making humans trust online learning tools. This paper studies the nature of both optimization and learning problems. In particular, we show that the learning problem is inapproximable within any factor (unless P = NP) and provide a pseudo-polynomial-time algorithm to solve its discretized version. Subsequently, we prove that no online learning algorithm can violate the (ROI or budget) constraints a sublinear number of times during the learning process while guaranteeing a sublinear regret. We provide the GCB algorithm that guarantees sublinear regret at the cost of a linear number of constraint violations and GCB safe  that guarantees w.h.p.a constant upper bound on the number of constraint violations at the cost of a linear regret. Moreover, we designed GCB safe (ψ, φ), which guarantees both sublinear regret and safety w.h.p. at the cost of accepting tolerances ψ and φ in the satisfaction of the ROI and budget constraints, respectively. Finally, we provide experimental results to compare the regret and constraint violations of GCB, GCB safe , and GCB safe (ψ, φ).}
}


@inproceedings{DBLP:conf/kdd/0003RKME25,
	author = {Hongjie Chen and
                  Ryan A. Rossi and
                  Sungchul Kim and
                  Kanak Mahadik and
                  Hoda Eldardiry},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Probabilistic Hypergraph Recurrent Neural Networks for Time-series
                  Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {82--93},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709202},
	doi = {10.1145/3690624.3709202},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0003RKME25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Leveraging graph structures for time-series forecasting has garnered significant attention due to their effective relationship modeling between nodes and their associated time-series. However, in scenarios entities communicate in a broadcasting manner, graph models fall short of pairwise modeling. Hypergraph models address this by capturing beyond-pairwise interactions among node time-series. Nevertheless, most hypergraph models overlook the dynamics between nodes and their incident hyperedges, assuming constant node-hyperedge connections. In this paper, we introduce a novel model,  Probabilistic Hypergraph Recurrent Neural Networks (PHRNN),  which leverages node-hyperedge dynamics for accurate time-series forecasting. PHRNN associates each time-series with a node and models node interactions on a hypergraph, capturing beyond-pairwise interactions. Moreover, PHRNN learns a probabilistic hypergraph in which node-hyperedge relations are modeled as probabilistic distributions instead of fixed values, capturing dynamic node-hyperedge relations. PHRNN further integrates a prior knowledge KNN hypergraph as regularization when learning the probabilistic hypergraph structure. To the best of our knowledge, PHRNN is the first time-series forecasting model that incorporates hypergraph modeling and probabilistic relationship modeling. Forecasting results from extensive experiments show that PHRNN outperforms state-of-the-art graph and hypergraph baselines on real-world datasets.}
}


@inproceedings{DBLP:conf/kdd/ChenLHHHC25,
	author = {Nan Chen and
                  Zemin Liu and
                  Bryan Hooi and
                  Bingsheng He and
                  Jun Hu and
                  Jia Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {NodeImport: Imbalanced Node Classification with Node Importance Assessment},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {94--105},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709215},
	doi = {10.1145/3690624.3709215},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ChenLHHHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world applications, node classification on graphs often faces the challenge of class imbalance, where majority classes dominate training, resulting in biased model performance. Traditional Graph Neural Networks (GNNs) often struggle in such scenarios, as they tend to overfit to majority classes while underrepresenting minority classes. Existing solutions, which either prioritize nodes based on class size or synthesize new nodes for minority classes, often fall short of effectively addressing this imbalance issue. This paper introduces a novel approach to class-imbalanced node classification by utilizing a balanced meta-set for importance measurement, where a training node is considered significant if it enhances model performance under an unbiased setting. Our method identifies important nodes that can counteract class imbalance and utilizes them for model training, allowing for fine-grained and dynamic node selection throughout the training process. We theoretically derive a formula to directly assess node importance, reducing computational overhead and providing an intuitive threshold for node selection. Guided by this metric, we develop a novel framework that filters valuable labeled, unlabeled, and synthetic nodes that enhance model performance in an unbiased context. A key advantage of this framework is its separation of the synthetic node generation process from the filtering process, ensuring compatibility with various node generation techniques. Furthermore, we introduce a strategy to construct a high-quality meta-set that closely approximates the overall feature distribution, ensuring robust representation of each class. We evaluate our framework, NodeImport, across multiple benchmark datasets using popular GNN architectures, demonstrating its superiority over state-of-the-art baselines. Our results highlight the flexibility and effectiveness of the framework in mitigating class imbalance, leading to improved node classification outcomes. The source code is available at https://github.com/NanChanNN/NodeImport.}
}


@inproceedings{DBLP:conf/kdd/ChenLFW25,
	author = {Qianyu Chen and
                  Xin Li and
                  Yujie Fang and
                  Mingzhong Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Advancing Confidence Calibration and Quantification in Medication
                  Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {106--117},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709232},
	doi = {10.1145/3690624.3709232},
	timestamp = {Tue, 13 May 2025 07:31:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ChenLFW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Medication recommendation (MR) has undergone rapid advancement in recent years, driven by its significant practical implications in healthcare. However, such high-risk scenarios still experience two critical yet overlooked challenges: the prevalent overconfidence in raw confidence for individual medications and the lack of a robust solution for confidence quantification in medication combinations. This paper represents the first in-depth study addressing this gap. We introduce two innovative methodologies tailored to the unique challenges of MR scenarios: 1) A discernible binning-based calibration method with theoretical guarantees for the confidence of individual medication. It guarantees distinct accuracy levels between adjacent bins and maintains consistent statistical reliability across calibration and test data, enabling calibrated confidence to reflect the correctness of medication recommendations distinctively. 2) A sample-based quantification method for the set confidence of medication combination, which is applicable for various existing performance metrics in MR. Utilizing representative deep MR models as backbones and conducting extensive experiments on the widely recognized MIMIC datasets, we empirically prove the effectiveness and robustness of our proposed methods. Our approaches not only improve the reliability of MR but also pave the way for more informed decision-making in clinical settings.}
}


@inproceedings{DBLP:conf/kdd/Chen000C025,
	author = {Weizhe Chen and
                  Wentao Li and
                  Min Gao and
                  Dong Wen and
                  Maolin Cai and
                  Wei Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Locally Balancing Signed Graphs},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {118--129},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709342},
	doi = {10.1145/3690624.3709342},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Chen000C025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Signed graphs capture both positive and negative relationships between entities, with balance being a fundamental concept. In these graphs, a vertex is considered balanced if all cycles it belongs to contain an even number of negative edges. On the other hand, unbalanced vertices often experience cognitive dissonance and emotional disturbance, motivating efforts to modify the graph to achieve balance for these vertices. Yet, most existing research emphasizes global balance, focusing on lengthy cycles that represent distant interactions. In contrast, this paper shifts the focus to local balance, where a vertex is deemed balanced when the triangles (length-three cycles) it participates in are positive, reflecting more immediate relationships. Building on this, we introduce the Locally Balancing Signed Graph (LBS) problem, which aims to maximize the number of locally balanced vertices through graph modification. Despite the NP-hard nature of the LBS problem and the absence of properties such as monotonicity and submodularity, our novel greedy method effectively addresses these challenges. We further enhance our method with dynamic computation and pruning techniques. Extensive experiments show the efficacy of our greedy method in solving the LBS problem and underscore the substantial runtime reductions achieved through our optimization techniques.}
}


@inproceedings{DBLP:conf/kdd/ChenT25,
	author = {Xiaolong Chen and
                  Jing Tang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Scalable Link Recommendation for Influence Maximization},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {130--141},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709190},
	doi = {10.1145/3690624.3709190},
	timestamp = {Tue, 13 May 2025 07:31:31 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ChenT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of link recommendation systems in online social networks has sparked significant research interest in strategically adding links to enhance social influence. This paper delves into the influence maximization with augmentation (IMA) problem that aims to add  k  edges connecting seed nodes and ordinary nodes to boost the influence propagation of the given seed set. IMA is a monotone submodular maximization problem so that the greedy algorithm provides a (1-1/e-ε)-approximate solution, where ε is an error term caused by the intractable nature of influence spread computation. Previous work often utilizes an unbiased estimator that relies on the chosen edges for influence estimation, resulting in non-submodular estimate with respect to edge selection. To ensure the overall error being bounded by ε, such an estimator requires Θ(ε/k) multiplicative error for each estimation, incurring prohibitive overhead. Meanwhile, some other work approximates IMA via conventional influence maximization (IM) on an  augmented  graph by adding a new node for every edge candidate, leading to heavy extra sampling due to a significant increase in graph size. To address these challenges, we design a novel unbiased estimator on the original graph that is independent of the chosen edges by leveraging the tractability of one-hop influence computation. We show that the estimate via our estimator is submodular so that it enables the estimate of all  k  edges in a whole with a bounded estimation error of Θ(ε), saving  O ( k 2 ) time compared to the chosen-edge-dependent estimator while retaining the same graph size. Moreover, we propose several techniques based on the properties of our estimator to further speed up the greedy selection. Putting it together, we develop a scalable algorithm for the IMA problem, namely  ScaLIM.  Finally, extensive experiments are conducted to validate the effectiveness and efficiency of our proposed approach, e.g.,  ScaLIM  is faster than baselines by nearly two orders of magnitude.}
}


@inproceedings{DBLP:conf/kdd/ChengLZ025,
	author = {Zhangtao Cheng and
                  Jian Lang and
                  Ting Zhong and
                  Fan Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Seeing the Unseen in Micro-Video Popularity Prediction: Self-Correlation
                  Retrieval for Missing Modality Generation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {142--152},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709308},
	doi = {10.1145/3690624.3709308},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ChengLZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Micro-video popularity prediction (MVPP) plays a crucial role in numerous real-world applications, including product marketing and recommendation systems. While existing methodologies predominantly assume complete modalities during multimodal learning, this assumption often fails to hold in practical scenarios due to various constraints, such as privacy concerns or data integrity issues. To address this limitation, we propose SCRAG, a novel Self-Correlation Retrieval-Augmented Generative framework designed to enhance missing-modality robustness in MVPP. SCRAG operates in a retrieval-guided generation manner that explores relevant knowledge to enhance the reconstruction of missing content, which consists of two primary components: (1) a  self-correlation retriever  and (2) a  multimodal mixture-of-experts generator . It first acquires instances pertinent to the missing content through multimodal prompt alignment. Subsequently, the generator extracts contextual modal information from the retrieved context-rich instances. By learning the joint distribution of modalities, SCRAG effectively recovers missing content and addresses the modal heterogeneity challenge inherent in cross-modal generation approaches. Extensive experiments conducted on three real-world datasets demonstrate that SCRAG consistently outperforms state-of-the-art baselines, underscoring its effectiveness in handling incomplete modalities and improving the accuracy of micro-video popularity prediction.}
}


@inproceedings{DBLP:conf/kdd/ChiharaMFS25,
	author = {Naoki Chihara and
                  Yasuko Matsubara and
                  Ren Fujiwara and
                  Yasushi Sakurai},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Modeling Time-evolving Causality over Data Streams},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {153--164},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709283},
	doi = {10.1145/3690624.3709283},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ChiharaMFS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given an extensive, semi-infinite collection of multivariate coevolving data sequences (e.g., sensor/web activity streams) whose observations influence each other, how can we discover the time-changing cause-and-effect relationships in co-evolving data streams? How efficiently can we reveal dynamical patterns that allow us to forecast future values? In this paper, we present a novel streaming method,  ModePlait , which is designed for modeling such causal relationships (i.e., time-evolving causality) in multivariate co-evolving data streams and forecasting their future values. The solution relies on characteristics of the causal relationships that evolve over time in accordance with the dynamic changes of exogenous variables.  ModePlait  has the following properties: (a)  Effective : it discovers the time-evolving causality in multivariate co-evolving data streams by detecting the transitions of distinct dynamical patterns adaptively. (b)  Accurate : it enables both the discovery of time-evolving causality and the forecasting of future values in a streaming fashion. (c)  Scalable : our algorithm does not depend on data stream length and thus is applicable to very large sequences. Extensive experiments on both synthetic and real-world datasets demonstrate that our proposed model outperforms state-of-the-art methods in terms of discovering the time-evolving causality as well as forecasting.}
}


@inproceedings{DBLP:conf/kdd/0001PGCNEK25,
	author = {Brian Cho and
                  Ana{-}Roxana Pop and
                  Kyra Gan and
                  Sam Corbett{-}Davies and
                  Israel Nir and
                  Ariel Evnine and
                  Nathan Kallus},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{CSPI-MT:} Calibrated Safe Policy Improvement with Multiple Testing
                  for Threshold Policies},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {165--176},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709176},
	doi = {10.1145/3690624.3709176},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0001PGCNEK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When modifying existing policies in high-risk settings, it is often necessary to ensure with high certainty that the newly proposed policy improves upon a baseline, such as the status quo. In this work, we consider the problem of  safe policy improvement,  where one only adopts a new policy if it is deemed to be better than the specified baseline with at least a pre-specified probability. We focus on threshold policies, a ubiquitous class of policies with applications in economics, healthcare, and digital advertising. Existing methods rely on potentially underpowered safety checks and limit the opportunities for finding safe improvements, so too often they must revert to the baseline to maintain safety. We overcome these issues by leveraging the most powerful safety test in the asymptotic regime and allowing for multiple candidates to be tested for improvement over the baseline. We show that in adversarial settings, our approach controls the rate of adopting a policy worse than the baseline to the pre-specified error level, even in moderate sample sizes. We present CSPI and CSPI-MT, two novel algorithms for selecting cutoff(s) to maximize the policy improvement from baseline. We demonstrate through both synthetic and external datasets that our approaches improve both the detection rates of safe policies and the realized improvement, particularly under stringent safety requirements and low signal-to-noise conditions.}
}


@inproceedings{DBLP:conf/kdd/Cui0W25,
	author = {Guanyu Cui and
                  Hanzhi Wang and
                  Zhewei Wei},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Mixing Time Matters: Accelerating Effective Resistance Estimation
                  via Bidirectional Method},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {177--188},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709298},
	doi = {10.1145/3690624.3709298},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Cui0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of efficiently approximating the  effective resistance  (ER) on undirected graphs, where ER is a widely used node proximity measure with applications in graph spectral sparsification, multi-class graph clustering, network robustness analysis, graph machine learning, and more. Specifically, given any nodes  s  and  t  in an undirected graph  G , we aim to efficiently estimate the ER value R(s,t) between nodes  s  and  t , ensuring a small absolute error ε. The previous best algorithm for this problem has a worst-case computational complexity of Õ(L max 3 /ε 2 d 2 ), where the value of L max  depends on the mixing time of random walks on  G , d = min d(s), d(t) , and d(s), d(t) denote the degrees of nodes  s  and  t , respectively. We improve this complexity to Õ ( min L max 7/3 /ε 2/3 , L max 3 /ε 2 d 2 , mL max  ), achieving a theoretical improvement of Õ (maxL max 2/3 /ε 4/3 d 2 , 1 , L max 2 /ε 2 d 2 m 2 ) over previous results. Here,  m  denotes the number of edges. Given that L max  is often very large in real-world networks (e.g., L max  > 10 4 ), our improvement on L max  is significant, especially for real-world networks. We also conduct extensive experiments on real-world and synthetic graph datasets to empirically demonstrate the superiority of our method. The experimental results show that our method achieves a 10× to 1000× speedup in running time while maintaining the same absolute error compared to baseline methods.}
}


@inproceedings{DBLP:conf/kdd/DehghankarRSA25,
	author = {Mohsen Dehghankar and
                  Rahul Raychaudhury and
                  Stavros Sintos and
                  Abolfazl Asudeh},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Fair Set Cover},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {189--200},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709184},
	doi = {10.1145/3690624.3709184},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/DehghankarRSA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The potential harms of algorithmic decisions have ignited algorithmic fairness as a central topic in computer science. One of the fundamental problems in computer science is Set Cover, which has numerous applications with societal impacts, such as assembling a small team of individuals that collectively satisfy a range of expertise requirements. However, despite its broad application spectrum and significant potential impact, set cover has yet to be studied through the lens of fairness. Therefore, in this paper, we introduce  Fair Set Cover,  which aims not only to cover with a minimum-size set but also to satisfy demographic parity in its selection of sets. To this end, we develop multiple versions of fair set cover, study their hardness, and devise efficient approximation algorithms for each variant. Notably, under certain assumptions, our algorithms always guarantee zero-unfairness, with only a small increase in the approximation ratio compared to regular set cover. Furthermore, our experiments on various data sets and across different settings confirm the negligible price of fairness, as (a) the output size increases only slightly (if any) and (b) the time to compute the output does not significantly increase.}
}


@inproceedings{DBLP:conf/kdd/DengJYQYC25,
	author = {Bangchao Deng and
                  Xin Jing and
                  Tianyue Yang and
                  Bingqing Qu and
                  Dingqi Yang and
                  Philippe Cudr{\'{e}}{-}Mauroux},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Revisiting Synthetic Human Trajectories: Imitative Generation and
                  Benchmarks Beyond Datasaurus},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {201--212},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709180},
	doi = {10.1145/3690624.3709180},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/DengJYQYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human trajectory data, which plays a crucial role in various applications such as crowd management and epidemic prevention, is challenging to obtain due to practical constraints and privacy concerns. In this context, synthetic human trajectory data is generated to simulate as close as possible to real-world human trajectories, often under summary statistics and distributional similarities. However, these similarities oversimplify complex human mobility patterns (a.k.a. ''Datasaurus''), resulting in intrinsic biases in both generative model design and benchmarks of the generated trajectories. Against this background, we propose MIRAGE, a huMan-Imitative tRAjectory GenErative model designed as a neural Temporal Point Process integrating an Exploration and Preferential Return model. It imitates the human decision-making process in trajectory generation, rather than fitting any specific statistical distributions as traditional methods do, thus avoiding the Datasaurus issue. We also propose a comprehensive task-based evaluation protocol beyond Datasaurus to systematically benchmark trajectory generative models on four typical downstream tasks, integrating multiple techniques and evaluation metrics for each task, to assess the ultimate utility of the generated trajectories. We conduct a thorough evaluation of MIRAGE on three real-world user trajectory datasets against a sizeable collection of baselines. Results show that compared to the best baselines, MIRAGE-generated trajectory data not only achieves the best statistical and distributional similarities with 59.0-67.7% improvement, but also yields the best performance in the task-based evaluation with 10.9-33.4% improvement. A series of ablation studies also validate the key design choices of MIRAGE.}
}


@inproceedings{DBLP:conf/kdd/0002W025,
	author = {Haipeng Ding and
                  Zhewei Wei and
                  Yuhang Ye},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Large-Scale Spectral Graph Neural Networks via Laplacian Sparsification},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {213--223},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709241},
	doi = {10.1145/3690624.3709241},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0002W025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) play a pivotal role in graph-based tasks for their proficiency in representation learning. Among the various GNN methods, spectral GNNs employing polynomial filters have shown promising performance on tasks involving both homophilous and heterophilous graph structures. However, The scalability of spectral GNNs on large graphs is limited because they learn the polynomial coefficients through multiple forward propagation executions during forward propagation. Existing works have attempted to scale up spectral GNNs by eliminating the linear layers on the input node features, a change that can disrupt end-to-end training, potentially impact performance, and become impractical with high-dimensional input features. To address the above challenges, we propose ''Spectral Graph Neural Networks with Laplacian Sparsification (SGNN-LS)'', a novel graph spectral sparsification method to approximate the propagation patterns of spectral GNNs. We prove that our proposed method generates Laplacian sparsifiers that can approximate both fixed and learnable polynomial filters with theoretical guarantees. Our method allows the application of linear layers on the input node features, enabling end-to-end training as well as the handling of raw text features. We conduct an extensive experimental analysis on datasets spanning various graph scales and properties to demonstrate the superior efficiency and effectiveness of our method. The results show that our method yields superior results in comparison with the corresponding approximated base models, especially on dataset Ogbn-papers100M(111M nodes, 1.6B edges) and MAG-scholar-C (2.8M features).}
}


@inproceedings{DBLP:conf/kdd/DingTLNMZL0C25,
	author = {Yucheng Ding and
                  Yangwenjian Tan and
                  Xiangyu Liu and
                  Chaoyue Niu and
                  Fandong Meng and
                  Jie Zhou and
                  Ning Liu and
                  Fan Wu and
                  Guihai Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Personalized Language Model Learning on Text Data Without User Identifiers},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {224--235},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709211},
	doi = {10.1145/3690624.3709211},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/DingTLNMZL0C25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many practical natural language applications, user data are highly sensitive, requiring anonymous uploads of text data from mobile devices to the cloud without user identifiers. However, the absence of user identifiers restricts the ability of cloud-based language models to provide personalized services, which are essential for catering to diverse user needs. The trivial method of replacing an explicit user identifier with a static user embedding as model input still compromises data anonymization. In this work, we propose to let each mobile device maintain a user-specific distribution to dynamically generate user embeddings, thereby breaking the one-to-one mapping between an embedding and a specific user. We further theoretically demonstrate that to prevent the cloud from tracking users via uploaded embeddings, the local distributions of different users should either be derived from a linearly dependent space to avoid identifiability or be close to each other to prevent accurate attribution. Evaluation on both public and industrial datasets using different language models reveals a remarkable improvement in accuracy from incorporating anonymous user embeddings, while preserving real-time inference requirement.}
}


@inproceedings{DBLP:conf/kdd/DongKQO25,
	author = {Junhao Dong and
                  Piotr Koniusz and
                  Xinghua Qu and
                  Yew{-}Soon Ong},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Stabilizing Modality Gap {\&} Lowering Gradient Norms Improve
                  Zero-Shot Adversarial Robustness of VLMs},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {236--247},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709296},
	doi = {10.1145/3690624.3709296},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/DongKQO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contemporary Vision-Language Models (VLMs) such as CLIP offer an attractive zero-shot classification functionality facilitated by large-scale vision-language pre-training. However, they remain vulnerable to adversarial attacks, a critical security threat in realistic deployment. Adversarially robust fine-tuning provides generalizable robustness on new datasets while preserving natural performance by fine-tuning the pre-trained models. Fine-tuning robust CLIP typically relies on adversaries generated solely from the vision branch. However, this singular focus on the vision modality, coupled with static text prompts used as fixed category prototypes, limits the robustness achieved through dual-modality fine-tuning. We observe for CLIP fine-tuning that zero-shot adversarial robustness improves when we (i) stabilize the modality gap (a phenomenon where image and text features occupy different feature space regions) and (ii) lower/stabilize gradient norms. Both these steps enjoy further improvement of robustness if one fine-tunes with both visual and text adversaries. For both modalities, we leverage (i) the maximization of an effective rank of features and (ii) noise modulation of features. We show that maximizing the effective rank helps lower and stabilize the modality gap over adversaries with varying perturbation radii. The noise modulation of features, achieved by the so-called count sketching, lowers/stabilizes gradient norms. We outperform the state of the art on 15 datasets. We provide the first insights into the effects of modality gap & gradient norms in VLM fine-tuning.}
}


@inproceedings{DBLP:conf/kdd/0005FZ25,
	author = {Zheng Dong and
                  Zekai Fan and
                  Shixiang Zhu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Conditional Generative Modeling for High-dimensional Marked Temporal
                  Point Processes},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {248--259},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709258},
	doi = {10.1145/3690624.3709258},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0005FZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Point processes offer a versatile framework for sequential event modeling. However, the computational challenges and constrained representational power of the existing point process models have impeded their potential for wider applications. This limitation becomes especially pronounced when dealing with event data that is associated with multi-dimensional or high-dimensional marks such as texts or images. To address this challenge, this study proposes a novel event-generation framework for modeling point processes with high-dimensional marks. We aim to capture the distribution of events without explicitly specifying the conditional intensity or probability density function. Instead, we use a conditional generator that takes the history of events as input and generates the high-quality subsequent event that is likely to occur given the prior observations. The proposed framework offers a host of benefits, including considerable representational power to capture intricate dynamics in multi- or even high-dimensional event space, as well as exceptional efficiency in learning the model and generating samples. Our numerical results demonstrate superior performance compared to other state-of-the-art baselines.}
}


@inproceedings{DBLP:conf/kdd/DuanDPW025,
	author = {Bowen Duan and
                  Henggang Deng and
                  Jinghua Piao and
                  Huandong Wang and
                  Yue Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Bi-Dynamic Graph {ODE} for Opinion Evolution},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {260--270},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709297},
	doi = {10.1145/3690624.3709297},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/DuanDPW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modeling opinion dynamics in social networks has been the focus of multiple disciplines in recent decades. Previous studies have often modeled the opinion dynamics as a discrete and homogeneous process, neglecting its continuous and complex nature. To fill this gap, we propose a Bi-Dynamics Graph Ordinary Differential Equation (BDG-ODE) framework, which models complex opinion dynamics as the result of two dynamical processes: the evolution of positive and negative opinions. The proposed model incorporates a dual opinion encoder that processes positive and negative opinions independently. Furthermore, the temporal opinion evolution is modeled through bidirectional graph ordinary differential equations, which allows the model to capture the changes in opinion in continuous time. We introduce an opinion synthesis decoder that effectively maps the evolved representations from the latent space back to the opinion space. Extensive experiments conducted on six datasets with varying characteristics highlight the superiority of BDG-ODE in forecasting opinion evolution within social networks. It achieved an average accuracy improvement of 23.16%, an average enhancement of 29.46% in the F1 score, and an average mean square error of difference improvement of 90. 30%, and an average correlation coefficient improvement of 45.93%, significantly outperforming eight state-of-the-art models. The code for reproduction is available: https://github.com/tsinghua-fib-lab/Bi-Dynamic-Graph-ODE-for-Opinion-Evolution.}
}


@inproceedings{DBLP:conf/kdd/DuanG00QT25,
	author = {Jinyu Duan and
                  Haicheng Guo and
                  Fan Zhang and
                  Kai Wang and
                  Zhengping Qian and
                  Zhihong Tian},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {The \emph{k}-Trine Cohesive Subgraph and Its Efficient Algorithms},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {271--282},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709174},
	doi = {10.1145/3690624.3709174},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/DuanG00QT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we introduce and study a novel cohesive subgraph model, named  k -trine, to address the defects in the classical  k -core and  k -truss models. Our analysis shows that the  k -trine is a more feasible model for capturing cohesive subgraphs by containing the strongly connected vertices. We analyze the theoretical properties of  k -trine and propose efficient algorithms to compute the  k -trine. Particularly, we design batch processing algorithms to update the decomposition of  k -trine against highly dynamic graphs. Extensive experiments on real-world networks validate the effectiveness of the  k -trine model and the efficiency of our algorithms.}
}


@inproceedings{DBLP:conf/kdd/DuanGZHRH25,
	author = {Wenying Duan and
                  Shujun Guo and
                  Zimu Zhou and
                  Wei Huang and
                  Hong Rao and
                  Xiaoxi He},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Dynamic Localisation of Spatial-Temporal Graph Neural Network},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {283--294},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709331},
	doi = {10.1145/3690624.3709331},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/DuanGZHRH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial-temporal data, fundamental to many intelligent applications, reveals dependencies indicating causal links between present measurements at specific locations and historical data at the same or other locations. Within this context, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged as valuable tools for modelling these dependencies, especially through a data-driven approach rather than pre-defined spatial graphs. While this approach offers higher accuracy, it presents increased computational demands. Addressing this challenge, this paper delves into the concept of localisation within ASTGNNs, introducing an innovative perspective that spatial dependencies should be dynamically evolving over time. We introduce  DynAGS,  a localised ASTGNN framework aimed at maximising efficiency and accuracy in distributed deployment. This framework integrates dynamic localisation, time-evolving spatial graphs, and personalised localisation, all orchestrated around the Dynamic Graph Generator, a light-weighted central module leveraging cross attention. The central module can integrate historical information in a node-independent manner to enhance the feature representation of nodes at the current moment. This improved feature representation is then used to generate a dynamic sparse graph without the need for costly data exchanges, and it supports personalised localisation. Performance assessments across two core ASTGNN architectures and nine real-world datasets from various applications reveal that  DynAGS  outshines current benchmarks, underscoring that the dynamic modelling of spatial dependencies can drastically improve model expressibility, flexibility, and system efficiency, especially in distributed settings.}
}


@inproceedings{DBLP:conf/kdd/0010ZWX0Z0F25,
	author = {Wei Fan and
                  Shun Zheng and
                  Pengyang Wang and
                  Rui Xie and
                  Kun Yi and
                  Qi Zhang and
                  Jiang Bian and
                  Yanjie Fu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {IN-Flow: Instance Normalization Flow for Non-stationary Time Series
                  Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {295--306},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709260},
	doi = {10.1145/3690624.3709260},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0010ZWX0Z0F25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the non-stationarity of time series, the distribution shift problem largely hinders the performance of time series forecasting. Existing solutions either rely on using certain statistics to specify the shift, or developing specific mechanisms for certain network architectures. However, the former would fail for the unknown shift beyond simple statistics, while the latter has limited compatibility on different forecasting models. To overcome these problems, we first propose a  decoupled formulation  for time series forecasting, with no reliance on fixed statistics and no restriction on forecasting architectures. This formulation regards the removing-shift procedure as a special transformation between a raw distribution and a desired target distribution and separates it from the forecasting. Such a formulation is further formalized into a  bi-level optimization  problem, to enable the joint learning of the transformation (outer loop) and forecasting (inner loop). Moreover, the special requirements of expressiveness and bi-direction for the transformation motivate us to propose  instance normalization flow  (IN-Flow), a novel invertible network for time series transformation. Different from the classic ''normalizing flow'' models, IN-Flow does not aim for normalizing input to the prior distribution (e.g., Gaussian distribution) for generation, but creatively transforms time series distribution by stacking normalization layers and flow-based invertible networks, which is thus named ''normalization'' flow. Finally, we have conducted extensive experiments on both synthetic data and real-world data, which demonstrate the superiority of our method.}
}


@inproceedings{DBLP:conf/kdd/0001L0S0LJ025,
	author = {Yuchen Fang and
                  Yuxuan Liang and
                  Bo Hui and
                  Zezhi Shao and
                  Liwei Deng and
                  Xu Liu and
                  Xinke Jiang and
                  Kai Zheng},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Efficient Large-Scale Traffic Forecasting with Transformers: {A} Spatial
                  Data Management Perspective},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {307--317},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709177},
	doi = {10.1145/3690624.3709177},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0001L0S0LJ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Road traffic forecasting is crucial in real-world intelligent transportation scenarios like traffic dispatching and path planning in city management and personal traveling. Spatio-temporal graph neural networks (STGNNs) stand out as the mainstream solution in this task. Nevertheless, the quadratic complexity of remarkable dynamic spatial modeling-based STGNNs has become the bottleneck over large-scale traffic data. From the spatial data management perspective, we present a novel Transformer framework called PatchSTG to efficiently and dynamically model spatial dependencies for large-scale traffic forecasting with interpretability and fidelity. Specifically, we design a novel irregular spatial patching to reduce the number of points involved in the dynamic calculation of Transformer. The irregular spatial patching first utilizes the leaf K-dimensional tree (KDTree) to recursively partition irregularly distributed traffic points into leaf nodes with a small capacity, and then merges leaf nodes belonging to the same subtree into occupancy-equaled and non-overlapped patches through padding and backtracking. Based on the patched data, depth and breadth attention are used interchangeably in the encoder to dynamically learn local and global spatial knowledge from points in a patch and points with the same index of patches. Experimental results on four real world large-scale traffic datasets show that our PatchSTG achieves train speed and memory utilization improvements up to 10x and 4x with the state-of-the-art performance.}
}


@inproceedings{DBLP:conf/kdd/FuL0WK25,
	author = {Kairui Fu and
                  Zheqi Lv and
                  Shengyu Zhang and
                  Fan Wu and
                  Kun Kuang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Forward Once for All: Structural Parameterized Adaptation for Efficient
                  Cloud-coordinated On-device Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {318--329},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709178},
	doi = {10.1145/3690624.3709178},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/FuL0WK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In cloud-centric recommender system, regular data exchanges between user devices and cloud could potentially elevate bandwidth demands and privacy risks. On-device recommendation emerges as a viable solution by performing reranking locally to alleviate these concerns. Existing methods primarily focus on developing local adaptive parameters, while potentially neglecting the critical role of tailor-made model architecture. Insights from broader research domains suggest that varying data distributions might favor distinct architectures for better fitting. In addition, imposing a uniform model structure across heterogeneous devices may result in risking inefficacy on less capable devices or sub-optimal performance on those with sufficient capabilities. In response to these gaps, our paper introduces  Forward-OFA , a novel approach for the dynamic construction of device-specific networks ( both structure and parameters ). Forward-OFA employs a structure controller to selectively determine whether each block needs to be assembled for each device. However, during the training of the structure controller, these assembled heterogeneous structures are jointly optimized, where the co-adaption among blocks might encounter gradient conflicts. To mitigate this, Forward-OFA is designed to establish a structure-guided mapping of real-time behaviors to individual parameters of assembled networks. Structure-related parameters and parallel components within the mapper prevent each part from receiving heterogeneous gradients from others, thus bypassing the gradient conflicts for coupled optimization. Besides, direct mapping enables Forward-OFA to achieve adaptation through  only one forward pass , allowing for swift adaptation to changing interests and eliminating the requirement for on-device backpropagation. Further sophisticated design protects user privacy and makes the consumption of additional modules on device negligible. Experiments on real-world datasets demonstrate the effectiveness and efficiency of Forward-OFA.}
}


@inproceedings{DBLP:conf/kdd/GaoPZ00025,
	author = {Kaiyuan Gao and
                  Qizhi Pei and
                  Gongbo Zhang and
                  Jinhua Zhu and
                  Kun He and
                  Lijun Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {FABind+: Enhancing Molecular Docking through Improved Pocket Prediction
                  and Pose Generation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {330--341},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709253},
	doi = {10.1145/3690624.3709253},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GaoPZ00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, deep learning has emerged as a promising alternative, offering improvements in both accuracy and efficiency. Building upon the foundational work of FABind, a model focused on speed and accuracy, we introduce FABind+, an enhanced iteration that significantly elevates the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and introduce an enhanced approach. In addition to the pocket prediction module, the docking module has also been upgraded with permutation loss and a more refined model design. These designs enable the regression-based FABind+ to surpass most of the generative models. In contrast, while sampling-based models often struggle with inefficiency, they excel in capturing a wide range of potential docking poses, leading to better overall performance. To bridge the gap between sampling and regression docking models, we incorporate a simple yet effective sampling technique coupled with a lightweight confidence model, transforming the regression-based FABind+ into a sampling version without requiring additional training. This involves the introduction of pocket clustering to capture multiple binding sites and dropout sampling for various conformations. The combination of a classification loss and a ranking loss enables the lightweight confidence model to select the most accurate prediction. Experimental results and analysis demonstrate that FABind+ (both the regression and sampling versions) not only significantly outperforms the original FABind, but also achieves competitive state-of-the-art performance. Our code is available at https://github.com/QizhiPei/FABind.}
}


@inproceedings{DBLP:conf/kdd/GaoFD0025,
	author = {Li Gao and
                  Chuanpu Fu and
                  Xinhao Deng and
                  Ke Xu and
                  Qi Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Wedjat: Detecting Sophisticated Evasion Attacks via Real-time Causal
                  Analysis},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {342--353},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709218},
	doi = {10.1145/3690624.3709218},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GaoFD0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic encryption has been widely adopted to protect the confidentiality and integrity of Internet traffic. However, attackers can also abuse such mechanism to deliver malicious traffic. Particularly, existing methods detecting encrypted malicious traffic are not robust against evasion attacks that manipulate traffic to obfuscate traffic features. Robust detection against evasion attacks remains an open problem. To the end, we develop Wedjat, which utilizes a causal network to model benign packet interactions among relevant flows, such that it recognizes abnormal causality that represents malicious traffic and disrupted causality incurred by evasion attacks. We extensively evaluate Wedjat with millions of flows collected from a real-world enterprise. The experimental results demonstrate that Wedjat achieves an accuracy of 0.957 F1-score when detecting various advanced attacks. Notably, five sophisticated evasion attacks, which have successfully evaded all existing methods, are accurately detected by Wedjat with over 0.915 F1. It demonstrates that Wedjat achieves exceptional robustness against evasions. Meanwhile, Wed- jat maintains an outstanding detection latency, i.e., it can predict each packet in less than 0.125 seconds.}
}


@inproceedings{DBLP:conf/kdd/GaoLLZWYYZ25,
	author = {Weibo Gao and
                  Qi Liu and
                  Rui Li and
                  Yuze Zhao and
                  Hao Wang and
                  Linan Yue and
                  Fangzhou Yao and
                  Zheng Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Denoising Programming Knowledge Tracing with a Code Graph-based Tuning
                  Adaptor},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {354--365},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709172},
	doi = {10.1145/3690624.3709172},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GaoLLZWYYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programming Knowledge Tracking (PKT) aims to dynamically diagnose learners' mastery levels of programming knowledge based on their coding activities, facilitating more effective and personalized programming education. However, current PKT studies primarily focus on the implicit relationship between code content and knowledge assessment, often overlooking two types of noise signals in long-term programming activities: unwanted signals from unrelated submissions and weak signals from minor modifications. This practical challenge significantly limits model performance and application. To address this issue, we propose  Coda,  a  Cod e graph-based tuning  a daptor designed to enhance existing PKT models by identifying and mitigating the impact of noise. Specifically, Coda first transforms the loose code sequences submitted by each learner into a compact code graph. By leveraging this code graph, unwanted signals can be identified from a semantic similarity perspective. We then apply a cluster-aware GCN to the code graph, which improves the discrimination of weak signals and enables their clustering for identification. Finally, a lightweight yet effective adaptor is incorporated into the PKT task through optimization with two noise feature-based constraints and a navigational regularization term, to correct knowledge states affected by noise. It is worth mentioning that the Coda framework is model-agnostic and can be adapted to most existing PKT solutions. Extensive experimental results on four real-world datasets demonstrate that Coda effectively performs the PKT task in the presence of noisy programming records, outperforming typical baselines.}
}


@inproceedings{DBLP:conf/kdd/GiobergiaPAB25,
	author = {Flavio Giobergia and
                  Eliana Pastor and
                  Luca de Alfaro and
                  Elena Baralis},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Detecting Interpretable Subgroup Drifts},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {366--377},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709259},
	doi = {10.1145/3690624.3709259},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GiobergiaPAB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ability to detect and adapt to changes in data distributions is crucial to maintain the accuracy and reliability of machine learning models. Detection is generally approached by observing the drift of model performance from a global point of view. However, drifts occurring in (fine-grained) data subgroups may go unnoticed when monitoring global drift.We take a different perspective, and introduce methods for observing drift at the finer granularity of subgroups. Relevant data subgroups are identified during training and monitored efficiently throughout the model's life. Performance drifts in any subgroup are detected, quantified and characterized so as to provide an interpretable summary of the model behavior over time. Experimental results confirm that our subgroup-level drift analysis identifies drifts that do not show at the (coarser) global dataset level. The proposed approach provides a valuable tool for monitoring model performance in dynamic real-world applications, offering insights into the evolving nature of data and ultimately contributing to more robust and adaptive models.}
}


@inproceedings{DBLP:conf/kdd/GoldbergFS025,
	author = {Alexander Goldberg and
                  Giulia Fanti and
                  Nihar B. Shah and
                  Steven Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Benchmarking Fraud Detectors on Private Graph Data},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {378--389},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709170},
	doi = {10.1145/3690624.3709170},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GoldbergFS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce the novel problem of benchmarking fraud detectors on private graph-structured data. Currently, many types of fraud are managed in part by automated detection algorithms that operate over graphs. We consider the scenario where a data holder wishes to outsource development of fraud detectors to third parties (e.g., vendors or researchers). The third parties submit their fraud detectors to the data holder, who evaluates these algorithms on a private dataset and then publicly communicates the results. We propose a realistic privacy attack on this system that allows an adversary to de-anonymize individuals' data based only on the evaluation results. In simulations of a privacy-sensitive benchmark for facial recognition algorithms by the National Institute of Standards and Technology (NIST), our attack achieves near perfect accuracy in identifying whether individuals' data is present in a private dataset, with a True Positive Rate of 0.98 at a False Positive Rate of 0.00. We then study how to benchmark algorithms while satisfying a formal  differential privacy (DP)  guarantee. We empirically evaluate two classes of solutions: subsample-and-aggregate and DP synthetic graph data. We demonstrate through extensive experiments that current approaches fail to provide utility when guaranteeing DP. Our results indicate that the error arising from DP trades off between bias from distorting graph structure and variance from adding random noise. Current methods lie on different points along this bias-variance trade-off, but more complex methods tend to require high-variance noise addition, undermining utility.}
}


@inproceedings{DBLP:conf/kdd/Gong00CS0Z25,
	author = {Peiliang Gong and
                  Mohamed Ragab and
                  Min Wu and
                  Zhenghua Chen and
                  Yongyi Su and
                  Xiaoli Li and
                  Daoqiang Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Augmented Contrastive Clustering with Uncertainty-Aware Prototyping
                  for Time Series Test Time Adaptation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {390--401},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709239},
	doi = {10.1145/3690624.3709239},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Gong00CS0Z25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Test-time adaptation aims to adapt pre-trained deep neural networks using solely online unlabelled test data during inference. Although TTA has shown promise in visual applications, its potential in time series contexts remains largely unexplored. Existing TTA methods, originally designed for visual tasks, may not effectively handle the complex temporal dynamics of real-world time series data, resulting in suboptimal adaptation performance. To address this gap, we propose Augmented Contrastive Clustering with Uncertainty-aware Prototyping (ACCUP), a straightforward yet effective TTA method for time series data. Initially, our approach employs augmentation ensemble on the time series data to capture diverse temporal information and variations, incorporating uncertainty-aware prototypes to distill essential characteristics. Additionally, we introduce an entropy comparison scheme to selectively acquire more confident predictions, enhancing the reliability of pseudo labels. Furthermore, we utilize augmented contrastive clustering to enhance feature discriminability and mitigate error accumulation from noisy pseudo labels, promoting cohesive clustering within the same class while facilitating clear separation between different classes. Extensive experiments conducted on three real-world time series datasets demonstrate the effectiveness and generalization potential of the proposed method, advancing the underexplored realm of TTA for time series data. Our code is available at https://github.com/Tokenmw/ACCUP-main.}
}


@inproceedings{DBLP:conf/kdd/GuLDLZ25,
	author = {Hengnian Gu and
                  Guoqian Luo and
                  Xiaoxiao Dong and
                  Shulin Li and
                  Dongdai Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Revisiting Cognition in Neural Cognitive Diagnosis},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {402--412},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709319},
	doi = {10.1145/3690624.3709319},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GuLDLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cognitive diagnosis is a fundamental task in intelligent education, aiming to measure students' proficiency on knowledge concepts based on practice data. Traditional methods utilize a broadly-defined latent trait θ to represent knowledge proficiency with some cognitive factors like skill or ability. However, existing methods simplify this to a narrowly-defined latent trait θ, which focuses only on knowledge or treats these cognitive factors as implicit features inferred from data. They fail to explicitly model these cognitive factors, resulting in limited performance and interpretability. To this end, we revisit essence of cognition in Educational Psychology Theory and propose a novel Cognition-aware Cognitive Diagnosis (CCD) model, where we first introduce the Cognition factor as a bridge into the long-standing three-basic-factors (Student, Exercise, Knowledge concept) paradigm. CCD has two main parts: cognition representations and a two-stage diagnostic process. In the first part, we explicitly model cognitive process (CP) dimensions from Bloom's Taxonomy of Educational Objectives, leading to two innovative concepts proposed: the student's Subjective Cognitive Ability (SCA) and the exercise's Objective Cognitive Attribute (OCA), derived by regulating the CP through S-K and E-K interactions, respectively. Then, the SCA and OCA are formed into a new cognition-aware latent trait θ. In the second part, we employ a basic interaction function and a slip and guess influence function, inputting our new θ, a continuous Q-matrix (generated by a siamese PLMs), and other features to obtain the ideal result, followed by feeding it into the slip and guess influence function to obtain the actual result. Extensive experiments on real-world datasets demonstrates the superior effectiveness and good interpretability.}
}


@inproceedings{DBLP:conf/kdd/GuHC25,
	author = {Yuechun Gu and
                  Jiajie He and
                  Keke Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Adaptive Domain Inference Attack with Concept Hierarchy},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {413--424},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709332},
	doi = {10.1145/3690624.3709332},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GuHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With increasingly deployed deep neural networks in sensitive application domains, such as healthcare and security, it's essential to understand what kind of sensitive information can be inferred from these models. Most known model-targeted attacks assume attackers have learned the application domain or training data distribution to ensure successful attacks. Can removing the domain information from model APIs protect models from these attacks? This paper studies this critical problem. Unfortunately, even with minimal knowledge, i.e., accessing the model as an unnamed function without leaking the meaning of input and output, the proposed adaptive domain inference attack (ADI) can still successfully estimate relevant subsets of training data. We show that the extracted relevant data can significantly improve, for instance, the performance of model-inversion attacks. Specifically, the ADI method utilizes the concept hierarchy extracted from the public and private datasets that the attacker can access and applies a novel algorithm to adaptively tune the likelihood of leaf concepts in the hierarchy showing up in the unseen training data. For comparison, we also designed a straightforward hypothesis-testing-based attack -- LDI. The ADI attack not only extracts partial training data at the concept level but also converges fastest and requires the fewest target-model accesses among all candidate methods. Our code is available at https://anonymous.4open.science/r/KDD-362D.}
}


@inproceedings{DBLP:conf/kdd/HanKKY25,
	author = {Gwangseok Han and
                  Wonbin Kweon and
                  Minsoo Kim and
                  Hwanjo Yu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Controlling Diversity at Inference: Guiding Diffusion Recommender
                  Models with Targeted Category Preferences},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {425--435},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709216},
	doi = {10.1145/3690624.3709216},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HanKKY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Diversity control is an important task to alleviate bias amplification and filter bubble problems. The desired degree of diversity may fluctuate based on users' daily moods or business strategies. However, existing methods for controlling diversity often lack flexibility, as diversity is decided during training and cannot be easily modified during inference. We propose  D3Rec  (Disentangled Diffusion model for Diversified Recommendation), an end-to-end method that controls the accuracy-diversity trade-off at inference. D3Rec meets our three desiderata by (1) generating recommendations based on category preferences, (2) controlling category preferences during the inference phase, and (3) adapting to arbitrary targeted category preferences. In the forward process, D3Rec removes category preferences lurking in user interactions by adding noises. Then, in the reverse process, D3Rec generates recommendations through denoising steps while reflecting desired category preferences. Extensive experiments on real-world and synthetic datasets validate the effectiveness of D3Rec in controlling diversity at inference.}
}


@inproceedings{DBLP:conf/kdd/He0Y25,
	author = {Neil He and
                  Menglin Yang and
                  Rex Ying},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Lorentzian Residual Neural Networks},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {436--447},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709292},
	doi = {10.1145/3690624.3709292},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/He0Y25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hyperbolic neural networks have emerged as a powerful tool for modeling hierarchical data structures prevalent in real-world datasets. Notably, residual connections, which facilitate the direct flow of information across layers, have been instrumental in the success of deep neural networks. However, current methods for constructing hyperbolic residual networks suffer from limitations such as increased model complexity, numerical instability, and errors due to multiple mappings to and from the tangent space. To address these limitations, we introduce LResNet, a novel Lorentzian residual neural network based on the  weighted Lorentzian centroid in the Lorentz model of hyperbolic geometry.  Our method enables the efficient integration of residual connections in Lorentz hyperbolic neural networks while preserving their hierarchical representation capabilities. We demonstrate that our method can theoretically derive previous methods while offering improved stability, efficiency, and effectiveness. Extensive experiments on both graph and vision tasks showcase the superior performance and robustness of our method compared to state-of-the-art Euclidean and hyperbolic alternatives. Our findings highlight the potential of LResNet for building more expressive neural networks in hyperbolic embedding space as a generally applicable method to multiple architectures, including CNNs, GNNs, and graph Transformers.}
}


@inproceedings{DBLP:conf/kdd/HeSHH25,
	author = {Yufei He and
                  Yuan Sui and
                  Xiaoxin He and
                  Bryan Hooi},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {UniGraph: Learning a Unified Cross-Domain Foundation Model for Text-Attributed
                  Graphs},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {448--459},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709277},
	doi = {10.1145/3690624.3709277},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HeSHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we recognize text as an effective unifying medium and employ Text-Attributed Graphs (TAGs) to leverage this potential. We present our UniGraph framework, designed to learn a foundation model for TAGs, which is capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages textual features for unifying node representations, even for graphs such as molecular graphs that do not naturally have textual features. We propose a novel cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks. Additionally, we propose the first pre-training algorithm specifically designed for large-scale self-supervised learning on TAGs, based on Masked Graph Modeling. We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets.}
}


@inproceedings{DBLP:conf/kdd/HigashiguchiMKM25,
	author = {Shingo Higashiguchi and
                  Yasuko Matsubara and
                  Koki Kawabata and
                  Taichi Murayama and
                  Yasushi Sakurai},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data
                  Streams},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {460--471},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709192},
	doi = {10.1145/3690624.3709192},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HigashiguchiMKM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large quantities of social activity data, such as weekly web search volumes and the number of new infections with infectious diseases, reflect peoples' interests and activities. It is important to discover temporal patterns from such data and to forecast future activities accurately. However, modeling and forecasting social activity data streams is difficult because they are high-dimensional and composed of multiple time-varying dynamics such as trends, seasonality, and interest diffusion. In this paper, we propose D-T racker , a method for continuously capturing time-varying temporal patterns within social activity tensor data streams and forecasting future activities. Our proposed method has the following properties: (a)  Interpretable : it incorporates the partial differential equation into a tensor decomposition framework and captures time-varying temporal patterns such as trends, seasonality, and interest diffusion between locations in an interpretable manner; (b)  Automatic:  it has no hyperparameters and continuously models tensor data streams fully automatically; (c)  Scalable:  the computation time of D-T racker  is independent of the time series length. Experiments using web search volume data obtained from GoogleTrends, and COVID-19 infection data obtained from COVID-19 Open Data Repository show that our method can achieve higher forecasting accuracy in less computation time than existing methods while extracting the interest diffusion between locations. Our source code and datasets are available at https://github.com/Higashiguchi-Shingo/D-Tracker.}
}


@inproceedings{DBLP:conf/kdd/Hou00HBW025,
	author = {Min Hou and
                  Yueying Wu and
                  Chang Xu and
                  Yu{-}Hao Huang and
                  Chenxi Bai and
                  Le Wu and
                  Jiang Bian},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {472--483},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709165},
	doi = {10.1145/3690624.3709165},
	timestamp = {Fri, 09 May 2025 20:27:53 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Hou00HBW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the most successful generative models, diffusion models have demonstrated remarkable efficacy in synthesizing high-quality images. These models learn the underlying high-dimensional data distribution in an unsupervised manner. Despite their success, diffusion models are highly data-driven and prone to inheriting the imbalances and biases present in real-world data. Some studies have attempted to address these issues by designing text prompts for known biases or using bias labels to construct unbiased data. While these methods have shown improved results, real-world scenarios often contain various unknown biases, and obtaining bias labels is particularly challenging. In this paper, we emphasize the necessity of mitigating bias in pre-trained diffusion models without relying on auxiliary bias annotations. To tackle this problem, we propose a framework,  InvDiff,  which aims to learn invariant semantic information for diffusion guidance. Specifically, we propose identifying underlying biases in the training data and designing a novel debiasing training objective. Then, we employ a lightweight trainable module that automatically preserves invariant semantic information and uses it to guide the diffusion model's sampling process toward unbiased outcomes simultaneously. Notably, we only need to learn a small number of parameters in the lightweight learnable module without altering the pre-trained diffusion model. Furthermore, we provide a theoretical guarantee that the implementation of  InvDiff  is equivalent to reducing the error upper bound of generalization. Extensive experimental results on three publicly available benchmarks demonstrate that  InvDiff  effectively reduces biases while maintaining the quality of image generation. Our code is available at https://github.com/Hundredl/InvDiff.}
}


@inproceedings{DBLP:conf/kdd/HouYY0XS25,
	author = {Yunbo Hou and
                  Haoran Ye and
                  Shuwen Yang and
                  Yingxue Zhang and
                  Siyuan Xu and
                  Guojie Song},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {TransPlace: Transferable Circuit Global Placement via Graph Neural
                  Network},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {484--495},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709185},
	doi = {10.1145/3690624.3709185},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HouYY0XS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Global placement, a critical step in designing the physical layout of computer chips, is essential to optimize chip performance. Prior global placement methods optimize each circuit design individually from scratch. Their neglect of transferable knowledge limits solution efficiency and chip performance as circuit complexity drastically increases. This study presents TransPlace, a global placement framework that learns to place millions of mixed-size cells in continuous space. TransPlace introduces i) Netlist Graph to efficiently model netlist topology, ii) Cell-flow and relative position encoding to learn SE(2)-invariant representation, iii) a tailored graph neural network architecture for informed parameterization of placement knowledge, and iv) a two-stage strategy for coarse-to-fine placement. Compared to state-of-the-art placement methods, TransPlace-trained on a few high-quality placements-can place unseen circuits with 1.2x speedup while reducing congestion by 30%, timing by 9%, and wirelength by 5%.}
}


@inproceedings{DBLP:conf/kdd/Hu0XSLLLR25,
	author = {Mengkang Hu and
                  Pu Zhao and
                  Can Xu and
                  Qingfeng Sun and
                  Jian{-}Guang Lou and
                  Qingwei Lin and
                  Ping Luo and
                  Saravan Rajmohan},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {AgentGen: Enhancing Planning Abilities for Large Language Model based
                  Agent via Environment and Task Generation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {496--507},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709321},
	doi = {10.1145/3690624.3709321},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Hu0XSLLLR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Language Model (LLM) based agents have garnered significant attention and are becoming increasingly popular. Furthermore,  planning  ability is a crucial component of an LLM-based agent, involving interaction with the  environment  and executing actions to complete a  planning task,  which generally entails achieving a desired goal from an initial state. This paper investigates enhancing the planning abilities of LLM-based agents through instruction tuning, referred to as  agent training.  Recent studies on agent training have demonstrated that utilizing expert-level trajectory data (sequences of action-observation pairs) for instruction-tuning LLMs effectively enhances their planning capabilities. However, existing work primarily focuses on synthesizing trajectories from manually designed planning tasks and environments. The labor-intensive nature of creating these environments and tasks impedes the generation of sufficiently varied and extensive trajectories for agent training. To address this limitation, this paper explores the automated synthesis of diverse environments and a gradual range of planning tasks, from easy to difficult. We introduce a framework, A gent G en , that leverages LLMs first to generate environments and subsequently generate planning tasks conditioned on these environments. Specifically, to improve  environmental diversity,  we propose using an inspiration corpus composed of various domain-specific text segments as the context for synthesizing environments. Moreover, to increase the  difficulty diversity  of generated planning tasks, we propose a bidirectional evolution method, B i -E vol , that evolves planning tasks from easier and harder directions to synthesize a task set with a smoother difficulty curve, thereby enhancing the learning process of LLMs more effectively. These methods collectively contribute to the generation of diverse trajectory data for instruction-tuning. Based on A gent G en , we greatly expanded the number of environments and planning tasks available for agent training. The evaluation results from AgentBoard indicate that A gent G en  greatly enhances the planning capabilities of LLMs. For instance, the A gent G en  instruction-tuned Llama-3.1-8B outperforms GPT-3.5 in overall performance. Moreover, the A gent G en -tuned Llama-3.1-70B model achieves state-of-the-art results in planning tasks. Project page: https://agent-gen.github.io/.}
}


@inproceedings{DBLP:conf/kdd/HuG0P25,
	author = {Zhiwei Hu and
                  V{\'{\i}}ctor Guti{\'{e}}rrez{-}Basulto and
                  Ru Li and
                  Jeff Z. Pan},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Multi-level Matching Network for Multimodal Entity Linking},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {508--519},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709306},
	doi = {10.1145/3690624.3709306},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HuG0P25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multimodal entity linking (MEL) aims to link ambiguous mentions within multimodal contexts to corresponding entities in a multimodal knowledge base. Most existing approaches to MEL are based on representation learning or vision-and-language pre-training mechanisms for exploring the complementary effect among multiple modalities. However, these methods suffer from two limitations. On the one hand, they overlook the possibility of considering negative samples from the same modality. On the other hand, they lack mechanisms to capture bidirectional cross-modal interaction. To address these issues, we propose a  M ulti-level  M atching network for  M ultimodal  E ntity  L inking( M 3 EL ). Specifically, M 3 EL is composed of three different modules: ( i ) a  Multimodal Feature Extraction  module, which extracts modality-specific representations with a multimodal encoder and introduces an intra-modal contrastive learning sub-module to obtain better discriminative embeddings based on uni-modal differences; ( ii ) an  Intra-modal Matching Network  module, which contains two levels of matching granularity:  Coarse-grained Global-to-Global  and  Fine-grained Global-to-Local , to achieve local and global level intra-modal interaction; ( iii ) a  Cross-modal Matching Network  module, which applies bidirectional strategies,  Textual-to-Visual  and  Visual-to-Textual  matching, to implement bidirectional cross-modal interaction. Extensive experiments conducted on WikiMEL, RichpediaMEL, and WikiDiverse datasets demonstrate the outstanding performance of M 3 EL when compared to the state-of-the-art baselines.}
}


@inproceedings{DBLP:conf/kdd/Huang025,
	author = {Jinchao Huang and
                  Sibo Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{DIPS:} Optimal Dynamic Index for Poisson {\(\pi\)}ps Sampling},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {520--531},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709162},
	doi = {10.1145/3690624.3709162},
	timestamp = {Mon, 05 May 2025 07:55:34 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Huang025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the Poisson πps sampling problem, a topic of significant academic interest in various domains and with practical data mining applications, such as influence maximization. The problem includes a set  S  of  n  elements, where each element  v  is assigned a weight  w ( v ) reflecting its importance. The goal is to generate a random subset  X  of  S , where each element  v  ∈  S  is included in  X  independently with probability  c  ⋅  w ( v )over ∑ v  ∈  S w ( v ), where 0 <  c ≤ 1 is a constant. The subsets must be independent across different queries. While the Poisson πps sampling problem can be reduced to the well-studied subset sampling problem, updates in Poisson πps sampling, such as adding a new element or removing an element, would cause the probabilities of all  n  elements to change in the corresponding subset sampling problem, making this approach impractical for dynamic scenarios. To address this, we propose a dynamic index specifically tailored for the Poisson πps sampling problem, supporting optimal expected  O  (1) query time and  O  (1) index update time, with an optimal  O  (n) space cost. Our solution involves recursively partitioning the set by weights and ultimately using table lookup. The core of our solution lies in addressing the challenges posed by weight explosion and correlations between elements. Empirical evaluations demonstrate that our approach achieves significant speedups in update time while maintaining consistently competitive query time compared to the subset-sampling-based methods.}
}


@inproceedings{DBLP:conf/kdd/HuangXGZL025,
	author = {Li Huang and
                  Yanzhe Xie and
                  Qiang Gao and
                  Kunpeng Zhang and
                  Guisong Liu and
                  Xueqin Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Progressive Dependency Representation Learning for Stock Ranking in
                  Uncertain Risk Contrasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {532--543},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709189},
	doi = {10.1145/3690624.3709189},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HuangXGZL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The practice of ranking a list of stocks to facilitate investment decisions has garnered a lot of attention in the fintech field, aiming at minimizing investment risk while maximizing profitable returns. With recent developments in deep representation learning such as temporal/relational dependency, prior efforts either strive to explore the temporal dynamics behind distinct stocks or expect to expose the collaborative signals from predefined relations, resulting in promising achievements in stock ranking. However, owing to the profound or intricate fluctuations of stock markets, existing insights rarely consider the uncertain risks underlying the learning of dependency representation, which could bring a narrow perspective on how to perceive market laws and ultimately yield an unprofitable decision-making procedure. In this study, we introduce a novel  P rogressive  D ependency representation learning solution with  U ncertain risk contrasting ( PDU ), primarily seeking to progressively uncover multiple dependency dynamics from historical trading signals for stock ranking in addition to addressing the uncertain risks. Specifically, we devise a  P rogressive  D ependency learning block (or  PD ) in PDU that can progressively capture the temporal and relational dependencies besides multi-term dependencies in the latent space, allowing a coupled exposure of diffusion impacts over historical trading. Furthermore, we introduce an uncertain risk contrasting mechanism in PDU by placing the PD block in a contrastive environment (i.e., certainty vs. uncertainty), aiming to stably enhance dependency learning in the latent space. The experimental results conducted on four real-world stock market datasets demonstrate the superiority of PDU over several baselines.}
}


@inproceedings{DBLP:conf/kdd/HuangXH25,
	author = {Liang Huang and
                  Kelin Xia and
                  Chuan{-}Shen Hu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Path Complex Neural Networks for Sequential Process Activities Classification},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {544--554},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709193},
	doi = {10.1145/3690624.3709193},
	timestamp = {Thu, 01 May 2025 20:24:57 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HuangXH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Process mining aims to uncover, track, and enhance real-world workflows by deriving insights from event logs commonly found in modern information systems. With the growing focus on improving productivity within complex business operations, recent research has looked into developing process models to improve business performance metrics. As such, this study aims to enhance process mining from event logs by proposing a novel path-complex construction based on process mining sequential data and a path-complex-based message-passing mechanism for higher-order structural information. We adopt path-complex representations for event logs and their temporal connections developed from instance graphs. Representations are identified and optimised for 0-paths (events), 1-paths (two events in chronological order) and 2-paths (three consecutive events) to characterise intrinsic higher-order information among events. The proposed framework, Path Complex Neural Networks (PCNN), leverages the advantages of topological deep learning and obtains representations for higher-order complexes inductively. Additionally, we evaluated the results with four real-world benchmark datasets and found that PCNN outperforms existing models in analysing sequential and complex process data.}
}


@inproceedings{DBLP:conf/kdd/HuangL25,
	author = {Mingyu Huang and
                  Ke Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {On the Hyperparameter Loss Landscapes of Machine Learning Models:
                  An Exploratory Study},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {555--564},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709229},
	doi = {10.1145/3690624.3709229},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HuangL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Previous efforts on hyperparameter optimization (HPO) of machine learning (ML) models predominately focus on algorithmic advances, yet little is known about the topography of the underlying hyperparameter (HP) loss landscape, which plays a fundamental role in governing the search process of HPO. While several works have conducted fitness landscape analysis (FLA) on various ML systems, they are limited to properties of isolated landscape without interrogating the potential structural similarities among landscapes induced on different scenarios. The exploration of such similarities can provide a novel perspective for understanding the mechanism behind modern HPO methods, but has been missing. In this paper, we mapped 1,500 HP loss landscapes of 6 representative ML models on 63 datasets across different fidelity levels, with 11M+ configurations. By conducting exploratory analysis on these landscapes with fine-grained visualizations and dedicated FLA metrics, we observed a similar landscape topography across a wide range of models, datasets, and fidelities, and shed light on the mechanism behind the success of several popular methods in HPO. The artifacts associated with this paper is available at https://github.com/COLA-Laboratory/GraphFLA.}
}


@inproceedings{DBLP:conf/kdd/HuynhBF25,
	author = {Van Quoc Phuong Huynh and
                  Florian Beck and
                  Johannes F{\"{u}}rnkranz},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Partial Pre-Post Code Tree: {A} Memory-Efficient Tree Structure for
                  Conjunctive Rule Mining},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {565--576},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709303},
	doi = {10.1145/3690624.3709303},
	timestamp = {Thu, 01 May 2025 20:24:57 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HuynhBF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {State-of-the-art rule mining algorithms rely on summarizing the training set into efficient data structures which allow to quickly answer arbitrary conjunctive queries about the data. The key limitation of such techniques is their memory consumption. Pre-post code trees (PPC-trees) which are the basis of several efficient association and classification rule mining algorithms, are only constructed as an intermediate representation and subsequently converted into a much more efficient N-lists structure. In this paper, we introduce partial pre-post code trees (P3C-trees), which are based on the idea that partial trees are iteratively constructed, and immediately converted into N-lists. This tight integration of these phases allows to avoid the memory bottleneck of a full PPC-tree construction, and thus enables these algorithms to tackle the memory scalability problem posed by large-scale datasets. Our experiments with big datasets confirm that the memory used by P3C-tree is orders of magnitude smaller than the memory consumed by PPC-tree, and the generated N-lists are also more effective than alternative structures such as Tidset or Diffset. Moreover, the N-list construction can also be considerably sped up with the P3C-tree structure.}
}


@inproceedings{DBLP:conf/kdd/JiZW025,
	author = {Jiahao Ji and
                  Wentao Zhang and
                  Jingyuan Wang and
                  Chao Huang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Seeing the Unseen: Learning Basis Confounder Representations for Robust
                  Traffic Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {577--588},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709201},
	doi = {10.1145/3690624.3709201},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/JiZW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic prediction is essential for intelligent transportation systems and urban computing. It aims to establish a relationship between historical traffic data  X  and future traffic states  Y  by employing various statistical or deep learning methods. However, the relations of  X  →  Y  are often influenced by external confounders that simultaneously affect both  X  and  Y , such as weather, accidents, and holidays. Existing deep-learning traffic prediction models adopt the classic front-door and back-door adjustments to address the confounder issue. However, these methods have limitations in addressing continuous or undefined confounders, as they depend on predefined discrete values that are often impractical in complex, real-world scenarios. To overcome this challenge, we propose the Spatial-Temporal sElf-superVised confoundEr learning (STEVE) model. This model introduces a basis vector approach, creating a base confounder bank to represent any confounder as a linear combination of a group of basis vectors. It also incorporates self-supervised auxiliary tasks to enhance the expressive power of the base confounder bank. Afterward, a confounder-irrelevant relation decoupling module is adopted to separate the confounder effects from direct  X  →  Y  relations. Extensive experiments across four large-scale datasets validate our model's superior performance in handling spatial and temporal distribution shifts and underscore its adaptability to unseen confounders. Our model implementation is available at https://github.com/bigscity/STEVE_CODE.}
}


@inproceedings{DBLP:conf/kdd/JiangZHW0K25,
	author = {Wenqi Jiang and
                  Shuai Zhang and
                  Boran Han and
                  Jie Wang and
                  Bernie Wang and
                  Tim Kraska},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {PipeRAG: Fast Retrieval-Augmented Generation via Adaptive Pipeline
                  Parallelism},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {589--600},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709194},
	doi = {10.1145/3690624.3709194},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/JiangZHW0K25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Retrieval-augmented generation (RAG) can enhance the generation quality of large language models (LLMs) by incorporating external token databases. However, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation. In this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality. PipeRAG integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware. Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6× speedup in end-to-end generation latency while improving generation quality. These promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of PipeRAG in future RAG systems.}
}


@inproceedings{DBLP:conf/kdd/JinC00025,
	author = {Jiarui Jin and
                  Xianyu Chen and
                  Weinan Zhang and
                  Yong Yu and
                  Jun Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Why Not Together? {A} Multiple-Round Recommender System for Queries
                  and Items},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {601--612},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709261},
	doi = {10.1145/3690624.3709261},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/JinC00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A fundamental technique of recommender systems involves modeling user preferences, where queries and items are widely used as symbolic representations of user interests. Queries delineate user needs at an abstract level, providing a high-level description, whereas items operate on a more specific and concrete level, representing the granular facets of user preference. While practical, both query and item recommendations encounter the challenge of sparse user feedback. To this end, we propose a novel approach named Multiple-round Auto Guess-and-Update System (MAGUS) that capitalizes on the synergies between both types, allowing us to leverage both query and item information to form user interests. This integrated system introduces a recursive framework that could be applied to  any  recommendation method to exploit queries and items in historical interactions and to provide recommendations for both queries and items in each interaction round. Concretely, MAGUS first represents queries and items through combinations of categorical words, and then constructs a relational graph to capture the interconnections and dependencies among these individual words and word combinations. In response to each user request, MAGUS employs an offline tuned recommendation model to assign estimated scores to words representing items; and these scores are subsequently disseminated throughout the graph, impacting each individual word or combination of words. Through multiple-round interactions, MAGUS initially guesses user interests by formulating meaningful word combinations and presenting them as potential queries or items. Subsequently, MAGUS is updated based on user feedback, enhancing its recommendations iteratively. Empirical results from testing 12 different recommendation methods demonstrate that integrating queries into item recommendations via MAGUS significantly enhances the efficiency, with which users can identify their preferred items during multiple-round interactions.}
}


@inproceedings{DBLP:conf/kdd/JoHBL0S25,
	author = {Hyeonsoo Jo and
                  Hyunjin Hwang and
                  Fanchen Bu and
                  Soo Yong Lee and
                  Chanyoung Park and
                  Kijung Shin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {On Measuring Unnoticeability of Graph Adversarial Attacks: Observations,
                  New Measure, and Applications},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {613--624},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709163},
	doi = {10.1145/3690624.3709163},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/JoHBL0S25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adversarial attacks are allegedly  unnoticeable.  Prior studies have designed attack noticeability measures on graphs, primarily using statistical tests to compare the topology of original and (possibly) attacked graphs. However, we observe two critical limitations in the existing measures. First, because the measures rely on simple rules, attackers can readily enhance their attacks to  bypass  them, reducing their attack ''noticeability'' and, yet, maintaining their attack performance. Second, because the measures naively leverage global statistics, such as degree distributions, they may entirely overlook attacks until severe perturbations occur, letting the attacks be almost ''totally unnoticeable.'' To address the limitations, we introduce  HideNSeek , a learnable measure for graph attack noticeability. First, to mitigate the bypass problem,  HideNSeek learns  to distinguish the original and (potential) attack edges using a learnable edge scorer (LEO), which scores each edge on its likelihood of being an attack. Second, to mitigate the overlooking problem,  HideNSeek  conducts  imbalance-aware aggregation  of all the edge scores to obtain the final noticeability score. Using six real-world graphs, we empirically demonstrate that  HideNSeek  effectively alleviates the observed limitations, and LEO (i.e., our learnable edge scorer) outperforms eleven competitors in distinguishing attack edges under five different attack methods. For an additional application, we show that LEO boost the performance of robust GNNs by removing attack-like edges.}
}


@inproceedings{DBLP:conf/kdd/KachanSG25,
	author = {Oleg Kachan and
                  Andrey V. Savchenko and
                  Gleb Gusev},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Simplicial {SMOTE:} Oversampling Solution to the Imbalanced Learning
                  Problem},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {625--635},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709268},
	doi = {10.1145/3690624.3709268},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/KachanSG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {SMOTE (Synthetic Minority Oversampling Technique) is the established geometric approach to random oversampling to balance classes in the imbalanced learning problem, followed by many extensions. Its idea is to introduce synthetic data points of the minor class, with each new point being the convex combination of an existing data point and one of its  k -nearest neighbors. In this paper, by viewing SMOTE as sampling from the edges of a geometric neighborhood graph and borrowing tools from the topological data analysis, we propose a novel technique, Simplicial SMOTE, that samples from the simplices of a geometric neighborhood simplicial complex. A new synthetic point is defined by the barycentric coordinates w.r.t. a simplex spanned by an arbitrary number of data points being sufficiently close rather than a pair. Such a replacement of the geometric data model results in better coverage of the underlying data distribution compared to existing geometric sampling methods and allows the generation of synthetic points of the minority class closer to the majority class on the decision boundary. We experimentally demonstrate that our Simplicial SMOTE outperforms several popular geometric sampling methods, including the original SMOTE. Moreover, we show that simplicial sampling can be easily integrated into existing SMOTE extensions. We generalize and evaluate simplicial extensions of the classic Borderline SMOTE, Safe-level SMOTE, and ADASYN algorithms, all of which outperform their graph-based counterparts.}
}


@inproceedings{DBLP:conf/kdd/KongZW25,
	author = {Fanshuang Kong and
                  Richong Zhang and
                  Ziqiao Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {LH-Mix: Local Hierarchy Correlation Guided Mixup over Hierarchical
                  Prompt Tuning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {636--646},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709326},
	doi = {10.1145/3690624.3709326},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/KongZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hierarchical text classification (HTC) aims to assign one or more labels in the hierarchy for each text. Many methods represent this structure as a global hierarchy, leading to redundant graph structures. To address this, incorporating a text-specific local hierarchy is essential. However, existing approaches often model this local hierarchy as a sequence, focusing on explicit parent-child relationships while ignoring implicit correlations among sibling/peer relationships. In this paper, we first integrate local hierarchies into a manual depth-level prompt to capture parent-child relationships. We then apply Mixup to this hierarchical prompt tuning scheme to improve the latent correlation within sibling/peer relationships. Notably, we propose a novel Mixup ratio guided by local hierarchy correlation to effectively capture intrinsic correlations. This Local Hierarchy Mixup (LH-Mix) model demonstrates remarkable performance across three widely-used datasets.}
}


@inproceedings{DBLP:conf/kdd/LeeLY0K25,
	author = {Yeon{-}Chang Lee and
                  JaeHyun Lee and
                  Michiharu Yamashita and
                  Dongwon Lee and
                  Sang{-}Wook Kim},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{CAPER:} Enhancing Career Trajectory Prediction using Temporal Knowledge
                  Graph and Ternary Relationship},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {647--658},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709329},
	doi = {10.1145/3690624.3709329},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LeeLY0K25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of  career trajectory prediction  (CTP) aims to predict one's future employer or job position. While several CTP methods have been developed for this problem, we posit that none of these methods (1) jointly considers the mutual ternary dependency between three key units ( i.e. , user, position, and company) of a career and (2) captures the characteristic shifts of key units in career over time, leading to an inaccurate understanding of the job movement patterns in the labor market. To address the above challenges, we propose a novel solution, named as CAPER, that solves the challenges via sophisticated temporal knowledge graph (TKG) modeling. It enables the utilization of a graph-structured knowledge base with rich expressiveness, effectively preserving the changes in job movement patterns. Furthermore, we devise an extrapolated career reasoning task on TKG for a realistic evaluation. The experiments on a real-world career trajectory dataset demonstrate that CAPER consistently and significantly outperforms four baselines, two recent TKG reasoning methods, and five state-of-the-art CTP methods in predicting one's future companies and positions--i.e., on average, yielding 6.80% and 34.58% more accurate predictions, respectively. The codebase of CAPER is available at https://github.com/Bigdasgit/CAPER.}
}


@inproceedings{DBLP:conf/kdd/LiRL025,
	author = {Jian Li and
                  Pu Ren and
                  Yang Liu and
                  Hao Sun},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Reasoning-Enhanced Object-Centric Learning for Videos},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {659--670},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709168},
	doi = {10.1145/3690624.3709168},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiRL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatiotemporal attention computations and fusion. Our experimental results on various datasets indicate that the STATM module can significantly enhance the capabilities of multiple state-of-the-art object-centric learning models for video. Moreover, as a predictive model, the STATM module also performs well in downstream prediction and Visual Question Answering (VQA) tasks. We will release our codes and data at https://github.com/intell-sci-comput/STATM.}
}


@inproceedings{DBLP:conf/kdd/00030CB0W25,
	author = {Mengxuan Li and
                  Ke Liu and
                  Hongyang Chen and
                  Jiajun Bu and
                  Hongwei Wang and
                  Haishuai Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{TSINR:} Capturing Temporal Continuity via Implicit Neural Representations
                  for Time Series Anomaly Detection},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {671--682},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709266},
	doi = {10.1145/3690624.3709266},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/00030CB0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series anomaly detection aims to identify unusual patterns in data or deviations from systems' expected behavior. The reconstruction-based methods are the mainstream in this task, which learn point-wise representation via unsupervised learning. However, the unlabeled anomaly points in training data may cause these reconstruction-based methods to learn and reconstruct anomalous data, resulting in the challenge of capturing normal patterns. In this paper, we propose a time series anomaly detection method based on implicit neural representation (INR) reconstruction, named TSINR, to address this challenge. Due to the property of spectral bias, TSINR enables prioritizing low-frequency signals and exhibiting poorer performance on high-frequency abnormal data. Specifically, we adopt INR to parameterize time series data as a continuous function and employ a transformer-based architecture to predict the INR of given data. As a result, the proposed TSINR method achieves the advantage of capturing the temporal continuity and thus is more sensitive to discontinuous anomaly data. In addition, we further design a novel form of INR continuous function to learn inter- and intra-channel information, and leverage a pre-trained large language model to amplify the intense fluctuations in anomalies. Extensive experiments demonstrate that TSINR achieves superior overall performance on both univariate and multivariate time series anomaly detection benchmarks compared to other state-of-the-art reconstruction-based methods. Our codes are available here.}
}


@inproceedings{DBLP:conf/kdd/LiC0WG25,
	author = {Qi Li and
                  Zhiguang Cao and
                  Yining Ma and
                  Yaoxin Wu and
                  Yue{-}Jiao Gong},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Diversity Optimization for Travelling Salesman Problem via Deep Reinforcement
                  Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {683--694},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709181},
	doi = {10.1145/3690624.3709181},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiC0WG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing neural methods for the Travelling Salesman Problem (TSP) mostly aim at finding a single optimal solution. To discover diverse yet high-quality solutions for Multi-Solution TSP (MSTSP), we propose a novel deep reinforcement learning based neural solver, which is primarily featured by an encoder-decoder structured policy. Concretely, on the one hand, a Relativization Filter (RF) is designed to enhance the robustness of the encoder to affine transformations of the instances, so as to potentially improve the quality of the found solutions. On the other hand, a Multi-Attentive Adaptive Active Search (MA3S) is tailored to allow the decoders to strike a balance between the optimality and diversity. Experimental evaluations on benchmark instances demonstrate the superiority of our method over recent neural baselines across different metrics, and its competitive performance against state-of-the-art traditional heuristics with significantly reduced computational time, ranging from 1.3× to 15× faster. Furthermore, we demonstrate that our method can also be applied to the Capacitated Vehicle Routing Problem (CVRP).}
}


@inproceedings{DBLP:conf/kdd/LiKLH0SZH25,
	author = {Rui Li and
                  Junfeng Kang and
                  Qi Liu and
                  Liyang He and
                  Zheng Zhang and
                  Yunhao Sha and
                  Linbo Zhu and
                  Zhenya Huang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{MGS3:} {A} Multi-Granularity Self-Supervised Code Search Framework},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {695--706},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709263},
	doi = {10.1145/3690624.3709263},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiKLH0SZH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the pursuit of enhancing software reusability and developer productivity, code search has emerged as a key area, aimed at retrieving code snippets relevant to functionalities based on natural language queries. Despite significant progress in self-supervised code pre-training utilizing the vast amount of code data in repositories, existing methods have primarily focused on leveraging contrastive learning to align natural language with function-level code snippets. These studies have overlooked the abundance of fine-grained (such as block-level and statement-level) code snippets prevalent within the function-level code snippets, which results in suboptimal performance across all levels of granularity. To address this problem, we first construct a multi-granularity code search dataset called  MGCodeSearchNet,  which contains 536K+ pairs of natural language and code snippets. Subsequently, we introduce a novel  M ulti- G ranularity  S elf- S upervised contrastive learning code  S earch framework ( MGS 3 ). First, MGS 3  features a Hierarchical Multi-Granularity Representation module (HMGR), which leverages syntactic structural relationships for hierarchical representation and aggregates fine-grained information into coarser-grained representations. Then, during the contrastive learning phase, we endeavor to construct positive samples of the same granularity for fine-grained code, and introduce in-function negative samples for fine-grained code. Finally, we conduct extensive experiments on code search benchmarks across various granularities, demonstrating that the framework exhibits outstanding performance in code search tasks of multiple granularities. These experiments also showcase its model-agnostic nature and compatibility with existing pre-trained code representation models.}
}


@inproceedings{DBLP:conf/kdd/LiZXCM25,
	author = {Weixian Waylon Li and
                  Yftah Ziser and
                  Yifei Xie and
                  Shay B. Cohen and
                  Tiejun Ma},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {TSPRank: Bridging Pairwise and Listwise Methods with a Bilinear Travelling
                  Salesman Model},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {707--718},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709234},
	doi = {10.1145/3690624.3709234},
	timestamp = {Tue, 13 May 2025 07:31:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiZXCM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional Learning-To-Rank (LETOR) approaches, including pairwise methods like RankNet and LambdaMART, often fall short by solely focusing on pairwise comparisons, leading to sub-optimal global rankings. Conversely, deep learning based listwise methods, while aiming to optimise entire lists, require complex tuning and yield only marginal improvements over robust pairwise models. To overcome these limitations, we introduce Travelling Salesman Problem Rank (TSPRank), a hybrid pairwise-listwise ranking method. TSPRank reframes the ranking problem as a Travelling Salesman Problem (TSP), a well-known combinatorial optimisation challenge that has been extensively studied for its numerous solution algorithms and applications. This approach enables the modelling of pairwise relationships and leverages combinatorial optimisation to determine the listwise ranking. TSPRank can be directly integrated as an additional component into embeddings generated by existing backbone models to enhance ranking performance. Our extensive experiments across three backbone models on diverse tasks, including stock ranking, information retrieval, and historical events ordering, demonstrate that TSPRank significantly outperforms both pure pairwise and listwise methods. Our qualitative analysis reveals that TSPRank's main advantage over existing methods is its ability to harness global information better while ranking. TSPRank's robustness and superior performance across different domains highlight its potential as a versatile and effective LETOR solution.}
}


@inproceedings{DBLP:conf/kdd/LiTS0GCYL25,
	author = {Xiaodong Li and
                  Hengzhu Tang and
                  Jiawei Sheng and
                  Xinghua Zhang and
                  Li Gao and
                  Suqi Cheng and
                  Dawei Yin and
                  Tingwen Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Exploring Preference-Guided Diffusion Model for Cross-Domain Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {719--728},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709220},
	doi = {10.1145/3690624.3709220},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiTS0GCYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain recommendation (CDR) has been proven as a promising way to alleviate the cold-start issue, in which the most critical problem is how to draw an informative user representation in the target domain via the transfer of user preference existing in the source domain. Prior efforts mostly follow the embedding-and-mapping paradigm, which first integrate the preference into user representation in the source domain, and then perform a mapping function on this representation to the target domain. However, they focus on mapping features across domains, neglecting to explicitly model the preference integration process, which may lead to learning coarse user representation. Diffusion models (DMs), which contribute to more accurate user/item representations due to their explicit information injection capability, have achieved promising performance in recommendation systems. Nevertheless, these DMs-based methods cannot directly account for valuable user preference in other domains, leading to challenges in adapting to the transfer of preference for cold-start users. Consequently, the feasibility of DMs for CDR remains underexplored. To this end, we explore to utilize the explicit information injection capability of DMs for user preference integration and propose a  Preference-Guided Diffusion Model  for CDR to cold-start users, termed as  DMCDR . Specifically, we leverage a preference encoder to establish the preference guidance signal with the user's interaction history in the source domain. Then, we explicitly inject the preference guidance signal into the user representation step by step to guide the reverse process, and ultimately generate the personalized user representation in the target domain, thus achieving the transfer of user preference across domains. Furthermore, we comprehensively explore the impact of six DMs-based variants on CDR. Extensive experiments on three real-world CDR scenarios demonstrate the superiority of our DMCDR over SOTA methods and six DMs-based variants.}
}


@inproceedings{DBLP:conf/kdd/0004SXLW25,
	author = {Zhihao Li and
                  Haoze Song and
                  Di Xiao and
                  Zhilu Lai and
                  Wei Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Harnessing Scale and Physics: {A} Multi-Graph Neural Operator Framework
                  for PDEs on Arbitrary Geometries},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {729--740},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709173},
	doi = {10.1145/3690624.3709173},
	timestamp = {Mon, 05 May 2025 07:55:32 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0004SXLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Partial Differential Equations (PDEs) underpin many scientific phenomena, yet traditional computational approaches often struggle with complex, nonlinear systems and irregular geometries. This paper introduces the  AMG  method, a  M ulti- G raph neural operator approach designed for efficiently solving PDEs on  A rbitrary geometries. AMG leverages advanced graph-based techniques and dynamic attention mechanisms within a novel GraphFormer architecture, enabling precise management of diverse spatial domains and complex data interdependencies. By constructing multi-scale graphs to handle variable feature frequencies and a physics graph to encapsulate inherent physical properties, AMG significantly outperforms previous methods, which are typically limited to uniform grids. We present a comprehensive evaluation of AMG across six benchmarks, demonstrating its consistent superiority over existing state-of-the-art models. Our findings highlight the transformative potential of tailored graph neural operators in surmounting the challenges faced by conventional PDE solvers. Our code and datasets are available on https://github.com/lizhihao2022/AMG.}
}


@inproceedings{DBLP:conf/kdd/LiFAH25,
	author = {Zihao Li and
                  Dongqi Fu and
                  Mengting Ai and
                  Jingrui He},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {APEX\({}^{\mbox{2}}\): Adaptive and Extreme Summarization for Personalized
                  Knowledge Graphs},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {741--752},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709213},
	doi = {10.1145/3690624.3709213},
	timestamp = {Thu, 01 May 2025 20:24:57 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiFAH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graphs (KGs), which store an extensive number of relational facts, serve various applications. Recently,  personalized knowledge graphs  (PKGs) have emerged as a solution to optimize storage costs by customizing their content to align with users' specific interests within particular domains. In the real world, on the one hand, user queries and their underlying interests are inherently evolving, requiring PKGs to adapt continuously; on the other hand, the summarization is constantly expected to be as small as possible in terms of storage cost. However, the existing PKG summarization methods implicitly assume that the user's interests are constant and do not shift. Furthermore, when the size constraint of PKG is extremely small, the existing methods cannot distinguish which facts are more of immediate interest and guarantee the utility of the summarized PKG. To address these limitations, we propose APEX 2 , a highly scalable PKG summarization framework designed with robust theoretical guarantees to excel in adaptive summarization tasks with extremely small size constraints. To be specific, after constructing an initial PKG, APEX 2  continuously tracks the interest shift and adjusts the previous summary. The experiments show that APEX outperforms state-of-the-art baselines in terms of both query-answering accuracy and efficiency.}
}


@inproceedings{DBLP:conf/kdd/Liang25,
	author = {Daojun Liang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {DistPred: {A} Distribution-Free Probabilistic Inference Method for
                  Regression and Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {753--764},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709286},
	doi = {10.1145/3690624.3709286},
	timestamp = {Thu, 01 May 2025 20:24:58 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Liang25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional regression and prediction tasks often only provide deterministic point estimates. To estimate the distribution or uncertainty of the response variable, traditional methods either assume that the posterior distribution of samples follows a Gaussian process or require thousands of forward passes for sample generation. We propose a novel approach called DistPred for regression and forecasting tasks, which overcomes the limitations of existing methods while remaining simple and powerful. Specifically, we transform proper scoring rules that measure the discrepancy between the predicted distribution and the target distribution into a differentiable discrete form and use it as a loss function to train the model end-to-end. This allows the model to sample numerous samples in a single forward pass to estimate the potential distribution of the response variable. We have compared our method with several existing approaches on multiple datasets and achieved state-of-the-art performance. Additionally, our method significantly improves computational efficiency. For example, compared to state-of-the-art models, DistPred has a 180x faster inference speed.}
}


@inproceedings{DBLP:conf/kdd/LiaoZZWWCWM25,
	author = {Weibin Liao and
                  Yinghao Zhu and
                  Zhongji Zhang and
                  Yuhang Wang and
                  Zixiang Wang and
                  Xu Chu and
                  Yasha Wang and
                  Liantao Ma},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Learnable Prompt as Pseudo-Imputation: Rethinking the Necessity of
                  Traditional {EHR} Data Imputation in Downstream Clinical Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {765--776},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709166},
	doi = {10.1145/3690624.3709166},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiaoZZWWCWM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analyzing the health status of patients based on Electronic Health Records (EHR) is a fundamental research problem in medical informatics. The presence of extensive missing values in EHR makes it challenging for deep neural networks (DNNs) to directly model the patient's health status. Existing DNNs training protocols, including  Impute-then-Regress Procedure  and  Jointly Optimizing of Impute-n-Regress Procedure,  require the additional imputation models to reconstruction missing values. However,  Impute-then-Regress Procedure  introduces the risk of injecting imputed, non-real data into downstream clinical prediction tasks, resulting in power loss, biased estimation, and poorly performing models, while  Jointly Optimizing of Impute-n-Regress Procedure  is also difficult to generalize due to the complex optimization space and demanding data requirements. Inspired by the recent advanced literature of learnable prompt in the fields of NLP and CV, in this work, we rethought the necessity of the imputation model in downstream clinical tasks, and proposed Learnable Prompt as Pseudo-Imputation (PAI) as a new training protocol to assist EHR analysis. PAI no longer introduces any imputed data but constructs a learnable prompt to model the implicit preferences of the downstream model for missing values, resulting in a significant performance improvement for  all state-of-the-arts EHR analysis models  on four real-world datasets across two clinical prediction tasks. Further experimental analysis indicates that PAI exhibits higher robustness in situations of data insufficiency and high missing rates. More importantly, as a plug-and-play protocol, PAI can be easily integrated into any existing or even imperceptible future EHR analysis models. The code of this work is deployed publicly available at https://github.com/MrBlankness/PAI to help the research community reproduce the results and assist the EHR analysis tasks.}
}


@inproceedings{DBLP:conf/kdd/LinDXJ0W25,
	author = {Minhua Lin and
                  Enyan Dai and
                  Junjie Xu and
                  Jinyuan Jia and
                  Xiang Zhang and
                  Suhang Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Stealing Training Graphs from Graph Neural Networks},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {777--788},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709289},
	doi = {10.1145/3690624.3709289},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LinDXJ0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have shown promising results in modeling graphs in various tasks. The training of GNNs, especially on specialized tasks such as bioinformatics, demands extensive expert annotations, which are expensive and usually contain sensitive information of data providers. The trained GNN models are often shared for deployment in the real world. As neural networks can memorize the training samples, the model parameters of GNNs have a high risk of leaking private training data. Our theoretical analysis shows the strong connections between trained GNN parameters and the training graphs used, confirming the training graph leakage issue. However, explorations into training data leakage from trained GNNs are rather limited. Therefore, we investigate a novel problem of stealing graphs from trained GNNs. To obtain high-quality graphs that resemble the target training set, a graph diffusion model with diffusion noise optimization is deployed as a graph generator. Furthermore, we propose a selection method that effectively leverages GNN model parameters to identify training graphs from samples generated by the graph diffusion model. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed framework in stealing training graphs from the trained GNN.}
}


@inproceedings{DBLP:conf/kdd/LinYZK25,
	author = {Xiaoyang Lin and
                  Renchi Yang and
                  Haoran Zheng and
                  Xiangyu Ke},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Spectral Subspace Clustering for Attributed Graphs},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {789--799},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709207},
	doi = {10.1145/3690624.3709207},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LinYZK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subspace clustering seeks to identify subspaces that segment a set of  n  data points into  k  ( k «  n ) groups, which has emerged as a powerful tool for analyzing data from various domains, especially images and videos. Recently, several studies have demonstrated the great potential of subspace clustering models for partitioning vertices in attributed graphs, referred to as SCAG. However, these works either demand significant computational overhead for constructing the  n x n  self-expressive matrix, or fail to incorporate graph topology and attribute data into the subspace clustering framework effectively, and thus, compromise result quality. Motivated by this, this paper presents two effective and efficient algorithms, S 2 CAG M-S 2 CAG for SCAG computation. Particularly, S 2 CAG obtains superb performance through three major contributions. First, we formulate a new objective function for SCAG with a refined representation model for vertices and two non-trivial constraints. On top of that, an efficient linear-time optimization solver is developed based on our theoretically grounded problem transformation and well-thought-out adaptive strategy. We then conduct an in-depth analysis to disclose the theoretical connection of S 2 CAG to conductance minimization, which further inspires the design of M-S 2 CAG that maximizes the modularity. Our extensive experiments, comparing S 2 CAG and M-S 2 CAG against 17 competitors over 8 benchmark datasets, exhibit that our solutions outperform all baselines in terms of clustering quality measured against the ground truth while delivering high efficiency.}
}


@inproceedings{DBLP:conf/kdd/LiuYJGTGJ25,
	author = {Gang Liu and
                  Fan Yang and
                  Yang Jiao and
                  Alireza Bagheri Garakani and
                  Tian Tong and
                  Yan Gao and
                  Meng Jiang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Learning Attribute as Explicit Relation for Sequential Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {800--811},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709267},
	doi = {10.1145/3690624.3709267},
	timestamp = {Tue, 08 Apr 2025 16:34:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiuYJGTGJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The data on user behaviors is sparse given the vast array of user-item combinations. Attributes related to users (e.g., age), items (e.g., brand), and behaviors (e.g., co-purchase) serve as crucial input sources for item-item transitions of user's behavior prediction. While recent Transformer-based sequential recommender systems learn the attention matrix for each attribute to update item representations, the attention of a specific attribute is optimized by gradients from all input sources, leading to potential information mixture. Besides, Transformers mainly focus on intra-sequence attention for item attributes, neglecting cross-sequence relations and user attributes. Addressing these challenges, we propose the  Att ribute Trans former  (AttrFormer) to learn attributes as explicit relations. This model transforms each type of attribute into an explicit relation defined in the feature space, and it ensures no information mixing among different input sources. Explicit relations introduce cross-sequence and intra-sequence relations. AttrFormer has novel relation-augmented heads to handle them at both the item and behavioral levels, seamlessly integrating the augmented heads into the multi-head attention mechanism. Furthermore, we employ position-to-position aggregation to refine behavior representation for users with similar patterns at the sequence level. To capture the subjective nature of user preferences, AttrFormer is trained using posterior targets where upcoming user behaviors follow a multinomial distribution with a Dirichlet prior. Our evaluations on four popular datasets, including Amazon (Toys & Games and Beauty) and MovieLens (1M and 25M versions), reveal that AttrFormer outperforms leading Transformer baselines, achieving around 20% improvement in NDCG@20 scores. Extensive ablation studies also demonstrate the efficiency of AttrFormer in managing long behavior sequences and inter-sequence relations.}
}


@inproceedings{DBLP:conf/kdd/LiuG0LZ0M025,
	author = {Han Liu and
                  Haotian Gao and
                  Xiaotong Zhang and
                  Changya Li and
                  Feng Zhang and
                  Wei Wang and
                  Fenglong Ma and
                  Hong Yu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{SEPTQ:} {A} Simple and Effective Post-Training Quantization Paradigm
                  for Large Language Models},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {812--823},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709287},
	doi = {10.1145/3690624.3709287},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiuG0LZ0M025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have shown remarkable performance in various domains, but they are constrained by massive computational and storage costs. Quantization, an effective technique for compressing models to fit resource-limited devices while preserving generative quality, encompasses two primary methods: quantization aware training (QAT) and post-training quantization (PTQ). QAT involves additional retraining or fine-tuning, thus inevitably resulting in high training cost and making it unsuitable for LLMs. Consequently, PTQ has become the research hotspot in recent quantization methods. However, existing PTQ methods usually rely on various complex computation procedures and suffer from considerable performance degradation under low-bit quantization settings. To alleviate the above issues, we propose a simple and effective post-training quantization paradigm for LLMs, named SEPTQ. Specifically, SEPTQ first calculates the importance score for each element in the weight matrix and determines the quantization locations in a static global manner. Then it utilizes the mask matrix which represents the important locations to quantize and update the associated weights column-by-column until the appropriate quantized weight matrix is obtained. Compared with previous methods, SEPTQ simplifies the post-training quantization procedure into only two steps, and considers the effectiveness and efficiency simultaneously. Experimental results on various datasets across a suite of models ranging from millions to billions in different quantization bit-levels demonstrate that SEPTQ significantly outperforms other strong baselines, especially in low-bit quantization scenarios.}
}


@inproceedings{DBLP:conf/kdd/LiuZG25,
	author = {Qu Liu and
                  Emil Zulawnik and
                  Tingjian Ge},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {SCode: {A} Spherical Code Metric Learning Approach to Continuously
                  Monitoring Predictive Events in Networked Data},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {824--835},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709246},
	doi = {10.1145/3690624.3709246},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiuZG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic graphs are common in many applications to conveniently model heterogeneous data integrated from multiple sources. We study the monitoring of predictive events in dynamic graphs. Treating the problem as a continuous multi-label classification, we use deep metric learning to manage the embedding space and to create spherical codes where each codeword is an embedding vector representing a cluster of data state embeddings with the same results of the predictive events. By continuously training data embeddings from a dynamic graph neural network (DGNN) model and a code generator together, our method, called SCode, achieves significantly better accuracy than DGNN baselines. Moreover, SCode is also about twice as fast as the DGNN baselines, owing to its efficient matching between data state embedding and codewords for multiple events together. Finally, our training sample complexity analysis also sheds light on the generalizability of the online learning.}
}


@inproceedings{DBLP:conf/kdd/LiuZLZQ25,
	author = {Shuo Liu and
                  Zihan Zhou and
                  Yuanhao Liu and
                  Jing Zhang and
                  Hong Qian},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Language Representation Favored Zero-Shot Cross-Domain Cognitive Diagnosis},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {836--847},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709281},
	doi = {10.1145/3690624.3709281},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiuZLZQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cognitive diagnosis aims to infer students' mastery levels based on their historical response logs. However, existing cognitive diagnosis models (CDMs), which rely on ID embeddings, often have to train specific models on specific domains. This limitation may hinder their directly practical application in various target domains, such as different subjects (e.g., Math, English and Physics) or different education platforms (e.g., ASSISTments, Junyi Academy and Khan Academy). To address this issue, this paper proposes the language representation favored zero-shot cross-domain cognitive diagnosis (LRCD). Specifically, LRCD first analyzes the behavior patterns of students, exercises and concepts in different domains, and then describes the profiles of students, exercises and concepts using textual descriptions. Via recent advanced text-embedding modules, these profiles can be transformed to vectors in the unified language space. Moreover, to address the discrepancy between the language space and the cognitive diagnosis space, we propose language-cognitive mappers in LRCD to learn the mapping from the former to the latter. Then, these profiles can be easily and efficiently integrated and trained with existing CDMs. Extensive experiments show that training LRCD on real-world datasets can achieve commendable zero-shot performance across different target domains, and in some cases, it can even achieve competitive performance with some classic CDMs trained on the full response data on target domains. Notably, we surprisingly find that LRCD can also provide interesting insights into the differences between various subjects (such as humanities and sciences) and sources (such as primary and secondary education).}
}


@inproceedings{DBLP:conf/kdd/LiuWTMWC25,
	author = {Xiaohao Liu and
                  Jie Wu and
                  Zhulin Tao and
                  Yunshan Ma and
                  Yinwei Wei and
                  Tat{-}Seng Chua},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Fine-tuning Multimodal Large Language Models for Product Bundling},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {848--858},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709255},
	doi = {10.1145/3690624.3709255},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiuWTMWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in product bundling have leveraged multimodal information through sophisticated encoders, but remain constrained by limited semantic understanding and a narrow scope of knowledge. Therefore, some attempts employ In-context Learning (ICL) to explore the potential of large language models (LLMs) for their extensive knowledge and complex reasoning abilities. However, these efforts are inadequate in understanding mulitmodal data and exploiting LLMs' knowledge for product bundling. To bridge the gap, we introduce Bundle-MLLM, a novel framework that fine-tunes LLMs through a hybrid item tokenization approach within a well-designed optimization strategy. Specifically, we integrate textual, media, and relational data into a unified tokenization, introducing a soft separation token to distinguish between textual and non-textual tokens. Additionally, a streamlined yet powerful multimodal fusion module is employed to embed all non-textual features into a single, informative token, significantly boosting efficiency. To tailor product bundling tasks for LLMs, we reformulate the task as a multiple-choice question with candidate items as options. We further propose a progressive optimization strategy that fine-tunes LLMs for disentangled objectives: learning bundle patterns and enhancing multimodal semantic understanding specific to product bundling. Extensive experiments demonstrate that our approach outperforms a range of state-of-the-art (SOTA) methods. Codes are available at https://github.com/Xiaohao-Liu/Bundle-MLLM}
}


@inproceedings{DBLP:conf/kdd/LiuLGL25,
	author = {Xufeng Liu and
                  Dongsheng Luo and
                  Wenhan Gao and
                  Yi Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {3DGraphX: Explaining 3D Molecular Graph Models via Incorporating Chemical
                  Priors},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {859--870},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709302},
	doi = {10.1145/3690624.3709302},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiuLGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the explanation of 3D graph neural networks (GNNs) in the field of molecular learning. Recent studies have modeled molecules as 3D graphs, but there exist formidable challenges for 3D graph explanation. In this work, we propose a novel and principled paradigm, known as 3DGraphX, for 3D molecular graph explanation. Unlike existing 2D GNN explanation methods, 3DGraphX focuses on 3D motifs, which are subgraphs showing great occurrence and function significance in molecular activities. Once generated, 3D motifs are fixed in the explanation model; hence, 3DGraphX produces more accurate and chemically plausible explanations in an efficient manner. 3DGraphX contains two branches with several novel methods for instance-level and geometry-level explanations, respectively. Two novel components, known as the mask pooling component and mask unpooling component, are developed to discover important motifs for each 3D molecule as the instance-level explanation. Local spherical coordinate systems are built to investigate the relative positions among motifs for geometry-level explanation. Altogether, 3DGraphX sheds light on the characteristics of molecules as well as the behaviors of 3D GNNs in molecular learning. Experimental results show that 3DGraphX significantly outperforms baselines in instance-level explanation with various explanation budgets. Additional experiments show that 3DGraphX reveals the important geometries taken by 3D GNNs for accurate molecular learning. The code is publicly available at https://github.com/xufliu/3DGraphX.}
}


@inproceedings{DBLP:conf/kdd/0001G00FG25,
	author = {Yonghao Liu and
                  Fausto Giunchiglia and
                  Ximing Li and
                  Lan Huang and
                  Xiaoyue Feng and
                  Renchu Guan},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Enhancing Unsupervised Graph Few-shot Learning via Set Functions and
                  Optimal Transport},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {871--882},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709208},
	doi = {10.1145/3690624.3709208},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0001G00FG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph few-shot learning has garnered significant attention for its ability to rapidly adapt to downstream tasks with limited labeled data, sparking considerable interest among researchers. Recent advancements in graph few-shot learning models have exhibited superior performance across diverse applications. Despite their successes, several limitations still exist. First, existing models in the meta-training phase predominantly focus on instance-level features within tasks, neglecting crucial set-level features essential for distinguishing between different categories. Second, these models often utilize query sets directly on classifiers trained with support sets containing only a few labeled examples, overlooking potential distribution shifts between these sets and leading to suboptimal performance. Finally, previous models typically require necessitate abundant labeled data from base classes to extract transferable knowledge, which is typically infeasible in real-world scenarios. To address these issues, we propose a novel model named  STAR,  which leverages  S et func T ions and optim A l t R ansport for enhancing unsupervised graph few-shot learning. Specifically, STAR utilizes expressive set functions to obtain set-level features in an unsupervised manner and employs optimal transport principles to align the distributions of support and query sets, thereby mitigating distribution shift effects. Theoretical analysis demonstrates that STAR can capture more task-relevant information and enhance generalization capabilities. Empirically, extensive experiments across multiple datasets validate the effectiveness of STAR. Our code can be found https://github.com/KEAML-JLU/STAR here.}
}


@inproceedings{DBLP:conf/kdd/Liu0L0DY25,
	author = {Yuxuan Liu and
                  Hongda Sun and
                  Wei Liu and
                  Jian Luan and
                  Bo Du and
                  Rui Yan},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution
                  to Automate Cross-App Instructions},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {883--893},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709171},
	doi = {10.1145/3690624.3709171},
	timestamp = {Wed, 14 May 2025 08:12:21 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Liu0L0DY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile phone agents can assist people in automating daily tasks on their phones, which have emerged as a pivotal research spotlight. However, existing procedure-oriented agents struggle with cross-app instructions, due to the following challenges: (1) complex task relationships, (2) diverse app environment, and (3) error propagation and information loss in multi-step execution. Drawing inspiration from object-oriented programming principles, we recognize that object-oriented solutions is more suitable for cross-app instruction. To address these challenges, we propose a self-evolving multi-agent framework named  MobileSteward  which integrates multiple app-oriented StaffAgents coordinated by a centralized StewardAgent. We design three specialized modules in MobileSteward: (1)  Dynamic Recruitment  generates a scheduling graph guided by information flow to explicitly associate tasks among apps. (2)  Assigned Execution  assigns the task to app-oriented StaffAgents, each equipped with app-specialized expertise to address the diversity between apps. (3)  Adjusted Evaluation  conducts evaluation to provide reflection tips or deliver key information, which alleviates error propagation and information loss during multi-step execution. To continuously improve the performance of MobileSteward, we develop a  Memory-based Self-evolution  mechanism, which summarizes the experience from successful execution, to improve the performance of MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench) in the real-world environment to evaluate the agents' capabilities of solving complex cross-app instructions. Experimental results demonstrate that MobileSteward achieves the best performance compared to both single-agent and multi-agent frameworks, highlighting the superiority of MobileSteward in better handling user instructions with diverse complexity.}
}


@inproceedings{DBLP:conf/kdd/LongYL25,
	author = {Qingyue Long and
                  Yuan Yuan and
                  Yong Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {A Universal Model for Human Mobility Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {894--905},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709236},
	doi = {10.1145/3690624.3709236},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LongYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting human mobility is crucial for urban planning, traffic control, and emergency response. Mobility behaviors can be categorized into individual and collective, and these behaviors are recorded by diverse mobility data, such as individual trajectory and crowd flow. As different modalities of mobility data, individual trajectory and crowd flow have a close coupling relationship. Crowd flows originate from the bottom-up aggregation of individual trajectories, while the constraints imposed by crowd flows shape these individual trajectories. Existing mobility prediction methods are limited to single tasks due to modal gaps between individual trajectory and crowd flow. In this work, we aim to unify mobility prediction to break through the limitations of task-specific models. We propose a universal human mobility prediction model (named UniMob), which can be applied to both individual trajectory and crowd flow. UniMob leverages a multi-view mobility tokenizer that transforms both trajectory and flow data into spatiotemporal tokens, facilitating unified sequential modeling through a diffusion transformer architecture. To bridge the gap between the different characteristics of these two data modalities, we implement a novel bidirectional individual and collective alignment mechanism. This mechanism enables learning common spatiotemporal patterns from different mobility data, facilitating mutual enhancement of both trajectory and flow predictions. Extensive experiments on real-world datasets validate the superiority of our model over state-of-the-art baselines in trajectory and flow prediction. Especially in noisy and scarce data scenarios, our model achieves the highest performance improvement of more than 14% and 25% in MAPE and Accuracy@5. The codes are available online: https://github.com/tsinghua-fib-lab/UniMob.}
}


@inproceedings{DBLP:conf/kdd/Luo0J00JYZS25,
	author = {Xiao Luo and
                  Junyu Luo and
                  Huiyu Jiang and
                  Hang Zhou and
                  Zhiping Xiao and
                  Wei Ju and
                  Carl Ji Yang and
                  Ming Zhang and
                  Yizhou Sun},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Future Matters for Present: Towards Effective Physical Simulation
                  over Meshes},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {906--917},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709340},
	doi = {10.1145/3690624.3709340},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Luo0J00JYZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the problem of learning mesh-based physical simulations, which is a crucial task with applications in fluid mechanics and aerodynamics. Recent works typically utilize graph neural networks (GNNs) to produce next-time states on irregular meshes by modeling interacting dynamics, and then adopt iterative rollouts for the whole trajectories. However, these methods cannot achieve satisfactory performance in long-term predictions due to the failure of capturing long-term dependency and potential error accumulations. To tackle this, we introduce a new future-to-present learning perspective, and further develop a simple yet effective approach named Foresight And Interpolation (FAIR) for long-term mesh-based simulations. The main idea of our FAIR is to first learn a graph ODE model for coarse long-term predictions and then refine short-term predictions via interpolation. Specifically, FAIR employs a continuous graph ODE model that incorporates past states into the evolution of interacting node representations, which is capable of learning coarse long-term trajectories under a multi-task learning framework. Then, we leverage a channel aggregation strategy to summarize the trajectories for refined short-term predictions, which can be illustrated using an interpolation process. Through pyramid-like alternative propagation between the foresight step and refinement step, our proposed framework FAIR can generate accurate long-term trajectories, achieving a significant error reduction compared with the best baseline on four benchmark datasets. Extensive ablation studies and visualization further validate the superiority of our proposed FAIR.}
}


@inproceedings{DBLP:conf/kdd/LuoLLZ25,
	author = {Yingtao Luo and
                  Zhixun Li and
                  Qiang Liu and
                  Jun Zhu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Fairness without Demographics through Learning Graph of Gradients},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {918--926},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709160},
	doi = {10.1145/3690624.3709160},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LuoLLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning systems are notoriously prone to biased predictions about certain demographic groups, leading to algorithmic fairness issues. Due to privacy concerns and data quality problems, some demographic information may not be available in the training data and the complex interaction of different demographics can lead to a lot of unknown minority subpopulations, which all limit the applicability of group fairness. Many existing works on fairness without demographics assume the correlation between groups and features. However, we argue that the model gradients are also valuable for fairness without demographics. In this paper, we show that the correlation between gradients and groups can help identify and improve group fairness. With an adversarial weighting architecture, we construct a graph where samples with similar gradients are connected and learn the weights of different samples from it. Unlike the surrogate grouping methods that cluster groups from features and labels as proxy sensitive attribute, our method leverages the graph structure as a soft grouping mechanism, which is much more robust to noises. The results show that our method is robust to noise and can improve fairness significantly without decreasing the overall accuracy too much.}
}


@inproceedings{DBLP:conf/kdd/LuoJJCWB0025,
	author = {Yunze Luo and
                  Yuezihan Jiang and
                  Yinjie Jiang and
                  Gaode Chen and
                  Jingchi Wang and
                  Kaigui Bian and
                  Peiyi Li and
                  Qi Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Online Item Cold-Start Recommendation with Popularity-Aware Meta-Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {927--937},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709336},
	doi = {10.1145/3690624.3709336},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LuoJJCWB0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rise of e-commerce and short videos, online recommender systems that can capture users' interests and update new items in real-time play an increasingly important role. In both online and offline recommendation systems, the cold-start problem caused by interaction sparsity has been impacting the effectiveness of recommendations for cold-start items. Many cold-start scheme based on fine-tuning or knowledge transferring shows excellent performance on offline recommendation. Yet, these schemes are infeasible for online recommendation on streaming data pipelines due to different training method, computational overhead and time constraints. Inspired by the above questions, we propose a model-agnostic recommendation algorithm called Popularity-Aware Meta-learning (PAM), to address the item cold-start problem under streaming data settings. PAM divides the incoming data into different meta-learning tasks by predefined item popularity thresholds. The model can distinguish and reweight behavior-related and content-related features in each task based on their different roles in different popularity levels, thus adapting to recommendations for cold-start samples. These task-fixing design significantly reduces additional computation and storage costs compared to offline methods. Furthermore, PAM also introduced data augmentation and an additional self-supervised loss specifically designed for low-popularity tasks, leveraging insights from high-popularity samples. This approach effectively mitigates the issue of inadequate supervision due to the scarcity of cold-start samples. Experimental results across multiple public datasets demonstrate the superiority of our approach over other baseline methods in addressing cold-start challenges in online streaming data scenarios.}
}


@inproceedings{DBLP:conf/kdd/LuoFAB25,
	author = {Zhiwen Luo and
                  Wentao Fan and
                  Manar Amayri and
                  Nizar Bouguila},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Dynamic Deep Clustering of High-Dimensional Directional Data via Hyperspherical
                  Embeddings with Bayesian Nonparametric Mixtures},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {938--949},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709230},
	doi = {10.1145/3690624.3709230},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LuoFAB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustering high-dimensional directional data (i.e., L 2  normalized vectors) presents significant challenges due to the intricate spherical representations of latent embeddings and the limitations of classical (non-deep) clustering techniques. Moreover, dynamically inferring the number of clusters remains a fundamental issue in existing deep clustering methods, especially those involving complex model-selection criteria. This paper addresses these challenges by introducing a novel deep nonparametric clustering framework that employs hyperspherical latent embeddings within a Variational Autoencoder architecture, enhanced by an infinite Von Mises-Fisher Mixture Model as a dynamic prior. This approach enables automatic adaptation of cluster numbers during training, eliminating the need for predefined clusters and traditional model selection processes. Our scalable architecture effectively integrates In-vMFMM with hyperspherical embeddings to tackle the complexities of directional data. Utilizing a joint training strategy, our method alternates between updating neural network parameters and adjusting mixture model priors via nonparametric variational Bayes. Empirical evaluations on benchmark datasets, including complex ImageNet-50, demonstrate that our approach significantly outperforms state-of-the-art deep nonparametric clustering methods. It also robustly estimates the number of clusters, showcasing its effectiveness and versatility in handling high-dimensional directional data.}
}


@inproceedings{DBLP:conf/kdd/00010LS025,
	author = {Zihan Luo and
                  Hong Huang and
                  Jianxun Lian and
                  Xiran Song and
                  Hai Jin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Towards Controllable Hybrid Fairness in Graph Neural Networks},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {950--961},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709224},
	doi = {10.1145/3690624.3709224},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/00010LS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks  (GNNs) have shown remarkable capabilities in mining graph-structured data. However, conventional GNNs often encounter various fairness issues, such as predictions with prejudices when dealing with nodes with different sensitive attributes like genders or races, or significantly different prediction performance when facing nodes with different degrees. Existing studies mainly focus on addressing one specific fairness issue, neglecting the fact that a GNN model may face multiple unfairness simultaneously in reality, and addressing only one specific fairness may still leave the GNNs in an unfair status. In this paper, we focus on achieving multiple fairness on GNNs simultaneously, which we call hybrid fairness. To achieve this objective, we propose a novel GNN framework called  LibraGNN.  Specifically, we adopt a multi-teacher knowledge distillation training framework that successfully unifies the learning paradigms for multiple fairness. To ensure LibraGNN strikes a better trade-off among different fairness, we transform the multi-teacher knowledge distillation into a multi-objective optimization problem and further employ Pareto efficiency for optimization guidance. Finally, a controllable preference vector is introduced to assist LibraGNN in modulating its capability towards various forms of fairness, thereby achieving controllable hybrid fairness. Extensive experiments on three real-world datasets demonstrate the effectiveness of LibraGNN on both hybrid fairness and utility.}
}


@inproceedings{DBLP:conf/kdd/LvZ0LZZ0K025,
	author = {Zheqi Lv and
                  Tianyu Zhan and
                  Wenjie Wang and
                  Xinyu Lin and
                  Shengyu Zhang and
                  Wenqiao Zhang and
                  Jiwei Li and
                  Kun Kuang and
                  Fei Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Collaboration of Large Language Models and Small Recommendation Models
                  for Device-Cloud Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {962--973},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709335},
	doi = {10.1145/3690624.3709335},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LvZ0LZZ0K025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising research direction that has demonstrated exceptional performance in this field. However, its inability to capture real-time user preferences greatly limits the practical application of LLM4Rec because (i) LLMs are costly to train and infer frequently, and (ii) LLMs struggle to access real-time data (its large number of parameters poses an obstacle to deployment on devices). Fortunately, small recommendation models (SRMs) can effectively supplement these shortcomings of LLM4Rec diagrams by consuming minimal resources for frequent training and inference, and by conveniently accessing real-time data on devices. In light of this, we designed the  Device-Cloud  L LM- S RM  C ollaborative  Rec ommendation Framework  (LSC4Rec) under a device-cloud collaboration setting. LSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the benefits of cloud and edge computing, achieving a complementary synergy. We enhance the practicability of LSC4Rec by designing three strategies: collaborative training, collaborative inference, and intelligent request. During training, LLM generates candidate lists to enhance the ranking ability of SRM in collaborative scenarios and enables SRM to update adaptively to capture real-time user interests. During inference, LLM and SRM are deployed on the cloud and on the device, respectively. LLM generates candidate lists and initial ranking results based on user behavior, and SRM get reranking results based on the candidate list, with final results integrating both LLM's and SRM's scores. The device determines whether a new candidate list is needed by comparing the consistency of the LLM's and SRM's sorted lists. Our comprehensive and extensive experimental analysis validates the effectiveness of each strategy in LSC4Rec.}
}


@inproceedings{DBLP:conf/kdd/LyuZLZ25,
	author = {Junliang Lyu and
                  Yixuan Zhang and
                  Xiaoling Lu and
                  Feng Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Task Diversity in Bayesian Federated Learning: Simultaneous Processing
                  of Classification and Regression},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {974--984},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709341},
	doi = {10.1145/3690624.3709341},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LyuZLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work addresses a key limitation in current federated learning approaches, which predominantly focus on homogeneous tasks, neglecting the task diversity on local devices. We propose a principled integration of multi-task learning using multi-output Gaussian processes (MOGP) at the local level and federated learning at the global level. MOGP handles correlated classification and regression tasks, offering a Bayesian non-parametric approach that naturally quantifies uncertainty. The central server aggregates the posteriors from local devices, updating a global MOGP prior redistributed for training local models until convergence. Challenges in performing posterior inference on local devices are addressed through the Polya-Gamma augmentation technique and mean-field variational inference, enhancing computational efficiency and convergence rate. Experimental results on both synthetic and real data demonstrate superior predictive performance, OOD detection, uncertainty calibration and convergence rate, highlighting the method's potential in diverse applications. Our code is publicly available at https://github.com/JunliangLv/task_diversity_BFL.}
}


@inproceedings{DBLP:conf/kdd/LyuZDL25,
	author = {Tengfei Lyu and
                  Weijia Zhang and
                  Jinliang Deng and
                  Hao Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {AutoSTF: Decoupled Neural Architecture Search for Cost-Effective Automated
                  Spatio-Temporal Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {985--996},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709323},
	doi = {10.1145/3690624.3709323},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LyuZDL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatio-temporal forecasting is a critical component of various smart city applications, such as transportation optimization, energy management, and socio-economic analysis. Recently, several automated spatio-temporal forecasting methods have been proposed to automatically search the optimal neural network architecture for capturing complex spatio-temporal dependencies. However, the existing automated approaches suffer from expensive neural architecture search overhead, which hinders their practical use and the further exploration of diverse spatio-temporal operators in a finer granularity. In this paper, we propose AutoSTF, a decoupled automatic neural architecture search framework for cost-effective automated spatio-temporal forecasting. From the efficiency perspective, we first decouple the mixed search space into temporal space and spatial space and respectively devise representation compression and parameter-sharing schemes to mitigate the parameter explosion. The decoupled spatio-temporal search not only expedites the model optimization process but also leaves new room for more effective spatio-temporal dependency modeling. From the effectiveness perspective, we propose a multi-patch transfer module to jointly capture multi-granularity temporal dependencies and extend the spatial search space to enable finer-grained layer-wise spatial dependency search. Extensive experiments on eight datasets demonstrate the superiority of AutoSTF in terms of both accuracy and efficiency. Specifically, our proposed method achieves up to 13.48x speed-up compared to state-of-the-art automatic spatio-temporal forecasting methods while maintaining the best forecasting accuracy. The source code and data are available at https://github.com/usail-hkust/AutoSTF.}
}


@inproceedings{DBLP:conf/kdd/MaXWWZ25,
	author = {Haiping Ma and
                  Aoqing Xia and
                  Changqian Wang and
                  Hai Wang and
                  Xingyi Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Diffusion-Inspired Cold Start with Sufficient Prior in Computerized
                  Adaptive Testing},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {997--1007},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709317},
	doi = {10.1145/3690624.3709317},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/MaXWWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computerized Adaptive Testing (CAT) aims to select the most appropriate questions based on the examinee's ability and is widely used in online education. However, existing CAT systems often lack initial understanding of the examinee's ability, requiring random probing questions. This can lead to poorly matched questions, extending the test duration and negatively impacting the examinee's mindset, a phenomenon referred to as the Cold Start with Insufficient Prior (CSIP) task. This issue occurs because CAT systems do not effectively utilize the abundant prior information about the examinee available from other courses on online platforms. These response records, due to the commonality of cognitive states across different knowledge domains, can provide valuable prior information for the target domain. However, no prior work has explored solutions for the CSIP task. In response to this gap, we propose Diffusion Cognitive States TransfeR Framework (DCSR), a novel domain transfer framework based on Diffusion Models (DMs) to address the CSIP task. Specifically, we construct a cognitive state transition bridge between domains, guided by the common cognitive states of examinees, encouraging the model to reconstruct the initial ability state in the target domain. To enrich the expressive power of the generated data, we analyze the causal relationships in the generation process from a causal perspective. Redundant and extraneous cognitive states can lead to limited transfer and negative transfer effects. Therefore, we designed three decoupling strategies to control confounding variables, thereby blocking backdoor paths that hinder causal discovery. Given that excessive uncertainty can affect the applicability of generated results to the CAT system, we propose consistency constraint and task-oriented constraint to control the randomness of the generated results and their relevance to the CAT task, respectively. Our DCSR can seamlessly apply the generated initial ability states in the target domain to existing question selection algorithms, thus improving the cold start performance of the CAT sys- tem. Extensive experiments conducted on five real-world datasets demonstrate that DCSR significantly outperforms existing baseline methods in addressing the CSIP task.}
}


@inproceedings{DBLP:conf/kdd/Ma0ZZ25,
	author = {Lanjihong Ma and
                  Yao{-}Xiang Ding and
                  Zhen{-}Yu Zhang and
                  Zhi{-}Hua Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Achieving Nearly-Optimal Regret and Sample Complexity in Dueling Bandits
                  with Applications in Online Recommendations},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1008--1019},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709279},
	doi = {10.1145/3690624.3709279},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Ma0ZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We focus on the dueling bandits problem, which has recently drawn significant attention due to its wide-ranging applications in online recommendation systems and the alignment of  large language models  (LLMs), considers an online preference learning scenario where the learner iteratively selects arms based on pairwise comparison feedback to infer user preferences. Two primary objectives are typically considered in dueling bandits:  Regret Minimization  (RM), which aims to improve the overall quality of selected arms over time, and  Best Arm Identification  (BAI), which seeks to efficiently identify the best item with minimal user feedback. For instance, RM is exemplified by the objective of consistently providing high-quality items, while BAI reduces the required human feedback by minimizing the number of necessary comparisons. Conventional research treats RM and BAI as two conflicting objectives, optimizing one at the expense of the other. In this paper, we propose a novel framework that demonstrates the near-consistency of RM and BAI in dueling bandits by reducing the BAI in dueling bandits into a sequential noisy identification problem. Based on our formulation, we propose a black-box reduction technique that transforms any RM algorithm into a BAI algorithm, and prove that such reduction with optimal RM algorithm achieves  optimal sample complexity and nearly-optimal cumulative weak regret simultaneously . Our proposed algorithm acheives a nearly-optimal BAI sample complexity and attains a cumulative weak regret that is order-wise equivalent to the best-known result simultaneously. Experiments on both synthetic benchmarks and real-world online recommendation tasks validate the effectiveness of the proposed method, providing empirical evidences for our theoretical findings.}
}


@inproceedings{DBLP:conf/kdd/Mahmood0K25,
	author = {Syed Hasan Amin Mahmood and
                  Ming Yin and
                  Rajiv Khanna},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {On the Support Vector Effect in DNNs: Rethinking Data Selection and
                  Attribution},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1020--1031},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709295},
	doi = {10.1145/3690624.3709295},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Mahmood0K25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Deep Neural Networks (DNNs), manipulating gradients is central to various algorithms, including data subset selection and instance attribution. For better tractability, practitioners often resort to using only the gradients of the last layer as a heuristic, instead of the full gradient across all model parameters, which we show is detrimental due to the  Support Vector Effect (SVE).  We introduce SVE, a max-margin-like behavior in the last layer(s) of DNNs and employ it to thoroughly scrutinize prevalent data selection and attribution methods relying on last layer gradients. Our investigation exposes limitations in these techniques and not only provides explanations for previously observed pitfalls, like lack of diversity and temporal performance degradation, but also offers fresh insights, including the vulnerability of existing methods to basic poisoning attacks and the potential for competitive performance using much simpler alternatives. Based on insights from SVE, we craft new methods RandE and PAE for data subset selection and instance attribution, respectively, which often outperform the purported state-of-the-art at a fraction of the cost, emphasizing the practical advantages of more efficient and less complex approaches.}
}


@inproceedings{DBLP:conf/kdd/ManLLZ0025,
	author = {Tianxing Man and
                  Xingchen Li and
                  Zhaogeng Liu and
                  Haozhen Zhang and
                  Bin Gu and
                  Yi Chang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Enhancing Black-Box Adversarial Attacks on Discrete Sequential Data
                  via Bilevel Bayesian Optimization in Hybrid Spaces},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1032--1043},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709265},
	doi = {10.1145/3690624.3709265},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ManLLZ0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Black-box attacks have emerged as a significant threat to deep neural networks. This challenge is particularly difficult in discrete sequential data compared to continuous data. Recently, the Blockwise Bayesian Attack (BBA) leveraging discrete Bayesian optimization with an adapted RBF kernel has gained prominence as a cutting-edge solution. However, it relies solely on alignment information (i.e., positional differences) within the RBF kernel, which may not fully capture the information (such as statistical, structural, and semantic information) inherent in discrete sequential data and potentially lacks the desired inductive bias necessary to approximate the target function accurately. To overcome this limitation, this paper proposes a novel bilevel Bayesian optimization approach to adaptively learn a hybrid space that better captures the similarity between discrete sequences. Specifically, we introduce a multi-kernel mechanism that incorporates multiple types of information, creating a more comprehensive similarity measure. Moreover, we develop a bilevel Bayesian optimization algorithm, where the outer-level objective determines the optimal weights of the multiple kernels, while the inner-level objective identifies the optimal adversarial sequence. Extensive experiments conducted on discrete sequential data demonstrate that our approach ensures secure multi-kernel selection and achieves a higher attack success rate with only a few additional queries, compared to BBA and other traditional optimization strategies.}
}


@inproceedings{DBLP:conf/kdd/MattosHKSS25,
	author = {Jo{\~{a}}o Mattos and
                  Zexi Huang and
                  Mert Kosan and
                  Ambuj Singh and
                  Arlei Silva},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Attribute-Enhanced Similarity Ranking for Sparse Link Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1044--1055},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709314},
	doi = {10.1145/3690624.3709314},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/MattosHKSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Link prediction is a fundamental problem in graph data. In its most realistic setting, the problem consists of predicting missing or future links between random pairs of nodes from the set of disconnected pairs. Graph Neural Networks (GNNs) have become the predominant framework for link prediction. GNN-based methods treat link prediction as a binary classification problem and handle the extreme class imbalance---real graphs are very sparse---by sampling (uniformly at random) a balanced number of disconnected pairs not only for training but also for evaluation. However, we show that the reported performance of GNNs for link prediction in the balanced setting does not translate to the more realistic imbalanced setting and that simpler topology-based approaches are often better at handling sparsity. These findings motivate Gelato, a similarity-based link-prediction method that applies (1) graph learning based on node attributes to enhance a topological heuristic, (2) a ranking loss for addressing class imbalance, and (3) a negative sampling scheme that efficiently selects hard training pairs via graph partitioning. Experiments show that Gelato outperforms existing GNN-based alternatives.}
}


@inproceedings{DBLP:conf/kdd/MiRX00GWSL25,
	author = {Yuan Mi and
                  Pu Ren and
                  Hongteng Xu and
                  Hongsheng Liu and
                  Zidong Wang and
                  Yike Guo and
                  Ji{-}Rong Wen and
                  Hao Sun and
                  Yang Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Conservation-informed Graph Learning for Spatiotemporal Dynamics Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1056--1067},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709244},
	doi = {10.1145/3690624.3709244},
	timestamp = {Wed, 09 Apr 2025 09:19:48 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/MiRX00GWSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data-centric methods have shown great potential in understanding and predicting spatiotemporal dynamics, enabling better design and control of the object system. However, deep learning models often lack interpretability, fail to obey intrinsic physics, and struggle to cope with the various domains. While geometry-based methods, e.g., graph neural networks (GNNs), have been proposed to further tackle these challenges, they still need to find the implicit physical laws from large datasets and rely excessively on rich labeled data. In this paper, we herein introduce the conservation-informed GNN (CiGNN), an end-to-end explainable learning framework, to learn spatiotemporal dynamics based on limited training data. The network is designed to conform to the general conservation law via symmetry, where conservative and non-conservative information passes over a multiscale space enhanced by a latent temporal marching strategy. The efficacy of our model has been verified in various spatiotemporal systems based on synthetic and real-world datasets, showing superiority over baseline models. Results demonstrate that CiGNN exhibits remarkable accuracy and generalizability, and is readily applicable to learning for prediction of various spatiotemporal dynamics in a spatial domain with complex geometry.}
}


@inproceedings{DBLP:conf/kdd/MyrtakisTC25,
	author = {Nikolaos Myrtakis and
                  Ioannis Tsamardinos and
                  Vassilis Christophides},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Data Glitches Discovery using Influence-based Model Explanations},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1068--1079},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709285},
	doi = {10.1145/3690624.3709285},
	timestamp = {Thu, 01 May 2025 20:24:58 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/MyrtakisTC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We address the problem of detecting data glitches in ML training sets, specifically mislabeled and anomalous samples. Detection of data glitches provides insights into the quality of the data sampling. Their repair may improve the reliability and the performance of the model. The proposed methodology is based on exploiting influence functions that estimate how much the loss of the model (or a given sample) is affected when a sample is removed from the training set. We introduce three novel signals for detecting, characterizing, and repairing data glitches in a training set based on sample influences. Influence-based signals form an explainable-by-design data glitch detection framework, producing intuitively explainable signals of the actual predictive model built. In contrast, specialized algorithms that are agnostic to the target ML model (e.g., anomaly detectors) replicate the work of fitting the data distribution and may detect glitches that are inconsistent with the decision boundary of the predictive model. Computational experiments on tabular and image data modalities demonstrate that the proposed signals outperform, in some cases up to a factor of 6, all existing influence-based signals, and generalize across different datasets and ML models. In addition, they often outperform specialized glitch detectors (e.g., mislabeled and anomaly detectors) and provide accurate label repairs for mislabeled samples.}
}


@inproceedings{DBLP:conf/kdd/Na025,
	author = {Gyoung S. Na and
                  Chanyoung Park},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Electron-Informed Coarse-Graining Molecular Representation Learning
                  for Real-World Molecular Physics},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1080--1091},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709270},
	doi = {10.1145/3690624.3709270},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Na025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Various representation learning methods for molecular structures have been devised to accelerate data-driven chemistry. However, the representation capabilities of existing methods are essentially limited to  atom -level information, which is not sufficient to describe real-world molecular physics. Although electron-level information can provide fundamental knowledge about chemical compounds beyond the atom-level information, obtaining the  electron -level information in real-world molecules is computationally impractical and sometimes infeasible. We propose a method for learning electron-informed molecular representations without additional computation costs by transferring readily accessible electron-level information about small molecules to large molecules of our interest. The proposed method achieved state-of-the-art prediction accuracy on extensive benchmark datasets containing experimentally observed molecular physics. The source code for HEDMoL is available at https://github.com/ngs00/HEDMoL.}
}


@inproceedings{DBLP:conf/kdd/Ouyang000025,
	author = {Dian Ouyang and
                  Dong Wen and
                  Jianye Yang and
                  Wentao Li and
                  Xuemin Lin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Weight-Constrained Simple Path Enumeration in Weighted Graph},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1092--1103},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709310},
	doi = {10.1145/3690624.3709310},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Ouyang000025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Path enumeration is a fundamental problem and has been extensively studied in the literature. Given two query vertices and a weight threshold, the problem aims to identify all simple paths with weight not exceeding the threshold. Existing studies on path enumeration include DFS-based solutions and join-based solutions, where the join-based solutions only work for unweighted graphs. In this paper, we are the first to propose a join-based framework for weighted graphs. By observing the characteristics of DFS, we design a series of novel data structures and operations based on the join-based framework. In this way, our final solution combines the advantages of both join and DFS. We conduct experiments on several real large graphs. For weighted graphs, our method is much more efficient than existing algorithms. For unweighted graphs, our method is still competitive compared with the state-of-the-art solution which only works for unweighted graphs.}
}


@inproceedings{DBLP:conf/kdd/PengLZ0025,
	author = {Bo Peng and
                  Jie Lu and
                  Yonggang Zhang and
                  Guangquan Zhang and
                  Zhen Fang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Distributional Prototype Learning for Out-of-distribution Detection},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1104--1114},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709294},
	doi = {10.1145/3690624.3709294},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/PengLZ0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Out-of-distribution (OOD) detection has emerged as a pivotal approach for enhancing the reliability of machine learning models, considering the potential for test data to be sampled from classes disparate from in-distribution (ID) data employed during model training. Detecting those OOD data is typically realized as a distance measurement problem, where those deviating far away from the training distribution in the learned feature space are considered OOD samples. Advanced works have shown great success in learning with prototypes for feature-based OOD detection methods, where each ID class is represented with single or multiple prototypes. However, modeling with a finite number of prototypes would fail to maximally capture intra-class variations. In view of this, this paper extends the existing prototype-based learning paradigm to an  infinite  setting. This motivates us to design two feasible formulations for the Distributional Prototype Learning (DPL) objective, where, to avoid intractable computation and exploding parameters caused by the infinity nature, our key idea is to model an infinite number of discrete prototypes of each ID class with a class-wise continuous distribution. We theoretically analyze both alternatives, identifying the more stable-converging version of the learning objective. We show that, by sampling prototypes from a mixture of class-conditioned Gaussian distributions, the objective can be efficiently computed in a closed form without resorting to the computationally expensive Monte-Carlo approximation of the involved expectation terms. Extensive evaluations across mainstream OOD detection benchmarks empirically manifest that our proposed DPL has established a new state-of-the-art in various OOD settings.}
}


@inproceedings{DBLP:conf/kdd/PrakashBSS00P0V25,
	author = {Jatin Prakash and
                  Anirudh Buvanesh and
                  Bishal Santra and
                  Deepak Saini and
                  Sachin Yadav and
                  Jian Jiao and
                  Yashoteja Prabhu and
                  Amit Sharma and
                  Manik Varma},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {On the Necessity of World Knowledge for Mitigating Missing Labels
                  in Extreme Classification},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1115--1126},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709290},
	doi = {10.1145/3690624.3709290},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/PrakashBSS00P0V25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extreme Classification (XC) aims to map a query to the most relevant documents from a very large document set. XC algorithms used in real-world applications typically learn this mapping from datasets curated from implicit feedback, such as user clicks. However, these datasets often suffer from missing labels. In this work, we observe that systematic missing labels lead to missing knowledge, which is critical for modelling relevance between queries and documents. We formally show that this absence of knowledge is hard to recover using existing methods such as propensity weighting and data imputation strategies that solely rely on the training dataset. While Large Language Models (LLMs) provide an attractive solution to augment the missing knowledge, leveraging them in applications with low latency requirements and large document sets is challenging. To mitigate missing knowledge at scale, we propose SKIM ( S calable  K nowledge  I nfusion for  M issing Labels), an algorithm that leverages a combination of Small Language Models or SLMs, e.g., Llama2-7b, and abundant unstructured meta-data to effectively address the missing label problem. We show the efficacy of our method on large-scale public datasets through a combination of unbiased evaluation strategies, such as exhaustive human annotations and simulation-based evaluation benchmarks. SKIM outperforms existing methods on  Recall@100  by more than 10 absolute points. Additionally, SKIM scales to proprietary query-ad retrieval datasets containing 10 million documents, outperforming baseline methods by 12% in offline evaluations and increasing ad click-yield by 1.23% in an online A/B test conducted on Bing Search. We release the code and trained models at: github.com/bicycleman15/skim}
}


@inproceedings{DBLP:conf/kdd/PuLC0LC25,
	author = {Yuanhao Pu and
                  Defu Lian and
                  Xiaolong Chen and
                  Jin Chen and
                  Ze Liu and
                  Enhong Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Understanding the Effect of Loss Functions on the Generalization of
                  Recommendations},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1127--1137},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709169},
	doi = {10.1145/3690624.3709169},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/PuLC0LC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The two-tower model has become prevalent in recommender systems for its computational efficiency and robust predictive capabilities. The model usually employs two independent neural networks to encode user and item data separately, and predicts the similarity score with inner product or cosine functions, depending on which the Top- k  ranked item list is generated. The optimization process typically involves a multi-label classification objective, often guided by surrogate loss functions like Softmax and One-vs-All (OvA), to enhance the recommendation performance. Despite both Softmax and OvA losses being Bayes-consistent, empirical observations reveal a significant performance gap in evaluation metrics, suggesting limitations in Bayes-consistency for analyzing loss effectiveness. To address this, we introduce ℋ-consistency into the discussion, which provides non-asymptotic and hypothesis-specific guarantees for Top- k  classification within the two-tower model's hypothesis space. Through theoretical analysis, we demonstrate that Softmax and Cosine Contrastive Loss exhibit ℋ-consistency, while the OvA loss does not, explaining the observed performance discrepancies. Our findings bridge the gap between theoretical properties and practical outcomes, offering deeper insights into the optimization of two-tower models and contributing to the development of more effective recommendation systems.}
}


@inproceedings{DBLP:conf/kdd/QiCC025,
	author = {QingGuo Qi and
                  Hongyang Chen and
                  Minhao Cheng and
                  Han Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Input Snapshots Fusion for Scalable Discrete-Time Dynamic Graph Neural
                  Networks},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1138--1149},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709316},
	doi = {10.1145/3690624.3709316},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/QiCC025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, there has been a surge in research on dynamic graph representation learning, primarily focusing on modeling the evolution of temporal-spatial patterns in real-world applications. However, within the domain of discrete-time dynamic graphs, the exploration of temporal edges remains underexplored. Existing approaches often rely on additional sequential models to capture dynamics, leading to high computational and memory costs, particularly for large-scale graphs. To address this limitation, we propose the Input  S napshots  F usion based  Dy namic  G raph Neural Network (SFDyG), which combines Hawkes processes with graph neural networks to capture temporal and structural patterns in dynamic graphs effectively. By fusing multiple snapshots into a single temporal graph, SFDyG decouples computational complexity from the number of snapshots, enabling efficient full-batch and mini-batch training. Experimental evaluations on eight diverse dynamic graph datasets for future link prediction tasks demonstrate that SFDyG consistently outperforms existing methods.}
}


@inproceedings{DBLP:conf/kdd/QianLZM0CD25,
	author = {Hongjin Qian and
                  Zheng Liu and
                  Peitian Zhang and
                  Kelong Mao and
                  Yujia Zhou and
                  Xu Chen and
                  Zhicheng Dou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Tackling the Length Barrier: Dynamic Context Browsing for Knowledge-Intensive
                  Task},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1150--1160},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709240},
	doi = {10.1145/3690624.3709240},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/QianLZM0CD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge-intensive tasks often require complex reasoning and contextual understanding over long contexts. However, the learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we propose that the short LLMs have great potentiality for solving knowledge-intensive tasks that have long context, i.e. they can be solved by purely working with oracle short-contexts within the input long-context. On top of this argument, we propose a framework called  DCISO  DynamiC knowledge-Intensive task S>Olver), which enables a short-LLM to address the knowledge-intensive tasks with long context via dynamic context browsing. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, DCISO can serve as a general framework to handle diversified knowledge-intensive long-context problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where DCISO is able to achieve a substantially improved performance. Our codes will be released at this repository.}
}


@inproceedings{DBLP:conf/kdd/QianWZ0Z25,
	author = {Yu{-}Yang Qian and
                  Yi{-}Han Wang and
                  Zhen{-}Yu Zhang and
                  Yuan Jiang and
                  Zhi{-}Hua Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Adapting to Generalized Online Label Shift by Invariant Representation
                  Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1161--1172},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709182},
	doi = {10.1145/3690624.3709182},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/QianWZ0Z25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of online label shift, where label distribution evolves over time while the label-conditional density remains unchanged, has attracted increasing attentions. Although existing approaches have achieved sound theoretical guarantees and encouraging performance, the assumption of an unchanged conditional distribution may limit its application in broader tasks. In this paper, we investigate an extended variant named  generalized online label shift  (GOLS) problem, in which we relax the label shift assumption on the raw feature space and instead assume the existence of an unknown  invariant representation  such that conditional distribution of this representation given the label remains constant. To handle GOLS, our main idea involves capturing the inherently stable information from non-stationary streams, in the form of learning an invariant representation. Specifically, we design a novel objective to learn the invariant representation, which exploits the unique structure in GOLS. To optimize this objective, we propose an algorithm employing online ensemble paradigm to perform  multi-resolution updates  using various historical data windows, thereby enhancing the stability of the representation. This approach is theoretically guaranteed to achieve an  optimal convergence rate.  To improve the efficiency of the ensemble framework, we further propose a mask-based implementation for ensembling with DNNs. Experiments on benchmarks and real-world tasks validate the effectiveness of our approach.}
}


@inproceedings{DBLP:conf/kdd/QiaoZZ25,
	author = {Wenbo Qiao and
                  Jiaming Zhao and
                  Peng Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Quantum Time-index Models with Reservoir for Time Series Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1173--1184},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709228},
	doi = {10.1145/3690624.3709228},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/QiaoZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The time-index models are a class of time series forecasting models that map time-index features to forecasts in continuous space. Compared to the historical-value models, the time-index models can avoid the effect of data sampling frequency and are usually more expressive. However, the vanilla deep time-index model is weak in modeling the high-frequency components of time series and often requires the introduction of many parameters to enhance the modeling capability. Moreover, the time-index model learns only a mapping relationship and ignores the sequence relationship between temporal features, leading to a weak extrapolation capability in the forecast horizon. In this paper, inspired by the ability of quantum implicit neural representations to model the high-frequency components of signals with fewer parameters, we propose Quantum Time-Index Models with Reservoir (QuantumTime). Specifically, we introduce variational quantum circuits to address the challenge of representing high-frequency components in time series. Then, we introduce a reservoir that empowers QuantumTime with powerful extrapolation capabilities by exploiting the rich dynamical properties of reservoir computing. Ultimately, experiments conducted on chaotic datasets and various real-world datasets demonstrate that QuantumTime achieves highly competitive results compared to the state-of-the-art deep time-index model while reducing training parameters by at least 95%. Our approach provides a paradigm for utilizing potential quantum advantage in practical tasks.}
}


@inproceedings{DBLP:conf/kdd/QiuW0GH025,
	author = {Xiangfei Qiu and
                  Xingjian Wu and
                  Yan Lin and
                  Chenjuan Guo and
                  Jilin Hu and
                  Bin Yang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{DUET:} Dual Clustering Enhanced Multivariate Time Series Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1185--1196},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709325},
	doi = {10.1145/3690624.3709325},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/QiuW0GH025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multivariate time series forecasting is crucial for various applications, such as financial investment, energy management, weather forecasting, and traffic optimization. However, accurate forecasting is challenging due to two main factors. First, real-world time series often show heterogeneous temporal patterns caused by distribution shifts over time. Second, correlations among channels are complex and intertwined, making it hard to model the interactions among channels precisely and flexibly. In this study, we address these challenges by proposing a general framework called  DUET,  which introduces DU al clustering on the temporal and channel dimensions to Enhance multivariate Time series forecasting. First, we design a Temporal Clustering Module (TCM) that clusters time series into fine-grained distributions to handle heterogeneous temporal patterns. For different distribution clusters, we design various pattern extractors to capture their intrinsic temporal patterns, thus modeling the heterogeneity. Second, we introduce a novel Channel-Soft-Clustering strategy and design a Channel Clustering Module (CCM), which captures the relationships among channels in the frequency domain through metric learning and applies sparsification to mitigate the adverse effects of noisy channels. Finally, DUET combines TCM and CCM to incorporate both the temporal and channel dimensions. Extensive experiments on 25 real-world datasets from 10 application domains, demonstrate the state-of-the-art performance of DUET.}
}


@inproceedings{DBLP:conf/kdd/RenZXZGZ25,
	author = {Yixin Ren and
                  Haocheng Zhang and
                  Yewei Xia and
                  Hao Zhang and
                  Jihong Guan and
                  Shuigeng Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Fast Causal Discovery by Approximate Kernel-based Generalized Score
                  Functions with Linear Computational Complexity},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1197--1208},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709338},
	doi = {10.1145/3690624.3709338},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/RenZXZGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Score-based causal discovery methods can effectively identify causal relationships by evaluating candidate graphs and selecting the one with the highest score. One popular class of scores is kernel-based generalized score functions, which can adapt to a wide range of scenarios and work well in practice because they circumvent assumptions about causal mechanisms and data distributions. Despite these advantages, kernel-based generalized score functions pose serious computational challenges in time and space, with a time complexity of  O  ( n 3 ) and a memory complexity of  O  ( n 2 ), where  n  is the sample size. In this paper, we propose an approximate kernel-based generalized score function with  O  ( n ) time and space complexities by using low-rank technique and designing a set of rules to handle the complex composite matrix operations required to calculate the score, as well as developing sampling algorithms for different data types to benefit the handling of diverse data types efficiently. Our extensive causal discovery experiments on both synthetic and real-world data demonstrate that compared to the state-of-the-art method, our method can not only significantly reduce computational costs, but also achieve comparable accuracy, especially for large datasets.}
}


@inproceedings{DBLP:conf/kdd/SeoL25,
	author = {Hyunwoo Seo and
                  Chiehyeon Lim},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{ST-MTM:} Masked Time Series Modeling with Seasonal-Trend Decomposition
                  for Time Series Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1209--1220},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709254},
	doi = {10.1145/3690624.3709254},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/SeoL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Forecasting complex time series is an important yet challenging problem that involves various industrial applications. Recently, masked time-series modeling has been proposed to effectively model temporal dependencies for forecasting by reconstructing masked segments from unmasked ones. However, since the semantic information in time series is involved in intricate temporal variations generated by multiple time series components, simply masking a raw time series ignores the inherent semantic structure, which may cause MTM to learn spurious temporal patterns present in the raw data. To capture distinct temporal semantics, we show that masked modeling techniques should address entangled patterns through a decomposition approach. Specifically, we propose ST-MTM, a masked time-series modeling framework with seasonal-trend decomposition, which includes a novel masking method for the seasonal-trend components that incorporates different temporal variations from each component. ST-MTM uses a period masking strategy for seasonal components to produce multiple masked seasonal series based on inherent multi-periodicity and a sub-series masking strategy for trend components to mask temporal regions that share similar variations. The proposed masking method presents an effective pre-training task for learning intricate temporal variations and dependencies. Additionally, ST-MTM introduces a contrastive learning task to support masked modeling by enhancing contextual consistency among multiple masked seasonal representations. Experimental results show that our proposed ST-MTM achieves consistently superior forecasting performance compared to existing masked modeling, contrastive learning, and supervised forecasting methods.}
}


@inproceedings{DBLP:conf/kdd/ShaoHYL25,
	author = {Jie{-}Jing Shao and
                  Hao{-}Ran Hao and
                  Xiao{-}Wen Yang and
                  Yu{-}Feng Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Abductive Learning for Neuro-Symbolic Grounded Imitation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1221--1232},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709344},
	doi = {10.1145/3690624.3709344},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ShaoHYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent learning-to-imitation methods have shown promise in planning by imitating within the observation-action space, yet they remain constrained in open environments, especially for long-horizon tasks. In contrast, while traditional symbolic planning excels in such tasks through logical reasoning over human-defined symbolic spaces, it struggles with high-dimensional visual inputs encountered in real-world scenarios. In this work, we draw inspiration from abductive learning and introduce a novel framework ABductive Imitation Learning (ABIL) that integrates the benefits of data-driven learning and symbolic-based reasoning, enabling long-horizon planning. Specifically, we employ abductive reasoning to understand the demonstrations in symbolic space and design the principles of sequential consistency to resolve the conflicts between perception and reasoning. ABIL generates predicate candidates to facilitate the perception from raw observations to symbolic space without laborious predicate annotations, providing a groundwork for symbolic planning. With the symbolic understanding, we develop a policy ensemble with base policies designed around different logical objectives, managed through symbolic reasoning. Experiments demonstrate that our method successfully comprehends observations with task-relevant symbolics to aid imitation learning. Importantly, ABIL demonstrates improved data efficiency and generalization across various long-horizon tasks, highlighting it as a promising solution for long-horizon planning. Project website: https://www.lamda.nju.edu.cn/shaojj/KDD25_ABIL/.}
}


@inproceedings{DBLP:conf/kdd/ShaoYGCZZWLW25,
	author = {Pengyang Shao and
                  Yonghui Yang and
                  Chen Gao and
                  Lei Chen and
                  Kun Zhang and
                  Chenyi Zhuang and
                  Le Wu and
                  Yong Li and
                  Meng Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Exploring Heterogeneity and Uncertainty for Graph-based Cognitive
                  Diagnosis Models in Intelligent Education},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1233--1243},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709264},
	doi = {10.1145/3690624.3709264},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ShaoYGCZZWLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-based Cognitive Diagnosis (CD) has attracted much research interest due to its strong ability on inferring students' proficiency levels on knowledge concepts. While graph-based CD models have demonstrated remarkable performance, we contend that they still cannot achieve optimal performance due to the neglect of edge heterogeneity and uncertainty. Edges involve both correct and incorrect response logs, indicating heterogeneity. Meanwhile, a response log can have uncertain semantic meanings, e.g.,  a correct log can indicate true mastery or fortunate guessing, and a wrong log can indicate a lack of understanding or a careless mistake . In this paper, we propose an  I nformative  S emantic-aware  G raph-based  C ognitive  D iagnosis model ( ISG-CD ), which focuses on how to utilize the heterogeneous graph in CD and minimize effects of uncertain edges. Specifically, to explore heterogeneity, we propose a semantic-aware graph neural networks based CD model. To minimize effects of edge uncertainty, we propose an Informative Edge Differentiation layer from an information bottleneck perspective, which suggests keeping a minimal yet sufficient reliable graph for CD in an unsupervised way. We formulate this process as maximizing mutual information between the reliable graph and response logs, while minimizing mutual information between the reliable graph and the original graph. After that, we prove that mutual information maximization can be theoretically converted to the classic binary cross entropy loss function, while minimizing mutual information can be realized by the Hilbert-Schmidt Independence Criterion.Finally, we adopt an alternating training strategy for optimizing learnable parameters of both the semantic-aware graph neural networks based CD model and the edge differentiation layer. Extensive experiments on three real-world datasets have demonstrated the effectiveness of ISG-CD.}
}


@inproceedings{DBLP:conf/kdd/ShiLZ00X25,
	author = {Qilong Shi and
                  Xirui Li and
                  Hanyue Zheng and
                  Tong Yang and
                  Yangyang Wang and
                  Mingwei Xu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {HeavyLocker: Lock Heavy Hitters in Distributed Data Streams},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1244--1255},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709167},
	doi = {10.1145/3690624.3709167},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ShiLZ00X25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, sketching has emerged as a pivotal technique for identifying heavy hitters (items with high frequency) in large-scale data streams. Despite this progress, the majority of existing sketch algorithms are tailored primarily for detecting local heavy hitters within a single data stream, with only a few capable of extending their application to global heavy hitters across distributed data streams. A common challenge encountered by these algorithms is balancing performance with accuracy. To address this challenge, we introduce HeavyLocker, a novel sketch algorithm that takes advantage of a distinct feature of real data streams: the  separability  of heavy hitters. By leveraging this attribute, HeavyLocker precisely locks and protects potential heavy hitters during the data stream processing, ensuring accuracy in local heavy hitter detection without compromising on speed. This unique capability also facilitates its application to global detection tasks. Through theoretical analysis, we validate the efficacy of HeavyLocker's locking mechanism. Our extensive experiments show that HeavyLocker outperforms five benchmarked algorithms in accuracy and maintains fast speed for both local and global heavy hitter detection, significantly reducing errors by up to an order of magnitude compared to the renowned Double-Anonymous Sketch.}
}


@inproceedings{DBLP:conf/kdd/ShimizuKMNTUS25,
	author = {Tatsuhiro Shimizu and
                  Kazuki Kawamura and
                  Takanori Muroi and
                  Yusuke Narita and
                  Kei Tateno and
                  Takuma Udagawa and
                  Yuta Saito},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Off-Policy Evaluation and Learning for the Future under Non-Stationarity},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1256--1264},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709237},
	doi = {10.1145/3690624.3709237},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ShimizuKMNTUS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the novel problem of future off-policy evaluation (F-OPE) and learning (F-OPL) for estimating and optimizing the future value of policies in non-stationary environments, where distributions vary over time. In e-commerce recommendations, for instance, our goal is often to estimate and optimize the policy value for the upcoming month using data collected by an old policy in the previous month. A critical challenge is that data related to the future environment is not observed in the historical data. Existing methods assume stationarity or depend on restrictive reward-modeling assumptions, leading to significant bias. To address these limitations, we propose a novel estimator named  O ff- P olicy Estimator for the  F uture  V alue ( OPFV ),  designed for accurately estimating policy values at any future time point. The key feature of OPFV is its ability to leverage the useful structure within time-series data. While future data might not be present in the historical log, we can leverage, for example, seasonal, weekly, or holiday effects that are consistent in both the historical and future data. Our estimator is the first to exploit these time-related structures via a new type of importance weighting, enabling effective F-OPE. Theoretical analysis identifies the conditions under which OPFV becomes low-bias. In addition, we extend our estimator to develop a new policy-gradient method to proactively learn a good future policy using only historical data. Empirical results show that our methods substantially outperform existing methods in estimating and optimizing the future policy value under non-stationarity for various experimental setups.}
}


@inproceedings{DBLP:conf/kdd/SongJK025,
	author = {Minkyoo Song and
                  Eugene Jang and
                  Jaehan Kim and
                  Seungwon Shin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Covering Cracks in Content Moderation: Delexicalized Distant Supervision
                  for Illicit Drug Jargon Detection},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1265--1276},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709183},
	doi = {10.1145/3690624.3709183},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/SongJK025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In light of rising drug-related concerns and the increasing role of social media, sales and discussions of illicit drugs have become commonplace online. Social media platforms hosting user-generated content must therefore perform content moderation, which is a difficult task due to the vast amount of jargon used in drug discussions. Previous works on drug jargon detection were limited to extracting a list of terms, but these approaches have fundamental problems in practical application. First, they are trivially evaded using word substitutions. Second, they cannot distinguish whether euphemistic terms  (pot, crack)  are being used as drugs or as their benign meanings. We argue that drug content moderation should be done using contexts, rather than relying on a banlist. However, manually annotated datasets for training such a task are not only expensive but also prone to becoming obsolete. We present JEDIS, a framework for detecting illicit drug jargon terms by analyzing their contexts. JEDIS utilizes a novel approach that combines distant supervision and delexicalization, which allows JEDIS to be trained without human-labeled data while being robust to new terms and euphemisms. Experiments on two manually annotated datasets show JEDIS significantly outperforms state-of-the-art word-based baselines in terms of F1-score and detection coverage in drug jargon detection. We also conduct qualitative analysis that demonstrates JEDIS is robust against pitfalls faced by existing approaches.}
}


@inproceedings{DBLP:conf/kdd/StepkaSL25,
	author = {Ignacy Stepka and
                  Jerzy Stefanowski and
                  Mateusz Lango},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Counterfactual Explanations with Probabilistic Guarantees on their
                  Robustness to Model Change},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1277--1288},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709300},
	doi = {10.1145/3690624.3709300},
	timestamp = {Thu, 01 May 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/StepkaSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Counterfactual explanations (CFEs) guide users on how to adjust inputs to machine learning models to achieve desired outputs. While existing research primarily addresses static scenarios, real-world applications often involve data or model changes, potentially invalidating previously generated CFEs and rendering user-induced input changes ineffective. Current methods addressing this issue often support only specific models or change types, require extensive hyperparameter tuning, or fail to provide probabilistic guarantees on CFE robustness to model changes. This paper proposes a novel approach for generating CFEs that provides probabilistic guarantees for any model and change type, while offering interpretable and easy-to-select hyperparameters. We establish a theoretical framework for probabilistically defining robustness to model change and demonstrate how our BetaRCE method directly stems from it. BetaRCE is a post-hoc method applied alongside a chosen base CFE generation method to enhance the quality of the explanation beyond robustness. It facilitates a transition from the base explanation to a more robust one with user-adjusted probability bounds. Through experimental comparisons with baselines, we show that BetaRCE yields robust, most plausible, and closest to baseline counterfactual explanations.}
}


@inproceedings{DBLP:conf/kdd/SuZZY25,
	author = {Ge Su and
                  Kaiping Zheng and
                  Tiancheng Zhao and
                  Jianwei Yin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{CLEAR:} Addressing Representation Contamination in Multimodal Healthcare
                  Analytics},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1289--1300},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709164},
	doi = {10.1145/3690624.3709164},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/SuZZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electronic health records (EHRs) are the de facto standard for analyzing comprehensive patient conditions. Existing methods mainly employ specialized neural networks to extract modality-specific information, followed by modality correlation modeling to support clinical decision-making. However, these methods generally overlook the issue of ''contaminated'' representations inherent in routine EHR data, which can undermine the model's discriminative ability, as less relevant representations associated with false positive correlations may impede the recognition of truly effective representations. To address the issue of representation contamination, we propose CLEAR, a counterfactual disparity learning model for explicit multimodal EHR analytics. The core idea is to first model the contamination in representations, and subsequently perform calibration and enhancement to construct highly discriminative representations. Specifically, CLEAR first proposes the Counterfactual Prompt Learning Module to capture the representation discrepancy to model representation contamination. Subsequently, an Adaptive Dynamic Imputation Module is devised to decouple the elementwise representations for representation calibration, while a gating mechanism is further proposed to incorporate discriminative discrepancy information for representation enhancement. Finally, the Multimodal Representation Fusion Module establishes intra- and inter-modality correlations, thereby creating a seamless integration towards downstream analytic tasks. To our knowledge, CLEAR is the first to model and resolve representation contamination in multimodal EHR analytics. Experimental results on two real-world datasets demonstrate that CLEAR consistently outperforms state-of-the-art baselines in facilitating multimodal healthcare analytics.}
}


@inproceedings{DBLP:conf/kdd/SuiSWLCL025,
	author = {Yongduo Sui and
                  Jie Sun and
                  Shuyao Wang and
                  Zemin Liu and
                  Qing Cui and
                  Longfei Li and
                  Xiang Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {A Unified Invariant Learning Framework for Graph Classification},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1301--1312},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709203},
	doi = {10.1145/3690624.3709203},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/SuiSWLCL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Invariant learning demonstrates substantial potential for enhancing the generalization of graph neural networks (GNNs) with out-of-distribution (OOD) data. It aims to recognize stable features in graph data for classification, based on the premise that these features causally determine the target label, and their influence is invariant to changes in distribution. Along this line, most studies have attempted to pinpoint these stable features by emphasizing explicit substructures in the graph, such as masked or attentive subgraphs, and primarily enforcing the invariance principle in the semantic space,  i.e. , graph representations. However, we argue that focusing only on the semantic space may not accurately identify these stable features. To address this, we introduce the Unified Invariant Learning (UIL) framework for graph classification. It provides a unified perspective on invariant graph learning, emphasizing both structural and semantic invariance principles to identify more robust stable features. In the graph space, UIL adheres to the structural invariance principle by reducing the distance between graphons over a set of stable features across different environments. Simultaneously, to confirm semantic invariance, UIL underscores that the acquired graph representations should demonstrate exemplary performance across diverse environments. We present both theoretical and empirical evidence to confirm our method's ability to recognize superior stable features. Moreover, through a series of comprehensive experiments complemented by in-depth analyses, we demonstrate that UIL considerably enhances OOD generalization, surpassing the performance of leading baseline methods. Our codes are available at https://github.com/yongduosui/UIL.}
}


@inproceedings{DBLP:conf/kdd/00020FWZWC25,
	author = {Yifei Sun and
                  Yang Yang and
                  Xiao Feng and
                  Zijun Wang and
                  Haoyang Zhong and
                  Chunping Wang and
                  Lei Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Handling Feature Heterogeneity with Learnable Graph Patches},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1313--1324},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709242},
	doi = {10.1145/3690624.3709242},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/00020FWZWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the rapid development of foundation models and graph pre-training technologies has spurred increasing interest in constructing a universal pre-trained graph model or Graph Foundation Model (GFM). However, a significant challenge is that existing models are unable to address feature heterogeneity in graph data without textual information, which hinders the transferability of graph models across different datasets. To bridge this gap, we propose the concept of learnable graph patches, which we regard as the smallest semantic units of any graph data. We decompose the graph into learnable graph patches by unfolding the node features and constructing corresponding patch structures separately. We then design PatchNet, a framework that mines transferable information from graph data across domains. Specifically, after extracting graph patches, we propose a patch encoder to extract knowledge from each unit and a patch aggregator to learn how the units are combined into a whole. Due to its domain-agnostic nature, the model can be applied to downstream data across different domains. Furthermore, we analyze the connection between PatchNet and existing graph models, as well as the transferability of the node embeddings it generates. Empirically, our method not only achieves the capability to use multi-domain graphs for pre-training, but also shows enhanced performance across various downstream datasets and tasks. Moreover, we observe consistent improvement in downstream performance as the volume of pre-training data increases.}
}


@inproceedings{DBLP:conf/kdd/SunHZGL025,
	author = {Zexu Sun and
                  Qiyu Han and
                  Minqin Zhu and
                  Hao Gong and
                  Dugang Liu and
                  Chen Ma},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Robust Uplift Modeling with Large-Scale Contexts for Real-time Marketing},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1325--1336},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709293},
	doi = {10.1145/3690624.3709293},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/SunHZGL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Improving user engagement and platform revenue is crucial for online marketing platforms. Uplift modeling is proposed to solve this problem, which applies different treatments (e.g., discounts, bonus) to satisfy corresponding users. Despite progress in this field, limitations persist. Firstly, most of them focus on scenarios where only user features exist. However, in real-world scenarios, there are rich contexts available in the online platform (e.g., short videos, news), and the uplift model needs to infer an incentive for each user on the specific item, which is called real-time marketing. Thus, only considering the user features will lead to biased prediction of the responses, which may cause the cumulative error for uplift prediction. Moreover, due to the large-scale contexts, directly concatenating the context features with the user features will cause a severe distribution shift in the treatment and control groups. Secondly, capturing the interaction relationship between the user features and context features can better predict the user response. To solve the above limitations, we propose a novel model-agnostic Robust Uplift Modeling with Large-Scale Contexts (UMLC) framework for Real-time Marketing. Our UMLC includes two customized modules. 1) A response-guided context grouping module for extracting context features information and condensing value space through clusters. 2) A feature interaction module for obtaining better uplift prediction. Specifically, this module contains two parts: a user-context interaction component for better modeling the response; a treatment-feature interaction component for discovering the treatment assignment sensitive feature of each instance to better predict the uplift. Moreover, we conduct extensive experiments on a synthetic dataset and a real-world product dataset to verify the effectiveness and compatibility of our UMLC.}
}


@inproceedings{DBLP:conf/kdd/TangWG0ZFWZ25,
	author = {Gu Tang and
                  Jinghe Wang and
                  Xiaoying Gan and
                  Bin Lu and
                  Ze Zhao and
                  Luoyi Fu and
                  Xinbing Wang and
                  Chenghu Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {R\({}^{\mbox{2}}\)MR: Review and Rewrite Modality for Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1337--1348},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709250},
	doi = {10.1145/3690624.3709250},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/TangWG0ZFWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive growth of online multimodal content, multimodal recommender systems(MRSs) have brought significant benefits to multimedia platforms. As MRSs evolve, many studies incorporate advanced technologies like graph neural networks(GNNs) and self-supervised learning(SSL), achieving remarkable results. However, these efforts still suffer from the  quality disparity problem . It refers to the mixture of high and low quality across items' multiple modalities, owing to disparities in construction costs or design levels. These low-quality modalities often lack crucial details or introduce noise to the depiction of item, leading to insufficient or polluted item representation. Therefore, we propose a novel framework  R 2 MR: Review and Rewrite Modality for Recommendation  to tackle this issue. Specifically, R 2 MR is composed of two key components:  Modality Reviewer and Modality Rewriter . The Modality Reviewer introduces a Consensus Review Mechanism. It performs perspective decomposition based on user representations and learns the consensus quality scores for modalities from diverse perspectives of multiple users. The Modality Rewriter proposes a Latent Mapping Model, which improves the quality of inferior modalities by learning various mapping patterns from high-quality modalities. Comprehensive experiments across three benchmark datasets reveal that R 2 MR substantially outperforms state-of-the-art methods, achieving an average improvement of 9.20%. The implementations are available at https://github.com/gutang-97/R2MR.}
}


@inproceedings{DBLP:conf/kdd/Tang0CL25,
	author = {Weike Tang and
                  Dingming Wu and
                  Tsz Nam Chan and
                  Kezhong Lu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Spatially Compact Dense Block Mining in Spatial Tensors},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1349--1360},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709221},
	doi = {10.1145/3690624.3709221},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Tang0CL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial tensors have been extensively used in a wide range of applications, including remote sensing, geospatial information systems, conservation planning, and urban planning. We study the problem of Spatially Compact Dense (SCD) block mining in a spatial tensor, which targets for discovering dense blocks that cover small spatial regions. However, most of existing dense block mining (DBM) algorithms cannot solve the SCD-block mining problem since they only focus on maximizing the density of candidate blocks, so that the discovered blocks are spatially loose, i.e., covering large spatial regions. Therefore, we first formulate the problem of mining  top-k Spatially Compact Dense blocks (SCD-blocks)  in spatial tensors, which ranks SCD-blocks based on a new scoring function that takes both the density value and the spatial coverage into account. Then, we adopt a filter-refinement framework that first generates candidate SCD-blocks with good scores in the filtering phase and then uses the traditional DBM algorithm to further maximize the density values of the candidates in the refinement phase. Due to the NP-hardness of the problem, we develop two types of solutions in the filtering phase, namely the top-down solution and the bottom-up solution, which can find good candidate SCD-blocks by approximately solving the new scoring function. The evaluations on four real datasets verify that compared with the dense blocks returned by existing DBM algorithms, the proposed solutions are able to find SCD-blocks with comparable density values and significantly smaller spatial coverage.}
}


@inproceedings{DBLP:conf/kdd/TianZS0L025,
	author = {Qin Tian and
                  Chen Zhao and
                  Minglai Shao and
                  Wenjun Wang and
                  Yujie Lin and
                  Dong Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{MLDGG:} Meta-Learning for Domain Generalization on Graphs},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1361--1372},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709188},
	doi = {10.1145/3690624.3709188},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/TianZS0L025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Domain generalization on graphs aims to develop models with robust generalization capabilities, ensuring effective performance on the testing set despite disparities between testing and training distributions. However, existing methods often rely on static encoders directly applied to the target domain, constraining its flexible adaptability. In contrast to conventional methodologies, which concentrate on developing specific generalized models, our framework, MLDGG, endeavors to achieve adaptable generalization across diverse domains by integrating cross-multi-domain meta-learning with structure learning and semantic identification.Initially, it introduces a generalized structure learner to mitigate the adverse effects of task-unrelated edges, enhancing the comprehensiveness of representations learned by Graph Neural Networks (GNNs) while capturing shared structural information across domains.Subsequently, a representation learner is designed to disentangle domain-invariant semantic and domain-specific variation information in node embedding by leveraging causal reasoning for semantic identification, further enhancing generalization. In the context of meta-learning, meta-parameters for both learners are optimized to facilitate knowledge transfer and enable effective adaptation to graphs through fine-tuning within the target domains, where target graphs are inaccessible during training.Our empirical results demonstrate that MLDGG surpasses baseline methods, showcasing the effectiveness in three different distribution shift settings.}
}


@inproceedings{DBLP:conf/kdd/Tong0FY0Z0Z25,
	author = {Zhenyu Tong and
                  Chuan Qin and
                  Chuyu Fang and
                  Kaichun Yao and
                  Xi Chen and
                  Jingshuai Zhang and
                  Chen Zhu and
                  Hengshu Zhu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {From Missteps to Mastery: Enhancing Low-Resource Dense Retrieval through
                  Adaptive Query Generation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1373--1384},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709225},
	doi = {10.1145/3690624.3709225},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Tong0FY0Z0Z25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Document retrieval, designed to recall query-relevant documents from expansive collections, is essential for information-seeking tasks, such as web search and open-domain question-answering. Advances in representation learning and pretrained language models (PLMs) have driven a paradigm shift from traditional sparse retrieval methods to more effective dense retrieval approaches, forging enhanced semantic connections between queries and documents and establishing new performance benchmarks. However, reliance on extensive annotated document-query pairs limits their competitiveness in low-resource scenarios. Recent research efforts employing the few-shot capabilities of large language models (LLMs) and prompt engineering for synthetic data generation have emerged as a promising solution. Nonetheless, these approaches are hindered by the generation of lower-quality data within the conventional dense retrieval training process. To this end, in this paper, we introduce  iGFT,  a framework aimed at enhancing low-resource dense retrieval by integrating a three-phase process ---  G eneration,  F iltering, and  T uning --- coupled with an iterative optimization strategy. Specifically, we first employ supervised fine-tuning on limited ground truth data, enabling an LLM to function as the generator capable of producing potential queries from given documents. Subsequently, we present a multi-stage filtering module to minimize noise in the generated data while retaining samples poised to significantly improve the dense retrieval model's performance in the follow-up fine-tuning process. Furthermore, we design a novel iterative optimization strategy that dynamically optimizes the query generator for producing more informative queries, thereby enhancing the efficacy of the entire framework. Finally, extensive experiments conducted on a series of publicly available retrieval benchmark datasets have demonstrated the effectiveness of the proposed iGFT.}
}


@inproceedings{DBLP:conf/kdd/TranNNH25,
	author = {Thanh V. T. Tran and
                  Nhat Khang Ngo and
                  Viet Anh Nguyen and
                  Truong Son Hy},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{GROOT:} Effective Design of Biological Sequences with Limited Experimental
                  Data},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1385--1396},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709291},
	doi = {10.1145/3690624.3709291},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/TranNNH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Latent space optimization (LSO) is a powerful method for designing discrete, high-dimensional biological sequences that maximize expensive black-box functions, such as wet lab experiments. This is accomplished by learning a latent space from available data and using a surrogate model  f Φ  to guide optimization algorithms toward optimal outputs. However, existing methods struggle when labeled data is limited, as training  f Φ  with few labeled data points can lead to subpar outputs, offering no advantage over the training data itself. We address this challenge by introducing  GROOT,  a  GR aph-based Latent Sm OO T hing for Biological Sequence Optimization. In particular, GROOT generates pseudo-labels for neighbors sampled around the training latent embeddings. These pseudo-labels are then refined and smoothed by Label Propagation. Additionally, we theoretically and empirically justify our approach, demonstrate GROOT's ability to extrapolate to regions beyond the training set while maintaining reliability within an upper bound of their expected distances from the training regions. We evaluate GROOT on various biological sequence design tasks, including protein optimization (GFP and AAV) and three tasks with exact oracles from Design-Bench. The results demonstrate that GROOT equalizes and surpasses existing methods without requiring access to black-box oracles or vast amounts of labeled data, highlighting its practicality and effectiveness. We release our code at https://github.com/Fsoft-AIC/GROOT.}
}


@inproceedings{DBLP:conf/kdd/UllahSZB25,
	author = {Nasib Ullah and
                  Erik Schultheis and
                  Jinbin Zhang and
                  Rohit Babbar},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {How Well Calibrated are Extreme Multi-label Classifiers? An Empirical
                  Analysis},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1397--1408},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709333},
	doi = {10.1145/3690624.3709333},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/UllahSZB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extreme multilabel classification (XMLC) problems occur in settings such as related product recommendation, large-scale document tagging, or ad prediction, and are characterized by a label space that can span millions of possible labels. There are two implicit tasks that the classifier performs:  Evaluating  each potential label for its expected worth, and then  selecting  the best candidates. For the latter task, only the relative order of scores matters, and this is what is captured by the standard evaluation procedure in the XMLC literature. However, in many practical applications, it is important to have a good estimate of the actual probability of a label being relevant, e.g., to decide whether to pay the fee to be allowed to display the corresponding ad. To judge whether an extreme classifier is indeed suited to this task, one can look, for example, to whether it returns  calibrated  probabilities, which has hitherto not been done in this field. Therefore, this paper aims to establish the current status quo of calibration in XMLC by providing a systematic evaluation, comprising nine models from four different model families across seven benchmark datasets. As naive application of Expected Calibration Error (ECE) leads to meaningless results in long-tailed XMC datasets, we instead introduce the notion of  calibration@k  (e.g., ECE@k), which focusses on the top- k  probability mass, offering a more appropriate measure for evaluating probability calibration in XMLC scenarios. While we find that different models can exhibit widely varying reliability plots, we also show that post-training calibration via a computationally efficient isotonic regression method enhances model calibration without sacrificing prediction accuracy. Thus, the practitioner can choose the model family based on accuracy considerations, and leave calibration to isotonic regression.}
}


@inproceedings{DBLP:conf/kdd/ValluriM0S0VS25,
	author = {Ravisri Valluri and
                  Akash Kumar Mohankumar and
                  Kushal Dave and
                  Amit Singh and
                  Jian Jiao and
                  Manik Varma and
                  Gaurav Sinha},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Scaling the Vocabulary of Non-autoregressive Models for Fast Generative
                  Retrieval},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1409--1420},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709330},
	doi = {10.1145/3690624.3709330},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ValluriM0S0VS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generative Retrieval introduces a new approach to Information Retrieval by reframing it as a constrained generation task, leveraging recent advancements in Autoregressive (AR) language models. However, AR-based Generative Retrieval methods suffer from high inference latency and cost compared to traditional dense retrieval techniques, limiting their practical applicability. This paper investigates fully Non-autoregressive (NAR) language models as a more efficient alternative for generative retrieval. While standard NAR models alleviate latency and cost concerns, they exhibit a significant drop in retrieval performance (compared to AR models) due to their inability to capture dependencies between target tokens. To address this, we question the conventional choice of limiting the target token space to solely words or sub-words. We propose PIXNAR, a novel approach that expands the target vocabulary of NAR models to include multi-word entities and common phrases (up to 5 million tokens), thereby reducing token dependencies. PIXNAR employs inference optimization strategies to maintain low inference latency despite the significantly larger vocabulary. Our results demonstrate that PIXNAR achieves a relative improvement of 31.0% in MRR@10 on MS MARCO and 23.2% in Hits@5 on Natural Questions compared to standard NAR models with similar latency and cost. Furthermore, online A/B experiments on a large commercial search engine show significant increase in clicks and revenue.}
}


@inproceedings{DBLP:conf/kdd/NessU25,
	author = {Mike Van Ness and
                  Madeleine Udell},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Interpretable Prediction and Feature Selection for Survival Analysis},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1421--1432},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709245},
	doi = {10.1145/3690624.3709245},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/NessU25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Survival analysis is widely used as a technique to model time-to-event data when some data is censored, particularly in healthcare for predicting future patient risk. In such settings, survival models must be both accurate and interpretable so that users (such as doctors) can trust the model and understand model predictions. While most literature focuses on discrimination, interpretability is equally as important. A successful interpretable model should be able to describe how changing each feature impacts the outcome, and should only use a small number of features. In this paper, we present DyS (pronounced "dice\'\'), a new survival analysis model that achieves both strong discrimination and interpretability. DyS is a feature-sparse Generalized Additive Model, combining feature selection and interpretable prediction into one model. While DyS works well for all survival analysis problems, it is particularly useful for large (in  n  and  p ) survival datasets such as those commonly found in observational healthcare studies. Empirical studies show that DyS competes with other state-of-the-art machine learning models for survival analysis, while being highly interpretable.}
}


@inproceedings{DBLP:conf/kdd/Wang025,
	author = {Jianian Wang and
                  Rui Song},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Dynamic Causal Structure Discovery and Causal Effect Estimation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1433--1444},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709345},
	doi = {10.1145/3690624.3709345},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Wang025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To represent the causal relationships between variables, a directed acyclic graph (DAG) is widely utilized in many areas, such as social sciences, epidemics, and genetics. Many causal structure learning approaches are developed to learn the hidden causal structure using deep learning approaches. However, these approaches have a hidden assumption that the causal relationship remains unchanged over time, which may not hold in real life. In this paper, we develop a new framework to model the dynamic causal graph where the causal relations are allowed to be time-varying. We incorporate the basis approximation method into the score-based causal discovery approach to capture the dynamic pattern of causal graphs. Utilizing the autoregressive model structure, we could capture both contemporaneous and time-lagged causal relationships while allowing them to vary with time. We propose an algorithm that could provide both past-time estimates and future-time predictions on the causal graphs, and conduct simulations to demonstrate the usefulness of the proposed method. We also apply the proposed method for the covid-data analysis, and provide causal estimates on how the effect of policy restriction changes.}
}


@inproceedings{DBLP:conf/kdd/00020YLM25,
	author = {Jiaqi Wang and
                  Ziyi Yin and
                  Quanzeng You and
                  Lingjuan Lyu and
                  Fenglong Ma},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Asymmetrical Reciprocity-based Federated Learning for Resolving Disparities
                  in Medical Diagnosis},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1445--1456},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709235},
	doi = {10.1145/3690624.3709235},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/00020YLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geographic health disparities pose a pressing global challenge, particularly in underserved regions of low- and middle-income nations. Addressing this issue requires a collaborative approach to enhance healthcare quality, leveraging support from medically more developed areas. Federated learning emerges as a promising tool for this purpose. However, the scarcity of medical data and limited computation resources in underserved regions make collaborative training of powerful machine learning models challenging. Furthermore, there exists an asymmetrical reciprocity between underserved and developed regions. To overcome these challenges, we propose a novel cross-silo federated learning framework, named FedHelp, aimed at alleviating geographic health disparities and fortifying the diagnostic capabilities of underserved regions. Specifically, FedHelp leverages foundational model knowledge via one-time API access to guide the learning process of underserved small clients, addressing the challenge of insufficient data. Additionally, we introduce a novel asymmetric dual knowledge distillation module to manage the issue of asymmetric reciprocity, facilitating the exchange of necessary knowledge between developed large clients and underserved small clients. We validate the effectiveness and utility of FedHelp through extensive experiments on both medical image classification and segmentation tasks. The experimental results demonstrate significant performance improvement compared to state-of-the-art baselines, particularly benefiting clients in underserved regions.}
}


@inproceedings{DBLP:conf/kdd/0002HHFZT0H025,
	author = {Jingwei Wang and
                  Qianyue Hao and
                  Wenzhen Huang and
                  Xiaochen Fan and
                  Qin Zhang and
                  Zhentao Tang and
                  Bin Wang and
                  Jianye Hao and
                  Yong Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {CoopRide: Cooperate All Grids in City-Scale Ride-Hailing Dispatching
                  with Multi-Agent Reinforcement Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1457--1468},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709205},
	doi = {10.1145/3690624.3709205},
	timestamp = {Thu, 01 May 2025 20:24:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0002HHFZT0H025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ride-hailing services offer convenient travel options in urban transportation. To improve passengers' experience and platforms' revenue, plentiful studies use multi-agent reinforcement learning (MARL) for efficient order dispatching, controlling each grid with one agent to balance the supply-demand (drivers-orders) distribution. However, despite the critical role of cooperation among grids for efficient dispatching strategies, existing works neglect it or limit it within neighboring grids. There exist three key challenges in scaling the cooperation to the whole city: (1) cooperative strategies cause complex interactions among grids, making the grids' states coupled and complicating the information extraction from the states for decision-making; (2) cooperation among grids requires both within- and cross-grid dispatching, where the priorities of these two types of actions are difficult to balance; (3) the value of cooperation is not only heterogeneous over different pairs of grids, but also varies temporally, adding difficulty to dynamically determine the intensities of cooperation for each pair of grids and obtain the global cooperation rewards. In this paper, we propose the CoopRide framework to solve the above challenges. We model the interactions among agents with graphs and utilize graph neural network (GNN) for efficient information extraction. We uniformly encode both within- and cross-grid dispatching, enabling flexible choice of both types of actions in the embedding space. We also design to automatically learn the cooperation intensities among grids, thereby obtaining the cooperative rewards to drive the learning of global cooperation actions. We conduct experiments in three real-world datasets with millions of orders, and extensive results demonstrate the superior performance of CoopRide, outperforming the state-of-the-art baselines by up to 12.4%. Our source codes are available at https://github.com/tsinghua-fib-lab/CoopRide.}
}


@inproceedings{DBLP:conf/kdd/Wang0CX000YZL25,
	author = {Mingzhao Wang and
                  You Zhou and
                  Zhiguang Cao and
                  Yubin Xiao and
                  Xuan Wu and
                  Wei Pang and
                  Yuan Jiang and
                  Hui Yang and
                  Peng Zhao and
                  Yuanshu Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {An Efficient Diffusion-based Non-Autoregressive Solver for Traveling
                  Salesman Problem},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1469--1480},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709343},
	doi = {10.1145/3690624.3709343},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Wang0CX000YZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in neural models have shown considerable promise in solving Traveling Salesman Problems (TSPs) without relying on much hand-crafted engineering. However, while non-autoregressive (NAR) approaches benefit from faster inference through parallelism, they typically deliver solutions of inferior quality compared to autoregressive ones. To enhance the solution quality while maintaining fast inference, we propose DEITSP, a diffusion model with efficient iterations tailored for TSP that operates in a NAR manner. Firstly, we introduce a one-step diffusion model that integrates the controlled discrete noise addition process with self-consistency enhancement, enabling optimal solution prediction through simultaneous denoising of multiple solutions. Secondly, we design a dual-modality graph transformer to bolster the extraction and fusion of features from node and edge modalities, while further accelerating the inference with fewer layers. Thirdly, we develop an efficient iterative strategy that alternates between adding and removing noise to improve exploration compared to previous diffusion methods. Additionally, we devise a scheduling framework to progressively refine the solution space by adjusting noise levels, facilitating a smooth search for optimal solutions. Extensive experiments on real-world and large-scale TSP instances demonstrate that DEITSP performs favorably against existing neural approaches in terms of solution quality, inference latency, and generalization ability.}
}


@inproceedings{DBLP:conf/kdd/WangLM0XJ25,
	author = {Qi (Cheems) Wang and
                  Yiqin Lv and
                  Yixiu Mao and
                  Yun Qu and
                  Yi Xu and
                  Xiangyang Ji},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Robust Fast Adaptation from Adversarially Explicit Task Distribution
                  Generation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1481--1491},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709337},
	doi = {10.1145/3690624.3709337},
	timestamp = {Tue, 13 May 2025 07:31:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WangLM0XJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Meta-learning is a practical learning paradigm to transfer skills across tasks from a few examples. Nevertheless, the existence of task distribution shifts tends to weaken meta-learners' generalization capability, particularly when the training task distribution is naively hand-crafted or based on simple priors that fail to cover critical scenarios sufficiently. Here, we consider explicitly generative modeling task distributions placed over task identifiers and propose robustifying fast adaptation from adversarial training. Our approach, which can be interpreted as a model of a Stackelberg game, not only uncovers the task structure during problem-solving from an explicit generative model but also theoretically increases the adaptation robustness in worst cases. This work has practical implications, particularly in dealing with task distribution shifts in meta-learning, and contributes to theoretical insights in the field. Our method demonstrates its robustness in the presence of task subpopulation shifts and improved performance over SOTA baselines in extensive experiments. The code is available at the project site https://sites.google.com/view/ar-metalearn.}
}


@inproceedings{DBLP:conf/kdd/Wang0CZQ25,
	author = {Rongzheng Wang and
                  Shuang Liang and
                  Qizhi Chen and
                  Jiasheng Zhang and
                  Ke Qin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {GraphTool-Instruction: Revolutionizing Graph Reasoning in LLMs through
                  Decomposed Subtask Instruction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1492--1503},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709238},
	doi = {10.1145/3690624.3709238},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Wang0CZQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have been demonstrated to possess the capabilities to understand fundamental graph properties and address various graph reasoning tasks. Existing methods fine-tune LLMs to understand and execute graph reasoning tasks by specially designed task instructions. However, these Text-Instruction methods generally exhibit poor performance. Inspired by tool learning, researchers propose Tool-Instruction methods to solve various graph problems by special tool calling ( e.g.,  function, API and model), achieving significant improvements in graph reasoning tasks. Nevertheless, current Tool-Instruction approaches focus on the tool information and ignore the graph structure information, which leads to significantly inferior performance on small-scale LLMs (less than 8B). To tackle this issue, we propose GraphTool-Instruction, an innovative Instruction-tuning approach that decomposes the graph reasoning task into three distinct subtasks ( i.e., graph extraction, tool name identification  and  tool parameter extraction ), and design specialized instructions for each subtask. Our GraphTool-Instruction can be used as a plug-and-play prompt for different LLMs without fine-tuning. Moreover, building on GraphTool-Instruction, we develop GTools, a dataset that includes twenty graph reasoning tasks, and create a graph reasoning LLM called GraphForge based on Llama3-8B. We conduct extensive experiments on twenty graph reasoning tasks with different graph types ( e.g.,  graph size or graph direction), and we find that GraphTool-Instruction achieves SOTA compared to Text-Instruction and Tool-Instruction methods. Fine-tuned on GTools, GraphForge gets further improvement of over 30% compared to the Tool-Instruction enhanced GPT-3.5-turbo, and it performs comparably to the high-cost GPT-4o. Our codes and data are available at https://github.com/RongzhengWang/GraphTool-Instruction.}
}


@inproceedings{DBLP:conf/kdd/00020D25,
	author = {Shuting Wang and
                  Yutao Zhu and
                  Zhicheng Dou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Embedding Prior Task-specific Knowledge into Language Models for Context-aware
                  Document Ranking},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1504--1514},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709282},
	doi = {10.1145/3690624.3709282},
	timestamp = {Thu, 01 May 2025 20:24:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/00020D25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Exploiting users' contextual behaviors in the current session has been proven favorable to the document ranking task. Recently, the context-aware document ranking task has benefited from pre-trained language models (PLMs) due to their superior ability in language modeling. Most PLM-based context-aware document ranking models implicitly learn task-specific knowledge by fine-tuning PLMs on historical search logs. However, since search log data is noisy and contains various user intents and search patterns, such a black-box way may prevent models from fully mastering effective context-aware search knowledge. To solve this problem, we propose LOCK, a PLM-based context-aware document ranking model that explicitly embeds task-specific prior knowledge into PLMs to guide the model optimization. From local to global, we identify three types of task-specific knowledge, including intra-turn signals, inter-turn signals, and global session signals. LOCK formulates such prior knowledge into prior attention biases for impacting the fine-tuning of PLMs. This operation can guide the ranking model by task-specific prior knowledge, thereby improving model convergence and ranking ability. Additionally, we introduce a task-specific pre-training stage that involves masked language modeling and the soft reconstruction of the prior attention matrix, which helps the PLMs adapt to our task. Extensive experiments validate the effectiveness and convergence of our method.}
}


@inproceedings{DBLP:conf/kdd/WangW0YSX0L25,
	author = {Xianquan Wang and
                  Likang Wu and
                  Zhi Li and
                  Haitao Yuan and
                  Shuanghong Shen and
                  Huibo Xu and
                  Yu Su and
                  Chenyi Lei},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Mitigating Redundancy in Deep Recommender Systems: {A} Field Importance
                  Distribution Perspective},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1515--1526},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709275},
	doi = {10.1145/3690624.3709275},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WangW0YSX0L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the realm of recommender systems, accurately predicting Click-Through Rate (CTR) is a critical task that involves learning user-item interaction features. Many researchers propose novel models to mine interaction signals, but they neglect that redundancy itself causes high computational cost and leads to suboptimal performance. Some tried to remove redundancy by dropping useless features, or shrinking the size of embedding table. However, current feature selection methods are vulnerable to training stochasticity and data dynamics, while embedding size assignment techniques neglect the importance relationships between feature fields. The simple combination of the two optimization ways will also yield poor performance due to the inherent gap in their optimization targets. Hence, there is no effective paradigm that can optimize feature fields from the two aspects in a simultaneous and coordinated way. In this paper, we identify the core issue as the lack of a practical score to measure the contribution of feature fields, and propose a distribution-based field optimization framework that adopts importance distribution to provide a comprehensive view for both methods. We innovatively design a learner for each field to acquire the stable and comprehensive importance situation. Then, based on this, we eliminate noise features, and assign adaptive embedding sizes for different feature fields according to the similarity of importance. With this field optimization, our proposed framework has extremely low pre-training overhead, greatly reduces training and inference time, and even achieves more accurate prediction results with fewer feature fields.}
}


@inproceedings{DBLP:conf/kdd/WangZSLH25,
	author = {Xiaotang Wang and
                  Yun Zhu and
                  Haizhou Shi and
                  Yongchao Liu and
                  Chuntao Hong},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Graph Triple Attention Networks: {A} Decoupled Perspective},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1527--1538},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709223},
	doi = {10.1145/3690624.3709223},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WangZSLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Transformers (GTs) have recently achieved significant success in the graph domain by effectively capturing both long-range dependencies and graph inductive biases. However, these methods face two primary challenges: (1)  multi-view chaos,  which results from coupling multi-view information (positional, structural, attribute), thereby impeding flexible usage and the interpretability of the propagation process. (2)  local-global chaos,  which arises from coupling local message passing with global attention, leading to issues of overfitting and over-globalizing. To address these challenges, we propose a high-level decoupled perspective of GTs, breaking them down into three components and two interaction levels: positional attention, structural attention, and attribute attention, alongside local and global interaction. Based on this decoupled perspective, we design a decoupled graph triple attention network named DeGTA, which separately computes multi-view attentions and adaptively integrates multi-view local and global information. This approach offers three key advantages: enhanced interpretability, flexible design, and adaptive integration of local and global information. Through extensive experiments, DeGTA achieves state-of-the-art performance across various datasets and tasks, including node classification and graph classification. Comprehensive ablation studies demonstrate that decoupling is essential for improving performance and enhancing interpretability. Our code is available at: https://github.com/wangxiaotang0906/DeGTA}
}


@inproceedings{DBLP:conf/kdd/WangZCLML025,
	author = {Xiong Wang and
                  Yi Zhang and
                  Yuxin Chen and
                  Yuqing Li and
                  Chuanhu Ma and
                  Bo Li and
                  Hai Jin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Runtime-Aware Pipeline for Vertical Federated Learning with Bounded
                  Model Staleness},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1539--1550},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709243},
	doi = {10.1145/3690624.3709243},
	timestamp = {Tue, 13 May 2025 07:31:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WangZCLML025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vertical federated learning  (VFL) enables a privacy-preserving collaboration among various parties to train a global model by melding their geo-distributed data features. Communication has been recognized as the primary bottleneck that impairs training efficiency due to frequent  cross-party statistics exchange  over wide area network. Existing synchronous VFL works often suffer from excessive communication overhead, while asynchronous schemes may introduce significant model staleness, potentially eroding the learning accuracy. In this paper, we propose BS-VFL, an  asynchronous  VFL with  bounded staleness,  to pipeline local computation and statistics transmission, substantially reducing the communication overhead while ensuring favorable model performance. Specifically, all data parties will give precedence to local model updates before generating embeddings to curtail model staleness. By analyzing convergence error, we show that BS-VFL can achieve a  comparable  result to synchronous VFL. Then, we develop a general framework to derive the  closed-form wall-clock time  of BS-VFL, offering a measure of its runtime efficiency and highlighting a marked communication reduction. Utilizing this convergence and time analysis, we  refine  learning parameters to minimize the convergence error for optimizing BS-VFL performance without compromising training efficiency. Extensive experiments on real-world datasets validate the superiority of BS-VFL over leading-edge methods, evidencing a reduction in training duration by 48%-90% while preserving model accuracy.}
}


@inproceedings{DBLP:conf/kdd/Wang0XWJSZZ025,
	author = {Yaxuan Wang and
                  Hao Cheng and
                  Jing Xiong and
                  Qingsong Wen and
                  Han Jia and
                  Ruixuan Song and
                  Liyuan Zhang and
                  Zhaowei Zhu and
                  Yang Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Noise-Resilient Point-wise Anomaly Detection in Time Series Using
                  Weak Segment Labels},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1551--1562},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709257},
	doi = {10.1145/3690624.3709257},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Wang0XWJSZZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting anomalies in temporal data has gained significant attention across various real-world applications, aiming to identify unusual events and mitigate potential hazards. In practice, situations often involve a mix of segment-level labels (detected abnormal events with segments of time points) and unlabeled data (undetected events), while the ideal algorithmic outcome should be point-level predictions. Therefore, the huge label information gap between training data and targets makes the task challenging. In this study, we formulate the above imperfect information as noisy labels and propose NRdetector, a noise-resilient framework that incorporates confidence-based sample selection, robust segment-level learning, and data-centric point-level detection for multivariate time series anomaly detection. Particularly, to bridge the information gap between noisy segment-level labels and missing point-level labels, we develop a novel loss function that can effectively mitigate the label noise and consider the temporal features. It encourages the smoothness of consecutive points and the separability of points from segments with different labels. Extensive experiments on real-world multivariate time series datasets with 11 different evaluation metrics demonstrate that NRdetector consistently achieves robust results across multiple real-world datasets, outperforming various baselines adapted to operate in our setting.}
}


@inproceedings{DBLP:conf/kdd/Wei0HBH25,
	author = {Tianxin Wei and
                  Yifan Chen and
                  Xinrui He and
                  Wenxuan Bao and
                  Jingrui He},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Connecting Domains and Contrasting Samples: {A} Ladder for Domain
                  Generalization},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1563--1574},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709280},
	doi = {10.1145/3690624.3709280},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Wei0HBH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distribution shifts between training and testing samples frequently occur in practice and impede model generalization performance. This crucial challenge thereby motivates studies on domain generalization (DG), which aim to predict the label on unseen target domain data by solely using data from source domains. It is intuitive to conceive the class-separated representations learned in contrastive learning (CL) are able to improve DG, while the reality is quite the opposite: users observe directly applying CL deteriorates the performance. We analyze the phenomenon with the insights from CL theory and discover lack of  intra-class connectivity  in the DG setting causes the deficiency. We thus propose a new paradigm, domain-connecting contrastive learning (DCCL), to enhance the conceptual connectivity across domains and obtain generalizable representations for DG. On the data side, more aggressive data augmentation and cross-domain positive samples are introduced to improve intra-class connectivity. On the model side, to better embed the unseen test domains, we propose model anchoring to exploit the intra-class connectivity in pre-trained representations and complement the anchoring with generative transformation loss. Extensive experiments on five standard DG benchmarks are performed. The results verify that DCCL outperforms state-of-the-art baselines even without domain supervision. The detailed model implementation and the code are provided through https://github.com/weitianxin/DCCL.}
}


@inproceedings{DBLP:conf/kdd/Wen0YCSY25,
	author = {Hechuan Wen and
                  Tong Chen and
                  Guanhua Ye and
                  Li Kheng Chai and
                  Shazia Sadiq and
                  Hongzhi Yin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Progressive Generalization Risk Reduction for Data-Efficient Causal
                  Effect Estimation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1575--1586},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709305},
	doi = {10.1145/3690624.3709305},
	timestamp = {Fri, 09 May 2025 20:27:54 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Wen0YCSY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Causal effect estimation (CEE) provides a crucial tool for predicting the unobserved counterfactual outcome for an entity. As CEE relaxes the requirement for "perfect\'\' counterfactual samples (e.g., patients with identical attributes and only differ in treatments received) that are impractical to obtain and can instead operate on observational data, it is usually used in high-stake domains like medical treatment effect prediction. Nevertheless, in those high-stake domains, gathering a decently sized, fully labelled observational dataset remains challenging due to hurdles associated with costs, ethics, expertise and time needed, etc., of which medical treatment surveys are a typical example. Consequently, if the training dataset is small in scale, low generalization risks can hardly be achieved on any CEE algorithms. Unlike existing CEE methods that assume the constant availability of a dataset with abundant samples, in this paper, we study a more realistic CEE setting where the labelled data samples are scarce at the beginning, while more can be gradually acquired over the course of training -- assuredly under a limited budget considering their expensive nature. Then, the problem naturally comes down to actively selecting the best possible samples to be labelled, e.g., identifying the next subset of patients to conduct the treatment survey. However, acquiring quality data for reducing the CEE risk under limited labelling budgets remains under-explored until now. To fill the gap, we theoretically analyse the generalization risk from an intriguing perspective of progressively shrinking its upper bound, and develop a principled label acquisition pipeline exclusively for CEE tasks. With our analysis, we propose the Model Agnostic Causal Active Learning (MACAL) algorithm for batch-wise label acquisition, which aims to reduce both the CEE model\'s uncertainty and the post-acquisition distributional imbalance simultaneously at each acquisition step. Extensive experiments are conducted on three datasets, where a clear empirical performance gain from MACAL is observed over state-of-the-art active learning baselines. The implementation repository is open-sourced at: https://github.com/uqhwen2/MACAL.}
}


@inproceedings{DBLP:conf/kdd/WenFWHX0HWJ25,
	author = {Zhenyu Wen and
                  Wanglei Feng and
                  Di Wu and
                  Haozhen Hu and
                  Chang Xu and
                  Bin Qian and
                  Zhen Hong and
                  Cong Wang and
                  Shouling Ji},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {FLMarket: Enabling Privacy-preserved Pre-training Data Pricing for
                  Federated Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1587--1598},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709346},
	doi = {10.1145/3690624.3709346},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WenFWHX0HWJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL), as a mainstream privacy-preserving machine learning paradigm, offers promising solutions for privacy-critical domains such as healthcare and finance. Although extensive efforts have been dedicated from both academia and industry to improve the vanilla FL, little work focuses on the data pricing mechanism. In contrast to the straightforward in-training and post-training pricing techniques, we study a more difficult problem of pre-training pricing without direct information from the learning process. We propose  FLMarket  that integrates a two-stage pricing mechanism with a security protocol to address the utility-privacy conflict. Through comprehensive experiments, we show that the client selection according to  FLMarket  can achieve more than 10% higher accuracy in subsequent FL training compared to state-of-the-art methods. In addition, it outperforms the in-training baseline with more than 2% accuracy increase and 3× run-time speedup.}
}


@inproceedings{DBLP:conf/kdd/WestphalHM25,
	author = {Charles Westphal and
                  Stephen Hailes and
                  Mirco Musolesi},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Feature Selection for Network Intrusion Detection},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1599--1610},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709339},
	doi = {10.1145/3690624.3709339},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WestphalHM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Intrusion Detection (NID) remains a key area of research within the information security community, while also being relevant to Machine Learning (ML) practitioners. The latter generally aim to detect attacks using network features, which have been extracted from raw network data typically using dimensionality reduction methods, such as principal component analysis (PCA). However, PCA is not able to assess the relevance of features for the task at hand. Consequently, the features available are of varying quality, with some being entirely non-informative. From this, two major drawbacks arise. Firstly, trained and deployed models have to process large amounts of unnecessary data, therefore draining potentially costly resources. Secondly, the noise caused by the presence of irrelevant features can, in some cases, impede a model's ability to detect an attack. In order to deal with these challenges, we present Feature Selection for Network Intrusion Detection (FSNID) a novel information-theoretic method that facilitates the exclusion of non-informative features when detecting network intrusions. The proposed method is based on function approximation using a neural network, which enables a version of our approach that incorporates a recurrent layer. Consequently, this version uniquely enables the integration of temporal dependencies. Through an extensive set of experiments, we demonstrate that the proposed method selects a significantly reduced feature set, while maintaining NID performance. Code available at https://github.com/c-s-westphal/FSNID.}
}


@inproceedings{DBLP:conf/kdd/WuLZKZ25,
	author = {Anpeng Wu and
                  Haoxuan Li and
                  Chunyuan Zheng and
                  Kun Kuang and
                  Kun Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Classifying Treatment Responders: Bounds and Algorithms},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1611--1622},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709191},
	doi = {10.1145/3690624.3709191},
	timestamp = {Tue, 06 May 2025 07:48:40 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WuLZKZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Treatment responders are individuals whose outcomes would change from negative to positive if treated, and learning a classifier to predict responders would help causal decision-making in real applications. Although many treatment effect estimation methods have been proposed to identify treatment responders, there are fundamental differences between treatment effect estimation and treatment responder classification, including: (1) accurate causal effect estimation is not necessary for optimal intervention decisions; (2) methods for accurate causal effect estimation do not directly optimize classification loss; (3) treatment responder classification requires identifying joint potential outcomes, while treatment effect estimation focuses on marginal distributions. To fill this gap, we tackle the treatment responder classification problem without assuming monotonicity. We derive sharp bounds of the probability that an individual is a responder and determine a sharp upper bound on the weighted classification risk to measure the worst classification performance. Based on these findings, we further propose a Classifying Treatment Responder Learning (CTRL) algorithm to accurately identify the treatment responders, and theoretically demonstrate the superiority of jointly learning over two-stage learning. Extensive experiments on semi-synthetic and real-world datasets show that our method better predicts treatment responders and adaptively trades off false-positives and false-negatives with varying weight coefficients.}
}


@inproceedings{DBLP:conf/kdd/Wu0025,
	author = {Yebo Wu and
                  Li Li and
                  Cheng{-}Zhong Xu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Breaking the Memory Wall for Heterogeneous Federated Learning via
                  Progressive Training},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1623--1632},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709284},
	doi = {10.1145/3690624.3709284},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Wu0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables multiple devices to collaboratively train a shared model while preserving data privacy. Most existing research assumes that all participating devices have sufficient resources to support the training process. However, the high memory requirements of model training present a significant challenge to deploying FL on resource-constrained devices in practical scenarios. To this end, this paper presents  ProFL,  a new framework that effectively addresses the memory constraints in FL. Rather than updating the full model during local training, ProFL partitions the model into blocks based on its original architecture and trains each block in a progressive fashion. It first trains the front blocks and safely freezes them after convergence. Training of the next block is then triggered. This process progressively grows the model to be trained until the training of the full model is completed. In this way, the peak memory footprint is effectively reduced for feasible deployment on heterogeneous devices. In order to preserve the feature representation of each block, the training process is divided into two stages: model shrinking and model growing. During the model shrinking stage, we meticulously design corresponding output modules to assist each block in learning the expected feature representation and obtain the initialization model parameters. Subsequently, the obtained output modules and initialization model parameters are utilized in the corresponding model growing stage, which progressively trains the full model. Additionally, a novel metric from the scalar perspective is proposed to assess the learning status of each block, enabling us to securely freeze it after convergence and initiate the training of the next one. Finally, we theoretically prove the convergence of ProFL and conduct extensive experiments on representative models and datasets to evaluate its effectiveness. The results demonstrate that ProFL effectively reduces the peak memory footprint by up to 57.4% and improves model accuracy by up to 82.4%.}
}


@inproceedings{DBLP:conf/kdd/Wu0C25,
	author = {Zhangkai Wu and
                  Xuhui Fan and
                  Longbing Cao},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {ProgDiffusion: Progressively Self-encoding Diffusion Models},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1633--1644},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709222},
	doi = {10.1145/3690624.3709222},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Wu0C25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning low-dimensional semantic representations in diffusion models (DMs) is an open task, since in standard DMs, the dimensions of its intermediate latents are the same as that of the observations and thus are unable to represent low-dimensional semantics. Existing methods address this task either by encoding observations into semantics which makes it difficult to generate samples without observations, or by synthesizing the U-Net's layers of pre-trained DMs into low-dimensional semantics, which is mainly used for downstream tasks rather than using semantics to facilitate the training process. Further, those generated static representations might not be aligned with dynamic timestep-wise intermediate latents. This work introduces a Progressive self-encoded Diffusion model (ProgDiffusion), which simultaneously learns semantic representations and reconstructs observations, does efficient unconditional generation, and produces progressively structured semantic representations. These benefits are gained by a novel self-encoder mechanism which takes the U-Net's upsampling features, intermediate latent and the denoising timestep as conditions to generate time-specific semantic representations, differing from existing work of conditioning on observations only. As a result, the learned intermediate latents are dynamic and mapped to a series of semantic representations that capture their gradual changes. Notably, our proposed encoder operates independently of the observations, making it feasible for unconditional generation as observations are not required. To evaluate ProgDiffusion, we design tasks to visualise the learned progressive semantic representations, in addition to other common tasks, which validate the effectiveness of ProgDiffusion against the state-of-the-art. The code is available at https://github.com/amasawa/ProgDiffusion.}
}


@inproceedings{DBLP:conf/kdd/XiaLWZW025,
	author = {Kaiwen Xia and
                  Li Lin and
                  Shuai Wang and
                  Qi Zhang and
                  Shuai Wang and
                  Tian He},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {ProST: Prompt Future Snapshot on Dynamic Graphs for Spatio-Temporal
                  Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1645--1656},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709273},
	doi = {10.1145/3690624.3709273},
	timestamp = {Wed, 09 Apr 2025 09:19:48 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XiaLWZW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatio-temporal prediction focuses on jointly modeling spatial correlations and temporal evolution and has a wide range of applications. Due to the heterogeneity of spatio-temporal data, accurate prediction relies on effectively integrating topological structures and sequential patterns. Although recurrent graph learning methods excel at capturing dynamic graph patterns, explicitly inferring future snapshots from historical dynamic graphs remains a significant challenge. Recently, prompt-based graph learning has shown the potential to improve future snapshot inference by leveraging node or task-specific prompts. However, these methods fail to fully capture edge information resulting in incomplete and less accurate representations of future snapshot structures. To bridge this gap, we propose  ProST,  a framework that  Pro mpts future snapshots on dynamic graphs for  S patio- T emporal prediction, which leverages dynamic graph pre-training to generate a premise graph containing historical graph information and then employs prompts on the premise graph to infer explicit future snapshots. Specifically, this framework comprises three steps: Firstly, dynamic graph pre-training is performed using multi-granularity evolution graph convolution to obtain the premise graph with both local and global features of dynamic graphs. Secondly, prompt subgraphs are used to prompt node pairs and edge features within the premise graph. The subgraph prompt aggregation mechanism propagates this information to generate future snapshots. Finally, we freeze the parameters of the pre-trained model and update the subgraph prompt parameters using meta-learning to adapt to downstream spatio-temporal prediction tasks. Extensive experiments on real-world datasets validate that  ProST  achieves state-of-the-art performance.}
}


@inproceedings{DBLP:conf/kdd/Xiong0JM25,
	author = {Wen Xiong and
                  Jinduo Liu and
                  Junzhong Ji and
                  Fenglong Ma},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Brain Effective Connectivity Estimation via Fourier Spatiotemporal
                  Attention},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1657--1668},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709226},
	doi = {10.1145/3690624.3709226},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Xiong0JM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating brain effective connectivity (EC) from functional magnetic resonance imaging (fMRI) data can aid in comprehending the neural mechanisms underlying human behavior and cognition, providing a foundation for disease diagnosis. However, current spatiotemporal attention modules handle temporal and spatial attention separately, extracting temporal and spatial features either sequentially or in parallel. These approach overlooks the inherent spatiotemporal correlations present in real world fMRI data. Additionally, the presence of noise in fMRI data further limits the performance of existing methods. In this paper, we propose a novel brain effective connectivity estimation method based on Fourier spatiotemporal attention (FSTA-EC), which combines Fourier attention and spatiotemporal attention to simultaneously capture inter-series (spatial) dynamics and intra-series (temporal) dependencies from high-noise fMRI data. Specifically, Fourier attention is designed to convert the high-noise fMRI data to frequency domain, and map the denoised fMRI data back to physical domain, and spatiotemporal attention is crafted to simultaneously learn spatiotemporal dynamics. Furthermore, through a series of proofs, we demonstrate that incorporating learnable filters into fast Fourier transform and inverse fast Fourier transform processes is mathematically equivalent to performing cyclic convolution. The experimental results on simulated and real-resting-state fMRI datasets demonstrate that the proposed method exhibits superior performance when compared to state-of-the-art methods. The code is available at https://github.com/XiongWenXww/FSTA.}
}


@inproceedings{DBLP:conf/kdd/XuWCLWH25,
	author = {Borui Xu and
                  Zeyi Wen and
                  Yao Chen and
                  Weiguo Liu and
                  Weng{-}Fai Wong and
                  Bingsheng He},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {ScalaGBM: Memory Efficient {GBDT} Training for High-Dimensional Data
                  on {GPU}},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1669--1678},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709271},
	doi = {10.1145/3690624.3709271},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XuWCLWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gradient Boosted Decision Trees (GBDTs) are classical machine learning algorithms widely employed in recommendation systems, database queries, etc. Due to the extensive memory access involved in histogram-based GBDT training methods, high-bandwidth GPUs have been widely adopted to accelerate the training. However, when handling millions of feature data, it requires significant memory to store the training data and histograms, posing challenges for training on limited GPU memories. In this paper, we develop a GPU-based GBDT framework named ScalaGBM, aiming to accelerate high-dimensional data training with less memory usage. We first employ a CSR-like data format and CSR-based histogram construction to reduce the memory occupation of the training data. Then, we reorganize the training workflow with a double buffer structure to reduce the overall memory consumption for the histogram. Finally, we develop multi-dimensional parallel histogram construction and global optimal split point reduction to speed up the training process. Experimental results demonstrate that ScalaGBM handles real-world datasets with over 100 million instances of 50 million features with a single commercial GPU while existing GBDT frameworks all run into out-of-memory errors. Meanwhile, ScalaGBM achieves a maximum speedup of 39× over state-of-the-art GBDT counterparts without sacrificing the training quality. The code is available at https://github.com/Xtra-Computing/thundergbm.}
}


@inproceedings{DBLP:conf/kdd/0008TTH25,
	author = {Chao Xu and
                  Xijia Tang and
                  Hong Tao and
                  Chenping Hou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Incremental Label Distribution Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1679--1690},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709318},
	doi = {10.1145/3690624.3709318},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0008TTH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Label distribution learning (LDL) has large practical application potentials due to its superiority in dealing with ambiguous label information. Most existing LDL methods are designed in a closed environment, wherein all the elements, e.g., feature and label space, are fixed. Nevertheless, in reality, data are dynamically acquired in the open environment, wherein the feature space can accumulate over time and the label space can be further enriched and refined accordingly with the accumulated feature space. Conducting LDL for such simultaneous augmentation of feature and label is crucial but rarely studied, particularly when the labeled samples with full observations are limited. In this paper, we propose a novel Incremental Label Distribution Learning (ILDL) method to tackle this brand new LDL problem by continuously transiting discriminative information from the previous model to the current one. Concretely, a prior compensation regularization is designed for such discriminative information transitivity. In this manner, the current model has the capacity to reuse the previous model to guide its own training. Furthermore, we present the theoretical analyses about the generalization bound, which provides guarantees for model inheritance. Comprehensive experimental studies validate the effectiveness of our proposal.}
}


@inproceedings{DBLP:conf/kdd/XuCS025,
	author = {Derek Xu and
                  Yuanzhou Chen and
                  Yizhou Sun and
                  Wei Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Neural Network Pruning for Invariance Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1691--1702},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709262},
	doi = {10.1145/3690624.3709262},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XuCS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Model scaling laws have driven the development of neural networks with more-and-more parameters. As a result, there is a growing need for neural network pruning to enable the efficient deployment during inference. We take a deeper look at neural network pruning from the lens of invariance preservation. We argue that successfully pruned neural networks should be invariant to transformations which do not alter the data's underlying semantics (ex. translations). To this goal, we first show existing post-pruning algorithms do not perserve desired invariances. We then propose a framework to discover novel architectures that do capture desired invariances from data via pruning. Specifically, we show contrastive learning with small initialization can effectively transfer invariance preservation encoded in the model weights to the pruning mask. Our approach consistently outperform traditional pruning algorithms on fully-connected, convolutional, and transformer networks across 3 vision, 40 tabular, and 1 natural language datasets.}
}


@inproceedings{DBLP:conf/kdd/0001CGGHY025,
	author = {Ronghui Xu and
                  Hanyin Cheng and
                  Chenjuan Guo and
                  Hongfan Gao and
                  Jilin Hu and
                  Sean Bin Yang and
                  Bin Yang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {MM-Path: Multi-modal, Multi-granularity Path Representation Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1703--1714},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709209},
	doi = {10.1145/3690624.3709209},
	timestamp = {Tue, 08 Apr 2025 16:34:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0001CGGHY025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Developing effective path representations has become increasingly essential across various fields within intelligent transportation. Although pre-trained path representation learning models have shown improved performance, they predominantly focus on the topological structures from single modality data, i.e., road networks, overlooking the geometric and contextual features associated with path-related images, e.g., remote sensing images. Similar to human understanding, integrating information from multiple modalities can provide a more comprehensive view, enhancing both representation accuracy and generalization. However, variations in information granularity impede the semantic alignment of road network-based paths (road paths) and image-based paths (image paths), while the heterogeneity of multi-modal data poses substantial challenges for effective fusion and utilization. In this paper, we propose a novel Multi-modal, Multi-granularity Path Representation Learning Framework (MM-Path), which can learn a generic path representation by integrating modalities from both road paths and image paths. To enhance the alignment of multi-modal data, we develop a multi-granularity alignment strategy that systematically associates nodes, road sub-paths, and road paths with their corresponding image patches, ensuring the synchronization of both detailed local information and broader global contexts. To address the heterogeneity of multi-modal data effectively, we introduce a graph-based cross-modal residual fusion component designed to comprehensively fuse information across different modalities and granularities. Finally, we conduct extensive experiments on two large-scale real-world datasets under two downstream tasks, validating the effectiveness of the proposed MM-Path.}
}


@inproceedings{DBLP:conf/kdd/XuCV25,
	author = {Sascha Xu and
                  Joscha C{\"{u}}ppers and
                  Jilles Vreeken},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Succinct Interaction-Aware Explanations},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1715--1726},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709175},
	doi = {10.1145/3690624.3709175},
	timestamp = {Thu, 01 May 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XuCV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shapley values ( Shap ) are a popular approach to explaining decisions of black-box models by revealing the importance of individual features.  Shap  explanations are easy to interpret, but as they do not incorporate feature interactions, they are also incomplete and potentially misleading. Interaction-aware methods such as  n Shap  report the additive importance of  all  subsets up to  n  features. These explanations are complete, but in practice excessively large and difficult to interpret. In this paper, we combine the best of both worlds. We partition the features into significantly interacting groups, and use these to compose a succinct, interpretable explanation. To determine which partitioning out of super-exponentially many explains a model best, we derive a criterion that weighs the complexity of an explanation against its representativeness for the model's behavior. To be able to find the best partitioning, we show how to prune sub-optimal solutions using a statistical test. This not only improves runtime but also helps to avoid explaining spurious interactions. Experiments show that  i Shap  represents underlying modeling more accurately than  Shap  and  n Shap , and a user study suggests that  i Shap  is perceived as more interpretable and trustworthy.}
}


@inproceedings{DBLP:conf/kdd/0002ZZ0025,
	author = {Yuanyuan Xu and
                  Wenjie Zhang and
                  Ying Zhang and
                  Xiwei Xu and
                  Xuemin Lin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Fast and Accurate Temporal Hypergraph Representation for Hyperedge
                  Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1727--1738},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709327},
	doi = {10.1145/3690624.3709327},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0002ZZ0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal hypergraph representation learning is a concept that integrates high-order structure learning with temporal dynamics, enabling more accurate analysis of temporal and high-order interactions. To enhance model expressiveness, the latest work samples multi-hop hyperedge-centric neighbors directly from temporal hypergraphs and encodes them for high-order structure learning, achieving promising performance. Such modeling, however, incurs prohibitive computational complexity, which increases exponentially with model depth and quadratically with average hyperedge cardinality, thereby limiting model scalability. In this paper, we propose FastHeP, a fast and accurate approach for temporal hyperedge prediction, which can handle large temporal hypergraphs. The key idea is to minimize computational complexity while maintaining model expressiveness. Concretely, we design an online hyperedge-centric neighbor store, which can store time-aware and redundancy-aware neighbors for nodes with rational theoretical guarantees. Upon the neighbor store, we propose a novel hybrid message passing to model temporal high-order structures, theoretically preserving strong expressive power. This explicitly learns local high-order structures for nodes of each hyperedge via graph attention, generating the node-wise structure features. These structure features are then fused into global correlations modeling among hyperedges, with a theoretical guarantee of permutation invariance. Last, FastHeP leverages local and global high-order semantics to generate temporal hyperedge embeddings, which is efficient in a  linear  complexity  w.r.t . model depth and average hyperedge cardinality. Extensive experiments show that FastHeP achieves up to  two orders of magnitude  speed-up against baselines, with an average accuracy improvement of 5.1%.}
}


@inproceedings{DBLP:conf/kdd/0024WJ025,
	author = {Chen Yang and
                  Jingyuan Wang and
                  Xiaohan Jiang and
                  Junjie Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Learning Universal Multi-level Market Irrationality Factors to Improve
                  Stock Return Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1739--1750},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709328},
	doi = {10.1145/3690624.3709328},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0024WJ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the perfect encounter of deep learning and quantitative trading has achieved great success in stock investment. Numerous deep learning-based models have been developed for forecasting stock returns, leveraging the powerful representation capabilities of neural networks to identify patterns and factors influencing stock prices. These models can effectively capture general patterns in the market, such as stock price trends, volume-price relationships, and time variations. However, the impact of special irrationality factors -- such as market sentiment, speculative behavior, market manipulation, and psychological biases -- has not been fully considered in existing deep stock forecasting models due to their relative abstraction as well as lack of explicit labels and data description. To fill this gap, we propose UMI, a Universal multi-level Market Irrationality factor model to enhance stock return forecasting. The UMI model learns factors that can reflect irrational behaviors in market from both individual stock and overall market levels. For the stock-level, UMI construct an estimated rational price for each stock, which is cointegrated with the stock's actual price. The discrepancy between the actual and the rational prices serves as a factor to indicate stock-level irrational events. Additionally, we define market-level irrational behaviors as anomalous synchronous fluctuations of stocks within a market. Using two self-supervised representation learning tasks, i.e., sub-market comparative learning and market synchronism prediction, the UMI model incorporates market-level irrationalities into a market representation vector, which is then used as the market-level irrationality factor. We also developed a forecasting model that captures both temporal and relational dependencies among stocks, accommodating the UMI factors. Extensive experiments on U.S. and Chinese stock markets with competitive baselines demonstrate our model's effectiveness and the universality of our factors in improving various forecasting models. We provide our code at https://github.com/lIcIIl/UMI.}
}


@inproceedings{DBLP:conf/kdd/YangY0ZD25,
	author = {Dezhi Yang and
                  Guoxian Yu and
                  Jun Wang and
                  Jinglin Zhang and
                  Carlotta Domeniconi},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Causal Discovery from Shifted Multiple Environments},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1751--1762},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709247},
	doi = {10.1145/3690624.3709247},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YangY0ZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A fundamental problem in many science domains is learning the causal structure of a system from observed data. The observed data canonically come from multiple environments (i.e. different times, locations, and measurements), and causal models may have unobserved shifts. Although the causal graphs can be identified by modeling the distribution changes among different environments, existing solutions can only learn causal structures when given environmental information. In contrast, we propose a causal discovery approach (CausalSME) which automatically identifies pseudo environments and unobserved distribution shifts. Specifically, CausalSME learns a causal model containing unobserved variables, which can correct the distribution shifts with mixed environments. The heart of CausalSME is a variational autoencoder that infers shifted causal effects of unobserved variables and guides the identification of environment information. It further divides the shifted samples by the identified environments to jointly learn an invariant causal model. We prove the structure identifiability of CausalSME with the causal additive model. In our extensive experiments we show that CausalSME achieves state-of-the-art performance.}
}


@inproceedings{DBLP:conf/kdd/YangC0DJH25,
	author = {Fuchao Yang and
                  Jianhong Cheng and
                  Hui Liu and
                  Yongqiang Dong and
                  Yuheng Jia and
                  Junhui Hou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Mixed Blessing: Class-Wise Embedding guided Instance-Dependent Partial
                  Label Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1763--1772},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709276},
	doi = {10.1145/3690624.3709276},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YangC0DJH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In partial label learning (PLL), every sample is associated with a candidate label set comprising the ground-truth label and several noisy labels. The conventional PLL assumes the noisy labels are randomly generated (instance-independent), while in practical scenarios, the noisy labels are always instance-dependent and are highly related to the sample features, leading to the instance-dependent partial label learning (IDPLL) problem. Instance-dependent noisy label is a double-edged sword. On one side, it may promote model training as the noisy labels can depict the sample to some extent. On the other side, it brings high label ambiguity as the noisy labels are quite undistinguishable from the ground-truth label. To leverage the nuances of IDPLL effectively, for the first time we create class-wise embeddings for each sample, which allow us to explore the relationship of instance-dependent noisy labels, i.e., the class-wise embeddings in the candidate label set should have high similarity, while the class-wise embeddings between the candidate label set and the non-candidate label set should have high dissimilarity. Moreover, to reduce the high label ambiguity, we introduce the concept of class prototypes containing global feature information to disambiguate the candidate label set. Extensive experimental comparisons with twelve methods on six benchmark data sets, including four fine-grained data sets, demonstrate the effectiveness of the proposed method. The code implementation is publicly available at https://github.com/Yangfc-ML/CEL.}
}


@inproceedings{DBLP:conf/kdd/YangGWFJSK25,
	author = {Xiaojie Yang and
                  Hangli Ge and
                  Jiawei Wang and
                  Zipei Fan and
                  Renhe Jiang and
                  Ryosuke Shibasaki and
                  Noboru Koshizuka},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {CausalMob: Causal Human Mobility Prediction with LLMs-derived Human
                  Intentions toward Public Events},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1773--1784},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709231},
	doi = {10.1145/3690624.3709231},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YangGWFJSK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale human mobility exhibits spatial and temporal patterns that can assist policymakers in decision making. Although traditional prediction models attempt to capture these patterns, they are often affected by nonperiodic public events, such as disasters and occasional celebrations. Since regular human mobility patterns are affected by these events, estimating their causal effects is critical to accurate mobility predictions. News articles provide unique perspectives on these events, though processing them is a challenge. In this study, we propose a causality based prediction model, CausalMob, to analyze the causal effects of public events. We first utilize large language models (LLMs) to extract human intentions from news and transform them into features that act as causal treatments. Next, the model learns representations of spatio-temporal regional covariates from multiple data sources to serve as confounders for causal inference. Finally, we present a causal effect estimation framework to ensure that event features remain independent of confounders during prediction. Based on large-scale real-world data, the experimental results show that the proposed model excels in human mobility prediction, outperforming state-of-the-art models.}
}


@inproceedings{DBLP:conf/kdd/YangH0025,
	author = {Zhe{-}Rui Yang and
                  Jindong Han and
                  Chang{-}Dong Wang and
                  Hao Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph
                  Transfer Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1785--1796},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709186},
	doi = {10.1145/3690624.3709186},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YangH0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in handling a range of graph analytical tasks across various domains, such as e-commerce and social networks. Despite their versatility, GNNs face significant challenges in transferability, limiting their utility in real-world applications. Existing research in GNN transfer learning overlooks discrepancies in distribution among various graph datasets, facing challenges when transferring across different distributions. How to effectively adopt a well-trained GNN to new graphs with varying feature and structural distributions remains an under-explored problem. Taking inspiration from the success of Low-Rank Adaptation (LoRA) in adapting large language models to various domains, we propose GraphLoRA, an effective and parameter-efficient method for transferring well-trained GNNs to diverse graph domains. Specifically, we first propose a Structure-aware Maximum Mean Discrepancy (SMMD) to align divergent node feature distributions across source and target graphs. Moreover, we introduce low-rank adaptation by injecting a small trainable GNN alongside the pre-trained one, effectively bridging structural distribution gaps while mitigating the catastrophic forgetting. Additionally, a structure-aware regularization objective is proposed to enhance the adaptability of the pre-trained GNN to target graph with scarce supervision labels. Extensive experiments on eight real-world datasets demonstrate the effectiveness of GraphLoRA against fourteen baselines by tuning only 20% of parameters, even across disparate graph domains. The code is available at https://github.com/AllminerLab/GraphLoRA.}
}


@inproceedings{DBLP:conf/kdd/YeKT25,
	author = {Rongguang Ye and
                  Wei{-}Bin Kou and
                  Ming Tang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {PraFFL: {A} Preference-Aware Scheme in Fair Federated Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1797--1808},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709217},
	doi = {10.1145/3690624.3709217},
	timestamp = {Tue, 13 May 2025 07:31:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YeKT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fairness in federated learning has emerged as a critical concern, aiming to develop an unbiased model among groups (e.g., male or female) of diverse sensitive features. However, there is a trade-off between model performance and fairness, i.e., improving model fairness will decrease model performance. Existing approaches have characterized such a trade-off by introducing hyperparameters to quantify client's preferences for model fairness and model performance. Nevertheless, these approaches are limited to scenarios where each client has only a single pre-defined preference, and fail to work in practical systems where each client generally has multiple preferences. To this end, we propose a  Pr eference- a ware scheme in  F air  F ederated  L earning (called PraFFL) to generate preference-specific models in real time. PraFFL can adaptively adjust the model based on each client's preferences to meet their needs. We theoretically prove that PraFFL can offer the optimal model tailored to an arbitrary preference of each client, and show its linear convergence. Experimental results show that our proposed PraFFL outperforms six fair federated learning algorithms in terms of the model's capability of adapting to clients' different preferences. Our implementation is available at https://github.com/rG223/PraFFL.}
}


@inproceedings{DBLP:conf/kdd/YiX0KS0W25,
	author = {Jingwei Yi and
                  Yueqi Xie and
                  Bin Zhu and
                  Emre Kiciman and
                  Guangzhong Sun and
                  Xing Xie and
                  Fangzhao Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Benchmarking and Defending against Indirect Prompt Injection Attacks
                  on Large Language Models},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1809--1820},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709179},
	doi = {10.1145/3690624.3709179},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YiX0KS0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of large language models (LLMs) with external content has enabled applications such as Microsoft Copilot but also introduced vulnerabilities to indirect prompt injection attacks. In these attacks, malicious instructions embedded within external content can manipulate LLM outputs, causing deviations from user expectations. To address this critical yet under-explored issue, we introduce the first benchmark for bindirect prompt injection attacks, named BIPIA, to assess the risk of such vulnerabilities. Using BIPIA, we evaluate existing LLMs and find them universally vulnerable. Our analysis identifies two key factors contributing to their success: LLMs' inability to distinguish between informational context and actionable instructions, and their lack of awareness in avoiding the execution of instructions within external content. Based on these findings, we propose two novel defense mechanisms --  boundary awareness  and  explicit reminder  -- to address these vulnerabilities in both black-box and white-box settings. Extensive experiments demonstrate that our black-box defense provides substantial mitigation, while our white-box defense reduces the attack success rate to near-zero levels, all while preserving the output quality of LLMs. We hope this work inspires further research into securing LLM applications and fostering their safe and reliable use. Our code is available at https://github.com/microsoft/BIPIA.}
}


@inproceedings{DBLP:conf/kdd/Yin00L25,
	author = {Gongzhu Yin and
                  Hongli Zhang and
                  Yuchen Yang and
                  Yi Luo},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Inductive Link Prediction on N-ary Relational Facts via Semantic Hypergraph
                  Reasoning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1821--1832},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709195},
	doi = {10.1145/3690624.3709195},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Yin00L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {N-ary relational facts represent semantic correlations among more than two entities. While recent studies have developed link prediction (LP) methods to infer missing relations for knowledge graphs (KGs) containing n-ary relational facts, they are generally limited to transductive settings. Fully inductive settings, where predictions are made on previously unseen entities, remain a significant challenge. As existing methods are mainly entity embedding-based, they struggle to capture entity-independent logical rules. To fill in this gap, we propose an n-ary subgraph reasoning framework for fully inductive link prediction (ILP) on n-ary relational facts. This framework reasons over local subgraphs and has a strong inductive inference ability to capture n-ary patterns. Specifically, we introduce a novel graph structure, the n-ary semantic hypergraph, to facilitate subgraph extraction. Moreover, we develop a subgraph aggregating network, NS-HART, to effectively mine complex semantic correlations within subgraphs. Theoretically, we provide a thorough analysis from the score function optimization perspective to shed light on NS-HART's effectiveness for n-ary ILP tasks. Empirically, we conduct extensive experiments on a series of inductive benchmarks, including transfer reasoning (with and without entity features) and pairwise subgraph reasoning. The results highlight the superiority of the n-ary subgraph reasoning framework and the exceptional inductive ability of NS-HART.}
}


@inproceedings{DBLP:conf/kdd/YooQXWT25,
	author = {Hyunsik Yoo and
                  Ruizhong Qiu and
                  Charlie Xu and
                  Fei Wang and
                  Hanghang Tong},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Generalizable Recommender System During Temporal Popularity Distribution
                  Shifts},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1833--1843},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709299},
	doi = {10.1145/3690624.3709299},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YooQXWT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many modern recommender systems represent user and item attributes as embedding vectors, relying on them for accurate recommendations. However, entangled embeddings often capture not only intrinsic property factors (e.g., user interest in item property) but also popularity factors (e.g., user conformity to item popularity) indistinguishably. These embeddings, influenced by popularity distribution, may face challenges when the popularity distribution at test time  differs  from historical distribution. Existing remedies in the literature involve disentangled embedding learning, which aims to separately capture intrinsic and popularity factors, demonstrating plausible generalization during popularity distribution shifts. However, we highlight that these methods often overlook a crucial aspect of popularity shifts- their temporal nature -in both training and inference phases. To address this, we propose Temporal Popularity distribution shift generalizABle recommender system (TPAB), a novel disentanglement framework incorporating temporal popularity. TPAB introduce a new (1) temporal-aware embedding design for users and items. Within this design, (2) popularity coarsening and (3) popularity bootstrapping are proposed to enhance generalization further. We also provide  theoretical analysis showing that the bootstrapping loss eliminates the effect of popularity on the learned model . During inference, we infer test-time popularity and corresponding embeddings, using them alongside property embeddings for prediction. Extensive experiments on real-world datasets validate TPAB, showcasing its outstanding generalization ability during temporal popularity distribution shifts.}
}


@inproceedings{DBLP:conf/kdd/YuZFJ25,
	author = {Xingtong Yu and
                  Jie Zhang and
                  Yuan Fang and
                  Renhe Jiang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Non-Homophilic Graph Pre-Training and Prompt Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1844--1854},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709219},
	doi = {10.1145/3690624.3709219},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YuZFJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs are ubiquitous for modeling complex relationships between objects across various fields. Graph neural networks (GNNs) have become a mainstream technique for graph-based applications, but their performance heavily relies on abundant labeled data. To reduce labeling requirement, pre-training and prompt learning has become a popular alternative. However, most existing prompt methods do not distinguish between homophilic and heterophilic characteristics in graphs. In particular, many real-world graphs are non-homophilic-neither strictly nor uniformly homophilic-as they exhibit varying homophilic and heterophilic patterns across graphs and nodes. In this paper, we propose ProNoG, a novel pre-training and prompt learning framework for such non-homophilic graphs. First, we examineexisting graph pre-training methods, providing insights into the choice of pre-training tasks. Second, recognizing that each node exhibits unique non-homophilic characteristics, we propose a conditional network to characterize node-specific patterns in downstream tasks. Finally, we thoroughly evaluate and analyze ProNoG through extensive experiments on ten public datasets.}
}


@inproceedings{DBLP:conf/kdd/YuanLYZH000025,
	author = {Chaohao Yuan and
                  Songyou Li and
                  Geyan Ye and
                  Yikun Zhang and
                  Long{-}Kai Huang and
                  Wenbing Huang and
                  Wei Liu and
                  Jianhua Yao and
                  Yu Rong},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Annotation-guided Protein Design with Multi-Level Domain Alignment},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1855--1866},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709199},
	doi = {10.1145/3690624.3709199},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YuanLYZH000025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The core challenge of de novo protein design lies in creating proteins with specific functions or properties, guided by certain conditions. Current models explore to generate protein using structural and evolutionary guidance, which only provide indirect conditions concerning functions and properties. However, textual annotations of proteins, especially the annotations for protein domains, which directly describe the protein's high-level functionalities, properties, and their correlation with target amino acid sequences, remain unexplored in the context of protein design tasks. In this paper, we propose Protein-Annotation Alignment Generation PAAG, a multi-modality protein design framework that integrates the textual annotations extracted from protein database for controllable generation in sequence space. Specifically, within a multi-level alignment module, PAAG can explicitly generate proteins containing specific domains conditioned on the corresponding domain annotations, and can even design novel proteins with flexible combinations of different kinds of annotations. Our experimental results underscore the superiority of the aligned protein representations from PAAG over 7 prediction tasks. Furthermore, PAAG demonstrates a significant increase in generation success rate (24.7% vs 4.7% in zinc finger, and 54.3% vs 22.0% in the immunoglobulin domain) in comparison to the existing model. We anticipate that PAAG will broaden the horizons of protein design by leveraging the knowledge from between textual annotation and proteins.}
}


@inproceedings{DBLP:conf/kdd/YuanHYW25,
	author = {Libing Yuan and
                  Shuaibo Hu and
                  Kui Yu and
                  Le Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Boosting Explainability through Selective Rationalization in Pre-trained
                  Language Models},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1867--1878},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709212},
	doi = {10.1145/3690624.3709212},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YuanHYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread application of pre-trained language models (PLMs) in natural language processing (NLP) has led to increasing concerns about their explainability. Selective rationalization is a self-explanatory framework that selects human-intelligible input subsets as rationales for predictions. Recent studies have shown that applying existing rationalization frameworks to PLMs will result in severe degeneration and failure problems, producing sub-optimal or meaningless rationales. Such failures severely damage trust in rationalization methods and constrain the application of rationalization techniques on PLMs. In this paper, we find that the homogeneity of tokens in the sentences produced by PLMs is the primary contributor to these problems. To address these challenges, we propose a method named Pre-trained Language Model's Rationalization (PLMR), which splits PLMs into a generator and a predictor to deal with NLP tasks while providing interpretable rationales. The generator in PLMR also alleviates homogeneity by pruning irrelevant tokens, while the predictor uses full-text information to standardize predictions. Experiments conducted on two widely used datasets across multiple PLMs demonstrate the effectiveness of the proposed method PLMR in addressing the challenge of applying selective rationalization to PLMs. Codes: https://github.com/ylb777/PLMR.}
}


@inproceedings{DBLP:conf/kdd/YuanT025,
	author = {Ruiwen Yuan and
                  Yongqiang Tang and
                  Wensheng Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {A Structure-aware Invariant Learning Framework for Node-level Graph
                  {OOD} Generalization},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1879--1890},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709227},
	doi = {10.1145/3690624.3709227},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YuanT025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have been proven effective in modeling graph data, mostly depending on the in-distribution assumption. While in the out-of-distribution (OOD) scenarios, especially for the more challenging node-level task, the feature and structure distribution shifts between training and test nodes lead to performance degradation. To improve node-level OOD generalization, typical approaches introduce graph augmentation to enrich the training environments and conduct invariant learning to learn stable representations across various augmented environments. However, their graph augmentations emphasize diversity but neglect the preservation of invariant patterns which are fundamental to invariant learning. Moreover, most of them simply conduct the classic invariant learning objective but lack the consideration of the graph-specific structure information. Therefore, to mitigate their weakness, we propose a Structure-aware Invariant learning framework for Node-level Graph OOD generalization (SING). Specifically, we develop the invariance constraint regularization terms during the optimization of augmentations. Additionally, we define the structure embedding to elucidate the structural property and design the structure embedding alignment loss to optimize the augmentations and the invariant representations. By introducing the structure information, we further integrate the unique structural property into invariant learning, thereby boosting the invariant message-passing GNNs. The extensive experiments on the transductive GOOD benchmark and the inductive datasets empirically validate our superior OOD generalization performance to baselines.}
}


@inproceedings{DBLP:conf/kdd/ZhaiM00Z025,
	author = {Chenhao Zhai and
                  Chang Meng and
                  Yu Yang and
                  Kexin Zhang and
                  Xuhao Zhao and
                  Xiu Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Combinatorial Optimization Perspective based Framework for Multi-behavior
                  Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1891--1902},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709278},
	doi = {10.1145/3690624.3709278},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhaiM00Z025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world recommendation scenarios, users engage with items through various types of behaviors. Leveraging diversified user behavior information for learning can enhance the recommendation of target behaviors (e.g., buy), as demonstrated by recent multi-behavior methods. The mainstream multi-behavior recommendation framework consists of two steps: fusion and prediction. Recent approaches utilize graph neural networks for multi-behavior fusion and employ multi-task learning paradigms for joint optimization in the prediction step, achieving significant success. However, these methods have limited perspectives on multi-behavior fusion, which leads to inaccurate capture of user behavior patterns in the fusion step. Moreover, when using multi-task learning for prediction, the relationship between the target task and auxiliary tasks is not sufficiently coordinated, resulting in negative information transfer. To address these problems, we propose a novel multi-behavior recommendation framework based on the combinatorial optimization perspective, named COPF. Specifically, we treat multi-behavior fusion as a combinatorial optimization problem, imposing different constraints at various stages of each behavior to restrict the solution space, thus significantly enhancing fusion efficiency (COGCN). In the prediction step, we improve both forward and backward propagation during the generation and aggregation of multiple experts to mitigate negative transfer caused by differences in both feature and label distributions (DFME). Comprehensive experiments on three real-world datasets indicate the superiority of COPF. Further analyses also validate the effectiveness of the COGCN and DFME modules. Our code is available at https://github.com/1918190/COPF.}
}


@inproceedings{DBLP:conf/kdd/ZhangXCL25,
	author = {Chao Zhang and
                  Deng Xu and
                  Chunlin Chen and
                  Huaxiong Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Semi-supervised Multi-view Clustering with Active Constraints},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1903--1912},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709204},
	doi = {10.1145/3690624.3709204},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangXCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view clustering has attracted increasing attention in recent years. However, most existing multi-view clustering approaches are performed in a purely unsupervised manner, while ignoring the valuable weak supervision information that can be obtained (e.g., active query) in many real applications. This paper considers the weak pairwise constraints among samples to enhance the clustering performance, and proposes a Semi-supervised Multi-view Clustering method with Active Constraints, SMCAC for short. SMCAC consists of two stages, clustering (C-stage) and active query (A-stage). In the C-stage, we design a tensor based multi-view graph learning model equipped with sample pairwise constraints regularization to facilitate the discriminative graph learning and fusion. An effective optimization algorithm based on alternating direction minimization is devised to solve the clustering model. In the A-stage, the most uncertain or difficult sample pairs are actively selected to query the constraints, based on the divergence of multi-view similarities learned in the C-stage. The two processes alternate iteratively until the maximum number of queries is reached. Extensive experiments on several popular datasets well validate the effectiveness of the proposed method.}
}


@inproceedings{DBLP:conf/kdd/0060HTWZ025,
	author = {Chi Zhang and
                  Qilong Han and
                  Qiaoyu Tan and
                  Shengjie Wang and
                  Xiangyu Zhao and
                  Rui Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {DimCL: Dimension-Aware Augmentation in Contrastive Learning for Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1913--1923},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709200},
	doi = {10.1145/3690624.3709200},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0060HTWZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contrastive learning (CL) has achieved remarkable success in addressing data sparsity issues in collaborative filtering (CF) for recommender systems (RSs). The key principle is to generate different augmented views given a user-item interaction graph. However, prior endeavors mainly focus on performing augmentation via stochastic functions, e.g., by injecting perturbations into different hidden dimensions uniformly. Without fine control, the hidden representations of augmentations may contain noisy dimensions that are harmful to CL and irrelevant to RSs. Removing dimension-specific noise is a challenging task due to the following two major bottlenecks. It is difficult to (i) distinguish different dimensions' efficacy for CL and (ii) bridge the semantic gap between CL and RSs. Overlooking these limitations may cause redundant, false-positive, and irrelevant noise in hidden dimensions of the augmented views. In this paper, we solve the above challenges from the perspective of robust learning and curriculum learning, and propose a novel  Dim ension-aware augmentation in  C eontrastive  L eearning for recommendation (DimCL). In DimCL, we first theoretically analyze the  easiness  and  hardness  of different dimensions for CL and RSs. With thorough analysis, we propose two propositions, which reveal that the gradients of different dimensions of augmented views are potentially related to the learning difficulty of optimizing CL and RSs. The comparison of gradients can provide detectable signals to reflect the efficacy of different dimensions for CL and the semantic gap between CL and RSs. Based on the analysis results, we devise three denoising factors, which can help DimCL to identify hard-to-learn dimensions as redundant or false-positive noise and pinpoint dimensions in different augmented views with inconsistent difficulties of RSs as irrelevant noise without requiring additional supervised labels. After denoising, DimCL can remove dimension-level noise to reduce unnecessary difficulty, making CL easier and maintaining more consistent difficulty in RSs. Extensive experiments on four public datasets demonstrate DimCL's superiority and flexible applications over various traditional and CL-based CF methods. The source code is publicly available online at https://github.com/zc-97/DimCL.}
}


@inproceedings{DBLP:conf/kdd/ZhangLL0CS25,
	author = {Liang Zhang and
                  Tao Long and
                  Yang Liu and
                  Lei Zhang and
                  Laizhong Cui and
                  Qingjiang Shi},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Generalizing Personalized Federated Graph Augmentation via Min-max
                  Adversarial Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1924--1935},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709311},
	doi = {10.1145/3690624.3709311},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangLL0CS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables the training of a global machine learning model among multiple local clients in a collaborative fashion without directly sharing the details of their data. Due to this advantage, it has been utilized in a wide range of applications where privacy is a critical concern and has attracted great attention for graph representation learning (GRL). Despite the offered advances, there still exist two major challenges in the FL for GRL across distributed graph data, including heterogeneity and complementarity. In order to tackle these challenges, a novel personalized federated graph augmentation (PFGA) framework is proposed in this work. Unlike existing techniques, it utilizes generative models as bridges to enable information sharing among clients, thereby facilitating the collaborative training of GRL models. Instead of directly using the generative model trained on each client individually, we aggregate them into the globally generative model to gain a global view of the entire graph, which effectively alleviates the heterogeneity and complementarity issues simultaneously. We formulate the training of the generative and GRL models as a min-max adversarial learning problem and theoretically prove the convergence. Furthermore, the effectiveness of the method is demonstrated using experimental results on six real-world datasets.}
}


@inproceedings{DBLP:conf/kdd/ZhangH0X25,
	author = {Sen Zhang and
                  Haibo Hu and
                  Qingqing Ye and
                  Jianliang Xu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {PrivDPR: Synthetic Graph Publishing with Deep PageRank under Differential
                  Privacy},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1936--1947},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709334},
	doi = {10.1145/3690624.3709334},
	timestamp = {Tue, 13 May 2025 07:31:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangH0X25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The objective of privacy-preserving synthetic graph publishing is to safeguard individuals' privacy while retaining the utility of original data. Most existing methods focus on graph neural networks under differential privacy (DP), and yet two fundamental problems in generating synthetic graphs remain open. First, the current research often encounters high sensitivity due to the intricate relationships between nodes in a graph. Second, DP is usually achieved through advanced composition mechanisms that tend to converge prematurely when working with a small privacy budget. In this paper, inspired by the simplicity, effectiveness, and ease of analysis of PageRank, we design PrivDPR, a novel privacy-preserving deep PageRank for graph synthesis. In particular, we achieve DP by adding noise to the gradient for a specific weight during learning. Utilizing weight normalization as a bridge, we theoretically reveal that increasing the number of layers in PrivDPR can effectively mitigate the high sensitivity and privacy budget splitting. Through formal privacy analysis, we prove that the synthetic graph generated by PrivDPR satisfies node-level DP. Experiments on real-world graph datasets show that PrivDPR preserves high data utility across multiple graph structural properties.}
}


@inproceedings{DBLP:conf/kdd/Zhang0WG0YW0025,
	author = {Sheng Zhang and
                  Maolin Wang and
                  Wanyu Wang and
                  Jingtong Gao and
                  Xiangyu Zhao and
                  Yu Yang and
                  Xuetao Wei and
                  Zitao Liu and
                  Tong Xu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{GLINT-RU:} Gated Lightweight Intelligent Recurrent Units for Sequential
                  Recommender Systems},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1948--1959},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709304},
	doi = {10.1145/3690624.3709304},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Zhang0WG0YW0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transformer-based models have gained significant traction in sequential recommender systems (SRSs) for their ability to capture user-item interactions effectively. However, these models often suffer from high computational costs and slow inference. Meanwhile, existing efficient SRS approaches struggle to embed high-quality semantic and positional information into latent representations. To tackle these challenges, this paper introduces  GLINT-RU,  a lightweight and efficient SRS leveraging a single-layer dense selective Gated Recurrent Units (GRU) module to accelerate inference. By incorporating a dense selective gate, GLINT-RU adaptively captures temporal dependencies and fine-grained positional information, generating high-quality latent representations. Additionally, a parallel mixing block infuses fine-grained positional features into user-item interactions, enhancing both recommendation quality and efficiency. Extensive experiments on three datasets demonstrate that GLINT-RU achieves superior prediction accuracy and inference speed, outperforming baselines based on RNNs, Transformers, MLPs, and SSMs. These results establish GLINT-RU as a powerful and efficient solution for SRSs. The implementation code is publicly available for reproducibility. https://github.com/szhang-cityu/GLINT-RU.}
}


@inproceedings{DBLP:conf/kdd/Zhang0ZZ025,
	author = {Shengming Zhang and
                  Le Zhang and
                  Jingbo Zhou and
                  Zhi Zheng and
                  Hui Xiong},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {LLM-Eraser: Optimizing Large Language Model Unlearning through Selective
                  Pruning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1960--1971},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709312},
	doi = {10.1145/3690624.3709312},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Zhang0ZZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We focus on unlearning unwanted knowledge in autoregressive large language models (LLMs) through pruning. Our goal is to selectively remove undesirable information (e.g., harmful responses, privacy-sensitive data) while ensuring the preservation of desirable knowledge (e.g., positive responses and objective facts). Previous approaches use gradient ascent (GA) over undesired knowledge to inversely optimize LLMs, which compromises the model's performance on desired knowledge. To address this limitation, we introduce a novel two-stage approach, named LLM-Eraser, for selectively identifying and editing parameters specifically associated with undesirable knowledge. LLM-Eraser operates in two stages: localization and unlearning. During the localization stage, we utilize neuron scores and trainable soft masks to identify parameters crucial to the undesired knowledge. In the unlearning stage, we prune these identified parameters and apply a selective post-training process to enhance the model's selectiveness. Our experiments, conducted across five task datasets, demonstrate that LLM-Eraser effectively unlearns undesirable knowledge-evidenced by the model's near-random performance on multiple-choice questions related to the erased knowledge-while maintaining high proficiency in desirable knowledge, with an average performance deficit of only 2.5%.}
}


@inproceedings{DBLP:conf/kdd/ZhangGDS25,
	author = {Tianyi Zhang and
                  Gaurav Gupta and
                  Aditya Desai and
                  Anshumali Shrivastava},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {IDentity with Locality: An Ideal Hash for Gene Sequence Search},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1972--1983},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709233},
	doi = {10.1145/3690624.3709233},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangGDS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gene sequence search is a fundamental operation in computational genomics with broad applications in medicine, evolutionary biology, metagenomics, and more. Due to the petabyte scale of genome archives, most gene search systems now use hashing-based data structures such as  Bloom Filters  (BF). The state-of-the-art systems such as  Compact bit-slicing signature index  (COBS) and  Repeated And Merged Bloom filters  (RAMBO) use BF with Random Hash (RH) functions for gene representation and identification. The standard recipe is to cast the gene search problem as a sequence of membership problems testing if each subsequent gene substring (called kmer) of Q is present in the set of kmers of the entire gene database D. We observe that RH functions, which are crucial to the memory and the computational advantage of BF, are also detrimental to the system performance of gene-search systems. While subsequent kmers being queried are likely very similar, RH, oblivious to any similarity, uniformly distributes the kmers to different parts of potentially large BF, thus triggering excessive cache misses and causing system slowdown. We propose a novel hash function called the Identity with Locality (IDL) hash family, which co-locates the keys close in input space without causing collisions. This approach ensures both cache locality and key preservation. IDL functions can be a drop-in replacement for RH functions and help improve the performance of information retrieval systems. We give a simple but practical construction of IDL function families and show that replacing the RH with IDL functions reduces cache misses by a factor of 5x, thus improving query and indexing times of SOTA methods such as COBS and RAMBO by factors up to 2x without compromising their quality. We also provide a theoretical analysis of the false positive rate of BF with IDL functions. Our hash function is the first study that bridges  Locality Sensitive Hash  (LSH) and RH to obtain cache efficiency. Our design and analysis could be of independent theoretical interest.}
}


@inproceedings{DBLP:conf/kdd/ZhangZ0LYKK25,
	author = {Yifei Zhang and
                  Hao Zhu and
                  Menglin Yang and
                  Jiahong Liu and
                  Rex Ying and
                  Irwin King and
                  Piotr Koniusz},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Understanding and Mitigating Hyperbolic Dimensional Collapse in Graph
                  Contrastive Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1984--1995},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709249},
	doi = {10.1145/3690624.3709249},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangZ0LYKK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning generalizable self-supervised graph representations for downstream tasks is challenging. To this end, Contrastive Learning (CL) has emerged as a leading approach. The embeddings of CL are arranged on a hypersphere where similarity is measured by the cosine distance. However, many real-world graphs, especially of hierarchical nature, cannot be embedded well in the Euclidean space. Although the hyperbolic embedding is suitable for hierarchical representation learning, naively applying CL to the hyperbolic space may result in the so-called dimension collapse, i.e., features will concentrate mostly within few density regions, leading to poor utilization of the whole feature space. Thus, we propose a novel contrastive learning framework to learn high-quality graph embeddings in hyperbolic space. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of the uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity related to properties of trees. In the ambient space of the hyperbolic manifold these notions translate into imposing an isotropic ring density towards boundaries of Poincaré ball. Our experiments support the efficacy of our method.}
}


@inproceedings{DBLP:conf/kdd/ZhangC00S0Q0B25,
	author = {Yutong Zhang and
                  Lixing Chen and
                  Shenghong Li and
                  Nan Cao and
                  Yang Shi and
                  Jiaxin Ding and
                  Zhe Qu and
                  Pan Zhou and
                  Yang Bai},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Way to Specialist: Closing Loop Between Specialized {LLM} and Evolving
                  Domain Knowledge Graph},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {1996--2007},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709187},
	doi = {10.1145/3690624.3709187},
	timestamp = {Thu, 01 May 2025 20:25:00 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangC00S0Q0B25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have demonstrated exceptional performance across a wide variety of domains. Nonetheless, generalist LLMs continue to fall short in reasoning tasks necessitating specialized knowledge, e.g., emotional sociology and medicine. Prior investigations into specialized LLMs focused on domain-specific training, which entails substantial efforts in domain data acquisition and model parameter fine-tuning. To address these challenges, this paper proposes the Way-to-Specialist (WTS) framework, which synergizes retrieval-augmented generation with knowledge graphs (KGs) to enhance the specialized capability of LLMs in the absence of specialized training. In distinction to existing paradigms that merely utilize external knowledge from general KGs or static domain KGs to prompt LLM for enhanced domain-specific reasoning, WTS proposes an innovative ''LLM↻KG'' paradigm, which achieves bidirectional enhancement between specialized LLM and domain knowledge graph (DKG). The proposed paradigm encompasses two closely coupled components: the  DKG-Augmented LLM  and the  LLM-Assisted DKG Evolution.  The former retrieves question-relevant domain knowledge from DKG and uses it to prompt LLM to enhance the reasoning capability for domain-specific tasks; the latter leverages LLM to generate new domain knowledge from processed tasks and use it to evolve DKG. WTS closes the loop between  DKG-Augmented LLM  and  LLM-Assisted DKG Evolution,  enabling continuous improvement in the domain specialization as it progressively answers and learns from domain-specific questions. We validate the performance of WTS on 7 datasets (e.g., TweetQA, ChatDoctor5k) spanning 6 domains, e.g., emotional sociology, medical, ect. The experimental results show that WTS surpasses the previous SOTA in 5 specialized domains, and achieves a maximum performance improvement of 11.3%.}
}


@inproceedings{DBLP:conf/kdd/Zhang0Z0Z0025,
	author = {Zhongjian Zhang and
                  Xiao Wang and
                  Huichi Zhou and
                  Yue Yu and
                  Mengmei Zhang and
                  Cheng Yang and
                  Chuan Shi},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Can Large Language Models Improve the Adversarial Robustness of Graph
                  Neural Networks?},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2008--2019},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709256},
	doi = {10.1145/3690624.3709256},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Zhang0Z0Z0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topology perturbations, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask:  Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs?  By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attacks. Therefore, another question is  how to extend the capabilities of LLMs on graph adversarial robustness.  In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN. The source code in https://github.com/zhongjian-zhang/LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph.}
}


@inproceedings{DBLP:conf/kdd/ZhaoS25,
	author = {Lifan Zhao and
                  Yanyan Shen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Proactive Model Adaptation Against Concept Drift for Online Time Series
                  Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2020--2031},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709210},
	doi = {10.1145/3690624.3709210},
	timestamp = {Thu, 01 May 2025 20:25:00 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhaoS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present  Proceed,  a novel proactive model adaptation framework for online time series forecasting.  Proceed  first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework,  Proceed  is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that  Proceed  brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.}
}


@inproceedings{DBLP:conf/kdd/ZhaoWZKZ25,
	author = {Tong Zhao and
                  Daixin Wang and
                  Zhiqiang Zhang and
                  Yulin Kang and
                  Jun Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Stable Representation Learning on Graphs from Multiple Environments
                  with Structure Distribution Shift},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2032--2042},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709269},
	doi = {10.1145/3690624.3709269},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhaoWZKZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, Graph Neural Networks (GNNs) become very effective methods to utilize graphs and have been applied to many real-world applications, including recommendation, advertisement, and financial fraud detection. In fact, GNNs are mostly trained and test in the environments with the same distribution. However, in the real cases, selection bias are inevitably existed in both the node features and the graph structures, which will lead to serious impact on the GNN performance. Several works of literature have investigated the out-of-distribution (OOD) problem on the feature distribution, but little research specifically studies the effect caused by the bias of graph structure. However, graph structure is very fundamental for GNNs since it greatly affects the message propagation mechanism. In order to solve the above problem, we propose an unsupervised Stable Graph Representation learning ( SGR ) framework to obtain stable graphs from multiple environments with graph structure bias, and to improve the stability ability of GNN model across environments. Comprehensive experiments have been carried out on 4 public benchmark dataset and a real-world financial dataset. The experimental results show that the proposed stable learning method significantly improves the stability of GNN model in varying test environments.}
}


@inproceedings{DBLP:conf/kdd/Zhao00HGY25,
	author = {Weichen Zhao and
                  Chenguang Wang and
                  Xinyan Wang and
                  Congying Han and
                  Tiande Guo and
                  Tianshu Yu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective
                  of Operator Semigroup Theory},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2043--2054},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709324},
	doi = {10.1145/3690624.3709324},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Zhao00HGY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents an analytical study of the oversmoothing issue in diffusion-based Graph Neural Networks (GNNs). Generalizing beyond extant approaches grounded in random walk analysis or particle systems, we approach this problem through operator semigroup theory. This theoretical framework allows us to rigorously prove that oversmoothing is intrinsically linked to the ergodicity of the diffusion operator. Relying on semigroup method, we can quantitatively analyze the dynamic of graph diffusion and give a specific mathematical form of the smoothing feature by ergodicity and invariant measure of operator, which improves previous works only show existence of oversmoothing. This finding further poses a general and mild ergodicity-breaking condition, encompassing the various specific solutions previously offered, thereby presenting a more universal and theoretically grounded approach to relieve oversmoothing in diffusion-based GNNs. Additionally, we offer a probabilistic interpretation of our theory, forging a link with prior works and broadening the theoretical horizon. Our experimental results reveal that this ergodicity-breaking term effectively mitigates oversmoothing measured by Dirichlet energy, and simultaneously enhances performance in node classification tasks.}
}


@inproceedings{DBLP:conf/kdd/ZhaoYXY25,
	author = {Xinjian Zhao and
                  Chaolong Ying and
                  Yaoyao Xu and
                  Tianshu Yu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Graph Learning with Distributional Edge Layouts},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2055--2066},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709206},
	doi = {10.1145/3690624.3709206},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhaoYXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) learn from graph-structured data by passing messages between neighboring nodes along edges on certain topological layouts. While layouts can be essential to GNNs' performance, extant methods generally consider obtaining layouts from limited perspectives. In this paper, we introduce  Distributional Edge Layouts  (DELs), a first-of-its-kind method to sample a collection of topological layouts from a Boltzmann distribution under physical energies. By integrating DELs into GNNs, a wide landscape of feasible graph layouts can be captured from a holistic perspective, overcoming the intrinsic drawbacks in existing GNN designs.In practice, DELs can complement various GNN architectures with high versatility. Our theoretical analysis proves that GNNs equipped with DELs maintain at least the same expressive as their original counterparts, with empirical potential offering extra expressivity. Extensive experiments demonstrate that DELs consistently and substantially improve the performance of a wide range of GNN baselines across multiple datasets, achieving state-of-the-art results. This improvement suggests that DELs capture important distributional information previously overlooked by traditional GNN approaches. DEL is open-sourced at https://github.com/LOGO-CUHKSZ/DEL.}
}


@inproceedings{DBLP:conf/kdd/Zhao0CYL0W25,
	author = {Yige Zhao and
                  Jianxiang Yu and
                  Yao Cheng and
                  Chengcheng Yu and
                  Yiding Liu and
                  Xiang Li and
                  Shuaiqiang Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Variational Graph Autoencoder for Heterogeneous Information Networks
                  with Missing and Inaccurate Attributes},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2067--2078},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709251},
	doi = {10.1145/3690624.3709251},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Zhao0CYL0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous Information Networks (HINs), which consist of various types of nodes and edges, have recently witnessed excellent performance in graph mining. However, most existing heterogeneous graph neural networks (HGNNs) fail to simultaneously handle the problems of missing attributes, inaccurate attributes and scarce node labels, which limits their expressiveness. In this paper, we propose a generative self-supervised model GraMI to address these issues simultaneously. Specifically, GraMI first initializes all the nodes in the graph with a low-dimensional representation matrix. After that, based on the variational graph autoencoder framework, GraMI learns both node-level and attribute-level embeddings in the encoder, which can provide fine-grained semantic information to construct node attributes. In the decoder, GraMI reconstructs both links and attributes. Instead of directly reconstructing raw features for attributed nodes, GraMI generates the initial low-dimensional representation matrix for all the nodes, based on which raw features of attributed nodes are further reconstructed. In this way, GraMI can not only complete informative features for non-attributed nodes, but rectify inaccurate ones for attributed nodes. Finally, we conduct extensive experiments to show the superiority of GraMI in tackling HINs with missing and inaccurate attributes. Our code and data can be found here: https://github.com/See-r/GraMI.}
}


@inproceedings{DBLP:conf/kdd/ZhaoWWSH025,
	author = {Yuhai Zhao and
                  Yejiang Wang and
                  Zhengkui Wang and
                  Wen Shan and
                  Miaomiao Huang and
                  Xingwei Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Graph Contrastive Learning with Progressive Augmentations},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2079--2088},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709307},
	doi = {10.1145/3690624.3709307},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhaoWWSH025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To be still yet still moving. - Do Hyun Choe Graph contrastive learning (GCL) has recently gained prominence in unsupervised graph representation learning. Traditional GCL approaches generally focus on creating a single contrastive view alongside the main graph view, targeting invariant representation learning in a static framework. Our study introduces a novel manner: despite using static graphs, we aim to learn invariant representations by generating a series of evolving contrastive views with temporal coherence and multi-viewpoint insights at various granularities. In this context, we propose the Progressive Augmentation framework for Graph Contrastive Learning (PaGCL). This framework advances beyond traditional methods by producing a sequence of augmented views, each evolving from the previous one, and assigning timestamps based on piecewise smoothness. This approach enables our model to more effectively extract invariant features from these dynamic views, capturing multi-grained structural and temporal information. Our experiments on diverse benchmark datasets demonstrate that PaGCL significantly outperforms current state-of-the-art methods.}
}


@inproceedings{DBLP:conf/kdd/0008SXLY0L25,
	author = {Ziming Zhao and
                  Zhuoxue Song and
                  Xiaofei Xie and
                  Zhaoxuan Li and
                  Jiongchi Yu and
                  Fan Zhang and
                  Tingting Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Towards Context-Aware Traffic Classification via Time-Wavelet Fusion
                  Network},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2089--2100},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709315},
	doi = {10.1145/3690624.3709315},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0008SXLY0L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Encrypted traffic classification occupies a significant role in cybersecurity and network management. The existing encrypted traffic classification technology mostly relies on intra-flow semantics for extracting features. However, considering that some attack behaviors inherently have similar patterns to legitimate behaviors, and powerful adversaries could simulate benign users to conceal their attack intentions, intra-flow features may be similar between different categories. In this paper, we propose TrafficScope, a time-wavelet fusion network based on Transformer to enhance the performance of encrypted traffic classification. Specifically, in addition to using intra-flow semantics, TrafficScope also extracts contextual information to construct more comprehensive representations. Moreover, to cope with the non-stationary and dynamic contextual traffic, we employ wavelet transform to extract invariant features. For feature fusion, the cross-attention mechanism is adopted to inline combine temporal and wavelet-domain features. We extensively evaluate TrafficScope compared with 7 state-of-the-art baselines based on four groups of real-world traffic datasets, the results show that TrafficScope outperforms existing methods. We conduct a series of experiments in terms of similar intra-flow feature evaluation, data pollution, flow manipulations, and dynamic context to demonstrate the robustness and stability of the proposed method. Furthermore, we produce additional experiments to present the potential of TrafficScope in cross-dataset scenarios.}
}


@inproceedings{DBLP:conf/kdd/ZhengZLCCZW25,
	author = {Bowen Zheng and
                  Junjie Zhang and
                  Hongyu Lu and
                  Yu Chen and
                  Ming Chen and
                  Wayne Xin Zhao and
                  Ji{-}Rong Wen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Enhancing Graph Contrastive Learning with Reliable and Informative
                  Augmentation for Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2101--2112},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709214},
	doi = {10.1145/3690624.3709214},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhengZLCCZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural network (GNN) has been a powerful approach in collaborative filtering (CF) due to its ability to model high-order user-item relationships. Recently, to alleviate the data sparsity and enhance representation learning, many efforts have been conducted to integrate contrastive learning (CL) with GNNs. Despite the promising improvements, the contrastive view generation based on structure and representation perturbations in existing methods potentially disrupts the collaborative information in contrastive views, resulting in limited effectiveness of positive alignment. To overcome this issue, we propose CoGCL, a novel framework that aims to enhance graph contrastive learning by constructing contrastive views with stronger collaborative information via discrete codes. The core idea is to map users and items into discrete codes rich in collaborative information for reliable and informative contrastive view generation. To this end, we initially introduce a multi-level vector quantizer in an end-to-end manner to quantize user and item representations into discrete codes. Based on these discrete codes, we enhance the collaborative information of contrastive views by considering neighborhood structure and semantic relevance respectively. For neighborhood structure, we propose virtual neighbor augmentation by treating discrete codes as virtual neighbors, which expands an observed user-item interaction into multiple edges involving discrete codes. Regarding semantic relevance, we identify similar users/items based on shared discrete codes and interaction targets to generate the semantically relevant view. Through these strategies, we construct contrastive views with stronger collaborative information and develop a triple-view graph contrastive learning approach. Extensive experiments on four public datasets demonstrate the effectiveness of our proposed approach. Moreover, detailed analyses highlight our contribution in enhancing graph CL for recommendation. Our code is available at https://github.com/RUCAIBox/CoGCL.}
}


@inproceedings{DBLP:conf/kdd/ZhouLZZZLG25,
	author = {Chuan Zhou and
                  Yaxuan Li and
                  Chunyuan Zheng and
                  Haiteng Zhang and
                  Min Zhang and
                  Haoxuan Li and
                  Mingming Gong},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {A Two-Stage Pretraining-Finetuning Framework for Treatment Effect
                  Estimation with Unmeasured Confounding},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2113--2123},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709161},
	doi = {10.1145/3690624.3709161},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhouLZZZLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating the conditional average treatment effect (CATE) from observational data plays a crucial role in areas such as e-commerce, healthcare, and economics. Existing studies mainly rely on the strong ignorability assumption that there are no unmeasured confounders, whose presence cannot be tested from observational data and can invalidate any causal conclusion. In contrast, data collected from randomized controlled trials (RCT) do not suffer from confounding, but are usually limited by a small sample size. In this paper, we propose a two-stage pretraining-finetuning (TSPF) framework using both large-scale observational data and small-scale RCT data to estimate the CATE in the presence of unmeasured confounding. In the first stage, a foundational representation of covariates is trained to estimate counterfactual outcomes through large-scale observational data. In the second stage, we propose to train an augmented representation of the covariates, which is concatenated to the foundational representation obtained in the first stage to adjust for the unmeasured confounding. To avoid overfitting caused by the small-scale RCT data in the second stage, we further propose a partial parameter initialization approach, rather than training a separate network. The superiority of our approach is validated on two public datasets with extensive experiments. The code is available at https://github.com/zhouchuanCN/KDD25-TSPF.}
}


@inproceedings{DBLP:conf/kdd/ZhouY0L25,
	author = {Renjie Zhou and
                  Haoran Ye and
                  Jian Wan and
                  Yong Liao},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{HRSTORY:} Historical News Review Based Online Story Discovery},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2124--2134},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709198},
	doi = {10.1145/3690624.3709198},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhouY0L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Story discovery on news streams can help people quickly find story from vast amounts of news, improving the efficiency of information acquisition. Recent online story discovery methods encode text topics and then cluster articles into stories based on similarity. However, the results obtained by these methods are one-time, and clustered news cannot adaptively update in a continuous news stream. Additionally, the inadequate quality of article encoding and the presence of noise data deteriorate the performance of story discovery. To this end, we propose HRSTORY for online story discovery on news streams, which employs a historical news review method to enable news to continuously adapt to the latest environment in the stream data and make corrections and updates. Furthermore, HRSTORY captures better article embeddings through modeling multi-layer relational dependencies within the text. By using sentence-level noise masking, HRSTORY improves the relevance of news article representation to core topics and reduces the interference of noise data. Experiments on real news datasets show that HRSTORY outperforms the state-of-the-art algorithms in unsupervised online story discovery performance.}
}


@inproceedings{DBLP:conf/kdd/ZhouS00J25,
	author = {Silin Zhou and
                  Shuo Shang and
                  Lisi Chen and
                  Peng Han and
                  Christian S. Jensen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Grid and Road Expressions Are Complementary for Trajectory Representation
                  Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2135--2146},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709272},
	doi = {10.1145/3690624.3709272},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhouS00J25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory representation learning (TRL) maps trajectories to vectors that can be used for many downstream tasks. Existing TRL methods use either  grid trajectories,  capturing movement in free space, or  road trajectories,  capturing movement in a road network, as input. We observe that the two types of trajectories are complementary, providing either region and location information or providing road structure and movement regularity. Therefore, we propose a novel multimodal TRL method, dubbed  GREEN,  to jointly utilize  G rid and  R oad trajectory  E xpressions for  E ffective representatio N  learning. In particular, we transform raw GPS trajectories into both grid and road trajectories and tailor  two encoders  to capture their respective information. To align the two encoders such that they complement each other, we adopt a  contrastive loss  to encourage them to produce similar embeddings for the same raw trajectory and design a  mask language model  (MLM) loss to use grid trajectories to help reconstruct masked road trajectories. To learn the final trajectory representation, a  dual-modal interactor  is used to fuse the outputs of the two encoders via cross-attention. We compare GREEN with 7 state-of-the-art TRL methods for 3 downstream tasks, finding that GREEN consistently outperforms all baselines and improves the accuracy of the best-performing baseline by an average of 15.99%. Code and data are available at https://github.com/slzhou-xy/GREEN.}
}


@inproceedings{DBLP:conf/kdd/ZhouL25,
	author = {Yu Zhou and
                  Bingyan Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{BTFL:} {A} Bayesian-based Test-Time Generalization Method for Internal
                  and External Data Distributions in Federated learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2147--2158},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709309},
	doi = {10.1145/3690624.3709309},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhouL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables multiple clients to collaboratively develop a global model while maintaining data privacy. However, online FL deployment faces challenges due to distribution shifts and evolving test samples. Personalized Federated Learning (PFL) tailors the global model to individual client distributions, but struggles with Out-Of-Distribution (OOD) samples during testing, leading to performance degradation. In real-world scenarios, balancing personalization and generalization during online testing is crucial and existing methods primarily focus on training-phase generalization. To address the test-time trade-off, we introduce a new scenario: Test-time Generalization for Internal and External Distributions in Federated Learning (TGFL), which evaluates adaptability under Internal Distribution (IND) and External Distribution (EXD). We propose  BTFL,  a Bayesian-based test-time generalization method for TGFL, which balances generalization and personalization at the sample level during testing. BTFL employs a two-head architecture to store local and global knowledge, interpolating predictions via a dual-Bayesian framework that considers both historical test data and current sample characteristics with theoretical guarantee and faster speed. Our experiments demonstrate that BTFL achieves improved performance across various datasets and models with less time cost. The source codes are made publicly available at https://github.com/ZhouYuCS/BTFL.}
}


@inproceedings{DBLP:conf/kdd/0002000LQ25,
	author = {Jiapeng Zhu and
                  Zichen Ding and
                  Jianxiang Yu and
                  Jiaqi Tan and
                  Xiang Li and
                  Weining Qian},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{RELIEF:} Reinforcement Learning Empowered Graph Feature Prompt Tuning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2159--2170},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709252},
	doi = {10.1145/3690624.3709252},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0002000LQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of the "pre-train, prompt\'\' paradigm has recently extended its generalization ability and data efficiency to graph representation learning, following its achievements in Natural Language Processing (NLP). Initial graph prompt tuning approaches tailored specialized prompting functions for Graph Neural Network (GNN) models pre-trained with specific strategies, such as edge prediction, thus limiting their applicability. In contrast, another pioneering line of research has explored universal prompting via adding prompts to the input graph\'s feature space, thereby removing the reliance on specific pre-training strategies. However, the necessity to add feature prompts to all nodes remains an open question. Motivated by findings from prompt tuning research in the NLP domain, which suggest that highly capable pre-trained models need less conditioning signal to achieve desired behaviors, we advocate for strategically incorporating necessary and lightweight feature prompts to certain graph nodes to enhance downstream task performance. This introduces a combinatorial optimization problem, requiring a policy to decide 1) which nodes to prompt and 2) what specific feature prompts to attach. We then address the problem by framing the prompt incorporation process as a sequential decision-making problem and propose our method, RELIEF, which employs Reinforcement Learning (RL) to optimize it. At each step, the RL agent selects a node (discrete action) and determines the prompt content (continuous action), aiming to maximize cumulative performance gain. Extensive experiments on graph and node-level tasks with various pre-training strategies in few-shot scenarios demonstrate that our RELIEF outperforms fine-tuning and other prompt-based approaches in classification performance and data efficiency. The code is available at https://github.com/JasonZhujp/RELIEF.}
}


@inproceedings{DBLP:conf/kdd/ZhuLCMSZZX0025,
	author = {Ruitao Zhu and
                  Yangsu Liu and
                  Dagui Chen and
                  Zhenjia Ma and
                  Chufeng Shi and
                  Zhenzhe Zheng and
                  Jie Zhang and
                  Jian Xu and
                  Bo Zheng and
                  Fan Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Contextual Generative Auction with Permutation-level Externalities
                  for Online Advertising},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2171--2181},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709313},
	doi = {10.1145/3690624.3709313},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhuLCMSZZX0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online advertising has become a core revenue driver for internet industry, with ad auctions playing a crucial role in ensuring platform revenue and advertiser incentives. Classical auction mechanisms, such as GSP, rely on the independent CTR assumption and fail to account for the interplay among the displayed items, also called as externalities in economics. Recent advancements in learning-based auctions enable the encoding of high-dimensional contextual features. However, existing methods are limited by the ''prediction-before-allocation'' design paradigm, which models set-level externalities within candidate ads and fails to consider the context of the final allocation, leading to suboptimal results. In this work, we introduce Contextual Generative Auction (CGA), a novel framework that incorporates permutation-level externalities in multi-slot ad auctions. Built on the structure of our theoretically derived optimal auction, CGA decouples the optimization of allocation and payment. We construct an autoregressive generative model for allocation, and reformulate incentive compatibility (IC) constraint into minimizing ex-post regret that supports gradient computation, enabling end-to-end learning of the optimal payment rule. Extensive offline and online experiments demonstrate that CGA significantly enhances platform revenue and CTR compared to existing methods, and effectively approximates the optimal auction with nearly maximal revenue and minimal regret.}
}


@inproceedings{DBLP:conf/kdd/ZhuZ25,
	author = {Zhangchi Zhu and
                  Wei Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Exploring Feature-based Knowledge Distillation for Recommender System:
                  {A} Frequency Perspective},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2182--2193},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709248},
	doi = {10.1145/3690624.3709248},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhuZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we analyze the feature-based knowledge distillation for recommendation from the frequency perspective. By defining knowledge as different frequency components of the features, we theoretically demonstrate that regular feature-based knowledge distillation is equivalent to equally minimizing losses on all knowledge and further analyze how this equal loss weight allocation method leads to important knowledge being overlooked. In light of this, we propose to emphasize important knowledge by redistributing knowledge weights. Furthermore, we propose FreqD, a lightweight knowledge reweighting method, to avoid the computational cost of calculating losses on each knowledge. Extensive experiments demonstrate that FreqD consistently and significantly outperforms state-of-the-art knowledge distillation methods for recommender systems. Our code is available at https://github.com/woriazzc/KDs.}
}


@inproceedings{DBLP:conf/kdd/CaiGYAXLRXHYLGW25,
	author = {Xufeng Cai and
                  Ziwei Guan and
                  Lei Yuan and
                  Ali Selman Aydin and
                  Tengyu Xu and
                  Boying Liu and
                  Wenbo Ren and
                  Renkai Xiang and
                  Songyi He and
                  Haichuan Yang and
                  Serena Li and
                  Mingze Gao and
                  Yue Weng and
                  Ji Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {HyperZero: {A} Customized End-to-End Auto-Tuning System for Recommendation
                  with Hourly Feedback},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2194--2203},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709409},
	doi = {10.1145/3690624.3709409},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/CaiGYAXLRXHYLGW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern recommendation systems can be broadly divided into two key stages: the ranking stage, where the system predicts various user engagements (e.g., click-through rate, like rate, follow rate, watch time), and the value model stage, which aggregates these predictive scores through a function (e.g., a linear combination defined by a weight vector) to measure the value of each content by a single numerical score. Both stages play roughly equally important roles in real industrial systems; however, how to optimize the model weights for the second stage still lacks systematic study. This paper focuses on optimizing the second stage through auto-tuning technology. Although general auto-tuning systems and solutions - both from established production practices and open-source solutions - can address this problem, they typically require weeks or even months to identify a feasible solution. Such prolonged tuning processes are unacceptable in production environments for recommendation systems, as suboptimal value models can severely degrade user experience. An effective auto-tuning solution is required to identify a viable model within 2-3 days, rather than the extended timelines typically associated with existing approaches. In this paper, we introduce a practical auto-tuning system named H yper Z ero  that addresses these time constraints while effectively solving the unique challenges inherent in modern recommendation systems. Moreover, this framework has the potential to be expanded to broader tuning tasks within recommendation systems.}
}


@inproceedings{DBLP:conf/kdd/ChenL0WZQZW0S25,
	author = {Kaixuan Chen and
                  Wei Luo and
                  Shunyu Liu and
                  Yaoquan Wei and
                  Yihe Zhou and
                  Yunpeng Qing and
                  Quan Zhang and
                  Yong Wang and
                  Jie Song and
                  Mingli Song},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Powerformer: {A} Section-adaptive Transformer for Power Flow Adjustment},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2204--2215},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709433},
	doi = {10.1145/3690624.3709433},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ChenL0WZQZW0S25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present a novel transformer architecture tailored for learning robust power system state representations, which strives to optimize power dispatch for the power flow adjustment across different transmission sections. Specifically, our proposed approach, named  Powerformer,  develops a dedicated section-adaptive attention mechanism, separating itself from the self-attention employed in conventional transformers. This mechanism effectively integrates power system states with transmission section information, which facilitates the development of robust state representations. Furthermore, by considering the graph topology of power system and the electrical attributes of bus nodes, we introduce two customized strategies to further enhance the expressiveness: graph neural network propagation and multi-factor attention mechanism. Extensive evaluations are conducted on three power system scenarios, including the IEEE 118-bus system, a realistic China 300-bus system, and a large-scale European system with 9241 buses, where Powerformer demonstrates its superior performance over several popular baseline methods. The code is available at: https://github.com/Cra2yDavid/Powerformer}
}


@inproceedings{DBLP:conf/kdd/0001HWZLYXZD25,
	author = {Zhijian Duan and
                  Yusen Huo and
                  Tianyu Wang and
                  Zhilin Zhang and
                  Yeshu Li and
                  Chuan Yu and
                  Jian Xu and
                  Bo Zheng and
                  Xiaotie Deng},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {An Adaptable Budget Planner for Enhancing Budget-Constrained Auto-Bidding
                  in Online Advertising},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2216--2225},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709414},
	doi = {10.1145/3690624.3709414},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0001HWZLYXZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In online advertising, advertisers commonly utilize auto-bidding services to bid for impression opportunities. A typical objective of the auto-bidder is to optimize the advertiser's cumulative value of winning impressions within specified budget constraints. However, such a problem is challenging due to the complex bidding environment faced by diverse advertisers. To address this challenge, we introduce ABPlanner, a few-shot adaptable budget planner designed to improve budget-constrained auto-bidding. ABPlanner is based on a hierarchical bidding framework that decomposes the bidding process into shorter, manageable stages. Within this framework, ABPlanner allocates the budget across all stages, allowing a low-level auto-bidder to bids based on the budget allocation plan. The adaptability of ABPlanner is achieved through a sequential decision-making approach, inspired by in-context reinforcement learning. For each advertiser, ABPlanner adjusts the budget allocation plan episode by episode, using data from previous episodes as prompt for current decisions. This enables ABPlanner to quickly adapt to different advertisers with few-shot data, providing a sample-efficient solution. Extensive simulation experiments and real-world A/B testing validate the effectiveness of ABPlanner, demonstrating its capability to enhance the cumulative value achieved by auto-bidders.}
}


@inproceedings{DBLP:conf/kdd/DupretSGZYCMGLB25,
	author = {Georges Dupret and
                  Konstantin Sozinov and
                  Carmen Barcena Gonzalez and
                  Ziggy Zacks and
                  Amber Yuan and
                  Ben Carterette and
                  Manuel Mai and
                  Andrey Gatash and
                  Gwo Liang Lien and
                  Shubham Bansal and
                  Roberto Sanchis{-}Ojeda and
                  Mounia Lalmas},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {ForTune: Running Offline Scenarios to Estimate Impact on Business
                  Metrics},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2226--2234},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709431},
	doi = {10.1145/3690624.3709431},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/DupretSGZYCMGLB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Making ideal decisions as a product leader in a web-facing company is incredibly challenging. Beyond navigating the ambiguity of customer satisfaction and achieving business goals, leaders must also ensure their products and services remain relevant, desirable, and profitable. Data and experimentation are crucial for testing product hypotheses and informing decisions. Online controlled experiments, such as A/B testing, can provide highly reliable data to support decisions. However, these experiments can be time-consuming and costly, particularly when assessing impacts on key business metrics like retention or long-term value. Offline experimentation allows for rapid iteration and testing but often lacks the same level of confidence and clarity regarding business metrics impact. To address this, we introduce a novel, lightweight, and flexible approach called  scenario analysis . This method aims to support product leaders' decisions by using user data and estimates of business metrics. While it cannot fully replace online experiments, it offers valuable insights into trade-offs involved in growth or consumption shifts, estimates trends in long-term outcomes like retention, and can generate hypotheses about relationships between metrics at scale. We implemented scenario analysis in a tool named ForTune. We conducted experiments with this tool using a publicly available dataset and reported the results of experiments carried out by Spotify, a large audio streaming service, using ForTune in production. In both cases, the tool reasonably predicted the outcomes of controlled experiments, provided that features were carefully chosen. We demonstrate how this method was used to make strategic decisions regarding the impact of prioritizing one type of content over another at Spotify.}
}


@inproceedings{DBLP:conf/kdd/FengDLBB25,
	author = {Qing Feng and
                  Samuel Daulton and
                  Benjamin Letham and
                  Maximilian Balandat and
                  Eytan Bakshy},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Experimenting, Fast and Slow: Bayesian Optimization of Long-term Outcomes
                  with Online Experiments},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2235--2246},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709419},
	doi = {10.1145/3690624.3709419},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/FengDLBB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online experiments in internet systems, also known as A/B tests, are used for a wide range of system tuning problems, such as optimizing recommender system ranking policies and learning adaptive streaming controllers. Decision-makers generally wish to optimize for long-term treatment effects of the system changes, which often requires running experiments for a long time as short-term measurements can be misleading due to non-stationarity in treatment effects over time. The sequential experimentation strategies---which typically involve several iterations---can be prohibitively long in such cases. We describe a novel approach that combines fast experiments (e.g., biased experiments run only for a few hours or days) and/or offline proxies (e.g., off-policy evaluation) with long-running, slow experiments to perform sequential, Bayesian optimization over large action spaces in a short amount of time.}
}


@inproceedings{DBLP:conf/kdd/GeXCCLP025,
	author = {Lin Ge and
                  Yang Xu and
                  Jianing Chu and
                  David Cramer and
                  Fuhong Li and
                  Kelly Paulson and
                  Rui Song},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Multi-Task Combinatorial Bandits for Budget Allocation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2247--2258},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709434},
	doi = {10.1145/3690624.3709434},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GeXCCLP025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today's top advertisers typically manage hundreds of campaigns simultaneously and consistently launch new ones throughout the year. A crucial challenge for marketing managers is determining the optimal allocation of limited budgets across various ad lines in each campaign to maximize cumulative returns, especially given the huge uncertainty in return outcomes. In this paper, we propose to formulate budget allocation as a multi-task combinatorial bandit problem and introduce a novel online budget allocation system. The proposed system: i) integrates a Bayesian hierarchical model to intelligently utilize the metadata of campaigns and ad lines and budget size, ensuring efficient information sharing; ii) provides the flexibility to incorporate diverse modeling techniques such as Linear Regression, Gaussian Processes, and Neural Networks, catering to diverse environmental complexities; and iii) employs the Thompson sampling technique to strike a balance between exploration and exploitation. Through extensive empirical evaluations with both synthetic data and real-world data from Amazon campaigns, our system demonstrates robustness and adaptability, effectively maximizing the overall cumulative returns. A Python implementation of the proposed procedure is available at https://anonymous.4open.science/r/MCMAB.}
}


@inproceedings{DBLP:conf/kdd/GhoshJHC25,
	author = {Medhasree Ghosh and
                  Chirag Dinesh Jain and
                  Raju Halder and
                  Joydeep Chandra},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{TEMPER:} Capturing Consistent and Fluctuating TEMPoral User Behaviour
                  for EtheReum Phishing Scam Detection},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2259--2270},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709399},
	doi = {10.1145/3690624.3709399},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GhoshJHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Phishing scams on the Ethereum network have become a serious threat, especially with the influx of new users into the cryptocurrency market. Current detection methods are mainly focused on long-term consistent transaction patterns with smooth temporal dynamics. However, these methods often struggle to differentiate between phishing and non-phishing users, whose behaviours may appear deceptively similar. Additionally, they face challenges such as network sparsity and data leakage, leading to significant performance limitations. To address these issues, we introduce TEMPER, a novel sequential learning framework designed to jointly capture the subtle distinctions between long- and short-term user behaviours and their correlations to provide more comprehensive insights. TEMPER effectively generates distinguishable user embeddings, enabling the accurate identification of phishing users. Unlike previous approaches, TEMPER mitigates data leakage through a novel sequential transaction sampling algorithm and addresses network sparsity with short-term temporal learning. Through extensive experimentation on three real-world Ethereum datasets, TEMPER demonstrates its efficacy by achieving a 3-4% improvement in the F1-Score compared to existing baseline models, representing a significant advancement in Ethereum phishing user detection.}
}


@inproceedings{DBLP:conf/kdd/GuoZZZ025,
	author = {Yue Guo and
                  Wentao Zhang and
                  Xiaojun Zhang and
                  Vincent W. Zheng and
                  Yi Yang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Efficient Multi-Expert Tabular Language Model for Banking},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2271--2281},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709400},
	doi = {10.1145/3690624.3709400},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/GuoZZZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pre-training large Tabular Language Models (TaLMs) on tabular data has shown effectiveness for table understanding tasks. However, training proprietary large TaLMs on a company's private databases requires substantial computational resources. This paper presents an efficient multi-expert TaLM architecture and training method tailored for multi-domain databases and modest infrastructure. This architecture leverages a divide-and-conquer pretraining approach and a sparsely activated fine-tuning paradigm to reduce computation. Using this architecture, we pre-train and fine-tune a TaLM with 10 billion parameters on a banking database under simple computational infrastructures. We apply our TaLM to support various important banking applications, including risk assessment, information prediction, and profit assessment. Compared with previous baselines, our model achieves +29.3% in  precision@0.6 % on risk assessment and +16.5% in accuracy on information prediction, showing great effectiveness and profitability of our model. This model is successfully deployed in WeBank and now supports many real business scenarios.}
}


@inproceedings{DBLP:conf/kdd/HuZWDLQL25,
	author = {Kun Hu and
                  Shumin Zhang and
                  Lixia Wu and
                  Yongjun Dai and
                  Minfang Lu and
                  Yuting Qiang and
                  Minglong Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Learning Adaptive Reserve Price in Display Advertising},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2282--2291},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709439},
	doi = {10.1145/3690624.3709439},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/HuZWDLQL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-Time Bidding  (RTB) is a trading mechanism that allocates advertising (ad) requests through online auctions. Participants in these auctions typically include an ad exchange (AdX) and several  demand-side platforms  (DSPs). When an RTB auction begins, the AdX first establishes the reserve price set by publishers as the starting bid, after which the DSPs bid to compete for potential ad impressions. The reserve price strategy is crucial to the ad revenue of publishers; however, due to the strategic and dynamic bidding behavior of DSPs, optimizing the reserve price presents a significant challenge. In this work, we report a novel adaptive reserve price strategy based on reinforcement learning (RL). In our scheme, value bucket identification is leveraged to estimate the intrinsic values of ad inventories. Following this estimation, specialized reward functions are utilized to generate informative reward signals for RL models. Furthermore, we study the issue of risk management on the publisher side and develop a risk-aware instantiation to model risk tendency, considering both empirical expert knowledge and real-time trading conditions. Extensive experiments using real-world datasets collected from operational environments have demonstrated the effectiveness of the proposed method.}
}


@inproceedings{DBLP:conf/kdd/JiangLC25,
	author = {Yanru Jiang and
                  Siyu Liang and
                  Junwon Choi},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Synthetic Survey Data Generation and Evaluation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2292--2302},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709421},
	doi = {10.1145/3690624.3709421},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/JiangLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Survey data are common and invaluable in social science research for understanding population processes and supporting policymaking and planning. Depending on the nature and scale, survey data sharing comes with privacy risks, and data collectors and agencies are constrained by disclosure permissions, limiting usage across research groups and institutes. Previous methods for synthetic data generation and deidentification may not entirely prevent information disclosures, or they may sacrifice data quality and granularity. Using a large-scale national voter file at both national and state levels, this paper introduces an end-to-end pipeline to streamline synthetic data generation and evaluation for survey researchers. This study selects four generative approaches based on different statistical assumptions: the regression-based Synthpop, the generative deep learning-based CTGAN and TVAE, and the large language model-based REaLDTabFormer, and compares them to the baseline synthetic minority oversampling technique (SMOTE). We consider three key dimensions of evaluation  (utility, fidelity,  and  privacy)  to highlight the strengths and weaknesses of each approach, and systematically evaluate across various datasets and training sizes. The results reveal that Synthpop is optimized for general utility (i.e., fidelity), while TVAE excels in downstream applications (i.e., target-specific utility) but compromises on general utility and potentially risks data overfitting. REaLDTabFormer demonstrates a balanced performance in both general and target-specific utility, whereas CTGAN offers the best privacy protection. We recommend that future researchers select a generative method by considering the trade-offs between performance across various evaluation dimensions, training size, data type, and computational infrastructure.}
}


@inproceedings{DBLP:conf/kdd/JinTLYHLL25,
	author = {Zhipeng Jin and
                  Wen Tao and
                  Yafei Li and
                  Yi Yang and
                  Cong Han and
                  Shuanglong Li and
                  Lin Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Large Vison-Language Foundation Model in Baidu {AIGC} Image Advertising},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2303--2312},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709401},
	doi = {10.1145/3690624.3709401},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/JinTLYHLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in generative artificial intelligence have revolutionized information retrieval and content generation, opening up new opportunities for the e-commerce industry. Alignment learning between small models and parallel corpora cannot meet current needs. The success of ChatGPT demonstrates that large models need to first establish a fundamental understanding, and then utilize high-quality corpora for generation. Having a large model foundation is indispensable. In this paper, we establish a fundamental 10B multimodal model foundation for multimodal generation tasks and propose a scene-based alignment learning approach called conditional sample supervised fine-tuning for downstream generation tasks. Meanwhile, diffusion models are known to be vulnerable to outliers in training data. To address this, we utilize an alternative diffusion loss function that preserves the high quality of generated data like the original squared L2 loss while being robust to outliers.In practical test sets, the multimodal foundation fully demonstrates its alignment and comprehension abilities for graphic and textual content. Additionally, conditional fine-tuning and the design of the loss function significantly enhance the quality of generated content. The quality rate of images has increased by 34.3 percentage points, and prompt control has improved by 19.8 percentage points. The application of our framework in Baidu Search Ads has led to significant revenue growth. For instance, ads with generated image creatives have achieved a 29% higher click-through rate (CTR), resulting in a daily consumption of 3 million yuan.}
}


@inproceedings{DBLP:conf/kdd/KastryulinKSLKT25,
	author = {Sergey Kastryulin and
                  Artem Konev and
                  Alexander Shishenya and
                  Eugene Lyapustin and
                  Artem Khurshudov and
                  Alexander Tselousov and
                  Nikita Vinokurov and
                  Denis Kuznedelev and
                  Alexander Markovich and
                  Grigoriy Livshits and
                  Alexey Kirillov and
                  Anastasiia Tabisheva and
                  Liubov Chubarova and
                  Marina Kaminskaia and
                  Alexander Ustyuzhanin and
                  Artemii Shvetsov and
                  Daniil Shlenskii and
                  Valerii Startsev and
                  Dmitrii Kornilov and
                  Mikhail Romanov and
                  Dmitry Baranchuk and
                  Artem Babenko and
                  Sergei Ovcharenko and
                  Valentin Khrulkov},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {YaART: Yet Another {ART} Rendering Technology},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2313--2324},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709404},
	doi = {10.1145/3690624.3709404},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/KastryulinKSLKT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier. This study introduces  YaART , a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF). During the development of  YaART , we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before. In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice. Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training. From the quality perspective,  YaART  is consistently preferred by users over many existing state-of-the-art models. The proposed system is integrated in five commercial products.  YaART  is also distributed for B2C clients via mobile and web applications and for B2B clients via API.}
}


@inproceedings{DBLP:conf/kdd/MorgiaMM25,
	author = {Massimo La Morgia and
                  Alessandro Mei and
                  Alberto Maria Mongardini},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {TGDataset: Collecting and Exploring the Largest Telegram Channels
                  Dataset},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2325--2334},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709397},
	doi = {10.1145/3690624.3709397},
	timestamp = {Thu, 01 May 2025 20:24:58 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/MorgiaMM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Telegram is a widely adopted instant messaging platform. It has become worldwide popular because of its emphasis on privacy and its social network features such as channels-virtual rooms in which only the admins can post and broadcast messages to all the subscribers. Channels are used to deliver live updates ( e.g. , weather alerts) and content to a large audience ( e.g. , COVID-19 announcements) but unfortunately also to disseminate radical ideologies and coordinate attacks such as the Capitol Hill riot. This paper introduces the TGDataset, the most extensive publicly available collection of Telegram channels, comprising 120,979 channels and over 400 million messages. We outline the data collection process and provide a comprehensive overview of the data set. Using language detection, we identify the predominant languages within the dataset. We then focus on English channels, employing topic modeling to analyze the subjects they cover. Finally, we discuss some use cases in which our dataset can be instrumental in understanding the Telegram ecosystem and studying the diffusion of questionable news. Alongside the raw dataset, we release the scripts used in our analysis, as well as a list of channels associated with a novel conspiracy theory known as Sabmyk.}
}


@inproceedings{DBLP:conf/kdd/Lai000025,
	author = {Siqi Lai and
                  Zhao Xu and
                  Weijia Zhang and
                  Hao Liu and
                  Hui Xiong},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {LLMLight: Large Language Models as Traffic Signal Control Agents},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2335--2346},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709379},
	doi = {10.1145/3690624.3709379},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Lai000025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional TSC methods, primarily based on transportation engineering and reinforcement learning (RL), often struggle with generalization abilities across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments conducted on ten real-world and synthetic datasets, along with evaluations by fifteen human experts, demonstrate the exceptional effectiveness, generalization ability, and interpretability of LLMLight with LightGPT, outperforming nine baseline methods and ten advanced LLMs. Our project is available at https://github.com/usail-hkust/LLMTSCS.}
}


@inproceedings{DBLP:conf/kdd/0002LC025,
	author = {Seungyeon Lee and
                  Ruoqi Liu and
                  Feixiong Cheng and
                  Ping Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {A Deep Subgrouping Framework for Precision Drug Repurposing via Emulating
                  Clinical Trials on Real-world Patient Data},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2347--2358},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709418},
	doi = {10.1145/3690624.3709418},
	timestamp = {Thu, 01 May 2025 20:24:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0002LC025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Drug repurposing identifies new therapeutic uses for existing drugs, reducing the time and costs compared to traditional  de novo  drug discovery. Most existing drug repurposing studies using real-world patient data often treat the entire population as homogeneous, ignoring the heterogeneity of treatment responses across patient subgroups. This approach may overlook promising drugs that benefit specific subgroups but lack notable treatment effects across the entire population, potentially limiting the number of repurposable candidates identified. To address this, we introduce  STEDR , a novel drug repurposing framework that integrates subgroup analysis with treatment effect estimation. Our approach first identifies repurposing candidates by emulating multiple clinical trials on real-world patient data and then characterizes patient subgroups by learning subgroup-specific treatment effects. We deploy  STEDR  to Alzheimer's Disease (AD), a condition with few approved drugs and known heterogeneity in treatment responses. We emulate trials for over one thousand medications on a large-scale real-world database covering over 8 million patients, identifying 14 drug candidates with beneficial effects to AD in characterized subgroups. Experiments demonstrate  STEDR  superior capability in identifying repurposing candidates compared to existing approaches. Additionally, our method can characterize clinically relevant patient subgroups associated with important AD-related risk factors, paving the way for precision drug repurposing.}
}


@inproceedings{DBLP:conf/kdd/LenceGFHSZP25,
	author = {Alex Lence and
                  Federica Granese and
                  Ahmad Fall and
                  Blaise Hanczar and
                  Joe{-}Elie Salem and
                  Jean{-}Daniel Zucker and
                  Edi Prifti},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {ECGrecover: {A} Deep Learning Approach for Electrocardiogram Signal
                  Completion},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2359--2370},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709405},
	doi = {10.1145/3690624.3709405},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LenceGFHSZP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we address the challenge of reconstructing the complete 12-lead ECG signal from its incomplete parts. We focus on two main scenarios: (i) reconstructing missing signal segments within an ECG lead and (ii) recovering entire leads from signal in another unique lead. Two emerging clinical applications emphasize the relevance of our work. The first is the increasing need to digitize paper-stored ECGs for utilization in AI-based applications, often limited to digital 12 lead 10s ECGs. The second is the widespread use of wearable devices that record ECGs but typically capture only one or a few leads. In both cases, a non-negligible amount of information is lost or not recorded. Our approach aims to recover this missing signal. We propose ECGrecover, a U-Net neural network model trained on a novel composite objective function to address the reconstruction problem. This function incorporates both spatial and temporal features of the ECG by combining the distance in amplitude and sycnhronization through time between the reconstructed and the real digital signals. We used real-life ECG datasets and through comprehensive assessments compared ECGrecover with three state-of-the-art methods based on generative adversarial networks (EKGAN, Pix2Pix) as well as the CopyPaste strategy. The results demonstrated that ECGrecover consistently outperformed state-of-the-art methods in standard distortion metrics as well as in preserving critical ECG characteristics, particularly the P, QRS, and T wave coordinates.}
}


@inproceedings{DBLP:conf/kdd/LiD0LCD25,
	author = {Lichi Li and
                  Zainul Abi Din and
                  Zhen Tan and
                  Sam London and
                  Tianlong Chen and
                  Ajay H. Daptardar},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {\emph{MerRec: } {A} Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer
                  Recommendation Systems},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2371--2382},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709394},
	doi = {10.1145/3690624.3709394},
	timestamp = {Mon, 12 May 2025 08:59:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiD0LCD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the evolving e-commerce field, recommendation systems crucially shape user experience and engagement. The rise of Consumer-to-Consumer (C2C) recommendation systems, noted for their flexibility and ease of access for customer vendors, marks a significant trend. However, the academic focus remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by the limited C2C recommendation datasets that lack in item attributes, user diversity, and scale. The intricacy of C2C recommendation systems is further accentuated by the dual roles users assume as both sellers and buyers, introducing a spectrum of less uniform and varied inputs. Addressing this, we introduce MerRec, the first large-scale dataset specifically for C2C recommendations, sourced from the Mercari e-commerce platform, covering millions of users and products over 6 months in 2023.  MerRec  not only includes standard features such as user_id, item_id, and session_id, but also unique elements like timestamped action types, product taxonomy, and textual product attributes, offering a comprehensive dataset for research. This dataset, extensively evaluated across three recommendation tasks, establishes a new benchmark for the development of advanced recommendation algorithms in real-world scenarios, bridging the gap between academia and industry and propelling the study of C2C recommendations. Experiment code (https://github.com/mercari/mercari-ml-merrec-pub-us) and dataset (https://huggingface.co/datasets/mercari-us/merrec) are released.}
}


@inproceedings{DBLP:conf/kdd/LiLYJ25,
	author = {Manwei Li and
                  Detao Lv and
                  Yao Yu and
                  Zihao Jiao},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Contrastive Learning for Inventory Add Prediction at Fliggy},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2383--2392},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709384},
	doi = {10.1145/3690624.3709384},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiLYJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online Travel Platforms (OTPs) serve as crucial bridges between hotels and users, hotel staff can synchronize room inventory information with OTPs through manual and auto modes. In the manual mode, the hotel staff must manually maintain the inventory information on the OTPs. This mode often leads to the "inventory synchronization delay\'\' phenomenon where OTPs show no availability while hotels still have available rooms, seriously affecting the competitiveness of OTPs and hotel sales. To address this issue, Fliggy uses inventory add prediction (IAP) to determine whether to add an inventory for the sold-out room type. However, in practice, accurate modeling of IAP faces significant challenges due to the data sparsity. In this paper, we propose a  C ontrastive  L earning framework for  I nventory  A dd  P rediction at Fliggy (CL4IAP), which consists of the  Joint Pay-Accept Prediction Module,  the  Data Augmentation Module,  and the  Contrastive Learning Module.  Specifically, the Joint Pay-Accept Prediction Module aims to predict the likelihood of generating an order and the hotel acceptance after adding an inventory. It also includes a specially designed correlation enhancement component that facilitates the expert prediction network\'s learning through knowledge transfer based on inter-task correlation. In the  Data Augmentation Module,  we design three novel data augmentation strategies for the first time based on the correlation and importance of features. In the  Contrastive Learning Module,  we design instance-level and cluster-level contrastive losses, which aim to minimize the distance between positive sample pairs and mitigate the negative impact of false negative sample pairs, respectively. Both offline and online experiments demonstrate the effectiveness of CL4IAP, and CL4IAP has been successfully deployed on Fliggy.}
}


@inproceedings{DBLP:conf/kdd/0006W00025,
	author = {Mingyuan Li and
                  Jiahao Wang and
                  Bo Du and
                  Jun Shen and
                  Qiang Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {FuzzyLight: {A} Robust Two-Stage Fuzzy Approach for Traffic Signal
                  Control Works in Real Cities},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2393--2404},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709393},
	doi = {10.1145/3690624.3709393},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0006W00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Effective traffic signal control (TSC) is crucial in mitigating urban congestion and reducing emissions. Recently, reinforcement learning (RL) has been the research trend for TSC. However, existing RL algorithms face several real-world challenges that hinder their practical deployment in TSC: (1) Sensor accuracy deteriorates with increased sensor detection range, and data transmission is prone to noise, potentially resulting in unsafe TSC decisions. (2) During the training of online RL, interactions with the environment could be unstable, potentially leading to inappropriate traffic signal phase (TSP) selection and traffic congestion. (3) Most current TSC algorithms focus only on TSP decisions, overlooking the critical aspect of phase duration, affecting safety and efficiency. To overcome these challenges, we propose a robust two-stage fuzzy approach called FuzzyLight, which integrates compressed sensing and RL for TSC deployment. FuzzyLight offers several key contributions: (1) It employs fuzzy logic and compressed sensing to address sensor noise and enhances the efficiency of TSP decisions. (2) It maintains stable performance during training and combines fuzzy logic with RL to generate precise phases. (3) It works in real cities across 22 intersections and demonstrates superior performance in both real-world and simulated environments. Experimental results indicate that FuzzyLight enhances traffic efficiency by 48% compared to expert-designed timings in the real world. Furthermore, it achieves state-of-the-art (SOTA) performance in simulated environments using six real-world datasets with transmission noise. The code and deployment video are available at the Github.}
}


@inproceedings{DBLP:conf/kdd/LiCHJHF25,
	author = {Ouxiang Li and
                  Jiayin Cai and
                  Yanbin Hao and
                  Xiaolong Jiang and
                  Yao Hu and
                  Fuli Feng},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Improving Synthetic Image Detection Towards Generalization: An Image
                  Transformation Perspective},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2405--2414},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709392},
	doi = {10.1145/3690624.3709392},
	timestamp = {Tue, 13 May 2025 07:31:05 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiCHJHF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With recent generative models facilitating photo-realistic image synthesis, the proliferation of synthetic images has also engendered certain negative impacts on social platforms, thereby raising an urgent imperative to develop effective detectors. Current synthetic image detection (SID) pipelines are primarily dedicated to crafting universal artifact features, accompanied by an oversight about SID training paradigm. In this paper, we re-examine the SID problem and identify two prevalent biases in current training paradigms,  i.e.,  weakened artifact features and overfitted artifact features. Meanwhile, we discover that the imaging mechanism of synthetic images contributes to heightened local correlations among pixels, suggesting that detectors should be equipped with local awareness. In this light, we propose SAFE, a lightweight and effective detector with three simple image transformations. Firstly, for weakened artifact features, we substitute the down-sampling operator with the crop operator in image pre-processing to help circumvent artifact distortion. Secondly, for overfitted artifact features, we include ColorJitter and RandomRotation as additional data augmentations, to help alleviate irrelevant biases from color discrepancies and semantic differences in limited training samples. Thirdly, for local awareness, we propose a patch-based random masking strategy tailored for SID, forcing the detector to focus on local regions at training. Comparative experiments are conducted on an open-world dataset, comprising synthetic images generated by  26 distinct generative models.  Our pipeline achieves a new state-of-the-art performance, with remarkable improvements of 4.5% in accuracy and 2.9% in average precision against existing methods. Our code is available at: https://github.com/Ouxiang-Li/SAFE.}
}


@inproceedings{DBLP:conf/kdd/LiXZB0LWKY25,
	author = {Yuchen Li and
                  Haoyi Xiong and
                  Yongqi Zhang and
                  Jiang Bian and
                  Tianhao Peng and
                  Xuhong Li and
                  Shuaiqiang Wang and
                  Linghe Kong and
                  Dawei Yin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {RankElectra: Semi-supervised Pre-training of Learning-to-Rank Electra
                  for Web-scale Search},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2415--2425},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709395},
	doi = {10.1145/3690624.3709395},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiXZB0LWKY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While representation learning has been used to boost the performance of Learning-to-Rank (LTR) models through distilling key features for webpage ranking, the weak supervision signals extracted from users' sparse click-through data lead to inadequate representation of query-webpage pairs for ranking score prediction. Recent studies in generative LTR pre-training demonstrate the feasibility of incorporating reconstruction loss for enhanced ranking score prediction. However, LTR is afterall a regression task and it might be reasonable to find an alternate route that pre-trains LTR models with discriminative losses. Following the success of Electra in representation learning for natural language processing (NLP), this work proposes  RankElectra  that pre-trains the LTR model as a discriminator module inside a generative learning framework. Specifically,  RankElectra  first structures sparsely-annotated query-webpage pairs into a bipartite graph, with query and webpage feature vectors as node types and ranking scores as the connecting edges, and then leverages positive and negative extension strategies to densify the graph by link predictions. Later, this work proposes a novel Electra module that pre-trains the LTR model as a discriminator module for node reconstruction tasks, where node features of selected edges would be randomly masked and reconstructed by a generator, and the discriminator learns to classify whether the reconstructed features are the original or replaced as well as perform correct ranking. Finally, the pre-trained discriminator module, rather than the generator, would be fine-tuned on the labeled graph. We carried out extensive offline and online evaluations using the real-world web traffic of Baidu search engine. The results show that  RankElectra  could significantly boost the ranking performance of Baidu Search compared with numbers of competitor systems.}
}


@inproceedings{DBLP:conf/kdd/LiuLLXYL25,
	author = {Bin Liu and
                  Yu Liu and
                  Zhiqian Li and
                  Jianghong Xiao and
                  Guosheng Yin and
                  Huazhen Lin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Automatic Radiotherapy Treatment Planning with Deep Functional Reinforcement
                  Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2426--2435},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709430},
	doi = {10.1145/3690624.3709430},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiuLLXYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intensity-modulated radiation therapy (IMRT) is a crucial radiotherapy technique, which is often formulated as an optimization problem. However, when the constraints are too restrictive to provide a feasible solution, human planners resort to relaxing the optimization parameters and re-evaluating the problem until an acceptable solution is obtained. However, this process is laborious and time-consuming, which has prompted attempts to automate radiotherapy through inverse planning studies using reinforcement learning. Unfortunately, these studies face two major limitations. First, a separate sub-network must be designed for each organ, rendering it difficult to apply to patients with an inconsistent number of structures. Second, the low signal-to-noise input and discrete action space result in low training efficiency. To address these issues, we propose an organ-sharing network that contains a functional embedding layer to extract curve features of the dose-volume histogram. It outputs continuous actions that can adjust the optimization parameters, thereby automating the radiotherapy planning process. The results from a cervical cancer dataset demonstrate the feasibility and efficiency of the proposed model in real-world radiotherapy. The code is available on https://github.com/Cissise/FatPIN.}
}


@inproceedings{DBLP:conf/kdd/LiuYF0LL0025,
	author = {Dugang Liu and
                  Chaohua Yang and
                  Yuwen Fu and
                  Xing Tang and
                  Gongfu Li and
                  Fuyuan Lyu and
                  Xiuqiang He and
                  Zhong Ming},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Scenario Shared Instance Modeling for Click-through Rate Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2436--2447},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709390},
	doi = {10.1145/3690624.3709390},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiuYF0LL0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-scenario recommendation (MSR) is a popular training paradigm in industrial platforms for uniformly integrating information from multiple scenarios and serving them simultaneously. A key challenge in MSR research is accurately identifying the commonalities and distinctive information between scenarios. Currently, most existing MSR methods focus on implicitly extracting this information from the architectural level. However, this continues to increase the complexity and training overhead of MSR. Furthermore, the custom components responsible for extracting implicit information in each MSR method are too dependent on the specific MSR architecture and are not easily reused in other methods. Given these challenges, we first show in a motivating experiment that it may be beneficial to explicitly select a reasonable set of shared instances that can affect parameter optimization in all scenarios during the training of MSR, i.e., to explicitly obtain the critical information required for MSR from the data level. Then, this paper proposes SSIM with an adaptive selection network. Specifically, SSIM can be integrated with existing MSR methods in a lightweight way to adaptively select an informative and shareable subset of instances from each scenario to improve recommendations. In particular, the selected multi-scenario shared subset has extraordinary reusability and can be easily saved to benefit model training of various future MSR models. Finally, we evaluate SSIM and demonstrate its effectiveness through experiments on two public multi-scenario benchmarks and an online A/B test.}
}


@inproceedings{DBLP:conf/kdd/0002WHSHSCBHWVT25,
	author = {Ping Liu and
                  Haichao Wei and
                  Xiaochen Hou and
                  Jianqiang Shen and
                  Shihai He and
                  Qianqi Shen and
                  Zhujun Chen and
                  Fedor Borisyuk and
                  Daniel Hewlett and
                  Liang Wu and
                  Srikant Veeraraghavan and
                  Alex Tsun and
                  Chengming Jiang and
                  Wenjing Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {LinkSAGE: Optimizing Job Matching Using Graph Neural Networks},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2448--2457},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709396},
	doi = {10.1145/3690624.3709396},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/0002WHSHSCBHWVT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIn's extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Network (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in near real-time, allowing for the effective integration of GNN insights through transfer learning. The subsequent nearline inference system serves the GNN encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time GNN infrastructure. Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact.}
}


@inproceedings{DBLP:conf/kdd/LiuXSHY25,
	author = {Tao Liu and
                  Qi Xu and
                  Wei Shi and
                  Zhigang Hua and
                  Shuang Yang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Session-Level Dynamic Ad Load Optimization using Offline Robust Reinforcement
                  Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2458--2468},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709437},
	doi = {10.1145/3690624.3709437},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/LiuXSHY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Session-level dynamic ad load optimization aims to personalize the density and types of delivered advertisements in real time during a user's online session by dynamically balancing user experience quality and ad monetization. Traditional causal learning-based approaches struggle with key technical challenges, especially in handling confounding bias and distribution shifts. In this paper, we develop an offline deep Q-network (DQN)-based framework that effectively mitigates confounding bias in dynamic systems and demonstrates more than 80% offline gains compared to the best causal learning-based production baseline. Moreover, to improve the framework's robustness against unanticipated distribution shifts, we further enhance our framework with a novel offline robust dueling DQN approach. This approach achieves more stable rewards on multiple OpenAI-Gym datasets as perturbations increase, and provides an additional 5% offline gains on real-world ad delivery data. Deployed across multiple production systems, our approach has achieved outsized topline gains. Post-launch online A/B tests have shown double-digit improvements in the engagement-ad score trade-off efficiency, significantly enhancing our platform's capability to serve both consumers and advertisers.}
}


@inproceedings{DBLP:conf/kdd/MollahDSCM25,
	author = {Md. Parvez Mollah and
                  Biplob Debnath and
                  Murugan Sankaradas and
                  Srimat Chakradhar and
                  Abdullah Mueen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Roadside Multi-LiDAR Data Fusion for Enhanced Traffic Safety},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2469--2479},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709410},
	doi = {10.1145/3690624.3709410},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/MollahDSCM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Roadside LiDAR (Light Detection and Ranging) sensors promise safer and faster traffic management and vehicular operations. However, occlusion and small view angles are significant challenges to widespread use of roadside LiDARs. We consider fusing data from multiple LiDARs at a traffic intersection to better estimate traffic parameters than one can estimate from a single LiDAR. The key challenge is to calibrate multiple LiDARs both in time and space. The problem is more complex when heterogeneous sensors differ in resolution and are positioned arbitrarily on a traffic intersection. We propose a calibration technique to fuse multiple LiDARs. We show that our technique works on various data granularity and enables real-time analytics for roadside traffic monitoring. We evaluate on a large number of simulated traffic scenarios and show that fusion improves accuracy of vehicle counting and near-collision detection. We apply our algorithm on real traffic data and demonstrate utility in classifying vehicles and detecting occluded traffic participants.}
}


@inproceedings{DBLP:conf/kdd/MukherjeeSF25,
	author = {Rajdeep Mukherjee and
                  Sonali Singh and
                  Sachin Farfade},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Using Instruction-Tuned LMs for Scalable Use Case-Based Shopping -
                  Where Customers Meet Their Needs},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2480--2491},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709411},
	doi = {10.1145/3690624.3709411},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/MukherjeeSF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Products on e-commerce platforms are usually organized based on seller-provided product attributes. Customers looking for a product typically have certain needs or use cases in mind, such as headphones for gym classes, or a printer for school projects. However, they often struggle to map these use cases to product attributes, thereby failing to find the product they need. To help customers shop online confidently, we propose a Use case-Based Shopping (UBS) system that facilitates customer experiences based on use cases (Fig. 1). UBS consists of three steps: 1) use case phrase extraction from product reviews, 2) clustering the extracted use case phrases to identify the dominant ones, and 3) mapping products in a given category to one or more of these dominant use cases. In this work, we utilize instruction-tuned LMs to primarily focus on the first two steps. However, the way we design them also helps us to seamlessly solve the third step to complete the design of our proposed system. First, we define the novel task of joint  Use Uase, Uentiment Uxtraction  (UCSE) from product reviews which can be used for both steps 1 and 3. We harness the task adaptation capability of instruction-tuned FLAN-T5 models and gradually improve their zero-shot UCSE performance through instruction tuning, multi-task training, and few-shot iterative re-training for new categories, achieving around ~90% reduction in annotation bandwidth. We then employ  Anthropic' s Claude 2 LLM to propose an unsupervised approach for hierarchical use case phrase clustering that demonstrates better clustering and cluster naming capabilities when compared to K-Means and LDA. In an online experiment targeting the top 7 product categories, UBS recommendations on search, browse, and product pages resulted in a revenue lift of 0.77%, 0.94%, and 0.44% respectively, and an average click rate lift of 0.15%.}
}


@inproceedings{DBLP:conf/kdd/NikolaouPT25,
	author = {Iasonas Nikolaou and
                  Konstantinos Pelechrinis and
                  Evimaria Terzi},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Understanding Team Collapse via Probabilistic Graphical Models},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2492--2503},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709386},
	doi = {10.1145/3690624.3709386},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/NikolaouPT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we develop a graphical model to capture team dynamics. We analyze the model and show how to learn its parameters from data. Using our model we study the phenomenon of  team collapse  from a computational perspective. We use simulations and real-world experiments to find the main causes of team collapse. We also provide the principles of building resilient teams,  i.e. , teams that avoid collapsing. Finally, we use our model to analyze the structure of NBA teams and dive deeper into games of interest.}
}


@inproceedings{DBLP:conf/kdd/QuY0BWS25,
	author = {Zhan Qu and
                  Shuzhou Yuan and
                  Michael F{\"{a}}rber and
                  Marius Brennfleck and
                  Niklas Wartha and
                  Anton Stephan},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Explainable LiDAR 3D Point Cloud Segmentation and Clustering for Detecting
                  Airplane-Generated Wind Turbulence},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2504--2513},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709436},
	doi = {10.1145/3690624.3709436},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/QuY0BWS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wake vortices-strong, coherent air turbulences created by aircrafts-pose a significant risk to aviation safety and therefore require accurate and reliable detection methods. In this paper, we present an advanced, explainable machine learning method that utilizes Light Detection and Ranging (LiDAR) data for effective wake vortex detection. Our method leverages a dynamic graph CNN (DGCNN) with semantic segmentation to partition a 3D LiDAR point cloud into meaningful segments. Further refinement is achieved through clustering techniques. A novel feature of our research is the use of a perturbation-based explanation technique, which clarifies the model's decision-making processes for air traffic regulators and controllers, increasing transparency and building trust. Our experimental results, based on measured and simulated LiDAR scans compared against four baseline methods, underscore the effectiveness and reliability of our approach. This combination of semantic segmentation and clustering for real-time wake vortex tracking significantly advances aviation safety measures, ensuring that these are both effective and comprehensible.}
}


@inproceedings{DBLP:conf/kdd/ShahBBFLWFDSSYL25,
	author = {Jaidev Shah and
                  Iman Barjasteh and
                  Amey Barapatre and
                  Rana Forsati and
                  Gang Luo and
                  Fan Wu and
                  Yuan Fang and
                  Xue Deng and
                  Blake Shepard and
                  Ronak Shah and
                  Linjun Yang and
                  Hongzhi Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Towards Web-scale Recommendations with LLMs: From Quality-aware Ranking
                  to Candidate Generation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2514--2524},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709413},
	doi = {10.1145/3690624.3709413},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ShahBBFLWFDSSYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Explore Further @ Bing is a webpage-to-webpage recommendation product, enhancing the search experience on Bing by surfacing engaging webpage recommendations tied to the search result URLs. In this paper, we present our approach for leveraging Large Language Models (LLMs) for enhancing our web-scale recommendation system. We describe the development and validation of our LLM-powered recommendation quality metric RecoDCG. We discuss our core techniques for utilizing LLMs to make our ranking stage quality-aware. Furthermore, we detail Q' recall, a recall path that enhances our system's candidate generation stage by leveraging LLMs to produce complementary and engaging recommendation candidates. We also address how we optimize our system for multiple objectives, balancing recommendation quality with click metrics. We deploy our work to production, achieving a significant improvement in recommendation quality. We share results from offline and online experiments as well as insights and steps we took to ensure our approaches scale effectively for our web-scale needs.}
}


@inproceedings{DBLP:conf/kdd/ShenCLA25,
	author = {Maying Shen and
                  Nadine Chang and
                  Sifei Liu and
                  Jos{\'{e}} M. {\'{A}}lvarez},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{SSE:} Multimodal Semantic Data Selection and Enrichment for Industrial-scale
                  Data Assimilation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2525--2535},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709417},
	doi = {10.1145/3690624.3709417},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ShenCLA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the data collected for artificial intelligence has grown to an unmanageable amount. Particularly within industrial applications, such as autonomous vehicles, model training computation budgets are being exceeded while model performance is saturating -- and yet more data continues to pour in. To navigate the flood of data, we propose a framework to select the most semantically diverse and important dataset portion. Then, we further semantically enrich it by discovering meaningful new data from a massive unlabeled data pool. Importantly, we can provide explainability by leveraging foundation models to generate semantics for every data point. We quantitatively show that our Semantic Selection and Enrichment framework (SSE) can a) successfully maintain model performance with a smaller training dataset and b) improve model performance by enriching the smaller dataset without exceeding the original dataset size. Consequently, we demonstrate that semantic diversity is imperative for optimal data selection and model performance.}
}


@inproceedings{DBLP:conf/kdd/ShiZ0LMWQ25,
	author = {Qitao Shi and
                  Jun Zhou and
                  Ya{-}Lin Zhang and
                  Longfei Li and
                  Chaoyi Ma and
                  Yifan Wu and
                  Xiaobo Qin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {AntAkso: Claims Management System for Health Insurance in Alipay},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2536--2547},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709398},
	doi = {10.1145/3690624.3709398},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ShiZ0LMWQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid growth of health insurance and the rising incidence of fraudulent claims underscore the necessity for an efficient and professional claims management system. However, there is a noticeable lack of shared relevant experience from previous research in this field. In response to this challenge, we introduce AntAkso, a robust claims management system specifically designed for health insurance operations within Alipay. AntAkso incorporates a digital and professional management system, achieving a notable decrease in the volume of false claims, reduction in administrative costs, and heightened satisfaction among its policyholders. We begin by highlighting the core components of this system, including the case stratification, hospital recommendation, and case dispatch modules, along with the pivotal algorithms employed, i.e., the fraud detection, recommendation, and robust satisficing algorithms. We also detail the system's implementation and deployment. We substantiate the proposed system's effectiveness and efficiency with empirical evidence from experiments on a large set of real-world health insurance claims data.}
}


@inproceedings{DBLP:conf/kdd/SonKKLK25,
	author = {Jiwon Son and
                  Jaeyoon Kim and
                  Taekin Kim and
                  Yeon{-}Chang Lee and
                  Sang{-}Wook Kim},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{CATER:} {A} Cluster-Based Alternative-Term Recommendation Framework
                  for Large-Scale Web Search at {NAVER}},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2548--2559},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709426},
	doi = {10.1145/3690624.3709426},
	timestamp = {Thu, 01 May 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/SonKKLK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, searching for information by using search engines such as Google, Bing, and NAVER has become ubiquitous. While they attempt to provide information based on the search queries that users enter, it is not trivial to accurately capture the  search intent of users.  Motivated by this situation, NAVER Corp., the largest portal company in Korea, has developed a framework named as  CATER  (Cluster-based Alternative TErm Recommendation) framework that suggests alternative terms (" al-terms, \'\' in short) for better search outcomes relevant to a user\'s search intent. We introduce four  design considerations (DCs)  that were considered when designing and implementing CATER. Then, we describe how our CATER addresses the four DCs by using a  clustering stage  that dynamically maintains a pool of topic-oriented clusters containing terms, and a  recommendation stage  that identifies the top- k  clusters ( i.e. , topics) and the top- k  al-terms for each cluster. Furthermore, we present the scalable architecture adopted by CATER. Through various offline and online A/B tests using real-world datasets from NAVER, we validate that CATER successfully incorporates all DCs and that all design choices help improve the recommendation accuracy.}
}


@inproceedings{DBLP:conf/kdd/SubhalingamKMS25,
	author = {D. Subhalingam and
                  Keshav Kolluru and
                  Mausam and
                  Saurabh Singal},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {A Framework for Leveraging Partially-Labeled Data for Product Attribute-Value
                  Identification},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2560--2571},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709427},
	doi = {10.1145/3690624.3709427},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/SubhalingamKMS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the e-commerce domain, the accurate extraction of attribute-value pairs (e.g., B rand :  Apple ) from product titles and user search queries is crucial for enhancing search and recommendation systems. A major challenge with neural models for this task is the lack of high-quality training data, as the annotations for attribute-value pairs in the available datasets are often incomplete. To address this, we introduce G en T o C, a model designed for training directly with partially-labeled data, eliminating the necessity for a fully annotated dataset. G en T o C employs a marker-augmented generative model to identify potential attributes, followed by a token classification model that determines the associated values for each attribute. G en T o C outperforms existing state-of-the-art models, exhibiting upto 56.3% increase in the number of accurate extractions. Furthermore, we utilize G en T o C to regenerate the training dataset to expand attribute-value annotations. This bootstrapping substantially improves the data quality for training other standard NER models, which are typically faster but less capable in handling partially-labeled data, enabling them to achieve comparable performance to G en T o C. Our results demonstrate G en T o C's unique ability to learn from a limited set of partially-labeled data and improve the training of more efficient models, advancing the automated extraction of attribute-value pairs. Finally, our model has been successfully integrated into IndiaMART, India's largest B2B e-commerce platform, achieving a significant increase of 20.2% in the number of correctly identified attribute-value pairs over the existing deployed system while achieving a high precision of 89.5%. We have released the code for G en T o C model at https://github.com/KnowDisAI/GenToC.}
}


@inproceedings{DBLP:conf/kdd/SunXLL25,
	author = {Zixun Sun and
                  Mingye Xu and
                  Guanming Liang and
                  Qi Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Unifying Adversarial Multi-Deconfounded Learning Paradigm for Fake
                  News Detection},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2572--2583},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709406},
	doi = {10.1145/3690624.3709406},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/SunXLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the task of fake news detection, ensuring authenticity and accuracy is of paramount importance. This task, however, is susceptible to the influence of confounders, necessitating effective confounder debiasing strategies. Conventional methods are typically designed to address specific confounders, resulting in frameworks that relatively lack generalization and overlook potential correlations among confounders. The presence of multiple confounders further escalates the complexity and challenges of debiasing learning. To tackle this issue, we introduce the Adversarial Multi-Deconfounded (AMD) Learning Paradigm, a generic training framework designed to eliminate biases from multiple confounders. Our approach leverages adversarial networks to extract confounder-invariant feature representations, guiding the model to ignore potential biases introduced by confounders and extract stable representations independent of these confounders, thereby enhancing generalization. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art methods on the Weibo and GossipCop datasets, and significantly exceeds other methods in generalization evaluation on CHEF. Additionally, we validate that our AMD framework exhibits improved robustness against confounders.}
}


@inproceedings{DBLP:conf/kdd/TanWQCCCX025,
	author = {Xiaoyu Tan and
                  Haoyu Wang and
                  Xihe Qiu and
                  Leijun Cheng and
                  Yuan Cheng and
                  Wei Chu and
                  Yinghui Xu and
                  Yuan Qi},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Struct-X: Enhancing the Reasoning Capabilities of Large Language Models
                  in Structured Data Scenarios},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2584--2595},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709381},
	doi = {10.1145/3690624.3709381},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/TanWQCCCX025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conducting reasoning tasks with large language models (LLMs) on structured and redundant data poses significant challenges, primarily due to the complexity introduced by the structured markdown tokens and the presence of extraneous contextual information. These elements can overburden and disrupt the generation process of LLMs, complicating the extraction of relevant insights and the production of coherent outputs. To address this, we propose  Struct-X , a novel framework that operates through five key phases: ''read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize structured data. It begins by encoding structured data into a topological space using graph embeddings, followed by filling in missing entity information with knowledge retrieval modules, and filtering out irrelevant tokens via a self-supervised module. The final phase involves constructing a topological network with selected tokens to further reduce the total token length for more effective LLM inference. Additionally,  Struct-X  includes an Auxiliary Module trained to generate prompts, aiding LLMs in analyzing structured data. Extensive experiments on open-source benchmarks, including the knowledge graph question-answer task and the long document reading comprehension task, show that  Struct-X  notably improves LLM reasoning in complex structured input context. Finally, we deployed  Struct-X  in a real-world financial report analysis task, where it exhibits enhanced reasoning capabilities when applied to authentic scenario. The code has been undergoing open-source development to facilitate easy replication.}
}


@inproceedings{DBLP:conf/kdd/TangZLZWGCY25,
	author = {Hengzhu Tang and
                  Zefeng Zhang and
                  Zhiping Li and
                  Zhenyu Zhang and
                  Xing Wu and
                  Li Gao and
                  Suqi Cheng and
                  Dawei Yin},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Multi-Branch Collaborative Learning Network for Video Quality Assessment
                  in Industrial Video Search},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2596--2605},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709408},
	doi = {10.1145/3690624.3709408},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/TangZLZWGCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video Quality Assessment (VQA) is a crucial component of broadscale video retrieval systems. Its goal is to accurately identify various quality issues in videos, thereby encouraging the video retrieval system to prioritize high-quality videos. In large-scale industrial video retrieval systems, we formulate the characteristics of low-quality videos into four categories: visual-related low-level quality problems such as mosaics and black boxes, textual-related low-level quality problems caused by video title and Optical Character Recognition (OCR) content, as well as semantic-level frame incoherence and frame-text mismatch caused by emerging AI-generated videos. These kinds of low-quality videos, which are widely present in industrial environments, have been overlooked in academic research before, and accurately identifying them is very challenging. In this paper, we introduce a  M ulti- B ranch  C ollaborative learning  N etwork ( MBCN ) to tackle the above issues. We carefully design four assessment branches for MBCN to adapt to the above four kinds of issues for industrial video retrieval systems. After obtaining independent scores for each branch, we perform a weighted aggregation of the various branches to dynamically address video quality issues in different scenarios with a squeeze-and-excitation mechanism. Finally, we integrate point-wise and pair-wise optimization objectives to ensure the predicted scores are stable and fall into a reasonable range. To demonstrate the effectiveness of our proposed MBCN, we conduct extensive offline and online experiments in a world-level video search engine. The experimental results show that due to the powerful ability of MBCN to identify video quality issues, the ranking ability of the video retrieval system has been significantly improved. We also conduct a series of detailed experimental analyses to verify that all four evaluation branches play a positive role. Besides that, for emerging low-quality AI-generated videos, the recognition accuracy of MBCN also improves significantly compared to the baseline.}
}


@inproceedings{DBLP:conf/kdd/Tang0GRLWYC25,
	author = {Yubao Tang and
                  Ruqing Zhang and
                  Jiafeng Guo and
                  Maarten de Rijke and
                  Shihao Liu and
                  Shuaiqiang Wang and
                  Dawei Yin and
                  Xueqi Cheng},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Generative Retrieval for Book Search},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2606--2617},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709435},
	doi = {10.1145/3690624.3709435},
	timestamp = {Fri, 09 May 2025 20:27:55 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Tang0GRLWYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In book search, relevant book information should be returned in response to a query. Books contain complex, multi-faceted information such as metadata, outlines, and main text, where the outline provides hierarchical information between chapters and sections. Generative retrieval (GR) is a new retrieval paradigm that consolidates corpus information into a single model to generate identifiers of documents that are relevant to a given query. How can GR be applied to book search? Directly applying GR to book search is a challenge due to the unique characteristics of book search: The model needs to retain the complex, multi-faceted information of the book, which increases the demand for labeled data. Splitting book information and treating it as a collection of separate segments for learning might result in a loss of hierarchical information. We propose an effective Generative retrieval framework for Book Search (GBS) that features two main components: data augmentation and outline-oriented book encoding. For data augmentation, GBS constructs multiple query-book pairs for training; it constructs multiple book identifiers based on the outline, various forms of book contents, and simulates real book retrieval scenarios with varied pseudo-queries. This includes coverage-promoting book identifier augmentation, allowing the model to learn to index effectively, and diversity-enhanced query augmentation, allowing the model to learn to retrieve effectively. Outline-oriented book encoding improves length extrapolation through bi-level positional encoding and retentive attention mechanisms to maintain context over long sequences. Experiments on a proprietary Baidu dataset demonstrate that GBS outperforms strong baselines, achieving a 9.8% improvement in terms of MRR@20, over the state-of-the-art RIPOR method. Experiments on public datasets confirm the robustness and generalizability of GBS, highlighting its potential to enhance book retrieval.}
}


@inproceedings{DBLP:conf/kdd/TurutovR25,
	author = {Sally Turutov and
                  Kira Radinsky},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Cross-Species Insights: Transforming Drug Efficacy from Rats to Humans
                  Using Tissue-Specific Generative Models},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2618--2627},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709389},
	doi = {10.1145/3690624.3709389},
	timestamp = {Thu, 01 May 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/TurutovR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Within the realm of drug development, the transition from successful animal trials to human clinical efficacy remains a daunting challenge. While initial outcomes may appear promising in animal studies, ensuring similar effectiveness in humans, especially across specific target tissues, presents a significant obstacle. To address this pressing concern, we introduce a novel generative model tailored to optimize molecules that have demonstrated efficacy in rats for enhanced performance in specific human tissues. Central to our solution is the transformer architecture, enhanced with intricate mechanisms such as molecule self-attention within the encoder and a novel dedicated tissue-specific generator. Intuitively, by learning to generate molecules simultaneously from multiple tissues, the generative model enhances its ability to perform the necessary adaptations from rats to humans. Through rigorous empirical evaluation across various tissues, our model consistently exhibits remarkable efficacy compared to existing methods. We anticipate that this model has the potential to minimize the requirement for lengthy and inconclusive trials, thereby streamlining the drug development process.}
}


@inproceedings{DBLP:conf/kdd/WangZZFWSYC25,
	author = {Chao Wang and
                  Yue Zheng and
                  Yujing Zhang and
                  Yan Feng and
                  Zhe Wang and
                  Xiaowei Shi and
                  An You and
                  Yu Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Breaker: Removing Shortcut Cues with User Clustering for Single-slot
                  Recommendation System},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2628--2637},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709387},
	doi = {10.1145/3690624.3709387},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WangZZFWSYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a single-slot recommendation system, users are only exposed to one item at a time, and the system cannot collect user feedback on multiple items simultaneously. Therefore, only pointwise modeling solutions can be adopted, focusing solely on modeling the likelihood of clicks or conversions for items by users to learn user-item preferences, without the ability to capture the ranking information among different items directly. However, since user-side information is often much more abundant than item-side information, the model can quickly learn the differences in user intrinsic tendencies, which are independent of the items they are exposed to. This can cause these intrinsic tendencies to become a shortcut bias for the model, leading to insufficient mining of the most concerned user-item preferences. To solve this challenge, we introduce the Breaker model. Breaker integrates an auxiliary task of user representation clustering with a multi-tower structure for cluster-specific preference modeling. By clustering user representations, we ensure that users within each cluster exhibit similar characteristics, which increases the complexity of the pointwise recommendation task on the user side. This forces the multi-tower structure with cluster-driven parameter learning to better model user-item preferences, ultimately eliminating shortcut biases related to user intrinsic tendencies. In terms of training, we propose a delayed parameter update mechanism to enhance training stability and convergence, enabling end-to-end joint training of the auxiliary clustering and classification tasks. Both offline and online experiments demonstrate that our method surpasses the baselines. It has already been deployed and is actively serving tens of millions of users daily on Meituan, one of the most popular e-commerce platforms for services.}
}


@inproceedings{DBLP:conf/kdd/WangCFGZ25,
	author = {Xu Wang and
                  Jiangxia Cao and
                  Zhiyi Fu and
                  Kun Gai and
                  Guorui Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at Kuaishou},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2638--2647},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709416},
	doi = {10.1145/3690624.3709416},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WangCFGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present the practical problems and the lessons learned at short-video services from Kuaishou. In industry, a widely-used multi-task framework is the Mixture-of-Experts (MoE) paradigm, which always introduces some shared and specific experts for each task and then uses gate networks to measure related experts' contributions. Although the MoE achieves remarkable improvements, we still observe three anomalies that seriously affect model performances in our iteration: (1)  Expert Collapse : We found that experts' output distributions are significantly different, and some experts have over 90% zero activations with ReLU, making it hard for gate networks to assign fair weights to balance experts. (2)  Expert Degradation : Ideally, the shared-expert aims to provide predictive information for all tasks simultaneously. Nevertheless, we find that some shared-experts are occupied by only one task, which indicates that shared-experts lost their ability but degenerated into some specific-experts. (3)  Expert Underfitting : In our services, we have dozens of behavior tasks that need to be predicted, but we find that some data-sparse prediction tasks tend to ignore their specific-experts and assign large weights to shared-experts. The reason might be that the shared-experts can perceive more gradient updates and knowledge from dense tasks, while specific-experts easily fall into underfitting due to their sparse behaviors. Motivated by those observations, we propose HoME to achieve a simple, efficient and balanced MoE system for multi-task learning. Specifically, we conduct three insightful modifications: (1)  Expert normalization&Swish mechanism  to align expert output distributions and avoid expert collapse. (2)  Hierarchy mask mechanism  to enhance sharing efficiency between tasks to reduce occupancy issues and away from expert degradation. (3)  Feature-gate&Self-gate mechanisms  to ensure each expert could obtain appropriate gradient to maximize its effectiveness. To our knowledge, this paper is the first work to focus on improving multi-task MoE system stability, and we conduct extensive offline&online (average improves 0.52% GAUC offline & 0.954% play-time per user online) experiments and ablation analyses to demonstrate our HoME effectiveness. From March 2024, our HoME has been widely deployed on various services at Kuaishou, supporting 400 Million active users daily.}
}


@inproceedings{DBLP:conf/kdd/WangB25,
	author = {Yan Wang and
                  Shan Ba},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Producer-Side Experiments Based on Counterfactual Interleaving Designs
                  for Online Recommender Systems},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2648--2659},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709428},
	doi = {10.1145/3690624.3709428},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WangB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems play a crucial role in online platforms, providing personalized recommendations for purchases, content consumption, and interpersonal connections. These systems involve two sides: producers (sellers, content creators, service providers, etc.) and consumers (buyers, viewers, customers, etc.). To optimize online recommender systems, A/B tests serve as the golden standard for comparing different ranking models and evaluating their impacts on both sides. While consumer-side experiments are relatively straightforward to design and commonly employed to assess ranking changes' effects on the behavior of consumers (buyers, viewers, etc.), designing producer-side experiments for an online recommender/ranking system is notably more complex. This complexity arises from the necessity of ranking producer items in the treatment and control groups by different models and then merging them into a unified ranking for presentation to each consumer. Existing design solutions in the literature lack rigorous guiding principles, leading to ad hoc approaches. In this paper, we address the limitations of current methods and propose the principles of consistency and monotonicity for designing producer-side experiments in online recommender systems. Building upon these principles, we also present a systematic solution based on counterfactual interleaving designs to accurately measure the impacts of ranking changes on the producers (sellers, content creators, etc.).}
}


@inproceedings{DBLP:conf/kdd/WangY0ZXTFFZZHL25,
	author = {Yuanchun Wang and
                  Jifan Yu and
                  Zijun Yao and
                  Jing Zhang and
                  Yuyang Xie and
                  Shangqing Tu and
                  Yiyang Fu and
                  Youhe Feng and
                  Jinkai Zhang and
                  Jingyao Zhang and
                  Bowen Huang and
                  Yuanyao Li and
                  Huihui Yuan and
                  Lei Hou and
                  Juanzi Li and
                  Jie Tang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {SoAy: {A} Solution-based {LLM} API-using Methodology for Academic
                  Information Seeking},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2660--2671},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709412},
	doi = {10.1145/3690624.3709412},
	timestamp = {Wed, 09 Apr 2025 09:19:47 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WangY0ZXTFFZZHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Applying large language models (LLMs) to academic API usage shows promise in reducing researchers' efforts to seek academic information. However, current LLM methods for using APIs struggle with the complex API coupling commonly encountered in academic queries. To address this, we introduce  SoAy , a solution-based LLM methodology for academic information seeking. S o A y  enables LLMs to generate code for invoking APIs, guided by a pre-constructed API calling sequence referred to as a solution. This solution simplifies the model's understanding of complex API relationships, while the generated code enhances reasoning efficiency. LLMs are aligned with this solution-oriented, code-based reasoning method by automatically enumerating valid API coupling sequences and transforming them into queries and executable code. To evaluate  SoAy , we introduce SoAyBench, an evaluation benchmark accompanied by SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental results demonstrate a 34.58-75.99% performance improvement compared to state-of-the-art LLM API-based baselines. All datasets, codes, tuned models, and deployed online services are publicly accessible at https://github.com/RUCKBReasoning/SoAy.}
}


@inproceedings{DBLP:conf/kdd/WangBCSBCC25,
	author = {Yuyan Wang and
                  Cheenar Banerjee and
                  Samer Chucri and
                  Fabio Soldo and
                  Sriraj Badam and
                  Ed H. Chi and
                  Minmin Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Beyond Item Dissimilarities: Diversifying by Intent in Recommender
                  Systems},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2672--2681},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709429},
	doi = {10.1145/3690624.3709429},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WangBCSBCC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It has become increasingly clear that recommender systems that overly focus on short-term engagement prevents users from exploring diverse interests, ultimately hurting long-term user experience. To tackle this challenge, numerous diversification algorithms have been proposed as the final stage of recommender systems. These algorithms typically rely on measures of item similarity, aiming to maximize the dissimilarity across items in the final set of recommendations. However, in this work, we demonstrate the benefits of going beyond item-level similarities by utilizing higher-level user understanding-specifically, user intents that persist across multiple interactions or recommendation sessions-in diversification. Our approach is motivated by the observation that user behaviors on online platforms are largely driven by their underlying intents. Therefore, final recommendations should ensure that a diverse set of user intents is accurately represented. While user intent has primarily been studied in the context of search, it is less clear how to incorporate real-time dynamic intent predictions in recommender systems. To address this gap, we develop a probabilistic intent-based whole-page diversification framework for the final stage of a recommender system. Starting with a prior belief of user intents, the proposed framework sequentially selects items for each position based on these beliefs and subsequently updates posterior beliefs about the intents. This approach ensures that different user intents are represented on a page, towards optimizing long-term user experience. We experiment with the intent diversification framework on YouTube, the world's largest video recommendation platform, serving billions of users daily. Live experiments on a diverse set of intents show that the proposed framework increases Daily Active Users (DAU) and overall user enjoyment, validating its effectiveness in facilitating long-term planning. Specifically, it enables users to consistently discover and engage with diverse content that aligns with their underlying intents over time, leading to an improved long-term user experience.}
}


@inproceedings{DBLP:conf/kdd/WuWZXL0WW25,
	author = {Hao Wu and
                  Haomin Wen and
                  Guibin Zhang and
                  Yutong Xia and
                  Yuxuan Liang and
                  Yu Zheng and
                  Qingsong Wen and
                  Kun Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal
                  Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2682--2692},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709391},
	doi = {10.1145/3690624.3709391},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/WuWZXL0WW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. These methods  dynamically adjust the activation times of sensors to optimize the detection process across each sub-region.  Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. Worse still, the complex technical design may ultimately lead to a model with weak generalizability. In this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions. To our knowledge, this is the  first  proposal ( termed DynST ) of an  industry-level  deployment optimization concept at the data level. However, due to the existence of the temporal dimension, pruning of spatio-temporal data may lead to conflicts at different timestamps. To achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect. During the training process, DynST utilize iterative pruning and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions. DynST demonstrates tremendous capability on industrial-grade data from  JD Technology  TaxiBJ+ and practical deployment scenarios such as meteorology, combustion dynamics, and turbulence. It seamlessly integrates with relevant models and efficiently prunes image and graph-type data, leading to significantly higher inference speeds without introducing noticeable performance degradation.}
}


@inproceedings{DBLP:conf/kdd/XiaZL0G0H0Y25,
	author = {Deguo Xia and
                  Weiming Zhang and
                  Xiyan Liu and
                  Wei Zhang and
                  Chenting Gong and
                  Xiao Tan and
                  Jizhou Huang and
                  Mengmeng Yang and
                  Diange Yang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {LDMapNet-U: An End-to-End System for City-Scale Lane-Level Map Updating},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2693--2702},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709383},
	doi = {10.1145/3690624.3709383},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XiaZL0G0H0Y25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An up-to-date city-scale lane-level map is an indispensable infrastructure and a key enabling technology for ensuring the safety and user experience of autonomous driving systems. In industrial scenarios, reliance on manual annotation for map updates creates a critical bottleneck. Lane-level updates require precise change information and must ensure consistency with adjacent data while adhering to strict standards. Traditional methods utilize a three-stage approach -- construction, change detection, and updating -- which often necessitates manual verification due to accuracy limitations. This results in labor-intensive processes and hampers timely updates. To address these challenges, we propose LDMapNet-U, which implements a new end-to-end paradigm for city-scale lane-level map updating. By reconceptualizing the update task as an end-to-end map generation process grounded in historical map data, we introduce a paradigm shift in map updating that simultaneously generates vectorized maps and change information. To achieve this, a Prior-Map Encoding (PME) module is introduced to effectively encode historical maps, serving as a critical reference for detecting changes. Additionally, we incorporate a novel Instance Change Prediction (ICP) module that learns to predict associations with historical maps. Consequently, LDMapNet-U simultaneously achieves vectorized map element generation and change detection. To demonstrate the superiority and effectiveness of LDMapNet-U, extensive experiments are conducted using large-scale real-world datasets. In addition, LDMapNet-U has been successfully deployed in production at Baidu Maps since April 2024, supporting lane-level map updating for over 360 cities and significantly shortening the update cycle from quarterly to weekly, thereby enhancing the timeliness and accuracy of lane-level map. The nationwide, high-frequency city-scale lane-level map has been instrumental in the development of the lane-level navigation product serving hundreds of millions of users, while also integrating into the autonomous driving systems of several leading vehicle companies.}
}


@inproceedings{DBLP:conf/kdd/XiangF0YS025,
	author = {Yinfeng Xiang and
                  Jiangyi Fang and
                  Chao Li and
                  Haitao Yuan and
                  Yiwei Song and
                  Jiming Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Effective AOI-level Parcel Volume Prediction: When Lookahead Parcels
                  Matter},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2703--2712},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709441},
	doi = {10.1145/3690624.3709441},
	timestamp = {Tue, 13 May 2025 07:31:04 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XiangF0YS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Last-mile Delivery Parcel Volume (LDPV) quantifies the number of parcels destined for a specific region, particularly a manually divided Area-Of-Interest (AOI). Accurate prediction of AOI-level LDPV is crucial for the efficient management of logistics resources. However, the straightforward adaptation of existing prediction models often falls short, primarily due to (I) a lack of consideration for the intuition behind AOI divisions, and (II) a reliance solely on fully observed historical data, which may not inform future trends. To overcome the above pitfalls, leveraging rich AOI data and advanced parcel travel time estimation services in JD Logistics, this paper introduces a novel framework called Dual-view Prediction Networks (DualPNs). It combines a Vector-Quantified AutoEncoder (VQ-AE) and a Template-Augmented Zero-Inflated Poisson (TA-ZIP), enabling both point and probabilistic distribution predictions of AOI-level LDPV. Specifically, VQ-AE utilizes a vector quantization technique to distill a large number of AOIs into representative templates, thereby addressing the first pitfall. Subsequently, TA-ZIP dynamically integrates fully observed and lookahead features, aligning them with template-specific decoders to parameterize the probabilistic distributions, thus resolving the second pitfall. We conduct extensive experiments in two cities, comprising over 47,000 and 126,000 AOIs respectively, to demonstrate the superiority of our DualPNs over other baselines. Moreover, a real-world case study highlights the effectiveness of DualPNs for enhancing downstream courier allocation by yielding an average improvement of 1.51% in the on-time delivery rate.}
}


@inproceedings{DBLP:conf/kdd/XieLSWYL0Z025,
	author = {Zejun Xie and
                  Wenjun Lyu and
                  Yiwei Song and
                  Haotian Wang and
                  Guang Yang and
                  Yunhuai Liu and
                  Tian He and
                  Desheng Zhang and
                  Guang Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Scalable Area Difficulty Assessment with Knowledge-enhanced {AI} for
                  Nationwide Logistics Systems},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2713--2724},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709407},
	doi = {10.1145/3690624.3709407},
	timestamp = {Wed, 09 Apr 2025 09:19:48 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XieLSWYL0Z025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Logistics services have become a core business in online-to-offline e-commerce like Amazon, Alibaba, and JD. In logistics services, a city is partitioned into distinct geographical areas, and each area is assigned a worker, responsible for all delivery tasks within it. Due to varying geographic conditions (e.g., high-rise buildings, buildings without elevators), the difficulty of completing tasks can differ significantly between areas, which results in unbalanced workloads and salaries for workers. The necessity for scalable data-driven methods to assess area difficulty in logistics is well-recognized. However, the significant expenses associated with ground truth data collection limit the capabilities of current machine learning methods. In this paper, we consider a frequently overlooked resource, i.e., the workers' firsthand knowledge of areas, to address this problem in a human-AI collaboration fashion. In particular, we design RAICA (Ranking-Aggregated Isotonic Calibration Assessment) framework, which includes two key modules: (i) a Judgment Rank Aggregation module, which aggregates individual workers' judgment rankings collected from surveys into an overall ranking to mitigate personal biases and inconsistency between different workers; (ii) an Isotonic Calibration module, which calibrates the assessment from existing machine learning models with the aggregated ranking through Isotonic regression to enhance the accuracy of area difficulty assessment with theoretical guarantees. Extensive evaluation based on real-world data including over 2 million orders collected from 97 areas during 6 months by one of the largest logistics companies in the world shows that RAICA outperforms existing methods, increasing F1 score by 0.25. More importantly, RAICA has been deployed by this logistics company, which significantly improved crowdsourcing couriers' salary fairness with a 0.2 decrease in the Gini coefficient across over 1,200 delivery stations nationwide and increased the on-time delivery rates for full-time couriers by 1.67%.}
}


@inproceedings{DBLP:conf/kdd/XuPLX25,
	author = {Han Xu and
                  Taoxing Pan and
                  Zhiqiang Liu and
                  Xiaoxiao Xu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Mutual Information-aware Knowledge Distillation for Short Video Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2725--2734},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709403},
	doi = {10.1145/3690624.3709403},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XuPLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Short-video sharing platforms engaging billions of users have attracted intense interest recently. A key insight is that user feedback on these platforms is heavily influenced by preceding exposed videos in the same request, called context cumulative effects. For example, multiple repeated videos in a request often cause user fatigue and influence user feedback. However, related factors, such as the other exposed items in the same request, are available during model training but not accessible during online serving. Vanilla distillation methods mitigate the training-inference inconsistency, struggling to capture the dynamic dependence between context cumulative effects and user feedback. To address this problem, we propose the Mutual Information-aware Knowledge Distillation (MIKD) framework, which fuses such effects and user-item matching degrees by evaluating their impacts on user feedback based on mutual information estimation. Rigorous analysis and extensive experiments demonstrate that MIKD precisely extracts personal interests and consistently improves performance. We conduct online A/B testing on a leading short-video sharing mobile app, and the results demonstrate the effectiveness of the proposed method. MIKD has been successfully deployed online to serve the main traffic and optimize user experiences.}
}


@inproceedings{DBLP:conf/kdd/XuHSY25,
	author = {Qingying Xu and
                  Liang Hong and
                  Mingxuan Shen and
                  Baokun Yi},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Disclosing Actual Controller based on Equity Knowledge Graph Learning},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2735--2744},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709432},
	doi = {10.1145/3690624.3709432},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XuHSY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Disclosing Actual Controllers (ACs) of a company has been the basis for financial risk governance. A shareholder in a winning stable coalition, where members make consistent decisions and win in votes, is considered an AC. However, existing methods fail to discover stable coalitions due to the ignorance of various relations other than the shareholding relation among shareholders, such as kinship, subsidiary and so on. Moreover, the above relations form a large-scale equity network, which brings challenges for efficiently identifying winning stable coalitions. We construct an Equity Knowledge Graph (EKG) to represent the semantic and structural information of the equity network. In this paper, we propose an AC disclosure method based on Equity Knowledge Graph Learning (EKGL). Specifically, to discover stable coalitions, EKGL designs a multi-relational aggregation module to aggregate the information of different relations horizontally. Based on the aggregated information, EKGL leverages a metapath-based aggregation module to encode the shareholding structure by capturing different shareholding paths on EKG vertically. To identify winning stable coalitions, we propose a control neural network to simulate the voting process of shareholders. Experiments and a case study on the EKG constructed from real datasets demonstrate that EKGL outperforms baselines by achieving 0.33 improvement in F1 score and reducing time cost.}
}


@inproceedings{DBLP:conf/kdd/XuW0ZYYLLC25,
	author = {Xiang Xu and
                  Hao Wang and
                  Wei Guo and
                  Luankang Zhang and
                  Wanshan Yang and
                  Runlong Yu and
                  Yong Liu and
                  Defu Lian and
                  Enhong Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Multi-granularity Interest Retrieval and Refinement Network for Long-Term
                  User Behavior Modeling in {CTR} Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2745--2755},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709438},
	doi = {10.1145/3690624.3709438},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/XuW0ZYYLLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Click-through Rate (CTR) prediction is crucial for online personalization platforms. Recent advancements have shown that modeling rich user behaviors can significantly improve the performance of CTR prediction. Current long-term user behavior modeling algorithms predominantly follow two cascading stages. The first stage retrieves subsequence related to the target item from the long-term behavior sequence, while the second stage models the relationship between the subsequence and the target item. Despite significant progress, these methods have two critical flaws. First, the retrieval query typically includes only target item information, limiting the ability to capture the user's diverse interests. Second, relational information, such as sequential and interactive information within the subsequence, is frequently overlooked. Therefore, it requires to be further mined to more accurately model user interests. To this end, we propose Multi-granularity Interest Retrieval and Refinement Network (MIRRN). Specifically, we first construct queries based on behaviors observed at different time scales to obtain subsequences, each capturing users' interest at various granularities. We then introduce an noval multi-head Fourier transformer to efficiently learn sequential and interactive information within the subsequences, leading to more accurate modeling of user interests. Finally, we employ multi-head target attention to adaptively assess the impact of these multi-granularity interests on the target item. Extensive experiments have demonstrated that MIRRN significantly outperforms state-of-the-art baselines. Furthermore, an A/B test shows that MIRRN increases the average number of listening songs by 1.32% and the average time of listening songs by 0.55% on the Huawei Music App. The implementation code is publicly available at https://github.com/USTC-StarTeam/MIRRN.}
}


@inproceedings{DBLP:conf/kdd/YangHCWX0L0Z25,
	author = {Qinchen Yang and
                  Zhiqing Hong and
                  Dongjiang Cao and
                  Haotian Wang and
                  Zejun Xie and
                  Tian He and
                  Yunhuai Liu and
                  Yu Yang and
                  Desheng Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {AddrLLM: Address Rewriting via Large Language Model on Nationwide
                  Logistics Data},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2756--2767},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709425},
	doi = {10.1145/3690624.3709425},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YangHCWX0L0Z25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Textual description of a physical location, commonly known as an address, plays an important role in location-based services(LBS) such as on-demand delivery and navigation. However, the prevalence of abnormal addresses, those containing inaccuracies that fail to pinpoint a location, have led to significant costs. Address rewriting has emerged as a solution to rectify these abnormal addresses. Despite the critical need, existing address rewriting methods are limited, typically tailored to correct specific error types, or frequently require retraining to process new address data effectively. In this study, we introduce AddrLLM, an innovative framework for address rewriting that is built upon a retrieval augmented large language model. AddrLLM overcomes aforementioned limitations through a meticulously designed Supervised Fine-Tuning module, an Address-centric Retrieval Augmented Generation module and a Bias-free Objective Alignment module. To the best of our knowledge, this study pioneers the application of LLM-based address rewriting approach to solve the issue of abnormal addresses. Through comprehensive offline testing with real-world data on a national scale and subsequent online deployment, AddrLLM has demonstrated superior performance in integration with existing logistics system. It has significantly decreased the rate of parcel re-routing by approximately 43%, underscoring its exceptional efficacy in real-world applications.}
}


@inproceedings{DBLP:conf/kdd/YangYDGPLLL25,
	author = {Shentao Yang and
                  Haichuan Yang and
                  Linna Du and
                  Adithya Ganesh and
                  Bo Peng and
                  Boying Liu and
                  Serena Li and
                  Ji Liu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {SWaT: Statistical Modeling of Video Watch Time through User Behavior
                  Analysis},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2768--2778},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709415},
	doi = {10.1145/3690624.3709415},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YangYDGPLLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The significance of estimating video watch time has been highlighted by the rising importance of (short) video recommendation, which has become a core product of mainstream social media platforms. Modeling video watch time, however, has been challenged by the complexity of user-video interaction, such as different user behavior modes in watching the recommended videos and varying watching probability over the video progress bar. Despite the importance and challenges, existing literature on modeling video watch time mostly focuses on relatively black-box mechanical enhancement of the classical regression/classification losses, without factoring in user behavior in a principled manner. In this paper, we for the first time take on a user-centric perspective to model video watch time, from which we propose a white-box statistical framework that directly translates various user behavior assumptions in watching (short) videos into statistical watch time models. These behavior assumptions are portrayed by our domain knowledge on users' behavior modes in video watching. We further employ bucketization to cope with user's non-stationary watching probability over the video progress bar, which additionally helps to respect the constraint of video length and facilitate the practical compatibility between the continuous regression event of watch time and other binary classification events. We test our models extensively on two public datasets, a large-scale offline industrial dataset, and an online A/B test on a short video platform with hundreds of millions of daily-active users. On all experiments, our models perform competitively against strong relevant baselines, demonstrating the efficacy of our user-centric perspective and proposed framework.}
}


@inproceedings{DBLP:conf/kdd/YinFYPCWC025,
	author = {Changchang Yin and
                  Shihan Fu and
                  Bingsheng Yao and
                  Thai{-}Hoang Pham and
                  Weidan Cao and
                  Dakuo Wang and
                  Jeffrey M. Caterino and
                  Ping Zhang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {SepsisCalc: Integrating Clinical Calculators into Early Sepsis Prediction
                  via Dynamic Temporal Graph Construction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2779--2790},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709402},
	doi = {10.1145/3690624.3709402},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YinFYPCWC025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sepsis is an organ dysfunction caused by a deregulated immune response to an infection. Early sepsis prediction and identification allow for timely intervention, leading to improved clinical outcomes. Clinical calculators ( e.g. , the six-organ dysfunction assessment of SOFA) play a vital role in sepsis identification within clinicians' workflow, providing evidence-based risk assessments essential for sepsis diagnosis. However, artificial intelligence (AI) sepsis prediction models typically generate a single sepsis risk score without incorporating clinical calculators for assessing organ dysfunctions, making the models less convincing and transparent to clinicians. To bridge the gap, we propose to mimic clinicians' workflow with a novel framework SepsisCalc to integrate clinical calculators into the predictive model, yielding a clinically transparent and precise model for utilization in clinical settings. Practically, clinical calculators usually combine information from multiple component variables in Electronic Health Records (EHR), and might not be applicable when the variables are (partially) missing. We mitigate this issue by representing EHRs as temporal graphs and integrating a learning module to dynamically add the accurately estimated calculator to the graphs. Experimental results on real-world datasets show that the proposed model outperforms state-of-the-art methods on sepsis prediction tasks. Moreover, we developed a system to identify organ dysfunctions and potential sepsis risks, providing a human-AI interaction tool for deployment, which can help clinicians understand the prediction outputs and prepare timely interventions for the corresponding dysfunctions, paving the way for actionable clinical decision-making support for early intervention.}
}


@inproceedings{DBLP:conf/kdd/YuXGWCY025,
	author = {Haiyang Yu and
                  Tian Xie and
                  Jiaping Gui and
                  Pengyang Wang and
                  Pengzhou Cheng and
                  Ping Yi and
                  Yue Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {BackdoorMBTI: {A} Backdoor Learning Multimodal Benchmark Tool Kit
                  for Backdoor Defense Evaluation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2791--2802},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709385},
	doi = {10.1145/3690624.3709385},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YuXGWCY025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past few years, the emergence of backdoor attacks has presented significant challenges to deep learning systems, allowing attackers to insert backdoors into neural networks. When data with a trigger is processed by a backdoor model, it can lead to mispredictions targeted by attackers, whereas normal data yields regular results. The scope of backdoor attacks is expanding beyond computer vision and encroaching into areas such as natural language processing and speech recognition. Nevertheless, existing backdoor defense methods are typically tailored to specific data modalities, restricting their application in multimodal contexts. While multimodal learning proves highly applicable in facial recognition, sentiment analysis, action recognition, visual question answering, the security of these models remains a crucial concern. Specifically, there are no existing backdoor benchmarks targeting multimodal applications or related tasks. In order to facilitate the research in multimodal backdoor, we introduce BackdoorMBTI, the first backdoor learning toolkit and benchmark designed for multimodal evaluation across three representative modalities from eleven commonly used datasets. BackdoorMBTI provides a systematic backdoor learning pipeline, encompassing data processing, data poisoning, backdoor training, and evaluation. The generated poison datasets and backdoor models enable detailed evaluation of backdoor defenses. Given the diversity of modalities, BackdoorMBTI facilitates systematic evaluation across different data types. Furthermore, BackdoorMBTI offers a standardized approach to handling practical factors in backdoor learning, such as issues related to data quality and erroneous labels. We anticipate that BackdoorMBTI will expedite future research in backdoor defense methods within a multimodal context. Code is available at https://github.com/SJTUHaiyangYu/BackdoorMBTI.}
}


@inproceedings{DBLP:conf/kdd/YuGSDLX25,
	author = {Pengfei Yu and
                  Jingjing Gu and
                  Dazhong Shen and
                  Xin Dong and
                  Yang Liu and
                  Hui Xiong},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Instruction Semantics Enhanced Dual-Flow Graph Model for {GPU} Error
                  Resilience Prediction},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2803--2814},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709424},
	doi = {10.1145/3690624.3709424},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/YuGSDLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As GPUs are widely deployed in High Performance Computing systems, it is critical to ensure that these systems can perform reliably. To improve system reliability, researchers estimate the error resilience of GPU programs by understanding resilience characteristics or modeling error propagation. However, features indicative of resilience rely on manual extraction from simulations of numerous faults, and error propagation analysis cannot target fine-grained bit-level faults. To address those problems, this paper introduces a novel paradigm, namely InstrDGM, for efficiently predicting GPU error resilience. Specifically, InstrDGM first fine-tunes a large language model using extensive sequences of GPU assembly instructions for extracting the semantic representation of instructions automatically. Meanwhile, we consider the propagation of bit-level faults during instruction execution and data transfer processes, and leverage graph neural networks to capture their distinct error propagation patterns. Then, the fault embeddings extracted from these error propagation patterns are integrated for error resilience prediction. Additionally, this paper releases a new dataset for GPU error resilience assessment, containing 1.2 million fault samples. Finally, extensive experiments show that InstrDGM significantly outperforms existing methods.}
}


@inproceedings{DBLP:conf/kdd/ZhangZWW00GHC25,
	author = {Chao Zhang and
                  Haoxin Zhang and
                  Shiwei Wu and
                  Di Wu and
                  Tong Xu and
                  Xiangyu Zhao and
                  Yan Gao and
                  Yao Hu and
                  Enhong Chen},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {NoteLLM-2: Multimodal Large Representation Models for Recommendation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2815--2826},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709440},
	doi = {10.1145/3690624.3709440},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangZWW00GHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Language Models (LLMs) have demonstrated exceptional proficiency in text understanding and embedding tasks. However, their potential in multimodal representation, particularly for item-to-item (I2I) recommendations, remains underexplored. While leveraging existing Multimodal Large Language Models (MLLMs) for such tasks is promising, challenges arise due to their delayed release compared to corresponding LLMs and the inefficiency in representation tasks. To address these issues, we propose an end-to-end fine-tuning method that customizes the integration of any existing LLMs and vision encoders for efficient multimodal representation. Preliminary experiments revealed that fine-tuned LLMs often neglect image content. To counteract this, we propose NoteLLM-2, a novel framework that enhances visual information. Specifically, we propose two approaches: first, a prompt-based method that segregates visual and textual content, employing a multimodal In-Context Learning strategy to balance focus across modalities; second, a late fusion technique that directly integrates visual information into the final representations. Extensive experiments, both online and offline, demonstrate the effectiveness of our approach. Code is available at https://github.com/Applied-Machine-Learning-Lab/NoteLLM.}
}


@inproceedings{DBLP:conf/kdd/ZhangLHSLWL25,
	author = {Ruixing Zhang and
                  Yunqi Liu and
                  Liangzhe Han and
                  Leilei Sun and
                  Chuanren Liu and
                  Jibin Wang and
                  Weifeng Lv},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Large-scale Human Mobility Data Regeneration for Open Urban Research},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2827--2836},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709380},
	doi = {10.1145/3690624.3709380},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangLHSLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale human mobility data contains rich spatial and temporal information for urban sensing, crowd flow modeling, and urban planning. However, it is usually difficult to access wide-coverage, long-term, and consistent-time human mobility data. Most of the publicly available datasets are actually only records of discontinuous trajectories of a very small portion of urban citizens in asynchronous time due to the limited usage of apps for location data collection or the limited number of volunteers. To address this problem and empower open urban research, this paper constructs a high-quality human mobility dataset by generating large-scale citizen trajectories based on massive cellular signaling data. Particularly, we first propose a heatmap diffusion module to generate a probability heatmap that produces plausible trajectories at both the individual and city scales. Then, we propose a masked trajectory AutoEncoder, which can generate individual trajectory embeddings from partially given or empty trajectories. Third, a flexible framework is provided to incorporate the heatmap diffusion module with the masked trajectory embeddings, demonstrating significant flexibility in handling both fully masked trajectories for city-wide analysis and partially masked trajectories for specific locations. We have conducted extensive experiments to validate the utility of the regenerated trajectories at both individual and region levels for various applications. Numerous case studies further illustrate that our model learns not only the distribution of the trajectories but also the semantics of different urban areas. In summary, this paper provides a Heatmap Diffusion framework based on a Masked Trajectory AutoEncoder to regenerate flexible trajectories for open urban research. Correspondingly, we will try to open a large-scale human mobility data service for open urban research. Further information can be found at https://github.com/Rising0321/FinalOpenUR.}
}


@inproceedings{DBLP:conf/kdd/ZhangWABTBV025,
	author = {Shuaicheng Zhang and
                  Tuo Wang and
                  Stephen Adams and
                  Sanmitra Bhattacharya and
                  Sunil Reddy Tiyyagura and
                  Edward Bowen and
                  Balaji Veeramani and
                  Dawei Zhou},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {MentorPDM: Learning Data-Driven Curriculum for Multi-Modal Predictive
                  Maintenance},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2837--2847},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709388},
	doi = {10.1145/3690624.3709388},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangWABTBV025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predictive Maintenance (PDM) systems are essential for preemptive monitoring of sensor signals to detect potential machine component failures in industrial assets such as bearings in rotating machinery. Existing PDM systems face two primary challenges: 1)  Irregular Signal Acquisition , where data collection from the sensors is intermittent, and 2)  Signal Heterogeneity , where the full spectrum of sensor modalities is not effectively integrated. To address these challenges, we propose a Curriculum Learning Framework for Multi-Modal Predictive Maintenance - M entor PDM. M entor PDM consists of 1) a graph-augmented pretraining module that captures intrinsic and structured temporal correlations across time segments via a temporal contrastive learning objective and 2) a bi-level curriculum learning module that captures task complexities for weighing the importance of signal modalities and samples via modality and sample curricula. Empirical results from M entor PDM show promising performance with better generalizability in PDM tasks compared to existing benchmarks. The efficacy of the M entor PDM model will be further demonstrated in real industry testbeds and platforms.}
}


@inproceedings{DBLP:conf/kdd/ZhangHWLQCX0WW25,
	author = {Xu Zhang and
                  Zhengang Huang and
                  Yunzhi Wu and
                  Xun Lu and
                  Erpeng Qi and
                  Yunkai Chen and
                  Zhongya Xue and
                  Qitong Wang and
                  Peng Wang and
                  Wei Wang},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Multi-period Learning for Financial Time Series Forecasting},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2848--2859},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709422},
	doi = {10.1145/3690624.3709422},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangHWLQCX0WW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series forecasting is important in finance domain. Financial time series (TS) patterns are influenced by both short-term public opinions and medium-/long-term policy and market trends. Hence, processing multi-period inputs becomes crucial for accurate financial time series forecasting (TSF). However, current TSF models either use only single-period input, or lack customized designs for addressing multi-period characteristics. In this paper, we propose a Multi-period Learning Framework (MLF) to enhance financial TSF performance. MLF considers both TSF's accuracy and efficiency requirements. Specifically, we design three new modules to better integrate the multi-period inputs for improving accuracy: (i) Inter-period Redundancy Filtering (IRF), that removes the information redundancy between periods for accurate self-attention modeling, (ii) Learnable Weighted-average Integration (LWI), that effectively integrates multi-period forecasts, (iii) Multi-period self-Adaptive Patching (MAP), that mitigates the bias towards certain periods by setting the same number of patches across all periods. Furthermore, we propose a Patch Squeeze module to reduce the number of patches in self-attention modeling for maximized efficiency. MLF incorporates multiple inputs with varying lengths (periods) to achieve better accuracy and reduces the costs of selecting input lengths during training. The codes and datasets are available at https://github.com/Meteor-Stars/MLF.}
}


@inproceedings{DBLP:conf/kdd/ZhangWMTNLMJCZ025,
	author = {Zheyuan Zhang and
                  Zehong Wang and
                  Tianyi Ma and
                  Varun Sameer Taneja and
                  Sofia Nelson and
                  Nhi Ha Lan Le and
                  Keerthiram Murugesan and
                  Mingxuan Ju and
                  Nitesh V. Chawla and
                  Chuxu Zhang and
                  Yanfang Ye},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {{MOPI-HFRS:} {A} Multi-objective Personalized Health-aware Food Recommendation
                  System with LLM-enhanced Interpretation},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2860--2871},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709382},
	doi = {10.1145/3690624.3709382},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhangWMTNLMJCZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prevalence of unhealthy eating habits has become a growing concern in the United States. However, popular food recommendation platforms, such as Yelp, tend to prioritize users' dietary preferences over the healthiness of their choices. While some efforts have focused on developing health-aware food recommendation systems, personalization based on specific health conditions remains underexplored. Additionally, the lack of interpretability in these systems prevents users from evaluating the reliability of recommendations, limiting their practical adoption. To address these issues, we introduce two large-scale  personalized  health-aware food recommendation benchmarks  at the first attempt . Building on this, we propose a novel framework called the  M ulti- O bjective  P ersonalized  I nterpretable  H ealth-aware  F ood  R ecommendation  S ystem ( MOPI-HFRS ). This system generates food recommendations by jointly optimizing three objectives: user preference, personalized healthiness, and nutritional diversity. It also incorporates a reasoning module enhanced by large language models (LLMs) to provide interpretable recommendations that promote healthy dietary knowledge. The framework integrates descriptive features and health data using two structure learning and pooling modules within a graph learning framework. Pareto optimization is applied to balance the multi-faceted objectives. To further enhance healthy dietary knowledge, the system leverages LLMs by infusing knowledge from the recommendation model, generating meaningful interpretations for the recommendations. Extensive experiments on the proposed benchmarks demonstrate that MOPI-HFRS outperforms state-of-the-art methods by delivering diverse, healthy food recommendations alongside reliable explanations.}
}


@inproceedings{DBLP:conf/kdd/Zhang-LiZYYTGW025,
	author = {Daniel Zhang{-}Li and
                  Zheyuan Zhang and
                  Jifan Yu and
                  Joy Lim Jia Yin and
                  Shangqing Tu and
                  Linlu Gong and
                  Haohua Wang and
                  Zhiyuan Liu and
                  Huiqin Liu and
                  Lei Hou and
                  Juanzi Li},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Awaking the Slides: {A} Tuning-free and Knowledge-regulated {AI} Tutoring
                  System via Language Model Coordination},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2872--2883},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709423},
	doi = {10.1145/3690624.3709423},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/Zhang-LiZYYTGW025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The vast pre-existing slides serve as rich and important materials to carry lecture knowledge. However, effectively leveraging lecture slides to serve students is difficult due to the multi-modal nature of slide content and the heterogeneous teaching actions. We study the problem of discovering effective designs that convert a slide into an interactive lecture. We develop Slide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring system that can (1) effectively convert an input lecture slide into a structured teaching agenda consisting of a set of heterogeneous teaching actions; (2) create and manage an interactive lecture that generates responsive interactions catering to student learning demands while regulating the interactions to follow teaching actions. Slide2Lecture contains a complete pipeline for learners to obtain an interactive classroom experience to learn the slide. For teachers and developers, Slide2Lecture enables customization to cater to personalized demands. Slide2Lecture's online deployment has made more than 200K interactions with students in the 3K lecture sessions. We release our implementation at https://github.com/NewEduAI/Release.}
}


@inproceedings{DBLP:conf/kdd/ZhuXYLZZLW25,
	author = {Ruitao Zhu and
                  Wendong Xiao and
                  Yao Yu and
                  Yangsu Liu and
                  Zhenzhe Zheng and
                  Shuqi Zhang and
                  Dong Li and
                  Fan Wu},
	editor = {Yizhou Sun and
                  Flavio Chierichetti and
                  Hady W. Lauw and
                  Claudia Perlich and
                  Wee Hyong Tok and
                  Andrew Tomkins},
	title = {Prices Do Matter: Modeling Price Competitiveness for Online Hotel
                  Industry},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, V.1, {KDD} 2025, Toronto, ON, Canada, August 3-7,
                  2025},
	pages = {2884--2893},
	publisher = {{ACM}},
	year = {2025},
	url = {https://doi.org/10.1145/3690624.3709420},
	doi = {10.1145/3690624.3709420},
	timestamp = {Fri, 09 May 2025 20:27:56 +0200},
	biburl = {https://dblp.org/rec/conf/kdd/ZhuXYLZZLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Broad adoption of Online Travel Platforms (OTPs) has led to increasing interest in accurately predicting users' hotel purchase behavior, with price being a key influencer in user decision-making and receiving significant focus. In examining the hotel purchasing process, we identify a pervasive trend that users make extensive price comparisons before making decisions. Existing research primarily focuses on a hotel's own price, neglecting the complex dynamics of market-driven price competition. In this paper, we propose the concept of Marketplace-oriented Hotel Price Competitiveness (MHPC) to model a hotel's pricing competitiveness within the marketplace. Being independent of specific user preferences, MHPC can be applied to and improve various downstream operations in the online hotel industry, such as hotel ranking and pricing, ultimately benefiting hoteliers, users, and OTPs. Furthermore, a novel Hotel Price Competitiveness-aware Purchase Prediction Model (HP3M) is constructed by incorporating MHPC and demand dynamics into a multi-task learning framework, featuring three distinct submodules to encompass the tri-dimensional facets of MHPC. Extensive offline and online experiments demonstrate HP3M's effectiveness in predicting hotel purchase probability and enhancing the performance of hotel ranking and pricing compared to the state-of-the-art methods. HP3M has been fully deployed on Fliggy, a leading OTP in China, serving thousands of hoteliers and tens of millions of users.}
}
