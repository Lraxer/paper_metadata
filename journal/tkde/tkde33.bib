@article{DBLP:journals/tkde/MancoRB21,
	author = {Giuseppe Manco and
                  Ettore Ritacco and
                  Nicola Barbieri},
	title = {A Factorization Approach for Survival Analysis on Diffusion Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {1--13},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2924369},
	doi = {10.1109/TKDE.2019.2924369},
	timestamp = {Thu, 23 Sep 2021 11:45:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MancoRB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a survival factorization framework that models information cascades by tying together social influence patterns, topical structure, and temporal dynamics. This is achieved through the introduction of a latent space which encodes: (a) the relevance of an information cascade on a topic; (b) the topical authoritativeness and the susceptibility of each individual involved in the information cascade, and (c) temporal topical patterns. By exploiting the cumulative properties of the survival function and of the likelihood of the model on a given adoption log, which records the observed activation times of users and side-information for each cascade, we show that the inference phase is linear in the number of users and in the number of adoptions. The evaluation on both synthetic and real-world data shows the effectiveness of the model in detecting the interplay between topics and social influence patterns, which ultimately provides high accuracy in predicting users activation times.}
}


@article{DBLP:journals/tkde/QiZJZ21,
	author = {Xiaodong Qi and
                  Zhao Zhang and
                  Cheqing Jin and
                  Aoying Zhou},
	title = {A Reliable Storage Partition for Permissioned Blockchain},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {14--27},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.3012668},
	doi = {10.1109/TKDE.2020.3012668},
	timestamp = {Mon, 02 Aug 2021 16:24:33 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QiZJZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The full-replication data storage mechanism, as commonly utilized in existing blockchains, is the barrier to the system's scalability, since it retains a copy of entire blockchain at each node so that the overall storage consumption per block is O(n) with n participants. Yet another drawback is that this mechanism may limit the throughput in permissioned blockchain. Moreover, due to the existence of Byzantine nodes, existing partitioning methods, though widely adopted in distributed systems for decades, cannot suit for blockchain systems directly, so that it is critical to devise new storage mechanism for blockchain systems. This article proposes a novel storage engine, called BFT-Store, to enhance storage scalability by integrating erasure coding with Byzantine Fault Tolerance (BFT) consensus protocol. The first property of BFT-store is that the storage consumption per block can be reduced to O(1) for the first time, which enlarges overall storage capability when more nodes attend the blockchain. Second, we design an efficient online re-encoding protocol for storage scale-out and a hybrid replication scheme to enhance reading performance. Analysis in theory and extensive experimental results illustrate the scalability, availability and efficiency of BFT-Store via the implementation in an open-source permissioned blockchain Tendermint.}
}


@article{DBLP:journals/tkde/SautotBJ21,
	author = {Lucile Sautot and
                  Sandro Bimonte and
                  Ludovic Journaux},
	title = {A Semi-Automatic Design Methodology for (Big) Data Warehouse Transforming
                  Facts into Dimensions},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {28--42},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2925621},
	doi = {10.1109/TKDE.2019.2925621},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SautotBJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A decision support system is used by decision makers for a long time. But, in some cases, the originally designed multidimensional schema does not cover the entire needs of decision makers, which can change over time. One such unfulfilled need, is using facts to describe dimension members. In this article, we propose a methodology to transform the constellation schema of a data warehouse by integrating factual data into a dimension. The proposed methodology and algorithms enrich a constellation multidimensional schema with new analytical possibilities for decision makers. This enrichment has repercussions for the entire multidimensional schema that are managed by multidimensional modeling, hierarchy calculation and the hierarchy version. In this article, we present a theoretical view of the proposed methodology supported by a case study, an implemented prototype and a complete evaluation based on a standard benchmark.}
}


@article{DBLP:journals/tkde/LiZ21,
	author = {Wei Li and
                  Hai Zhuge},
	title = {Abstractive Multi-Document Summarization Based on Semantic Link Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {43--54},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2922957},
	doi = {10.1109/TKDE.2019.2922957},
	timestamp = {Thu, 13 Jan 2022 07:48:25 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The key to realize advanced document summarization is semantic representation of documents. This paper investigates the role of Semantic Link Network in representing and understanding documents for multi-document summarization. It proposes a novel abstractive multi-document summarization framework by first transforming documents into a Semantic Link Network of concepts and events and then transforming the Semantic Link Network into the summary of the documents based on the selection of important concepts and events while keeping semantics coherence. Experiments on benchmark datasets show that the proposed summarization approach significantly outperforms relevant state-of-the-art baselines and the Semantic Link Network plays an important role in representing and understanding documents.}
}


@article{DBLP:journals/tkde/DautovD21,
	author = {Rustem Dautov and
                  Salvatore Distefano},
	title = {Automating IoT Data-Intensive Application Allocation in Clustered
                  Edge Computing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {55--69},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2923638},
	doi = {10.1109/TKDE.2019.2923638},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DautovD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enabling data processing at the network edge, as close to the actual source of data as possible, is a challenging, yet realistic goal to be achieved by the Internet of Things (IoT), which still primarily relies on the Cloud for data processing. By further extending the Fog and Edge computing principles, recent research advancements enabled aggregation of computing resources from multiple edge devices to support data-intensive task processing using Big Data clustering middleware. The use of these existing solutions, however, is hindered by the heterogeneous, dynamic, mobile, resource-constrained, and time-critical nature of IoT ecosystems. More specifically, a particularly challenging goal is to discover, select, and cluster suitable edge devices - on the one hand, and decompose and allocate data-intensive tasks with respect to discovered resources - on the other. To address this challenge, this paper introduces a novel decentralized architecture for clustering heterogeneous edge devices and executing data-intensive IoT workflows. The proposed approach first breaks down a complex workflow into simpler tasks, then discovers and selects suitable edge devices, and finally allocates the tasks to the selected nodes, connecting them to recompose the original workflow. The proposed approach benefits from an intelligent mapping algorithm that takes into account available cluster resources and processing demands to efficiently allocate fine-grained tasks to selected nodes. To support the clusterisation process, the proposed solution relies on a unified semantic knowledge base that provides a common vocabulary of terms for modelling task requirements and edge device properties, as well as enables automated task grouping and match-making for device discovery and selection, using built-in reasoning capabilities.}
}


@article{DBLP:journals/tkde/LeeHL21,
	author = {Roy Ka{-}Wei Lee and
                  Tuan{-}Anh Hoang and
                  Ee{-}Peng Lim},
	title = {Discovering Hidden Topical Hubs and Authorities Across Multiple Online
                  Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {70--84},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2922962},
	doi = {10.1109/TKDE.2019.2922962},
	timestamp = {Mon, 03 Jan 2022 21:57:03 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LeeHL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding influential users in online social networks (OSNs) is an important problem with many possible useful applications. Many methods have been proposed to identify influential users in OSNs. PageRank and HITs are two well known examples that determine influential users through link analysis. In recent years, new models that consider both content and social network links have been developed. The Hub and Authority Topic (HAT) model is one that extends HITS to identify topic-specific hubs and authorities by jointly learning hubs, authorities, and topical interests from users' relationship and textual content. However, many of the previous works are confined to identifying influential users within a single OSN. These models, when applied to multiple OSNs, could not learn influential users under a common set of topics nor address platform preferences. In this paper, we therefore propose the MPHAT model, an extension of HAT, to jointly model the topic-specific hub users, authority users, their topical interests and platform preferences. We evaluate MPHAT against several existing state-of-the-art methods in three tasks: (i) modeling of topics, (ii) platform choice prediction, and (iii) link recommendation. Based on our extensive experiments in multiple OSNs settings using synthetic datasets and real-world datasets from Twitter and Instagram, we show that MPHAT is comparable to state-of-the-art topic models in learning topics but outperforms the state-of-the-art models in platform prediction and link recommendation tasks. We also empirically demonstrate the ability of MPHAT to determine influential users within and across multiple OSNs.}
}


@article{DBLP:journals/tkde/LeeC21,
	author = {Jong{-}Ryul Lee and
                  Chin{-}Wan Chung},
	title = {Efficient Distance Sensitivity Oracles for Real-World Graph Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {85--99},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2924419},
	doi = {10.1109/TKDE.2019.2924419},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LeeC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A distance sensitivity oracle is a data structure answering queries that ask the shortest distance from a node to another in a network expecting node/edge failures. It has been mainly studied in theory literature, but all the existing oracles for a directed graph suffer from prohibitive preprocessing time and space. Motivated by this, we develop two practical distance sensitivity oracles for directed graphs as variants of Transit Node Routing. The first oracle consists of a novel fault-tolerant index structure, which is used to construct a solution path and to detect and localize the impact of network failures, and an efficient query algorithm for it. The second oracle is made by applying the A* heuristics to the first oracle, which exploits lower bound distances to effectively reduce search space. In addition, we propose additional speed-up techniques to make our oracles faster with a slight loss of accuracy. We conduct extensive experiments with real-life datasets, which demonstrate that our oracles greatly outperform all of competitors in most cases. To the best of our knowledge, our oracles are the first distance sensitivity oracles that handle real-world graph data with million-level nodes.}
}


@article{DBLP:journals/tkde/LiuHYCXSH21,
	author = {Qi Liu and
                  Zhenya Huang and
                  Yu Yin and
                  Enhong Chen and
                  Hui Xiong and
                  Yu Su and
                  Guoping Hu},
	title = {{EKT:} Exercise-Aware Knowledge Tracing for Student Performance Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {100--115},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2924374},
	doi = {10.1109/TKDE.2019.2924374},
	timestamp = {Fri, 02 Aug 2024 11:44:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuHYCXSH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For offering proactive services (e.g., personalized exercise recommendation) to the students in computer supported intelligent education, one of the fundamental tasks is predicting student performance (e.g., scores) on future exercises, where it is necessary to track the change of each student's knowledge acquisition during her exercising activities. Unfortunately, to the best of our knowledge, existing approaches can only exploit the exercising records of students, and the problem of extracting rich information existed in the materials (e.g., knowledge concepts, exercise content) of exercises to achieve both more precise prediction of student performance and more interpretable analysis of knowledge acquisition remains underexplored. To this end, in this paper, we present a holistic study of student performance prediction. To directly achieve the primary goal of performance prediction, we first propose a general Exercise-Enhanced Recurrent Neural Network (EERNN) framework by exploring both student's exercising records and the text content of corresponding exercises. In EERNN, we simply summarize each student's state into an integrated vector and trace it with a recurrent neural network, where we design a bidirectional LSTM to learn the encoding of each exercise from its content. For making final predictions, we design two implementations on the basis of EERNN with different prediction strategies, i.e., EERNNM with Markov property and EERNNA with Attention mechanism. Then, to explicitly track student's knowledge acquisition on multiple knowledge concepts, we extend EERNN to an explainable Exercise-aware Knowledge Tracing (EKT) framework by incorporating the knowledge concept information, where the student's integrated state vector is now extended to a knowledge state matrix. In EKT, we further develop a memory network for quantifying how much each exercise can affect the mastery of students on multiple knowledge concepts during the exercising process. Finally, we conduct extensive experiments and evaluate both EERNN and EKT frameworks on a large-scale real-world data. The results in both general and cold-start scenarios clearly demonstrate the effectiveness of two frameworks in student performance prediction as well as the superior interpretability of EKT.}
}


@article{DBLP:journals/tkde/AkcoraGKLT21,
	author = {Cuneyt Gurcan Akcora and
                  Yulia R. Gel and
                  Murat Kantarcioglu and
                  Vyacheslav Lyubchich and
                  Bhavani Thuraisingham},
	title = {GraphBoot: Quantifying Uncertainty in Node Feature Learning on Large
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {116--127},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2925355},
	doi = {10.1109/TKDE.2019.2925355},
	timestamp = {Sun, 06 Oct 2024 21:41:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/AkcoraGKLT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, as online social networks continue to grow in size, estimating node features, such as sociodemographics, preferences and health status, in a scalable and reliable way has become a primary research direction in social network mining. Although many techniques have been developed for estimating various node features, quantifying uncertainty in such estimations has received little attention. Furthermore, most existing methods study networks parametrically, which limits insights about necessary quantity of queried data, reliable feature estimation, and estimator uncertainty. Uncertainty quantification is critical for answering key questions, such as, given a limited availability of social network data, how much data should be queried from the network?, and which node features can be learned reliably? More importantly, how can we evaluate uncertainty of our estimators? Uncertainty quantification is not equivalent to network sampling but constitutes a key complementary concept to sampling and the associated reliability analysis. To our knowledge, this paper is the first work that sheds light on uncertainty quantification and uncertainty propagation in social network feature mining. We propose a novel non-parametric bootstrap method for uncertainty analysis of node features in social network mining, derive its asymptotic properties, and demonstrate its effectiveness with extensive experiments. Furthermore, we develop a new metric based on dispersion of estimations, enabling analysts to assess how much more information is needed for increasing prediction reliability based on the estimated uncertainty. We demonstrate the effectiveness of our new uncertainty quantification methodology with extensive experiments on real life social networks, and a case study of mental health on Twitter.}
}


@article{DBLP:journals/tkde/WangDTZT21,
	author = {Zengmao Wang and
                  Bo Du and
                  Weiping Tu and
                  Lefei Zhang and
                  Dacheng Tao},
	title = {Incorporating Distribution Matching into Uncertainty for Multiple
                  Kernel Active Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {128--142},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2923211},
	doi = {10.1109/TKDE.2019.2923211},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangDTZT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the lack of the labeled data and the complex structures of various data, it is very hard to learn the uncertainty and representativeness accurately in active learning. In this paper, we propose a multiple kernel active learning framework that incorporates a group regularizer of distribution information into the estimation of uncertainty. The proposed method takes the advantage of multiple kernel learning to learn the kernel space in which the complex structures can be well captured by kernel weights. Meanwhile, we have developed an efficient optimization algorithm to solve the proposed method. Experimental results on twelve UCI benchmark data sets and eight subsets of ImageNet show that the proposed method outperforms several state-of-the-art active learning methods. Moreover, we also have applied the proposed method to multiple feature scenario on Caltech101, and the promising results are also obtained compared with single feature scenario.}
}


@article{DBLP:journals/tkde/GaoTS21,
	author = {Shan Gao and
                  Ah{-}Hwee Tan and
                  Rossi Setchi},
	title = {Learning {ADL} Daily Routines with Spatiotemporal Neural Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {143--153},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2924623},
	doi = {10.1109/TKDE.2019.2924623},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GaoTS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Activities of daily living (ADLs) refer to the activities performed by individuals on a daily basis and are the indicators of a person's habits, lifestyle, and wellbeing. Consequently, learning an individual's ADL daily routines has significant value in the healthcare domain. Specifically, ADL recognition and inter-ADL pattern learning problems have been studied extensively in the past couple of decades. However, discovering the patterns of ADLs performed in a day and clustering them into ADL daily routines has been a relatively unexplored research area. In this paper, a self-organizing neural network model, called the Spatiotemporal ADL Adaptive Resonance Theory (STADLART), is proposed for learning ADL daily routines. STADLART integrates multimodal contextual information that involves the time and space wherein the ADLs are performed. By encoding spatiotemporal information explicitly as input features, STADLART enables the learning of time-sensitive knowledge. Moreover, a STADLART variation named STADLART-NC is proposed to normalize and customize ADL weighting for daily routine learning. A weighting assignment scheme is presented that facilitates the assignment of weighting according to ADL importance in specific domains. Empirical experiments using both synthetic and real-world public data sets validate the performance of STADLART and STADLART-NC when compared with alternative pattern discovery methods. The results show that STADLART could cluster ADL routines with better performance than baseline algorithms.}
}


@article{DBLP:journals/tkde/FangLZWSCL21,
	author = {Yuan Fang and
                  Wenqing Lin and
                  Vincent W. Zheng and
                  Min Wu and
                  Jiaqi Shi and
                  Kevin Chen{-}Chuan Chang and
                  Xiaoli Li},
	title = {Metagraph-Based Learning on Heterogeneous Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {154--168},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2922956},
	doi = {10.1109/TKDE.2019.2922956},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/FangLZWSCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data in the form of graphs are prevalent, ranging from biological and social networks to citation graphs and the Web. In particular, most real-world graphs are heterogeneous, containing objects of multiple types, which present new opportunities for many problems on graphs. Consider a typical proximity search problem on graphs, which boils down to measuring the proximity between two given nodes. Most earlier studies on homogeneous or bipartite graphs only measure a generic form of proximity, without accounting for different “semantic classes”—for instance, on a social network two users can be close for different reasons, such as being classmates or family members, which represent two distinct semantic classes. Learning these semantic classes are made possible on heterogeneous graphs through the concept of metagraphs . In this study, we identify metagraphs as a novel and effective means to characterize the common structures for a desired class of proximity. Subsequently, we propose a family of metagraph-based proximity, and employ a learning-to-rank technique that automatically learns the right parameters to suit the desired semantic class. In terms of efficiency, we develop a symmetry-based matching algorithm to speed up the computation of metagraph instances. Empirically, extensive experiments reveal that our metagraph-based proximity substantially outperforms the best competitor by more than 10 percent, and our matching algorithm can reduce matching time by more than half. As a further generalization, we aim to derive a general node and edge representation for heterogeneous graphs, in order to support arbitrary machine learning tasks beyond proximity search. In particular, we propose the finer-grained anchored metagraph , which is capable of discriminating the roles of nodes within the same metagraph. Finally, further experiments on the general representation show that we can outperform the state of the art significantly and consistently across various machine learning tasks.}
}


@article{DBLP:journals/tkde/YangXSDWZ21,
	author = {Yang Yang and
                  Yuhong Xu and
                  Yizhou Sun and
                  Yuxiao Dong and
                  Fei Wu and
                  Yueting Zhuang},
	title = {Mining Fraudsters and Fraudulent Strategies in Large-Scale Mobile
                  Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {169--179},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2924431},
	doi = {10.1109/TKDE.2019.2924431},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangXSDWZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of modern communication technologies-in particular, (mobile) phone communications-has largely facilitated human social interactions and information exchange. However, the emergence of telemarketing frauds can significantly dissipate individual fortune and social wealth, resulting in a potential slow down or damage to economics. In this work, we propose to spot telemarketing frauds, with an emphasis on unveiling the “precise fraud” phenomenon and the strategies that are used by fraudsters to precisely select targets. To study this problem, we employ a one-month complete dataset of telecommunication metadata in Shanghai with 54 million users and 698 million call logs. Through our study, we find that user's information might have been seriously leaked, and fraudsters have a preference over the target user's age and activity in mobile network. We further propose a novel semi-supervised learning framework to distinguish fraudsters from non-fraudsters. Experimental results on a real-world data show that our approach outperforms several state-of-the-art algorithms in accuracy of detecting fraudsters (e.g., +0.278 in terms of F1 on average). We believe that our study can potentially inform policymaking for government and mobile service providers.}
}


@article{DBLP:journals/tkde/YanYYLJC21,
	author = {Huan Yan and
                  Chunfeng Yang and
                  Donghan Yu and
                  Yong Li and
                  Depeng Jin and
                  Dah Ming Chiu},
	title = {Multi-Site User Behavior Modeling and Its Application in Video Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {180--193},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2926078},
	doi = {10.1109/TKDE.2019.2926078},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YanYYLJC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As online video service continues to grow in popularity, video content providers compete hard for more eyeball engagement. Some users visit multiple video sites to enjoy videos of their interest while some visit exclusively one site. However, due to the isolation of data, mining and exploiting user behaviors in multiple video websites remain unexplored so far. In this work, we try to model user preferences in six popular video websites with user viewing records obtained from a large ISP in China. The empirical study shows that users exhibit both consistent cross-site interests as well as site-specific interests. To represent this dichotomous pattern of user preferences, we propose a generative model of Multi-site Probabilistic Factorization (MPF) to capture both the cross-site as well as site-specific preferences. Besides, we discuss the design principle of our model by analyzing the sources of the observed site-specific user preferences, namely, site peculiarity and data sparsity. Through conducting extensive recommendation validation, we show that our MPF model achieves the best results compared to several other state-of-the-art factorization models with significant improvements of F-measure by 12.96, 8.24 and 6.88 percent, respectively. Our findings provide insights on the value of integrating user data from multiple sites, which stimulates collaboration between video service providers.}
}


@article{DBLP:journals/tkde/LiLHS21,
	author = {Jingjing Li and
                  Ke Lu and
                  Zi Huang and
                  Heng Tao Shen},
	title = {On Both Cold-Start and Long-Tail Recommendation with Social Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {194--208},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2924656},
	doi = {10.1109/TKDE.2019.2924656},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiLHS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The number of “hits” has been widely regarded as the lifeblood of many web systems, e.g., e-commerce systems, advertising systems and multimedia consumption systems. However, users would not hit an item if they cannot see it, or they are not interested in the item. Recommender system plays a critical role of discovering interesting items from near-infinite inventory and exhibiting them to potential users. Yet, two issues are crippling the recommender systems. One is “how to handle new users”, and the other is “how to surprise users”. The former is well-known as cold-start recommendation. In this paper, we show that the latter can be investigated as long-tail recommendation. We also exploit the benefits of jointly challenging both cold-start and long-tail recommendation, and propose a novel approach which can simultaneously handle both of them in a unified objective. For the cold-start problem, we learn from side information, e.g., user attributes, user social relationships, etc. Then, we transfer the learned knowledge to new users. For the long-tail recommendation, we decompose the overall interesting items into two parts: a low-rank part for short-head items and a sparse part for long-tail items. The two parts are independently revealed in the training stage, and transfered into the final recommendation for new users. Furthermore, we effectively formulate the two problems into a unified objective and present an iterative optimization algorithm. A fast extension of the method is proposed to reduce the complexity, and extensive theoretical analysis are provided to proof the bounds of our approach. At last, experiments of social recommendation on various real-world datasets, e.g., images, blogs, videos and musics, verify the superiority of our approach compared with the state-of-the-art work.}
}


@article{DBLP:journals/tkde/CivitareseSRBS21,
	author = {Gabriele Civitarese and
                  Timo Sztyler and
                  Daniele Riboni and
                  Claudio Bettini and
                  Heiner Stuckenschmidt},
	title = {{POLARIS:} Probabilistic and Ontological Activity Recognition in Smart-Homes},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {209--223},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2930050},
	doi = {10.1109/TKDE.2019.2930050},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CivitareseSRBS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recognition of activities of daily living (ADLs) is an enabling technology for several ubiquitous computing applications. Most activity recognition systems rely on supervised learning to extract activity models from labeled datasets. A problem with that approach is the acquisition of comprehensive activity datasets, which is an expensive task. The problem is particularly challenging when focusing on complex ADLs characterized by large variability of execution. Moreover, several activity recognition systems are limited to offline recognition, while many applications claim for online activity recognition. In this paper, we propose POLARIS, a framework for unsupervised activity recognition. POLARIS can recognize complex ADLs exploiting the semantics of activities, context data, and sensors. Through ontological reasoning, our algorithm derives semantic correlations among activities and sensor events. By matching observed events with semantic correlations, a statistical reasoner formulates initial hypotheses about the occurred activities. Those hypotheses are refined through probabilistic reasoning, exploiting semantic constraints derived from the ontology. Our system supports online recognition, thanks to a novel segmentation algorithm. Extensive experiments with real-world datasets show that the accuracy of our unsupervised method is comparable to the one of supervised approaches. Moreover, the online version of our system achieves essentially the same accuracy of the offline version.}
}


@article{DBLP:journals/tkde/ZhangSSQL21,
	author = {Zhongbao Zhang and
                  Li Sun and
                  Sen Su and
                  Jielun Qu and
                  Gen Li},
	title = {Reconciling Multiple Social Networks Effectively and Efficiently:
                  An Embedding Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {224--238},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2929786},
	doi = {10.1109/TKDE.2019.2929786},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangSSQL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, reconciling social networks, identifying the accounts belonging to the same individual across social networks, receives significant attention from both academic and industry. Most of the existing studies have limitations in the following three aspects: multiplicity, comprehensiveness and robustness. To address these limitations, we rethink this problem and, for the first time, robustly and comprehensively reconcile multiple social networks. In this paper, we propose two frameworks, MASTER and MASTER+, i.e., across Multiple social networks, integrate Attribute and STructure Embedding for Reconciliation. In MASTER, we first design a novel Constrained Dual Embedding model, simultaneously embedding and reconciling multiple social networks, to formulate this problem into a unified optimization. To address this optimization, we then design an effective NS-Alternating algorithm and prove it converges to KKT points. To further speed up MASTER, we propose a scalable framework, namely MASTER+. The core idea is to group accounts into clusters and then perform MASTER in each cluster in parallel. Specifically, we design an efficient Augmented Pre-Embedding model and Balance-aware Fuzzy Clustering algorithm for the high efficiency and the high accuracy. Extensive experiments demonstrate that both MASTER and MASTER+ outperform the state-of-the-art approaches. Moreover, MASTER+ inherits the effectiveness of MASTER and enjoys higher efficiency.}
}


@article{DBLP:journals/tkde/SamaraweeraC21,
	author = {G. Dumindu Samaraweera and
                  J. Morris Chang},
	title = {Security and Privacy Implications on Database Systems in Big Data
                  Era: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {239--258},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2929794},
	doi = {10.1109/TKDE.2019.2929794},
	timestamp = {Mon, 28 Aug 2023 21:37:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SamaraweeraC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For over many decades, relational database model has been considered as the leading model for data storage and management. However, as the Big Data explosion has generated a large volume of data, alternative models like NoSQL and NewSQL have emerged. With the advancement of communication technology, these database systems have given the potential to change the existing architecture from centralized mechanism to distributed in nature, to deploy as cloud-based solutions. Though all of these evolving technologies mostly focus on performance guarantees, it is still being a major concern how these systems can ensure the security and privacy of the information they handle. Different datastores support different types of integrated security mechanisms, however, most of the non-relational database systems have overlooked the security requirements of modern Big Data applications. This paper reviews security implementations in today's leading database models giving more emphasis on security and privacy attributes. A set of standard security mechanisms have been identified and evaluated based on different security classifications. Further, it provides a thorough review and a comprehensive analysis on maturity of security and privacy implementations in these database models along with future directions/enhancements so that data owners can decide on most appropriate datastore for their data-driven Big Data applications.}
}


@article{DBLP:journals/tkde/ZhaoZLTGY21,
	author = {Yawei Zhao and
                  En Zhu and
                  Xinwang Liu and
                  Chang Tang and
                  Deke Guo and
                  Jianping Yin},
	title = {Simultaneous Clustering and Optimization for Evolving Datasets},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {259--270},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2923239},
	doi = {10.1109/TKDE.2019.2923239},
	timestamp = {Mon, 12 Feb 2024 10:23:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoZLTGY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Simultaneous clustering and optimization (SCO) has recently drawn much attention due to its wide range of practical applications. Many methods have been previously proposed to solve this problem and obtain the optimal model. However, when a dataset evolves over time, those existing methods have to update the model frequently to guarantee accuracy; such updating is computationally infeasible. In this paper, we propose a new formulation of SCO to handle evolving datasets. Specifically, we propose a new variant of the alternating direction method of multipliers (ADMM) to solve this problem efficiently. The guarantee of model accuracy is analyzed theoretically for two specific tasks: ridge regression and convex clustering. Extensive empirical studies confirm the effectiveness of our method.}
}


@article{DBLP:journals/tkde/LiuYXPLGMF21,
	author = {Jinfei Liu and
                  Juncheng Yang and
                  Li Xiong and
                  Jian Pei and
                  Jun Luo and
                  Yuzhang Guo and
                  Shuaicheng Ma and
                  Chenglin Fan},
	title = {Skyline Diagram: Efficient Space Partitioning for Skyline Queries},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {271--286},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2923914},
	doi = {10.1109/TKDE.2019.2923914},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuYXPLGMF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Skyline queries are important in many application domains. In this paper, we propose a novel structure Skyline Diagram, which given a set of points, partitions the plane into a set of regions, referred to as skyline polyominos. All query points in the same skyline polyomino have the same skyline query results. Similar to kth-order Voronoi diagram commonly used to facilitate k nearest neighbor (kNN) queries, skyline diagram can be used to facilitate skyline queries and many other applications. However, it may be computationally expensive to build the skyline diagram. By exploiting some interesting properties of skyline, we present several efficient algorithms for building the diagram with respect to three kinds of skyline queries, quadrant, global, and dynamic skylines. In addition, we propose an approximate skyline diagram which can significantly reduce the space cost. Experimental results on both real and synthetic datasets show that our algorithms are efficient and scalable.}
}


@article{DBLP:journals/tkde/KuoCC21,
	author = {Li{-}Yen Kuo and
                  Chung{-}Kuang Chou and
                  Ming{-}Syan Chen},
	title = {The Framework of Personalized Ranking on Poisson Factorization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {287--301},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2924894},
	doi = {10.1109/TKDE.2019.2924894},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/KuoCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Matrix factorization (MF) has earned great success on recommender systems. However, the common-used regression-based MF not only is sensitive to outliers but also unable to guarantee that the predicted values are in line with the user preference orders, which is the basis of common measures of recommender systems, e.g., nDCG. To overcome the aforementioned drawbacks, we propose a framework for personalized ranking of Poisson factorization that utilizes learning-to-rank based posteriori instead of the classical regression-based ones. Owing to the combination, the proposed framework not only preserves user preference but also performs well on a sparse matrix. Since the posteriori that combines learning to rank and Poisson factorization does not follow the conjugate prior relationship, we estimate variational parameters approximately and propose two optimization approaches based on variational inference. As long as the used learning-to-rank model has the 1st and 2nd order partial derivatives, by exploiting our framework, the proposed optimizing algorithm can maximize the posteriori whichever the used learning-to-rank model is. In the experiment, we show that the proposed framework outperforms the state-of-the-art methods and achieves promising results on consuming log and rating datasets for multiple recommendation tasks.}
}


@article{DBLP:journals/tkde/DoLFT21,
	author = {Quan Do and
                  Wei Liu and
                  Jin Fan and
                  Dacheng Tao},
	title = {Unveiling Hidden Implicit Similarities for Cross-Domain Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {1},
	pages = {302--315},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2923904},
	doi = {10.1109/TKDE.2019.2923904},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DoLFT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {E-commerce businesses are increasingly dependent on recommendation systems to introduce personalized services and products to targeted customers. Providing effective recommendations requires sufficient knowledge about user preferences and product (item) characteristics. Given the current abundance of available data across domains, achieving a thorough understanding of the relationship between users and items can bring in more collaborative filtering power and lead to a higher recommendation accuracy. However, how to effectively utilize different types of knowledge obtained across domains is still a challenging problem. In this paper, we propose to discover both explicit and implicit similarities from latent factors across domains based on matrix tri-factorization. In our research, common factors in a shared dimension (users or items) of two coupled matrices are discovered, while at the same time, domain-specific factors of the shared dimension are also preserved. We will show that such preservation of both common and domain-specific factors are significantly beneficial to cross-domain recommendations. Moreover, on the non-shared dimension, we propose to use the middle matrix of the tri-factorization to match the unique factors, and align the matched unique factors to transfer cross-domain implicit similarities and thus further improve the recommendation. This research is the first that proposes the transfer of knowledge across the non-shared (non-coupled) dimensions. Validated on real-world datasets, our approach outperforms existing algorithms by more than two times in terms of recommendation accuracy. These empirical results illustrate the potential of utilizing both explicit and implicit similarities for making across-domain recommendations.}
}


@article{DBLP:journals/tkde/LiWWYCL21,
	author = {Liang Li and
                  Guoren Wang and
                  Gang Wu and
                  Ye Yuan and
                  Lei Chen and
                  Xiang Lian},
	title = {A Comparative Study of Consistent Snapshot Algorithms for Main-Memory
                  Database Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {316--330},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2930987},
	doi = {10.1109/TKDE.2019.2930987},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiWWYCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-memory databases (IMDBs) are gaining increasing popularity in big data applications, where clients commit updates intensively. Specifically, it is necessary for IMDBs to have efficient snapshot performance to support certain special applications (e.g., consistent checkpoint, HTAP). Formally, the in-memory consistent snapshot problem refers to taking an in-memory consistent time-in-point snapshot with the constraints that 1) clients can read the latest data items and 2) any data item in the snapshot should not be overwritten. Various snapshot algorithms have been proposed in academia to trade off throughput and latency, but industrial IMDBs such as Redis adhere to the simple fork algorithm. To understand this phenomenon, we conduct comprehensive performance evaluations on mainstream snapshot algorithms. Surprisingly, we observe that the simple fork algorithm indeed outperforms the state-of-the-arts in update-intensive workload scenarios. On this basis, we identify the drawbacks of existing research and propose two lightweight improvements. Extensive evaluations on synthetic data and Redis show that our lightweight improvements yield better performance than fork, the current industrial standard, and the representative snapshot algorithms from academia. Finally, we have opensourced the implementation of all the above snapshot algorithms so that practitioners are able to benchmark the performance of each algorithm and select proper methods for different application scenarios.}
}


@article{DBLP:journals/tkde/DingWCJ21,
	author = {Xiaofeng Ding and
                  Cui Wang and
                  Kim{-}Kwang Raymond Choo and
                  Hai Jin},
	title = {A Novel Privacy Preserving Framework for Large Scale Graph Data Publishing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {331--343},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931903},
	doi = {10.1109/TKDE.2019.2931903},
	timestamp = {Mon, 28 Aug 2023 21:37:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DingWCJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The need to efficiently store and query large scale graph datasets is evident in the growing number of data-intensive applications, particularly to maximize the mining of intelligence from these data (e.g., to inform decision making). However, directly releasing graph dataset for analysis may leak sensitive information of an individual even if the graph is anonymized, as demonstrated by the re-identification attacks on the DBpedia datasets. A key challenge in the design of graph sanitization methods is scalability, as existing execution models generally have significant memory requirements. In this paper, we propose a novel k-decomposition algorithm and define a new information loss matrix designed for utility measurement in massively large graph datasets. We also propose a novel privacy preserving framework that can be seamlessly integrated with graph storage, anonymization, query processing, and analysis. Our experimental studies show that the proposed solution achieves privacy-preserving, utility, and efficiency.}
}


@article{DBLP:journals/tkde/WuFLH21,
	author = {Chenggang Wu and
                  Jose M. Faleiro and
                  Yihan Lin and
                  Joseph M. Hellerstein},
	title = {Anna: {A} {KVS} for Any Scale},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {344--358},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2898401},
	doi = {10.1109/TKDE.2019.2898401},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuFLH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern cloud providers offer dense hardware with multiple cores and large memories, hosted in global platforms. This raises the challenge of implementing high-performance software systems that can effectively scale from a single core to multicore to the globe. Conventional wisdom says that software designed for one scale point needs to be rewritten when scaling up by 10 - 100× [1]. In contrast, we explore how a system can be architected to scale across many orders of magnitude by design. We explore this challenge in the context of a new key-value store system called Anna: a partitioned, multi-mastered system that achieves high performance and elasticity via wait-free execution and coordination-free consistency. Our design rests on a simple architecture of coordination-free actors that perform state update via merge of lattice-based composite data structures. We demonstrate that a wide variety of consistency models can be elegantly implemented in this architecture with unprecedented consistency, smooth fine-grained elasticity, and performance that far exceeds the state of the art.}
}


@article{DBLP:journals/tkde/HuL21,
	author = {Huan Hu and
                  Jianzhong Li},
	title = {Bit-Oriented Sampling for Aggregation on Big Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {359--373},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931014},
	doi = {10.1109/TKDE.2019.2931014},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The efficiency of big data analysis has become a bottleneck. Aggregation is a fundamental analytical task. It usually consumes a lot of time so that sampling based aggregation is often used to improve response time at a loss of result accuracy. In all of the related works, sampling is conducted at the granularity of data item. Considering the bits at different bit positions of each data item have different contributions to an aggregation result, the performance of sampling based aggregation has a chance of being improved if sampling is conducted at the granularity of bit. Thus, this paper studies bit-oriented sampling for aggregation. Two methods of bit-oriented uniform sampling based aggregation, i.e., DVBM and DVFM, are proposed which are based on the central limit theorem or the Chebyshev's inequality. They are much more efficient than the methods of the traditional data-oriented uniform sampling based aggregation. DVBM can guarantee a given error bound of aggregation with the assumption that sample variance equals dataset variance. By contrast, DVFM achieves the same goal without that assumption, but it could result in a larger sampling size. Extensive experiments are carried out and the results show that DVBM and DVFM are both efficient and effective.}
}


@article{DBLP:journals/tkde/ChengZHWY21,
	author = {Dongdong Cheng and
                  Qingsheng Zhu and
                  Jinlong Huang and
                  Quanwang Wu and
                  Lijun Yang},
	title = {Clustering with Local Density Peaks-Based Minimum Spanning Tree},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {374--387},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2930056},
	doi = {10.1109/TKDE.2019.2930056},
	timestamp = {Sun, 02 Oct 2022 15:51:31 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChengZHWY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustering analysis has been widely used in statistics, machine learning, pattern recognition, image processing, and so on. It is a great challenge for most existing clustering algorithms to discover clusters with arbitrary shapes. Clustering algorithms based on Minimum spanning tree (MST) are able to discover clusters with arbitrary shapes, but they are time consuming and susceptible to noise points. In this paper, we employ local density peaks (LDP) to represent the whole data set and define a shared neighbors-based distance between local density peaks to better measure the dissimilarity between objects on manifold data. On the basis of local density peaks and the new distance, we propose a novel MST-based clustering algorithm called LDP-MST. It first uses local density peaks to construct MST and then repeatedly cuts the longest edge until a given number of clusters are found. The experimental results on synthetic data sets and real data sets show that our algorithm is competent with state-of-the-art methods when discovering clusters with complex structures.}
}


@article{DBLP:journals/tkde/MaQZHJH21,
	author = {Renfeng Ma and
                  Xipeng Qiu and
                  Qi Zhang and
                  Xiangkun Hu and
                  Yu{-}Gang Jiang and
                  Xuanjing Huang},
	title = {Co-Attention Memory Network for Multimodal Microblog's Hashtag Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {388--400},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2932406},
	doi = {10.1109/TKDE.2019.2932406},
	timestamp = {Thu, 23 Mar 2023 16:22:21 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MaQZHJH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hashtags are keywords describing a topic or a theme and are usually chosen by microblogging users. Hence, the hashtags can be used to categorize microblog posts. With the fast development of the social network, the task of recommending suitable hashtags has received considerable attention in recent years. Recently, most neural network methods have treated the task as a multi-class classification problem. In fact, users are constantly introducing new hashtags in a highly dynamic way. Treating the task as a multi-class classification problem with a fixed number of target categories does not allow the method to deal with the new hashtags. To address this problem, the task is reinterpreted as a matching problem and a novel co-attention memory network is proposed to represent the multimodal microblogs and hashtags. We utilize a co-attention mechanism to model the multimodal mircroblogs, and utilize the post history to represent the hashtags. Experimental results on a Twitter-based dataset demonstrated that the proposed method can achieve better performance than the current state-of-the-art methods that treat the task as a multi-class classification problem.}
}


@article{DBLP:journals/tkde/GeYL21,
	author = {Yong Ge and
                  Zijun Yao and
                  Huayu Li},
	title = {Computing Co-Location Patterns in Spatial Data with Extended Objects:
                  {A} Scalable Buffer-Based Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {401--414},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2930598},
	doi = {10.1109/TKDE.2019.2930598},
	timestamp = {Wed, 10 Nov 2021 12:34:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GeYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial co-location patterns are subsets of spatial features usually located together in geographic space. Recent literature has provided different approaches to discover co-location patterns over point spatial data. However, most approaches consider the neighborhood relationship among spatial objects as binary and are mainly designed for point spatial features, thus are not appropriate for extended spatial features such as line strings and polygons, the neighborhood relationship among which is naturally continuous. This paper adopts a buffer-based model for measuring the spatial relationship of extended objects and mining co-location patterns. While the buffer-based model has several advantages for extended spatial features, it involves high computational complexity due to the expensive buffer-level overlay operation. To tackle this challenge, we introduce a coarse-level co-location mining framework, which follows a filter-and-refine paradigm. Within the framework, we develop a serious of rigorous upper bounds based on geometric property and progressively prune search space with these upper bounds. Moreover, we develop a join-less schema to further reduce computation cost of size-k(\nk>2\n) co-location patterns. Finally, we conduct experiments with large-scale spatial data to validate the efficiency of the developed algorithms against several state-of-art methods. All experimental results demonstrate the superiority of our methods.}
}


@article{DBLP:journals/tkde/CormodeKS21,
	author = {Graham Cormode and
                  Tejas Kulkarni and
                  Divesh Srivastava},
	title = {Constrained Private Mechanisms for Count Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {415--430},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2912179},
	doi = {10.1109/TKDE.2019.2912179},
	timestamp = {Thu, 27 Jul 2023 08:18:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CormodeKS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Concern about how to aggregate sensitive user data without compromising individual privacy is a major barrier to greater availability of data. Differential privacy has emerged as an accepted model to release sensitive information while giving a statistical guarantee for privacy. Many different algorithms are possible to address different target functions. We focus on the core problem of count queries, and seek to design mechanisms to release data associated with a group of\nn\nindividuals. Prior work has focused on designing mechanisms by raw optimization of a loss function, without regard to the consequences on the results. This can lead to mechanisms with undesirable properties, such as never reporting some outputs (gaps), and overreporting others (spikes). We tame these pathological behaviors by introducing a set of desirable properties that mechanisms can obey. Any combination of these can be satisfied by solving a linear program (LP) which minimizes a cost function, with constraints enforcing the properties. We focus on a particular cost function, and provide explicit constructions that are optimal for certain combinations of properties, and show a closed form for their cost. In the end, there are only a handful of distinct optimal mechanisms to choose between: one is the well-known (truncated) geometric mechanism; the second a novel mechanism that we introduce here, and the remainder are found as the solution to particular LPs. These all avoid the bad behaviors we identify. We demonstrate in a set of experiments on real and synthetic data which is preferable in practice, for different combinations of data distributions, constraints, and privacy parameters.}
}


@article{DBLP:journals/tkde/HeLZWH21,
	author = {Zhicheng He and
                  Jie Liu and
                  Yuyuan Zeng and
                  Lai Wei and
                  Yalou Huang},
	title = {Content to Node: Self-Translation Network Embedding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {431--443},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2932388},
	doi = {10.1109/TKDE.2019.2932388},
	timestamp = {Mon, 28 Jun 2021 14:42:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HeLZWH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper concerns the problem of network embedding (NE), which aims to learn low-dimensional representations for network nodes. Such dense representations offer great promises for many network analysis problems. However, existing approaches are still faced with challenges posed by the characteristics of complex real-world networks. First, for networks associated with rich content information, previous methods often learn separated content and structure representations, which requires post-processing of combination. Empirical combination strategies often make the final vectors suboptimal. Second, existing methods preserve the structure information by considering short and fixed neighborhood scope, such as the first- and/or the second-order proximities. However, it is hard to decide the neighborhood scope in complex problems. To this end, we propose a novel sequence to sequence model based NE framework referred to as Self-Translation Network Embedding (STNE). With the sampled node sequences, STNE translates each sequence itself from the content sequence to the node sequence. On the one hand, the bi-directional LSTM encoder fuses the content and structure information seamlessly from the raw input. On the other hand, high-order proximity can be flexibly learned with the memories of LSTM to capture long-range structural information. Experimental results on three real-world datasets demonstrate the superiority of STNE.}
}


@article{DBLP:journals/tkde/WangFCLJL21,
	author = {Yue Wang and
                  Zonghao Feng and
                  Lei Chen and
                  Zijian Li and
                  Xun Jian and
                  Qiong Luo},
	title = {Efficient Similarity Search for Sets over Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {444--458},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931901},
	doi = {10.1109/TKDE.2019.2931901},
	timestamp = {Wed, 14 Jun 2023 18:52:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangFCLJL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Measuring similarities among different nodes is important in graph analysis tasks, such as link prediction, and recommendation. Among different similarity measures, SimRank is one of the most popular and promising ones, and has received a lot of research attention. While most current studies focus on single-pair, single-source/top-k, and all-pairs SimRank computation, few of them have studied finding similar pairs given a set of node pairs, which has attractive applications in personalized search and recommendation tasks. In this paper, we present Carmo, an efficient algorithm for retrieving the top-k similarities from an arbitrary set of pairs. In addition, we introduce two types of indexes to boost the efficiency of Carmo: one is hub-based, the other is tree-based. We show the effectiveness and efficiency of our proposed methods by extensive experiments.}
}


@article{DBLP:journals/tkde/WuWGZZCL21,
	author = {Yuncheng Wu and
                  Ke Wang and
                  Ruoyang Guo and
                  Zhilin Zhang and
                  Dan Zhao and
                  Hong Chen and
                  Cuiping Li},
	title = {Enhanced Privacy Preserving Group Nearest Neighbor Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {459--473},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2930696},
	doi = {10.1109/TKDE.2019.2930696},
	timestamp = {Mon, 19 Aug 2024 15:02:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuWGZZCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Group\nk\n-nearest neighbor (\nk\nGNN) search allows a group of\nn\nmobile users to jointly retrieve\nk\npoints from a location-based service provider (LSP) that minimizes the aggregate distance to them. We identify four protection objectives in the privacy preserving\nk\nGNN search: (i) every user's location should be protected from LSP; (ii) the group's query and the query answer should be protected from LSP; (iii) LSP's private database information should be protected from users; (iv) every user's location should be protected from other users in the group. We design two privacy preserving solutions under two types of threat model to the privacy preserving\nk\nGNN search in the full user collusion environment, where any\nn−1\nusers in the group may collude to infer the location of the remaining user. Our solutions do not rely on heavy pre-computation on LSP like previous works. Though we consider\nk\nGNN, the proposed privacy preserving solutions can be easily adopted to any group query as it treats the query answering (i.e.,\nk\nGNN) as a black box. Theoretical and experimental analysis suggest that our solutions are highly efficient in both communication cost and user computational cost while incurring some reasonable overhead on LSP.}
}


@article{DBLP:journals/tkde/ChengYCGWL21,
	author = {Yurong Cheng and
                  Ye Yuan and
                  Lei Chen and
                  Christophe G. Giraud{-}Carrier and
                  Guoren Wang and
                  Boyang Li},
	title = {Event-Participant and Incremental Planning over Event-Based Social
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {474--488},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931906},
	doi = {10.1109/TKDE.2019.2931906},
	timestamp = {Sun, 02 Oct 2022 15:51:31 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChengYCGWL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, online Event Based Social Network (EBSN) platforms have become increasingly popular. One typical task of EBSN platforms is to help users make suitable and personalized plans for participating in different interesting social events. Existing techniques either ignore the minimum-participant requirement constraint for each event, which is crucially needed for some events to be held successfully, or assume that events would not change once announced. In this paper, we address the above inadequacies of existing EBSN techniques. We formally define the Global Event Planning with Constraints (GEPC) problem, and its incremental variant. Since these problems are NP-hard, and provide approximate solutions. Finally, we verify the effectiveness and efficiency of our proposed algorithms through extensive experiments over real and synthetic datasets.}
}


@article{DBLP:journals/tkde/QinXWWLIW21,
	author = {Jianbin Qin and
                  Chuan Xiao and
                  Yaoshu Wang and
                  Wei Wang and
                  Xuemin Lin and
                  Yoshiharu Ishikawa and
                  Guoren Wang},
	title = {Generalizing the Pigeonhole Principle for Similarity Search in Hamming
                  Space},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {489--505},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2899597},
	doi = {10.1109/TKDE.2019.2899597},
	timestamp = {Wed, 27 Jan 2021 15:14:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/QinXWWLIW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A distance search in Hamming space finds binary vectors whose Hamming distances are no more than a threshold from a query vector. It is a fundamental problem in many applications, such as image retrieval, near-duplicate Web page detection, and scientific databases. State-of-the-art approaches to Hamming distance search are mainly based on the pigeonhole principle to generate a set of candidates and then verify them. We observe that the constraint by the pigeonhole principle is not always tight and may bring about unnecessary candidates. We also observe that the distribution in real data is often skewed, but most existing solutions adopt a simple equi-width partitioning and allocate the same threshold to all the parts, hence failing to exploit the data skewness to optimize query processing. In this paper, we propose a new form of the pigeonhole principle which allows variable partitioning and threshold allocation. Based on the new principle, we develop a tight constraint of candidates and devise cost-aware methods for partitioning and threshold allocation to optimize query processing. In addition, we extend our methods to answer Hamming distance join queries. We also discuss the application of the pigeonhole principle in set similarity search, a problem that can be converted to Hamming distance search equivalently. Our evaluation on datasets with various data distributions shows the robustness of our solution and its superior query processing performance to the state-of-the-art methods.}
}


@article{DBLP:journals/tkde/JiangXS21,
	author = {Zhe Jiang and
                  Miao Xie and
                  Arpan Man Sainju},
	title = {Geographical Hidden Markov Tree},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {506--520},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2930518},
	doi = {10.1109/TKDE.2019.2930518},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/JiangXS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a spatial raster framework with explanatory feature layers, a spatial contextual layer (e.g., a potential field), as well as a set of training samples with class labels, the spatial prediction problem aims to learn a model that can predict a class layer. The problem is important in societal applications such as flood extent mapping for disaster response and national water forecasting, but is challenging due to the noise, obstacles, and heterogeneity in feature maps, implicit spatial dependency between locations based on the contextual layer (e.g., gradient directions on a potential field), and the large number of sample locations. Existing work often assumes undirected spatial dependency, or directed dependency with a total order, and thus cannot reflect complex directed dependency with a partial order. In contrast, we recently proposed geographical hidden Markov tree, a probabilistic graphical model that generalizes the common hidden Markov model from a one-dimensional sequence to a two-dimensional map. Partial order class dependency is incorporated in the hidden class layer with a reverse tree structure. We also investigated computational algorithms for reverse tree construction, model parameter learning and class inference. This paper extends our recent model with overlaying class nodes between observation nodes and underlying hidden class nodes. The additional overlaying class layer makes the model more robust to large scale feature obstacles. We also proposed corresponding learning and inference methods. Extensive evaluations on real world datasets show that our models outperform multiple baselines in flood mapping applications, our algorithms are scalable on large data sizes, and the proposed extension enhances classification performance.}
}


@article{DBLP:journals/tkde/LyuFWLL21,
	author = {Gengyu Lyu and
                  Songhe Feng and
                  Tao Wang and
                  Congyan Lang and
                  Yidong Li},
	title = {{GM-PLL:} Graph Matching Based Partial Label Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {521--535},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2933837},
	doi = {10.1109/TKDE.2019.2933837},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LyuFWLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Partial Label Learning (PLL) aims to learn from the data where each training example is associated with a set of candidate labels, among which only one is correct. The key to deal with such problem is to disambiguate the candidate label sets and obtain the correct assignments between instances and their candidate labels. In this paper, we interpret such assignments as instance-to-label matchings, and reformulate the task of PLL as a matching selection problem. To model such problem, we propose a novel Graph Matching based Partial Label Learning (GM-PLL) framework, where Graph Matching (GM) scheme is incorporated owing to its excellent capability of exploiting the instance and label relationship. Meanwhile, since conventional one-to-one GM algorithm does not satisfy the constraint of PLL problem that multiple instances may correspond to the same label, we extend a traditional one-to-one probabilistic matching algorithm to the many-to-one constraint, and make the proposed framework accommodate to the PLL problem. Moreover, we also propose a relaxed matching prediction model, which can improve the prediction accuracy via GM strategy. Extensive experiments on both artificial and real-world data sets demonstrate that the proposed method can achieve superior or comparable performance against the state-of-the-art methods.}
}


@article{DBLP:journals/tkde/MinLZ21,
	author = {Wenwen Min and
                  Juan Liu and
                  Shihua Zhang},
	title = {Group-Sparse {SVD} Models via {\textdollar}L{\_}1{\textdollar}L1-
                  and {\textdollar}L{\_}0{\textdollar}L0-norm Penalties and their Applications
                  in Biological Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {536--550},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2932063},
	doi = {10.1109/TKDE.2019.2932063},
	timestamp = {Wed, 04 Aug 2021 07:41:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MinLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparse Singular Value Decomposition (SVD) models have been proposed for biclustering high dimensional gene expression data to identify block patterns with similar expressions. However, these models do not take into account prior group effects upon variable selection. To this end, we first propose group-sparse SVD models with group Lasso (GL 1 -SVD) and group L 0 -norm penalty (GL 0 -SVD) for non-overlapping group structure of variables. However, such group-sparse SVD models limit their applicability in some problems with overlapping structure. Thus, we also propose two group-sparse SVD models with overlapping group Lasso (OGL 1 -SVD) and overlapping group L 0 -norm penalty (OGL 0 -SVD). We first adopt an alternating iterative strategy to solve GL 1 -SVD based on a block coordinate descent method, and GL 0 -SVD based on a projection method. The key of solving OGL 1 -SVD is a proximal operator with overlapping group Lasso penalty. We employ an alternating direction method of multipliers (ADMM) to solve the proximal operator. Similarly, we develop an approximate method to solve OGL 0 -SVD. Applications of these methods and comparison with competing ones using simulated data demonstrate their effectiveness. Extensive applications of them onto several real gene expression data with gene prior group knowledge identify some biologically interpretable gene modules.}
}


@article{DBLP:journals/tkde/LuoCCGLNZ21,
	author = {Zhaojing Luo and
                  Shaofeng Cai and
                  Gang Chen and
                  Jinyang Gao and
                  Wang{-}Chien Lee and
                  Kee Yuan Ngiam and
                  Meihui Zhang},
	title = {Improving Data Analytics with Fast and Adaptive Regularization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {551--568},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2916683},
	doi = {10.1109/TKDE.2019.2916683},
	timestamp = {Wed, 15 Dec 2021 12:36:56 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LuoCCGLNZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep Learning and Machine Learning models have recently been shown to be effective in many real world applications. While these models achieve increasingly better predictive performance, their structures have also become much more complex. A common and difficult problem for complex models is overfitting. Regularization is used to penalize the complexity of the model in order to avoid overfitting. However, in most learning frameworks, regularization function is usually set with some hyper-parameters where the best setting is difficult to find. In this paper, we propose an adaptive regularization method, as part of a large end-to-end healthcare data analytics software stack, which effectively addresses the above difficulty. First, we propose a general adaptive regularization method based on Gaussian Mixture (GM) to learn the best regularization function according to the observed parameters. Second, we develop an effective update algorithm which integrates Expectation Maximization (EM) with Stochastic Gradient Descent (SGD). Third, we design a lazy update and sparse update algorithm to reduce the computational cost by 4x and 20x, respectively. The overall regularization framework is fast, adaptive, and easy-to-use. We validate the effectiveness of our regularization method through an extensive experimental study over 14 standard benchmark datasets and three kinds of deep learning/machine learning models. The results illustrate that our proposed adaptive regularization method achieves significant improvement over state-of-the-art regularization methods.}
}


@article{DBLP:journals/tkde/ChenTZWL21,
	author = {Dan Chen and
                  Yunbo Tang and
                  Hao Zhang and
                  Lizhe Wang and
                  Xiaoli Li},
	title = {Incremental Factorization of Big Time Series Data with Blind Factor
                  Approximation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {569--584},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931687},
	doi = {10.1109/TKDE.2019.2931687},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenTZWL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extracting the latent factors of big time series data is an important means to examine the dynamic complex systems under observation. These low-dimensional and “small” representations reveal the key insights to the overall mechanisms, which can otherwise be obscured by the notoriously high dimensionality and scale of big data as well as the enormously complicated interdependencies amongst data elements. However, grand challenges still remain: (1) to incrementally derive the multi-mode factors of the augmenting big data and (2) to achieve this goal under the circumstance of insufficient a priori knowledge. This study develops an incrementally parallel factorization solution (namely I-PARAFAC) for huge augmenting tensors (multi-way arrays) consisting of three phases over a cutting-edge GPU cluster: in the “giant-step” phase, a variational Bayesian inference (VBI) model estimates the distribution of the close neighborhood of each factor in a high confidence level without the need for a priori knowledge of the tensor or problem domain; in the “baby-step” phase, a massively parallel Fast-HALS algorithm (namely G-HALS) has been developed to derive the accurate subfactors of each subtensor on the basis of the initial factors; in the final fusion phase, I-PARAFAC fuses the known factors of the original tensor and those accurate subfactors of the “increment” to achieve the final full factors. Experimental results indicate that: (1) the VBI model enables a blind factor approximation, where the distribution of the close neighborhood of each final factor can be quickly derived (10 iterations for the test case). As a result, the model of a low time complexity significantly accelerates the derivation of the final accurate factors and lowers the risks of errors; (2) I-PARAFAC significantly outperforms even the latest high performance counterpart when handling augmenting tensors, e.g., the increased overhead is only proportional to the increment while the latter has to repeatedly factorize the whole tensor, and the overhead in fusing subfactors is always minimal; (3) I-PARAFAC can factorize a huge tensor (volume up to 500 TB over 50 nodes) as a whole with the capability several magnitudes higher than conventional methods, and the runtime is in the order of 1/n to the number of compute nodes; (4) I-PARAFAC supports correct factorization-based analysis of a real 4-order EEG dataset captured from a variety of epilepsy patients. Overall, it should also be noted that counterpart methods have to derive the whole tensor from the scratch if the tensor is augmented in any dimension; as a contrast, the I-PARAFAC framework only needs to incrementally compute the full factors of the huge augmented tensor.}
}


@article{DBLP:journals/tkde/XiaoCS21,
	author = {Han Xiao and
                  Yidong Chen and
                  Xiaodong Shi},
	title = {Knowledge Graph Embedding Based on Multi-View Clustering Framework},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {585--596},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931548},
	doi = {10.1109/TKDE.2019.2931548},
	timestamp = {Mon, 27 May 2024 13:23:22 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XiaoCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge representation is one of the critical problems in knowledge engineering and artificial intelligence, while knowledge embedding as a knowledge representation methodology indicates entities and relations in knowledge graph as low-dimensional, continuous vectors. In this way, knowledge graph is compatible with numerical machine learning models. Major knowledge embedding methods employ geometric translation to design score function, which is weak-semantic for natural language processing. To overcome this disadvantage, in this paper, we propose our model based on multi-view clustering framework, which could generate semantic representations of knowledge elements (i.e., entities/relations ). With our semantic model, we also present an empowered solution to entity retrieval with entity description. Extensive experiments show that our model achieves substantial improvements against baselines on the task of knowledge graph completion, triple classification, entity classification, and entity retrieval.}
}


@article{DBLP:journals/tkde/KohnLN21,
	author = {Andr{\'{e}} Kohn and
                  Viktor Leis and
                  Thomas Neumann},
	title = {Making Compiling Query Engines Practical},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {597--612},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2905235},
	doi = {10.1109/TKDE.2019.2905235},
	timestamp = {Thu, 27 Jul 2023 08:32:11 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KohnLN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compiling queries to machine code is a very efficient way for executing queries. One often overlooked problem with compilation is the time it takes to generate machine code. Even with fast compilation frameworks like LLVM, generating machine code for complex queries often takes hundreds of milliseconds. Such durations can be a major disadvantage for workloads that execute many complex, but quick queries. To solve this problem, we propose an adaptive execution framework, which dynamically switches from interpretation to compilation. We also propose a fast bytecode interpreter for LLVM, which can execute queries without costly translation to machine code and dramatically reduces the query latency. Adaptive execution is fine-grained, and can execute code paths of the same query using different execution modes. Our evaluation shows that this approach achieves optimal performance in a wide variety of settings-low latency for small data sets and maximum throughput for large data sizes. Besides compilation time, we also focus on debugging, which is another important challenge of compilation-based query engines. To address this problem, we present a novel, database-specific debugger for compiling query engines.}
}


@article{DBLP:journals/tkde/LiL21,
	author = {Dong Li and
                  Jiming Liu},
	title = {Modeling Influence Diffusion over Signed Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {613--625},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2930690},
	doi = {10.1109/TKDE.2019.2930690},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In offline or online worlds, many social systems can be represented as signed social networks including both positive and negative relationships. Although a variety of studies on signed social networks have been conducted motivated by the great application value of unique polarity characteristics, how to model the process of influence propagation over signed social networks is still an important problem that remains pretty much open. Currently, a few studies extended traditional diffusion models (e.g., Independent Cascade model and Linear Threshold model) from unsigned social networks to signed social networks for estimating positive and negative influence of user sets. However, all of above extension models are stochastic and descriptive models. In order to ensure the accuracy of estimated influence, existing models require a significant number of Monte-Carlo simulations which are very time-consuming and not scalable. Aiming at this issue, we propose the Polarity-related Linear Influence Diffusion (PLID) model which can quickly and accurately calculate polarity-related influence of user sets without simulations. To validate effectiveness and efficiency of our proposed model, we make use of our PLID model to solve the positive influence maximization problem in signed social networks under rigorous mathematical proofs. Extensive experiments demonstrate that our PLID model and approximation algorithm significantly outperform state-of-the-art methods in terms of positive influence spread and running time, using Epinions and Slashdot datasets.}
}


@article{DBLP:journals/tkde/LinAR21,
	author = {Sangdi Lin and
                  Bahareh Azarnoush and
                  George C. Runger},
	title = {{MTBR:} Multi-Target Boosting for Regression},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {626--636},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2930516},
	doi = {10.1109/TKDE.2019.2930516},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LinAR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gradient boosting method has been successfully used for single target prediction problems. In real world applications, however, problems involving the prediction of multiple target attributes are often of interest. In this paper, a multi-target boosting method for regression problems, named as MTBR, is proposed. Although MTBR builds one model for each target attribute separately, all the target attributes are utilized when building each model. In each boosting iteration, the base learner, the regression tree in particular, is learned by selecting the best models from all the target attributes. We also introduce a novel knowledge transfer approach. That is, the tree structure learned from one target attribute, representing a way to partition the feature space, is used to predict another target attribute. Experiments with six data sets compare MTBR to other ensemble regression methods, and prove the effectiveness of MTBR in leveraging the knowledge of multiple target attributes and improving the model accuracy.}
}


@article{DBLP:journals/tkde/BerberidisG21,
	author = {Dimitris Berberidis and
                  Georgios B. Giannakis},
	title = {Node Embedding with Adaptive Similarities for Scalable Learning over
                  Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {637--650},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931542},
	doi = {10.1109/TKDE.2019.2931542},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/BerberidisG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Node embedding is the task of extracting informative and descriptive features over the nodes of a graph. The importance of node embedding for graph analytics as well as learning tasks, such as node classification, link prediction, and community detection, has led to a growing interest and a number of recent advances. Nonetheless, node embedding faces several major challenges. Practical embedding methods have to deal with real-world graphs that arise from different domains, with inherently diverse underlying processes as well as similarity structures and metrics. On the other hand, similar to principal component analysis in feature vector spaces, node embedding is an inherently unsupervised task. Lacking metadata for validation, practical schemes motivate standardization and limited use of tunable hyperparameters. Finally, node embedding methods must be scalable in order to cope with large-scale real-world graphs of networks with ever-increasing size. The present work puts forth an adaptive node embedding framework that adjusts the embedding process to a given underlying graph, in a fully unsupervised manner. This is achieved by leveraging the notion of a tunable node similarity matrix that assigns weights on multihop paths. The design of multihop similarities ensures that the resultant embeddings also inherit interpretable spectral properties. The proposed model is thoroughly investigated, interpreted, and numerically evaluated using stochastic block models. Moreover, an unsupervised algorithm is developed for training the model parameters effieciently. Extensive node classification, link prediction, and clustering experiments are carried out on many real-world graphs from various domains, along with comparisons with state-of-the-art scalable and unsupervised node embedding alternatives. The proposed method enjoys superior performance in many cases, while also yielding interpretable information on the underlying graph structure.}
}


@article{DBLP:journals/tkde/XuZZLZZ21,
	author = {Jiajie Xu and
                  Jing Zhao and
                  Rui Zhou and
                  Chengfei Liu and
                  Pengpeng Zhao and
                  Lei Zhao},
	title = {Predicting Destinations by a Deep Learning based Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {651--666},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2932984},
	doi = {10.1109/TKDE.2019.2932984},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XuZZLZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Destination prediction is known as an important problem for many location based services (LBSs). Existing solutions generally apply probabilistic models or neural network models to predict destinations over a subtrajectory, and adopt the standard attention mechanism to improve the prediction accuracy. However, the standard attention mechanism uses fixed feature representations, and has a limited ability to represent distinct features of locations. Besides, existing methods rarely take the impact of spatial and temporal characteristics of the trajectory into account. Their accuracies in fine-granularity prediction are always not satisfactory due to the data sparsity problem. Thus, in this paper, a carefully designed deep learning model called LATL model is presented. It not only adopts an adaptive attention network to model the distinct features of locations, but also implements time gates and distance gates into the Long Short-Term Memory (LSTM) network to capture the spatial-temporal relation between consecutive locations. Furthermore, to better understand the mobility patterns in different spatial granularities, and explore the fusion of multi-granularity learning capability, a hierarchical model that utilizes tailored combination of different neural networks under multiple spatial granularities is further proposed. Extensive empirical studies verify that the newly proposed models perform effectively and settle the problem nicely.}
}


@article{DBLP:journals/tkde/DingYHFLJ21,
	author = {Jingtao Ding and
                  Guanghui Yu and
                  Xiangnan He and
                  Fuli Feng and
                  Yong Li and
                  Depeng Jin},
	title = {Sampler Design for Bayesian Personalized Ranking by Leveraging View
                  Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {667--681},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931327},
	doi = {10.1109/TKDE.2019.2931327},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DingYHFLJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bayesian Personalized Ranking (BPR) is a representative pairwise learning method for optimizing recommendation models. It is widely known that the performance of BPR depends largely on the quality of negative sampler. In this paper, we make two contributions with respect to BPR. First, we find that sampling negative items from the whole space is unnecessary and may even degrade the performance. Second, focusing on the purchase feedback of E-commerce, we propose a negative sampler for BPR by leveraging the additional view data. In our proposed sampler, users' viewed interactions are considered as an intermediate feedback between the purchased and unobserved interactions. We jointly learn the pairwise rankings of user preference among these three types of interactions and design a user-oriented weighting strategy during learning process, which is more effective and flexible. Compared to the vanilla BPR that applies a uniform sampler on all candidates, our view-enhanced sampler enhances BPR with a relative improvement over 36.64 and 16.40 percent on Beibei and Tmall datasets, respectively. Empirical studies demonstrate the importance of considering users' additional feedback when modeling their preference on different items, which can effectively improve the quality of sampled negative items towards learning a better personalized ranking function. Our implementation is available at https://github.com/dingjingtao/NegativeSamplerBPR.}
}


@article{DBLP:journals/tkde/YangZWLXJ21,
	author = {Yang Yang and
                  De{-}Chuan Zhan and
                  Yi{-}Feng Wu and
                  Zhi{-}Bin Liu and
                  Hui Xiong and
                  Yuan Jiang},
	title = {Semi-Supervised Multi-Modal Clustering and Classification with Incomplete
                  Modalities},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {682--695},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2932742},
	doi = {10.1109/TKDE.2019.2932742},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangZWLXJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a novel Semi-supervised Learning with Incomplete Modality (SLIM) method considering the modal consistency and complementarity simultaneously, and Kernel SLIM (SLIM-K) based on matrix completion for further solving the modal incompleteness. As is well known, most realistic data have multi-modal representations, multi-modal learning refers to the process of learning a precise model for complete modalities. However, due to the failures of data collection, self-deficiencies, or other various reasons, multi-modal examples are usually with incomplete modalities, which generate utility obstacle using previous methods. In this paper, SLIM integrates the intrinsic consistency and extrinsic complementary information for prediction and cluster simultaneously. In detail, SLIM forms different modal classifiers and clustering learner consistently in a unified framework, while using the extrinsic complementary information from unlabeled data against the insufficiencies brought by the incomplete modal issue. Moreover, in order to deal with missing modality in essence, we propose the SLIM-K, which takes the complemented kernel matrix into the classifiers and the cluster learner respectively. Thus, SLIM-K can solve the defects of missing modality in result. Finally, we give the discussion of generalization of incomplete modalities. Experiments on 13 benchmark multi-modal datasets and two real-world incomplete multi-modal datasets validate the effectiveness of our methods.}
}


@article{DBLP:journals/tkde/YangFZLJ21,
	author = {Yang Yang and
                  Zhao{-}Yang Fu and
                  De{-}Chuan Zhan and
                  Zhi{-}Bin Liu and
                  Yuan Jiang},
	title = {Semi-Supervised Multi-Modal Multi-Instance Multi-Label Deep Network
                  with Optimal Transport},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {696--709},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2932666},
	doi = {10.1109/TKDE.2019.2932666},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangFZLJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Complex objects are usually with multiple labels, and can be represented by multiple modal representations, e.g., the complex articles contain text and image information as well as multiple annotations. Previous methods assume that the homogeneous multi-modal data are consistent, while in real applications, the raw data are disordered, e.g., the article constitutes with variable number of inconsistent text and image instances. Therefore, Multi-modal Multi-instance Multi-label (M3) learning provides a framework for handling such task and has exhibited excellent performance. However, M3 learning is facing two main challenges: 1) how to effectively utilize label correlation and 2) how to take advantage of multi-modal learning to process unlabeled instances. To solve these problems, we first propose a novel Multi-modal Multi-instance Multi-label Deep Network (M3DN), which considers M3 learning in an end-to-end multi-modal deep network and utilizes consistency principle among different modal bag-level predictions. Based on the M3DN, we learn the latent ground label metric with the optimal transport. Moreover, we introduce the extrinsic unlabeled multi-modal multi-instance data, and propose the M3DNS, which considers the instance-level auto-encoder for single modality and modified bag-level optimal transport to strengthen the consistency among modalities. Thereby M3DNS can better predict label and exploit label correlation simultaneously. Experiments on benchmark datasets and real world WKG Game-Hub dataset validate the effectiveness of the proposed methods.}
}


@article{DBLP:journals/tkde/LiDQWXYQ21,
	author = {Rong{-}Hua Li and
                  Qiangqiang Dai and
                  Lu Qin and
                  Guoren Wang and
                  Xiaokui Xiao and
                  Jeffrey Xu Yu and
                  Shaojie Qiao},
	title = {Signed Clique Search in Signed Networks: Concepts and Algorithms},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {710--727},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2904569},
	doi = {10.1109/TKDE.2019.2904569},
	timestamp = {Tue, 04 Jan 2022 17:01:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiDQWXYQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mining cohesive subgraphs from a network is a fundamental problem in network analysis. Most existing cohesive subgraph models are mainly tailored to unsigned networks. In this paper, we study the problem of seeking cohesive subgraphs in a signed network, in which each edge can be positive or negative, denoting friendship or conflict, respectively. We propose a novel model, called maximal (a, k)-clique, that represents a cohesive subgraph in signed networks. Specifically, a maximal (α, k)-clique is a clique in which every node has at most k negative neighbors and at least ⌈ak⌉ positive neighbors (α ≥ 1). We show that the problem of enumerating all maximal (a, k)-cliques in a signed network is NP-hard. To enumerate all maximal (a, k)-cliques efficiently, we first develop an elegant signed network reduction technique to significantly prune the signed network. Then, we present an efficient branch and bound enumeration algorithm with several carefully-designed pruning rules to enumerate all maximal (a, k)-cliques in the reduced signed network. In addition, we also propose an efficient algorithm with three novel upper-bounding techniques to find the maximum (a, k)-clique in a signed network. The results of extensive experiments on five large real-life datasets demonstrate the efficiency, scalability, and effectiveness of our algorithms.}
}


@article{DBLP:journals/tkde/WangLZAF21,
	author = {Pengyang Wang and
                  Xiaolin Li and
                  Yu Zheng and
                  Charu Aggarwal and
                  Yanjie Fu},
	title = {Spatiotemporal Representation Learning for Driving Behavior Analysis:
                  {A} Joint Perspective of Peer and Temporal Dependencies},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {728--741},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2935203},
	doi = {10.1109/TKDE.2019.2935203},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangLZAF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driving is a complex activity that requires multi-level skilled operations (e.g., acceleration, braking, and turning). Analyzing driving behaviors can help us assess driver performances, improve traffic safety, and, ultimately, promote the development of intelligent and resilient transportation systems. While some efforts have been made for analyzing driving behaviors, existing methods can be improved via representation learning by jointly exploring the peer and temporal dependencies of driving behaviors. To that end, in this paper, we develop a Peer and Temporal-Aware Representation Learning based framework (PTARL) for driving behavior analysis with GPS trajectory data. Specifically, we first detect the driving operations and states of each driver from their GPS traces. Then, we derive a sequence of multi-view driving state transition graphs from the driving state sequences, in order to characterize a driver's driving behaviors that vary over time. In addition, we develop a peer and temporal-aware representation learning method to learn a sequence of time-varying yet relational vectorized representations from the driving state transition graphs. The proposed method can simultaneously model both the graph-graph peer dependency and the current-past temporal dependency in a unified optimization framework. Also, we provide two effective solutions for the optimization problem: (i) a joint optimization solution of representation learning and prediction; and (ii) a step-by-step solution of representation learning and prediction. Besides, we explore two strategies to fuse the learned representations from multi-view transition graphs: (i) simple alignment and (ii) collective fusion. Moreover, we apply the developed framework to the two applications of quantitative transportation safety: (i) scoring of driving performances, and (ii) detection of dangerous regions. Finally, we present extensive experimental results with big trajectory data to demonstrate the enhanced performances of the proposed method for quantitative transportation safety.}
}


@article{DBLP:journals/tkde/HuangYCGJ21,
	author = {Hao Huang and
                  Qian Yan and
                  Lu Chen and
                  Yunjun Gao and
                  Christian S. Jensen},
	title = {Statistical Inference of Diffusion Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {742--753},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2930060},
	doi = {10.1109/TKDE.2019.2930060},
	timestamp = {Wed, 07 Dec 2022 23:01:52 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuangYCGJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To infer structures in diffusion networks, existing approaches mostly need to know not only the final infection statuses of network nodes, but also the exact times when infections occur. In contrast, in many real-world settings, such as disease propagation, monitoring exact infection times is often infeasible due to a high cost. We investigate the problem of how to learn diffusion network structures based on only the final infection statuses of nodes. Instead of utilizing sequences of timestamps to determine potential parent-child influence relationships between nodes, we propose to find influence relationships with high statistical significance. To this end, we design a probabilistic generative model of the final infection statuses to quantitatively measure the likelihood of potential structures of the objective diffusion network, taking into account network complexity. Based on this model, we can infer an appropriate number of most probable parent nodes for each node in the network. Furthermore, to reduce redundant inference computations, we are able to preclude insignificant candidate parent nodes from being considered during inferencing, if their infections have little correlation with the infections of the corresponding child nodes. Extensive experiments on both synthetic and real-world networks offer evidence that the proposed approach is effective and efficient.}
}


@article{DBLP:journals/tkde/YuZFZHSXC21,
	author = {Shanqing Yu and
                  Minghao Zhao and
                  Chenbo Fu and
                  Jun Zheng and
                  Huimin Huang and
                  Xincheng Shu and
                  Qi Xuan and
                  Guanrong Chen},
	title = {Target Defense Against Link-Prediction-Based Attacks via Evolutionary
                  Perturbations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {754--767},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2933833},
	doi = {10.1109/TKDE.2019.2933833},
	timestamp = {Tue, 07 Sep 2021 15:04:30 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuZFZHSXC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In social networks, by removing some target-sensitive links, privacy protection might be achieved. However, some hidden links can still be re-observed by link prediction methods on observable networks. In this paper, the conventional link prediction method named Resource Allocation Index (RA) is adopted for privacy attacks. Several defense methods are proposed, including heuristic and evolutionary approaches, to protect targeted links from RA attack. In particular, incremental computation is proposed for accelerating the calculation of fitness in evolutionary approaches. This is the first time to study privacy protection for targeted links against similarity based link prediction attacks. Some links are randomly selected from original network as targeted links for experimentation. The experimental results on nine real-world networks demonstrate the superiority of the evolutionary perturbations, especially EDA, for defending against RA attack. Moreover, experimental results show that the proposed perturbation generated by EDA is transferable and can even defend against other link prediction attacks which are based on high order similarity between pairwise nodes, although it is designed to prevent RA attack.}
}


@article{DBLP:journals/tkde/ChanYU21,
	author = {Tsz Nam Chan and
                  Man Lung Yiu and
                  Leong Hou U},
	title = {The Power of Bounds: Answering Approximate Earth Mover's Distance
                  with Parametric Bounds},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {768--781},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931969},
	doi = {10.1109/TKDE.2019.2931969},
	timestamp = {Sat, 30 Sep 2023 10:29:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChanYU21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Earth Mover's Distance (EMD) is a robust similarity measure between two histograms (e.g., probability distributions). It has been extensively used in a wide range of applications, e.g., multimedia, data mining, computer vision, etc. As EMD is a computationally intensive operation, many efficient lower and upper bound functions of EMD have been developed. However, they provide no guarantee on the error. In this work, we study how to compute approximate EMD value with bounded error. First, we develop a parametric dual bound function for EMD, in order to offer sufficient trade-off points for optimization. After that, we propose an approximation framework that leverages on lower and upper bound functions to compute approximate EMD with error guarantee. Then, we present three solutions to solve our problem. Experimental results on real data demonstrate the efficiency and the effectiveness of our proposed solutions.}
}


@article{DBLP:journals/tkde/JiaLDZGXZ21,
	author = {Xiaowei Jia and
                  Xiaoyi Li and
                  Nan Du and
                  Yuan Zhang and
                  Vishrawas Gopalakrishnan and
                  Guangxu Xun and
                  Aidong Zhang},
	title = {Tracking Community Consistency in Dynamic Networks: An Influence-Based
                  Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {782--795},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2933516},
	doi = {10.1109/TKDE.2019.2933516},
	timestamp = {Thu, 01 Aug 2024 07:45:44 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiaLDZGXZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The dynamic network data have become ubiquitous with the rapid development of Internet and smart devices. To effectively manage the involved vertices in networks, it is crucial to track the special community patterns and analyze the relationships among vertices. In this paper, we propose a new method to measure the coherence strength, also referred to as community consistency, of a community over a specific observation period. The measurement of community consistency is especially challenging given the dynamic community structure over time, i.e., vertices can leave their original communities and join new communities. In order to interpret the causes of evolving community structure and model the influence of evolving community structure on community consistency, we introduce an influence propagation process having a causal relation with the community consistency. Specifically, a generative model is proposed to combine the influence propagation and the network topological structure at each time step. The proposed influence-based approach for modeling evolution can be instantiated in a variety of real-world network data. The comprehensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed framework in estimating the community consistency. Besides, we conduct a case study to show the effectiveness of the proposed method in real-world applications.}
}


@article{DBLP:journals/tkde/BendimeradPRA21,
	author = {Anes Bendimerad and
                  Marc Plantevit and
                  C{\'{e}}line Robardet and
                  Sihem Amer{-}Yahia},
	title = {User-Driven Geolocated Event Detection in Social Media},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {2},
	pages = {796--809},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2931340},
	doi = {10.1109/TKDE.2019.2931340},
	timestamp = {Tue, 26 Jan 2021 08:43:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/BendimeradPRA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Event detection is one of the most important research topics in social media analysis. Despite this interest, few researchers have addressed the problem of identifying geolocated events in an unsupervised way, and none includes user interests during the process. In this paper, we tackle the problem of local event detection from social media data. We present a method to automatically identify events by evaluating the burstiness of hashtags in a geographical area and a time interval, and at the same time integrating user feedback. We devise two algorithms to discover user-driven events. The first one relies on an exact enumeration process, while the other directly samples the space of events. In our empirical study, we provide evidence that geolocated events cannot be detected by non location-aware methods. We also show that our methods (i) outperform by a factor of two to several orders of magnitude state-of-the-art methods designed to discover geolocated events, (ii) are more robust to noise, and (iii) produce high quality events with respect to user interests.}
}


@article{DBLP:journals/tkde/SinghB21,
	author = {Satendra Pal Singh and
                  Gaurav Bhatnagar},
	title = {A Novel Biometric Inspired Robust Security Framework for Medical Images},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {810--823},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2935710},
	doi = {10.1109/TKDE.2019.2935710},
	timestamp = {Mon, 28 Aug 2023 21:37:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SinghB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The protection of sensitive and confidential data become a challenging task in the present scenario as more and more digital data is stored and transmitted between the end users. The privacy is vitally necessary in case of medical data, which contains the important information of the patients. In this article, a novel biometric inspired medical encryption technique is proposed based on newly introduced parameterized all phase orthogonal transformation (PR-APBST), singular value, and QR decomposition. The proposed technique utilizes the biometrics of the patient/owner to generate a key management system to obtain the parameters involved in the proposed technique. The medical image is then encrypted employing PR-APBST, QR and singular value decomposition and is ready for secure transmission or storage. Finally, a reliable decryption process is employed to reconstruct the original medical image from the encrypted image. The validity and feasibility of the proposed framework have been demonstrated using an extensive experiments on various medical images and security analysis.}
}


@article{DBLP:journals/tkde/ZhouWGGZ21,
	author = {Pan Zhou and
                  Kehao Wang and
                  Linke Guo and
                  Shimin Gong and
                  Bolong Zheng},
	title = {A Privacy-Preserving Distributed Contextual Federated Online Learning
                  Framework with Big Data Support in Social Recommender Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {824--838},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2936565},
	doi = {10.1109/TKDE.2019.2936565},
	timestamp = {Thu, 20 Jun 2024 15:06:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouWGGZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, the booming demand of big data analytics and the constraints of computational ability and network bandwidth have made it difficult for a stand-alone agent/service provider to provide suitable information for every user from the large volume online data within the limited time. To handle this challenge, a recommender system (RS) can call in a group of agents to collaborate to learn users' preference and taste, which is known as a distributed recommender system (DRS). DRSs can improve the accuracy of a traditional RS by requesting agents to share information with each other. However, it is challenging for DRSs to make personalized recommendations for each user due to the large amount of candidates. In addition, information sharing among agents raises a privacy concern. Thus, we propose a privacy-preserving DRS in this paper, and then model each service provider as a distributed online learner with context-awareness. Service providers collaborate to make personalized recommendations by learning users' preferences according to the user context and users' history behaviors. We adopt the federated learning framework to help train a high quality privacy- preserving centralized model over a large number of distributed agents which is probably unreliable with relatively slow network connections. To handle big data scenario, we build an item-cluster tree to deal with online and increasing datasets from top to the bottom. We further consider the structure of social network and present an efficient algorithm to avoid more performance loss adaptively. Theoretical proofs show that our proposed algorithm can achieve sublinear regret and differential privacy protection simultaneously for service providers and users. Numerical results confirm that our novel framework can handle increasing big datasets and strike a trade-off between privacy-preserving level and the prediction accuracy.}
}


@article{DBLP:journals/tkde/SibliniKM21,
	author = {Wissam Siblini and
                  Pascale Kuntz and
                  Frank Meyer},
	title = {A Review on Dimensionality Reduction for Multi-Label Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {839--857},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2940014},
	doi = {10.1109/TKDE.2019.2940014},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SibliniKM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-label classification has gained in importance in the last decade and it is today confronted to the current needs to process massive raw data from heterogeneous sources. Therefore, dimensionality reduction, which aims at reducing the number of features, labels, or both, knows a renewed interest to enhance the scaling properties of the classifiers and their predictive performances. In this paper we review more than fifty papers presenting dimensionality reduction approaches for multi-label classification and we propose an analysis in three steps : (i) a typology of the methods describing the main components of their strategies, the problem they tackle and the way they solve it (ii) a unified formalization of the problems to help to distinguish the similarities and differences between the approaches, and (iii) a meta-analysis of the published experimental results inspired by the consensus theory to identify the most efficient algorithms.}
}


@article{DBLP:journals/tkde/YuGW21,
	author = {Chao{-}Hua Yu and
                  Fei Gao and
                  Qiao{-}Yan Wen},
	title = {An Improved Quantum Algorithm for Ridge Regression},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {858--866},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937491},
	doi = {10.1109/TKDE.2019.2937491},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuGW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ridge regression (RR) is an important machine learning technique which introduces a regularization hyperparameter\nα\nto ordinary multiple linear regression for analyzing data suffering from multicollinearity. In this paper, we present a quantum algorithm for RR, where the technique of parallel Hamiltonian simulation to simulate a number of Hermitian matrices in parallel is proposed and used to develop a quantum version of\nK\n-fold cross-validation approach, which can efficiently estimate the predictive performance of RR. Our algorithm consists of two phases: (1) using quantum\nK\n-fold cross-validation to efficiently determine a good\nα\nwith which RR can achieve good predictive performance, and then (2) generating a quantum state encoding the optimal fitting parameters of RR with such\nα\n, which can be further utilized to predict new data. Since indefinite dense Hamiltonian simulation has been adopted as a key subroutine, our algorithm can efficiently handle non-sparse data matrices. It is shown that our algorithm can achieve exponential speedup over the classical counterpart for (low-rank) data matrices with low condition numbers. But when the condition numbers of data matrices are large to be amenable to full or approximately full ranks of data matrices, only polynomial speedup can be achieved.}
}


@article{DBLP:journals/tkde/ZhangZWHCD21,
	author = {Feng Zhang and
                  Jidong Zhai and
                  Bo Wu and
                  Bingsheng He and
                  Wenguang Chen and
                  Xiaoyong Du},
	title = {Automatic Irregularity-Aware Fine-Grained Workload Partitioning on
                  Integrated Architectures},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {867--881},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2940184},
	doi = {10.1109/TKDE.2019.2940184},
	timestamp = {Thu, 14 Oct 2021 08:57:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZWHCD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integrated architecture that features both CPU and GPU on the same die is an emerging and promising architecture for fine-grained CPU-GPU collaboration. However, the integration also brings forward several programming and system optimization challenges, especially for irregular applications such as graph processing. The complex interplay between heterogeneity and irregularity leads to very low processor utilization of running irregular applications on integrated architectures. Furthermore, fine-grained co-processing on the CPU and GPU is still an open problem. Particularly, in this paper, we show that the previous workload partitioning for CPU-GPU co-processing is far from ideal in terms of resource utilization and performance. To solve this problem, we propose a system software called FinePar, which considers architectural differences of the CPU and GPU and leverages fine-grained collaboration enabled by integrated architectures. Through irregularity-aware performance modeling and online auto-tuning, FinePar partitions irregular workloads and achieves both device-level and thread-level load balance. We evaluate FinePar with eight irregular applications in graphs and sparse matrices on two integrated architectures and compare it with state-of-the-art partitioning approaches. Results show that FinePar demonstrates better resource utilization and achieves an average of 1.6X speedup over the optimal coarse-grained partitioning method.}
}


@article{DBLP:journals/tkde/HongLMZLCLLZ21,
	author = {Xin Hong and
                  Hailin Li and
                  Paul Miller and
                  Jianjiang Zhou and
                  Ling Li and
                  Danny Crookes and
                  Yonggang Lu and
                  Xuelong Li and
                  Huiyu Zhou},
	title = {Component-Based Feature Saliency for Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {882--896},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2936847},
	doi = {10.1109/TKDE.2019.2936847},
	timestamp = {Tue, 21 Mar 2023 21:10:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HongLMZLCLLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Simultaneous feature selection and clustering is a major challenge in unsupervised learning. In particular, there has been significant research into saliency measures for features that result in good clustering. However, as datasets become larger and more complex, there is a need to adopt a finer-grained approach to saliency by measuring it in relation to a part of a model. Another issue is learning the feature saliency and advanced model parameters. We address the first by presenting a novel Gaussian mixture model, which explicitly models the dependency of individual mixture components on each feature giving a new component-based feature saliency measure. For the second, we use Markov Chain Monte Carlo sampling to estimate the model and hidden variables. Using a synthetic dataset, we demonstrate the superiority of our approach, in terms of clustering accuracy and model parameter estimation, over an approach using a model-based feature saliency with expectation maximisation. We performed an evaluation of our approach with six synthetic trajectory datasets obtaining an average clustering accuracy of 97 percent. To demonstrate the generality of our approach, we applied it to a network traffic flow dataset obtaining an accuracy of 93 percent for intrusion detection. Finally, we performed a comparison with state-of-the-art clustering techniques using three real-world trajectory datasets of vehicle traffic. Our approach achieved an average clustering accuracy of 96 percent compared to 77-95 percent for the other techniques. In conclusion, for the datasets considered, component based feature saliency measures gave improved clustering over those based on whole models.}
}


@article{DBLP:journals/tkde/ZhangDW21,
	author = {Bo Zhang and
                  Boxiang Dong and
                  Wendy Hui Wang},
	title = {CorrectMR: Authentication of Distributed {SQL} Execution on MapReduce},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {897--908},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2935968},
	doi = {10.1109/TKDE.2019.2935968},
	timestamp = {Thu, 16 Sep 2021 17:58:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangDW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider the SQL Selection-GroupBy-Aggregation (SGA) query evaluation on an untrusted MapReduce system in which mappers and reducers may return incorrect results. We design CorrectMR, a system that supports efficient verification of result correctness for both intermediate and final results of SGA queries. CorrectMR includes the design of Pedersen Merkle R-tree (PMR-tree), a new authenticated data structure (ADS). To enable efficient verification, CorrectMR includes a distributed ADS construction mechanism that allows mappers/reducers to construct PMR-trees in parallel without a centralized party. CorrectMR provides the following verification functionality: (1) correctness verification of PMR-trees by replication; (2) correctness verification of intermediate (final, resp.) query results by constructing local (global, resp.) PMR-trees and verification objects. Our experimental results demonstrate the efficiency and effectiveness of CorrectMR.}
}


@article{DBLP:journals/tkde/IoannidisZGS21,
	author = {Vassilis N. Ioannidis and
                  Ahmed S. Zamzam and
                  Georgios B. Giannakis and
                  Nicholas D. Sidiropoulos},
	title = {Coupled Graphs and Tensor Factorization for Recommender Systems and
                  Community Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {909--920},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2941716},
	doi = {10.1109/TKDE.2019.2941716},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/IoannidisZGS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Joint analysis of data from multiple information repositories facilitates uncovering the underlying structure in heterogeneous datasets. Single and coupled matrix-tensor factorization (CMTF) has been widely used in this context for imputation-based recommendation from ratings, social network, and other user-item data. When this side information is in the form of item-item correlation matrices or graphs, existing CMTF algorithms may fall short. Alleviating current limitations, we introduce a novel model coined coupled graph-tensor factorization (CGTF) that judiciously accounts for graph-related side information. The CGTF model has the potential to overcome practical challenges, such as missing slabs from the tensor and/or missing rows/columns from the correlation matrices. A novel alternating direction method of multipliers (ADMM) is also developed that recovers the nonnegative factors of CGTF. Our algorithm enjoys closed-form updates that result in reduced computational complexity and allow for convergence claims. A novel direction is further explored by employing the interpretable factors to detect graph communities having the tensor as side information. The resulting community detection approach is successful even when some links in the graphs are missing. Results with real data sets corroborate the merits of the proposed methods relative to state-of-the-art competing factorization techniques in providing recommendations and detecting communities.}
}


@article{DBLP:journals/tkde/WangWLZF21,
	author = {Jingyuan Wang and
                  Ning Wu and
                  Xinxi Lu and
                  Wayne Xin Zhao and
                  Kai Feng},
	title = {Deep Trajectory Recovery with Fine-Grained Calibration using Kalman
                  Filter},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {921--934},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2940950},
	doi = {10.1109/TKDE.2019.2940950},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangWLZF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of location-acquisition technologies, there are a huge number of mobile trajectories generated and accumulated in a variety of domains. However, due to the constraints of device and environment, many trajectories are recorded at low sampling rate, which increases the uncertainty between two consecutive sampled points in the trajectories. Our task is to recover a high-sampled trajectory based on the irregular low-sampled trajectory in free space, i.e., without road network information. There are two major problems with traditional solutions. First, many of these methods rely on heuristic search algorithms or simple probabilistic models. They cannot well capture complex sequential dependencies or global data correlations. Second, for reducing the predictive complexity of the unconstrained numerical coordinates, most of the previous studies have adopted a common preprocessing strategy by mapping the space into discrete units. As a side effect, using discrete units is likely to bring noise or inaccurate information. Hence, a principled post-calibration step is required to produce accurate results, which has been seldom studied by existing methods. To address the above difficulties, we propose a novel Deep Hybrid Trajectory Recovery model, named DHTR. Our recovery model extends the classic sequence-to-sequence generation framework by implementing a subsequence-to-sequence recovery model tailored for the current task, named subseq2seq. In order to effectively capture spatiotemporal correlations, we adopt both spatial and temporal attentions for enhancing the model performance. With the attention mechanisms, our model is able to characterize long-range correlations among trajectory points. Furthermore, we integrate the subseq2seq with a calibration component of Kalman filter (KF) for reducing the predictive uncertainty. At each timestep, the noisy predictions from the subseq2seq component will be fed into the KF component for calibration, and then the refined predictions will be forwarded to the subseq2seq component for the computation of the next timestep. Extensive results on real-world datasets have shown the superiority of the proposed model in both performance and interpretability.}
}


@article{DBLP:journals/tkde/Taha21,
	author = {Kamal Taha},
	title = {Detecting Disjoint Communities in a Social Network Based on the Degrees
                  of Association Between Edges and Influential Nodes},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {935--950},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2940189},
	doi = {10.1109/TKDE.2019.2940189},
	timestamp = {Thu, 27 Jul 2023 08:18:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/Taha21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting communities is crucial to understanding the dynamics of their members. However, the detection of “good” communities is deemed demonstrably problematic, which is mainly due to the following two factors. First, real-world networks are complex and require optimizing multi-objective functions for capturing their community structures, whereas most current approaches optimize only one or two objective functions. Second, most current approaches detect communities in respect of the independence of how closely associated their connections are based on the global relative influences of the edges connecting them. To overcome these limitations, a clustering method needs to optimize multi-objective functions and employ global preprocessing techniques that consider the topology of the entire network. We, therefore, proposed a system called DAVE, which optimizes four objective functions that capture the community structures in most real-word network settings, and detects communities with regards of how closely associated their connections are based on the relative influences of the edges connecting them. We proposed novel formulas that capture these functions. Our method is the first to utilize the prediction of node-node associations based on global node-edge degrees of association. After ranking nodes based on their global relative influences on the network, some of the top-ranked ones will serve as core seeds for constructing communities. Then, the degrees of association between influential edges and seed nodes are computed. DAVE assigns a node to a community, only if each edge in the shortest path from this node to the community's core seed node is both influential and has significant degree of association with the core node. We evaluated DAVE by comparing it empirically and experimentally with 16 methods. Results showed a remarkable improvement.}
}


@article{DBLP:journals/tkde/ZhaoWXGZ21,
	author = {Peng Zhao and
                  Xinqiang Wang and
                  Siyu Xie and
                  Lei Guo and
                  Zhi{-}Hua Zhou},
	title = {Distribution-Free One-Pass Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {951--963},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937078},
	doi = {10.1109/TKDE.2019.2937078},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoWXGZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many large-scale machine learning applications, data are accumulated over time, and thus, an appropriate model should be able to update in an online style. In particular, it would be ideal to have a storage independent from the data volume, and scan each data item only once. Meanwhile, the data distribution usually changes during the accumulation procedure, making distribution-free one-pass learning a challenging task. In this paper, we propose a simple yet effective approach for this task, without requiring prior knowledge about the change, where every data item can be discarded once scanned. We also present a variant for high-dimensional situations, by exploiting compressed sensing to reduce computational and storage complexity. Theoretical analysis shows that our proposal converges under mild assumptions, and the performance is validated on both synthetic and real-world datasets.}
}


@article{DBLP:journals/tkde/ShahrivariJ21,
	author = {Saeed Shahrivari and
                  Saeed Jalili},
	title = {Efficient Distributed k-Clique Mining for Large Networks Using MapReduce},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {964--974},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2936027},
	doi = {10.1109/TKDE.2019.2936027},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShahrivariJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mining cliques of a network is an important problem that has many applications in different fields like social networks, bioinformatics, and web analysis. In most applications, mining fixed sized cliques, known as k-cliques, is enough. However, mining cliques of a large network is very challenging using current solutions, and it takes a considerable time using a commodity machine. Also, very large networks cannot be efficiently loaded into memory of a single machine. To overcome these limitations, we have proposed a solution named KCminer, which is based on state space search and can be totally fitted into the MapReduce framework. Using the MapReduce framework, it is possible to run KCminer on cloud computing platforms and hence, process very large networks in feasible time. Our experiments which were performed on a cloud computing platform with 100 machines show that KCminer is both fast and scalable. Besides the MapReduce framework, KCminer executes efficiently on parallel shared memory systems. We performed some experiments on a commodity multicore desktop and showed that KCminer can effectively use the power of all cores. The experimental results show that even using a single thread, KCminer is much faster than available serial tools like MACE.}
}


@article{DBLP:journals/tkde/NiuZWTGC21,
	author = {Chaoyue Niu and
                  Zhenzhe Zheng and
                  Fan Wu and
                  Shaojie Tang and
                  Xiaofeng Gao and
                  Guihai Chen},
	title = {{ERATO:} Trading Noisy Aggregate Statistics over Private Correlated
                  Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {975--990},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2934100},
	doi = {10.1109/TKDE.2019.2934100},
	timestamp = {Tue, 02 Jan 2024 17:01:00 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NiuZWTGC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the commoditization of personal privacy, pricing private data has become an intriguing problem. In this paper, we study noisy aggregate statistics trading from the perspective of a data broker in data markets. We thus propose ERATO, which enables aggrEgate statistics pRicing over privATe cOrrelated data. On one hand, ERATO guarantees arbitrage freeness against cunning data consumers. On the other hand, ERATO compensates data owners for their privacy losses using both bottom-up and top-down designs. We further apply ERATO to three practical aggregate statistics, namely weighted sum, probability distribution fitting, and degree distribution, and extensively evaluate their performances on MovieLens dataset, 2009 RECS dataset, and two SNAP large social network datasets, respectively. Our analysis and evaluation results reveal that ERATO well balances utility and privacy, achieves arbitrage freeness, and compensates data owners more fairly than differential privacy based approaches.}
}


@article{DBLP:journals/tkde/KhezerlouZTLL21,
	author = {Amin Vahedian Khezerlou and
                  Xun Zhou and
                  Ling Tong and
                  Yanhua Li and
                  Jun Luo},
	title = {Forecasting Gathering Events through Trajectory Destination Prediction:
                  {A} Dynamic Hybrid Model},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {991--1004},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937082},
	doi = {10.1109/TKDE.2019.2937082},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KhezerlouZTLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Identifying urban gathering events is an important problem due to challenges it brings to urban management. In our prior work, we proposed a hybrid model (H-VIGO-GIS) to predict future gathering events through trajectory destination prediction. Our approach consisted of two models: historical and recent and continuously predicted future gathering events. However, H-VIGO-GIS has limitations. (1) The recent model does not capture the newly-emerged abnormal patterns effectively, since it uses all recent trajectories, including normal ones. (2) The recent model is sparse due to limited number of trajectories it learns, i.e., it cannot produce predictions in many cases, forcing us to rely only on the historical model. (3) The accuracy of both recent and historical models varies by space and time. Therefore, combining them the same way at all times and places undermines the overall accuracy of the hybrid model. Addressing these issues, in this paper we propose a Dynamic Hybrid model called (DH-VIGO-TKDE) that addresses the above-mentioned issues. We perform comprehensive evaluations using two large real-world datasets and an event simulator. The experiments show the proposed model significantly improves the prediction accuracy and timeliness of forecasting gathering events, resulting in average precision of 0.91 and recall of 0.67 as opposed to 0.74 and 0.50 of H-VIGO-GIS.}
}


@article{DBLP:journals/tkde/LiLZLP21,
	author = {Pengfei Li and
                  Hua Lu and
                  Qian Zheng and
                  Shijian Li and
                  Gang Pan},
	title = {HisRect: Features from Historical Visits and Recent Tweet for Co-Location
                  Judgement},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1005--1018},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2934686},
	doi = {10.1109/TKDE.2019.2934686},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiLZLP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enabled by smartphones, social media users are increasingly going mobile. This trend fosters various location based services on social media platforms (e.g., Twitter). Many services like friends notification and community detection benefit from co-location judgement, i.e., to decide whether two Twitter users are co-located in some point-of-interest (POI). This problem is challenging due to the limited information in tweets and the lack of explicit geo-tags in tweets that can be used as labeled data. Our approach to this problem is based on a novel concept of HisRect features extracted from users' historical visits and recent tweets: The former has impacts on where a user visits in general, whereas the latter gives more hints about where a user is currently. In practice, labeled data is scarce. Therefore, we design a semi-supervised learning (SSL) framework that leverages unlabeled data to extract HisRect features. Moreover, we employ an embedding neural network layer to process HisRect features of two users, which decides co-location based on the embedding difference between the two features. Our model is extensively evaluated on two large sets of real Twitter data from more than one million users. The experimental results demonstrate that our HisRect features and SSL framework are highly effective at deciding co-locations. In terms of multiple metrics, our approach clearly outperforms alternative approaches using state-of-the-art techniques.}
}


@article{DBLP:journals/tkde/HaoCMB21,
	author = {Yifan Hao and
                  Huiping Cao and
                  Abdullah Mueen and
                  Sukumar Brahma},
	title = {Identify Significant Phenomenon-Specific Variables for Multivariate
                  Time Series},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1019--1031},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2934464},
	doi = {10.1109/TKDE.2019.2934464},
	timestamp = {Sat, 07 Sep 2024 13:34:29 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HaoCMB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multivariate time series (MTS) are collected for different variables in studying scientific phenomena or monitoring system health where each time series records the values of one variable for a time period. Among the different variables, it is common that only a few variables contribute significantly to a specific phenomenon. Furthermore, the variables contributing significantly to different phenomena are often different. We denote the different variables that contribute to the occurrences of different phenomena as Phenomenon-specific Variables (PVs). In this paper, we formulate a novel problem of identifying significant PVs from MTS datasets. To analyze MTS data, feature extraction techniques have been extensively studied. However, most of them identify important global features for one dataset and do not utilize the temporal order of time series. To solve the newly introduced problem, we propose a solution framework, CNN mts -X, which is a new variant of the Convolutional Neural Networks (CNN) and can embed other feature extraction techniques (as X). Furthermore, we design a CNN mts -LR method that implements a new feature identification approach (LR) as Xin the CNN mts -X framework. The LR method leverages both Linear Discriminant Analysis (LDA) and Random Forest (RF). Our extensive experiments on five real datasets show that the CNN mts -LR method has exhibited much better performance than several other baseline methods. Using 30 percent of the PVs discovered from the CNN mts -LR, classifications can achieve better or similar performance than using all the variables.}
}


@article{DBLP:journals/tkde/ZhangXKNZ21,
	author = {Yao Zhang and
                  Yun Xiong and
                  Xiangnan Kong and
                  Zhuang Niu and
                  Yangyong Zhu},
	title = {{IGE+:} {A} Framework for Learning Node Embeddings in Interaction
                  Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1032--1044},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2940459},
	doi = {10.1109/TKDE.2019.2940459},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangXKNZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Node embedding techniques have gained prominence since they produce continuous and low-dimensional features, which are effective for various tasks. Most existing approaches learn node embeddings by exploring the structure of networks and are mainly focused on static non-attributed graphs. However, many real-world applications, such as stock markets and public review websites, involve bipartite graphs with dynamic and attributed edges, called attributed interaction graphs. Different from conventional graph data, attributed interaction graphs involve two kinds of entities (e.g. investors/stocks and users/businesses) and edges of temporal interactions with attributes (e.g. transactions and reviews). In this paper, we study the problem of node embedding in attributed interaction graphs. Learning embeddings in interaction graphs is highly challenging due to the dynamics and heterogeneous attributes of edges. Different from conventional static graphs, in attributed interaction graphs, each edge can have totally different meanings when the interaction is at different times or associated with different attributes. To tackle the above challenges, we introduce the temporal dependency and conditional proximity, which are two fundamental characteristics of interaction graphs. Then, we propose a deep node embedding method called IGE+ (Interaction Graph Embedding+). By preserving these two characteristics, IGE+ is able to produce effective node embeddings in interaction graphs. We evaluate our proposed method and various comparing methods on four real-world datasets. The experimental results prove the effectiveness of the learned embeddings by IGE+ on both node-based and edge-based tasks.}
}


@article{DBLP:journals/tkde/YangP21,
	author = {Yu Yang and
                  Jian Pei},
	title = {Influence Analysis in Evolving Networks: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1045--1063},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2934447},
	doi = {10.1109/TKDE.2019.2934447},
	timestamp = {Thu, 16 Sep 2021 17:58:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence analysis aims at detecting influential vertices in networks and utilizing them in cost-effective business strategies. Influence analysis in large-scale networks is a key technique in many important applications ranging from viral marketing and online advertisement to recommender systems, and thus has attracted great interest from both academia and industry. Early investigations on influence analysis often assume static networks. However, it is well recognized that real networks like social networks and the web network are not static but evolve rapidly over time. Thus, to make the results of influence analysis in real networks up-to-date, we have to take network evolution into consideration. Incorporating evolution of networks into influence analysis raises many new challenges, since an evolving network often updates at a fast rate and, except for the network owner, the evolution is usually even not entirely known to people. In this survey, we provide an overview on recent research in influence analysis in evolving networks, which has not been systematically reviewed in literature. We first revisit mathematical models of evolving networks and commonly used influence models. Then, we review recent research in five major tasks of evolving network influence analysis. We also discuss some future directions to explore.}
}


@article{DBLP:journals/tkde/DengBTZSZ21,
	author = {Dingxiong Deng and
                  Fan Bai and
                  Yiqi Tang and
                  Shuigeng Zhou and
                  Cyrus Shahabi and
                  Linhong Zhu},
	title = {Label Propagation on K-Partite Graphs with Heterophily},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1064--1077},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937493},
	doi = {10.1109/TKDE.2019.2937493},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DengBTZSZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, for the first time, we study label propagation in heterogeneous graphs under heterophily assumption. Homophily label propagation (i.e., two connected nodes share similar labels) in homogeneous graph (with same types of vertices and relations) has been extensively studied before. Unfortunately, real-life networks (e.g., social networks) are heterogeneous, they contain different types of vertices (e.g., users, images, and texts) and relations (e.g., friendships and co-tagging) and allow for each node to propagate both the same and opposite copy of labels to its neighbors. We propose a IC-partite label propagation model to handle the mystifying combination of heterogeneous nodes/relations and heterophily propagation. With this model, we develop a novel label inference algorithm framework with update rules in near-linear time complexity. Since real networks change overtime, we devise an incremental approach, which supports fast updates for both new data and evidence (e.g., ground truth labels) with guaranteed efficiency. We further provide a utility function to automatically determine whether an incremental or a re-modeling approach is favored. Extensive experiments on real datasets have verified the effectiveness and efficiency of our approach, and its superiority over the state-of-the-art label propagation methods.}
}


@article{DBLP:journals/tkde/TanHYCGW21,
	author = {Mingkui Tan and
                  Zhibin Hu and
                  Yuguang Yan and
                  Jiezhang Cao and
                  Dong Gong and
                  Qingyao Wu},
	title = {Learning Sparse {PCA} with Stabilized {ADMM} Method on Stiefel Manifold},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1078--1088},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2935449},
	doi = {10.1109/TKDE.2019.2935449},
	timestamp = {Mon, 05 Feb 2024 20:21:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/TanHYCGW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparse principal component analysis (SPCA) produces principal components with sparse loadings, which is very important for handling data with many irrelevant features and also critical to interpret the results. To deal with orthogonal constraints, most previous approaches address SPCA with several components using techniques such as deflation technique and convex relaxations. However, the deflation technique usually suffers from suboptimal solutions due to poor approximations. On the other hand, the convex relaxations are often computationally expensive. To address the above issues, in this paper, we propose to address SPCA over the Stiefel manifold directly, and develop a stabilized Alternating Direction Method of Multipliers (SADMM) to handle the nonconvex orthogonal constraints. Compared to traditional ADMM, the proposed SADMM method converges well with a wide range of parameters and obtains a better solution. We also theoretically study the convergence property of the proposed SADMM method. Furthermore, most existing methods ignore an inherent drawback of SPCA — the importance of different components is not considered when doing feature selection, which often makes the selected features nonoptimal. To address this, we further propose a two-stage method which considers the importance of different components to select the most important features. Empirical studies on both synthetic and real-world datasets show that the proposed algorithms achieve better performance compared to existing state-of-the-art methods.}
}


@article{DBLP:journals/tkde/ChenYZKL21,
	author = {Xixian Chen and
                  Haiqin Yang and
                  Shenglin Zhao and
                  Irwin King and
                  Michael R. Lyu},
	title = {Making Online Sketching Hashing Even Faster},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1089--1101},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2934687},
	doi = {10.1109/TKDE.2019.2934687},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenYZKL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data-dependent hashing methods have demonstrated good performance in various machine learning applications to learn a low-dimensional representation from the original data. However, they still suffer from several obstacles: First, most of existing hashing methods are trained in a batch mode, yielding inefficiency for training streaming data. Second, the computational cost and the memory consumption increase extraordinarily in the big data setting, which perplexes the training procedure. Third, the lack of labeled data hinders the improvement of the model performance. To address these difficulties, we utilize online sketching hashing (OSH) and present a FasteR Online Sketching Hashing (FROSH) algorithm to sketch the data in a more compact form via an independent transformation. We provide theoretical justification to guarantee that our proposed FROSH consumes less time and achieves a comparable sketching precision under the same memory cost of OSH. We also extend FROSH to its distributed implementation, namely DFROSH, to further reduce the training time cost of FROSH while deriving the theoretical bound of the sketching precision. Finally, we conduct extensive experiments on both synthetic and real datasets to demonstrate the attractive merits of FROSH and DFROSH.}
}


@article{DBLP:journals/tkde/LuoZNL21,
	author = {Wenjian Luo and
                  Daofu Zhang and
                  Li Ni and
                  Nannan Lu},
	title = {Multiscale Local Community Detection in Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1102--1112},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2938173},
	doi = {10.1109/TKDE.2019.2938173},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuoZNL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world social networks, global information (e.g., the number of nodes and the connections between them) is incomplete or expensive to acquire; therefore, local community detection becomes especially important. Local community detection is used to identify the local community to which the given starting node belongs according to local information. For a given node, most existing local community detection methods can only find single scale local communities but not those of variable sizes. However, local communities with different scales are often required. Therefore, it is necessary and meaningful to find local communities of the given starting node with different scales; we call this multiscale local community detection. In this paper, we propose a new local modularity inspired by the global modularity and prove the equivalence of the proposed local modularity with two other typical local modularities. Furthermore, to detect local communities with different scales, we present a method based on the proposed local modularity. We test this method on several synthetic and real datasets, and the experimental results indicate that the detected community is meaningful and its scale can be changed reasonably.}
}


@article{DBLP:journals/tkde/HuYY21,
	author = {Shizhe Hu and
                  Xiaoqiang Yan and
                  Yangdong Ye},
	title = {Multi-Task Image Clustering through Correlation Propagation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1113--1127},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937026},
	doi = {10.1109/TKDE.2019.2937026},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuYY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional image clustering algorithms deal with single-task clustering (STC) problem on a single domain. However, with the increasing number of related images on the Web, it is challenging for STCs to perform related image clustering tasks independently without considering the between-task relationship, which mainly consists of similar visual features and image patterns among tasks. Therefore, it is intuitive to resort to multi-task clustering (MTC) algorithms. However, most existing MTCs learn a shared feature subspace, which may lead to negative transfer when facing the image clustering tasks that are not strongly related. In this paper, we propose a novel multi-task image clustering algorithm, which performs multiple image clustering tasks simultaneously and propagates the task correlation to improve clustering performance. Specifically, we first extend the information bottleneck method to cluster tasks independently. The related and unrelated images between the pairwise clusters of different tasks are then discovered. Meanwhile, two corresponding types of correlations are propagated among the tasks, where only the positive correlation benefits the clustering of each task. A sequential and collaborative method is further designed to ensure an optimal solution. Moreover, we perform a theoretical analysis of the properties on correlation propagation and the convergence of our algorithm. The experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art clustering methods.}
}


@article{DBLP:journals/tkde/YangSLHLL21,
	author = {Cheng Yang and
                  Maosong Sun and
                  Haoran Liu and
                  Shiyi Han and
                  Zhiyuan Liu and
                  Huanbo Luan},
	title = {Neural Diffusion Model for Microscopic Cascade Study},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1128--1139},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2939796},
	doi = {10.1109/TKDE.2019.2939796},
	timestamp = {Fri, 01 Sep 2023 13:50:23 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangSLHLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The study of information diffusion or cascade has attracted much attention over the last decade. Most related works target on studying cascade-level macroscopic properties such as the final size of a cascade. Existing microscopic cascade models which focus on user-level modeling either make strong assumptions on how a user gets infected by a cascade or limit themselves to a specific scenario where “who infected whom” information is explicitly labeled. The strong assumptions oversimplify the complex diffusion mechanism and prevent these models from better fitting real-world cascade data. Also, the methods which focus on specific scenarios cannot be generalized to a general setting where the diffusion graph is unobserved. To overcome the drawbacks of previous works, we propose a Neural Diffusion Model (NDM) for general microscopic cascade study. NDM makes relaxed assumptions and employs deep learning techniques including attention mechanism and convolutional network for cascade modeling. Both advantages enable our model to go beyond the limitations of previous methods, better fit the diffusion data and generalize to unseen cascades. Experimental results on diffusion identification task over four realistic cascade datasets show that our model can achieve a relative improvement up to 26 percent against the best performing baseline in terms of F1 score.}
}


@article{DBLP:journals/tkde/AlabdulkarimAG21,
	author = {Yazeed Alabdulkarim and
                  Marwan Almaymoni and
                  Shahram Ghandeharizadeh},
	title = {Polygraph: {A} Plug-n-Play Framework to Quantify Application Anomalies},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1140--1155},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2939520},
	doi = {10.1109/TKDE.2019.2939520},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/AlabdulkarimAG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Polygraph is a tool to quantify application anomalies attributed to violating atomicity, isolation, and linearizability properties of transactions. It is a plug-n-play framework that includes visualization tools to empower an experimentalist to (a) quickly incorporate Polygraph into an existing application or benchmark and (b) quantify the number of anomalies. We demonstrate Polygraph using existing benchmarks, including TPC-C, SEATS, TATP, YCSB, and BG. We highlight Polygraph as an on-line tool by showing it scales for almost all benchmarks to process their transaction log records faster than their rate of production.}
}


@article{DBLP:journals/tkde/TuCC21,
	author = {Jiayang Tu and
                  Peng Cheng and
                  Lei Chen},
	title = {Quality-Assured Synchronized Task Assignment in Crowdsourcing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1156--1168},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2935443},
	doi = {10.1109/TKDE.2019.2935443},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TuCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of crowdsourcing platforms that aggregate the intelligence of Internet workers, crowdsourcing has been widely utilized to address problems that require human cognitive abilities. Considering great dynamics of worker arrival and departure, it is of vital importance to design a task assignment scheme to adaptively select the most beneficial tasks for the available workers. In this paper, in order to make the most efficient utilization of the worker labor and balance the accuracy of answers and the overall latency, we a) develop a parameter estimation model that assists in estimating worker expertise, question easiness, and answer confidence; b) propose a quality-assured synchronized task assignment scheme that executes in batches and maximizes the number of potentially completed questions (MCQ) within each batch. We prove that MCQ problem is NP-hard and present two greedy approximation solutions to address the problem. The effectiveness and efficiency of the approximation solutions are further evaluated through extensive experiments on synthetic and real datasets. The experimental results show that the accuracy and the overall latency of the MCQ approaches outperform the existing online task assignment algorithms in the synchronized task assignment scenario.}
}


@article{DBLP:journals/tkde/GeLT21,
	author = {Yong Ge and
                  Huayu Li and
                  Alexander Tuzhilin},
	title = {Route Recommendations for Intelligent Transportation Services},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1169--1182},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937864},
	doi = {10.1109/TKDE.2019.2937864},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GeLT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The accumulated large amount of mobility data and the ability to track moving people or objects have enabled us to develop advanced mobile recommendations, which are essential to recommend a sequence of locations to an individual user on the move. In this paper, we study a particular case of mobile recommendations, route recommendations to drivers, by utilizing vehicle GPS data. Specifically, we formulate a new Route Recommendation with Relaxed Assumptions (RR-RA) problem, the goal of which is to recommend a sequence of locations to a driver based on his current location in order to maximize his business success. To make our recommendation practical and scalable for real practice, we need to produce recommendation results in a timely fashion once a request emerges. Therefore, we propose an efficient algorithm to efficiently generate recommendations. Furthermore, we identify and address a destination-oriented route recommendation (DORR) problem. Without solving DORR problem, RR-RA alone does not work well in practice because drivers may encounter the destination constraint on a daily basis. We develop a dedicated and efficient algorithm for solving DORR problem. The package of solutions for both RR-RA and DORR problems provide a comprehensive approach for route recommendations to drivers. We evaluate our methods using both real-world GPS data and synthetic data, and demonstrate the effectiveness and efficiency of proposed methods with different evaluation metrics.}
}


@article{DBLP:journals/tkde/ShiLCZZK21,
	author = {Hongzhi Shi and
                  Yong Li and
                  Hancheng Cao and
                  Xiangxin Zhou and
                  Chao Zhang and
                  Vassilis Kostakos},
	title = {Semantics-Aware Hidden Markov Model for Human Mobility},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1183--1194},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937296},
	doi = {10.1109/TKDE.2019.2937296},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShiLCZZK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Understanding human mobility benefits numerous applications such as urban planning, traffic control, and city management. Previous work mainly focuses on modeling spatial and temporal patterns of human mobility. However, the semantics of trajectory are ignored, thus failing to model people's motivation behind mobility. In this paper, we propose a novel semantics-aware mobility model that captures human mobility motivation using large-scale semantic-rich spatial-temporal data from location-based social networks. In our system, we first develop a multimodal embedding method to project user, location, time, and activity on the same embedding space in an unsupervised way while preserving original trajectory semantics. Then, we use hidden Markov model to learn latent states and transitions between them in the embedding space, which is the location embedding vector, to jointly consider spatial, temporal, and user motivations. In order to tackle the sparsity of individual mobility data, we further propose a von Mises-Fisher mixture clustering for user grouping so as to learn a reliable and fine-grained model for groups of users sharing mobility similarity. We evaluate our proposed method on two large-scale real-world datasets, where we validate the ability of our method to produce high-quality mobility models. We also conduct extensive experiments on the specific task of location prediction. The results show that our model outperforms state-of-the-art mobility models with higher prediction accuracy and much higher efficiency.}
}


@article{DBLP:journals/tkde/CaoHMXCX21,
	author = {Da Cao and
                  Xiangnan He and
                  Lianhai Miao and
                  Guangyi Xiao and
                  Hao Chen and
                  Jia Xu},
	title = {Social-Enhanced Attentive Group Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1195--1209},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2936475},
	doi = {10.1109/TKDE.2019.2936475},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CaoHMXCX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of social networks, group activities have become an essential ingredient of our daily life. A growing number of users share their group activities online and invite their friends to join in. This imposes the need of an in-depth study on the group recommendation task, i.e., recommending items to a group of users. Despite its value and significance, group recommendation remains an unsolved problem due to 1) the weights of group members are crucial to the recommendation performance but are rarely learnt from data; 2) social followee information is beneficial to understand users’ preferences but is rarely considered; and 3) user-item interactions are helpful to reinforce the performance of group recommendation but are seldom investigated. Toward this end, we devise neural network-based solutions by utilizing the recent developments of attention network and neural collaborative filtering (NCF). First of all, we adopt an attention network to form the representation of a group by aggregating the group members’ embeddings, which allows the attention weights of group members to be dynamically learnt from data. Second, the social followee information is incorporated via another attention network to enhance the representation of individual user, which is helpful to capture users’ personal preferences. Third, considering that many online group systems also have abundant interactions of individual users on items, we further integrate the modeling of user-item interactions into our method. Through this way, the recommendation for groups and users can be mutually reinforced. Extensive experiments on the scope of both macro-level performance comparison and micro-level analyses justify the effectiveness and rationality of our proposed approaches.}
}


@article{DBLP:journals/tkde/NieZL21,
	author = {Feiping Nie and
                  Wei Zhu and
                  Xuelong Li},
	title = {Structured Graph Optimization for Unsupervised Feature Selection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1210--1222},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937924},
	doi = {10.1109/TKDE.2019.2937924},
	timestamp = {Mon, 28 Aug 2023 21:37:41 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NieZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsupervised feature selection has attracted more and more attention due to the rapid growth of the large amount of unlabelled and high-dimensional data. The performance of traditional spectral-based unsupervised methods always depends on the quality of constructed similarity matrix. However, real world data always contain a large number of noise samples and features that make the similarity matrix created by original data cannot be fully relied. We propose an unsupervised feature selection method which conducts feature selection and local structure learning simultaneously. Moreover, we add an important constraint on the similarity matrix to allow it to capture more accurate information of the data structure. To perform feature selection, orthogonal constraint and `2;p-norm are adopted on the projection matrix. An efficient and simple algorithm is derived to tackle the problem. We conduct comprehensive experiments on various benchmark data sets, including handwritten digit, face image, and biomedical data, to validate the effectiveness of the proposed approach.}
}


@article{DBLP:journals/tkde/TangXYNZ21,
	author = {Yongqiang Tang and
                  Yuan Xie and
                  Xuebing Yang and
                  Jinghao Niu and
                  Wensheng Zhang},
	title = {Tensor Multi-Elastic Kernel Self-Paced Learning for Time Series Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1223--1237},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937027},
	doi = {10.1109/TKDE.2019.2937027},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TangXYNZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series clustering has attracted growing attention due to the abundant data accessible and extensive value in various applications. The unique characteristics of time series, including high-dimension, warping, and the integration of multiple elastic measures, pose challenges for the present clustering algorithms, most of which take into account only part of these difficulties. In this paper, we make an effort to simultaneously address all aforementioned issues in time series clustering under a unified multiple kernels clustering (MKC) framework. Specifically, we first implicitly map the raw time series space into multiple kernel spaces via elastic distance measure functions. In such high-dimensional spaces, we resort to the tensor constraint based self-representation subspace clustering approach, which involves the self-paced learning paradigm, to explore the essential low-dimensional structure of the data, as well as the high-order complementary information from different elastic kernels. The proposed approach can be extended to more challenging multivariate time series clustering scenario in a direct but elegant way. Extensive experiments on 85 univariate and 10 multivariate time series datasets demonstrate the significant superiority of the proposed approach beyond the baseline and several state-of-the-art MKC methods.}
}


@article{DBLP:journals/tkde/MitraSB21,
	author = {Shubhadip Mitra and
                  Priya Saraf and
                  Arnab Bhattacharya},
	title = {{TIPS:} Mining Top-K Locations to Minimize User-Inconvenience for
                  Trajectory-Aware Services},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1238--1250},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2935448},
	doi = {10.1109/TKDE.2019.2935448},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MitraSB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Facility location problems aim to identify the best locations to set up new services. The majority of the existing works typically assume that the users are static. However, there exists a wide array of services such as fuel stations, ATMs, food joints, etc., that are widely accessed by mobile users besides the static ones. Such trajectory-aware services should, therefore, factor in the trajectories of its users rather than simply their static locations. In this work, we introduce the problem of optimal placement of facility locations for such trajectory-aware services that minimize the user inconvenience. The inconvenience of a user is the extra distance traveled by her from her regular path to avail a service. We call this the TIPS problem (Trajectory-aware Inconvenience-minimizing Placement of Services) and consider two variants of it. The goal of the first variant, MAX-TIPS, is to minimize the maximum inconvenience faced by any user, while that of the second, AVG-TIPS, is to minimize the average inconvenience over all the users. We show that both these problems are NP-hard, and propose multiple efficient heuristics to solve them. Empirical evaluation on real urban-scale road networks validate the efficiency and effectiveness of the proposed heuristics.}
}


@article{DBLP:journals/tkde/LiWCXFGLX21,
	author = {Yafei Li and
                  Ji Wan and
                  Rui Chen and
                  Jianliang Xu and
                  Xiaoyi Fu and
                  Hongyan Gu and
                  Pei Lv and
                  Mingliang Xu},
	title = {Top-{\textdollar}k{\textdollar}k Vehicle Matching in Social Ridesharing:
                  {A} Price-Aware Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1251--1263},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2937031},
	doi = {10.1109/TKDE.2019.2937031},
	timestamp = {Wed, 11 Sep 2024 20:36:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiWCXFGLX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the past few years ridesharing has largely reshaped the transportation marketplace. It is envisioned as a promising solution to transportation-related problems in metropolitan cities, such as traffic congestion and air pollution. In the current ridesharing research, social ridesharing, which makes use of social relations among drivers and riders to address safety issues, and dynamic pricing are two active directions with important business implications. Simultaneously optimizing social cohesion and revenue is vital to a commercial ridesharing platform's sustainable development, which, however, has not been previously studied. In this paper, we first present a new pricing scheme that better incentivizes drivers and riders to participate in ridesharing, and then propose a novel type of Price-aware Top-\nk\nMatching (PTkM) queries which retrieve the top-\nk\nvehicles for a rider's request by taking into account both social relations and revenue. We design an efficient algorithm with a set of powerful pruning techniques to tackle this problem. Moreover, we propose a novel index tailored to our problem to further speed up query processing. Extensive experimental results on real datasets show that our proposed algorithms achieve desirable performance for real-world deployment.}
}


@article{DBLP:journals/tkde/LyuOWSC21,
	author = {Shanshan Lyu and
                  Wentao Ouyang and
                  Yongqing Wang and
                  Huawei Shen and
                  Xueqi Cheng},
	title = {Truth Discovery by Claim and Source Embedding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1264--1275},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2936189},
	doi = {10.1109/TKDE.2019.2936189},
	timestamp = {Mon, 28 Aug 2023 21:37:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LyuOWSC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information gathered from multiple sources on the Web often exhibits conflicts. This phenomenon motivates the need of truth discovery, which aims to automatically find the true claim among multiple conflicting claims. Existing truth discovery methods are mainly based on iterative updates, optimization or probabilistic models. Although these methods have shown their own effectiveness, they have a common limitation. These methods do not model relationships between each pair of source and target such that they do not well capture the underlying interactions in the data. In this paper, we propose a new model for truth discovery, learning the representations of sources and claims automatically from the interactions between sources and targets. Our model first constructs a heterogenous network including source-claim, source-source and truth-claim relationships. It then embeds the network into a low dimensional space such that trustworthy sources and true claims are close. In this way, truth discovery can be conveniently performed in the embedding space. Moreover, our model can be implemented in both semi-supervised and un-supervised manners to deal with the label scarcity problem in practical truth discovery. Experiments on three real-world datasets demonstrate that our model outperforms existing state-of-the-art methods for truth discovery.}
}


@article{DBLP:journals/tkde/WangWNLYW21,
	author = {Fei Wang and
                  Quan Wang and
                  Feiping Nie and
                  Zhongheng Li and
                  Weizhong Yu and
                  Rong Wang},
	title = {Unsupervised Linear Discriminant Analysis for Jointly Clustering and
                  Subspace Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1276--1290},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2939524},
	doi = {10.1109/TKDE.2019.2939524},
	timestamp = {Sat, 30 Sep 2023 10:29:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangWNLYW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Linear discriminant analysis (LDA) is one of commonly used supervised subspace learning methods. However, LDA will be powerless faced with the no-label situation. In this paper, the unsupervised LDA (Un-LDA) is proposed and first formulated as a seamlessly unified objective optimization which guarantees convergence during the iteratively alternative solving process. The objective optimization is in both the ratio trace and the trace ratio forms, forming a complete framework of a new approach to jointly clustering and unsupervised subspace learning. The extension of LDA into Un-LDA enables to not only complete unsupervised subspace learning via the explicitly presented subspace projection matrix but also simultaneously finish clustering and even clustering out-of-sample data via the explicitly presented transformation matrix. To overcome the difficulty in solving the non-convex objective optimization, we mathematically prove that the Un-LDA optimization in both forms can be transformed into the simple K-means clustering optimization when the subspace is determined. The Un-LDA optimization is eventually completed by alternatively optimizing the clusters using K-means and the subspace using the supervised LDA methods and iterating this whole process until convergence or stopping criterion. The experiments demonstrate that our proposed Un-LDA algorithms are comparable or even much superior to the counterparts.}
}


@article{DBLP:journals/tkde/NettasingheK21,
	author = {Buddhika Nettasinghe and
                  Vikram Krishnamurthy},
	title = {"What Do Your Friends Think?": Efficient Polling Methods for Networks
                  Using Friendship Paradox},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {3},
	pages = {1291--1305},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2940914},
	doi = {10.1109/TKDE.2019.2940914},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NettasingheK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper deals with randomized polling of a social network. In the case of forecasting the outcome of an election between two candidates A and B, classical intent polling asks randomly sampled individuals: who will you vote for? Expectation polling asks: who do you think will win? In this paper, we propose a novel neighborhood expectation polling (NEP) strategy that asks randomly sampled individuals: what is your estimate of the fraction of votes for A? Therefore, in NEP, sampled individuals will naturally look at their neighbors (defined by the underlying social network graph) when answering this question. Hence, the mean squared error (MSE) of NEP methods rely on selecting the optimal set of samples from the network. To this end, we propose two NEP algorithms for the following cases: (i) the social network graph is not known but, random walks (sequential exploration) can be performed on the graph, and (ii) the social network graph is unknown but, uniformly sampled nodes from the network are available. For both cases, algorithms based on a graph theoretic consequence called friendship paradox are proposed. Theoretical results on the dependence of the MSE of the algorithms on the properties of the network are established. Numerical results on real and synthetic data sets are provided to illustrate the performance of the algorithms.}
}


@article{DBLP:journals/tkde/GanLFCTY21,
	author = {Wensheng Gan and
                  Jerry Chun{-}Wei Lin and
                  Philippe Fournier{-}Viger and
                  Han{-}Chieh Chao and
                  Vincent S. Tseng and
                  Philip S. Yu},
	title = {A Survey of Utility-Oriented Pattern Mining},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1306--1327},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2942594},
	doi = {10.1109/TKDE.2019.2942594},
	timestamp = {Thu, 27 Jul 2023 08:18:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GanLFCTY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The main purpose of data mining and analytics is to find novel, potentially useful patterns that can be utilized in real-world applications to derive beneficial knowledge. For identifying and evaluating the usefulness of different kinds of patterns, many techniques and constraints have been proposed, such as support, confidence, sequence order, and utility parameters (e.g., weight, price, profit, quantity, satisfaction, etc.). In recent years, there has been an increasing demand for utility-oriented pattern mining (UPM, or called utility mining). UPM is a vital task, with numerous high-impact applications, including cross-marketing, e-commerce, finance, medical, and biomedical applications. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods of UPM. First, we introduce an in-depth understanding of UPM, including concepts, examples, and comparisons with related concepts. A taxonomy of the most common and state-of-the-art approaches for mining different kinds of high-utility patterns is presented in detail, including Apriori-based, tree-based, projection-based, vertical-/horizontal-data-format-based, and other hybrid approaches. A comprehensive review of advanced topics of existing high-utility pattern mining techniques is offered, with a discussion of their pros and cons. Finally, we present several well-known open-source software packages for UPM. We conclude our survey with a discussion on open and practical challenges in this field.}
}


@article{DBLP:journals/tkde/RohHW21,
	author = {Yuji Roh and
                  Geon Heo and
                  Steven Euijong Whang},
	title = {A Survey on Data Collection for Machine Learning: {A} Big Data - {AI}
                  Integration Perspective},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1328--1347},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2946162},
	doi = {10.1109/TKDE.2019.2946162},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/RohHW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.}
}


@article{DBLP:journals/tkde/OmranWW21,
	author = {Pouya Ghiasnezhad Omran and
                  Kewen Wang and
                  Zhe Wang},
	title = {An Embedding-Based Approach to Rule Learning in Knowledge Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1348--1359},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2941685},
	doi = {10.1109/TKDE.2019.2941685},
	timestamp = {Wed, 27 Jul 2022 08:20:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/OmranWW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is natural and effective to use rules for representing explicit knowledge in knowledge graphs. However, it is challenging to learn rules automatically from very large knowledge graphs such as Freebase and YAGO. This paper presents a new approach, RLvLR (Rule Learning via Learning Representations), to learning rules from large knowledge graphs by using the technique of embedding in representation learning together with a new sampling method. Based on RLvLR, a new method RLvLR-Stream is developed for learning rules from streams of knowledge graphs. Both RLvLR and RLvLR-Stream have been implemented and experiments conducted to validate the proposed methods regarding the tasks of rule learning and link prediction. Experimental results show that our systems are able to handle the task of rule learning from large knowledge graphs with high accuracy and outperform some state-of-the-art systems. Specifically, for massive knowledge graphs with hundreds of predicates and over 10M facts, RLvLR is much faster and can learn much more quality rules than major systems for rule learning in knowledge graphs such as AMIE+. In the setting of knowledge graph streams, RLvLR-Stream significantly improved RLvLR for both rule learning and link prediction.}
}


@article{DBLP:journals/tkde/MiaoGGCYL21,
	author = {Xiaoye Miao and
                  Yunjun Gao and
                  Su Guo and
                  Lu Chen and
                  Jianwei Yin and
                  Qing Li},
	title = {Answering Skyline Queries Over Incomplete Data With Crowdsourcing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1360--1374},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2946798},
	doi = {10.1109/TKDE.2019.2946798},
	timestamp = {Mon, 28 Aug 2023 21:37:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MiaoGGCYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the pervasiveness of incomplete data, incomplete data queries are vital in a large number of real-life scenarios. Current models and approaches for incomplete data queries mainly rely on the machine power. In this paper, we study the problem of skyline queries over incomplete data with crowdsourcing . We propose a novel query framework, termed as\nBayesCrowd\n, which takes into account the data correlation using the Bayesian network. We leverage the typical c-table model on incomplete data to represent objects. Considering budget and latency constraints, we present a suite of effective task selection strategies. Moreover, we introduce a marginal utility function to measure the benefit of crowdsourcing one task. In particular, the probability computation of each object being an answer object is at least as hard as #SAT problem. To this end, we propose an adaptive DPLL (i.e., Davis-Putnam-Logemann- Loveland) algorithm to speed up the computation. Extensive experiments using both real and synthetic data sets confirm the superiority of\nBayesCrowd\nto the state-of-the-art method, in terms of execution time, monetary cost, and latency minimization.}
}


@article{DBLP:journals/tkde/CaiZ21,
	author = {Hongyun Cai and
                  Fuzhi Zhang},
	title = {{BS-SC:} An Unsupervised Approach for Detecting Shilling Profiles
                  in Collaborative Recommender Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1375--1388},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2946247},
	doi = {10.1109/TKDE.2019.2946247},
	timestamp = {Wed, 28 Apr 2021 13:10:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CaiZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative recommender systems are vulnerable to shilling attacks. To address this issue, many methods including supervised and unsupervised have been proposed. However, supervised detection methods require training classifiers and they only apply to detect known types of attacks. The existing unsupervised detection methods need to know the prior knowledge of attacks, otherwise they suffer from low detection precision. In this paper, we present BS-SC, an unsupervised approach for detecting shilling profiles, which does not need to know the attack size or to label the candidate spammers. BS-SC starts from an in-depth analysis of user behaviors and uses two key mechanisms (i.e., behavior features extraction and behavior similarity matrix clustering) to distinguish shilling profiles from genuine ones. The behavior features reflect the behavior difference between genuine and shilling profiles, and the behavior similarity matrix clustering is to cluster shilling profiles based on their highly similar behaviors. Experimental results on the MovieLens and the sampled Amazon review datasets indicate that BS-SC outperforms the baseline unsupervised approaches, even when the prior knowledge is given for them.}
}


@article{DBLP:journals/tkde/YuYWD21,
	author = {Xianxue Yu and
                  Guoxian Yu and
                  Jun Wang and
                  Carlotta Domeniconi},
	title = {Co-Clustering Ensembles Based on Multiple Relevance Measures},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1389--1400},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2942029},
	doi = {10.1109/TKDE.2019.2942029},
	timestamp = {Fri, 17 Sep 2021 12:31:46 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuYWD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Co-clustering aims at discovering groups of both objects and features from a given data matrix. Co-clustering ensembles can produce robust co-clusters by combining multiple base co-clusterings. However, current co-clustering ensemble solutions either ignore the constraints resulting from feature-to-feature and object-to-object relevance information, or ignore feature-to-object relevance information. In this paper, we advocate that all three information sources contribute to the achievement of good consensus solutions, and propose a co-clustering ensemble (CoCE) approach based on multiple relevance measures. CoCE first evaluates the quality of base co-clusters and consequently measures feature-to-object relevance. The latter, along with feature-to-feature and object-to-object relevance measures, contribute to the definition of a hybrid graph. The consensus process uses the resulting hybrid graph; it's formulated as a trace minimization problem and introduces a block-wise matrix multiplication technique to perform the optimization. Experimental results on various datasets show that CoCE not only frequently outperforms other related co-clustering ensembles, but also has reduced runtime cost and is more robust to poor base co-clusterings.}
}


@article{DBLP:journals/tkde/BajajCS21,
	author = {Sumeet Bajaj and
                  Anrin Chakraborti and
                  Radu Sion},
	title = {ConcurDB: Concurrent Query Authentication for Outsourced Databases},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1401--1412},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2943557},
	doi = {10.1109/TKDE.2019.2943557},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BajajCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clients of outsourced databases need Query Authentication (QA) guaranteeing the integrity and authenticity of query results returned by potentially compromised providers. Prior work provides QA assurances for a limited class of queries by deploying several software-based cryptographic constructs. The constructs are often designed assuming read-only or infrequently updated databases. For dynamic datasets, the data owner is required to perform all updates on behalf of clients. Hence, for concurrent updates by multiple clients, such as for OLTP workloads, existing QA solutions are inefficient. We present ConcurDB, a concurrent QA scheme that enables simultaneous updates by multiple clients. To realize concurrent QA, we have designed several new mechanisms. First, we identify and use an important relationship between QA and memory checking to decouple query execution and verification. We allow clients to execute transactions concurrently and perform verifications in parallel using an offline memory checking based protocol. Then, to extend QA to a multi-client scenario, we design new protocols that enable clients to securely exchange a small set of authentication data even when using the untrusted provider as a communication hub. Finally, we overcome provider-side replay attacks. Using ConcurDB, we provide and evaluate concurrent QA for the full TPC-C benchmark. For updates, ConcurDB shows a 4x performance increase over existing solutions.}
}


@article{DBLP:journals/tkde/ShiHSWWDY21,
	author = {Chuan Shi and
                  Xiaotian Han and
                  Li Song and
                  Xiao Wang and
                  Senzhang Wang and
                  Junping Du and
                  Philip S. Yu},
	title = {Deep Collaborative Filtering with Multi-Aspect Information in Heterogeneous
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1413--1425},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2941938},
	doi = {10.1109/TKDE.2019.2941938},
	timestamp = {Mon, 07 Jun 2021 12:47:23 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShiHSWWDY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, recommender systems play a pivotal role in alleviating the problem of information overload. Latent factor models have been widely used for recommendation. Most existing latent factor models mainly utilize the interaction information between users and items, although some recently extended models utilize some auxiliary information to learn a unified latent factor for users and items. The unified latent factor only represents the characteristics of users and the properties of items from the aspect of purchase history. However, the characteristics of users and the properties of items may stem from different aspects, e.g., the brand-aspect and category-aspect of items. Moreover, the latent factor models usually use the shallow projection, which cannot capture the characteristics of users and items well. Deep neural network has shown tremendous potential to model the non-linearity relationship between users and items. It can be used to replace shallow projection to model the complex correlation between users and items. In this paper, we propose a Neural network based Aspect-level Collaborative Filtering model (NeuACF) to exploit different aspect latent factors. Through modelling the rich object properties and relations in recommender system as a heterogeneous information network, NeuACF first extracts different aspect-level similarity matrices of users and items, respectively, through different meta-paths, and then feeds an elaborately designed deep neural network with these matrices to learn aspect-level latent factors. Finally, the aspect-level latent factors are fused for the top-N recommendation. Moreover, to fuse information from different aspects more effectively, we further propose NeuACF++ to fuse aspect-level latent factors with self-attention mechanism. Extensive experiments on three real world datasets show that NeuACF and NeuACF++ significantly outperform both existing latent factor models and recent neural network models.}
}


@article{DBLP:journals/tkde/CaoIGRG21,
	author = {Guanqun Cao and
                  Alexandros Iosifidis and
                  Moncef Gabbouj and
                  Vijay Raghavan and
                  Raju Gottumukkala},
	title = {Deep Multi-View Learning to Rank},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1426--1438},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2942590},
	doi = {10.1109/TKDE.2019.2942590},
	timestamp = {Wed, 07 Apr 2021 15:58:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CaoIGRG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of learning to rank from multiple information sources. Though multi-view learning and learning to rank have been studied extensively leading to a wide range of applications, multi-view learning to rank as a synergy of both topics has received little attention. The aim of the paper is to propose a composite ranking method while keeping a close correlation with the individual rankings simultaneously. We present a generic framework for multi-view subspace learning to rank (MvSL2R), and two novel solutions are introduced under the framework. The first solution captures information of feature mappings from within each view as well as across views using autoencoder-like networks. Novel feature embedding methods are formulated in the optimization of multi-view unsupervised and discriminant autoencoders. Moreover, we introduce an end-to-end solution to learning towards both the joint ranking objective and the individual rankings. The proposed solution enhances the joint ranking with minimum view-specific ranking loss, so that it can achieve the maximum global view agreements in a single optimization process. The proposed method is evaluated on three different ranking problems, i.e., university ranking, multi-view lingual text ranking, and image data ranking, providing superior results compared to related methods.}
}


@article{DBLP:journals/tkde/LukasBKL21,
	author = {Petr Luk{\'{a}}s and
                  Radim Baca and
                  Michal Kr{\'{a}}tk{\'{y}} and
                  Tok Wang Ling},
	title = {Demythization of Structural {XML} Query Processing: Comparison of
                  Holistic and Binary Approaches},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1439--1452},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2946157},
	doi = {10.1109/TKDE.2019.2946157},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LukasBKL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {XML queries can be modeled by twig pattern queries (TPQs) specifying predicates on XML nodes and XPath relationships satisfied between them. A lot of TPQ types have been proposed; this paper takes into account a TPQ model extended by a specification of output and non-output query nodes since it complies with the XQuery semantics and, in many cases, it leads to a more efficient query processing. In general, there are two types of approaches to process a TPQ: holistic joins and binary joins. Whereas the binary join approach builds a query plan as a tree of interconnected binary operators, the holistic join approach evaluates a whole query using one operator (i.e., using one complex algorithm). Surprisingly, a thorough analytical and experimental comparison is still missing despite an enormous research effort in this area. In this paper, we try to fill this gap; we analytically and experimentally show that the binary joins used in a fully-pipelined plan (i.e., the plan where each join operation does not wait for the complete result of the previous operation and no explicit sorting is used) can often outperform the holistic joins, especially for TPQs with a higher ratio of non-output query nodes. The main contributions of this paper can be summarized as follows: (i) we introduce several improvements of existing binary join approaches allowing to build a fully-pipelined plan for a TPQ considering non-output query nodes, (ii) we prove that for a certain class of TPQs such a plan has the linear time complexity with respect to the size of the input and output as well as the linear space complexity with respect to the XML document depth (i.e., the same complexity as the holistic join approaches), (iii) we show that our improved binary join approach outperforms the holistic join approaches in many situations, and (iv) we propose a simple combined approach that utilizes advantages of both types of approaches.}
}


@article{DBLP:journals/tkde/LuC21,
	author = {Xiaolei Lu and
                  Tommy W. S. Chow},
	title = {Duration Modeling with Semi-Markov Conditional Random Fields for Keyphrase
                  Extraction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1453--1466},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2942295},
	doi = {10.1109/TKDE.2019.2942295},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing methods for keyphrase extraction need preprocessing to generate candidate phrase or post-processing to transform keyword into keyphrase. In this paper, we propose a novel approach called duration modeling with semi-Markov Conditional Random Fields (DM-SMCRFs) for keyphrase extraction. First of all, based on the property of semi-Markov chain, DM-SMCRFs can encode segment-level features and sequentially classify the phrase in the sentence as keyphrase or non-keyphrase. Second, by assuming the independence between state transition and state duration, DM-SMCRFs model the distribution of duration (length) of keyphrases to further explore state duration information, which can help identify the size of keyphrase. Based on the convexity of parametric duration feature derived from duration distribution, a constrained Viterbi algorithm is derived to improve the performance of decoding in DM-SMCRFs. We thoroughly evaluate the performance of DM-SMCRFs on the datasets from various domains. The experimental results demonstrate the effectiveness of proposed model.}
}


@article{DBLP:journals/tkde/BhardwajBCLMTT21,
	author = {Akansha Bhardwaj and
                  Albert Blarer and
                  Philippe Cudr{\'{e}}{-}Mauroux and
                  Vincent Lenders and
                  Boris Motik and
                  Axel Tanner and
                  Alberto Tonon},
	title = {Event Detection on Microposts: {A} Comparison of Four Approaches},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1467--1478},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2944815},
	doi = {10.1109/TKDE.2019.2944815},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BhardwajBCLMTT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Microblogging services such as Twitter are important, up-to-date, and live sources of information on a multitude of topics and events. An increasing number of systems use such services to detect and analyze events in real-time as they unfold. In this context, we recently proposed ArmaTweet —a system developed in collaboration among armasuisse and the Universities of Oxford and Fribourg to support semantic event detection on Twitter streams. Our experiments have shown that ArmaTweet is successful at detecting many complex events that cannot be detected by simple keyword-based search methods alone. Building up on this work, we explore in this paper several approaches for event detection on microposts. In particular, we describe and compare four different approaches based on keyword search ( Plain-Seed-Query ), information retrieval ( Temporal Query Expansion ), Word2Vec word embeddings ( Embedding ), and semantic retrieval ( ArmaTweet ). We provide an extensive empirical evaluation of these techniques using a benchmark dataset of about 200 million tweets on six event categories that we collected. While the performance of individual systems varies depending on the event category, our results show that ArmaTweet outperforms the other approaches on five out of six categories, and that a combined approach offers highest recall without adversely affecting precision of event detection.}
}


@article{DBLP:journals/tkde/HaririKB21,
	author = {Sahand Hariri and
                  Matias Carrasco Kind and
                  Robert J. Brunner},
	title = {Extended Isolation Forest},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1479--1489},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947676},
	doi = {10.1109/TKDE.2019.2947676},
	timestamp = {Thu, 27 Jul 2023 08:18:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HaririKB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present an extension to the model-free anomaly detection algorithm, Isolation Forest. This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points. We motivate the problem using heat maps for anomaly scores. These maps suffer from artifacts generated by the criteria for branching operation of the binary tree. We explain this problem in detail and demonstrate the mechanism by which it occurs visually. We then propose two different approaches for improving the situation. First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. This approach results in remedying the artifact seen in the anomaly score heat maps. We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.}
}


@article{DBLP:journals/tkde/ChengLQ21,
	author = {Gong Cheng and
                  Daxin Liu and
                  Yuzhong Qu},
	title = {Fast Algorithms for Semantic Association Search and Pattern Mining},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1490--1502},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2942031},
	doi = {10.1109/TKDE.2019.2942031},
	timestamp = {Thu, 22 Dec 2022 10:12:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChengLQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a large graph representing relations between entities, searching for complex relationships (called semantic associations, or SAs for short) between a set of entities is a common type of information needs in many domains. Further, numerous SAs are often abstracted into a few frequent high-level conceptual graph patterns (called SA patterns, or SAPs for short), which organize SAs into interpretable subgroups. Whereas the quality and usefulness of SAs and SAPs have been extensively studied in the literature, in this article we aim to develop faster algorithms for SA search and frequent SAP mining. For the former problem, we leverage distances to prune the search space, and implement a distance oracle to balance the time and space for distance calculation. For the latter problem, we exploit both graph structure and labels to induce fine-grained skeleton-based partitions of SAs, which may be pruned to reduce SAP enumeration. Besides, we generate canonical codes for SAs, which not only enable result deduplication but also are reused in SAP mining to improve the overall performance. We extensively evaluate the efficiency of our algorithms on four large graphs, using both random queries and simulated queries which reproduce the extreme case of finding numerous SAs.}
}


@article{DBLP:journals/tkde/LipaniLZL21,
	author = {Aldo Lipani and
                  David E. Losada and
                  Guido Zuccon and
                  Mihai Lupu},
	title = {Fixed-Cost Pooling Strategies},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1503--1522},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947049},
	doi = {10.1109/TKDE.2019.2947049},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LipaniLZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The empirical nature of Information Retrieval (IR) mandates strong experimental practices. A keystone of such experimental practices is the Cranfield evaluation paradigm. Within this paradigm, the collection of relevance judgments has been the subject of intense scientific investigation. This is because, on one hand, consistent, precise, and numerous judgements are keys to reducing evaluation uncertainty and test collection bias; on the other hand, however, relevance judgements are costly to collect. The selection of which documents to judge for relevance, known as pooling method, has therefore a great impact on IR evaluation. In this paper we focus on the bias introduced by the pooling method, known as pool bias, which affects the reusability of test collections, in particular when building test collections with a limited budget. In this paper we formalize and evaluate a set of 22 pooling strategies based on: traditional strategies, voting systems, retrieval fusion methods, evaluation measures, and multi-armed bandit models. To do this we run a large-scale evaluation by considering a set of 9 standard TREC test collections, in which we show that the choice of the pooling strategy has significant effects on the cost needed to obtain an unbiased test collection. We also identify the least biased pooling strategy in terms of pool bias according to three IR evaluation measures: AP, NDCG, and P@10.}
}


@article{DBLP:journals/tkde/ZhangZLLZYW21,
	author = {Zhao Zhang and
                  Yan Zhang and
                  Sheng Li and
                  Guangcan Liu and
                  Dan Zeng and
                  Shuicheng Yan and
                  Meng Wang},
	title = {Flexible Auto-Weighted Local-Coordinate Concept Factorization: {A}
                  Robust Framework for Unsupervised Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1523--1539},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2940576},
	doi = {10.1109/TKDE.2019.2940576},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZLLZYW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Concept Factorization (CF) and its variants may produce inaccurate representation and clustering results due to the sensitivity to noise, hard constraint on the reconstruction error, and pre-obtained approximate similarities. To improve the representation ability, a novel unsupervised Robust Flexible Auto-weighted Local-coordinate Concept Factorization (RFA-LCF) framework is proposed for clustering high-dimensional data. Specifically, RFA-LCF integrates the robust flexible CF by clean data space recovery, robust sparse local-coordinate coding, and adaptive weighting into a unified model. RFA-LCF improves the representations by enhancing the robustness of CF to noise and errors, providing a flexible constraint on the reconstruction error and optimizing the locality jointly. For robust learning, RFA-LCF clearly learns a sparse projection to recover the underlying clean data space, and then the flexible CF is performed in the projected feature space. RFA-LCF also uses a L 2,1 -norm based flexible residue to encode the mismatch between the recovered data and its reconstruction, and uses the robust sparse local-coordinate coding to represent data using a few nearby basis concepts. For auto-weighting, RFA-LCF jointly preserves the manifold structures in the basis concept space and new coordinate space in an adaptive manner by minimizing the reconstruction errors on clean data, anchor points and coordinates. By updating the local-coordinate preserving data, basis concepts and new coordinates alternately, the representation abilities can be potentially improved. Extensive results on public databases show that RFA-LCF delivers the state-of-the-art clustering results compared with other related methods.}
}


@article{DBLP:journals/tkde/TolomeiS21,
	author = {Gabriele Tolomei and
                  Fabrizio Silvestri},
	title = {Generating Actionable Interpretations from Ensembles of Decision Trees},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1540--1553},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2945326},
	doi = {10.1109/TKDE.2019.2945326},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TolomeiS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine-learned models are often perceived as “black boxes”: they are given inputs and hopefully produce desired outputs. There are many circumstances, however, where human-interpretability is crucial to understand (i) why a model outputs a certain prediction on a given instance, (ii) which adjustable features of that instance should be modified, and finally (iii) how to alter a prediction when the mutated instance is input back to the model. In this paper, we present a technique that exploits the feedback loop originated from the internals of any ensemble of decision trees to offer recommendations for transforming a k-labelled predicted instance into a k'-labelled one (for any possible pair of class labels k, k'). Our proposed algorithm perturbs individual feature values of an instance, so as to change the original prediction output by the ensemble on the so-transformed instance. This is also achieved under two constraints: the cost- and tolerance of transformation. Finally, we evaluate our approach on four distinct application domains: online advertising, healthcare, spam filtering, and handwritten digit recognition. Experiments confirm that our solution is able to suggest changes to feature values that help interpreting the rationale of model predictions, making it indeed useful in practice especially if implemented efficiently.}
}


@article{DBLP:journals/tkde/HeWZC21,
	author = {Dan He and
                  Sibo Wang and
                  Xiaofang Zhou and
                  Reynold Cheng},
	title = {{GLAD:} {A} Grid and Labeling Framework with Scheduling for Conflict-Aware
                  kNN Queries},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1554--1566},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2942585},
	doi = {10.1109/TKDE.2019.2942585},
	timestamp = {Sun, 12 Nov 2023 02:19:27 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HeWZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The intelligent transportation systems, e.g., DiDi and Uber, have served as essential travel tools for customers, which foster plenty of studies for the location-based queries on road network. In particular, given a set O of objects and a query point q on a road network, the k Nearest Neighbor (kNN) query returns the k nearest objects in O with the shortest road network distance to q. In literature, most existing solutions for kNN queries tend to reduce the query time, indexing storage, or throughput of the kNN queries while overlooking the correctness of the queries caused by query-query and update-query conflicts. In our work, we propose a grid-based framework on conflict-aware kNN queries on moving objects which aims to optimize system throughput while guaranteeing query correctness. In particular, we first propose efficient index structures and new query algorithms that significantly improve the throughput. We further present novel scheduling algorithms that aim to avoid conflicts and improve the system throughput. Moreover, we devise approximate solutions that provide a controllable trade-off between the conflict of kNN queries and system throughput. Finally, we propose a cost-based dispatching strategy to assign the kNN results to the corresponding queries. Extensive experiments on real-world data demonstrate the effectiveness and efficiency of our proposed solutions over alternatives.}
}


@article{DBLP:journals/tkde/KanellosVSDV21,
	author = {Ilias Kanellos and
                  Thanasis Vergoulis and
                  Dimitris Sacharidis and
                  Theodore Dalamagas and
                  Yannis Vassiliou},
	title = {Impact-Based Ranking of Scientific Publications: {A} Survey and Experimental
                  Evaluation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1567--1584},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2941206},
	doi = {10.1109/TKDE.2019.2941206},
	timestamp = {Sat, 09 Apr 2022 12:22:57 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KanellosVSDV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the rate at which scientific work is published continues to increase, so does the need to discern high-impact publications. In recent years, there have been several approaches that seek to rank publications based on their expected citation-based impact. Despite this level of attention, this research area has not been systematically studied. Past literature often fails to distinguish between short-term impact, the current popularity of an article, and long-term impact, the overall influence of an article. Moreover, the evaluation methodologies applied vary widely and are inconsistent. In this work, we aim to fill these gaps, studying impact-based ranking theoretically and experimentally. First, we provide explicit definitions for short-term and long-term impact, and introduce the associated ranking problems. Then, we identify and classify the most important ideas employed by state-of-the-art methods. After studying various evaluation methodologies of the literature, we propose a specific benchmark framework that can help us better differentiate effectiveness across impact aspects. Using this framework we investigate: (1) the practical difference between ranking by short- and long-term impact, and (2) the effectiveness and efficiency of ranking methods in different settings. To avoid reporting results that are discipline-dependent, we perform our experiments using four datasets from different scientific disciplines.}
}


@article{DBLP:journals/tkde/SunLWOSZ21,
	author = {Guohao Sun and
                  Guanfeng Liu and
                  Yan Wang and
                  Mehmet A. Orgun and
                  Quan Z. Sheng and
                  Xiaofang Zhou},
	title = {Incremental Graph Pattern Based Node Matching with Multiple Updates},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1585--1600},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2942294},
	doi = {10.1109/TKDE.2019.2942294},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SunLWOSZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Pattern based Node Matching (GPNM) has been proposed to find all the matches of the nodes in a data graph GD based on a given pattern graph GP. GPNM has been increasingly adopted in many applications such as group finding and expert recommendation, in which data graphs are frequently updated overtime. Moreover, many typical pattern graphs frequently and repeatedly appear in users' queries in a short period of time, e.g., social graph searches on Facebook. To deliver a GPNM result in such applications, the existing GPNM methods have to perform an incremental GPNM procedure for each of the updates in the data graph, which is computationally expensive. To address this problem, in this paper, we first analyze the elimination relationships between multiple updates in GD and the hierarchical structure between these elimination relationships. Then, we generate an Elimination Hierarchy Tree (EH-Tree) to index the elimination relationships and propose an EH-Tree based GPNM method, called EHGPNM, considering the elimination relationships between multiple updates in GD. EH-GPNM first delivers the GPNM result of an initial query, and then delivers the GPNM result of a subsequent query, based on the initial GPNM result and the multiple updates of GD that occur between those two queries. The experimental results on five real-world social graphs demonstrate that our proposed EH-GPNM is much more efficient than the state-of-the-art GPNM methods.}
}


@article{DBLP:journals/tkde/ZhangDW21a,
	author = {Bo Zhang and
                  Boxiang Dong and
                  Wendy Hui Wang},
	title = {Integrity Authentication for {SQL} Query Evaluation on Outsourced
                  Databases: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1601--1618},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947061},
	doi = {10.1109/TKDE.2019.2947061},
	timestamp = {Thu, 12 Aug 2021 18:47:46 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangDW21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spurred by the development of cloud computing, there has been considerable recent interest in the Database-as-a-Service (DaaS) paradigm. Users lacking in expertise or computational resources can outsource their data and database management needs to a third-party service provider. Outsourcing, however, raises an important issue of result integrity: how can the client verify with lightweight overhead that the query results returned by the service provider are correct (i.e., the same as the results of query execution locally)? This survey focuses on categorizing and reviewing the progress on the current approaches for result integrity of SQL query evaluation in the DaaS model. The survey also includes some potential future research directions for result integrity verification of the outsourced computations.}
}


@article{DBLP:journals/tkde/JiaLZLH21,
	author = {Xiuyi Jia and
                  Zechao Li and
                  Xiang Zheng and
                  Weiwei Li and
                  Sheng{-}Jun Huang},
	title = {Label Distribution Learning with Label Correlations on Local Samples},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1619--1631},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2943337},
	doi = {10.1109/TKDE.2019.2943337},
	timestamp = {Mon, 17 Jun 2024 08:05:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiaLZLH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Label distribution learning (LDL) is proposed for solving the label ambiguity problem in recent years, which can be seen as an extension of multi-label learning. To improve the performance of label distribution learning, some existing algorithms exploit label correlations in a global manner that assumes the label correlations are shared by all instances. However, the instances in different groups may share different label correlations, and few label correlations are globally applicable in real-world tasks. In this paper, two novel label distribution learning algorithms are proposed by exploiting label correlations on local samples, which are called GD-LDL-SCL and Adam-LDL-SCL, respectively. To utilize the label correlations on local samples, the influence of local samples is encoded, and a local correlation vector is designed as the additional features for each instance, which is based on the different clustered local samples. Then, the label distribution for an unseen instance can be predicted by exploiting the original features and the additional features simultaneously. Extensive experiments on some real-world data sets validate that our proposed methods can address the label distribution problems effectively and outperform state-of-the-art methods.}
}


@article{DBLP:journals/tkde/XuLG21,
	author = {Ning Xu and
                  Yun{-}Peng Liu and
                  Xin Geng},
	title = {Label Enhancement for Label Distribution Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1632--1643},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947040},
	doi = {10.1109/TKDE.2019.2947040},
	timestamp = {Fri, 03 Nov 2023 18:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XuLG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Label distribution is more general than both single-label annotation and multi-label annotation. It covers a certain number of labels, representing the degree to which each label describes the instance. The learning process on the instances labeled by label distributions is called label distribution learning (LDL). Unfortunately, many training sets only contain simple logical labels rather than label distributions due to the difficulty of obtaining the label distributions directly. To solve this problem, one way is to recover the label distributions from the logical labels in the training set via leveraging the topological information of the feature space and the correlation among the labels. Such process of recovering label distributions from logical labels is defined as label enhancement (LE), which reinforces the supervision information in the training sets. This paper proposes a novel LE algorithm called Graph Laplacian Label Enhancement (GLLE). Experimental results on one artificial dataset and fourteen real-world LDL datasets show clear advantages of GLLE over several existing LE algorithms. Furthermore, experimental results on eleven multi-label learning datasets validate the advantage of GLLE over the state-of-the-art multi-label learning approaches.}
}


@article{DBLP:journals/tkde/LiMCS21,
	author = {Mao{-}Lin Li and
                  Francesco Di Mauro and
                  K. Sel{\c{c}}uk Candan and
                  Maria Luisa Sapino},
	title = {Matrix Factorization with Interval-Valued Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1644--1658},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2942310},
	doi = {10.1109/TKDE.2019.2942310},
	timestamp = {Wed, 07 Apr 2021 15:58:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiMCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With many applications relying on multi-dimensional datasets for decision making, matrix factorization (or decomposition) is becoming the basis for many knowledge discoveries and machine learning tasks, from clustering, trend detection, anomaly detection, to correlation analysis. Unfortunately, a major shortcoming of matrix analysis operations is that, despite their effectiveness when the data is scalar, these operations become difficult to apply in the presence of non-scalar data, as they are not designed for data that include non-scalar observations, such as intervals. Yet, in many applications, the available data are inherently non-scalar for various reasons, including imprecision in data collection, conflicts in aggregated data, data summarization, or privacy issues, where one is provided with a reduced, clustered, or intentionally noisy and obfuscated version of the data to hide information. In this paper, we propose matrix decomposition techniques that consider the existence of interval-valued data. We show that naive ways to deal with such imperfect data may introduce errors in analysis and present factorization techniques that are especially effective when the amount of imprecise information is large.}
}


@article{DBLP:journals/tkde/WuPCGZX21,
	author = {Sai Wu and
                  Zhifei Pang and
                  Gang Chen and
                  Yunjun Gao and
                  Cenjiong Zhao and
                  Shili Xiang},
	title = {{NEIST:} {A} Neural-Enhanced Index for Spatio-Temporal Queries},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1659--1673},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2945947},
	doi = {10.1109/TKDE.2019.2945947},
	timestamp = {Wed, 07 Apr 2021 15:58:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuPCGZX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Previous work on the spatio-temporal index often adopts a simple linear model to predict the future positions of moving objects, which may generate numerous errors for complex road networks and fast moving objects. In this paper, we propose NEIST, a neural-enhanced index to process spatio-temporal queries with enhanced efficiency and accuracy, by intelligently leveraging the movement patterns among moving objects. NEIST applies a Recurrent Neural Network (RNN) model to predict future positions of moving objects based on observed trajectories. To reduce the prediction overhead, a suffix-tree is further built to index trajectories with similar suffixes, and thus similar objects within a given similarity bound are grouped together to share the same prediction result. A prediction result in NEIST represents possible positions of a group of moving objects in the next t time slots. Inside each time slot, traditional linear prediction model is then adopted and a TPR-Tree is built to support spatio-temporal queries. We use Singapore and Porto taxi trajectory datasets to evaluate NEIST. Compared to previous approaches, NEIST achieves a much more efficient query performance and is able to produce about 70 percent more accurate results.}
}


@article{DBLP:journals/tkde/HalimAK21,
	author = {Zahid Halim and
                  Omer Ali and
                  Muhammad Ghufran Khan},
	title = {On the Efficient Representation of Datasets as Graphs to Mine Maximal
                  Frequent Itemsets},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1674--1691},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2945573},
	doi = {10.1109/TKDE.2019.2945573},
	timestamp = {Fri, 09 Apr 2021 18:22:30 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HalimAK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Frequent itemsets mining is an active research problem in the domain of data mining and knowledge discovery. With the advances in database technology and an exponential increase in data to be stored, there is a need for efficient approaches that can quickly extract useful information from such large datasets. Frequent Itemsets (FIs) mining is a data mining task to find itemsets in a transactional database which occur together above a certain frequency. Finding these FIs usually requires multiple passes over the databases; therefore, making efficient algorithms crucial for mining FIs. This work presents a graph-based approach for representing a complete transactional database. The proposed graph-based representation enables the storing of all relevant information (for extracting FIs) of the database in one pass. Later, an algorithm that extracts the FIs from the graph-based structure is presented. Experimental results are reported comparing the proposed approach with 17 related FIs mining methods using six benchmark datasets. Results show that the proposed approach performs better than others in terms of time.}
}


@article{DBLP:journals/tkde/PengGZOXZ21,
	author = {Peng Peng and
                  Qi Ge and
                  Lei Zou and
                  M. Tamer {\"{O}}zsu and
                  Zhiwei Xu and
                  Dongyan Zhao},
	title = {Optimizing Multi-Query Evaluation in Federated {RDF} Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1692--1707},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947050},
	doi = {10.1109/TKDE.2019.2947050},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PengGZOXZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper revisits the classical problem of multiple query optimization in federated RDF systems. We propose a heuristic query rewriting-based approach to optimize the evaluation of multiple queries. This approach can take advantage of SPARQL 1.1 to share the common computation of multiple queries while considering the cost of both query evaluation and data shipment. Although we prove that finding the optimal rewriting for multiple queries is NP-complete, we propose a heuristic rewriting algorithm with a bounded approximation ratio. Furthermore, we propose an efficient method to use the interconnection topology between RDF sources to filter out irrelevant sources, and utilize some characteristics of SPARQL 1.1 to optimize multiple joins of intermediate matches. The extensive experimental studies show that the proposed techniques are effective, efficient and scalable.}
}


@article{DBLP:journals/tkde/ZhaoXLSZXZ21,
	author = {Pengpeng Zhao and
                  Chengfeng Xu and
                  Yanchi Liu and
                  Victor S. Sheng and
                  Kai Zheng and
                  Hui Xiong and
                  Xiaofang Zhou},
	title = {Photo2Trip: Exploiting Visual Contents in Geo-Tagged Photos for Personalized
                  Tour Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1708--1721},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2943854},
	doi = {10.1109/TKDE.2019.2943854},
	timestamp = {Fri, 21 Jan 2022 21:59:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoXLSZXZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently accumulated massive amounts of geo-tagged photos provide an excellent opportunity to understand human behaviors and can be used for personalized tour recommendation. However, no existing work has considered the visual content information in these photos for tour recommendation. We believe the visual features of photos provide valuable information on measuring user / Point-of-Interest (POI) similarities, which is challenging due to data sparsity. To this end, in this paper, we propose a visual feature enhanced tour recommender system, named ‘Photo2Trip’, to utilize the visual contents and collaborative filtering models for recommendation. Specifically, we propose a Visual-enhanced Probabilistic Matrix Factorization model (VPMF), which integrates visual features into the collaborative filtering model, to learn user interests by leveraging the historical travel records. We then extend VPMF to End-to-End training framework to incorporate users (POIs) latent factors into the learning process of the visual content of photos, which generalizes the applicability of the proposed VPMF framework in tour recommendation. Extensive empirical studies verify that our proposed visual-enhanced personalized tour recommendation method outperforms other benchmark methods in terms of recommendation accuracy. The results also show that visual features are effective in alleviating the data sparsity and cold start problems on personalized tour recommendation.}
}


@article{DBLP:journals/tkde/BaiAEM21,
	author = {Xiao Bai and
                  Reza Abasi and
                  Bora Edizel and
                  Amin Mantrach},
	title = {Position-Aware Deep Character-Level {CTR} Prediction for Sponsored
                  Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1722--1736},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2941881},
	doi = {10.1109/TKDE.2019.2941881},
	timestamp = {Wed, 07 Apr 2021 15:58:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BaiAEM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting the click-through rate of an advertisement is a critical component of online advertising platforms. In sponsored search, the click-through rate estimates the probability that a displayed advertisement is clicked by a user after she submits a query to the search engine. Commercial search engines typically rely on machine learning models trained with a large number of features to make such predictions. This inevitably requires a lot of engineering efforts to define, compute, and select the appropriate features. In this paper, we propose two novel approaches (one working at character level and the other working at word level) that use deep convolutional neural networks to predict the click-through rate of a query-advertisement pair. Specifically, the proposed architectures consider as input only the textual content appearing in a query-advertisement pair and the page position at which the advertisement appears on the search result page of the query, and produce as output a click-through rate prediction. By comparing the character-level model with the word-level model, we show that language representation can be learnt from scratch at character level when trained on enough data. Through extensive experiments using billions of query-advertisement pairs of a popular commercial search engine, we demonstrate that both approaches significantly outperform a baseline model built on well-selected text features and a state-of-the-art word2vec-based approach. We also show the importance of the position feature in the proposed approaches in improving the prediction accuracy. When combining the predictions of the deep models introduced in this study with the prediction of the model in production of the same commercial search engine, we significantly improve the accuracy and the calibration of the click-through rate prediction of the production system. We also show the potential of leveraging the CTR prediction of the proposed deep learning models for query-ad relevance modeling and query-ad matching tasks in sponsored search.}
}


@article{DBLP:journals/tkde/WangHF21,
	author = {Yashen Wang and
                  Heyan Huang and
                  Chong Feng},
	title = {Query Expansion With Local Conceptual Word Embeddings in Microblog
                  Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1737--1749},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2945764},
	doi = {10.1109/TKDE.2019.2945764},
	timestamp = {Wed, 07 Apr 2021 15:58:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangHF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the length of microblog texts, such as tweets, is strictly limited to 140 characters, traditional Information Retrieval techniques suffer from the vocabulary mismatch problem severely and cannot yield good performance in the context of microblogosphere. To address this critical challenge, in this paper, we focus on the use of local conceptual word embeddings for enhance microblog retrieval effectiveness. In particular, we propose a novel k-Nearest Neighbor (kNN) based Query Expansion (QE) algorithm to generate words from local word embeddings to expand the original query, which leads to better understanding of the information need. Besides, in order to further satisfy users' real-time information need, we incorporate temporal evidences into the expansion algorithm, which can boost recent tweets in the retrieval results with respect to a given topic. Experimental results on the official TREC Twitter corpora demonstrate the significant superiority of our approach over baseline methods.}
}


@article{DBLP:journals/tkde/KekecT21,
	author = {Taygun Keke{\c{c}} and
                  David M. J. Tax},
	title = {Sem2Vec: Semantic Word Vectors with Bidirectional Constraint Propagations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1750--1762},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2942021},
	doi = {10.1109/TKDE.2019.2942021},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KekecT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Word embeddings learn a vector representation of words, which can be utilized in a large number of natural language processing applications. Learning these vectors shares the drawback of unsupervised learning: representations are not specialized for semantic tasks. In this work, we propose a full-fledged formulation to effectively learn semantically specialized word vectors (Sem2Vec) by creating shared representations of online lexical sources such as Thesaurus and lexical dictionaries. These shared representations are treated as semantic constraints for learning the word embeddings. Our methodology addresses size limitation and weak informativeness of these lexical sources by employing a bidirectional constraint propagation step. Unlike raw unsupervised embeddings that exhibit low stability and easily subject to changes under randomness, our semantic formulation learns word vectors that are quite stable. An extensive empirical evaluation on the word similarity task comprised of 11 word similarity datasets is provided where our vectors suggest notable performance gains over state of the art competitors. We further demonstrate the merits of our formulation in document text classification task over large collections of documents.}
}


@article{DBLP:journals/tkde/LiuXWXKTK21,
	author = {Jiaying Liu and
                  Feng Xia and
                  Lei Wang and
                  Bo Xu and
                  Xiangjie Kong and
                  Hanghang Tong and
                  Irwin King},
	title = {Shifu2: {A} Network Representation Learning Based Model for Advisor-Advisee
                  Relationship Mining},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1763--1777},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2946825},
	doi = {10.1109/TKDE.2019.2946825},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuXWXKTK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advisor-advisee relationship represents direct knowledge heritage, and such relationship may not be readily available from academic libraries and search engines. This work aims to discover advisor-advisee relationships hidden behind scientific collaboration networks. For this purpose, we propose a novel model based on Network Representation Learning (NRL), namely Shifu2, which takes the collaboration network as input and the identified advisor-advisee relationship as output. In contrast to existing NRL models, Shifu2 considers not only the network structure but also the semantic information of nodes and edges. Shifu2 encodes nodes and edges into low-dimensional vectors respectively, both of which are then utilized to identify advisor-advisee relationships. Experimental results illustrate improved stability and effectiveness of the proposed model over state-of-the-art methods. In addition, we generate a large-scale academic genealogy dataset by taking advantage of Shifu2.}
}


@article{DBLP:journals/tkde/YeWRS21,
	author = {Wei Ye and
                  Zhen Wang and
                  Rachel Redberg and
                  Ambuj K. Singh},
	title = {Tree++: Truncated Tree Based Graph Kernels},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1778--1789},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2946149},
	doi = {10.1109/TKDE.2019.2946149},
	timestamp = {Sun, 16 May 2021 00:12:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YeWRS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-structured data arise ubiquitously in many application domains. A fundamental problem is to quantify their similarities. Graph kernels are often used for this purpose, which decompose graphs into substructures and compare these substructures. However, most of the existing graph kernels do not have the property of scale-adaptivity, i.e., they cannot compare graphs at multiple levels of granularities. Many real-world graphs such as molecules exhibit structure at varying levels of granularities. To tackle this problem, we propose a new graph kernel called Tree++ in this paper. At the heart of Tree++ is a graph kernel called the path-pattern graph kernel. The path-pattern graph kernel first builds a truncated BFS tree rooted at each vertex and then uses paths from the root to every vertex in the truncated BFS tree as features to represent graphs. The path-pattern graph kernel can only capture graph similarity at fine granularities. In order to capture graph similarity at coarse granularities, we incorporate a new concept called super path into it. The super path contains truncated BFS trees rooted at the vertices in a path. Our evaluation on a variety of real-world graphs demonstrates that Tree++ achieves the best classification accuracy compared with previous graph kernels.}
}


@article{DBLP:journals/tkde/LiSM21,
	author = {Jing Li and
                  Aixin Sun and
                  Yukun Ma},
	title = {Neural Named Entity Boundary Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1790--1795},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2981329},
	doi = {10.1109/TKDE.2020.2981329},
	timestamp = {Mon, 28 Jun 2021 08:44:53 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiSM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we focus on named entity boundary detection, which is to detect the start and end boundaries of an entity mention in text, without predicting its type. The detected entities are input to entity linking or fine-grained typing systems for semantic enrichment. We propose BdryBot, a recurrent neural network encoder-decoder framework with a pointer network to detect entity boundaries from a given sentence. The encoder considers both character-level representations and word-level embeddings to represent the input words. In this way, BdryBot does not require any hand-crafted features. Because of the pointer network, BdryBot overcomes the problem of variable size output vocabulary and the issue of sparse boundary tags. We conduct two sets of experiments, in-domain detection and cross-domain detection, on six datasets. Our results show that BdryBot achieves state-of-the-art performance against five baselines. In addition, our proposed approach can be further enhanced when incorporating contextualized language embeddings into token representations.}
}


@article{DBLP:journals/tkde/MagguMC21,
	author = {Jyoti Maggu and
                  Angshul Majumdar and
                  Emilie Chouzenoux},
	title = {Transformed Subspace Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1796--1801},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2969354},
	doi = {10.1109/TKDE.2020.2969354},
	timestamp = {Mon, 27 May 2024 22:16:33 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MagguMC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subspace clustering assumes that the data is separable into separate subspaces. Such a simple assumption, does not always hold. We assume that, even if the raw data is not separable into subspaces, one can learn a representation (transform coefficients) such that the learnt representation is separable into subspaces. To achieve the intended goal, we embed subspace clustering techniques (locally linear manifold clustering, sparse subspace clustering and low rank representation) into transform learning. The entire formulation is jointly learnt; giving rise to a new class of methods called transformed subspace clustering (TSC). In order to account for non-linearity, kernelized extensions of TSC are also proposed. To test the performance of the proposed techniques, benchmarking is performed on image clustering and document clustering datasets. Comparison with state-of-the-art clustering techniques shows that our formulation improves upon them.}
}


@article{DBLP:journals/tkde/QiangW21,
	author = {Jipeng Qiang and
                  Xindong Wu},
	title = {Unsupervised Statistical Text Simplification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {4},
	pages = {1802--1806},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947679},
	doi = {10.1109/TKDE.2019.2947679},
	timestamp = {Wed, 07 Apr 2021 15:58:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QiangW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most recent approaches for Text Simplification (TS) have drawn on insights from machine translation to learn simplification rewrites from the monolingual parallel corpus of complex and simple sentences, yet their effectiveness strongly relies on large amounts of parallel sentences. However, there has been a serious problem haunting TS for decades, that is, the availability of parallel TS corpora is scarce or not fit for the learning task. In this paper, we will focus on one especially useful and challenging problem of unsupervised TS without a single parallel sentence. To the best of our knowledge, we present the first unsupervised text simplification system based on phrase-based machine translation system, which leverages a careful initialization of phrase tables and language models. On the widely used WikiLarge and WikiSmall benchmarks, our system respectively obtains 39.08 and 25.12 SARI points, even outperforms some supervised baselines.}
}


@article{DBLP:journals/tkde/KhoslaSA21,
	author = {Megha Khosla and
                  Vinay Setty and
                  Avishek Anand},
	title = {A Comparative Study for Unsupervised Network Representation Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1807--1818},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2951398},
	doi = {10.1109/TKDE.2019.2951398},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KhoslaSA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There has been significant progress in unsupervised network representation learning (UNRL) approaches over graphs recently with flexible random-walk approaches, new optimization objectives, and deep architectures. However, there is no common ground for systematic comparison of embeddings to understand their behavior for different graphs and tasks. We argue that most of the UNRL approaches either model and exploit neighborhood or what we call context information of a node. These methods largely differ in their definitions and exploitation of context. Consequently, we propose a framework that casts a variety of approaches – random walk based, matrix factorization and deep learning based – into a unified context-based optimization function. We systematically group the methods based on their similarities and differences. We study their differences which we later use to explain their performance differences (on downstream tasks). We conduct a large-scale empirical study considering nine popular and recent UNRL techniques and 11 real-world datasets with varying structural properties and two common tasks – node classification and link prediction. We find that for non-attributed graphs there is no single method that is a clear winner and that the choice of a suitable method is dictated by certain properties of the embedding methods, task and structural properties of the underlying graph. In addition, we also report the common pitfalls in evaluation of UNRL methods and come up with suggestions for experimental design and interpretation of results.}
}


@article{DBLP:journals/tkde/GatziouraVJS21,
	author = {Anna Gatzioura and
                  Jo{\~{a}}o Vinagre and
                  Al{\'{\i}}pio M{\'{a}}rio Jorge and
                  Miquel S{\`{a}}nchez{-}Marr{\`{e}}},
	title = {A Hybrid Recommender System for Improving Automatic Playlist Continuation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1819--1830},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2952099},
	doi = {10.1109/TKDE.2019.2952099},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GatziouraVJS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although widely used, the majority of current music recommender systems still focus on recommendations' accuracy, user preferences and isolated item characteristics, without evaluating other important factors, like the joint item selections and the recommendation moment. However, when it comes to playlist recommendations, additional dimensions, as well as the notion of user experience and perception, should be taken into account to improve recommendations' quality. In this work, HybA, a hybrid recommender system for automatic playlist continuation, that combines Latent Dirichlet Allocation and Case-Based Reasoning, is proposed. This system aims to address “similar concepts” rather than similar users. More than generating a playlist based on user requirements, like automatic playlist generation methods, HybA identifies the semantic characteristics of a started playlist and reuses the most similar past ones, to recommend relevant playlist continuations. In addition, support to beyond accuracy dimensions, like increased coherence or diverse items' discovery, is provided. To overcome the semantic gap between music descriptions and user preferences, identify playlist structures and capture songs' similarity, a graph model is used. Experiments on real datasets have shown that the proposed algorithm is able to outperform other state of the art techniques, in terms of accuracy, while balancing between diversity and coherence.}
}


@article{DBLP:journals/tkde/WuHCH21,
	author = {Junfeng Wu and
                  Jing He and
                  Chi{-}Hung Chi and
                  Guangyan Huang},
	title = {Absorbing Diagonal Algorithm: An Eigensolver of {\textdollar}O{\textbackslash}left(n{\^{}}\{2.584963\}{\textbackslash}log
                  {\textbackslash}frac\{1\}\{{\textbackslash}varepsilon \}{\textbackslash}right){\textdollar}On2.584963log1{\(\varepsilon\)}
                  Complexity at Accuracy {\textdollar}{\textbackslash}varepsilon{\textdollar}{\(\varepsilon\)}},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1831--1847},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2949309},
	doi = {10.1109/TKDE.2019.2949309},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuHCH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Eigenvalue decomposition is widely used in dimensionality reduction for knowledge engineering, in particular principal component analysis and other similar spectral methods. Traditional eigenvalue decomposition algorithms for decomposing a matrix of size n ×nn×n are usually of complexity O(n 3 )O(n3), due to a bottleneck in using Householder/Givens transforms to convert a general matrix to a tri-diagonal one. It is proposed in this article a new algorithm that takes only O(n 2.584963 log 1/ε) computational complexity to achieve accuracy ε of eigenvalue decomposition for any ε > 0ε>0. The basic idea of our algorithm is to convert a matrix into a diagonal form in multi-scale divide and conquer scheme, and the conversion is to iteratively and recursively apply two phases of operations called diagonal attractions and diagonal absorptions respectively. In a diagonal attraction, it attracts the off-diagonal entries to make the entries nearer to the diagonal larger in magnitude than those farther away from the diagonal. In a diagonal absorption, it absorbs the near-to-diagonal nonzero entries into the diagonal. In such a scheme, no Householder or Givens transforms are involved. Moreover, diagonal attractions and diagonal absorptions can be implemented with fast matrix multiplications. The scheme's divide and conquer pattern also allows our algorithm to be easily mapped to modern computer hardware. Our algorithm also complements well the family of randomized eigenvalue/SVD algorithms using sampling techinques, which are of complexity O(n α polylog(1/ε)) with small α but very large overheads in the polylog. Their strength in the small exponent αα of nn in complexity was easily cancelled by the exploding overheads in the polylog. Now, with their low-accuracy estimate refined by our algorithm for high accuracy, their strength can be boosted significantly.}
}


@article{DBLP:journals/tkde/RenAZ21,
	author = {Yuxiang Ren and
                  Charu C. Aggarwal and
                  Jiawei Zhang},
	title = {ActiveIter: Meta Diagram Based Active Learning in Social Networks
                  Alignment},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1848--1860},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947908},
	doi = {10.1109/TKDE.2019.2947908},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/RenAZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network alignment aims at inferring a set of anchor links matching the shared entities between different information networks, which has become a prerequisite step for effective fusion of multiple information networks. In this paper, we will study the network alignment problem to fuse online social networks specifically. Social network alignment is extremely challenging to address due to several reasons, i.e., lack of training data, network heterogeneity and one-to-one constraint. Existing network alignment works usually require a large number of training instances, but such a demand can hardly be met in applications, as manual anchor link labeling is extremely expensive. Significantly different from other homogeneous network alignment works, information in online social networks is usually of heterogeneous categories, the incorporation of which in model building is not an easy task. Furthermore, the one-to-one cardinality constraint on anchor links renders their inference process intertwistingly correlated. To resolve these three challenges, a novel network alignment model, namely ActiveIter (Active Iterative Alignment), is introduced in this paper. The model ActiveIter defines a set of inter-network meta diagrams for anchor link feature extraction, adopts active learning for effective label query and uses greedy link selection for anchor link cardinality filtering. Extensive experiments were performed on a real-world aligned networks dataset, and the experimental results have demonstrated the effectiveness of ActiveIter compared with other state-of-the-art baseline methods.}
}


@article{DBLP:journals/tkde/FangZZZZZ21,
	author = {Junhua Fang and
                  Rong Zhang and
                  Yan Zhao and
                  Kai Zheng and
                  Xiaofang Zhou and
                  Aoying Zhou},
	title = {{A-DSP:} An Adaptive Join Algorithm for Dynamic Data Stream on Cloud
                  System},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1861--1876},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947055},
	doi = {10.1109/TKDE.2019.2947055},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FangZZZZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The join operations, including both equi and non-equi joins, are essential to the complex data analytics in the big data era. However, they are not inherently supported by existing DSPEs ( D istributed S tream P rocessing E ngines). The state-of-the-art join solutions on DSPEs rely on either complicated routing strategies or resource-inefficient processing structures, which are susceptible to dynamic workload, especially when the DSPEs face various join predicate operations and skewed data distribution. In this paper, we propose a new cost-effective stream join framework, named A-DSP ( A daptive D imensional S pace P rocessing), which enhances the adaptability of real-time join model and minimizes the resource used over the dynamic workloads. Our proposal includes: 1) a join model generation algorithm devised to adaptively switch between different join schemes so as to minimize the number of processing task required; 2) a load-balancing mechanism which maximizes the processing throughput; and 3) a lightweight algorithm designed for cutting down unnecessary migration cost. Extensive experiments are conducted to compare our proposal against state-of-the-art solutions on both benchmark and real-world workloads. The experimental results verify the effectiveness of our method, especially on reducing the operational cost under pay-as-you-go pricing scheme.}
}


@article{DBLP:journals/tkde/LaiHLYZZ21,
	author = {Yongxuan Lai and
                  Songyao He and
                  Zhijie Lin and
                  Fan Yang and
                  Qifeng Zhou and
                  Xiaofang Zhou},
	title = {An Adaptive Robust Semi-Supervised Clustering Framework Using Weighted
                  Consensus of Random {\textdollar}k{\textdollar}k-Means Ensemble},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1877--1890},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2952596},
	doi = {10.1109/TKDE.2019.2952596},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LaiHLYZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semi-supervised cluster ensemble usually introduces a small amount of supervision in the first stage of cluster ensemble, i.e., ensemble generation, by performing many runs of semi-supervised clustering algorithms. However, it is neither efficient in terms of computational complexity, nor flexible in a dynamic learning environment where limited supervision changes over time. In this article we propose a new framework which generates base partitions in an unsupervised manner and attributes different weights to each cluster of the base partitions. The weighting scheme considers both the internal validation measures of clustering and the degrees of satisfaction of pairwise constraints. A weighted co-association matrix based consensus approach is then applied to achieve a final partition. To handle high-dimensional data, we generate base partitions using k-means with both random sampling and random subspace techniques. The new framework retains a high accuracy, and is efficient since it avoids performing semi-supervised clustering in ensemble generation and the complexity of the weighting scheme is independent of the number of instances in a dynamic environment. It is more adaptive than the traditional approach because it does not require rerunning semi-supervised clustering algorithms when the limited supervision changes. Empirical results on 12 datasets demonstrate that it is also more robust to noisy constraints.}
}


@article{DBLP:journals/tkde/TianZZZF21,
	author = {Yong Tian and
                  Lian Zhou and
                  Yuejie Zhang and
                  Tao Zhang and
                  Weiguo Fan},
	title = {Deep Cross-Modal Face Naming for People News Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1891--1905},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2948875},
	doi = {10.1109/TKDE.2019.2948875},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TianZZZF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How to integrate multimodal information sources for face naming in multimodal news is a hot and yet challenging problem. A novel deep cross-modal face naming scheme is developed in this paper to facilitate more effective people news retrieval for large-scale multimodal news. This scheme integrates deep multimodal analysis, cross-modal correlation learning, and multimodal information mining, in which the efficient naming mechanism aims to cluster the deep features of different modalities into a common space to explore their inter-related correlations, and a special Web mining pattern is designed to optimize the name-face matching for rare non-celebrity. Such a cross-modal face naming model can be treated as a problem of bi-media semantic mapping and modeled as an inter-related correlation distribution over deep representations of multimodal news, in which the most important is to create more effective cross-modal name-face correlation and measure to what degree they are correlated. The experiments on a large number of public data from Yahoo! News have obtained very positive results and demonstrated the effectiveness of the proposed model.}
}


@article{DBLP:journals/tkde/ShenYLZZLX21,
	author = {Xiaoxuan Shen and
                  Baolin Yi and
                  Hai Liu and
                  Wei Zhang and
                  Zhaoli Zhang and
                  Sannyuya Liu and
                  Naixue Xiong},
	title = {Deep Variational Matrix Factorization with Knowledge Embedding for
                  Recommendation System},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1906--1918},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2952849},
	doi = {10.1109/TKDE.2019.2952849},
	timestamp = {Wed, 13 Jul 2022 08:35:39 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShenYLZZLX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automatic recommendation has become an increasingly relevant problem to industries, which allows users to discover new items that match their tastes and enables the system to target items to the right users. In this article, we have proposed a deep learning based fully Bayesian treatment recommendation framework, DVMF, which has high-quality performance and ability to integrate any kinds of side information handily and efficiently. In DVMF, the variational inference technique and the reparameterization tricks are introduced to make DVMF possible to be optimized by the stochastic gradient-based methods, in addition, two novel deep neural networks have been constructed to infer the hyper-parameters of the distributions of latent factors from the knowledge of user and item, which are represented as low-dimensional real-valued vectors retaining primary features. Experimental results on five public databases indicate that the proposed method performs better than the state-of-the-art recommendation algorithms on prediction accuracy in terms of quantitative assessments.}
}


@article{DBLP:journals/tkde/LianXC21,
	author = {Defu Lian and
                  Xing Xie and
                  Enhong Chen},
	title = {Discrete Matrix Factorization and Extension for Fast Item Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1919--1933},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2951386},
	doi = {10.1109/TKDE.2019.2951386},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LianXC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Binary representation of users and items can dramatically improve efficiency of recommendation and reduce size of recommendation models. However, learning optimal binary codes for them is challenging due to binary constraints, even if squared loss is optimized. In this article, we propose a general framework for discrete matrix factorization based on discrete optimization, which can 1) optimize multiple loss functions; 2) handle both explicit and implicit feedback datasets; and 3) take auxiliary information into account without any hyperparameters. To tackle the challenging discrete optimization problem, we propose block coordinate descent based on semidefinite relaxation of binary quadratic programming. We theoretically show that it is equivalent to discrete coordinate descent when only one coordinate is in each block. We extensively evaluate the proposed algorithms on eight real-world datasets. The results of evaluation show that they outperform the state-of-the-art baselines significantly and that auxiliary information of items improves recommendation performance. For better showing the advantages of binary representation, we further propose a two-stage recommender system, consisting of an item-recalling stage and a subsequent fine-ranking stage. Its extensive evaluation shows hashing can dramatically accelerate item recommendation with little degradation of accuracy.}
}


@article{DBLP:journals/tkde/ZhuangCL21,
	author = {Di Zhuang and
                  J. Morris Chang and
                  Mingchen Li},
	title = {DynaMo: Dynamic Community Detection by Incrementally Maximizing Modularity},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1934--1945},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2951419},
	doi = {10.1109/TKDE.2019.2951419},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuangCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community detection is of great importance for online social network analysis. The volume, variety and velocity of data generated by today's online social networks are advancing the way researchers analyze those networks. For instance, real-world networks, such as Facebook, LinkedIn and Twitter, are inherently growing rapidly and expanding aggressively over time. However, most of the studies so far have been focusing on detecting communities on the static networks. It is computationally expensive to directly employ a well-studied static algorithm repeatedly on the network snapshots of the dynamic networks. We propose DynaMo, a novel modularity-based dynamic community detection algorithm, aiming to detect communities of dynamic networks as effective as repeatedly applying static algorithms but in a more efficient way. DynaMo is an adaptive and incremental algorithm, which is designed for incrementally maximizing the modularity gain while updating the community structure of dynamic networks. In the experimental evaluation, a comprehensive comparison has been made among DynaMo, Louvain (static) and 5 other dynamic algorithms. Extensive experiments have been conducted on 6 real-world networks and 10,000 synthetic networks. Our results show that DynaMo outperforms all the other 5 dynamic algorithms in terms of the effectiveness, and is 2 to 5 times (by average) faster than Louvain algorithm.}
}


@article{DBLP:journals/tkde/DongWZXYL21,
	author = {Jianfeng Dong and
                  Xun Wang and
                  Leimin Zhang and
                  Chaoxi Xu and
                  Gang Yang and
                  Xirong Li},
	title = {Feature Re-Learning with Data Augmentation for Video Relevance Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1946--1959},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947442},
	doi = {10.1109/TKDE.2019.2947442},
	timestamp = {Thu, 04 Aug 2022 11:33:17 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DongWZXYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting the relevance between two given videos with respect to their visual content is a key component for content-based video recommendation and retrieval. Thanks to the increasing availability of pre-trained image and video convolutional neural network models, deep visual features are widely used for video content representation. However, as how two videos are relevant is task-dependent, such off-the-shelf features are not always optimal for all tasks. Moreover, due to varied concerns including copyright, privacy and security, one might have access to only pre-computed video features rather than original videos. We propose in this paper feature re-learning for improving video relevance prediction, with no need of revisiting the original video content. In particular, re-learning is realized by projecting a given deep feature into a new space by an affine transformation. We optimize the re-learning process by a novel negative-enhanced triplet ranking loss. In order to generate more training data, we propose a new data augmentation strategy which works directly on frame-level and video-level features. Extensive experiments in the context of the Hulu Content-based Video Relevance Prediction Challenge 2018 justify the effectiveness of the proposed method and its state-of-the-art performance for content-based video relevance prediction.}
}


@article{DBLP:journals/tkde/XiongZKCZ21,
	author = {Yun Xiong and
                  Yizhou Zhang and
                  Xiangnan Kong and
                  Huidi Chen and
                  Yangyong Zhu},
	title = {GraphInception: Convolutional Neural Networks for Collective Classification
                  in Heterogeneous Information Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1960--1972},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947458},
	doi = {10.1109/TKDE.2019.2947458},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XiongZKCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collective classification has attracted considerable attention in the last decade, where the labels within a group of instances are correlated and should be inferred collectively, instead of independently. Conventional approaches on collective classification mainly focus on exploiting simple relational features (such as count and exists aggregators on neighboring nodes). However, many real-world applications involve complex dependencies among the instances, which are obscure/hidden in the networks. To capture these dependencies in collective classification, we need to go beyond simple relational features and extract deep dependencies between the instances. In this paper, we study the problem of deep collective classification in Heterogeneous Information Networks (HINs), which involve different types of autocorrelations, from simple to complex relations, among the instances. Different from conventional autocorrelations, which are given explicitly by the links in the network, complex autocorrelations are obscure/hidden in HINs, and should be inferred from existing links in a hierarchical order. This problem is highly challenging due to the multiple types of dependencies among the nodes and the complexity of the relational features. In this study, we proposed a deep convolutional collective classification method, called GraphInception, to learn the deep relational features in HINs. And we presented two versions of the models with different inference styles. The proposed methods can automatically generate a hierarchy of relational features with different complexities. Extensive experiments on four real-world networks demonstrate that our approach can improve the collective classification performance by considering deep relational features in HINs.}
}


@article{DBLP:journals/tkde/WangGBYYW21,
	author = {Zhigang Wang and
                  Yu Gu and
                  Yubin Bao and
                  Ge Yu and
                  Jeffrey Xu Yu and
                  Zhiqiang Wei},
	title = {HGraph: I/O-Efficient Distributed and Iterative Graph Computing by
                  Hybrid Pushing/Pulling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1973--1987},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2951407},
	doi = {10.1109/TKDE.2019.2951407},
	timestamp = {Mon, 28 Aug 2023 21:37:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangGBYYW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the big data era, distributed computation is becoming a preferred solution for iterative graph analysis. However, graphs are rapidly growing in size and more importantly, there exist a lot of messages across iterations. For better scalability, many distributed systems keep graph data and message data on disk. Now these systems solely employ either pushing or pulling mode to manage data, but neither can always work well during the entire computation. This is mainly because I/O access patterns are dynamic and complex. This article proposes a hybrid solution. It achieves the optimal performance in different scenarios by dynamically and adaptively switching modes between pushing and pulling. Specifically, we first devise a new block-centric pulling technique. It pulls messages much more I/O-efficiently than the existing vertex-centric pulling mode. We then combine pushing and pulling. For general-purpose, we categorize graph algorithms and accordingly present two seamless switching frameworks. We also design performance prediction components specialized to the two frameworks, to decide how and when we can switch modes. Some optimization strategies are also given to further enhance performance, such as priority scheduling and lightweight fault-tolerance. Extensive experiments against state-of-the-art solutions confirm the effectiveness of our proposals.}
}


@article{DBLP:journals/tkde/Razavi-FarFWSC21,
	author = {Roozbeh Razavi{-}Far and
                  Maryam Farajzadeh{-}Zanjani and
                  Boyu Wang and
                  Mehrdad Saif and
                  Shiladitya Chakrabarti},
	title = {Imputation-Based Ensemble Techniques for Class Imbalance Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {1988--2001},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2951556},
	doi = {10.1109/TKDE.2019.2951556},
	timestamp = {Tue, 09 Nov 2021 13:25:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/Razavi-FarFWSC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Correct classification of rare samples is a vital data mining task and of paramount importance in many research domains. This article mainly focuses on the development of the novel class-imbalance learning techniques, which make use of oversampling methods integrated with bagging and boosting ensembles. Two novel oversampling strategies based on the single and the multiple imputation methods are proposed. The proposed techniques aim to create useful synthetic minority class samples, similar to the original minority class samples, by estimation of missing values that are already induced in the minority class samples. The re-balanced datasets are then used to train base-learners of the ensemble algorithms. In addition, the proposed techniques are compared with the commonly used class imbalance learning methods in terms of three performance metrics including AUC, F-measure, and G-mean over several synthetic binary class datasets. The empirical results show that the proposed multiple imputation-based oversampling combined with bagging significantly outperforms other competitors.}
}


@article{DBLP:journals/tkde/WangCALPL21,
	author = {Yanyan Wang and
                  Qun Chen and
                  Murtadha H. M. Ahmed and
                  Zhanhuai Li and
                  Wei Pan and
                  Hailong Liu},
	title = {Joint Inference for Aspect-Level Sentiment Analysis by Deep Neural
                  Networks and Linguistic Hints},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2002--2014},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947587},
	doi = {10.1109/TKDE.2019.2947587},
	timestamp = {Mon, 01 May 2023 13:02:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangCALPL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The state-of-the-art techniques for aspect-level sentiment analysis focused on feature modeling using a variety of deep neural networks (DNN). Unfortunately, their performance may still fall short of expectation in real scenarios due to the semantic complexity of natural languages. Motivated by the observation that many linguistic hints (e.g., sentiment words and shift words) are reliable polarity indicators, we propose a joint framework, SenHint, which can seamlessly integrate the output of deep neural networks and the implications of linguistic hints in a unified model based on Markov logic network (MLN). SenHint leverages the linguistic hints for multiple purposes: (1) to identify the easy instances, whose polarities can be automatically determined by the machine with high accuracy; (2) to capture the influence of sentiment words on aspect polarities; (2) to capture the implicit relations between aspect polarities. We present the required techniques for extracting linguistic hints, encoding their implications as well as the output of DNN into the unified model, and joint inference. Finally, we have empirically evaluated the performance of SenHint on both English and Chinese benchmark datasets. Our extensive experiments have shown that compared to the state-of-the-art DNN techniques, SenHint can effectively improve polarity detection accuracy by considerable margins.}
}


@article{DBLP:journals/tkde/ZhuZTG21,
	author = {Qiannan Zhu and
                  Xiaofei Zhou and
                  Jianlong Tan and
                  Li Guo},
	title = {Knowledge Base Reasoning with Convolutional-Based Recurrent Neural
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2015--2028},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2951103},
	doi = {10.1109/TKDE.2019.2951103},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuZTG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recurrent neural network(RNN) has achieved remarkable performances in complex reasoning on knowledge bases, which usually takes as inputs vector embeddings of relations along a path between an entity pair. However, it is insufficient to extract local correlations of a path due to RNN is better at capturing global sequential information of a path. In this paper, we take full advantages of convolutional neural network that can effectively extract local features, and propose a convolutional-based RNN architecture denoted as C-RNN to perform reasoning. C-RNN first utilizes CNN to extract local high-level correlation features of a path, and then feeds the correlation features into recurrent neural network to model the path representation. Our C-RNN architecture is adaptable to obtain not only local features but also global sequential features of a path. Based on C-RNN architecture, we devise two models, the unidirectional C-RNN and bidirectional C-RNN. We empirically evaluate them on a large-scale FreeBase+ClueWeb prediction task. Experimental results show that C-RNN models achieve state-of-the-art predictive performance.}
}


@article{DBLP:journals/tkde/LiASYWZZ21,
	author = {Qingzhe Li and
                  Amir Alipour{-}Fanid and
                  Martin Slawski and
                  Yanfang Ye and
                  Lingfei Wu and
                  Kai Zeng and
                  Liang Zhao},
	title = {Large-scale Cost-Aware Classification Using Feature Computational
                  Dependency Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2029--2044},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2948607},
	doi = {10.1109/TKDE.2019.2948607},
	timestamp = {Fri, 07 Jun 2024 23:10:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiASYWZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of real-time machine learning applications, the process of feature selection and model optimization requires to integrate with the constraints on computational budgets. A specific computational resource in this regard is the time needed for evaluating predictions on test instances. The joint optimization problem of prediction accuracy and prediction-time efficiency draws more and more attention in the data mining and machine learning communities. The runtime cost is dominated by the feature generation process that contains significantly redundant computations across different features that sharing the same computational component in practice. Eliminating such redundancies would obviously reduce the time costs in the feature generation process. Our previous Cost-aware classification using Feature computational dependencies heterogeneous Hypergraph (CAFH) model has achieved excellent performance on the effectiveness. In the big data era, the high dimensionality caused by the heterogeneous data sources leads to the difficulty in fitting the entire hypergraph into the main memory and the high computational cost during the optimization process. Simply partitioning the features into batches cannot give the optimal solution since it will lose some feature dependencies across the batches. To improve the high memory and computational costs in the CAFH model, we propose an equivalent Accelerated CAFH (ACAFH) model based on the lossless heterogeneous hypergraph decomposition. An efficient and effective nonconvex optimization algorithm based on the alternating direction method of multipliers (ADMM) is developed to optimize the ACAFH model. The time and space complexities of the optimization algorithm for the ACAFH model are three and one polynomial degrees less than our previous algorithm for the CAFH model, respectively. Extensive experiments demonstrate the proposed ACAFH model achieves competitive performance on the effectiveness and much better performance on the efficiency.}
}


@article{DBLP:journals/tkde/NikolentzosV21,
	author = {Giannis Nikolentzos and
                  Michalis Vazirgiannis},
	title = {Learning Structural Node Representations Using Graph Kernels},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2045--2056},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947478},
	doi = {10.1109/TKDE.2019.2947478},
	timestamp = {Mon, 28 Aug 2023 21:37:41 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NikolentzosV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many applications require identifying nodes that perform similar functions in a graph. For instance, identifying structurally equivalent nodes can provide insight into the structure of complex networks. Learning latent representations that capture such structural role information about nodes has recently gained a lot of attention. Existing techniques for learning such representations typically rely on manually engineered features or are very expensive in terms of time and memory requirements. In this paper, we propose SEGK, a powerful framework for computing structural node representations. SEGK learns node representations by generating (or approximating) and decomposing a kernel matrix that incorporates structural similarity between nodes. To compute the similarity between two nodes, the proposed framework builds on well-established concepts from graph mining. Specifically, it compares the neighborhood subgraphs of increasing size of two nodes using graph kernels. SEGK is very flexible, and besides unlabeled graphs, it can also handle node-labeled and node-attributed graphs. We evaluate the proposed framework on several synthetic and real-world datasets, and compare its performance to state-of-the-art techniques for learning structural node embeddings. In almost all cases, the instances of the proposed framework outperform the competing methods, while their time complexity remains very attractive.}
}


@article{DBLP:journals/tkde/ZhangZFLG21,
	author = {Min{-}Ling Zhang and
                  Qian{-}Wen Zhang and
                  Jun{-}Peng Fang and
                  Yu{-}Kun Li and
                  Xin Geng},
	title = {Leveraging Implicit Relative Labeling-Importance Information for Effective
                  Multi-Label Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2057--2070},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2951561},
	doi = {10.1109/TKDE.2019.2951561},
	timestamp = {Fri, 03 Nov 2023 18:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZFLG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-label learning deals with training examples each represented by a single instance while associated with multiple class labels, and the task is to train a predictive model which can assign a set of proper labels for the unseen instance. Existing approaches employ the common assumption of equal labeling-importance, i.e., all associated labels are regarded to be relevant to the training instance while their relative importance in characterizing its semantics are not differentiated. Nonetheless, this common assumption does not reflect the fact that the importance degree of each relevant label is generally different, though the importance information is not directly accessible from the training examples. In this article, we show that it is beneficial to leverage the implicit relative labeling-importance (RLI) information to help induce multi-label predictive model with strong generalization performance. Specifically, RLI degrees are formalized as multinomial distribution over the label space, which can be estimated by either global label propagation procedure or local k-nearest neighbor reconstruction. Correspondingly, the multi-label predictive model is induced by fitting modeling outputs with estimated RLI degrees along with multi-label empirical loss regularization. Extensive experiments clearly validate that leveraging implicit RLI information serves as a favorable strategy to achieve effective multi-label learning.}
}


@article{DBLP:journals/tkde/LiL21a,
	author = {Yufeng Li and
                  De{-}Ming Liang},
	title = {Lightweight Label Propagation for Large-Scale Network Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2071--2082},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2949297},
	doi = {10.1109/TKDE.2019.2949297},
	timestamp = {Mon, 13 May 2024 08:07:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiL21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Label propagation spreads the soft labels from few labeled data to a large amount of unlabeled data according to the intrinsic graph structure. Nonetheless, most label propagation solutions work under relatively small-scale data and fail to cope with many real applications, such as social network analysis, where graphs usually have millions of nodes. In this paper, we propose a novel algorithm named SLP to deal with large-scale data. A lightweight iterative process derived from the well-known stochastic gradient descent strategy is used to accelerate the solving process. We also give a theoretical analysis on the necessity of the warm-start technique for label propagation. Experiments show that our algorithm is several times faster than state-of-the-art methods while achieving highly competitive performance.}
}


@article{DBLP:journals/tkde/ZhangW21,
	author = {Jing Zhang and
                  Xindong Wu},
	title = {Multi-Label Truth Inference for Crowdsourcing Using Mixture Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2083--2095},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2951668},
	doi = {10.1109/TKDE.2019.2951668},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When acquiring labels from crowdsourcing platforms, a task may be designed to include multiple labels and the values of each label may belong to a set of various distinct options, which is the so-called multi-class multi-label annotation. To improve the quality of labels, requesters usually let one task be independently completed by a group of heterogeneous crowdsourced workers. Then, the true values of the multiple labels of each task are inferred from these repeated noisy labels. In this paper, we propose two novel probabilistic models MCMLI and MCMLD to address the multi-class multi-label inference problem in crowdsourcing. MCMLI assumes that the labels of each task are mutually independent and MCMLD utilizes a mixture of multiple independently multinoulli distributions to capture the correlation among the labels. Both models can jointly infer multiple true labels of each instance as well as estimate the reliability of crowdsourced workers modeled by a set of confusion matrices with an expectation-maximization algorithm. Experiments with three typical crowdsourcing scenarios and a real-world dataset show that our proposed models significantly outperform existing competitive alternatives. When the labels are strongly correlated, MCMLD substantially outperforms MCMLI. Furthermore, our models can be easily simplified to the one-coin models, which show more advantageous when errors are uniformly distributed, or labels are sparse.}
}


@article{DBLP:journals/tkde/ZhouSHZZ21,
	author = {Xian Zhou and
                  Yanyan Shen and
                  Linpeng Huang and
                  Tianzi Zang and
                  Yanmin Zhu},
	title = {Multi-Level Attention Networks for Multi-Step Citywide Passenger Demands
                  Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2096--2108},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2948005},
	doi = {10.1109/TKDE.2019.2948005},
	timestamp = {Tue, 08 Feb 2022 09:06:53 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouSHZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For the emerging mobility-on-demand services, it is of great significance to predict passenger demands based on historical mobility trips towards better vehicle distribution. Prior works have focused on predicting next-step passenger demands at selected locations or hotspots. However, we argue that multi-step citywide passenger demands encapsulate both time-varying demand trends and global statuses, and hence are more beneficial to avoiding demand-service mismatching and developing effective vehicle distribution/scheduling strategies. Furthermore, we find that adaptations of single-step methods are unable to achieve robust prediction with high accuracy for further steps. In this paper, we propose an end-to-end deep neural network model to the prediction task. We employ an encoder-decoder framework based on convolutional and ConvLSTM units to identify complex features that capture spatiotemporal influence and pickup-dropoff interactions on citywide passenger demands. We introduce a multi-level attention model (global attention and temporal attention) to emphasize the effects of latent citywide mobility regularities and capture relevant temporal dependencies. We evaluate our proposed method using real-world mobility trips (taxis and bikes) and the experimental results show that our method achieves higher prediction accuracy than the state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/WeiYMWSZ21,
	author = {Xiu{-}Shen Wei and
                  Han{-}Jia Ye and
                  Xin Mu and
                  Jianxin Wu and
                  Chunhua Shen and
                  Zhi{-}Hua Zhou},
	title = {Multi-Instance Learning With Emerging Novel Class},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2109--2120},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2952588},
	doi = {10.1109/TKDE.2019.2952588},
	timestamp = {Mon, 19 Sep 2022 15:19:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WeiYMWSZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Diverse applications involving complicated data objects such as proteins and images are solved by applying multi-instance learning (MIL) algorithms. However, few MIL algorithms can deal with problems in an open and dynamic environment, where new categories of samples emerge. In this type of emerging novel class setting, algorithms should be able to not only classify the samples from the observed classes accurately, but also recognize the samples from the novel class. In this paper, we focus on the Multi-Instance learning with Emerging Novel class (MIEN) problem, and formulate MIEN from a metric learning perspective. We extract key instances to form the “super-bag” for each observed class, and non-key instances from all the observed classes to form a “meta super-bag”. Based on these super-bags, we propose the MIEN-metric method to learn discriminative metrics for classifying MIL bags from the observed classes and recognizing bags from the novel class. Experimental results of diverse domains, e.g., biological function annotation, text categorization, and object-centric/scene-centric image classification, show MIEN-metric outperforms other baseline methods significantly when the novel class emerges. Meanwhile, MIEN-metric is comparable with state-of-the-art MIL algorithms for binary classification in the traditional MIL setting.}
}


@article{DBLP:journals/tkde/HsuSY21,
	author = {Bay{-}Yuan Hsu and
                  Chih{-}Ya Shen and
                  Xifeng Yan},
	title = {Network Intervention for Mental Disorders with Minimum Small Dense
                  Subgroups},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2121--2136},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2949294},
	doi = {10.1109/TKDE.2019.2949294},
	timestamp = {Thu, 16 Sep 2021 17:58:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HsuSY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {According to the literature in psychology, the existence of small dense subgroups is closely related to many mental illnesses, such as depression, bullying, and psychotic disorders. Here, small dense subgroups refer to the small groups in the social network in which members are socially dense but have no or few links to other individuals outside the group. Therefore, in this article, we make the first attempt to address the issue of small dense subgroups with the concept of network intervention from Psychology. We first introduce the new notion of\nΔ\n-Subgroups (\nΔ\n-SGs) to quantify the small dense subgroups. Then, following the concept of network intervention, we formulate a new research problem, Small Subgroup Maximum Reduction Problem (SSMP) , to reduce the number of small dense subgroups (i.e.,\nΔ\n-SGs) in the social network. We prove that SSMP is NP-Hard and propose a linear-time algorithm, namely 3-SMMTG , to find the optimal solution for a special case of SSMP with\nΔ=3\n. We then devise a\n1\n2\n(1−\n1\ne\n)\n-approximation algorithm, namely ESGR , for the general SSMP and enhance its efficiency with effective pruning methods. We conduct a 8-week evaluation study with 812 participants to validate the proposed SSMP and ESGR. The results show that the participants with the network intervention recommended by ESGR have significant improvements on Internet addiction and depression, as compared to those individuals without any intervention. We also perform experiments on 7 real datasets, and the experimental results manifest that the proposed algorithms outperform the other baselines in both efficiency and solution quality.}
}


@article{DBLP:journals/tkde/TalLHYA21,
	author = {Omer Tal and
                  Yang Liu and
                  Jimmy X. Huang and
                  Xiaohui Yu and
                  Bushra Aljbawi},
	title = {Neural Attention Frameworks for Explainable Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2137--2150},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2953157},
	doi = {10.1109/TKDE.2019.2953157},
	timestamp = {Wed, 03 Aug 2022 15:48:38 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TalLHYA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Neural attention, an emerging technique used to identify important inputs within neural networks, have become increasingly popular in the area of recommender systems. Not only allowing to better identify what defines users and items, attention-based recommender systems are further able to provide accompanying explanations. However, these representations usually capture only part of users' preferences and items' attributes, resulting in limited reasoning and accuracy. We therefore propose Dual Attention Recommender with Items and Attributes (DARIA), a novel approach able to combine two dependable neural attention mechanisms to better justify its suggestions. Utilizing the personalized history of users, DARIA identifies the most relevant past activities while considering the real-world features that contributed to the similarity. In addition, we adopt the novel approach of self-attention and introduce Self-Attention Recommender based on Attributes and History (SARAH). As a variation to DARIA, SARAH utilizes two self-attention components to describe users by their most characteristic past activities and items by their best depicting attributes. Various experiments establish the significant improvement of SARAH and DARIA over seven key baselines in diverse recommendation settings. By comparing our two proposed frameworks, we demonstrate the potential benefit of applying self-attention in different scenarios.}
}


@article{DBLP:journals/tkde/PengFP21,
	author = {Botao Peng and
                  Panagiota Fatourou and
                  Themis Palpanas},
	title = {ParIS+: Data Series Indexing on Multi-Core Architectures},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2151--2164},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2975180},
	doi = {10.1109/TKDE.2020.2975180},
	timestamp = {Thu, 16 Sep 2021 17:58:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PengFP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data series similarity search is a core operation for several data series analysis applications across many different domains. Nevertheless, even state-of-the-art techniques cannot provide the time performance required for large data series collections. We propose ParIS and ParIS+, the first disk-based data series indices carefully designed to inherently take advantage of multi-core architectures, in order to accelerate similarity search processing times. Our experiments demonstrate that ParIS+ completely removes the CPU latency during index construction for disk-resident data, and for exact query answering is up to 1 order of magnitude faster than the current state of the art index scan method, and up to 3 orders of magnitude faster than the optimized serial scan method. ParIS+ (which is an evolution of the ADS+ index) owes its efficiency to the effective use of multi-core and multi-socket architectures, in order to distribute and execute in parallel both index construction and query answering, and to the exploitation of the Single Instruction Multiple Data (SIMD) capabilities of modern CPUs, in order to further parallelize the execution of instructions inside each core.}
}


@article{DBLP:journals/tkde/GaoZCTCD21,
	author = {Xiaofeng Gao and
                  Zuowu Zheng and
                  Quanquan Chu and
                  Shaojie Tang and
                  Guihai Chen and
                  Qianni Deng},
	title = {Popularity Prediction for Single Tweet Based on Heterogeneous Bass
                  Model},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2165--2178},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2952856},
	doi = {10.1109/TKDE.2019.2952856},
	timestamp = {Tue, 02 Jan 2024 17:01:00 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GaoZCTCD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting the popularity of a single tweet is useful for both users and enterprises. However, adopting existing topic or event prediction models cannot obtain satisfactory results. The reason is that one topic or event that consists of multiple tweets, has more features and characteristics than a single tweet. In this article, we propose two variations of Heterogeneous Bass models (HBass), originally developed in the field of marketing science, namely Spatial-Temporal Heterogeneous Bass Model (ST-HBass) and Feature-Driven Heterogeneous Bass Model (FD-HBass), to predict the popularity of a single tweet at the early stage and the stable stage. We further design an Interaction Enhancement to improve the performance, which considers the competition and cooperation from different tweets with the common topic. In addition, it is often difficult to depict popularity quantitatively. We design an experiment to get the weight of favorite, retweet and reply, and apply the linear regression to calculate the popularity. Furthermore, we design a clustering method to bound the popular threshold. Once the weight and popular threshold are determined, the status whether a tweet will be popular or not can be justified. Our model is validated by conducting experiments on real-world Twitter data, and the results show the efficiency and accuracy of our model, with less absolute percent error and the best Precision and F-score. In all, we introduce Bass model into social network single-tweet prediction to show it can achieve excellent performance.}
}


@article{DBLP:journals/tkde/ZhaoXSLCG21,
	author = {Huan Zhao and
                  Xiaogang Xu and
                  Yangqiu Song and
                  Dik Lun Lee and
                  Zhao Chen and
                  Han Gao},
	title = {Ranking Users in Social Networks with Motif-Based PageRank},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2179--2192},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2953264},
	doi = {10.1109/TKDE.2019.2953264},
	timestamp = {Mon, 09 Sep 2024 19:07:22 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoXSLCG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {PageRank has been widely used to measure the authority or the influence of a user in social networks. However, conventional PageRank only makes use of edge-based relations, which represent first-order relations between two connected nodes. It ignores higher-order relations that may exist between nodes. In this article, we propose a novel framework, motif-based PageRank (MPR), to incorporate higher-order relations into the conventional PageRank computation. Motifs are subgraphs consisting of a small number of nodes. We use motifs to capture higher-order relations between nodes in a network and introduce two methods, one linear and one non-linear, to combine first-order and higher-order relations in PageRank computation. We conduct extensive experiments on three real-world networks, namely, DBLP, Epinions, and Ciao. We study different types of motifs, including 3-node simple and anchor motifs, 4-node and 5-node motifs. Besides using single motif, we also run MPR with ensemble of multiple motifs. We also design a learning task to evaluate the abilities of authority prediction with motif-based features. All experimental results demonstrate that MPR can significantly improve the performance of user ranking in social networks compared to the baseline methods.}
}


@article{DBLP:journals/tkde/ZhangCZ21,
	author = {Qi Zhang and
                  Tianguang Chu and
                  Cishen Zhang},
	title = {Semi-Supervised Graph Based Embedding With Non-Convex Sparse Coding
                  Techniques},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2193--2207},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2953668},
	doi = {10.1109/TKDE.2019.2953668},
	timestamp = {Mon, 28 Aug 2023 21:37:39 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of semi-supervised graph-based learning upon multimodal and mixmodal data. Since in semi-supervised settings, the labeled information is very limited, we first propose a non-convex sparse-coding based label propagation (αα-SLP) method to estimate the soft labels, and thereby to enrich the supervised information. By considering the structural properties of multimodal and mixmodal data, we present a semi-supervised graph-based embedding (SGE) approach that incorporates the soft label information with the hierarchical local geometric information of within-class, between-class, and overall-class data. Based on this, subspaces characterizing the multimodal and mixmodal data structure can be derived by maximizing the weighted between-class separability and minimizing the locality-preserved within-class as well as overall-class distances of the training samples. We further extend SGE into semi-supervised sparse subspace learning scenarios and present an αα-structural-regularization-induced SGE (αα-SSGE) model, which can give better results in extracting discriminative groups of features by utilizing the non-convex structural regularization techniques. Experiments for multimodal and mixmodal digit as well as face recognition verify the validity and effectiveness of the proposed models.}
}


@article{DBLP:journals/tkde/YinHCYL21,
	author = {Xiaoyan Yin and
                  Xiao Hu and
                  Yanjiao Chen and
                  Xu Yuan and
                  Baochun Li},
	title = {Signed-PageRank: An Efficient Influence Maximization Framework for
                  Signed Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2208--2222},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2947421},
	doi = {10.1109/TKDE.2019.2947421},
	timestamp = {Mon, 25 Sep 2023 12:16:55 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YinHCYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence maximization in social networks is of great importance for marketing new products. Signed social networks with both positive (friends) and negative (foes) relationships pose new challenges and opportunities, since the influence of negative relationships can be leveraged to promote information propagation. In this paper, we study the problem of influence maximization for advertisement recommendation in signed social networks. We propose a new framework to characterize the information propagation process in signed social networks, which models the dynamics of individuals' beliefs and attitudes towards the advertisement based on recommendations from both positive and negative neighbours. To achieve influence maximization in signed social networks, we design a novel Signed-PageRank (SPR) algorithm, which selects the initial seed nodes by jointly considering their positive and negative connections with the rest of the network. Our extensive experimental results confirm that our proposed SPR algorithm can effectively and efficiently influence a broader range of individuals in the signed social networks than benchmark algorithms on both synthetic and real datasets.}
}


@article{DBLP:journals/tkde/HanLLCWTY21,
	author = {Rui Han and
                  Chi Harold Liu and
                  Shilin Li and
                  Lydia Y. Chen and
                  Guoren Wang and
                  Jian Tang and
                  Jieping Ye},
	title = {SlimML: Removing Non-Critical Input Data in Large-Scale Iterative
                  Machine Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2223--2236},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2951388},
	doi = {10.1109/TKDE.2019.2951388},
	timestamp = {Thu, 16 Sep 2021 17:58:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HanLLCWTY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The core of many large-scale machine learning (ML) applications, such as neural networks (NN), support vector machine (SVM), and convolutional neural network (CNN), is the training algorithm that iteratively updates model parameters by processing massive datasets. From a plethora of studies aiming at accelerating ML, being data parallelization and parameter server, the prevalent assumption is that all data points are equivalently relevant to model parameter updating. In this article, we challenge this assumption by proposing a criterion to measure a data point's effect on model parameter updating, and experimentally demonstrate that the majority of data points are non-critical in the training process. We develop a slim learning framework, termed SlimML, which trains the ML models only on the critical data and thus significantly improves training performance. To such an end, SlimML efficiently leverages a small number of aggregated data points per iteration to approximate the criticalness of original input data instances. The proposed approach can be used by changing a few lines of code in a standard stochastic gradient descent (SGD) procedure, and we demonstrate experimentally, on NN regression, SVM classification, and CNN training, that for large datasets, it accelerates model training process by an average of 3.61 times while only incurring accuracy losses of 0.37 percent.}
}


@article{DBLP:journals/tkde/ZhaoSYZ21,
	author = {Kangfei Zhao and
                  Jiao Su and
                  Jeffrey Xu Yu and
                  Hao Zhang},
	title = {{SQL-G:} Efficient Graph Analytics by {SQL}},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2237--2251},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2950620},
	doi = {10.1109/TKDE.2019.2950620},
	timestamp = {Mon, 28 Aug 2023 21:37:41 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoSYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Querying graphs and conducting graph analytics become important in data processing since many real applications are dealing with massive graphs, such as online social networks, Semantic Web, knowledge graphs, etc. Over the years, many distributed graph processing systems have been developed to support graph analytics using various programming models, and many graph querying languages have been proposed. A natural question that arises is how to integrate graph data and traditional non-graph data in a distributed system for users to conduct analytics. There are two issues. One issue is related to expressiveness on how to specify graph analytics as well as data analytics by a querying language. The other issue is related to efficiency on how to process analytics in a distributed system. For the first issue, SQL is a best candidate, since SQL is a well-accepted language for data processing. We concentrate on SQL for graph analytics. Our early work shows that graph analytics can be supported by SQL in a way from “semiring + while” to “relational algebra + while” via the enhanced recursive SQL queries. In this article, we focus on the second issue on how to process such enhanced recursive SQL queries based on the GAS (Gather-Apply-Scatter) model under which efficient graph processing systems can be developed. To demonstrate the efficiency, we implemented a system by tightly coupling Spark SQL and GraphX on Spark which is one of the most popular in-memory data-flow processing platforms. First, we enhance Spark SQL by adding the capability of supporting the enhanced recursive SQL queries for graph analytics. In this regard, graph analytics can be processed using a distributed SQL engine alone. Second, we further propose new transformation rules to optimize/translate the operations for recursive SQL queries to the operations by GraphX. In this regard, graph analytics by SQL can be processed in a similar way as done by a distributed graph processing system using the APIs provided by the system. We conduct extensive performance studies to test graph analytics using large real graphs. We show that our approach can achieve similar or even higher efficiency, in comparison to the built-in graph algorithms in the existing graph processing systems.}
}


@article{DBLP:journals/tkde/HanXNL21,
	author = {Junwei Han and
                  Kai Xiong and
                  Feiping Nie and
                  Xuelong Li},
	title = {Structured Graph Reconstruction for Scalable Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2252--2265},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2948850},
	doi = {10.1109/TKDE.2019.2948850},
	timestamp = {Thu, 08 Aug 2024 07:51:02 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HanXNL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectral clustering is a quite simple but effective method for solving graph clustering problem. It projects the original data points into a lower dimensional space with spectral embedding, and then relies on an algorithm to obtain the cluster labels. Since it involves eigendecomposition of the graph Laplacian matrix for embedding, spectral clustering has high time complexity and is not able to process large scale data. The performance of spectral clustering is also limited by a post-processing algorithm such as kmeans. To tackle the two issues, we propose a method called Orthogonal and Nonnegative Graph Reconstruction (ONGR) for large scale clustering. The two constraints serve as a structure constraint with which the graph reconstructed by the indicator matrix is structured. The proposed method has linear time complexity with respect to the data size that it mainly needs to implicitly construct a graph and iteratively perform economical singular value decomposition for a small size matrix. Moreover, the interpretability of the indicator matrix is offered due to the nonnegative constraint, and thus our method can provide the cluster labels with no post-processing. The experiments on benchmark datasets show the effectiveness of the proposed scalable clustering method.}
}


@article{DBLP:journals/tkde/HanRGSCMD21,
	author = {Lei Han and
                  Kevin Roitero and
                  Ujwal Gadiraju and
                  Cristina Sarasua and
                  Alessandro Checco and
                  Eddy Maddalena and
                  Gianluca Demartini},
	title = {The Impact of Task Abandonment in Crowdsourcing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2266--2279},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2948168},
	doi = {10.1109/TKDE.2019.2948168},
	timestamp = {Mon, 28 Aug 2023 21:37:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HanRGSCMD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowdsourcing has become a standard methodology to collect manually annotated data such as relevance judgments at scale. On crowdsourcing platforms like Amazon MTurk or FigureEight, crowd workers select tasks to work on based on different dimensions such as task reward and requester reputation. Requesters then receive the judgments of workers who self-selected into the tasks and completed them successfully. Several crowd workers, however, preview tasks, begin working on them, reaching varying stages of task completion without finally submitting their work. Such behavior results in unrewarded effort which remains invisible to requesters. In this paper, we conduct an investigation of the phenomenon of task abandonment, the act of workers previewing or beginning a task and deciding not to complete it. We follow a three-fold methodology which includes 1) investigating the prevalence and causes of task abandonment by means of a survey over different crowdsourcing platforms, 2) data-driven analysis of logs collected during a large-scale relevance judgment experiment, and 3) controlled experiments measuring the effect of different dimensions on abandonment. Our results show that task abandonment is a widely spread phenomenon. Apart from accounting for a considerable amount of wasted human effort, this bears important implications on the hourly wages of workers as they are not rewarded for tasks that they do not complete. We also show how task abandonment may have strong implications on the use of collected data (for example, on the evaluation of Information Retrieval systems).}
}


@article{DBLP:journals/tkde/JungKS21,
	author = {Woohwan Jung and
                  Suyong Kwon and
                  Kyuseok Shim},
	title = {{TIDY:} Publishing a Time Interval Dataset With Differential Privacy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2280--2294},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2952351},
	doi = {10.1109/TKDE.2019.2952351},
	timestamp = {Thu, 29 Apr 2021 15:11:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JungKS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log data from mobile devices generally contain a series of events with temporal information including time intervals which consist of the start and finish times. However, the problem of releasing differentially private time interval datasets has not been tackled yet. A time interval dataset can be represented by a two dimensional (2D) histogram. Most of the methods to publish 2D histograms partition the data into rectangular spaces to reduce the aggregated noise error for range queries. However, the existing algorithms to publish 2D histograms suffer from the structural error when applied to time interval datasets. To reduce the aggregated noise errors and suppress the increase in the structural error, we propose the TIDY (publishing Time Intervals via Differential privacY) algorithm. We use the frequency vectors as a compact representation of the time interval dataset. After applying the Laplace mechanism to the frequency vectors, we improve the utility of the frequency vectors based on a maximum likelihood estimation. We also develop a new partitioning method adapted for the frequency vectors to balance the trade-off between the noise and structural errors. Our empirical study on real-life and synthetic datasets confirms that TIDY outperforms the existing algorithms for 2D histograms.}
}


@article{DBLP:journals/tkde/TongZDWC21,
	author = {Yongxin Tong and
                  Yuxiang Zeng and
                  Bolin Ding and
                  Libin Wang and
                  Lei Chen},
	title = {Two-Sided Online Micro-Task Assignment in Spatial Crowdsourcing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {5},
	pages = {2295--2309},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2948863},
	doi = {10.1109/TKDE.2019.2948863},
	timestamp = {Sun, 04 Aug 2024 19:47:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TongZDWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of smartphones, spatial crowdsourcing platforms are getting popular. A foundational research of spatial crowdsourcing is to allocate micro-tasks to suitable crowd workers. Many existing studies focus on the offline scenario, where all the spatiotemporal information of micro-tasks and crowd workers is given. In this paper, we focus on the online scenario and identify a more practical micro-task allocation problem, called the Global Online Micro-task Allocation in spatial crowdsourcing (GOMA) problem. We first extend the state-of-the-art algorithm for the online maximum weighted bipartite matching problem to the GOMA problem as the baseline algorithm. Although the baseline algorithm provides a theoretical guarantee for the worst case, its average performance in practice is not good enough since the worst case happens with a very low probability in the real world. Thus, we consider the average performance of online algorithms, a.k.a. random order model. We propose a two-phase-based framework, based on which we present the TGOA algorithm with a\n1\n4\n-competitive ratio under the random order model. To improve its efficiency, we further design the TGOA-Greedy and TGOA-OP algorithm following this framework, which runs faster than the TGOA algorithm with a competitive ratio of\n1\n8\nand\n1\n4\n, respectively. We also revisit the average performance of Greedy, which has long been considered as the worst due to its unbounded competitive ratio in the worst case. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on synthetic and real datasets.}
}


@article{DBLP:journals/tkde/ChenY21,
	author = {Jianguo Chen and
                  Philip S. Yu},
	title = {A Domain Adaptive Density Clustering Algorithm for Data With Varying
                  Density Distribution},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2310--2321},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2954133},
	doi = {10.1109/TKDE.2019.2954133},
	timestamp = {Thu, 07 Mar 2024 15:07:53 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one type of efficient unsupervised learning methods, clustering algorithms have been widely used in data mining and knowledge discovery with noticeable advantages. However, clustering algorithms based on density peak have limited clustering effect on data with varying density distribution (VDD), equilibrium distribution (ED), and multiple domain-density maximums (MDDM), leading to the problems of sparse cluster loss and cluster fragmentation. To address these problems, we propose a Domain-Adaptive Density Clustering (DADC) algorithm, which consists of three steps: domain-adaptive density measurement, cluster center self-identification, and cluster self-ensemble. For data with VDD features, clusters in sparse regions are often neglected by using uniform density peak thresholds, which results in the loss of sparse clusters. We define a domain-adaptive density measurement method based on K K-Nearest Neighbors (KNN) to adaptively detect the density peaks of different density regions. We treat each data point and its KNN neighborhood as a subgroup to better reflect its density distribution in a domain view. In addition, for data with ED or MDDM features, a large number of density peaks with similar values can be identified, which results in cluster fragmentation. We propose a cluster center self-identification and cluster self-ensemble method to automatically extract the initial cluster centers and merge the fragmented clusters. Experimental results demonstrate that compared with other comparative algorithms, the proposed DADC algorithm can obtain more reasonable clustering results on data with VDD, ED and MDDM features. Benefitting from a few parameter requirement and non-iterative nature, DADC achieves low computational complexity and is suitable for large-scale data clustering.}
}


@article{DBLP:journals/tkde/JiangCXB21,
	author = {Jiaxin Jiang and
                  Byron Choi and
                  Jianliang Xu and
                  Sourav S. Bhowmick},
	title = {A Generic Ontology Framework for Indexing Keyword Search on Massive
                  Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2322--2336},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2956535},
	doi = {10.1109/TKDE.2019.2956535},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiangCXB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the unstructuredness and the lack of schema information of knowledge graphs, social networks and RDF graphs, keyword search has been proposed for querying such graphs/networks. Recently, various keyword search semantics have been designed. In this paper, we propose a generic ontology-based indexing framework for keyword search, called Bisimulation of Generalized Graph Index ( \\mathsf {BiG\\hbox{-}index}\n), to enhance the search performance. The novelties of \\mathsf {BiG\\hbox{-}index}\nreside in using an ontology graph G_{Ont}\nto summarize and index a data graph G\niteratively, to form a hierarchical index structure \\mathbb {G}\n. \\mathsf {BiG\\hbox{-}index}\nis generic since it only requires keyword search algorithms to generate query answers from summary graphs having two simple properties. Regarding query evaluation, we transform a keyword search q\ninto \\mathbb {Q}\naccording to G_{Ont}\nin runtime. The transformed query is searched on the summary graphs in \\mathbb {G}\n. The efficiency is due to the small sizes of the summary graphs and the early pruning of semantically irrelevant subgraphs. To illustrate \\mathsf {BiG\\hbox{-}index}\n's applicability, we show popular indexing techniques for keyword search (e.g., \\mathsf {Blinks}\nand \\mathsf {r\\hbox{-}clique}\n) can be easily implemented on top of \\mathsf {BiG\\hbox{-}index}\n. Our extensive experiments show that \\mathsf {BiG\\hbox{-}index}\nreduced the runtimes of popular keyword search work \\mathsf {Blinks}\nby 50.5 percent and \\mathsf {r\\hbox{-}clique}\nby 29.5 percent.}
}


@article{DBLP:journals/tkde/Cai21,
	author = {Deng Cai},
	title = {A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2337--2348},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2953897},
	doi = {10.1109/TKDE.2019.2953897},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/Cai21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many areas of machine learning and data mining. During the past decade, numerous hashing algorithms are proposed to solve this problem. Every proposed algorithm claims to outperform Locality Sensitive Hashing (LSH), which is the most popular hashing method. However, the evaluation of these hashing article was not thorough enough, and the claim should be re-examined. If implemented correctly, almost all the hashing methods will have their performance improved as the code length increases. However, many existing hashing article only report the performance with the code length shorter than 128. In this article, we carefully revisit the problem of search-with-a-hash-index and analyze the pros and cons of two popular hash index search procedures. Then we proposed a simple but effective novel hash index search approach and made a thorough comparison of eleven popular hashing algorithms. Surprisingly, the random-projection-based Locality Sensitive Hashing ranked the first, which is in contradiction to the claims in all the other 10 hashing article. Despite the extreme simplicity of random-projection-based LSH, our results show that the capability of this algorithm has been far underestimated. For the sake of reproducibility, all the codes used in the article are released on GitHub, which can be used as a testing platform for a fair comparison between various hashing algorithms.}
}


@article{DBLP:journals/tkde/YangLLT21,
	author = {Xinghao Yang and
                  Weifeng Liu and
                  Wei Liu and
                  Dacheng Tao},
	title = {A Survey on Canonical Correlation Analysis},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2349--2368},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2958342},
	doi = {10.1109/TKDE.2019.2958342},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangLLT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the advances in data collection and statistical analysis promotes canonical correlation analysis (CCA) available for more advanced research. CCA is the main technique for two-set data dimensionality reduction such that the correlation between the pairwise variables in the common subspace is mutually maximized. Over 80-years of developments, a number of CCA models have been proposed according to different machine learning mechanisms. However, the field lacks an insightful review for the state-of-art developments. This survey targets to provide a well-organized overview for CCA and its extensions. Specifically, we first review the CCA theory from the perspective of both model formation and model optimization. The association between two popular solution methods, i.e., eigen value decomposition (EVD) and singular value decomposition (SVD), are discussed. Following that, we present a taxonomy of current progresses and classify them into seven groups: 1) multi-view CCA, 2) probabilistic CCA, 3) deep CCA, 4) kernel CCA, 5) discriminative CCA, 6) sparse CCA and 7) locality preserving CCA. For each group, we demonstrate two or three representative mathematical models, identifying their strengths and limitations. We summarize the representative applications and numerical results of these seven groups in real-world practices, collecting the data sets and open-sources for implementation. In the end, we provide several promising future research directions that can improve the current state of the art.}
}


@article{DBLP:journals/tkde/LiuLWF21,
	author = {Hongfu Liu and
                  Jun Li and
                  Yue Wu and
                  Yun Fu},
	title = {Clustering With Outlier Removal},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2369--2379},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2954317},
	doi = {10.1109/TKDE.2019.2954317},
	timestamp = {Fri, 01 Dec 2023 11:57:30 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLWF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cluster analysis and outlier detection are two continuously rising topics in data mining area, which in fact connect to each other deeply. Cluster structure is vulnerable to outliers; inversely, outliers are the points belonging to none of any clusters. Unfortunately, most existing studies do not notice the coupled relationship between these two tasks and handle them separately. In this article, we consider the joint cluster analysis and outlier detection problem, and propose the Clustering with Outlier Removal (COR) algorithm. Specifically, the original space is transformed into a binary space via generating basic partitions. We employ Holoentropy to measure the compactness of each cluster without involving several outlier candidates. To provide a neat and efficient solution, an auxiliary binary matrix is introduced so that COR completely and efficiently solves the challenging problem via a unified K-means— with theoretical supports. Extensive experimental results on numerous data sets in various domains demonstrate the effectiveness and efficiency of COR significantly over state-of-the-art methods in terms of cluster validity and outlier detection. Some key factors including the basic partition number and generation strategy in COR with an application on abnormal flight trajectory detection are further analyzed for practical use.}
}


@article{DBLP:journals/tkde/GevaY21,
	author = {Tomer Geva and
                  Inbal Yahav},
	title = {Data-Driven Link Screening for Increasing Network Predictability},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2380--2391},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2955650},
	doi = {10.1109/TKDE.2019.2955650},
	timestamp = {Tue, 01 Jun 2021 08:34:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GevaY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Prediction methods applied to digital network data offer powerful capabilities that have radically affected a host of industries and services. Specifically, numerous works have shown that the use of network information to predict focal node properties produces significantly more accurate results compared with exclusive reliance on other types of data. In this study, we propose that it may be possible to improve network-information-based predictions by identifying network links that actually carry predictive power for a given prediction task. For this purpose, we suggest a problem referred to as the problem of Increasing Network Predictability (INP) by data-driven link screening. To address this problem we develop a new algorithm with three different implementations. We find that the algorithm is robust and consistently outperforms baseline link selection methods. We thus suggest that our algorithm has the potential to improve the efficacy of network data use for classification purposes.}
}


@article{DBLP:journals/tkde/MakryniotiV21,
	author = {Nantia Makrynioti and
                  Vasilis Vassalos},
	title = {Declarative Data Analytics: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2392--2411},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2958084},
	doi = {10.1109/TKDE.2019.2958084},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MakryniotiV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The area of declarative data analytics explores the application of the declarative paradigm on data science and machine learning. It proposes declarative languages for expressing data analysis tasks and develops systems which optimize programs written in those languages. The execution engine can be either centralized or distributed, as the declarative paradigm advocates independence from particular physical implementations. The survey explores a wide range of declarative data analysis frameworks by examining both the programming model and the optimization techniques used, in order to provide conclusions on the current state of the art in the area and identify open challenges.}
}


@article{DBLP:journals/tkde/DuLYH21,
	author = {Shengdong Du and
                  Tianrui Li and
                  Yan Yang and
                  Shi{-}Jinn Horng},
	title = {Deep Air Quality Forecasting Using Hybrid Deep Learning Framework},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2412--2424},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2954510},
	doi = {10.1109/TKDE.2019.2954510},
	timestamp = {Thu, 15 Jul 2021 13:46:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DuLYH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Air quality forecasting has been regarded as the key problem of air pollution early warning and control management. In this article, we propose a novel deep learning model for air quality (mainly PM2.5) forecasting, which learns the spatial-temporal correlation features and interdependence of multivariate air quality related time series data by hybrid deep learning architecture. Due to the nonlinear and dynamic characteristics of multivariate air quality time series data, the base modules of our model include one-dimensional Convolutional Neural Networks (1D-CNNs) and Bi-directional Long Short-term Memory networks (Bi-LSTM). The former is to extract the local trend features and spatial correlation features, and the latter is to learn spatial-temporal dependencies. Then we design a jointly hybrid deep learning framework based on one-dimensional CNNs and Bi-LSTM for shared representation features learning of multivariate air quality related time series data. We conduct extensive experimental evaluations using two real-world datasets, and the results show that our model is capable of dealing with PM2.5 air pollution forecasting with satisfied accuracy.}
}


@article{DBLP:journals/tkde/ZhuYZZ21,
	author = {Xiaofeng Zhu and
                  Jianye Yang and
                  Chengyuan Zhang and
                  Shichao Zhang},
	title = {Efficient Utilization of Missing Data in Cost-Sensitive Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2425--2436},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2956530},
	doi = {10.1109/TKDE.2019.2956530},
	timestamp = {Mon, 29 Jul 2024 08:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuYZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Different from previous imputation methods which impute missing values in the incomplete samples by using the information in the complete samples, this paper proposes a Date-drive Incremental imputation Model, DIM for short, which uses all available information in the data set to impute missing values economically, effectively, orderly, and iteratively. To this end, we propose a scoring rule to rank the missing features by taking into account both the economical criterion and the effective imputation information. The economical criterion takes both the imputation cost and the discriminative ability of the feature into account, while the effective imputation information enables to use all observed information in the data set including the imputed missing values to impute the left missing values. During the imputation process, our DIM first detects the neednot-impute samples for reducing the imputation cost and noise, and then selects the missing features with the top rank to impute first. The imputation process orderly imputes the missing features until all missing values are imputed or the imputation cost is exhausted. Experimental results on UCI data sets demonstrated the advantages of our proposed DIM, compared to the comparison methods, in terms of prediction accuracy and classification accuracy.}
}


@article{DBLP:journals/tkde/XuMCM21,
	author = {Ke Xu and
                  Junwen Mo and
                  Yi Cai and
                  Huaqing Min},
	title = {Enhancing Recommender Systems With a Stimulus-Evoked Curiosity Mechanism},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2437--2451},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2957770},
	doi = {10.1109/TKDE.2019.2957770},
	timestamp = {Fri, 13 Aug 2021 09:21:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuMCM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Classical algorithms in recommender systems (RS) mainly emphasis on achieving high accuracy and thus recommend items precisely matching a user's past choices. However, the user may gradually lose interest and crave something more inspiring. In psychology, curiosity is a critical human nature and can be efficient bootstrap exploratory behaviors, thus this phenomenon can be explained as insufficient stimulation to induce curiosity regard to recommended items. Inspired from the above, this work proposes a Curiosity-drive Recommendation Framework (CdRF) which incorporates a highly innovative Stimulus-evoked Curiosity mechanism (SeCM) together with a basic accuracy-oriented algorithm via Borda count. In SeCM, we first estimate the stimulus intensity appearing on each item for each user and then model personalized curiosity among the calculated intensities using Wundt curve. For the target user, the output of CdRF is a ranked list of N N items which are both relevant and highly curiousness. We conduct extensive experiments using four public datasets to evaluate the performance of each specification of SeCM as well as the whole framework CdRF. The results reveal that SeCM can flexibly generate diversified items and CdRF can increase diversity in terms of ILS, Newness and AD while compromising very little Precision. This kind of research also offers a way to understand both individual differences in curiosity and how curiosity contributes to item exploration at the level of RS.}
}


@article{DBLP:journals/tkde/ArzenoV21,
	author = {Natalia M. Arzeno and
                  Haris Vikalo},
	title = {Evolutionary Clustering via Message Passing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2452--2466},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2954869},
	doi = {10.1109/TKDE.2019.2954869},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ArzenoV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We are often interested in clustering objects that evolve over time and identifying solutions to the clustering problem for every time step. Evolutionary clustering provides insight into cluster evolution and temporal changes in cluster memberships while enabling performance superior to that achieved by independently clustering data collected at different time points. In this article we introduce evolutionary affinity propagation (EAP), an evolutionary clustering algorithm that groups data points by exchanging messages on a factor graph. EAP promotes temporal smoothness of the solution to clustering time-evolving data by linking the nodes of the factor graph that are associated with adjacent data snapshots, and introduces consensus nodes to enable cluster tracking and identification of cluster births and deaths. Unlike existing evolutionary clustering methods that require additional processing to approximate the number of clusters or match them across time, EAP determines the number of clusters and tracks them automatically. A comparison with existing methods on simulated and experimental data demonstrates effectiveness of the proposed EAP algorithm.}
}


@article{DBLP:journals/tkde/MaZXXCLY21,
	author = {Ke Ma and
                  Jinshan Zeng and
                  Jiechao Xiong and
                  Qianqian Xu and
                  Xiaochun Cao and
                  Wei Liu and
                  Yuan Yao},
	title = {Fast Stochastic Ordinal Embedding With Variance Reduction and Adaptive
                  Step Size},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2467--2478},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2956700},
	doi = {10.1109/TKDE.2019.2956700},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MaZXXCLY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning representation from relative similarity comparisons, often called ordinal embedding, gains rising attention in recent years. Most of the existing methods are based on semi-definite programming ( SDP ), which is generally time-consuming and degrades the scalability, especially confronting large-scale data. To overcome this challenge, we propose a stochastic algorithm called SVRG-SBB , which has the following features: i) achieving good scalability via dropping positive semi-definite ( PSD ) constraints as serving a fast algorithm, i.e., stochastic variance reduced gradient ( SVRG ) method, and ii) adaptive learning via introducing a new, adaptive step size called the stabilized Barzilai-Borwein ( SBB ) step size. Theoretically, under some natural assumptions, we show the\nO(\n1\nT\n)\nrate of convergence to a stationary point of the proposed algorithm, where\nT\nis the number of total iterations. Under the further Polyak-Łojasiewicz assumption, we can show the global linear convergence (i.e., exponentially fast converging to a global optimum) of the proposed algorithm. Numerous simulations and real-world data experiments are conducted to show the effectiveness of the proposed algorithm by comparing with the state-of-the-art methods, notably, much lower computational cost with good prediction performance.}
}


@article{DBLP:journals/tkde/LeiZCWYF21,
	author = {Mingtao Lei and
                  Xi Zhang and
                  Lingyang Chu and
                  Zhefeng Wang and
                  Philip S. Yu and
                  Binxing Fang},
	title = {Finding Route Hotspots in Large Labeled Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2479--2492},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2956924},
	doi = {10.1109/TKDE.2019.2956924},
	timestamp = {Tue, 27 Aug 2024 07:00:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LeiZCWYF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many advanced network analysis applications, like social networks, e-commerce, and network security, hotspots are generally considered as a group of vertices that are tightly connected owing to the similar characteristics, such as common habits and location proximity. In this article, we investigate the formation of hotspots from an alternative perspective that considers the routes along the network paths as the auxiliary information, and attempt to find the route hotspots in large labeled networks. A route hotspot is a cohesive subgraph that is covered by a set of routes, and these routes correspond to the same sequential pattern consisting of vertices’ labels. To the best of our knowledge, the problem of Finding Route Hotspots in Large Labeled Networks has not been tackled in the literature. However, it is challenging as counting the number of hotspots in a network is #P-hard. Inspired by the observation that the sizes of hotspots decrease with the increasing lengths of patterns, we prove several anti-monotonicity properties of hotspots, and then develop a scalable algorithm called FastRH that can use these properties to effectively prune the patterns that cannot form any hotspots. In addition, to avoid the duplicate computation overhead, we judiciously design an effective index structure called RH-Index for storing the hotspot and pattern information collectively, which also enables incremental updating and efficient query processing. Our experimental results on real-world datasets clearly demonstrate the effectiveness and scalability of our proposed methods.}
}


@article{DBLP:journals/tkde/FengHTC21,
	author = {Fuli Feng and
                  Xiangnan He and
                  Jie Tang and
                  Tat{-}Seng Chua},
	title = {Graph Adversarial Training: Dynamically Regularizing Based on Graph
                  Structure},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2493--2504},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2957786},
	doi = {10.1109/TKDE.2019.2957786},
	timestamp = {Tue, 01 Jun 2021 08:34:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FengHTC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent efforts show that neural networks are vulnerable to small but intentional perturbations on input features in visual classification tasks. Due to the additional consideration of connections between examples (e.g., articles with citation link tend to be in the same class), graph neural networks could be more sensitive to the perturbations, since the perturbations from connected examples exacerbate the impact on a target example. Adversarial Training (AT), a dynamic regularization technique, can resist the worst-case perturbations on input features and is a promising choice to improve model robustness and generalization. However, existing AT methods focus on standard classification, being less effective when training models on graph since it does not model the impact from connected examples. In this work, we explore adversarial training on graph, aiming to improve the robustness and generalization of models learned on graph. We propose Graph Adversarial Training (GraphAT), which takes the impact from connected examples into account when learning to construct and resist perturbations. We give a general formulation of GraphAT, which can be seen as a dynamic regularization scheme based on the graph structure. To demonstrate the utility of GraphAT, we employ it on a state-of-the-art graph neural network model - Graph Convolutional Network (GCN). We conduct experiments on two citation graphs (Citeseer and Cora) and a knowledge graph (NELL), verifying the effectiveness of GraphAT which outperforms normal training on GCN by 4.51 percent in node classification accuracy. Codes are available via: https://github.com/fulifeng/GraphAT.}
}


@article{DBLP:journals/tkde/PengLWWGYLYH21,
	author = {Hao Peng and
                  Jianxin Li and
                  Senzhang Wang and
                  Lihong Wang and
                  Qiran Gong and
                  Renyu Yang and
                  Bo Li and
                  Philip S. Yu and
                  Lifang He},
	title = {Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for
                  Large-Scale Multi-Label Text Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2505--2519},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2959991},
	doi = {10.1109/TKDE.2019.2959991},
	timestamp = {Thu, 15 Feb 2024 19:05:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/PengLWWGYLYH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {CNNs, RNNs, GCNs, and CapsNets have shown significant insights in representation learning and are widely used in various text mining tasks such as large-scale multi-label text classification. Most existing deep models for multi-label text classification consider either the non-consecutive and long-distance semantics or the sequential semantics. However, how to coherently take them into account is still far from studied. In addition, most existing methods treat output labels as independent medoids, ignoring the hierarchical relationships among them, which leads to a substantial loss of useful semantic information. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent CNNs framework for large-scale multi-label text classification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding word matrix representation preserving both the non-consecutive, long-distance and local sequential semantics. Then the word matrix is input to the proposed attentional graph capsule recurrent CNNs for effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-label text classification by comparing with state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/HuangWC21,
	author = {Ling Huang and
                  Chang{-}Dong Wang and
                  Hongyang Chao},
	title = {HM-Modularity: {A} Harmonic Motif Modularity Approach for Multi-Layer
                  Network Community Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2520--2533},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2956532},
	doi = {10.1109/TKDE.2019.2956532},
	timestamp = {Tue, 12 Nov 2024 16:50:49 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuangWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-layer network community detection has drawn an increasing amount of attention recently. Despite success, the existing methods mainly focus on the lower-order connectivity structure at the level of individual nodes and edges. And the higher-order connectivity structure has been largely ignored, which contains better signature of community compared with edges. The main challenges in utilizing higher-order structure for multi-layer network community detection are that the most representative higher-order structure may vary from one layer to another and the connectivity structure formed by the same node subset may exhibit different higher-order connectivity patterns in different layers. To this end, this paper proposes a novel higher-order structure, termed harmonic motif, which is a dense subgraph having on average the largest statistical significance in each layer. Based on the harmonic motif, a primary layer is constructed by integrating higher-order structural information from all layers. Additionally, the higher-order structural information of each individual layer is taken as the auxiliary information. A coupling is established between the primary layer and each auxiliary layer. Accordingly, a harmonic motif modularity is designed to generate the community structure. Extensive experiments on eleven real-world multi-layer network datasets have been conducted to confirm the effectiveness of the proposed method.}
}


@article{DBLP:journals/tkde/GuLLXZZ21,
	author = {Binbin Gu and
                  Zhixu Li and
                  An Liu and
                  Jiajie Xu and
                  Lei Zhao and
                  Xiaofang Zhou},
	title = {Improving the Quality of Web-Based Data Imputation With Crowd Intervention},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2534--2547},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2954087},
	doi = {10.1109/TKDE.2019.2954087},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GuLLXZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data incompleteness is a common data quality problem in databases. Recent work proposes to retrieve missing string values from the World Wide Web for higher imputation recall, but on the other hand, takes the risk of introducing web noises into the imputation results. So far there lacks an effective way to control the quality of web-based data imputation, given the complexity of the quality model and lacking of enough ground truth data. In this article, an EM-based quality model is first built for web-based data imputation which investigates three key factors jointly, i.e., precision of web sources, correlation among web sources, and precision and recall of the employed extractors. However, the accuracy of the EM-based quality model could be harmed when the EM (Expectation Maximization) assumption that “the majority agree on the truth” does not hold in some cases. To solve this problem, we introduce crowd intervention to help improve the quality model. While a straightforward but expensive way is to let the crowd to identify all these undesirable cases and provide the right imputation values for these blanks, a most crowd-economic way is to select a small set of blanks for crowd-based imputation, whose results could help to adjust the EM-based quality model towards a better one. To achieve this, an adaptive blank selection strategy is proposed to select a sequence of blanks for crowd-based imputation. Also, we work on finding a proper time to stop further crowd intervention for the balance of crowd efficiency and quality improvement. Our experiments performed on three real world and one simulated data collections prove that the proposed quality model can effectively help improve the quality of the web-based imputation results by more than 15 percent, while our crowd cost saving strategy saves more than 75 percent crowd cost.}
}


@article{DBLP:journals/tkde/WanWWWLZ21,
	author = {Pengfei Wan and
                  Xiaoming Wang and
                  Xinyan Wang and
                  Liang Wang and
                  Yaguang Lin and
                  Wei Zhao},
	title = {Intervening Coupling Diffusion of Competitive Information in Online
                  Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2548--2559},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2954901},
	doi = {10.1109/TKDE.2019.2954901},
	timestamp = {Tue, 27 Aug 2024 18:45:46 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WanWWWLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The vigorously rising of social media brings a new opportunity for information diffusion in online social networks. However, the existing models of information diffusion only consider the single information, such as rumor. What's more, most of intervention frameworks are modeled under the ideal circumstances without reality constraints. In this article, we propose a novel model of competitive information coupling diffusion to describe the complex process of information diffusion in online social networks. Especially, in order to intervene the process of competitive information coupling diffusion, we introduce three intervention strategies and propose an intervention framework. More importantly, we take the dynamic constraints into consideration such as the budget of intervention and current state of the system, and further propose the constrained intervention model. To reduce the system loss, we establish an optimal control problem with constraints to achieve the optimal allocation of intervention strategies over time and minimize the total loss. We theoretically prove the existence and uniqueness of the optimal solution of the problem, and derive the optimal control solution. Through the experiments, we verify the effectiveness of the model and analyze the efficiency of different intervention strategies about competitive information coupling diffusion with or without constraints, respectively. The results show that the collaborative intervention strategies can effectively impact the process of diffusion and get the minimum system loss. This article provides high realistic significance to the commercial marketing in online social networks.}
}


@article{DBLP:journals/tkde/LiWHZ21,
	author = {Jiangtao Li and
                  Jianshe Wu and
                  Weiquan He and
                  Peng Zhou},
	title = {Large-Scale Nodes Classification With Deep Aggregation Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2560--2572},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2955502},
	doi = {10.1109/TKDE.2019.2955502},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiWHZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The most fundamental task of network representation learning (NRL) is nodes classification which requires an algorithm to map nodes to vectors and use machine learning models to predict nodes' labels. Recently, many methods based on neighborhood aggregation have achieved brilliant results in this task. However, the recursive expansion of neighborhood aggregation poses scalability and efficiency problems for deep models. Existing methods are limited to shallow architectures and cannot capture the high order proximity in networks. In this article, we propose the deep aggregation network (DAN). DAN uses a layer-wise greedy optimization strategy which stacks several sequential trained base models to form the final deep model. The high order neighborhood aggregation is performed in a dynamic programming manner, which allows the recursion nature of neighborhood aggregation to be eliminated. The reverse random walk is also proposed, and combined with the classic random walk in formulating a novel sampling strategy that allows DAN to flexibly adapt to different tasks related to communities or structural roles. DAN is more efficient and effective than previous neighborhood aggregation based methods, especially when it is intended to handle large-scale networks with dense connections. Extensive experiments are conducted on both synthetic and real-world networks to empirically demonstrate the effectiveness and efficiency of the proposed method.}
}


@article{DBLP:journals/tkde/BaggagASZAMS21,
	author = {Abdelkader Baggag and
                  Sofiane Abbar and
                  Ankit Sharma and
                  Tahar Zanouda and
                  Abdulaziz Al{-}Homaid and
                  Abhiraj Mohan and
                  Jaideep Srivastava},
	title = {Learning Spatiotemporal Latent Factors of Traffic via Regularized
                  Tensor Factorization: Imputing Missing Values and Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2573--2587},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2954868},
	doi = {10.1109/TKDE.2019.2954868},
	timestamp = {Mon, 28 Aug 2023 21:37:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BaggagASZAMS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent transportation systems are a key component in smart cities, and the estimation and prediction of the spatiotemporal traffic state is critical to capture the dynamics of traffic congestion, i.e., its generation, propagation and mitigation, in order to increase operational efficiency and improve livability within smart cities. And while spatiotemporal data related to traffic is becoming common place due to the wide availability of cheap sensors and the rapid deployment of IoT platforms, the data still suffer some challenges related to sparsity, incompleteness, and noise which makes the traffic analytics difficult. In this article, we investigate the problem of missing data or noisy information in the context of real-time monitoring and forecasting of traffic congestion for road networks in a city. The road network is represented as a directed graph in which nodes are junctions (intersections) and edges are road segments. We assume that the city has deployed high-fidelity sensors for speed reading in a subset of edges; and the objective is to infer the speed readings for the remaining edges in the network; and to estimate the missing values in the segments for which sensors have stopped generating data due to technical problems (e.g., battery, network, etc.). We propose a tensor representation for the series of road network snapshots, and develop a regularized factorization method to estimate the missing values, while learning the latent factors of the network. The regularizer, which incorporates spatial properties of the road network, improves the quality of the results. The learned factors, with a graph-based temporal dependency, are then used in an autoregressive algorithm to predict the future state of the road network with a large horizon. Extensive numerical experiments with real traffic data from the cities of Doha (Qatar) and Aarhus (Denmark) demonstrate that the proposed approach is appropriate for imputing the missing data and predicting the traffic state. It is accurate and efficient and can easily be applied to other traffic datasets.}
}


@article{DBLP:journals/tkde/GaoHGCFLCYSJ21,
	author = {Chen Gao and
                  Xiangnan He and
                  Dahua Gan and
                  Xiangning Chen and
                  Fuli Feng and
                  Yong Li and
                  Tat{-}Seng Chua and
                  Lina Yao and
                  Yang Song and
                  Depeng Jin},
	title = {Learning to Recommend With Multiple Cascading Behaviors},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2588--2601},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2958808},
	doi = {10.1109/TKDE.2019.2958808},
	timestamp = {Sun, 06 Oct 2024 21:41:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GaoHGCFLCYSJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most existing recommender systems leverage user behavior data of one type only, such as the purchase behavior in E-commerce that is directly related to the business Key Performance Indicator (KPI) of conversion rate. Besides the key behavioral data, we argue that other forms of user behaviors also provide valuable signal, such as views, clicks, adding a product to shopping carts and so on. They should be taken into account properly to provide quality recommendation for users. In this work, we contribute a new solution named short for Neural Multi-Task Recommendation (NMTR) for learning recommender systems from user multi-behavior data. We develop a neural network model to capture the complicated and multi-type interactions between users and items. In particular, our model accounts for the cascading relationship among different types of behaviors (e.g., a user must click on a product before purchasing it). To fully exploit the signal in the data of multiple types of behaviors, we perform a joint optimization based on the multi-task learning framework, where the optimization on a behavior is treated as a task. Extensive experiments on two real-world datasets demonstrate that NMTR significantly outperforms state-of-the-art recommender systems that are designed to learn from both single-behavior data and multi-behavior data. Further analysis shows that modeling multiple behaviors is particularly useful for providing recommendation for sparse users that have very few interactions.}
}


@article{DBLP:journals/tkde/HouZZ21,
	author = {Bo{-}Jian Hou and
                  Lijun Zhang and
                  Zhi{-}Hua Zhou},
	title = {Learning With Feature Evolvable Streams},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2602--2615},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2954090},
	doi = {10.1109/TKDE.2019.2954090},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HouZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning with streaming data has attracted much attention during the past few years. Though most studies consider data stream with fixed features, in real practice the features may be evolvable. For example, features of data gathered by limited-lifespan sensors will change when these sensors are substituted by new ones. In this article, we propose a novel learning paradigm: Feature Evolvable Streaming Learning where old features would vanish and new features would occur. Rather than relying on only the current features, we attempt to recover the vanished features and exploit it to improve performance. Specifically, we learn a mapping from the overlapping period to recover old features and then we learn two models from the recovered features and the current features, respectively. To benefit from the recovered features, we develop two ensemble methods. In the first method, we combine the predictions from two models and theoretically show that with the assistance of old features, the performance on new features can be improved and we provide a tighter bound when the loss function is exponentially concave. In the second approach, we dynamically select the best single prediction and establish a better performance guarantee when the best model switches. Experiments on both synthetic and real data validate the effectiveness of our proposal.}
}


@article{DBLP:journals/tkde/ZhuMK21,
	author = {Yan Zhu and
                  Abdullah Mueen and
                  Eamonn J. Keogh},
	title = {Matrix Profile {IX:} Admissible Time Series Motif Discovery With Missing
                  Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2616--2626},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2950623},
	doi = {10.1109/TKDE.2019.2950623},
	timestamp = {Sat, 07 Sep 2024 13:34:29 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuMK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The discovery of time series motifs has emerged as one of the most useful primitives in time series data mining. Researchers have shown its utility for exploratory data mining, summarization, visualization, segmentation, classification, clustering, and rule discovery. Although there has been more than a decade of extensive research, there is still no technique to allow the discovery of time series motifs in the presence of missing data, despite the well-documented ubiquity of missing data in scientific, industrial, and medical datasets. In this work, we introduce a technique for motif discovery in the presence of missing data. We formally prove that our method is admissible , producing no false negatives. We also show that our method can “piggy-back” off the fastest known motif discovery method with a small constant factor time/space overhead. We will demonstrate our approach on diverse datasets with varying amounts of missing data.}
}


@article{DBLP:journals/tkde/XuWHCWYY21,
	author = {Linchuan Xu and
                  Jing Wang and
                  Lifang He and
                  Jiannong Cao and
                  Xiaokai Wei and
                  Philip S. Yu and
                  Kenji Yamanishi},
	title = {MixSp: {A} Framework for Embedding Heterogeneous Information Networks
                  With Arbitrary Number of Node and Edge Types},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2627--2639},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2955945},
	doi = {10.1109/TKDE.2019.2955945},
	timestamp = {Mon, 13 Mar 2023 11:20:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XuWHCWYY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous information network (HIN) embedding is to encode network structure into node representations with the heterogeneous semantics of different node and edge types considered. However, since each HIN may have a unique nature, e.g., a unique set of node and edge types, a model designed for one type of networks may not be applicable to or effective on another type. In this article, we thus attempt to propose a framework for HINs with arbitrary number of node and edge types. The proposed framework constructs a novel mixture-split representation of an HIN, and hence is named as MixSp. The mixture sub-representation and the split sub-representation serve as two different views of the network. Compared with existing models which only learn from the original view, MixSp thus may exploit more comprehensive information. Node representations in each view are learned by embedding the respective network structure. Moreover, the node representations are further refined through cross-view co-regularization. The framework is instantiated in three models which differ from each other in the co-regularization. Extensive experiments on three real-world datasets show MixSp outperforms several recent models in both node classification and link prediction tasks even though MixSp is not designed for a particular type of HINs.}
}


@article{DBLP:journals/tkde/LiCLCG21,
	author = {Dongsheng Li and
                  Chao Chen and
                  Tun Lu and
                  Stephen M. Chu and
                  Ning Gu},
	title = {Mixture Matrix Approximation for Collaborative Filtering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2640--2653},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2955100},
	doi = {10.1109/TKDE.2019.2955100},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiCLCG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Matrix approximation (MA) methods are integral parts of today's recommender systems. In standard MA methods, only one feature vector is learned for each user/item, which may not be accurate enough to characterize the diverse interests of users/items. For instance, users could have different opinions on a given item, so that they may need different feature vectors for the item to represent their unique interests. To this end, this article proposes a mixture matrix approximation (MMA) method, in which we assume that the user-item ratings follow mixture distributions and the user/item feature vectors vary among different stars to better characterize the diverse interests of users/items. Furthermore, we show that the proposed method can tackle both rating prediction and the top-N recommendation problems. Empirical studies on MovieLens, Netflix and Amazon datasets demonstrate that the proposed method can outperform state-of-the-art MA-based collaborative filtering methods in both rating prediction and top-N recommendation tasks.}
}


@article{DBLP:journals/tkde/ChenHHVZZ21,
	author = {Xiaoyang Chen and
                  Hongwei Huo and
                  Jun Huan and
                  Jeffrey Scott Vitter and
                  Weiguo Zheng and
                  Lei Zou},
	title = {MSQ-Index: {A} Succinct Index for Fast Graph Similarity Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2654--2668},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2954527},
	doi = {10.1109/TKDE.2019.2954527},
	timestamp = {Tue, 21 Mar 2023 21:10:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenHHVZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph similarity search under the graph edit distance constraint has received considerable attention in many applications, such as bioinformatics, data mining, pattern recognition and social networks. Existing methods for this problem have limited scalability because of the huge amount of memory they consume when handling very large graph databases with tens of millions of graphs. In this article, we present a succinct index that incorporates succinct data structures and hybrid encoding to achieve improved query time performance with minimal space usage. Specifically, the space usage of our index requires only 5–15 percent of the previous state-of-the-art indexing size while at the same time achieving several times acceleration in query time on the tested data. We also improve the query performance by augmenting the global filter with range searching, which allows us to perform similarity search in a reduced region. In addition, we propose two effective lower bounds together with a boosting technique to obtain the smallest possible candidate set. Extensive experiments demonstrate that our proposed approach is superior both in space and filtering to the state-of-the-art approaches. To the best of our knowledge, our index is the first in-memory index for this problem that successfully scales to cope with the large dataset of 25 million chemical structure graphs from the PubChem dataset. The source code is available online.}
}


@article{DBLP:journals/tkde/LuCL21,
	author = {Hsin{-}Min Lu and
                  Jih{-}Shin Chen and
                  Wei{-}Chun Liao},
	title = {Nonparametric Regression via Variance-Adjusted Gradient Boosting Gaussian
                  Process Regression},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2669--2679},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2953728},
	doi = {10.1109/TKDE.2019.2953728},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Regression models have broad applications in data analytics. Gaussian process regression is a nonparametric regression model that learns nonlinear maps from input features to real-valued output using a kernel function that constructs the covariance matrix among all pairs of data. Gaussian process regression often performs well in various applications. However, the time complexity of Gaussian process regression is O(n 3 ) O(n3) for a training dataset of size n n. The cubic time complexity hinders Gaussian process regression from scaling up to large datasets. Guided by the properties of Gaussian distributions, we developed a variance-adjusted gradient boosting algorithm for approximating a Gaussian process regression (VAGR). VAGR sequentially approximates the full Gaussian process regression model using the residuals computed from variance-adjusted predictions based on randomly sampled training subsets. VAGR has a time complexity of O(nm 3 ) O(nm3) for a training dataset of size n n and the chosen batch size m m. The reduced time complexity allows us to apply VAGR to much larger datasets compared with the full Gaussian process regression. Our experiments suggest that VAGR has a prediction performance comparable to or better than models that include random forest, gradient boosting machines, support vector regressions, and stochastic variational inference for Gaussian process regression.}
}


@article{DBLP:journals/tkde/ZhangZNWCHT21,
	author = {Yifan Zhang and
                  Peilin Zhao and
                  Shuaicheng Niu and
                  Qingyao Wu and
                  Jiezhang Cao and
                  Junzhou Huang and
                  Mingkui Tan},
	title = {Online Adaptive Asymmetric Active Learning With Limited Budgets},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2680--2692},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2955078},
	doi = {10.1109/TKDE.2019.2955078},
	timestamp = {Mon, 05 Feb 2024 20:21:14 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZNWCHT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online Active Learning (OAL) aims to manage unlabeled datastream by selectively querying the label of data. OAL is applicable to many real-world problems, such as anomaly detection in health-care and finance. In these problems, there are two key challenges: the query budget is often limited; the ratio between classes is highly imbalanced. In practice, it is quite difficult to handle imbalanced unlabeled datastream when only a limited budget of labels can be queried for training. To solve this, previous OAL studies adopt either asymmetric losses or queries (an isolated asymmetric strategy) to tackle the imbalance, and use first-order methods to optimize the cost-sensitive measure. However, the isolated strategy limits their performance in class imbalance, while first-order methods restrict their optimization performance. In this article, we propose a novel Online Adaptive Asymmetric Active learning algorithm, based on a new asymmetric strategy (merging both asymmetric losses and queries strategies), and second-order optimization. We theoretically analyze its mistake bound and cost-sensitive metric bounds. Moreover, to better balance performance and efficiency, we enhance our algorithm via a sketching technique, which significantly accelerates the computational speed with quite slight performance degradation. Promising results demonstrate the effectiveness and efficiency of the proposed methods.}
}


@article{DBLP:journals/tkde/WangHZ21,
	author = {Chengyu Wang and
                  Xiaofeng He and
                  Aoying Zhou},
	title = {Open Relation Extraction for Chinese Noun Phrases},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2693--2708},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2953839},
	doi = {10.1109/TKDE.2019.2953839},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangHZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relation Extraction (RE) aims at harvesting relational facts from texts. A majority of existing research targets at knowledge acquisition from sentences, where subject-verb-object structures are usually treated as the signals of existence of relations. In contrast, relational facts expressed within noun phrases are highly implicit. Previous works mostly relies on human-compiled assertions and textual patterns in English to address noun phrase-based RE. For Chinese, the corresponding task is non-trivial because Chinese is a highly analytic language with flexible expressions. Additionally, noun phrases tend to be incomplete in grammatical structures, where clear mentions of predicates are often missing. In this article, we present an unsupervised Noun Phrase-based Open RE system for the Chinese language (NPORE), which employs a three-layer data-driven architecture. The system contains three components, i.e., Modifier-sensitive Phrase Segmenter, Candidate Relation Generator and Missing Relation Predicate Detector. It integrates with a graph clique mining algorithm to chunk Chinese noun phrases, considering how relations are expressed. We further propose a probabilistic method with knowledge priors and a hypergraph-based random walk process to detect missing relation predicates. Experiments over Chinese Wikipedia show NPORE outperforms state-of-the-art, capable of extracting 55.2 percent more relations than the most competitive baseline, with a comparable precision at 95.4 percent.}
}


@article{DBLP:journals/tkde/DuTD21,
	author = {Zhengxiao Du and
                  Jie Tang and
                  Yuhui Ding},
	title = {{POLAR++:} Active One-Shot Personalized Article Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2709--2722},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2953721},
	doi = {10.1109/TKDE.2019.2953721},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DuTD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of personalized article recommendation, in particular when the user's preference data is missing or limited, which is knowns as the user cold-start issue in recommender systems. We propose POLAR++, an active recommendation framework that utilizes Bayesian neural networks to capture the uncertainty of user preference, actively selects articles to query the user for feedback, and adaptively learns user preference with one-shot learning. For the article recommendation, we design an attention-based CNN to quantify the similarity between user preference and recommended articles, which significantly improves the performance with only a few articles rated by the users. We evaluate the proposed POLAR++ on datasets of different scale and sources. Experimental results demonstrate the effectiveness of the proposed model. We have successfully deployed POLAR++ into AMiner as the recommendation engine for article recommendation, which further confirms the effectiveness of the proposed model.}
}


@article{DBLP:journals/tkde/ZhaoKV21,
	author = {Kai Zhao and
                  Denis Khryashchev and
                  Huy T. Vo},
	title = {Predicting Taxi and Uber Demand in Cities: Approaching the Limit of
                  Predictability},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2723--2736},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2955686},
	doi = {10.1109/TKDE.2019.2955686},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoKV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series prediction has wide applications ranging from stock price prediction, product demand estimation to economic forecasting. In this article, we treat the taxi and Uber demand in each location as a time series, and reduce the taxi and Uber demand prediction problem to a time series prediction problem. We answer two key questions in this area. First, time series have different temporal regularity. Some are easy to be predicted and others are not. Given a predictive algorithm such as LSTM (deep learning) or ARIMA (time series), what is the maximum prediction accuracy that it can reach if it captures all the temporal patterns of that time series? Second, given the maximum predictability, which algorithm could approach the upper bound in terms of prediction accuracy? To answer these two question, we use temporal-correlated entropy to measure the time series regularity and obtain the maximum predictability. Testing with 14 million data samples, we find that the deep learning algorithm is not always the best algorithm for prediction. When the time series has a high predictability a simple Markov prediction algorithm (training time 0.5s) could outperform a deep learning algorithm (training time 6 hours). The predictability can help determine which predictor to use in terms of the accuracy and computational costs. We also find that the Uber demand is easier to be predicted compared the taxi demand due to different cruising strategies as the former is demand driven with higher temporal regularity.}
}


@article{DBLP:journals/tkde/JinWHDZ21,
	author = {Di Jin and
                  Xiaobao Wang and
                  Dongxiao He and
                  Jianwu Dang and
                  Weixiong Zhang},
	title = {Robust Detection of Link Communities With Summary Description in Social
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2737--2749},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2958806},
	doi = {10.1109/TKDE.2019.2958806},
	timestamp = {Thu, 30 Mar 2023 17:55:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JinWHDZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community detection has been extensively studied for various applications. Recent research has started to explore node contents to identify semantically meaningful communities. However, links in real networks typically have semantic descriptions and communities of links can better characterize community behaviors than communities of nodes. The second issue in community finding is that the most existing methods assume network topologies and descriptive contents carry the same or compatible information of node group membership, restricting them to one topic per community, which is generally violated in real networks. The third issue is that the existing methods use top ranked words or phrases to label topics when interpreting communities, which is often inadequate for comprehension. To address these issues altogether, we propose a new Bayesian probabilistic approach for modeling real networks and developing an efficient variational algorithm for model inference. Our new method explores the intrinsic correlation between communities and topics to discover link communities and extract semantically meaningful community summaries at the same time. If desired, it is able to derive more than one topical summary per community to provide rich explanations. We present experimental results to show the effectiveness of our new approach and evaluate the method by a case study.}
}


@article{DBLP:journals/tkde/SongJZ21,
	author = {Wei Song and
                  Hans{-}Arno Jacobsen and
                  Pengcheng Zhang},
	title = {Self-Healing Event Logs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2750--2763},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2956520},
	doi = {10.1109/TKDE.2019.2956520},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SongJZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Event logs of process-aware information systems play an increasingly critical role in today's enterprises because they are the basis for a number of business intelligence applications such as complex event processing, provenance analysis, performance analysis, and process mining. However, due to incorrect manual recording, system errors, and resource constraints, event logs inevitably contain noise in the form of deviating event sequences with redundant, missing, or dislocated events. To repair event logs, existing approaches rely on predefined process models to obtain a minimum recovery for each deviating event sequence. However, process models are typically unavailable in practice, rendering existing approaches inapplicable. In this scenario, can event logs be self-healing? To address this problem, we propose an approach that leverages compliant event sequences to repair deviating sequences. Our approach is effective if the compliant event sequences contain sufficient knowledge for repair. We implement our approach in a prototype and employ the tool to conduct experiments. The experimental results demonstrate that our approach can achieve efficient repairs without the help of process models.}
}


@article{DBLP:journals/tkde/XuZTLL21,
	author = {Jianpeng Xu and
                  Jiayu Zhou and
                  Pang{-}Ning Tan and
                  Xi Liu and
                  Lifeng Luo},
	title = {Spatio-Temporal Multi-Task Learning via Tensor Decomposition},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2764--2775},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2956713},
	doi = {10.1109/TKDE.2019.2956713},
	timestamp = {Thu, 14 Oct 2021 08:57:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuZTLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predictive modeling of large-scale spatio-temporal data is an important but challenging problem as it requires training models that can simultaneously predict the target variables of interest at multiple locations while preserving the spatial and temporal dependencies of the data. In this paper, we investigate the effectiveness of applying a multi-task learning approach based on supervised tensor decomposition to the spatio-temporal prediction problem. Our proposed framework, known as SMART, encodes the data as a third-order tensor and extracts a set of interpretable, spatial and temporal latent factors from the data. An ensemble of spatial and temporal prediction models are trained using the latent factors as their predictor variables. Outputs from the ensemble model are aggregated to make predictions on test instances. The framework also allows known patterns from the domain to be incorporated as constraints to guide the tensor decomposition and ensemble learning processes. As the data may grow over space and time, an incremental learning version of the framework is given to efficiently update the models. We perform extensive experiments using a global-scale climate dataset to evaluate the accuracy and efficiency of the models as well as interpretability of the latent factors.}
}


@article{DBLP:journals/tkde/XuanWZYFRC21,
	author = {Qi Xuan and
                  Jinhuan Wang and
                  Minghao Zhao and
                  Junkun Yuan and
                  Chenbo Fu and
                  Zhongyuan Ruan and
                  Guanrong Chen},
	title = {Subgraph Networks With Application to Structural Feature Space Expansion},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2776--2789},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2957755},
	doi = {10.1109/TKDE.2019.2957755},
	timestamp = {Tue, 07 Sep 2021 15:04:30 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuanWZYFRC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-world networks exhibit prominent hierarchical and modular structures, with various subgraphs as building blocks. Most existing studies simply consider distinct subgraphs as motifs and use only their numbers to characterize the underlying network. Although such statistics can be used to describe a network model, or even to design some network algorithms, the role of subgraphs in such applications can be further explored so as to improve the results. In this article, the concept of subgraph network (SGN) is introduced and then applied to network models, with algorithms designed for constructing the 1st-order and 2nd-order SGNs, which can be easily extended to build higher-order ones. Furthermore, these SGNs are used to expand the structural feature space of the underlying network, beneficial for network classification. Numerical experiments demonstrate that the network classification model based on the structural features of the original network together with the 1st-order and 2nd-order SGNs always performs the best as compared to the models based only on one or two of such networks. In other words, the structural features of SGNs can complement that of the original network for better network classification, regardless of the feature extraction method used, such as the handcrafted, network embedding and kernel-based methods.}
}


@article{DBLP:journals/tkde/ShaoLCYC21,
	author = {Yingxia Shao and
                  Xupeng Li and
                  Yiru Chen and
                  Lele Yu and
                  Bin Cui},
	title = {Sys-TM: {A} Fast and General Topic Modeling System},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2790--2802},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2956518},
	doi = {10.1109/TKDE.2019.2956518},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShaoLCYC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Topic models, such as LDA and its variants, are popular probabilistic models for discovering the abstract “topics” that occur in a collection of documents. However, the performance of topic models may vary a lot for different workloads, and it is not a trivial task to achieve a well-optimized implementation. In this paper, we systematically study all recently proposed samplers over LDA: AliasLDA, F+LDA, LightLDA, and WarpLDA, and discover a novel system tradeoff by considering the diversity and skewness of workloads. Then, we propose a hybrid sampler which can cleverly choose an efficient sampler with the tradeoff, and apply the hybrid sampler to LDA and its variants, including STM, TOT and CTM. Finally, we build a fast and general topic modeling system Sys-TM, which provides a unified topic modeling framework by integrating the hybrid sampler. Based on our empirical studies, the hybrid sampler outperforms the state-of-the-art samplers by up to 2× 2× over various topic models, and with carefully engineered implementation, Sys-TM is able to outperform the existing systems by up to 10× 10×.}
}


@article{DBLP:journals/tkde/WangLZZH21,
	author = {Ze{-}ke Wang and
                  Xue Liu and
                  Kai Zhang and
                  Haihang Zhou and
                  Bingsheng He},
	title = {Understanding and Optimizing Conjunctive Predicates Under Memory-Efficient
                  Storage Layouts},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {6},
	pages = {2803--2817},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2958672},
	doi = {10.1109/TKDE.2019.2958672},
	timestamp = {Tue, 01 Jun 2021 08:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangLZZH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database queries can contain multiple predicates. The optimization of conjunctive predicates is still vital to the overall performance of analytic data processing tasks. Prior work proposes several memory-efficient storage layouts, e.g., BitWeaving and ByteSlice, to significantly accelerate predicate evaluation, as circuit-level intra-cycle parallelism available in modern CPUs can be exploited such that the total number of instructions can be dramatically reduced. However, the performance potential of conjunctive predicates has not been harvested yet under such storage layouts as there is no accurate cost model to provide necessary insights that guide the optimization process. In this paper, we propose a hybrid empirical/analytical cost model (Understanding) to unveil the performance characteristics of such storage layouts when applying to predicate evaluation. Our cost model takes into account effect of non-linear factors, e.g., cache miss and branch misprediction, and easily applies to different CPUs. The main finding from our cost model is to distinguish high-cost instruction (which suffers from cache miss and/or branch misprediction) from low-cost instruction (which enjoys cache hit and correct branch prediction) in the context of predicate evaluation under these storage layouts. Guided by such a finding, we propose a simple execution scheme Hebe (Optimizing), which is order-oblivious while maintaining high performance. Hebe is attractive to the query optimizer (QO), as the QO does not need to go through a sampling process to decide the optimal evaluation order in advance. The intuition behind Hebe is to significantly reduce the number of high-cost instructions while keeping low-cost instructions unchanged. Our finding from Hebe sheds light on the importance of accurate cost model that guide us to derive an efficient execution scheme for query processing on modern CPUs.}
}


@article{DBLP:journals/tkde/RezvaniBE21,
	author = {Roonak Rezvani and
                  Payam M. Barnaghi and
                  Shirin Enshaeifar},
	title = {A New Pattern Representation Method for Time-Series Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2818--2832},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2961097},
	doi = {10.1109/TKDE.2019.2961097},
	timestamp = {Thu, 27 Jul 2023 08:18:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/RezvaniBE21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid growth of Internet of Things (IoT) and sensing technologies has led to an increasing interest in time-series data analysis. In many domains, detecting patterns of IoT data and interpreting these patterns are challenging issues. There are several methods in time-series analysis that deal with issues such as volume and velocity of IoT data streams. However, analysing the content of the data streams and extracting insights from dynamic IoT data is still a challenging task. In this paper, we propose a pattern representation method which represents time-series frames as vectors by first applying Piecewise Aggregate Approximation (PAA) and then applying Lagrangian Multipliers. This method allows representing continuous data as a series of patterns that can be used and processed by various higher-level methods. We introduce a new change point detection method which uses the constructed patterns in its analysis. We evaluate and compare our representation method with Blocks of Eigenvalues Algorithm (BEATS) and Symbolic Aggregate approXimation (SAX) methods to cluster various datasets. We have evaluated our algorithm using UCR time-series datasets and also a healthcare dataset. The evaluation results show significant improvements in analysing time-series data in our proposed method.}
}


@article{DBLP:journals/tkde/ZhaoHZWW21,
	author = {Hong Zhao and
                  Qinghua Hu and
                  Pengfei Zhu and
                  Yu Wang and
                  Ping Wang},
	title = {A Recursive Regularization Based Feature Selection Framework for Hierarchical
                  Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2833--2846},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2960251},
	doi = {10.1109/TKDE.2019.2960251},
	timestamp = {Wed, 24 Jul 2024 21:43:31 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoHZWW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The sizes of datasets in terms of the number of samples, features, and classes have dramatically increased in recent years. In particular, there usually exists a hierarchical structure among class labels as hundreds of classes exist in a classification task. We call these tasks hierarchical classification, and hierarchical structures are helpful for dividing a very large task into a collection of relatively small subtasks. Various algorithms have been developed to select informative features for flat classification. However, these algorithms ignore the semantic hyponymy in the directory of hierarchical classes, and select a uniform subset of the features for all classes. In this paper, we propose a new feature selection framework with recursive regularization for hierarchical classification. This framework takes the hierarchical information of the class structure into account. In contrast to flat feature selection, we select different feature subsets for each node in a hierarchical tree structure with recursive regularization. The proposed framework uses parent-child, sibling, and family relationships for hierarchical regularization. By imposing \\ell _{2,1}\n-norm regularization to different parts of the hierarchical classes, we can learn a sparse matrix for the feature ranking at each node. Extensive experiments on public datasets demonstrate the effectiveness and efficiency of the proposed algorithms.}
}


@article{DBLP:journals/tkde/XuYCCY21,
	author = {Yuhong Xu and
                  Zhiwen Yu and
                  Wenming Cao and
                  C. L. Philip Chen and
                  Jane You},
	title = {Adaptive Classifier Ensemble Method Based on Spatial Perception for
                  High-Dimensional Data Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2847--2862},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2961076},
	doi = {10.1109/TKDE.2019.2961076},
	timestamp = {Tue, 13 Jul 2021 13:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuYCCY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Classifying high-dimensional small-size data is challenging in the field of pattern recognition. Traditional ensemble learning methods have several limitations: 1) sample-space based methods are easily affected by noise and redundant features; 2) feature-space based methods cannot excavate the essential characteristics of features; 3) feature subspaces cause information loss, which leads to a decline in accuracy; 4) most selective ensemble methods only consider the diversity and performance of sub-classifiers and ignore the impact on integration systems. To address the above limitations, we propose an adaptive classifier ensemble learning method (AdaSPEL) based on spatial perception for high-dimensional data. First, we design a local-space perception method for feature transformation, which encourages both high performance and diversity of the ensemble members. Second, we design a cross-space perception method based on the distribution of samples to obtain the cross-space enhanced features to provide a macro analysis for the characteristics of data. Furthermore, an adaptive selective ensemble method based on local and global evaluation mechanisms is proposed, which considers the impact of sub-classifiers on integrated systems. Experimental results on 33 high-dimensional data sets verify that our method outperforms mainstream ensemble learning methods based on feature space and sample space, and neural network-based algorithms.}
}


@article{DBLP:journals/tkde/ZareHJ21,
	author = {Hadi Zare and
                  Mahdi Hajiabadi and
                  Mahdi Jalili},
	title = {Detection of Community Structures in Networks With Nodal Features
                  based on Generative Probabilistic Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2863--2874},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2960222},
	doi = {10.1109/TKDE.2019.2960222},
	timestamp = {Sat, 31 Jul 2021 17:21:34 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZareHJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community detection is considered as a fundamental task in analyzing social networks. Even though many techniques have been proposed for community detection, most of them are based exclusively on the connectivity structures. However, there are node features in real networks, such as gender types in social networks, feeding behavior in ecological networks, and location on e-trading networks, that can be further leveraged with the network structure to attain more accurate community detection methods. We propose a novel probabilistic graphical model to detect communities by taking into account both network structure and nodes' features. The proposed approach learns the relevant features of communities through a generative probabilistic model without any prior assumption on the communities. Furthermore, the model is capable of determining the strength of node features and structural elements of the networks on shaping the communities. The effectiveness of the proposed approach over the state-of-the-art algorithms is revealed on synthetic and benchmark networks.}
}


@article{DBLP:journals/tkde/MartinezBC21,
	author = {V{\'{\i}}ctor Mart{\'{\i}}nez and
                  Fernando Berzal and
                  Juan C. Cubero},
	title = {Disambiguation of Semantic Relations Using Evidence Aggregation According
                  to a Sense Inventory},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2875--2887},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2021.3055621},
	doi = {10.1109/TKDE.2021.3055621},
	timestamp = {Tue, 13 Jul 2021 13:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MartinezBC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper describes EPROP, a novel technique requiring little prior knowledge for word sense disambiguation of semantic relations between pairs of ambiguous concepts in knowledge bases. Our method makes inferences by aggregating evidences from ambiguous word interpretations and propagating the acquired knowledge over a taxonomy to generalize or specialize this knowledge. This propagation process allows the estimation of the degree of belief for each possible word sense assignment given the available evidence. EPROP only requires a sense inventory structured as a taxonomy to disambiguate a knowledge base by combining evidence from the ambiguous facts stored in the knowledge base. We have performed different experiments that show that our method achieves good results on the disambiguation of the semantic relations included in WordNet and ConceptNet. We also show how our method can be used to improve the performance of state-of-the-art word sense disambiguation methods.}
}


@article{DBLP:journals/tkde/LuoBCC21,
	author = {Hui Luo and
                  Zhifeng Bao and
                  Farhana Murtaza Choudhury and
                  J. Shane Culpepper},
	title = {Dynamic Ridesharing in Peak Travel Periods},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2888--2902},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2961341},
	doi = {10.1109/TKDE.2019.2961341},
	timestamp = {Tue, 13 Jul 2021 13:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuoBCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose and study a variant of the dynamic ridesharing problem with a specific focus on peak hours: Given a set of drivers and a set of rider requests, we aim to match drivers to each rider request by achieving two objectives: maximizing the served rate and minimizing the total additional distance, subject to a series of spatio-temporal constraints. Our problem can be distinguished from existing ridesharing solutions in three aspects: (1) Previous work did not fully explore the impact of peak travel periods where the number of rider requests is much greater than the number of available drivers. (2) Existing ridesharing solutions usually rely on single objective optimization techniques, such as minimizing the total travel cost (either distance or time). (3) When evaluating the overall system performance, the runtime spent on updating drivers' trip schedules as per newly coming rider requests should be incorporated, while it is unfortunately excluded by most existing solutions. In order to achieve our goal, we propose an underlying index structure on top of a partitioned road network, and compute the lower bounds of the shortest path distance between any two vertices. Using the proposed index together with a set of new pruning rules, we develop an efficient algorithm to dynamically include new riders directly into an existing trip schedule of a driver. In order to respond to new rider requests more effectively, we propose two algorithms that bilaterally match drivers with rider requests. Finally, we perform extensive experiments on a large-scale test collection to validate the effectiveness and efficiency of the proposed methods.}
}


@article{DBLP:journals/tkde/ChenZWLXDH21,
	author = {Jingwu Chen and
                  Fuzhen Zhuang and
                  Tianxin Wang and
                  Leyu Lin and
                  Feng Xia and
                  Lihuan Du and
                  Qing He},
	title = {Follow the Title Then Read the Article: Click-Guide Network for Dwell
                  Time Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2903--2913},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2960693},
	doi = {10.1109/TKDE.2019.2960693},
	timestamp = {Tue, 13 Jul 2021 13:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenZWLXDH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In article recommendation, the amount of time user spends on viewing articles, dwell time, is an important metric to measure the post-click engagement of user on content and has been widely used as a proxy to user satisfaction, complementing the click feedback. Recently, the sequential pattern of impression-click-read has become one of the most popular type of article recommendation service in real world, where users are presented with a list of titles at first, then get interested in one and click in for reading. Predicting dwell time in such service is conditioned on the click, since the user reads the article only after he clicks the corresponding title. We argue that conventional models for dwell time prediction, which mainly focus on the relevance between the content and the general preference of user, are not well-designed for such service. There is a natural assumption in recommendation system that the click indicates user's getting attracted by the item. Therefore, in the pattern of impression-click-read, the user might get interested and curious on some other concepts different from his general preference while reading, due to the attraction of the title. Conventional models tend to ignore the gap between such temporary interest and the general preference of user in the reading behavior, which fails to use the pattern of impression-click-read and the assumption of the click very well. In this work, we propose a framework, Click-guide Network (CGN) for dwell time prediction, which makes good use of the sequential pattern and the assumption to model the ”guidance” of the click on user preference. CGN is a joint learner for dwell time and click through rate (CTR). We introduce the CTR task as an auxiliary task to help us better learn the preference of user and the representation of title. Besides, we propose the Guider to capture the user's temporary interest raised by the title. We collect the data from WeChat, a widely-used mobile app in China, for experiments. The results demonstrate the advantages of CGN over several competitive baselines on dwell time prediction, while our case studies show how the Guider effectively capture the temporary interest of user.}
}


@article{DBLP:journals/tkde/LiuXPLZY21,
	author = {Jinfei Liu and
                  Li Xiong and
                  Jian Pei and
                  Jun Luo and
                  Haoyu Zhang and
                  Wenhui Yu},
	title = {Group-Based Skyline for Pareto Optimal Groups},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2914--2929},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2960347},
	doi = {10.1109/TKDE.2019.2960347},
	timestamp = {Tue, 13 Jul 2021 13:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuXPLZY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Skyline computation, aiming at identifying a set of skyline points that are not dominated by any other point, is particularly useful for multi-criteria data analysis and decision making. Traditional skyline computation, however, is inadequate to answer queries that need to analyze not only individual points but also groups of points. To address this gap, we generalize the original skyline definition to the novel group-based skyline (G-Skyline), which represents Pareto optimal groups that are not dominated by other groups. In order to compute G-Skyline groups consisting of s points efficiently, we present a novel structure that represents the points in a directed skyline graph and captures the dominance relationships among the points based on the first s skyline layers. We propose efficient algorithms to compute the first s skyline layers. We then present two heuristic algorithms to efficiently compute the G-Skyline groups: the point-wise algorithm and the unit group-wise algorithm, using various pruning strategies. We observe that the number of G-Skyline groups of a dataset can be significantly large, we further propose the top- k representative G-Skyline groups based on the number of dominated points and the number of dominated groups and present efficient algorithms for computing them. The experimental results on the real NBA dataset and the synthetic datasets show that G-Skyline is interesting and useful, and our algorithms are efficient and scalable.}
}


@article{DBLP:journals/tkde/DingZWLBZL21,
	author = {Yichen Ding and
                  Xun Zhou and
                  Guojun Wu and
                  Yanhua Li and
                  Jie Bao and
                  Yu Zheng and
                  Jun Luo},
	title = {Mining Spatio-Temporal Reachable Regions With Multiple Sources over
                  Massive Trajectory Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2930--2942},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2959531},
	doi = {10.1109/TKDE.2019.2959531},
	timestamp = {Tue, 13 Jul 2021 13:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DingZWLBZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a set of user-specified locations and a massive trajectory dataset, the task of mining spatio-temporal reachable regions aims at finding which road segments are reachable from these locations within a given temporal period based on the historical trajectories. Determining such spatio-temporal reachable regions with high accuracy is vital for many urban applications, such as location-based recommendations and advertising. Traditional approaches to answering such queries essentially perform a distance-based range query over the given road network, which does not consider dynamic travel time at different time of day. By contrast, we propose a data-driven approach to formulate the problem as mining actual reachable regions based on a real historical trajectory dataset. Efficient algorithms for the Single-location spatio-temporal reachability Query (S-Query) and the Union-of-multi-location spatio-temporal reachability Query (U-Query) were presented in our recent work. In this paper, we extend the previous ideas by introducing a new type of reachability query with multiple sources, namely, the Intersection-of-multi-location spatio-temporal reachability Query (I-Query). As we demonstrate, answering I-Queries efficiently is generally more computationally challenging than answering either S-Queries or U-Queries because I-Queries involve complicated intersect conditions. We propose two new algorithms called the Intersection-of-Multi-location Query Maximum Bounding region search (I-MQMB) algorithm and the I-Query Trace Back Search (I-TBS) algorithm to efficiently answer I-Queries, which utilize an indexing schema composed of a spatio-temporal index and a connection index. We evaluate our system extensively by using a large-scale real taxi trajectory dataset that records taxi rides in Shenzhen, China. Our results demonstrate that the proposed approach reduces the running time of I-Queries by 50 percent on average compared to the baseline method.}
}


@article{DBLP:journals/tkde/LiuCXYW21,
	author = {Zhicheng Liu and
                  Jun Cao and
                  Renjie Xie and
                  Junyan Yang and
                  Qiao Wang},
	title = {Modeling Submarket Effect for Real Estate Hedonic Valuation: {A} Probabilistic
                  Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2943--2955},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.3010548},
	doi = {10.1109/TKDE.2020.3010548},
	timestamp = {Tue, 13 Jul 2021 13:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuCXYW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is critical for urban planners and real estate developers to understand how the built environment and house characteristics are valued in housing market. However, this problem is challenging because of the existence of the submarket effect resulted from the heterogeneity nature of city. In this paper, we propose a probabilistic approach to residential property hedonic valuation problem modeling the full scope of submarket effect based on built environment and house characteristics. Specifically, we introduce a latent variable representing housing submarket and model both of the submarket criteria and hedonic price model(HPM) into a Bayesian network. Utilizing the probabilistic dependencies in the Bayesian network, our model is able to capture the full scope of the submarket effect. Furthermore, to analyze the relationship among the discovered submarkets, we propose a probabilistic hierarchical clustering method to infer the hierarchical structure of housing market. In particular, we perform Bayesian hypothesis testings to find the most similar submarkets and agglomerate submarkets step-by-step, thus revealing the hierarchical structure of housing market. Finally, we conduct comprehensive experiments in the housing market of Nanjing which is a metropolis in eastern China. The experimental results demonstrate the effectiveness of our proposed modeling method.}
}


@article{DBLP:journals/tkde/LiuJYN21,
	author = {Huafeng Liu and
                  Liping Jing and
                  Jian Yu and
                  Michael K. Ng},
	title = {Social Recommendation With Learning Personal and Social Latent Factors},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2956--2970},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2961666},
	doi = {10.1109/TKDE.2019.2961666},
	timestamp = {Thu, 07 Nov 2024 15:00:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuJYN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to leveraging social relationships between users as well as their past social behavior, social recommendation becomes a core component in recommendation systems. Most existing social recommendation methods only consider direct social relationships among users (e.g., explicit and observed social relations). Recently, researchers proved that indirect social relationships can be effective to improve the recommendation quality when users only have few social connections, because it can identify the user interesting group even though the users have no observed social connection. In the literature, separate two-stage methods are studied, but they cannot explicitly capture the natural relationship between indirect social relations and latent user/item factors. In this paper, the main contribution is to propose a new joint recommendation model taking advantage of the Indirect Social Relations detection and Matrix Factorization collaborative filtering on social network and rating behavior information, which is called as InSRMF. In our work, the user latent factors can simultaneously and seamlessly capture user's personal preferences and social group characteristics. To optimize the InSRMF model, we develop a parallel graph vertex programming algorithm for efficiently handling large scale social recommendation data. Experiments based on four real-world datasets (Ciao, Epinions, Douban and Yelp) are conducted to demonstrate the performance of the proposed model. The experimental results have shown that InSRMF has ability to mine the proper indirect social relations and improve the recommendation performance compared with the testing methods in the literature, especially on the users with few social neighbors, Near-cold-start Users, Pure-cold-start Users and Long-tail Items.}
}


@article{DBLP:journals/tkde/VinagreJRG21,
	author = {Jo{\~{a}}o Vinagre and
                  Al{\'{\i}}pio M{\'{a}}rio Jorge and
                  Concei{\c{c}}{\~{a}}o Rocha and
                  Jo{\~{a}}o Gama},
	title = {Statistically Robust Evaluation of Stream-Based Recommender Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2971--2982},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2960216},
	doi = {10.1109/TKDE.2019.2960216},
	timestamp = {Thu, 14 Oct 2021 08:57:05 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/VinagreJRG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online incremental models for recommendation are nowadays pervasive in both the industry and the academia. However, there is not yet a standard evaluation methodology for the algorithms that maintain such models. Moreover, online evaluation methodologies available in the literature generally fall short on the statistical validation of results, since this validation is not trivially applicable to stream-based algorithms. We propose a\nk\n-fold validation framework for the pairwise comparison of recommendation algorithms that learn from user feedback streams, using prequential evaluation. Our proposal enables continuous statistical testing on adaptive-size sliding windows over the outcome of the prequential process, allowing practitioners and researchers to make decisions in real time based on solid statistical evidence. We present a set of experiments to gain insights on the sensitivity and robustness of two statistical tests—McNemar’s and Wilcoxon signed rank—in a streaming data environment. Our results show that besides allowing a real-time, fine-grained online assessment, the online versions of the statistical tests are at least as robust as the batch versions, and definitely more robust than a simple prequential single-fold approach.}
}


@article{DBLP:journals/tkde/ChenYZLK21,
	author = {Kaiqi Chen and
                  Lanlan Yu and
                  Tingting Zhu and
                  Ping Li and
                  J{\"{u}}rgen Kurths},
	title = {Succinct Representation of Dynamic Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2983--2994},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2960240},
	doi = {10.1109/TKDE.2019.2960240},
	timestamp = {Tue, 13 Jul 2021 13:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenYZLK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many network analysis tasks like classification over nodes require careful efforts in engineering features used by learning algorithms. Most of recent studies have been made and succeeded in the field of static network representation learning. However, real-world networks are often dynamic and little work has been done on how to describe dynamic networks. In this work, we pose the problem of condensing dynamic networks and introduce SuRep , an encoding-decoding framework which utilizes matrix factorization technique to derive a succinct representation of a dynamic network in any stationary phase. We show that the succinct representation method can uncover the invariant structural properties in the network evolution and derive dense feature representations of the nodes as the byproduct. This method can be easily extended to dynamic attribute networks. For experiments on detecting change points in dynamic networks and network classification with real-world datasets we demonstrate SuRep ’s potential for capturing latent patterns among nodes.}
}


@article{DBLP:journals/tkde/ParkH21,
	author = {Yubin Park and
                  Joyce C. Ho},
	title = {Tackling Overfitting in Boosting for Noisy Healthcare Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {7},
	pages = {2995--3006},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2959988},
	doi = {10.1109/TKDE.2019.2959988},
	timestamp = {Tue, 13 Jul 2021 13:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ParkH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analyzing healthcare data poses several challenges including the limited number of samples, missing measurements, noisy labels, and heterogeneous data types. Tree-based boosting is well-suited for modeling such data as it is insensitive to data types and missingness. Moreover, Stochastic Gradient TreeBoost is often found in many winning solutions in public data science challenges. Unfortunately, the best performance requires extensive hyperparameter tuning and can be prone to overfitting. We propose PaloBoost, a Stochastic Gradient TreeBoost model that uses novel regularization techniques to guard against overfitting and is robust to hyperparameter settings. PaloBoost uses the out-of-bag samples to perform gradient-aware pruning and estimate adaptive learning rates. Unlike other Stochastic Gradient TreeBoost models that use the out-of-bag samples to estimate test errors, PaloBoost treats the samples as a second batch of training samples to prune the trees and adjust the learning rates. As a result, PaloBoost can dynamically adjust tree depths and learning rates to achieve faster learning at the start and slower learning as the algorithm converges. Experimental results on four datasets demonstrate that PaloBoost is robust to overfitting and is less sensitive to the hyperparameters.}
}


@article{DBLP:journals/tkde/ZhuYXWZRZ21,
	author = {Fangzhou Zhu and
                  Mingxuan Yuan and
                  Xike Xie and
                  Ting Wang and
                  Shenglin Zhao and
                  Weixiong Rao and
                  Jia Zeng},
	title = {A Data-Driven Sequential Localization Framework for Big Telco Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3007--3019},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2961657},
	doi = {10.1109/TKDE.2019.2961657},
	timestamp = {Thu, 16 Sep 2021 17:58:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuYXWZRZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of telco networks and mobile terminals brings the accumulation of tremendous amounts of measure report(MR) data at a rapid pace. The MR data is generated by mobile objects while connecting to data services and is stored in backend data centers. To geo-tag or localize such MR data is believed to have a profound effect on the analytics and optimizations of telco and traffic networks. However, MR records are of noisy and partial observations regarding to mobile objects' geo-locations and hence pose challenges to accurate telco data localization. There have been quite a few attempts. Single-point localization methods map a MR record to a location, but come out with limited accuracies due to the ignorance of spatiotemporal coherence of successive MR records. Recent efforts on sequential localization techniques alleviate this by mapping a sequence of MR records to a trajectory. However, existing solutions are often with assumptions on specific models, e.g., mobility and signal strength distributions, or priori knowledge on topology space, e.g., road networks, limiting the deployment in practice. To this end, we propose a data-driven framework to tackle the challenges in sequential telco localization. We solely use raw MR records and a public third-party GPS dataset for the learning of the correlations between mobile objects' locations and MR records, requiring no model assumptions and priori knowledge. To handle the data-intensive workloads during the learning process, we use materialized views for efficient online localization and light-weighted indexing techniques for periodical parameters tuning, in order to improve the efficiency and scalability. Results on real data show that our solution achieves 58.8 percent improvement in median localization errors compared with state-of-art sequential localization techniques that require hypothesis models and priori knowledge, making our solution superior in terms of effectiveness, efficiency, and employability.}
}


@article{DBLP:journals/tkde/WuFLYLWC21,
	author = {Xudong Wu and
                  Luoyi Fu and
                  Huan Long and
                  Dali Yang and
                  Yucheng Lu and
                  Xinbing Wang and
                  Guihai Chen},
	title = {Adaptive Diffusion of Sensitive Information in Online Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3020--3034},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2964242},
	doi = {10.1109/TKDE.2020.2964242},
	timestamp = {Sat, 09 Apr 2022 12:22:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuFLYLWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The cascading of sensitive information such as private contents and rumors is a severe issue in online social networks. One approach for limiting the cascading of sensitive information is constraining the diffusion among social network users. However, the diffusion constraining measures limit the diffusion of non-sensitive information diffusion as well, resulting in the bad user experiences. To tackle this issue, in this paper, we study the problem of how to minimize the sensitive information diffusion while preserve the diffusion of non-sensitive information, and formulate it as a constrained minimization problem where we characterize the intention of preserving non-sensitive information diffusion as the constraint. We study the problem of interest over the fully-known network with known diffusion abilities of all users and the semi-known network where diffusion abilities of partial users remain unknown in advance. By modeling the sensitive information diffusion size as the reward of a bandit, we utilize the bandit framework to jointly design the solutions with polynomial complexity in the both scenarios. Moreover, the unknown diffusion abilities over the semi-known network induce it difficult to quantify the information diffusion size in algorithm design. For this issue, we propose to learn the unknown diffusion abilities from the diffusion process in real time and then adaptively conduct the diffusion constraining measures based on the learned diffusion abilities, relying on the bandit framework. Extensive experiments on real and synthetic datasets demonstrate that our solutions can effectively constrain the sensitive information diffusion, and enjoy a 40 percent less diffusion loss of non-sensitive information comparing with four baseline algorithms.}
}


@article{DBLP:journals/tkde/SongYCTLS21,
	author = {Changhe Song and
                  Cheng Yang and
                  Huimin Chen and
                  Cunchao Tu and
                  Zhiyuan Liu and
                  Maosong Sun},
	title = {{CED:} Credible Early Detection of Social Media Rumors},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3035--3047},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2961675},
	doi = {10.1109/TKDE.2019.2961675},
	timestamp = {Fri, 01 Sep 2023 13:50:23 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SongYCTLS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rumors spread dramatically fast through online social media services, and people are exploring methods to detect rumors automatically. Existing methods typically learn semantic representations of all reposts to a rumor candidate for prediction. However, it is crucial to efficiently detect rumors as early as possible before they cause severe social disruption, which has not been well addressed by previous works. In this paper, we present a novel early rumor detection model, Credible Early Detection (CED). By regarding all reposts to a rumor candidate as a sequence, the proposed model will seek an early point-in-time for making a credible prediction. We conduct experiments on three real-world datasets, and the results demonstrate that our proposed model can remarkably reduce the time span for prediction by more than 85 percent, with better accuracy performance than all state-of-the-art baselines.}
}


@article{DBLP:journals/tkde/Martinez-Plumed21,
	author = {Fernando Mart{\'{\i}}nez{-}Plumed and
                  Lidia Contreras Ochando and
                  C{\`{e}}sar Ferri and
                  Jos{\'{e}} Hern{\'{a}}ndez{-}Orallo and
                  Meelis Kull and
                  Nicolas Lachiche and
                  Mar{\'{\i}}a Jos{\'{e}} Ram{\'{\i}}rez{-}Quintana and
                  Peter A. Flach},
	title = {{CRISP-DM} Twenty Years Later: From Data Mining Processes to Data
                  Science Trajectories},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3048--3061},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2962680},
	doi = {10.1109/TKDE.2019.2962680},
	timestamp = {Wed, 03 Nov 2021 08:25:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/Martinez-Plumed21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.}
}


@article{DBLP:journals/tkde/GuoY21,
	author = {Shun Guo and
                  Nianmin Yao},
	title = {Document Vector Extension for Documents Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3062--3074},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2961343},
	doi = {10.1109/TKDE.2019.2961343},
	timestamp = {Thu, 16 Sep 2021 17:58:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GuoY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Simple linear models, which usually learn word-level representations that are later combined to form document representations, have recently shown impressive performance. To improve the performance of document-level classification, it is crucial to explore the factors affecting the quality of the document vector. In this paper, we propose the concept of containers and further explore the properties of word containers and document containers by experiments and theoretical demonstrations. We find that the document container has a fixed capacity and that the document vector obtained by a simple average of too many word embeddings undoubtedly cannot be fully loaded by the container and will lose some semantic and syntactic information on very large text datasets. We also propose an efficient approach for document representation, using clustering algorithms to divide a document container into several subcontainers and establishing the relationship between the subcontainers. We additionally report and discuss the properties of two methods of clustering algorithms, DVEM-Kmeans and DVEM-Random, on large text datasets by sentiment analysis and topic classification tasks. Compared to simple linear models, the results show that our models outperform the existing state-of-the-art in generating high-quality document representations for document-level classification relatedness tasks. Our approaches can also be introduced to other models based on neural networks, such as convolutional neural networks, recurrent neural networks and generative adversarial networks, in supervised or semisupervised settings.}
}


@article{DBLP:journals/tkde/NetoSCN21,
	author = {Ant{\^{o}}nio Cavalcante Ara{\'{u}}jo Neto and
                  J{\"{o}}rg Sander and
                  Ricardo J. G. B. Campello and
                  Mario A. Nascimento},
	title = {Efficient Computation and Visualization of Multiple Density-Based
                  Clustering Hierarchies},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3075--3089},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2962412},
	doi = {10.1109/TKDE.2019.2962412},
	timestamp = {Thu, 27 Jul 2023 08:18:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NetoSCN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {HDBSCAN*, a state-of-the-art density-based hierarchical clustering method, produces a hierarchical organization of clusters in a dataset w.r.t. a parameter mpts. While a small change in mpts typically leads to a small change in the clustering structure, choosing a “good” mpts value can be challenging: depending on the data distribution, a high or low mpts value may be more appropriate, and certain clusters may reveal themselves at different values. To explore results for a range of mpts values, one has to run HDBSCAN* for each value independently, which can be computationally impractical. In this paper, we propose an approach to efficiently compute all HDBSCAN* hierarchies for a range of mpts values by building upon results from computational geometry to replace HDBSCAN*'s complete graph with a smaller equivalent graph. An experimental evaluation shows that our approach can obtain over one hundred hierarchies for the computational cost equivalent to running HDBSCAN* about twice, which corresponds to a speedup of more than 60 times, compared to running HDBSCAN* independently that many times. We also propose a series of visualizations that allow users to analyze a collection of hierarchies for a range of mpts values, along with case studies that illustrate how these analyses are performed.}
}


@article{DBLP:journals/tkde/WangWWZZZLXG21,
	author = {Hongwei Wang and
                  Jialin Wang and
                  Jia Wang and
                  Miao Zhao and
                  Weinan Zhang and
                  Fuzheng Zhang and
                  Wenjie Li and
                  Xing Xie and
                  Minyi Guo},
	title = {Learning Graph Representation With Generative Adversarial Nets},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3090--3103},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2961882},
	doi = {10.1109/TKDE.2019.2961882},
	timestamp = {Thu, 14 Oct 2021 08:57:04 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangWWZZZLXG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph representation learning aims to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in a graph, and discriminative models that predict the probability of edge between a pair of vertices. In this paper, we propose GraphGAN , an innovative graph representation learning framework unifying the above two classes of methods, in which the generative and the discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces “fake” samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, we propose a novel graph softmax as the implementation of the generative model to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization , graph structure awareness , and computational efficiency . Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including graph reconstruction, link prediction, node classification, recommendation, and visualization, over state-of-the-art baselines.}
}


@article{DBLP:journals/tkde/GalRS21,
	author = {Avigdor Gal and
                  Haggai Roitman and
                  Roee Shraga},
	title = {Learning to Rerank Schema Matches},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3104--3116},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2962124},
	doi = {10.1109/TKDE.2019.2962124},
	timestamp = {Thu, 16 Sep 2021 17:58:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GalRS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Schema matching is at the heart of integrating structured and semi-structured data with applications in data warehousing, data analysis recommendations, Web table matching, etc. Schema matching is known as an uncertain process and a common method to overcome this uncertainty introduces a human expert with a ranked list of possible schema matches to choose from, known as top-\nK\nK matching . In this work we propose a learning algorithm that utilizes an innovative set of features to rerank a list of schema matches and improves upon the ranking of the best match. We provide a bound on the size of an initial match list, tying the number of matches with a desired level of confidence in finding the best match. We also propose the use of matching predictors as features in a learning task, and tailored nine new matching predictors for this purpose. The proposed algorithm assists the matching process by introducing a quality set of alternative matches to a human expert. It also serves as a step towards eliminating the involvement of human experts as decision makers in a matching process altogether. A large scale empirical evaluation with real-world benchmark shows the effectiveness of the proposed algorithmic solution.}
}


@article{DBLP:journals/tkde/FuSCH21,
	author = {Xiao Fu and
                  Eugene Seo and
                  Justin Clarke and
                  Rebecca A. Hutchinson},
	title = {Link Prediction Under Imperfect Detection: Collaborative Filtering
                  for Ecological Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3117--3128},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2962031},
	doi = {10.1109/TKDE.2019.2962031},
	timestamp = {Mon, 28 Aug 2023 21:37:41 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FuSCH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Matrix completion based collaborative filtering is considered scalable and effective for online service link prediction (e.g., movie recommendation) but does not meet the challenges of link prediction in ecological networks. A unique challenge of ecological networks is that the observed data are subject to systematic imperfect detection , due to the difficulty of accurate field sampling. In this work, we propose a new framework customized for ecological bipartite network link prediction. Our approach starts with incorporating the Poisson\nN\n-mixture model, a widely used framework in statistical ecology for modeling imperfect detection of a single species in field sampling. Despite its extensive use for single species analysis, this model has never been considered for link prediction between different species, perhaps because of the complex nature of both link prediction and\nN\n-mixture model inference. By judiciously combining the Poisson\nN\n-mixture model with a probabilistic nonnegative matrix factorization (NMF) model in latent space, we propose an intuitive statistical model for the problem of interest. We also offer a scalable and convergence-guaranteed optimization algorithm to handle the associated maximum likelihood identification problem. Experimental results on synthetic data and two real-world ecological networks data are employed to validate our proposed approach.}
}


@article{DBLP:journals/tkde/HanTXCFY21,
	author = {Bo Han and
                  Ivor W. Tsang and
                  Xiaokui Xiao and
                  Ling Chen and
                  Sai{-}Fu Fung and
                  Celina Ping Yu},
	title = {Privacy-Preserving Stochastic Gradual Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3129--3140},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2963977},
	doi = {10.1109/TKDE.2020.2963977},
	timestamp = {Thu, 14 Oct 2021 08:57:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HanTXCFY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is challenging for stochastic optimization to handle large-scale sensitive data safely. Duchi et al. recently proposed a private sampling strategy to solve privacy leakage in stochastic optimization. However, this strategy leads to a degeneration in robustness, since this strategy is equal to noise injection on each gradient, which adversely affects updates of the primal variable. To address this challenge, we introduce a robust stochastic optimization under the framework of local privacy, which is called Privacy-pREserving StochasTIc Gradual lEarning (PRESTIGE). PRESTIGE bridges private updates of the primal variable (by private sampling) with gradual curriculum learning (CL). The noise injection leads to similar issue from label noise, but the robust learning process of CL can combat with label noise. Thus, PRESTIGE yields “private but robust” updates of the primal variable on the curriculum, that is, a reordered label sequence provided by CL. In theory, we reveal the convergence rate and maximum complexity of PRESTIGE. Empirical results on six datasets show that PRESTIGE achieves a good tradeoff between privacy preservation and robustness over baselines.}
}


@article{DBLP:journals/tkde/CaoXXBY21,
	author = {Yang Cao and
                  Yonghui Xiao and
                  Li Xiong and
                  Liquan Bai and
                  Masatoshi Yoshikawa},
	title = {Protecting Spatiotemporal Event Privacy in Continuous Location-Based
                  Services},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3141--3154},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2019.2963312},
	doi = {10.1109/TKDE.2019.2963312},
	timestamp = {Thu, 16 Sep 2021 17:58:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CaoXXBY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Location privacy-preserving mechanisms (LPPMs) have been extensively studied for protecting users' location privacy by releasing a perturbed location to third parties such as location-based service providers. However, when a user's perturbed locations are released continuously, existing LPPMs may not protect the sensitive information about the user's real-world activities, such as “visited hospital in the last week” or “regularly commuting between location A and location B every weekday” (it is easy to infer that location A and location B may be home and office), which we call it spatiotemporal event. In this paper, we first formally define spatiotemporal event as Boolean expressions between location and time predicates, and then we define ε-spatiotemporal event privacy by extending the notion of differential privacy. Second, to understand how much spatiotemporal event privacy that existing LPPMs can provide, we design computationally efficient algorithms to quantify the spatiotemporal event privacy leakage of state-of-the-art LPPMs. It turns out that the existing LPPMs may not adequately protect spatiotemporal event privacy. Third, we propose a framework, PriSTE, to transform an existing LPPM into one protecting spatiotemporal event privacy by calibrating the LPPM's privacy budgets. Our experiments on real-life and synthetic data verified that the proposed method is effective and efficient.}
}


@article{DBLP:journals/tkde/ParkL21,
	author = {Saerom Park and
                  Jaewook Lee},
	title = {Stability Analysis of Denoising Autoencoders Based on Dynamical Projection
                  System},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {8},
	pages = {3155--3159},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.3010277},
	doi = {10.1109/TKDE.2020.3010277},
	timestamp = {Thu, 16 Sep 2021 17:58:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ParkL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this study, we give a stability analysis of denoising autoencoder(DAE) from the novel perspective of dynamical systems when the input density is defined as a distribution on a manifold. We demonstrate the connection between the corrupted distribution and the learned reconstruction function of a nonlinear DAE, which motivates the use of a dynamic projection system (DPS) associated with the learned reconstruction function. Utilizing the constructed DPS, we prove that the high-density region of the corrupted data distribution asymptotically converges to the data manifold. Then, we show that the region is the attracting stable equilibrium manifold of the DPS which is completely stable. These results serve a theoretical basis of the DAE in recognizing the high-density region of the highly corrupted data with large deviations through the DPS. The effectiveness of this analysis is verified by conducting experiments on several toy examples and real image datasets with various types of noise.}
}


@article{DBLP:journals/tkde/ZhaoLCQ21,
	author = {Guoshuai Zhao and
                  Zhidan Liu and
                  Yulu Chao and
                  Xueming Qian},
	title = {{CAPER:} Context-Aware Personalized Emoji Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3160--3172},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2966971},
	doi = {10.1109/TKDE.2020.2966971},
	timestamp = {Wed, 25 Aug 2021 16:08:51 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoLCQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularity of social platforms, emoji appears and becomes extremely popular with a large number of users. It expresses more beyond plaintexts and makes the content more vivid. Using appropriate emojis in messages and microblog posts makes you lovely and friendly. Recently, emoji recommendation becomes a significant task since it is hard to choose the appropriate one from thousands of emoji candidates. In this paper, we propose a Context-Aware Personalized Emoji Recommendation (CAPER) model fusing the contextual information and the personal information. It is to learn latent factors of contextual and personal information through a score-ranking matrix factorization framework. The personal factors such as user preference, user gender, and the current time can make the recommended emojis meet users' individual needs. Moreover, we consider the co-occurrence factors of the emojis which could improve the recommendation accuracy. We conduct a series of experiments on the real-world datasets, and experiment results show better performance of our model than existing methods, demonstrating the effectiveness of the considering contextual and personal factors.}
}


@article{DBLP:journals/tkde/Balasubramaniam21,
	author = {Thirunavukarasu Balasubramaniam and
                  Richi Nayak and
                  Chau Yuen and
                  Yu{-}Chu Tian},
	title = {Column-Wise Element Selection for Computationally Efficient Nonnegative
                  Coupled Matrix Tensor Factorization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3173--3186},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2967045},
	doi = {10.1109/TKDE.2020.2967045},
	timestamp = {Mon, 28 Aug 2023 21:37:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/Balasubramaniam21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Coupled Matrix Tensor Factorization (CMTF) facilitates the integration and analysis of multiple data sources and helps discover meaningful information. Nonnegative CMTF (N-CMTF) has been employed in many applications for identifying latent patterns, prediction, and recommendation. However, due to the added complexity with coupling between tensor and matrix data, existing N-CMTF algorithms exhibit poor computation efficiency. In this paper, a computationally efficient N-CMTF factorization algorithm is presented based on the column-wise element selection, preventing frequent gradient updates. Theoretical and empirical analyses show that the proposed N-CMTF factorization algorithm is not only more accurate but also more computationally efficient than existing algorithms in approximating the tensor as well as in identifying the underlying nature of factors.}
}


@article{DBLP:journals/tkde/PeregoPV21,
	author = {Raffaele Perego and
                  Giulio Ermanno Pibiri and
                  Rossano Venturini},
	title = {Compressed Indexes for Fast Search of Semantic Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3187--3198},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2966609},
	doi = {10.1109/TKDE.2020.2966609},
	timestamp = {Thu, 12 Aug 2021 17:50:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PeregoPV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The sheer increase in volume of RDF data demands efficient solutions for the triple indexing problem, that is to devise a compressed data structure to compactly represent RDF triples by guaranteeing, at the same time, fast pattern matching operations. This problem lies at the heart of delivering good practical performance for the resolution of complex SPARQL queries on large RDF datasets. In this work, we propose a trie-based index layout to solve the problem and introduce two novel techniques to reduce its space of representation for improved effectiveness. The extensive experimental analysis, conducted over a wide range of publicly available real-world datasets, reveals that our best space/time trade-off configuration substantially outperforms existing solutions at the state-of-the-art, by taking 30-60 percent less space and speeding up query execution by a factor of 2 - 81×.}
}


@article{DBLP:journals/tkde/NunesLCCBJ21,
	author = {Cec{\'{\i}}lia Nunes and
                  H{\'{e}}l{\`{e}}ne Langet and
                  Mathieu De Craene and
                  Oscar Camara and
                  Bart H. Bijnens and
                  Anders Jonsson},
	title = {Decision Tree Learning for Uncertain Clinical Measurements},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3199--3211},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2967378},
	doi = {10.1109/TKDE.2020.2967378},
	timestamp = {Wed, 24 Jan 2024 16:32:16 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NunesLCCBJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clinical decision requires reasoning in the presence of imperfect data. DTs are a well-known decision support tool, owing to their interpretability, fundamental in safety-critical contexts such as medical diagnosis. However, learning DTs from uncertain data leads to poor generalization, and generating predictions for uncertain data hinders prediction accuracy. Several methods have suggested the potential of probabilistic decisions at the internal nodes in making DTs robust to uncertainty. Some approaches only employ probabilistic thresholds during evaluation. Others also consider the uncertainty in the learning phase, at the expense of increased computational complexity or reduced interpretability. The existing methods have not clarified the merit of a probabilistic approach in the distinct phases of DT learning, nor when the uncertainty is present in the training or the test data. We present a probabilistic DT approach that models measurement uncertainty as a noise distribution, independently realized: (1) when searching for the split thresholds, (2) when splitting the training instances, and (3) when generating predictions for unseen data. The soft training approaches (1, 2) achieved a regularizing effect, leading to significant reductions in DT size, while maintaining accuracy, for increased noise. Soft evaluation (3) showed no benefit in handling noise.}
}


@article{DBLP:journals/tkde/CaruccioDNP21,
	author = {Loredana Caruccio and
                  Vincenzo Deufemia and
                  Felix Naumann and
                  Giuseppe Polese},
	title = {Discovering Relaxed Functional Dependencies Based on Multi-Attribute
                  Dominance},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3212--3228},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2967722},
	doi = {10.1109/TKDE.2020.2967722},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CaruccioDNP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advent of big data and data lakes, data are often integrated from multiple sources. Such integrated data are often of poor quality, due to inconsistencies, errors, and so forth. One way to check the quality of data is to infer functional dependencies ( fd s). However, in many modern applications it might be necessary to extract properties and relationships that are not captured through fd s, due to the necessity to admit exceptions, or to consider similarity rather than equality of data values. Relaxed fd s ( rfd s) have been introduced to meet these needs, but their discovery from data adds further complexity to an already complex problem, also due to the necessity of specifying similarity and validity thresholds. We propose Domino , a new discovery algorithm for rfd s that exploits the concept of dominance in order to derive similarity thresholds of attribute values while inferring rfd s. An experimental evaluation on real datasets demonstrates the discovery performance and the effectiveness of the proposed algorithm.}
}


@article{DBLP:journals/tkde/ShaoCTLY21,
	author = {Zhou Shao and
                  Muhammad Aamir Cheema and
                  David Taniar and
                  Hua Lu and
                  Shiyu Yang},
	title = {Efficiently Processing Spatial and Keyword Queries in Indoor Venues},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3229--3244},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2964206},
	doi = {10.1109/TKDE.2020.2964206},
	timestamp = {Thu, 23 Jun 2022 20:04:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShaoCTLY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the growing popularity of indoor location-based services, indoor data management has received significant research attention in the past few years. However, we observe that the existing indexing and query processing techniques for the indoor space do not fully exploit the properties of the indoor space. Consequently, they provide below par performance which makes them unsuitable for large indoor venues with high query workloads. In this paper, we first propose two novel indexes called Indoor Partitioning Tree (IP-Tree) and Vivid IP-Tree (VIP-Tree) that are carefully designed by utilizing the properties of indoor venues. The proposed indexes are lightweight, have small pre-processing cost and provide near-optimal performance for shortest distance and shortest path queries. We are also the first to study spatial keyword queries in indoor venues. We propose a novel data structure called Keyword Partitioning Tree (KP-Tree) that indexes objects in an indoor partition. We propose an efficient algorithm based on VIP-Tree and KP-Trees to efficiently answer spatial keyword queries. Our extensive experimental study on real and synthetic data sets demonstrates that our proposed indexes outperform the existing solutions by several orders of magnitude.}
}


@article{DBLP:journals/tkde/HeNWHJL21,
	author = {Fang He and
                  Feiping Nie and
                  Rong Wang and
                  Haojie Hu and
                  Weimin Jia and
                  Xuelong Li},
	title = {Fast Semi-Supervised Learning With Optimal Bipartite Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3245--3257},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2968523},
	doi = {10.1109/TKDE.2020.2968523},
	timestamp = {Mon, 14 Feb 2022 16:41:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HeNWHJL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, with the explosive increase in Internet data, the traditional Graph-based Semi-Supervised Learning (GSSL) model is not suitable to deal with large scale data as the high computation complexity. Besides, GSSL models perform classification on a fixed input data graph. The quality of initialized graph has a great effect on the classification result. To solve this problem, in this paper, we propose a novel approach, named optimal bipartite graph-based SSL (OBGSSL). Instead of fixing the input data graph, we learn a new bipartite graph to make the result more robust. Based on the learned bipartite graph, the labels of the original data and anchors can be calculated simultaneously, which solves co-classification problem in SSL. Then, we use the label of anchor to handle out-of-sample problem, which preserves well classification performance and saves much time. The computational complexity of OBGSSL is O(ndmt+nm 2 ), which is a significant improvement compared with traditional GSSL methods that need O(n 2 d+n 3 ), where n, d, m and t are the number of samples, features anchors and iterations, respectively. Experimental results demonstrate the effectiveness and efficiency of our OBGSSL model.}
}


@article{DBLP:journals/tkde/BarlacchiLM21,
	author = {Gianni Barlacchi and
                  Bruno Lepri and
                  Alessandro Moschitti},
	title = {Land Use Classification With Point of Interests and Structural Patterns},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3258--3269},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2967381},
	doi = {10.1109/TKDE.2020.2967381},
	timestamp = {Thu, 12 Aug 2021 17:50:00 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BarlacchiLM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present a framework for performing automatic analysis of Land Use Zones based on Location-Based Social Networks (LBSNs). We model city areas using a hierarchical structure of POIs extracted from foursquare. We encode such structures in kernel machines, e.g., Support Vector Machines, using a new Tree Kernel, i.e., the Hierarchical POI Kernel (HPK), which can take the importance of the individual POIs into account during the substructure matching. This way, HPK projects structures in the space of all their possible substructures such that each dimension corresponds to a semantic structural feature, weighted according to the discriminative power of POIs . We generated four different datasets for the following cities: Barcelona, Lisbon, Amsterdam and Milan, where we trained and tested our models. The results show that our approach largely outperforms previous work and standard baseline built on simple features, such as counts of different POIs. Finally, we apply a mining algorithm to extract the most relevant features (tree fragments) from the implicit TK space according to the weights the kernel machine assigned to them. Our approach can produce an explicit set of representative features that can be used to classify and characterize urban areas.}
}


@article{DBLP:journals/tkde/ShahamDLDLL21,
	author = {Sina Shaham and
                  Ming Ding and
                  Bo Liu and
                  Shuping Dang and
                  Zihuai Lin and
                  Jun Li},
	title = {Privacy Preserving Location Data Publishing: {A} Machine Learning
                  Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3270--3283},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2964658},
	doi = {10.1109/TKDE.2020.2964658},
	timestamp = {Thu, 26 Aug 2021 15:08:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShahamDLDLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Publishing datasets plays an essential role in open data research and promoting transparency of government agencies. However, such data publication might reveal users’ private information. One of the most sensitive sources of data is spatiotemporal trajectory datasets. Unfortunately, merely removing unique identifiers cannot preserve the privacy of users. Adversaries may know parts of the trajectories or be able to link the published dataset to other sources for the purpose of user identification. Therefore, it is crucial to apply privacy preserving techniques before the publication of spatiotemporal trajectory datasets. In this paper, we propose a robust framework for the anonymization of spatiotemporal trajectory datasets termed as machine learning based anonymization (MLA). By introducing a new formulation of the problem, we are able to apply machine learning algorithms for clustering the trajectories and propose to use\nk\n-means algorithm for this purpose. A variation of\nk\n-means algorithm is also proposed to preserve the privacy in overly sensitive datasets. Moreover, we improve the alignment process by considering multiple sequence alignment as part of the MLA. The framework and all the proposed algorithms are applied to T-Drive, Geolife, and Gowalla location datasets. The experimental results indicate a significantly higher utility of datasets by anonymization based on MLA framework.}
}


@article{DBLP:journals/tkde/LianXCX21,
	author = {Defu Lian and
                  Xing Xie and
                  Enhong Chen and
                  Hui Xiong},
	title = {Product Quantized Collaborative Filtering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3284--3296},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2964232},
	doi = {10.1109/TKDE.2020.2964232},
	timestamp = {Thu, 12 Aug 2021 17:50:00 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LianXCX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Because of strict response-time constraints, efficiency of top-k recommendation is crucial for real-world recommender systems. Locality sensitive hashing and index-based methods usually store both index data and item feature vectors in main memory, so they handle a limited number of items. Hashing-based recommendation methods enjoy low memory cost and fast retrieval of items, but suffer from large accuracy degradation. In this paper, we propose product Quantized Collaborative Filtering (pQCF) for better trade-off between efficiency and accuracy. pQCF decomposes a joint latent space of users and items into a Cartesian product of low-dimensional subspaces, and learns clustered representation within each subspace. A latent factor is then represented by a short code, which is composed of subspace cluster indexes. A user's preference for an item can be efficiently calculated via table lookup. We then develop block coordinate descent for efficient optimization and reveal the learning of latent factors is seamlessly integrated with quantization. We further investigate an asymmetric pQCF, dubbed as QCF, where user latent factors are not quantized and shared across different subspaces. The extensive experiments with 6 real-world datasets show that pQCF significantly outperforms the state-of-the-art hashing-based CF and QCF increases recommendation accuracy compared to pQCF.}
}


@article{DBLP:journals/tkde/ZhangLLJZ21,
	author = {Xinsong Zhang and
                  Tianyi Liu and
                  Pengshuai Li and
                  Weijia Jia and
                  Hai Zhao},
	title = {Robust Neural Relation Extraction via Multi-Granularity Noises Reduction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3297--3310},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2964747},
	doi = {10.1109/TKDE.2020.2964747},
	timestamp = {Thu, 21 Mar 2024 16:25:29 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLLJZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distant supervision is widely used to extract relational facts with automatically labeled datasets to reduce high cost of human annotation. However, current distantly supervised methods suffer from the common problems of word-level and sentence-level noises, which come from a large proportion of irrelevant words in a sentence and inaccurate relation labels for numerous sentences. The problems lead to unacceptable precision in relation extraction and are critical for the success of using distant supervision. In this paper, we propose a novel and robust neural approach to deal with both problems by reducing influences of the multi-granularity noises. Three levels of noises from word, sentence until knowledge type are carefully considered in this work. We first initiate a question-answering based relation extractor (QARE) to remove noisy words in a sentence. Then we use multi-focus multi-instance learning (MMIL) to alleviate the effects of sentence-level noise by utilizing wrongly labeled sentences properly. Finally, to enhance our method against all the noises, we initialize parameters in our method with a priori knowledge learned from the relevant task of entity type classification by transfer learning. Extensive experiments on both existing benchmark and an improved larger dataset demonstrate that our proposed approach remarkably achieves new state-of-the-art performance.}
}


@article{DBLP:journals/tkde/WangSR21,
	author = {Ping Wang and
                  Tian Shi and
                  Chandan K. Reddy},
	title = {A Novel Tensor-Based Temporal Multi-Task Survival Analysis Model},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {9},
	pages = {3311--3322},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2967700},
	doi = {10.1109/TKDE.2020.2967700},
	timestamp = {Tue, 04 Jan 2022 17:17:29 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangSR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Survival analysis aims at predicting the time to event of interest along with its probability on longitudinal data. It is commonly used to make predictions for a single specific event of interest at a given time point. However, predicting the occurrence of multiple events of interest simultaneously and dynamically is needed in many real-world applications. An intuitive way to solve this problem is to simply apply the standard survival analysis method independently to each prediction task at each time point. However, it often leads to a sub-optimal solution since the underlying dependencies between these tasks are ignored. This motivates us to analyze these prediction tasks jointly in order to select the common features shared across all the tasks. In this paper, we formulate a temporal (Multiple Time points) Multi-Task learning framework (MTMT) for survival analysis problems using tensor representation. More specifically, given a survival dataset and a sequence of time points, which are considered as the monitored time points for the events of interest, we reformulate the survival analysis problem to jointly handle each task at each time point and optimize them simultaneously. We demonstrate the performance of the proposed MTMT model on important real-world datasets, including employee attrition and medical records. We show the superior performance of the MTMT model compared to several state-of-the-art models using standard metrics. We also provide the list of important features selected by our MTMT model thus demonstrating the interpretability of the proposed model.}
}


@article{DBLP:journals/tkde/LiTWC21,
	author = {Qing Li and
                  Jinghua Tan and
                  Jun Wang and
                  Hsinchun Chen},
	title = {A Multimodal Event-Driven {LSTM} Model for Stock Prediction Using
                  Online News},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3323--3337},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2968894},
	doi = {10.1109/TKDE.2020.2968894},
	timestamp = {Tue, 05 Oct 2021 10:00:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiTWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In finance, it is believed that market information, namely, fundamentals and news information, affects stock movements. Such media-aware stock movements essentially comprise a multimodal problem. Two unique challenges arise in processing these multimodal data. First, information from one data mode will interact with information from other data modes. A common strategy is to concatenate various data modes into one compound vector; however, this strategy ignores the interactions among different modes. The second challenge is the heterogeneity of the data in terms of sampling time. Specifically, fundamental data consist of continuous values sampled at fixed time intervals, whereas news information emerges randomly. This heterogeneity can cause valuable information to be partially missing or can distort the feature spaces. In addition, the study of media-aware stock movements in previous work has focused on the one-to-one problem, in which it is assumed that news affects only the performance of the stocks mentioned in the reports. However, news articles also impact related stocks and cause stock co-movements. In this article, we propose a tensor-based event-driven LSTM model to address these challenges. Experiments performed on the China securities market demonstrate the superiority of the proposed approach over state-of-the-art algorithms, including AZFinText, eMAQT, and TeSIA.}
}


@article{DBLP:journals/tkde/WanSZ21,
	author = {Yuan Wan and
                  Shengzi Sun and
                  Cheng Zeng},
	title = {Adaptive Similarity Embedding for Unsupervised Multi-View Feature
                  Selection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3338--3350},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2969860},
	doi = {10.1109/TKDE.2020.2969860},
	timestamp = {Tue, 05 Oct 2021 10:00:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WanSZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view learning has become a significant research topic in image processing, data mining and machine learning due to the proliferation of multi-view data. Considering the difficulty in obtaining labeled data in many real applications, we focus on the multi-view unsupervised feature selection problem. Most existing multi-view feature selection introduce an identical similarity matrix among different views, which cannot preserve the specific correlation between each single view. Also, some of these methods just consider either global or local structures. In this paper, we propose an embedding method, Adaptive Similarity Embedding for Unsupervised Multi-View Feature Selection (ASE-UMFS). This method reduces the high-dimensional data to the low dimensions and unifies different views to a combination weight matrix. We also use parameters to constraint the similarity matrix for the local structure, where the regularization term is used to add a prior of uniform distribution; taking into account of the independence in projection matrix among different views, optimization of the similarity matrix is further improved. To confirm the effectiveness of ASE-UMFS, comparisons are made with benchmark algorithm on real-world data sets. The experimental results demonstrate that the proposed algorithm outperforms several state-of-the-art methods in multi-view learning.}
}


@article{DBLP:journals/tkde/ShenLYXHSH21,
	author = {Heng Tao Shen and
                  Luchen Liu and
                  Yang Yang and
                  Xing Xu and
                  Zi Huang and
                  Fumin Shen and
                  Richang Hong},
	title = {Exploiting Subspace Relation in Semantic Labels for Cross-Modal Hashing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3351--3365},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2970050},
	doi = {10.1109/TKDE.2020.2970050},
	timestamp = {Tue, 05 Oct 2021 10:00:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShenLYXHSH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hashing methods have been extensively applied to efficient multimedia data indexing and retrieval on account of the explosion of multimedia data. Cross-modal hashing usually learns binary codes by mapping multi-modal data into a common Hamming space. Most supervised methods utilize relation information like class labels as pairwise similarities of cross-modal data pair to narrow intra-modal and inter-modal gap. In this paper, we propose a novel supervised cross-modal hashing method dubbed Subspace Relation Learning for Cross-modal Hashing (SRLCH), which exploits relation information of labels in semantic space to make similar data from different modalities closer in the low-dimension Hamming subspace. SRLCH preserves the modality relationships, the discrete constraints and nonlinear structures, while admitting a closed-form binary codes solution, which effectively enhances the training efficiency. An iterative alternative optimization algorithm is developed to simultaneously learn both hash functions and unified binary codes. With these binary codes and hash functions, we can index multimedia data and search them in an efficient way. Evaluations in two cross-modal retrieval tasks on several widely-used datasets show that the proposed SRLCH outperforms most cross-modal hashing methods. Theoretical analysis also illustrates reasons for our method’s promotion in subspace relation learning.}
}


@article{DBLP:journals/tkde/WangKCXFT21,
	author = {Meijia Wang and
                  Jian Kang and
                  Nan Cao and
                  Yinglong Xia and
                  Wei Fan and
                  Hanghang Tong},
	title = {Graph Ranking Auditing: Problem Definition and Fast Solutions},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3366--3380},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2969415},
	doi = {10.1109/TKDE.2020.2969415},
	timestamp = {Sat, 16 Dec 2023 19:13:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangKCXFT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ranking on graphs is a centerpiece in many high-impact application domains, such as information retrieval, recommender systems, team management, neuroscience and many more. PageRank, along with many of its variants, is widely used across these application domains thanks to its mathematical elegance and the superior performance. Although PageRank and its variants are effective in ranking nodes on graphs, they often lack an efficient and effective way to audit the ranking results in terms of the input graph structure, e.g., which node or edge in the graph contributes most to the top-1 ranked node; which subgraph plays a crucial role in generating the overall ranking result? In this paper, we propose to audit graph ranking by finding the influential graph elements (e.g., edges, nodes, attributes, and subgraphs) regarding their impact on the ranking results. First, we formulate graph ranking auditing problem as quantifying the influence of graph elements on the ranking results. Second, we show that our formulation can be applied to a variety of graph structures. Third, we propose effective and efficient algorithms to find the top-k influential edges/nodes/subgraph. Finally, we perform extensive empirical evaluations on real-world datasets to demonstrate that the proposed methods (Aurora) provide intuitive auditing results with linear scalability.}
}


@article{DBLP:journals/tkde/LiuZXG21,
	author = {Qing Liu and
                  Ziyuan Zhu and
                  Jianliang Xu and
                  Yunjun Gao},
	title = {MaxiZone: Maximizing Influence Zone Over Geo-Textual Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3381--3393},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2968908},
	doi = {10.1109/TKDE.2020.2968908},
	timestamp = {Tue, 14 Mar 2023 18:13:01 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuZXG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a geo-textual dataset O, a set φ of keywords, a query object q ∈ O keyword-based location query returns the influence zone R of q such that q belongs to the result of a top-k spatial keyword query with query keywords φ and any location in R as arguments. For a query object q, the influence zone of q varies for different keywords φ. Users may be interested in identifying the maximum influence zone of the query object. To this end, in this paper, we study the problem called MaxiZone that finds the keyword set maximizing the influence zone of a specified query object. The MaxiZone problem has many real-life applications, e.g., a business owner would like to identify the maximum influence zone so as to attract as many customers as possible. A straightforward way to tackle the MaxiZone problem is to compute the influence zone for every candidate keyword set. Obviously, this is infeasible if there are a large number of candidate keyword sets. We propose a more efficient index-centric algorithm together with a series of optimizations as well as a sampling-based algorithm, to facilitate the query processing. Moreover, we extend the proposed algorithms to address a variant of MaxiZone problem called τ-MaxiZone problem, which finds top-τ keyword sets having the maximum influence zones. Extensive empirical study using real-world datasets demonstrates the effectiveness and efficiency of our proposed algorithms.}
}


@article{DBLP:journals/tkde/DavoudiRAZE21,
	author = {Heidar Davoudi and
                  Zana Rashidi and
                  Aijun An and
                  Morteza Zihayat and
                  Gordon Edall},
	title = {Paywall Policy Learning in Digital News Media},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3394--3409},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2969419},
	doi = {10.1109/TKDE.2020.2969419},
	timestamp = {Tue, 05 Oct 2021 10:00:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DavoudiRAZE21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subscription-based online newspapers usually offer non-subscribed users a certain number of free articles in a period of time, and then directs them to a page (called paywall) asking for subscription. This approach (also known as metered or fixed paywall) does not consider the user's reading history nor the articles that the user may read in the future, and consequently, it may disengage many potential subscribers. To that end, we propose adaptive paywall mechanisms to make optimal paywall decisions (i.e., showing the article or the paywall) by balancing the benefit of showing the article against that of presenting the paywall. We define the notions of utility and cost which are used to define an objective function for the optimal paywall decision problem. We propose the Lookahead policy (LAP) and QPaywall policy (QP) as two data-driven approaches to solve the adaptive paywall problem. While the LAP method makes paywall decisions on the fly by simulating trajectories of article requests using Monte Carlo sampling, the QP approach is based on reinforcement learning and learns a neural network-based action-value (Q) function for this purpose. We compare advantages of the proposed approaches and discuss the practical considerations of using them in a real environment. Empirical studies on a real dataset from a major newspaper in Canada show that the proposed methods outperform several baseline approaches in terms of various business objectives.}
}


@article{DBLP:journals/tkde/GaoHLWC21,
	author = {Xiaofeng Gao and
                  Haowei Huang and
                  Chenlin Liu and
                  Fan Wu and
                  Guihai Chen},
	title = {Quality Inference Based Task Assignment in Mobile Crowdsensing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3410--3423},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2965932},
	doi = {10.1109/TKDE.2020.2965932},
	timestamp = {Mon, 13 Jun 2022 14:32:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GaoHLWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increase of mobile devices, Mobile Crowdsensing (MCS) has become an efficient way to ubiquitously sense and collect environment data. Comparing to traditional sensor networks, MCS has a vital advantage that workers play an active role in collecting and sensing data. However, due to the openness of MCS, workers and sensors are of different qualities. Low quality sensors and workers may yield noisy data or even inaccurate data. Which gives the importance of inferring the quality of workers and sensors and seeking a valid task assignment with enough total qualities for MCS. To solve the problem, we adopt truth inference methods to iteratively infer the truth and qualities. Based on the quality inference, this paper proposes a task assignment problem called quality-bounded task assignment with redundancy constraint (QTAR). Different from traditional task assignment problem, redundancy constraint is added to satisfy the preliminaries of truth inference, which requires that each task should be assigned a certain or more amount of workers. We prove that QTAR is NP-complete and propose a (2+ε) - approximation algorithm for QTAR, called QTA. Finally, experiments are conducted on both synthesis data and real dataset. The results of the experiments prove the efficiency and effectiveness of our algorithms.}
}


@article{DBLP:journals/tkde/WuLWHX21,
	author = {Wei Wu and
                  Jian Liu and
                  Huimei Wang and
                  Jialu Hao and
                  Ming Xian},
	title = {Secure and Efficient Outsourced k-Means Clustering using Fully Homomorphic
                  Encryption With Ciphertext Packing Technique},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3424--3437},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2969633},
	doi = {10.1109/TKDE.2020.2969633},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuLWHX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, more individuals and corporations tend to use machine learning as a service (MLaaS) in cloud computing environment. However, when enjoying the pay-as-you-go mode and flexible capacity of cloud computing, it also increases the risk of privacy leakage for sensitive data. In this paper, we aim to efficiently implement privacy-preserving MLaaS, and focus on k-means clustering over outsourced encrypted cloud databases. Previous works mainly utilize partially homomorphic encryptions, which require a great number of interactive protocols with high computation and communication costs, making them not practical in real-world applications. To better solve this problem, we propose a new secure and efficient outsourced k-means clustering (SEOKC) scheme using fully homomorphic encryption with ciphertext packing technique, which achieves parallel computation without extra cost. The proposed scheme preserves privacy in three aspects: (1) database security, (2) privacy of clustering results and (3) hiding of data access patterns. We provide formal security analysis and evaluate the performance of the proposed scheme through extensive experiments. The experiment results show that our scheme needs much less computation cost (more than three orders of magnitude lower) than the state-of-the-art schemes, and is suitable to be applied on large databases.}
}


@article{DBLP:journals/tkde/QiWZZWTLG21,
	author = {Yiyan Qi and
                  Pinghui Wang and
                  Yuanming Zhang and
                  Qiaozhu Zhai and
                  Chenxu Wang and
                  Guangjian Tian and
                  John C. S. Lui and
                  Xiaohong Guan},
	title = {Streaming Algorithms for Estimating High Set Similarities in LogLog
                  Space},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3438--3452},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2969423},
	doi = {10.1109/TKDE.2020.2969423},
	timestamp = {Tue, 05 Oct 2021 10:00:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QiWZZWTLG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating set similarity and detecting highly similar sets are fundamental problems in areas such as databases and machine learning. MinHash is a well-known technique for approximating Jaccard similarity of sets and has been successfully used for many applications. Its two compressed versions, b-bit MinHash and Odd Sketch, can significantly reduce the memory usage of the MinHash, especially for estimating high similarities (i.e., similarities around 1). Although MinHash can be applied to static sets as well as streaming sets, of which elements are given in a streaming fashion, unfortunately, b-bit MinHash and Odd Sketch fail to deal with streaming data. To solve this problem, we previously designed a memory-efficient sketch method, MaxLogHash, to accurately estimate Jaccard similarities in streaming sets. Compared with MinHash, our method uses smaller sized registers (each register consists of less than 7 bits) to build a compact sketch for each set. In this paper, we further develop a faster method, MaxLogOPH++. Compared with MaxLogHash, MaxLogOPH++ reduces the time complexity for updating each coming element from O(k) with a small additional memory. We conduct experiments on a variety of datasets, and experimental results demonstrate the efficiency and effectiveness of our methods.}
}


@article{DBLP:journals/tkde/GaoZQLC21,
	author = {Yunjun Gao and
                  Tianming Zhang and
                  Linshan Qiu and
                  Qingyuan Linghu and
                  Gang Chen},
	title = {Time-Respecting Flow Graph Pattern Matching on Temporal Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3453--3467},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2968901},
	doi = {10.1109/TKDE.2020.2968901},
	timestamp = {Tue, 19 Apr 2022 14:59:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GaoZQLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph pattern matching has been extensively investigated on general graphs without time information over decades. Nevertheless, few studies focus on temporal graphs, where a relationship between two vertices takes place at a specific moment and lingers for some time. In this paper, we propose a new notion so-called time-respecting flow graph , in which all paths are time-respecting (i.e., a sequence of contacts with non-decreasing time), and one vertex is distinguished as the root, from which other vertices can be reached via a time-respecting path. Based on this, we explore the problem of time-respecting flow graph pattern matching on temporal graphs . This problem motivates important applications in epidemiology, information diffusion, crime detection, etc. To address it, we present one baseline algorithm as well as two optimized algorithms that utilize several efficient matching strategies and topological sort based technique to boost efficiency. Extensive experimental evaluation using both real and synthetic data sets demonstrates the effectiveness and efficiency of our proposed algorithms. Compared with baseline method, our optimized algorithms could achieve up to three orders of magnitude speedup.}
}


@article{DBLP:journals/tkde/XiaLYXLJ21,
	author = {Tong Xia and
                  Yong Li and
                  Yue Yu and
                  Fengli Xu and
                  Qingmin Liao and
                  Depeng Jin},
	title = {Understanding Urban Dynamics via State-Sharing Hidden Markov Model},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {10},
	pages = {3468--3481},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2968432},
	doi = {10.1109/TKDE.2020.2968432},
	timestamp = {Mon, 28 Aug 2023 21:37:39 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XiaLYXLJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the ever-increasing urbanization process, systematically modeling people's activities in the urban space is being recognized as a crucial socioeconomic task. It is extremely challenging due to the lack of reliable data and suitable methods, yet the emergence of population-scale urban mobility data sheds new light on it. However, recent works on discovering activity patterns from urban mobility data are still limited in terms of concisely and specifically modeling the temporal dynamics of people's urban activities. To bridge the gap, we present a State-sharing Hidden Markov Model (SSHMM), a novel time-series modeling method that uncovers urban dynamics with massive urban mobility data. SSHMM models the urban dynamics from two aspects. First, it extracts the urban states from the whole city, which captures the volume of population flows as well as the frequency of each type of Point of Interests (PoIs) visited. Second, it characterizes the urban dynamics of each urban region as the state transition on the shared-states, which reveals distinct daily rhythms of urban activities. We evaluate our method via large-scale real-life mobility dataset. The results demonstrate that SSHMM learns semantics-rich urban dynamics, which are highly correlated with the functions of the region. Besides, it recovers the urban dynamics in different time slots with RMSE of 0.0793 when only learn limited states for the whole city, which outperforms the general HMM by 54.2 percent.}
}


@article{DBLP:journals/tkde/PanagiotouATKG21,
	author = {Nikolaos Panagiotou and
                  Cem Akkaya and
                  Kostas Tsioutsiouliklis and
                  Vana Kalogeraki and
                  Dimitrios Gunopulos},
	title = {A General Framework for First Story Detection Utilizing Entities and
                  Their Relations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3482--3493},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2970051},
	doi = {10.1109/TKDE.2020.2970051},
	timestamp = {Wed, 03 Nov 2021 08:25:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/PanagiotouATKG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {News portals, such as Yahoo News or Google News, collect large amounts of news articles from a variety of sources on a daily basis. Only a small portion of these documents can be selected and displayed on the homepage. Thus, there is a strong preference for major, recent events. In this work, we propose a scalable First Story Detection (FSD) pipeline that identifies fresh news. This pipeline is used in order to instantiate a variety of FSD approaches. In addition we suggest a novel FSD technique that in comparison to existing systems, relies on relation extraction algorithms and exploits the named entities and their relations in order to decide about the freshness of an article. We evaluate our technique by instantiating existing state of art FSD techniques within our generic pipeline. As ground truth we use multiple datasets that cover different categories. Experimental results demonstrate that our FSD method in many cases provides an improvement over state-of-the-art techniques. In addition, we show using a large synthetic dataset that our general FSD pipeline has constant space and time requirements and is suitable for very high volume streams.}
}


@article{DBLP:journals/tkde/SunLCGXX21,
	author = {Leilei Sun and
                  Chuanren Liu and
                  Guoqing Chen and
                  Chonghui Guo and
                  Hui Xiong and
                  Yanming Xie},
	title = {Automatic Treatment Regimen Design},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3494--3506},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2972276},
	doi = {10.1109/TKDE.2020.2972276},
	timestamp = {Wed, 03 Nov 2021 08:25:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunLCGXX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a data-driven healthcare service, automatic treatment regimen design has great potential to improve healthcare efficiency and quality. However, it is a nontrivial endeavor to develop such a healthcare service due to two major challenges: 1) the treatment records are complex data objects consisting of various semantic and temporal information, and 2) the treatment outcome usually depends on a large number of internal and external factors. Because of these difficulties, automatic treatment regimen design is still an open research problem nowadays. To fill this research gap, this paper first formulates a treatment sequence as temporal sets, then provides a novel Extended Jaccard Similarity (EJS) measure to quantify the similarities between treatment sequences. We show that the proposed EJS is a general and effective measure to capture the similarity between two complex temporal sets. Further, we develop an efficient clustering algorithm which can achieve reasonable clustering results with only a portion of the pairwise similarities between treatment sequences and then extract a semantic prototype of the treatment regimen from each cluster of treatment sequences. Finally, we adopt a matrix factorization framework to predict the treatment outcomes by integrating multiple internal and external factors. We conduct comprehensive experiments on Electronic Medical Records (EMRs) of more than 28,000 patients from 14 hospitals. The results demonstrate the effectiveness of our approach and its superiority over the state-of-the-art ones.}
}


@article{DBLP:journals/tkde/WangLNSZX21,
	author = {Yongxin Wang and
                  Xin Luo and
                  Liqiang Nie and
                  Jingkuan Song and
                  Wei Zhang and
                  Xin{-}Shun Xu},
	title = {{BATCH:} {A} Scalable Asymmetric Discrete Cross-Modal Hashing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3507--3519},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2974825},
	doi = {10.1109/TKDE.2020.2974825},
	timestamp = {Wed, 10 Nov 2021 14:30:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangLNSZX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Supervised cross-modal hashing has attracted much attention. However, there are still some challenges, e.g., how to effectively embed the label information into binary codes, how to avoid using a large similarity matrix and make a model scalable to large-scale datasets, how to efficiently solve the binary optimization problem. To address these challenges, in this paper, we present a novel supervised cross-modal hashing method, i.e., scalaBle Asymmetric discreTe Cross-modal Hashing, BATCH for short. It leverages collective matrix factorization to learn a common latent space for the labels and different modalities, and embeds the labels into binary codes by minimizing a distance-distance difference problem. Furthermore, it builds a connection between the common latent space and the hash codes by an asymmetric strategy. In the light of this, it can perform cross-modal retrieval and embed more similarity information into the binary codes. In addition, it introduces a quantization minimization term and orthogonal constraints into the optimization problem, and generates the binary codes discretely. Therefore, the quantization error and redundancy may be much reduced. Moreover, it is a two-step method, making the optimization simple and scalable to large-scale datasets. Extensive experimental results on three benchmark datasets demonstrate that BATCH outperforms some state-of-the-art cross-modal hashing methods in terms of accuracy and efficiency.}
}


@article{DBLP:journals/tkde/GuoYWCLW21,
	author = {Fangda Guo and
                  Ye Yuan and
                  Guoren Wang and
                  Lei Chen and
                  Xiang Lian and
                  Zimeng Wang},
	title = {Cohesive Group Nearest Neighbor Queries on Road-Social Networks under
                  Multi-Criteria},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3520--3536},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2974943},
	doi = {10.1109/TKDE.2020.2974943},
	timestamp = {Wed, 03 Nov 2021 08:25:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GuoYWCLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The group nearest neighbor (GNN) search on a road network\nG\nr\n, i.e., finding the spatial objects as activity assembly points with the smallest sum of distances to query users on\nG\nr\n, has been extensively studied; however, previous works neglected the fact that social relationships among query users, which ensure the maximally favorable atmosphere in the activity, can play an important role in GNN queries. Meanwhile, the ratings of spatial objects can also be used as recommended guidelines. Many real-world applications, such as location-based social networking services, require such queries. In this paper, we study two new problems: (1) a GNN search on a road network that incorporates cohesive social relationships (CGNN) and (2) a CGNN query under multi-criteria (MCGNN). Specifically, both the query users of highest closeness and the corresponding top-\nj\nobjects are retrieved. To address critical challenges on the effectiveness of results and the efficiency of computation over large road-social networks: (1) for CGNN, we propose a filtering-and-verification framework. During filtering, we prune substantial unpromising users and objects using social and geospatial constraints. During verification, we obtain the object candidates, among which the top\nj\nare selected, with respect to the qualified users; (2) for MCGNN, we propose threshold-based selection and expansion strategies, where different strict boundaries are proposed to ensure that correct top-\nj\nobjects are found early. Moreover, we further optimize search strategies to improve query performance. Finally, experimental results on real social and road networks significantly demonstrate the efficiency and efficacy of our solutions.}
}


@article{DBLP:journals/tkde/ChaiLFL21,
	author = {Chengliang Chai and
                  Guoliang Li and
                  Ju Fan and
                  Yuyu Luo},
	title = {CrowdChart: Crowdsourced Data Extraction From Visualization Charts},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3537--3549},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2972543},
	doi = {10.1109/TKDE.2020.2972543},
	timestamp = {Mon, 28 Aug 2023 21:37:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChaiLFL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Visualization charts are widely utilized for presenting structured data. Under many circumstances, people want to digitalize the data in the charts collected from various sources (e.g., papers and websites), in oder to further analyze the data or create new charts. However, existing automatic and semi-automatic approaches are not always effective due to the variety of charts. In this paper, we introduce a crowdsourcing approach that leverages human ability to extract data from visualization charts. There are several challenges. The first is how to avoid tedious human interaction with charts and design effective crowdsourcing tasks. Second, it is challenging to evaluate worker’s quality for truth inference, because workers may not only provide inaccurate values but also misalign values to wrong data series. Third, to guarantee quality, one may assign a task to many workers, leading to a high crowdsourcing cost. To address these challenges, we design an effective crowdsourcing task scheme that splits a chart into simple micro-tasks. We introduce a novel worker quality model by considering worker’s accuracy and task difficulty. We also devise effective task assignment and early-termination mechanisms to save the cost. We evaluate our approach on real-world datasets on real crowdsourced platforms, and the results demonstrate the effectiveness of our method.}
}


@article{DBLP:journals/tkde/CaoLHFZY21,
	author = {Bin Cao and
                  Yuqi Liu and
                  Chenyu Hou and
                  Jing Fan and
                  Baihua Zheng and
                  Jianwei Yin},
	title = {Expediting the Accuracy-Improving Process of SVMs for Class Imbalance
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3550--3567},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2974949},
	doi = {10.1109/TKDE.2020.2974949},
	timestamp = {Fri, 25 Oct 2024 08:42:52 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CaoLHFZY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To improve the classification performance of support vector machines (SVMs) on imbalanced datasets, cost-sensitive learning methods have been proposed, e.g., Different Error Costs (DEC) and Fuzzy SVM for Class Imbalance Learning (FSVM-CIL). They relocate the hyperplane by adjusting the costs associated with misclassifying samples. However, the error costs are determined either empirically or by performing an exhaustive search in the parameter space. Both strategies can not guarantee effectiveness and efficiency simultaneously. In this paper, we propose ATEC , a solution that can efficiently find a preferable hyperplane by automatically tuning the error cost for between-class samples. ATEC distinguishes itself from all existing parameter tuning strategies by two main features: (1) it can evaluate how effective an error cost is in terms of classification accuracy; and (2) it changes the error cost in the right direction if it is not effective. Extensive experiments show that compared with the state-of-art methods, SVMs that are equipped with ATEC can not only obtain comparable improvements in terms of F1 score of minority class, area under the precision-recall curve (AUC-PR) and area under the ROC curve (AUC-ROC) scores, but also outperform the grid-search parameter tuning strategy by two orders of magnitude in terms of the training time when a high F1 score is required.}
}


@article{DBLP:journals/tkde/ChivukulaYLZZ21,
	author = {Aneesh Sreevallabh Chivukula and
                  Xinghao Yang and
                  Wei Liu and
                  Tianqing Zhu and
                  Wanlei Zhou},
	title = {Game Theoretical Adversarial Deep Learning With Variational Adversaries},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3568--3581},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2972320},
	doi = {10.1109/TKDE.2020.2972320},
	timestamp = {Thu, 21 Apr 2022 14:42:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChivukulaYLZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A critical challenge in machine learning is the vulnerability of learning models in defending attacks from malicious adversaries. In this research, we propose game theoretical learning between a variational adversary and a Convolutional Neural Network (CNN), participating in a variable-sum two-player sequential Stackelberg game. Our adversary manipulates the input data distribution to make the CNN misclassify the manipulated data. Our ideal adversarial manipulation is a minimum change to the data which yet is large enough to mislead the CNNs. We propose an optimization procedure to find optimal adversarial manipulations by solving for the Nash equilibrium of the Stackelberg game. Specifically, the adversary's payoff function depends on the data manipulation which is determined by a Variational Autoencoder, while the CNN classifier's payoff functions are evaluated by misclassification errors. The optimization of our adversarial manipulations is defined by Alternating Least Squares and Simulated Annealing. Experimental results demonstrate that our game-theoretic manipulations are able to mislead CNNs that are well trained on the original data as well as on data generated by other models. We then let the CNNs to incorporate our manipulated data which leads to secure classifiers that are empirically the most robust in defending various types of adversarial attacks.}
}


@article{DBLP:journals/tkde/WangWWTL21,
	author = {Liang Wang and
                  Simeng Wu and
                  Tianheng Wu and
                  Xianping Tao and
                  Jian Lu},
	title = {{HKMF-T:} Recover From Blackouts in Tagged Time Series With Hankel
                  Matrix Factorization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3582--3593},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2971190},
	doi = {10.1109/TKDE.2020.2971190},
	timestamp = {Mon, 28 Aug 2023 21:37:39 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangWWTL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recovering missing values in time series is critical when performing time series analysis. And the blackouts issue studied in this paper, described as losing all the data during a certain period, is among the most urgent issues due to its devastating impact on service quality, and is challenging because of the absence of coevolving data sequences for reference. As a result, many existing approaches that rely on data from other coevolving sequences for missing value recovery are infeasible in handling blackouts. To address the issue, this work proposes a novel Hankel matrix factorization approach, HKMF-T, to recover missing values during blackouts for tagged time series, where a tagged time series consists of a data sequence and a corresponding tag sequence. Motivated by real-world observations, HKMF-T decomposes the data sequence into two components: 1) an internal, slowly-varying smooth trend, and 2) external impacts indicated by the tag sequence. By transforming a partially observed data sequence into a corresponding Hankel matrix, we learn the above two components and estimate the missing values under a unified framework of Hankel matrix factorization. Extensive experiments are conducted to evaluate the practical performance of HKMF-T on real-world data sets. And the results suggest HKMF-T outperforms the baseline approaches for blackouts with long durations.}
}


@article{DBLP:journals/tkde/XieLQLZMWT21,
	author = {Yuan Xie and
                  Bingqian Lin and
                  Yanyun Qu and
                  Cuihua Li and
                  Wensheng Zhang and
                  Lizhuang Ma and
                  Yonggang Wen and
                  Dacheng Tao},
	title = {Joint Deep Multi-View Learning for Image Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3594--3606},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2973981},
	doi = {10.1109/TKDE.2020.2973981},
	timestamp = {Wed, 03 Nov 2021 08:25:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XieLQLZMWT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a novel D eep M ulti-view J oint C lustering ( DMJC ) framework is proposed, where multiple deep embedded features, multi-view fusion mechanism, and clustering assignments can be learned simultaneously. Through the joint learning strategy, the clustering-friendly multi-view features and useful multi-view complementary information can be exploited effectively to improve the clustering performance. Under the proposed joint learning framework, we design two ingenious variants of deep multi-view joint clustering models, whose multi-view fusion is implemented by two kinds of simple yet effective schemes. The first model, called DMJC-S, performs multi-view fusion in an implicit way via a novel multi-view soft assignment distribution. The second model, termed DMJC-T, defines a novel multi-view auxiliary target distribution to conduct the multi-view fusion explicitly. Both DMJC-S and DMJC-T are optimized under a KL divergence objective. Experiments on eight challenging image datasets demonstrate the superiority of both DMJC-S and DMJC-T over single/multi-view baselines and the state-of-the-art multi-view clustering methods, which proves the effectiveness of the proposed DMJC framework. To the best of our knowledge, this is the first work to model the multi-view clustering in a deep joint framework, which will provide a meaningful thinking in unsupervised multi-view learning.}
}


@article{DBLP:journals/tkde/ShenDZLY21,
	author = {Ying Shen and
                  Ning Ding and
                  Hai{-}Tao Zheng and
                  Yaliang Li and
                  Min Yang},
	title = {Modeling Relation Paths for Knowledge Graph Completion},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3607--3617},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2970044},
	doi = {10.1109/TKDE.2020.2970044},
	timestamp = {Tue, 24 Jan 2023 15:23:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ShenDZLY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graphs (KG) often encounter knowledge incompleteness. The path reasoning that predicts the unknown path relation between pairwise entities based on existing facts is one of the most promising approaches to the knowledge graph completion. However, most conventional path reasoning methods exclusively consider the entity description included in fact triples, ignoring both the type information of entities and the interaction between different semantic representations. In this study, we propose a novel method, Type-aware Attentive Path Reasoning (TAPR), to complete the knowledge graph by simultaneously considering KG structural information, textual information, and type information. More specifically, we first leverage types to enrich the representational learning of entities and relationships. Next, we describe a type-level attention to select the most relevant type of given entity in a specific triple without any predefined rules or patterns to reduce the impact of noisy types. After learning the distributed representation of all paths, path-level attention assigns different weights to paths, from which relations among entity pairs are calculated. We conduct a series of experiments on a real-world dataset to demonstrate the effectiveness of TAPR. Experimental results show that our method significantly outperforms all baselines on link prediction and entity prediction tasks.}
}


@article{DBLP:journals/tkde/LiuSYWYY21,
	author = {Yinan Liu and
                  Wei Shen and
                  Zonghai Yao and
                  Jianyong Wang and
                  Zhenglu Yang and
                  Xiaojie Yuan},
	title = {Named Entity Location Prediction Combining Twitter and Web},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3618--3633},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2973261},
	doi = {10.1109/TKDE.2020.2973261},
	timestamp = {Mon, 05 Feb 2024 20:21:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuSYWYY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge bases are critical to many applications. However, they are greatly incomplete. Enriching knowledge bases with new entities and new location attributes becomes increasingly important. Given a named entity with tweets and Web documents where the entity appears, we aim to predict the entity city-level location combining the geographical location knowledge embedded in both Twitter and Web. This task is helpful for knowledge base enrichment and tweet location prediction. In this paper we propose NELPTW, the first unsupervised framework for N amed E ntity L ocation P rediction by leveraging the knowledge from T witter and W eb. Based on each data source, NELPTW utilizes a linear function ranking model to generate several rankings to the candidate location set for each entity. To combine the knowledge from two sources which have different reliability and importance for the location prediction, an unsupervised rank aggregation algorithm is developed to aggregate multiple rankings for each entity to obtain a better ranking. A learning algorithm based on the EM method is proposed to automatically learn the parameters of the ranking model without requiring any training labels. The experimental results over a real world Twitter and Web data set show that our framework significantly outperforms the baselines in terms of accuracy.}
}


@article{DBLP:journals/tkde/WangYWCY21,
	author = {Zheng Wang and
                  Xiaojun Ye and
                  Chaokun Wang and
                  Jian Cui and
                  Philip S. Yu},
	title = {Network Embedding With Completely-Imbalanced Labels},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3634--3647},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2971490},
	doi = {10.1109/TKDE.2020.2971490},
	timestamp = {Tue, 07 Feb 2023 13:33:24 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangYWCY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network embedding, aiming to project a network into a low-dimensional space, is increasingly becoming a focus of network research. Semi-supervised network embedding takes advantage of labeled data, and has shown promising performance. However, existing semi-supervised methods would get unappealing results in the completely-imbalanced label setting where some classes have no labeled nodes at all. To alleviate this, we propose two novel semi-supervised network embedding methods. The first one is a shallow method named RSDNE. Specifically, to benefit from the completely-imbalanced labels, RSDNE guarantees both intra-class similarity and inter-class dissimilarity in an approximate way. The other method is RECT which is a new class of graph neural networks. Different from RSDNE, to benefit from the completely-imbalanced labels, RECT explores the class-semantic knowledge. This enables RECT to handle networks with node features and multi-label setting. Experimental results on several real-world datasets demonstrate the superiority of the proposed methods.}
}


@article{DBLP:journals/tkde/RahmanHCC21,
	author = {Mizanur Rahman and
                  Nestor Hernandez and
                  Bogdan Carbunar and
                  Duen Horng Chau},
	title = {Towards De-Anonymization of Google Play Search Rank Fraud},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {11},
	pages = {3648--3661},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2975170},
	doi = {10.1109/TKDE.2020.2975170},
	timestamp = {Wed, 03 Nov 2021 08:25:23 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/RahmanHCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Search rank fraud, the fraudulent promotion of products hosted on peer-review sites, is driven by expert workers recruited online, often from crowdsourcing sites. In this paper we introduce the fraud de-anonymization problem, that goes beyond fraud detection, to unmask the human masterminds responsible for posting search rank fraud in peer-review sites. We collect and study data from crowdsourced search rank fraud jobs, and survey the capabilities and behaviors of 58 search rank fraud workers recruited from 6 crowdsourcing sites. We collect a gold standard dataset of Google Play user accounts attributed to 23 crowdsourced workers and analyze their fraudulent behaviors in the wild. We propose Dolos , a fraud de-anonymization system that leverages traits and behaviors we extract from our studies, to attribute detected fraud to crowdsourcing site workers, thus to real identities and bank accounts. We introduce MCDense, a min-cut dense component detection algorithm to uncover groups of user accounts controlled by different workers, and use stylometry and supervised learning to attribute them to crowdsourcing site profiles. Dolos correctly identified the owners of 95 percent of fraud worker-controlled communities, and uncovered fraud workers who promoted as many as 97.5 percent of fraud apps we collected from Google Play. When evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6 months, Dolos identified 1,056 apps with suspicious reviewer groups. We report orthogonal evidence of their fraud, including fraud duplicates and fraud re-posts. Dolos significantly outperformed adapted dense subgraph detection and loopy belief propagation competitors, on two new coverage scores that measure the quality of detected community partitions.}
}


@article{DBLP:journals/tkde/LaforetOBB21,
	author = {Fabian Laforet and
                  Christian Olms and
                  Rudolf Biczok and
                  Klemens B{\"{o}}hm},
	title = {An Ensemble Technique for Better Decisions Based on Data Streams and
                  its Application to Data Privacy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {12},
	pages = {3662--3674},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2977035},
	doi = {10.1109/TKDE.2020.2977035},
	timestamp = {Mon, 28 Aug 2023 21:37:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LaforetOBB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we address the problem of making decisions based on data streams, i.e., choosing an action when a new value is recorded. For instance, actions can be trading decisions in financial markets, choices of controllers in dynamic systems or perturbations of the data stream itself. To start with, we propose a language that allows individuals to formulate requirements on the action space. We use prediction techniques to identify the best possible action. However, for many scenarios there is not just one technique that predicts the future precisely, and different techniques behave quite differently. Thus, since there is no technique that dominates all the others, our conclusion is to take multiple predictions generated by different techniques into account. While ensemble techniques aggregating the predictions seem promising, existing techniques have issues, such as unnecessary information losses or the need for a predefined quality measure. Thus, we propose a new ensemble approach that weights predictions techniques according to requirements and solves an optimization problem that derives decisions directly from weighted predictions. We apply our solution to data privacy on data streams. For this setting, the benefits provided by prediction techniques have not been studied yet. In three case studies, we show that our solution consistently achieves better decision-making quality than approaches from related work.}
}


@article{DBLP:journals/tkde/LuoHLSYC21,
	author = {Yadan Luo and
                  Zi Huang and
                  Yang Li and
                  Fumin Shen and
                  Yang Yang and
                  Peng Cui},
	title = {Collaborative Learning for Extremely Low Bit Asymmetric Hashing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {12},
	pages = {3675--3685},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2977633},
	doi = {10.1109/TKDE.2020.2977633},
	timestamp = {Fri, 17 Feb 2023 09:02:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LuoHLSYC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hashing techniques are in great demand for a wide range of real-world applications such as image retrieval and network compression. Nevertheless, existing approaches could hardly guarantee a satisfactory performance with the extremely low-bit (e.g., 4-bit) hash codes due to the severe information loss and the shrink of the discrete solution space. In this article, we propose a novel Collaborative Learning strategy that is tailored for generating high-quality low-bit hash codes. The core idea is to jointly distill bit-specific and informative representations for a group of pre-defined code lengths. The learning of short hash codes among the group can benefit from the manifold shared with other long codes, where multiple views from different hash codes provide the supplementary guidance and regularization, making the convergence faster and more stable. To achieve that, an asymmetric hashing framework with two variants of multi-head embedding structures is derived, termed as Multi-head Asymmetric Hashing (MAH), leading to great efficiency of training and querying. Extensive experiments on three benchmark datasets have been conducted to verify the superiority of the proposed MAH, and have shown that the 8-bit hash codes generated by MAH achieve 94.3 percent of the MAP 1 1.\nMean Average Precision (MAP)\nscore on the CIFAR-10 dataset, which significantly surpasses the performance of the 48-bit codes by the state-of-the-arts in image retrieval tasks.}
}


@article{DBLP:journals/tkde/WangCLCL21,
	author = {Yue Wang and
                  Yulin Che and
                  Xiang Lian and
                  Lei Chen and
                  Qiong Luo},
	title = {Fast and Accurate SimRank Computation via Forward Local Push and its
                  Parallelization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {12},
	pages = {3686--3700},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2976988},
	doi = {10.1109/TKDE.2020.2976988},
	timestamp = {Wed, 15 Dec 2021 10:27:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangCLCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Measuring similarity among data objects is important in data analysis and mining. SimRank is a popular link-based similarity measurement among nodes in a graph. To compute the all-pairs SimRank matrix accurately, iterative methods are usually used. For static graphs, current iterative solutions are not efficient enough, both in time and space, due to the unnecessary cost and storage by the nature of iterative updating. For dynamic graphs, all current incremental solutions for updating the SimRank matrix are based on an approximated SimRank definition, and thus have no accuracy guarantee. In this paper, we propose a novel local push based algorithm for computing and tracking all-pairs SimRank. Furthermore, we develop an iterative parallel two-step framework for local push to take advantage of modern hardwares with multicore CPUs. We show that our algorithms outperform the state-of-the-art methods.}
}


@article{DBLP:journals/tkde/ChenYWGSMG21,
	author = {Zhongpu Chen and
                  Bin Yao and
                  Zhi{-}Jie Wang and
                  Xiaofeng Gao and
                  Shuo Shang and
                  Shuai Ma and
                  Minyi Guo},
	title = {Flexible Aggregate Nearest Neighbor Queries and its Keyword-Aware
                  Variant on Road Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {12},
	pages = {3701--3715},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2975998},
	doi = {10.1109/TKDE.2020.2975998},
	timestamp = {Mon, 13 Jun 2022 14:32:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenYWGSMG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aggregate nearest neighbor ( Ann ) query in both the euclidean space and road networks has been extensively studied, and the flexible aggregate nearest neighbor ( Fann ) problem further generalizes Ann by introducing an extra flexibility parameter\nϕ\nthat ranges in\n(0,1]\n. In this article, we focus on Fann on road networks, denoted as Fann\nR\n, and its keyword-aware variant, denoted as KFann\nR\n. To solve these problems, we propose a series of universal (i.e., suitable for both max and sum ) algorithms, including a Dijkstra-based algorithm that enumerates\nP\ninstead of\nϕ|Q|\n-combinations of\nQ\n, a queue-based approach that processes data points from-near-to-far, and a framework that combines incremental euclidean restriction (IER) and\nk\nNN. We also propose a specific exact solution to max - Fann\nR\nand a constant-factor ratio approximate solution to sum - Fann\nR\n. These specific algorithms are easy to implement and can achieve excellent performance in some scenarios. Besides, we further extend this problem to top-\nk\nand multiple Fann\nR\n(resp., KFann\nR\n) queries. We conduct a comprehensive experimental evaluation for the proposed algorithms on real datasets to demonstrate their superior efficiency and high quality.}
}


@article{DBLP:journals/tkde/YeMBSP21,
	author = {Wei Ye and
                  Dominik Mautz and
                  Christian B{\"{o}}hm and
                  Ambuj K. Singh and
                  Claudia Plant},
	title = {Incorporating User's Preference into Attributed Graph Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {12},
	pages = {3716--3728},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2976063},
	doi = {10.1109/TKDE.2020.2976063},
	timestamp = {Tue, 30 Jan 2024 13:00:56 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YeMBSP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph clustering has been studied extensively on both plain graphs and attributed graphs. However, all these methods need to partition the whole graph to find cluster structures. Sometimes, based on domain knowledge, people may have information about a specific target region in the graph and only want to find a single cluster concentrated on this local region. Such a task is called local clustering. In contrast to global clustering, local clustering aims to find only one cluster that is concentrating on the given seed vertex (and also on the designated attributes for attributed graphs). Currently, very few methods can deal with this kind of task. To this end, we propose two quality measures for a local cluster: Graph Unimodality ( GU ) and Attribute Unimodality ( AU ). The former measures the homogeneity of the graph structure while the latter measures the homogeneity of the subspace that is composed of the designated attributes. We call their linear combination as Compactness . Further, we propose LOCLU to optimize the Compactness score. The local cluster detected by LOCLU concentrates on the region of interest, provides efficient information flow in the graph and exhibits a unimodal data distribution in the subspace of the designated attributes.}
}


@article{DBLP:journals/tkde/ChenLLWHSX21,
	author = {Xiaofeng Chen and
                  Hui Li and
                  Jin Li and
                  Qian Wang and
                  Xinyi Huang and
                  Willy Susilo and
                  Yang Xiang},
	title = {Publicly Verifiable Databases With All Efficient Updating Operations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {33},
	number = {12},
	pages = {3729--3740},
	year = {2021},
	url = {https://doi.org/10.1109/TKDE.2020.2975777},
	doi = {10.1109/TKDE.2020.2975777},
	timestamp = {Thu, 24 Feb 2022 11:50:52 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenLLWHSX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The primitive of verifiable database (VDB) can enable a resource-limited client to securely outsource an encrypted database to an untrusted cloud server and the client could efficiently retrieve and update the data at will. Meanwhile, the client can undoubtedly detect any misbehavior by the server if the database has been tampered with. We argue that most of the existing VDB schemes can only support the updating operation of replacement, rather than other common updating operations such as insertion and deletion . Recently, the first publicly verifiable VDB schemes that supports all updating operations was proposed based on the idea of hierarchical vector commitment. However, one disadvantage of the proposed VDB scheme is that the computation and storage complexity increases linearly when the client continually inserts data records in the same index of the database. As a result, it remains an open problem how to construct an efficient (and publicly verifiable) VDB scheme that can support all updating operations regardless of the manner of insertion. In this paper, we first introduce a new primitive called committed invertible Bloom filter (CIBF) and utilize it to propose a new publicly verifiable VDB scheme that can support all kinds of updating operations. Additionally, the proposed construction is efficient regardless of the manner of updating operations and thus provides an affirmative answer to the above open problem.}
}
