@article{DBLP:journals/tkde/PazhoNPVMT24,
	author = {Armin Danesh Pazho and
                  Ghazal Alinezhad Noghre and
                  Arnab A. Purkayastha and
                  Jagannadh Vempati and
                  Martin Otto and
                  Hamed Tabkhi},
	title = {A Survey of Graph-Based Deep Learning for Anomaly Detection in Distributed
                  Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {1--20},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3282898},
	doi = {10.1109/TKDE.2023.3282898},
	timestamp = {Fri, 26 Jan 2024 21:32:57 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/PazhoNPVMT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection is a crucial task in complex distributed systems. A thorough understanding of the requirements and challenges of anomaly detection is pivotal to the security of such systems, especially for real-world deployment. While there are many works and application domains that deal with this problem, few have attempted to provide an in-depth look at such systems. In this survey, we explore the potentials of graph-based algorithms to identify anomalies in distributed systems. These systems can be heterogeneous or homogeneous, which can result in distinct requirements. One of our objectives is to provide an in-depth look at graph-based approaches to conceptually analyze their capability to handle real-world challenges such as heterogeneity and dynamic structure. This study gives an overview of the State-of-the-Art (SotA) research articles in the field and compare and contrast their characteristics. To facilitate a more comprehensive understanding, we present three systems with varying abstractions as use cases. We examine the specific challenges involved in anomaly detection within such systems. Subsequently, we elucidate the efficacy of graphs in such systems and explicate their advantages. We then delve into the SotA methods and highlight their strength and weaknesses, pointing out the areas for possible improvements and future works.}
}


@article{DBLP:journals/tkde/DongSLLD24,
	author = {Chenhe Dong and
                  Ying Shen and
                  Shiyang Lin and
                  Zhenzhou Lin and
                  Yang Deng},
	title = {A Unified Framework for Contextual and Factoid Question Generation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {21--34},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3280182},
	doi = {10.1109/TKDE.2023.3280182},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DongSLLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Question generation (QG) aims to automatically generate fluent and relevant questions, where the two most mainstream directions are generating questions from unstructured contextual texts (CQG), such as news articles, and generating questions from structured factoid texts (FQG), such as knowledge graphs or tables. Existing methods for these two tasks mainly face challenges of limited internal structural information as well as scarce background information, while these two tasks can benefit each other for alleviating these issues. For example, when meeting the entity mention “United Kingdom” in CQG, it can be inferred that it is a country in European continent based on the structural knowledge “(Europe, countries_within, United Kingdom)” in FQG. And when meeting the entity “Houston Rockets” in FQG, more background information, such as “an American professional basketball team based in Houston since 1971”, can be found in the related passages of CQG. To this end, we propose a unified framework for the tasks of CQG and FQG, where: (i) two types of task-sharing modules are developed to learn shared contextual and structural knowledge, where the task format is unified with a pseudo passage reformulation strategy; (ii) for the CQG task, a task-specific knowledge module with a knowledge selection and aggregation mechanism is introduced, so as to incorporate more factoid knowledge from external knowledge graphs and alleviate the word ambiguity problem; and (iii) for the FQG task, a task-specific passage module with a multi-level passage fusion mechanism is designed to extract fine-grained word-level knowledge. Experimental results in both automatic and human evaluation show the effectiveness of our proposed method.}
}


@article{DBLP:journals/tkde/LiuLZLK24,
	author = {An{-}An Liu and
                  Haochun Lu and
                  Heyu Zhou and
                  Tian{-}Bao Li and
                  Mohan S. Kankanhalli},
	title = {Balanced Class-Incremental 3D Object Classification and Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {35--48},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3284032},
	doi = {10.1109/TKDE.2023.3284032},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLZLK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most existing 3D object classification and retrieval algorithms rely on one-off supervised learning on closed 3D object sets and tend to provide rigid convolutional neural networks with little scalability. Such limitations substantially restrict their potential to learn newly emerged 3D object classes continually in the real world. Aiming to go beyond these limitations, we innovatively propose two new and challenging tasks: class-incremental 3D object classification (CI-3DOC) and class-incremental 3D object retrieval (CI-3DOR), the key to which is class-incremental 3D representation learning. It expects the network to update continually to learn new 3D class representations without forgetting the previously learned ones. To this end, we design a novel balanced distillation network (BDNet) that uses a dual supervision mechanism to balance between consolidating old knowledge (stability) and adapting to new 3D object classes (plasticity) carefully. On the one hand, we employ stability-based supervision to retain the stable and discriminative information of old classes that greatly benefit both classification and retrieval tasks. On the other hand, we use plasticity-based supervision to improve the network's generalization for learning new class 3D representations by transferring knowledge from a temporary teacher network to the current model. By properly handling the relationship between the two modules, we achieve a surprising performance improvement. Furthermore, considering there is no available dataset for evaluation, we build two 3D datasets, INOR-1 and INOR-2, to evaluate these two new tasks. Extensive experimental results demonstrate that our method can significantly outperform other state-of-the-art class-incremental learning methods. Even if we store 500-1000 fewer 3D objects than SOTA methods, BDNet still achieves comparable performance.}
}


@article{DBLP:journals/tkde/YuZYCC24,
	author = {Zhiwen Yu and
                  Zhijie Zhong and
                  Kaixiang Yang and
                  Wenming Cao and
                  C. L. Philip Chen},
	title = {Broad Learning Autoencoder With Graph Structure for Data Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {49--61},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3283425},
	doi = {10.1109/TKDE.2023.3283425},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YuZYCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Broad learning system (BLS) is a simple yet efficient learning algorithm that only needs to train a three-layer feedforward neural network. Although various BLS variants have been designed for supervised learning, none have been used for unsupervised learning. This paper proposes BLS-AE, a novel data clustering scheme that seamlessly combines BLS and auto-encoder. Then, graph regularization is introduced into BLS-AE to increase the capability of learning intrinsic structures in data and adaptation to various data simultaneously, which is termed BLSg-AE. Moreover, different concatenation styles of feature and enhancement nodes are investigated for reusing the learned features, followed by designing two special strategies (i.e., pruning optimization and incremental learning) to reduce the parameter scale significantly and improve performance, which is termed xBLSg-AE. To address the performance instability issue caused by random subspace in a single xBLSg-AE, the x-cascade broad learning system graph regularization multi-auto-encoder (xBLSg-MAE) algorithm is proposed. Extensive experiments are conducted on multiple real data sets to demonstrate that the proposed methods are more effective and robust than competing approaches.}
}


@article{DBLP:journals/tkde/OuyangYZZWH24,
	author = {Xiaocao Ouyang and
                  Yan Yang and
                  Wei Zhou and
                  Yiling Zhang and
                  Hao Wang and
                  Wei Huang},
	title = {CityTrans: Domain-Adversarial Training With Knowledge Transfer for
                  Spatio-Temporal Prediction Across Cities},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {62--76},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3283520},
	doi = {10.1109/TKDE.2023.3283520},
	timestamp = {Sun, 19 Jan 2025 13:53:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/OuyangYZZWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the spatio-temporal data of a city is not always available, insufficient data would lead to poor performance in some urban prediction tasks. Existing works utilize transfer learning to solve the data scarcity problem, but they ignore the differences in data distributions across cities, which leads to the ineffectiveness of knowledge transfer. In this paper, we propose a domain adversarial model with knowledge transfer for spatio-temporal prediction across cities, entitled CityTrans. Specifically, 1) the self-adaptive spatio-temporal knowledge (namely ST-Knowledge) is mined, to learn the latent spatial and temporal patterns among cities; 2) the domain-adversarial training strategy is introduced to enhance domain invariance; 3) a knowledge attention mechanism is proposed to extract the transferable information from the ST-Knowledge. Note that our CityTrans is an end-to-end domain adversarial spatio-temporal network without two-stage training (i.e., pre-training and fine-tuning). Finally, we conduct extensive experiments on two spatio-temporal prediction tasks: traffic (flow and speed) prediction, and air quality prediction. Experimental results demonstrate that CityTrans outperforms state-of-the-art models on all tasks by a significant margin.}
}


@article{DBLP:journals/tkde/HouCL24,
	author = {Liangshao Hou and
                  Delin Chu and
                  Li{-}Zhi Liao},
	title = {Convergence of a Fast Hierarchical Alternating Least Squares Algorithm
                  for Nonnegative Matrix Factorization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {77--89},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3279369},
	doi = {10.1109/TKDE.2023.3279369},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HouCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The hierarchical alternating least squares (HALS) algorithms are powerful tools for nonnegative matrix factorization (NMF), among which the Fast-HALS, proposed in [A. Cichocki and A.-H. Phan, 2009], is one of the most efficient. This paper investigates the convergence of Fast-HALS. First, a more general weak convergence (converged subsequences exist and converge to the stationary point set) is established without any assumption, while most existing results assume all the columns of iterates are strictly away from the origin. Then, a simplified strong convergence (the entire sequence converges to a stationary point) proof is provided. The existing strong convergence is attributed to the block prox-linear (BPL) method, which is a more general framework including Fast-HALS as a special case. So, the convergence proof under BPL is quite complex. Our simplified proof explores the structure of Fast-HALS and can be regarded as a complement to the results under BPL. In addition, some numerical verifications are presented.}
}


@article{DBLP:journals/tkde/ShenLZZ24,
	author = {Zhihao Shen and
                  Shun Li and
                  Xi Zhao and
                  Jianhua Zou},
	title = {CT-Auth: Capacitive Touchscreen-Based Continuous Authentication on
                  Smartphones},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {90--106},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3277879},
	doi = {10.1109/TKDE.2023.3277879},
	timestamp = {Fri, 07 Feb 2025 10:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ShenLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continuous authentication, which provides identity verification using behavioral biometrics in an implicit and transparent manner, has shown potentials for protecting privacy. As the most common way of human-computer interaction, touch behavior pattern of each user has been proven distinctive and widely adopted for continuous authentication. However, most touch based solutions rely on the touchscreen signals obtained from high-level application programming interfaces, which are hard to characterize fine-grained appearance and contour profile of contact fingertips as well as dynamic sliding information in a touch gesture. In this paper, we propose a continuous authentication framework called CT-Auth, which leverages raw capacitive value collected from capacitive touchscreen on smartphone as a descriptor of touch behavior for authentication. Specifically, we first develop a three-dimensional convolution neural network model for capturing intra-gesture spatial-temporal feature and a structure extraction model for capturing structural information between moving fingertips of a touch gesture and touchscreen. A recurrent neural network based model is also applied for capturing temporal patterns among a sequence of touch gestures. To evaluate the effectiveness of our framework, we recruit 100 volunteers over 2 months and collect a large-scale dataset in the unconstrained conditions. Extensive experiments reveal that CT-Auth provides the state-of-the-art authentication accuracy.}
}


@article{DBLP:journals/tkde/WanHWL24,
	author = {Xiaolong Wan and
                  Xixian Han and
                  Jinbao Wang and
                  Jianzhong Li},
	title = {Efficient Discovery of Functional Dependencies on Massive Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {107--121},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3288209},
	doi = {10.1109/TKDE.2023.3288209},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WanHWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Functional dependencies are the most common constraints in the design theory for relational databases, which have very important practical applications in many areas. Different kinds of algorithms are proposed to discover the functional dependencies. However, it is found in this paper that the existing algorithms cannot deal well with massive data which cannot be held entirely in memory due to high memory consumption and high computation cost. In this paper, a novel algorithm FSC is presented to compute functional dependencies on massive data. The two-step execution of FSC relies on a pre-computed update-friendly assistant structure of comparable pairs which reflect the identifier pairs for tuples with at least one equal attribute. In step 1, FSC determines the violated functional dependencies and introduces the selective comparison by comparable pairs to reduce the required pairwise comparison significantly. The direct value-combination compression strategy is devised to process attributes of small cardinality. In step 2, FSC induces the required functional dependencies by the results in step 1. The extensive experimental results, conducted on synthetic and real-life data sets, show that FSC can discover functional dependencies on massive data efficiently.}
}


@article{DBLP:journals/tkde/MiaoYLWLD24,
	author = {Yinbin Miao and
                  Yutao Yang and
                  Xinghua Li and
                  Linfeng Wei and
                  Zhiquan Liu and
                  Robert H. Deng},
	title = {Efficient Privacy-Preserving Spatial Data Query in Cloud Computing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {122--136},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3283020},
	doi = {10.1109/TKDE.2023.3283020},
	timestamp = {Wed, 24 Jan 2024 17:49:53 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MiaoYLWLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of geographic location technology and the explosive growth of data, a large amount of spatial data is outsourced to the cloud server for reducing the local high storage and computing burdens, but at the same time causes security issues. Thus, extensive privacy-preserving spatial data query schemes have been proposed. Most of the existing schemes use Asymmetric Scalar-Product-Preserving Encryption (ASPE) to encrypt data, but ASPE has proven to be insecure against known plaintext attack. And the existing schemes require users to provide more information about query range and thus generate a large amount of ciphertexts, which causes high storage and computational burdens. To solve these issues, based on enhanced ASPE designed in our conference version, we first propose a basic Privacy-preserving Spatial Data Query (PSDQ) scheme by using a new unified index structure, which only requires users to provide less information about query range. Then, we propose an enhanced PSDQ scheme (PSDQ^+) by using Geohash-based R-tree structure (called GR-tree) and efficient pruning strategy, which greatly reduces the query time. Formal security analysis proves that our schemes achieve Indistinguishability under Chosen Plaintext Attack (IND-CPA), and extensive experiments demonstrate that our schemes are efficient in practice.}
}


@article{DBLP:journals/tkde/RamaManeiroVL24,
	author = {Efr{\'{e}}n Rama{-}Maneiro and
                  Juan Carlos Vidal and
                  Manuel Lama},
	title = {Embedding Graph Convolutional Networks in Recurrent Neural Networks
                  for Predictive Monitoring},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {137--151},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3286017},
	doi = {10.1109/TKDE.2023.3286017},
	timestamp = {Wed, 24 Jan 2024 17:49:53 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/RamaManeiroVL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predictive monitoring of business processes is a subfield of process mining that aims to predict, among other things, the characteristics of the next event or the sequence of the next events. Although multiple approaches based on deep learning have been proposed, mainly recurrent neural networks and convolutional neural networks, none of them really exploit the structural information available in process models. This paper proposes an approach that simultaneously learns spatio-temporal information from both the event log and the process model by combining recurrent neural networks with graph convolutional networks. Thus, common patterns from process models, such as loops or parallels, can be learned while avoiding overwriting information during the encoding phase. An experimental evaluation of real-life event logs shows that our approach is more consistent and outperforms the current state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/WangWFMQCF24,
	author = {Ziyang Wang and
                  Wei Wei and
                  Shanshan Feng and
                  Xian{-}Ling Mao and
                  Minghui Qiu and
                  Dangyang Chen and
                  Rui Fang},
	title = {Exploiting Group-Level Behavior Pattern for Session-Based Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {152--166},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3280310},
	doi = {10.1109/TKDE.2023.3280310},
	timestamp = {Sun, 09 Feb 2025 10:52:29 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangWFMQCF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Session-based recommendation (SBR) is a challenging task, which aims to predict users’ future interests based on anonymous behavior sequences. Existing methods leverage powerful representation learning approaches to encode sessions into a low-dimensional space. However, despite such achievements, the existing studies focus on the instance-level session learning, while neglecting the group-level users’ preferences (e.g., the common preferences of group users in repeat consumption). To this end, we propose a novel Repeat-aware Neural Mechanism for Session-based Recommendation (RNMSR). In RNMSR, we propose to learn the user preference from two levels: (i) instance-level, which employs GNNs on a similarity-based item-pairwise session graph to capture the users’ preference in instance-level. (ii) group-level, which converts sessions into group-level behavior patterns to model the group-level users’ preferences. In RNMSR, we combine instance-level and group-level user preference to model the repeat consumption of users, i.e., whether users take repeated consumption and which items are preferred by users. Extensive experiments are conducted on three real-world datasets, i.e., Diginetica, Yoochoose, and Nowplaying, demonstrating that the proposed method consistently achieves state-of-the-art performance in all the tests.}
}


@article{DBLP:journals/tkde/RongCMSB24,
	author = {Huan Rong and
                  Gongchi Chen and
                  Tinghuai Ma and
                  Victor S. Sheng and
                  Elisa Bertino},
	title = {FuFaction: Fuzzy Factual Inconsistency Correction on Crowdsourced
                  Documents With Hybrid-Mask at the Hidden-State Level},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {167--183},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3286097},
	doi = {10.1109/TKDE.2023.3286097},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/RongCMSB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, crowdsourced documents like Wikipedia pages and comments on products are all over the Internet. However, documents generated by crowdsourcing participants may contain inconsistent facts, implicit semantics and fabricated contents, thus threatening the trustworthiness of information content security available in Internet. To address this problem, we propose FuFaction, enabled by an enhanced observation mechanism based on the notion of hybrid-mask consisting of a hard-mask and a soft-mask, to eliminate factual inconsistencies on crowdsourced documents at the hidden-state level (or in a fuzzy way), according to the given evidence retrieved from an external open domain. Specifically, instead of focusing on a specific category of factual inconsistency, FuFaction captures anomalous hidden-states between a crowdsourced document and evidence obtained via a reverse-attention mechanism, where a hard-mask controls the attending direction as bidirectional and unidirectional for better understanding on semantics. Then, a soft-mask is generated with the help of the hard-masked reverse-attention to revise or mask anomalous hidden-states on the crowdsourced document. Afterwards, the masked hidden-states are further refined by a cross reverse-attention and factual consistency reinforcement strategy, based on which a new crowdsourced document with higher factual consistency is generated via neural text generation. According to our experimental results, FuFaction can effectively deal with the fuzzy factual inconsistencies on crowdsourced documents, achieving the overall best performance in terms of factual consistency metrics with a little higher (yet still competitive) editing cost on literal vocabulary, so as to reflect factually consistent semantics supported by the given evidence.}
}


@article{DBLP:journals/tkde/LiuGLHZWLL24,
	author = {Yu Liu and
                  Qian Ge and
                  Wei Luo and
                  Qiang Huang and
                  Lei Zou and
                  Haixu Wang and
                  Xin Li and
                  Chang Liu},
	title = {GraphMM: Graph-Based Vehicular Map Matching by Leveraging Trajectory
                  and Road Correlations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {184--198},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3287739},
	doi = {10.1109/TKDE.2023.3287739},
	timestamp = {Wed, 10 Jul 2024 07:43:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuGLHZWLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Map matching of sparse vehicle trajectories is a fundamental problem in location-based services, such as traffic flow analysis and vehicle routing. Existing literature mainly relies on sequence-to-sequence (Seq2Seq) models to capture the intra-trajectory correlation of an input trajectory and to sequentially predict the matched road segments. Due to the limited expressive capability of sequential models, these methods fall short of extracting inter-trajectory and trajectory-road correlations as well as correlation between road segments. We present GraphMM, a graph-based approach that explicitly utilizes all aforementioned correlations. Our model exploits the graph nature of map matching and incorporates graph neural networks and conditional models to leverage both road and trajectory graph topology, while manages to align road segments and trajectories in latent space. We formally analyze the expressive power of our model in capturing various correlations and propose efficient algorithms for model training and inference. In particular, our optimization techniques dramatically reduce the computational complexity, making our model feasible on datasets with thousands of road segments. Extensive experiments show that our model significantly enhances prediction accuracy, while improving training and inference efficiency by up to an order of magnitude over both the industrial implementation of the hidden Markov model and state-of-the-art Seq2Seq-based methods.}
}


@article{DBLP:journals/tkde/SuWXLY24,
	author = {Ting{-}Ting Su and
                  Chang{-}Dong Wang and
                  Wudong Xi and
                  Jian{-}Huang Lai and
                  Philip S. Yu},
	title = {Hierarchical Alignment With Polar Contrastive Learning for Next-Basket
                  Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {199--210},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3282914},
	doi = {10.1109/TKDE.2023.3282914},
	timestamp = {Thu, 13 Feb 2025 10:32:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SuWXLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next-basket recommendation methods focus on the inference of the next basket by considering the corresponding basket sequence. Although many methods have been developed for the task, they usually suffer from data sparsity. The number of interactions between entities is relatively small compared to their huge bases, so it is crucial to mine as much hidden information as possible from the limited historical interactions for prediction. However, the existing methods mainly just treat the next-basket recommendation task as a single-view sequential prediction problem, which leads to the inadequate mining of the information hidden in multiple views, and the mining of other patterns in the historical interactions is neglected, thus making it difficult to learn high-quality representations and limiting the recommendation effect. To alleviate the above issues, we propose a novel method named HapCL for next-basket recommendation, which mines information from multiple views and patterns with the help of polar contrastive learning. A hierarchical module is designed to mine multiple patterns of historical interactions from different views at two levels. In order to mine self-supervised signals, we design a polar contrastive learning module with a novel graph-based augmentation approach. Experiments on three real-world datasets validate the effectiveness of HapCL.}
}


@article{DBLP:journals/tkde/WuLZWZ24,
	author = {Lianwei Wu and
                  Pusheng Liu and
                  Yongqiang Zhao and
                  Peng Wang and
                  Yanning Zhang},
	title = {Human Cognition-Based Consistency Inference Networks for Multi-Modal
                  Fake News Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {211--225},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3280555},
	doi = {10.1109/TKDE.2023.3280555},
	timestamp = {Mon, 13 Jan 2025 13:58:07 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuLZWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The existing models for multi-modal fake news detection focus mainly on capturing common similar semantics between different modalities to improve detection performance. However, they ignore the extraction of inconsistent features between these modalities. The intuitive cognition way people identify a piece of fake news is generally to discover if there are inconsistent semantics among news content itself and its comments, which could be abstracted as “comparing news image-text consistency - finding valuable comments - reasoning in-/consistency between news and comments”. Inspired by the cognitive process, we propose Human Cognition-based Consistency Inference Networks (HCCIN) to comprehensively explore consistent and inconsistent semantics for multi-modal fake news detection. Specifically, we first design cross-modal alignment layer to learn consistent semantics between textual and visual information within the multi-modal news, and then the comment clue discovery layer is devoted to ascertaining the most-concerned semantics by audiences between comments. Finally, we develop collaborative inference layer to drive news consistent semantics and the most-concerned semantics to reason and discover consistent and inconsistent information between them. Experiments on three public datasets, including Weibo, Twitter, and PHEME, reveal the superiority of our HCCIN.}
}


@article{DBLP:journals/tkde/LiangLZTWYDL24,
	author = {Ke Liang and
                  Yue Liu and
                  Sihang Zhou and
                  Wenxuan Tu and
                  Yi Wen and
                  Xihong Yang and
                  Xiangjun Dong and
                  Xinwang Liu},
	title = {Knowledge Graph Contrastive Learning Based on Relation-Symmetrical
                  Structure},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {226--238},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3282989},
	doi = {10.1109/TKDE.2023.3282989},
	timestamp = {Fri, 16 Feb 2024 11:29:49 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiangLZTWYDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graph embedding (KGE) aims at learning powerful representations to benefit various artificial intelligence applications. Meanwhile, contrastive learning has been widely leveraged in graph learning as an effective mechanism to enhance the discriminative capacity of the learned representations. However, the complex structures of KG make it hard to construct appropriate contrastive pairs. Only a few attempts have integrated contrastive learning strategies with KGE. But, most of them rely on language models (e.g., Bert) for contrastive pair construction instead of fully mining information underlying the graph structure, hindering expressive ability. Surprisingly, we find that the entities within a relational symmetrical structure are usually similar and correlated. To this end, we propose a knowledge graph contrastive learning framework based on relation-symmetrical structure, KGE-SymCL, which mines symmetrical structure information in KGs to enhance the discriminative ability of KGE models. Concretely, a plug-and-play approach is proposed by taking entities in the relation-symmetrical positions as positive pairs. Besides, a self-supervised alignment loss is designed to pull together positive pairs. Experimental results on link prediction and entity classification datasets demonstrate that our KGE-SymCL can be easily adopted to various KGE models for performance improvements. Moreover, extensive experiments show that our model could outperform other state-of-the-art baselines.}
}


@article{DBLP:journals/tkde/ZhuZGLYS24,
	author = {Lei Zhu and
                  Chaoqun Zheng and
                  Weili Guan and
                  Jingjing Li and
                  Yang Yang and
                  Heng Tao Shen},
	title = {Multi-Modal Hashing for Efficient Multimedia Retrieval: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {239--260},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3282921},
	doi = {10.1109/TKDE.2023.3282921},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuZGLYS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive growth of multimedia contents, multimedia retrieval is facing unprecedented challenges on both storage cost and retrieval speed. Hashing technique can project the high-dimensional data into compact binary hash codes. With it, the most time-consuming semantic similarity computation during the multimedia retrieval process can be significantly accelerated with fast Hamming distance computation, and meanwhile the storage cost can be reduced greatly by the binary embedding. In the light of this, multi-modal hashing has recently received considerable attention to support large-scale multimedia retrieval. Different from uni-modal hashing, the multi-modal hashing focuses on modeling the multi-modal semantics and further preserving them into binary hash codes with hash learning. In this paper, we first systematically review the existing learning to hash methods for efficient multimedia retrieval, categorizing them according to the multimedia retrieval tasks, the specific multi-modal semantic modeling techniques, and hash learning strategies. Thereafter, we present the performance comparison results. We ultimately discuss the challenges and potential research directions that may require further investigation in multi-modal hash learning.}
}


@article{DBLP:journals/tkde/ZhangRFWCZ24,
	author = {Haonan Zhang and
                  Yuyang Ren and
                  Luoyi Fu and
                  Xinbing Wang and
                  Guihai Chen and
                  Chenghu Zhou},
	title = {Multi-Scale Self-Supervised Graph Contrastive Learning With Injective
                  Node Augmentation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {261--274},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3278463},
	doi = {10.1109/TKDE.2023.3278463},
	timestamp = {Sun, 19 Jan 2025 13:53:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangRFWCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Contrastive Learning (GCL) with Graph Neural Networks (GNN) has emerged as a promising method for learning latent node representations in a self-supervised manner. Most of existing GCL methods employ random sampling for graph view augmentation and maximize the agreement of the node representations between the views. However, the random augmentation manner, which is likely to produce very similar graph view samplings, may easily result in incomplete nodal contextual information, thus weakening the discrimination of node representations. To this end, this paper proposes a novel trainable scheme from the perspective of node augmentation, which is theoretically proved to be injective and utilizes the subgraphs consisting of each node with its neighbors to enhance the distinguishability of nodal view. Notably, our proposed scheme tries to enrich node representations via a multi-scale contrastive training that integrates three different levels of training granularity, i.e., subgraph level, graph- and node-level contextual information. In particular, the subgraph-level objective between augmented and original node views is constructed to enhance the discrimination of node representations while graph- and node-level objectives with global and local information from the original graph are developed to improve the generalization ability of representations. Experiment results demonstrate that our framework outperforms existing state-of-the-art baselines and even surpasses several supervised counterparts on four real-world datasets for node classification.}
}


@article{DBLP:journals/tkde/ChenJTSZC24,
	author = {Yixin Chen and
                  Di Jiang and
                  Conghui Tan and
                  Yuanfeng Song and
                  Chen Jason Zhang and
                  Lei Chen},
	title = {Neural Moderation of {ASMR} Erotica Content in Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {275--280},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3283501},
	doi = {10.1109/TKDE.2023.3283501},
	timestamp = {Sat, 30 Nov 2024 21:08:01 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenJTSZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularity of video/audio streaming applications in recent years, the wide spread of Autonomous Sensory Meridian Response (ASMR) erotica content is becoming a serious issue in social networks. Due to the subtle nature of ASMR erotica and its relative rareness in real scenario, detecting ASMR erotica contents is a challenging task. In this article, we propose a novel neural framework for ASMR erotica content moderation. The proposed framework consists of a pipeline of novel strategies to tackle challenges unique in ASMR Erotica Contents such as data scarcity and imbalanced data. Based on large-scale industrial data, the proposed framework demonstrates high moderation accuracy in quantitative analysis and significantly outperforming the existing counterparts.}
}


@article{DBLP:journals/tkde/WangYWCZL24,
	author = {Hanchen Wang and
                  Jianke Yu and
                  Xiaoyang Wang and
                  Chen Chen and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {Neural Similarity Search on Supergraph Containment},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {281--295},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3279920},
	doi = {10.1109/TKDE.2023.3279920},
	timestamp = {Mon, 29 Jul 2024 16:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangYWCZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Supergraph search is a fundamental graph query processing problem. Supergraph search aims to find all data graphs contained in a given query graph based on the subgraph isomorphism. Existing algorithms construct the indices and adopt the filtering-and-verification framework which is usually computationally expensive and can cause redundant computations. Recently, various learning-based methods have been proposed for a good trade-off between accuracy and efficiency for query processing tasks. However, to the best of our knowledge, there is no learning-based method proposed for the supergraph search task. In this paper, we propose the first learning-based method for similarity search on supergraph containment, named Neural Supergraph similarity Search (NSS). NSS first learns the representations for query and data graphs and then efficiently conducts the supergraph search on the representation space whose complexity is linear to the number of data graphs. The carefully designed Wasserstein discriminator and reconstruction network enable NSS to better capture the interrelation, structural and label information between and within the query and data graphs. Experiments demonstrate that the NSS is up to 6 orders of magnitude faster than the state-of-the-art exact supergraph search algorithm in terms of query processing and more accurate compared to the other learning-based solutions.}
}


@article{DBLP:journals/tkde/SunYHDXWX24,
	author = {Xinyue Sun and
                  Qingqing Ye and
                  Haibo Hu and
                  Jiawei Duan and
                  Qiao Xue and
                  Tianyu Wo and
                  Jie Xu},
	title = {{PUTS:} Privacy-Preserving and Utility-Enhancing Framework for Trajectory
                  Synthesization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {296--310},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3288154},
	doi = {10.1109/TKDE.2023.3288154},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunYHDXWX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle trajectory data is essential for traffic management and location-based services. However, publishing real-life trajectory data has been challenging because vehicle trajectories contain users’ sensitive information. Differential privacy addresses such problems by publishing a synthetic version of the input dataset, but existing works always assume the real-world data is absolutely accurate. This assumption no longer holds in trajectory data because it typically contains errors due to inaccurate positioning services, which leads to poor performance of data synthesized by such trajectories. Even worse, existing works may generate unrealistic trajectories due to their coarse data synthesis methods, resulting in low practical utility or even inability to handle complex tasks. In this paper, we propose a Privacy-preserving and Utility-enhancing framework for Trajectory Synthesization (PUTS). Our framework mitigates the impact of data errors in trajectories on differential privacy mechanisms, by exploiting map-matching techniques and real-world road network structure. In PUTS, a two-layer approach from path to trajectory synthesis is proposed to not only guarantee the reality of synthetic trajectories, but also scale up PUTS in real-world applications. Extensive experiments on real-world datasets show that PUTS significantly outperforms existing methods in terms of utility in a range of real-world applications.}
}


@article{DBLP:journals/tkde/ChoXHD24,
	author = {Byungjin Cho and
                  Yu Xiao and
                  Pan Hui and
                  Daoyi Dong},
	title = {Quantum Bandit With Amplitude Amplification Exploration in an Adversarial
                  Environment},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {311--317},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3279207},
	doi = {10.1109/TKDE.2023.3279207},
	timestamp = {Fri, 08 Mar 2024 13:21:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChoXHD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid proliferation of learning systems in an arbitrarily changing environment mandates the need to manage tensions between exploration and exploitation. This work proposes a quantum-inspired bandit learning approach for the learning-and-adapting-based offloading problem where a client observes and learns the costs of each task offloaded to the candidate resource providers, e.g., fog nodes. In this approach, a new action update strategy and novel probabilistic action selection are adopted, provoked by the amplitude amplification and collapse postulate in quantum computation theory. We devise a locally linear mapping between a quantum-mechanical phase in a quantum domain, e.g., Grover-type search algorithm, and a distilled probability-magnitude in a value-based decision-making domain, e.g., adversarial multi-armed bandit algorithm. The proposed algorithm is generalized, via the devised mapping, for better learning weight adjustments on favorable/unfavorable actions, and its effectiveness is verified via simulation.}
}


@article{DBLP:journals/tkde/HeHLY24,
	author = {Rundong He and
                  Zhongyi Han and
                  Xiankai Lu and
                  Yilong Yin},
	title = {{SAFER-STUDENT} for Safe Deep Semi-Supervised Learning With Unseen-Class
                  Unlabeled Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {318--334},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3279139},
	doi = {10.1109/TKDE.2023.3279139},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HeHLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep semi-supervised learning (SSL) methods aim to utilize abundant unlabeled data to improve the seen-class classification. However, in the open-world scenario, collected unlabeled data tend to contain unseen-class data, which would degrade the generalization to seen-class classification. Formally, we define the problem as safe deep semi-supervised learning with unseen-class unlabeled data. One intuitive solution is removing these unseen-class instances after detecting them during the SSL process. Nevertheless, the performance of unseen-class identification is limited by the lack of suitable score function, the uncalibrated model, and the small number of labeled data. To this end, we propose a safe SSL method called SAFER-STUDENT from the teacher-student view. First, to enhance the ability of teacher model to identify seen and unseen classes, we propose a general scoring framework called Discrepancy with Raw (DR). Second, based on unseen-class data mined by teacher model from unlabeled data, we calibrate student model by newly proposed Unseen-class Energy-bounded Calibration (UEC) loss. Third, based on seen-class data mined by teacher model from unlabeled data, we propose Weighted Confirmation Bias Elimination (WCBE) loss to boost seen-class classification of student model. Extensive studies show that SAFER-STUDENT remarkably outperforms the state-of-the-art, verifying the effectiveness of our method in the under-explored problem.}
}


@article{DBLP:journals/tkde/YuYXCLH24,
	author = {Junliang Yu and
                  Hongzhi Yin and
                  Xin Xia and
                  Tong Chen and
                  Jundong Li and
                  Zi Huang},
	title = {Self-Supervised Learning for Recommender Systems: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {335--355},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3282907},
	doi = {10.1109/TKDE.2023.3282907},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YuYXCLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, neural architecture-based recommender systems have achieved tremendous success, but they still fall short of expectation when dealing with highly sparse data. Self-supervised learning (SSL), as an emerging technique for learning from unlabeled data, has attracted considerable attention as a potential solution to this issue. This survey paper presents a systematic and timely review of research efforts on self-supervised recommendation (SSR). Specifically, we propose an exclusive definition of SSR, on top of which we develop a comprehensive taxonomy to divide existing SSR methods into four categories: contrastive, generative, predictive, and hybrid. For each category, we elucidate its concept and formulation, the involved methods, as well as its pros and cons. Furthermore, to facilitate empirical comparison, we release an open-source library SELFRec (https://github.com/Coder-Yu/SELFRec), which incorporates a wide range of SSR models and benchmark datasets. Through rigorous experiments using this library, we derive and report some significant findings regarding the selection of self-supervised signals for enhancing recommendation. Finally, we shed light on the limitations in the current research and outline the future research directions.}
}


@article{DBLP:journals/tkde/ZhangGOYLT24,
	author = {Fan Zhang and
                  Haicheng Guo and
                  Dian Ouyang and
                  Shiyu Yang and
                  Xuemin Lin and
                  Zhihong Tian},
	title = {Size-Constrained Community Search on Large Networks: An Effective
                  and Efficient Solution},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {356--371},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3280483},
	doi = {10.1109/TKDE.2023.3280483},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangGOYLT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a fundamental graph problem, community search is applied in various areas, e.g., social networks, the world wide web, and biology. A common requirement from real applications is to return a community with a bounded size while most existing solutions do not constrain community size. Recent studies on size-constrained community search still have some critical issues, e.g., the existence of a better cohesiveness objective, some queries returning empty results, and inefficiency on partial queries. Thus, in this paper, we study the size-constrained truss community search (STCS). Given a graph G, a query vertex q, and size constraint [l,h], the STCS problem aims to find a subgraph containing q with the largest min-support among all connected subgraphs having at least l and at most h vertices. We prove the STCS problem is NP-hard and APX-hard unless P = NP. An effective heuristic is proposed to quickly find a high-quality initial result. Then, a branch and bound algorithm is introduced to find the exact result, with novel optimizations, e.g., budget-cost-based bounding and branching strategies. Extensive experiments verify that the community quality returned by our algorithm is better and our algorithm is faster by up to 5 orders of magnitude, compared with the state-of-the-art.}
}


@article{DBLP:journals/tkde/ZhengFPJPWWY24,
	author = {Chuanpan Zheng and
                  Xiaoliang Fan and
                  Shirui Pan and
                  Haibing Jin and
                  Zhaopeng Peng and
                  Zonghan Wu and
                  Cheng Wang and
                  Philip S. Yu},
	title = {Spatio-Temporal Joint Graph Convolutional Networks for Traffic Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {372--385},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3284156},
	doi = {10.1109/TKDE.2023.3284156},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengFPJPWWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent studies have shifted their focus towards formulating traffic forecasting as a spatio-temporal graph modeling problem. Typically, they constructed a static spatial graph at each time step and then connected each node with itself between adjacent time steps to create a spatio-temporal graph. However, this approach failed to explicitly reflect the correlations between different nodes at different time steps, thus limiting the learning capability of graph neural networks. Additionally, those models overlooked the dynamic spatio-temporal correlations among nodes by using the same adjacency matrix across different time steps. To address these limitations, we propose a novel approach called Spatio-Temporal Joint Graph Convolutional Networks (STJGCN) for accurate traffic forecasting on road networks over multiple future time steps. Specifically, our method encompasses the construction of both pre-defined and adaptive spatio-temporal joint graphs (STJGs) between any two time steps, which represent comprehensive and dynamic spatio-temporal correlations. We further introduce dilated causal spatio-temporal joint graph convolution layers on the STJG to capture spatio-temporal dependencies from distinct perspectives with multiple ranges. To aggregate information from different ranges, we propose a multi-range attention mechanism. Finally, we evaluate our approach on five public traffic datasets and experimental results demonstrate that STJGCN is not only computationally efficient but also outperforms 11 state-of-the-art baseline methods.}
}


@article{DBLP:journals/tkde/WuYLL24,
	author = {Yuankai Wu and
                  Hongyu Yang and
                  Yi Lin and
                  Hong Liu},
	title = {Spatiotemporal Propagation Learning for Network-Wide Flight Delay
                  Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {386--400},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3286690},
	doi = {10.1109/TKDE.2023.3286690},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuYLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate and interpretable delay predictions are vital for decision-making in the aviation industry. However, effectively incorporating spatiotemporal dependencies and external factors related to delay propagation remains a challenge. To address this challenge, we propose the SpatioTemporal Propagation Network (STPN), a novel space-time separable graph convolutional network that models delay propagation by considering both spatial and temporal factors. STPN uses a multi-graph convolution model that considers both geographic proximity and airline schedules from a spatial perspective, while employing a multi-head self-attention mechanism that can be learned end-to-end and explicitly accounts for various types of temporal dependencies in delay time series from a temporal perspective. Experiments on two real-world delay datasets show that STPN outperforms state-of-the-art methods for multi-step ahead arrival and departure delay prediction in large-scale airport networks. Additionally, the counterfactuals generated by STPN provide evidence of its ability to learn explainable delay propagation patterns. Comprehensive experiments also demonstrate that STPN sets a robust benchmark for general spatiotemporal forecasting. The code for STPN is available at https://github.com/Kaimaoge/STPN.}
}


@article{DBLP:journals/tkde/WangHWWW24,
	author = {Zeyu Wang and
                  Zhenying He and
                  Peng Wang and
                  Yang Wang and
                  Wei Wang},
	title = {Static and Streaming Discovery of Maximal Linear Representation Between
                  Time Series},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {401--415},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3287273},
	doi = {10.1109/TKDE.2023.3287273},
	timestamp = {Tue, 13 Aug 2024 08:01:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangHWWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, many applications, like the Internet of Things and Industrial Internet, collect data points from sensors continuously to form long time series. Finding the correlation between time series is a fundamental task for many time series mining problems. However, it is meaningless to directly measure the global correlation between two long time series due to concept shift or noise data. To tackle this challenge, in this paper, we formulate the novel problem of finding maximal significant linear representation. The major idea is that, given two time series and a quality constraint, we want to find the longest gapped time interval on which a time series can be linearly represented by the other within the quality constraint requirement. We develop both exact and approximate algorithms (with approximation quality guarantees), which exploit a novel representation of the linear correlation between time series on subsequences, and transform the problem into a geometric search. Moreover, we propose an online approach to find this correlation in each sliding window incrementally for the streaming data. We present a systematic empirical study to verify the efficiency and effectiveness of our approaches.}
}


@article{DBLP:journals/tkde/LuoZQJZ24,
	author = {Xiao Luo and
                  Yusheng Zhao and
                  Yifang Qin and
                  Wei Ju and
                  Ming Zhang},
	title = {Towards Semi-Supervised Universal Graph Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {416--428},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3280859},
	doi = {10.1109/TKDE.2023.3280859},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LuoZQJZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks have pushed state-of-the-arts in graph classifications recently. Typically, these methods are studied within the context of supervised end-to-end training, which necessities copious task-specific labels. However, in real-world circumstances, labeled data could be limited, and there could be a massive corpus of unlabeled data, even from unknown classes as a complementary. Towards this end, we study the problem of semi-supervised universal graph classification, which not only identifies graph samples which do not belong to known classes, but also classifies the remaining samples into their respective classes. This problem is challenging due to a severe lack of labels and potential class shifts. In this paper, we propose a novel graph neural network framework named UGNN, which makes the best of unlabeled data from the subgraph perspective. To tackle class shifts, we estimate the certainty of unlabeled graphs using multiple subgraphs, which facilities the discovery of unlabeled data from unknown categories. Moreover, we construct semantic prototypes in the embedding space for both known and unknown categories and utilize posterior prototype assignments inferred from the Sinkhorn-Knopp algorithm to learn from abundant unlabeled graphs across different subgraph views. Extensive experiments on six datasets verify the effectiveness of UGNN in different settings.}
}


@article{DBLP:journals/tkde/HuangKHCFGZ24,
	author = {Junfan Huang and
                  Peipei Kang and
                  Na Han and
                  Yonghao Chen and
                  Xiaozhao Fang and
                  Hongbo Gao and
                  Guoxu Zhou},
	title = {Two-Stage Asymmetric Similarity Preserving Hashing for Cross-Modal
                  Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {429--444},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3283984},
	doi = {10.1109/TKDE.2023.3283984},
	timestamp = {Sat, 13 Jan 2024 17:37:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuangKHCFGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hashing-based techniques present appealing solutions for cross-modal retrieval due to its low storage requirements and excellent query efficiency. The majority of cross-modal hashing methods typically adopt equal-length encoding scheme to represent multimodal data and achieve cross-modal similarity search. However, such scheme can be regarded as a relatively strict limitation, because it sacrifices the flexible representation of multimodal data in reality and cannot always guarantee the optimal retrieval performance. To address the challenge, this paper focuses on encoding heterogeneous data with varying hash lengths. To achieve this purpose, we propose a flexible cross-modal hashing approach, named Two-stage Asymmetric Similarity Preserving Hashing, TASPH for short, which can be applied to both unequal-length and equal-length retrieval scenarios. Specifically, in the first stage, TASPH designs a novel discrete asymmetric strategy to learn the modality-specific hash codes with varying lengths, enabling a flexible representation of heterogeneous data. Simultaneously, TASPH utilizes two semantic transformation matrices to establish the semantic correlations between varying hash codes. Different from most of the existing approaches that employ relaxation solutions, TASPH satisfies the discrete constraints without any relaxation. In the second stage, the learned semantic transformation matrices are employed to alleviate cross-modal heterogeneity, which guarantees that TASPH can learn more powerful hash functions to improve the discriminative ability of hash codes. Abundant experiments conducted on three benchmark datasets demonstrate encouraging results compared with the state-of-the-art approaches under different retrieval scenarios.}
}


@article{DBLP:journals/tkde/LiuCLCZLW24,
	author = {Li Liu and
                  Penggang Chen and
                  Xin Li and
                  William Kwok{-}Wai Cheung and
                  Youmin Zhang and
                  Qun Liu and
                  Guoyin Wang},
	title = {WL-Align: Weisfeiler-Lehman Relabeling for Aligning Users Across Networks
                  via Regularized Representation Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {1},
	pages = {445--458},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3277843},
	doi = {10.1109/TKDE.2023.3277843},
	timestamp = {Fri, 19 Jan 2024 08:33:29 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuCLCZLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aligning users across networks using graph representation learning has been found effective where the alignment is accomplished in a low-dimensional embedding space. Yet, highly precise alignment remains challenging, especially for nodes with long-range connectivity to labeled anchors. To alleviate this limitation, we propose WL-Align which employs a regularized representation learning framework to learn distinctive node representations. It extends the Weisfeiler-Lehman Isormorphism Test and learns the alignment in alternating phases of “across-network Weisfeiler-Lehman relabeling” and “proximity-preserving representation learning”. The across-network Weisfeiler-Lehman relabeling is achieved through iterating the anchor-based label propagation and a similarity-based hashing to exploit the known anchors’ connectivity to different nodes in an efficient and robust manner. The representation learning module preserves the second-order proximity within individual networks and is regularized by the across-network Weisfeiler-Lehman hash labels. Extensive experiments on real-world and synthetic datasets have demonstrated that our proposed WL-Align outperforms the state-of-the-art methods, achieving significant performance improvements in the “exact matching” scenario.}
}


@article{DBLP:journals/tkde/ZhangJYFKZLYCW24,
	author = {Shengyu Zhang and
                  Ziqi Jiang and
                  Jiangchao Yao and
                  Fuli Feng and
                  Kun Kuang and
                  Zhou Zhao and
                  Shuo Li and
                  Hongxia Yang and
                  Tat{-}Seng Chua and
                  Fei Wu},
	title = {Causal Distillation for Alleviating Performance Heterogeneity in Recommender
                  Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {459--474},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3290545},
	doi = {10.1109/TKDE.2023.3290545},
	timestamp = {Fri, 14 Feb 2025 11:54:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangJYFKZLYCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation performance usually exhibits a long-tail distribution over users — a small portion of head users enjoy much more accurate recommendation services than the others. We reveal two sources of this performance heterogeneity problem: the uneven distribution of historical interactions (a natural source); and the biased training of recommender models (a model source). As addressing this problem cannot sacrifice the overall performance, a wise choice is to eliminate the model bias while maintaining the natural heterogeneity. The key to debiased training lies in eliminating the effect of confounders that influence both the user's historical behaviors and the next behavior. The emerging causal recommendation methods achieve this by modeling the causal effect between user behaviors, however potentially neglect unobserved confounders (e.g., friend suggestions) that are hard to measure in practice. To address unobserved confounders, we resort to the front-door adjustment (FDA) in causal theory and propose a causal multi-teacher distillation framework (CausalD). FDA requires proper mediators in order to estimate the causal effects of historical behaviors on the next behavior. To achieve this, we equip CausalD with multiple heterogeneous recommendation models to model the mediator distribution. Then, the causal effect estimated by FDA is the expectation of recommendation prediction over the mediator distribution and the prior distribution of historical behaviors, which is technically achieved by multi-teacher ensemble. To pursue efficient inference, CausalD further distills multiple teachers into one student model to directly infer the causal effect for making recommendations. We instantiate CausalD on two representative models, DeepFM and DIN, and conduct extensive experiments on three real-world datasets, which validate the superiority of CausalD over state-of-the-art methods. Through in-depth analysis, we find that CausalD largely improves the performance of tail users, reduces the performance heterogeneity, and enhances the overall performance.}
}


@article{DBLP:journals/tkde/WangWBCXLYH24,
	author = {Yue Wang and
                  Yao Wan and
                  Lu Bai and
                  Lixin Cui and
                  Zhuo Xu and
                  Ming Li and
                  Philip S. Yu and
                  Edwin R. Hancock},
	title = {Collaborative Knowledge Graph Fusion by Exploiting the Open Corpus},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {475--489},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3289949},
	doi = {10.1109/TKDE.2023.3289949},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangWBCXLYH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To ease the process of building Knowledge Graphs (KGs) from scratch, a cost-effective method is required to enrich a KG using the triples extracted from a corpus. However, it is challenging to enrich a KG with newly extracted triples since they contain noisy information. This paper proposes to refine a KG by leveraging information extracted from a corpus. In particular, we first formulate the task of building KGs as two coupled sub-tasks, namely join event extraction and knowledge graph fusion. We then propose a collaborative knowledge graph fusion framework, which is composed of an explorer and a supervisor, to allow the involved two sub-tasks to mutually assist each other in an alternative manner. More concretely, an explorer extracts triples from a corpus supervised by both the ground-truth annotation and the KG provided by the supervisor. Furthermore, a supervisor then evaluates the extracted triples and enriches the KG with those that are highly ranked. To implement this evaluation, we further propose a translated relation alignment scoring mechanism to align and translate the extracted triples to the KG. Experimental results verify that this collaboration can improve both the performance of our sub-tasks, and contribute to high-quality enriched knowledge graphs.}
}


@article{DBLP:journals/tkde/WuWMWY24,
	author = {Yangyang Wu and
                  Jun Wang and
                  Xiaoye Miao and
                  Wenjia Wang and
                  Jianwei Yin},
	title = {Differentiable and Scalable Generative Adversarial Models for Data
                  Imputation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {490--503},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3293129},
	doi = {10.1109/TKDE.2023.3293129},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuWMWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data imputation has been extensively explored to solve the missing data problem. The dramatically increasing volume of incomplete data makes the imputation models computationally infeasible in many real-life applications. In this paper, we propose an effective scalable imputation system named {\\sf SCIS} to significantly speed up the training of the differentiable generative adversarial imputation models under accuracy-guarantees for large-scale incomplete data. {\\sf SCIS} consists of two modules, differentiable imputation modeling (DIM) and sample size estimation (SSE). DIM leverages a new masking Sinkhorn divergence function to make an arbitrary generative adversarial imputation model differentiable, while for such a differentiable imputation model, SSE can estimate an appropriate sample size to ensure the user-specified imputation accuracy of the final model. Moreover, {\\sf SCIS} can also accelerate the autoencoder based imputation models. Extensive experiments upon several real-life large-scale datasets demonstrate that, our proposed system can accelerate the generative adversarial model training by 6.23x. Using around 1.27% samples, {\\sf SCIS} yields competitive accuracy with the state-of-the-art imputation methods in much shorter computation time.}
}


@article{DBLP:journals/tkde/ChenZCSS24,
	author = {Xinyu Chen and
                  Chengyuan Zhang and
                  Xiaoxu Chen and
                  Nicolas Saunier and
                  Lijun Sun},
	title = {Discovering Dynamic Patterns From Spatiotemporal Data With Time-Varying
                  Low-Rank Autoregression},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {504--517},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3294440},
	doi = {10.1109/TKDE.2023.3294440},
	timestamp = {Fri, 21 Feb 2025 10:22:55 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenZCSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of discovering interpretable dynamic patterns from spatiotemporal data is studied in this paper. For that purpose, we develop a time-varying reduced-rank vector autoregression (VAR) model whose coefficient matrices are parameterized by low-rank tensor factorization. Benefiting from the tensor factorization structure, the proposed model can simultaneously achieve model compression and pattern discovery. In particular, the proposed model allows one to characterize nonstationarity and time-varying system behaviors underlying spatiotemporal data. To evaluate the proposed model, extensive experiments are conducted on various spatiotemporal datasets representing different nonlinear dynamical systems, including fluid dynamics, sea surface temperature, USA surface temperature, and NYC taxi trips. Experimental results demonstrate the effectiveness of the proposed model for analyzing spatiotemporal data and characterizing spatial/temporal patterns. In the spatial context, the spatial patterns can be automatically extracted and intuitively characterized by the spatial modes. In the temporal context, the complex time-varying system behaviors can be revealed by the temporal modes in the proposed model. Thus, our model lays an insightful foundation for understanding complex spatiotemporal data in real-world dynamical systems.}
}


@article{DBLP:journals/tkde/ZhangFWQST24,
	author = {Dan Zhang and
                  Wenzheng Feng and
                  Yuandong Wang and
                  Zhongang Qi and
                  Ying Shan and
                  Jie Tang},
	title = {DropConn: Dropout Connection Based Random GNNs for Molecular Property
                  Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {518--529},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3290032},
	doi = {10.1109/TKDE.2023.3290032},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangFWQST24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, molecular data mining has attracted a lot of attention owing to its great application potential in material and drug discovery. However, this mining task faces a challenge posed by the scarcity of labeled molecular graphs. To overcome this challenge, we introduce a novel data augmentation and a semi-supervised confidence-aware consistency regularization training framework for molecular property prediction. The core of our framework is a data augmentation strategy on molecular graphs, named DropConn (Dropout Connection). DropConn generates pseudo molecular graphs by softening the hard connections of chemical bonds (as edges), where the soft weights are calculated from edge features so that the adaptive interactions between different atoms can be incorporated. Besides, to enhance the model's generalization ability, a consistency regularization training strategy is proposed to take full advantage of massive unlabeled data. Furthermore, DropConn can serve as a plugin that can be seamlessly added to many existing models. Extensive experiments under both non-pre-training setting and fine-tuning setting demonstrate that DropConn can obtain superior performance (up to 8.22%) over state-of-the-art methods on molecular property prediction tasks.}
}


@article{DBLP:journals/tkde/ZhangLJWLT24,
	author = {Donghao Zhang and
                  Zhenyu Liu and
                  Weiqiang Jia and
                  Fei Wu and
                  Hui Liu and
                  Jianrong Tan},
	title = {Dual Attention Graph Convolutional Network for Relation Extraction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {530--543},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3289879},
	doi = {10.1109/TKDE.2023.3289879},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLJWLT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dependency-based models are widely used to extract semantic relations in text. Most existing dependency-based models establish stacked structures to merge contextual and dependency information, which encode the contextual information first and then encode the dependency information. However, this unidirectional information flow weakens the representation of words in the sentence, which further restricts the performance of existing models. To establish bidirectional information flow, a dual attention graph convolutional network (DAGCN) with a parallel structure is proposed. Most importantly, DAGCN can build multi-turn interactions between contextual and dependency information to imitate the multi-turn looking-back actions of human beings. In addition, multi-layer adjacency matrix-aware multi-head attention (AMAtt), including context-to-dependency attention and dependency-to-context attention, is carefully designed as a merge mechanism in the parallel structure to preserve the structural information of sentences and dependency trees during interactions. Furthermore, DAGCN is evaluated on the popular PubMed dataset, TACRED dataset and SemEval 2010 Task 8 dataset to demonstrate its validity. Experimental results show that our model outperforms the existing dependency-based models.}
}


@article{DBLP:journals/tkde/LiQT24,
	author = {Pan Li and
                  Maofei Que and
                  Alexander Tuzhilin},
	title = {Dual Contrastive Learning for Efficient Static Feature Representation
                  in Sequential Recommendations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {544--555},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3289469},
	doi = {10.1109/TKDE.2023.3289469},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiQT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Static user and item features constitute important information to be taken into account in the recommendation process. However, as these features are usually sparse and of large-vocabulary, existing deep learning-based methods typically construct large tables of high-dimensional feature embeddings, which is inefficient in terms of memory storage and is computationally problematic. On the other hand, while product quantization-based methods have been proposed to compress latent embeddings, they usually come at the cost of compromising recommendation performance due to the restrictive expressive power, as feature correlations and user-item interactions are not properly captured in the compression process. To address these issues, we propose a novel Dual Contrastive Learning method to generate low-dimensional discrete static feature representations that significantly reduce memory storage and computational complexity, while simultaneously producing superior recommendation performance. Extensive offline experiments on three large-scale industrial datasets demonstrate that our proposed model significantly outperforms the selected baselines. In addition, we conducted an online A/B test at Alibaba and show that the proposed model significantly improves the average video streaming time, while reducing the size of the feature embedding table by 90% over the currently deployed system.}
}


@article{DBLP:journals/tkde/QinPW24,
	author = {Yalan Qin and
                  Nan Pu and
                  Hanzhou Wu},
	title = {Elastic Multi-View Subspace Clustering With Pairwise and High-Order
                  Correlations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {556--568},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3293498},
	doi = {10.1109/TKDE.2023.3293498},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/QinPW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view clustering has become an important research topic in machine learning and computer vision communities, which aims at achieving a consensus partition of data points across different views. However, the existing multi-view clustering methods fail to simultaneously consider the pairwise and high-order correlations among different views in the process of obtaining the final results. In this article, we propose the Elastic multi-view Subspace Clustering with pairwise and high-order Correlations (ESCC) to solve this problem. ESCC simultaneously explores the pairwise and high-order correlations among different views, resulting in a more comprehensive shared representation. ESCC formulates these two kinds of correlations into a unified objective framework, which are able to be jointly optimized to refine each other. As an instantiation, we construct an example of ESCC (e-ESCC) in this work. To be specific, e-ESCC uses the multi-layer neural networks to study the pairwise correlation from multiple views with the guidance of the latent representation. It is also able to help obtain the nonlinear subspaces of the multi-view data. e-ESCC collects multi-view similarity matrices into a tensor and utilizes the low-rank tensor norm to exploit the high-order correlation among different views. The augmented Lagrangian multiplier is adopted to solve the formulated problem of e-ESCC. Experiments on eight data sets validate the superiority of our method over 15 state-of-the-art multi-view clustering methods under six metrics.}
}


@article{DBLP:journals/tkde/XieG24,
	author = {Tianyang Xie and
                  Yong Ge},
	title = {Enhance Knowledge Graph Embedding by Mixup},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {569--580},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3292379},
	doi = {10.1109/TKDE.2023.3292379},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XieG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graphs have important applications for many computational tasks, such as personalized recommendations, information search, and natural language processing. Knowledge graph embedding, which is to learn representations of nodes and relations, is very critical to facilitate these applications and thus has been studied extensively in the literature. Most existing knowledge graphs (e.g., Freebase) have a data scarcity issue, i.e., the number of observed triplets is much less than that of all possible pairs of nodes. While the data augmentation technique has been widely applied to addressing data scarcity in other domains (e.g., image data), there are few prior studies exploring it for knowledge graph embedding probably because the discrete data structure of knowledge graphs prohibits the employment of most data augmentation methods. To fill this research gap, this paper introduces a novel data augmentation framework, namely knowledge graph mixup (KG Mixup), to enhance knowledge graph embedding. Based on the proposed framework, we develop two specific methods: vanilla mixup and influence mixup. Both approaches generate virtual mixup triplets and incorporate them into the learning process through a new mixup loss function. While vanilla mixup generates virtual triplets based on a uniform distribution, the influence mixup approach employs the influence function to guide the generation of mixup samples. Experiments with multiple datasets have shown that both approaches significantly outperform knowledge graph embedding models trained by the ordinary training framework.}
}


@article{DBLP:journals/tkde/HaoLLNWL24,
	author = {Zhezheng Hao and
                  Zhoumin Lu and
                  Guoxu Li and
                  Feiping Nie and
                  Rong Wang and
                  Xuelong Li},
	title = {Ensemble Clustering With Attentional Representation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {581--593},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3292573},
	doi = {10.1109/TKDE.2023.3292573},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HaoLLNWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ensemble clustering has emerged as a powerful framework for analyzing heterogeneous and complex data. Despite the abundance of existing schemes, co-association matrix-based methods remain the mainstream approach. However, focusing solely on pairwise correlations falls short of fully capturing the intricate cluster relationships. Moreover, despite its potential, ensemble clustering has yet to effectively leverage the powerful representation capabilities of neural networks. To address these limitations, we propose a deep ensemble clustering method called Ensemble Clustering with Attentional Representation (ECAR). Our method considers the results of base partitions as groups with related information to explore higher-order fusion information. ECAR captures the importance of each sample’s association with its related group by employing an attentional network, and encodes this information into a low-dimensional representation. The attentional network is trained by jointly optimizing the clustering loss from soft assignments learned from the embeddings and the reconstruction loss from the weighted graph generated from ensemble clustering. During training, the weights of base partitions are adaptively refined to promote diversity and consistency while reducing the impact of low-quality and redundant base partitions. Extensive experimental results on real-world datasets demonstrate the substantial improvement of our method over existing baseline ensemble clustering methods and deep clustering methods.}
}


@article{DBLP:journals/tkde/XuQHLXLZZ24,
	author = {Tianyu Xu and
                  Jianfeng Qu and
                  Wen Hua and
                  Zhixu Li and
                  Jiajie Xu and
                  An Liu and
                  Lei Zhao and
                  Xiaofang Zhou},
	title = {Evidence Reasoning and Curriculum Learning for Document-Level Relation
                  Extraction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {594--607},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3292974},
	doi = {10.1109/TKDE.2023.3292974},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XuQHLXLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Document-level Relation Extraction (RE) is a promising task aiming at identifying relations of multiple entity pairs in a document. Compared with the sentence-level counterpart, it has raised two significant challenges: a) In most cases, a relational fact can be adequately expressed via a small subset of sentences from the document, namely evidence. But the traditional method cannot model such strong semantic correlations between evidence sentences that collaborate to describe a specific relation; b) The data of this task is extremely long-tail in terms of too many NA instances and imbalanced relational types. Such data can mislead the tail prediction bias to the head categories in the RE model. In this paper, we present a novel Evidence reasoning and Curriculum learning method for DocRE (DRE-EC) to address these challenges. Particularly, we first formulate evidence extraction as a sequential decision problem through a crafted reinforcement learning mechanism with an efficient path searching strategy to reduce the action space. Providing the evidence for each entity pair as a customized-filtered document in advance helps infer the relations better. To address the long-tail issue, we further develop a hybrid curriculum learning method at the NA-level (NC) and relation-level (RC) with our customized difficulty measure score. In NC, the NA samples are scheduled in an easy-to-hard scheme and gradually added, resulting in the data distribution from ideal and balanced to real and unbalanced. In RC, the scheme is switched into hard-to-easy to enhance the hard and tail samples. In addition, we propose a new Equalization adaptive Focal Loss(EFLoss) that can adjust to the changing data distribution and focus more on the tail categories. We conduct various experiments on two document-level RE benchmarks and achieve a remarkable improvement over previous competitive baselines. Furthermore, we provide detailed analyses of the advantages and effectiveness of our method.}
}


@article{DBLP:journals/tkde/ZengUYLHT24,
	author = {Jian Zeng and
                  Leong Hou U and
                  Xiao Yan and
                  Yan Li and
                  Mingji Han and
                  Bo Tang},
	title = {Extracting Top- Frequent and Diversified Patterns in Knowledge Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {608--626},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2022.3233594},
	doi = {10.1109/TKDE.2022.3233594},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZengUYLHT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A knowledge graph contains many real-world facts that can be used to support various analytical tasks, e.g., exceptional fact discovery and the check of claims. In this work, we attempt to extract top-k frequent and diversified patterns from knowledge graph by well capturing user interest. Specifically, we first formalize the core-based top-k frequent pattern discovery problem, which finds the top-k frequent patterns that are extended from a core pattern specified by user query and have the highest frequency. In addition, to diversify the top-k frequent patterns, we define a distance function to measure the dissimilarity between two patterns, and return top-k patterns in which the pairwise diversity of any two resultant patterns exceeds a given threshold. As the search space of candidate patterns is exponential w.r.t. the number of nodes and edges in the knowledge graph, discovering frequent and diversified patterns is computationally challenging. To achieve high efficiency, we propose a suite of techniques, including (1) We devise a meta-index to avoid generating invalid candidate patterns; (2) We propose an upper bound of the frequency score (i.e., \\mathsf {MNI}) of the candidate pattern, which is used to prune unqualified candidates earlier and prioritize the enumeration order of patterns; (3) We design an advanced join-based approach to compute the \\mathsf {MNI} of candidate patterns efficiently; and (4) We develop a lower bound for distance function and incrementally compute the pairwise diversity among the patterns. Using real-world knowledge graphs, we experimentally verify the efficiency and effectiveness of our proposed techniques. We also demonstrate the utility of the extracted patterns by case studies.}
}


@article{DBLP:journals/tkde/SunSY24,
	author = {Yu Sun and
                  Shaoxu Song and
                  Xiaojie Yuan},
	title = {From Minimum Change to Maximum Density: On Determining Near-Optimal
                  S-Repair},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {627--639},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3294401},
	doi = {10.1109/TKDE.2023.3294401},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dirty data are commonly observed in real applications, making cleaning them a key step in data preparation. The widely adopted idea of cleaning dirty data is based on detecting conflicts w.r.t. integrity constraints. Typical S-repair methods remove a minimal set of tuples (to avoid excessive removal and information loss) such that integrity constraints are no longer violated in remaining tuples. Unfortunately, multiple candidates of minimal removal sets may exist and are difficult to determine which one is indeed proper. We intuitively notice that a clean tuple often has more close neighbors (i.e., higher density) than dirty tuples. Hence, in this paper, we study the problem of finding the optimal S-repair under integrity constraints with the highest density, among various minimal removal sets. Our major contributions include (1) the np-hardness analysis on solving the problem, (2) a heuristic algorithm for efficiently tackling the problem and returning the optimal solution in certain cases, (3) an approximation performance bounded method with the same optimal solution guarantee. Experiments on real datasets collected from industry with real-world errors demonstrate the superiority of our work in cleaning dirty tuples.}
}


@article{DBLP:journals/tkde/XiaLXTWLL24,
	author = {Jun Xia and
                  Haitao Lin and
                  Yongjie Xu and
                  Cheng Tan and
                  Lirong Wu and
                  Siyuan Li and
                  Stan Z. Li},
	title = {{GNN} Cleaner: Label Cleaner for Graph Structured Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {640--651},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3288002},
	doi = {10.1109/TKDE.2023.3288002},
	timestamp = {Fri, 22 Nov 2024 16:34:27 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XiaLXTWLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Network (GNN) has emerged as a predominant tool for graph data analysis. Despite their proliferation, the low-quality labels of many real-world graphs will undermine their performance dramatically. Existing studies on learning neural networks with noisy labels mainly focus on independent data and thus cannot fully exploit the structural information of graph data. Currently, there are few studies of robustness to noisy labels for graph-structured data even if this problem is commonly seen in real-world settings. To remedy this deficiency, we propose GNN Cleaner which utilizes structural information of graph data to combat noisy labels. More specifically, a pseudo label is computed from the neighboring labels for each node in the training set via a modified version of label propagation. Additionally, a novel method is developed to learn to correct the labels adaptively and dynamically. Extensive experiments show that GNN Cleaner can train GNNs robustly and correct both the synthetic and real-world noisy labels even if the noise is severe. Moreover, GNN Cleaner is model-agnostic and can be combined with various GNNs to improve their robustness against label noise.}
}


@article{DBLP:journals/tkde/FangJLSWL24,
	author = {Wei Fang and
                  Haipeng Jiang and
                  Hengyang Lu and
                  Jun Sun and
                  Xiaojun Wu and
                  Jerry Chun{-}Wei Lin},
	title = {GPU-Based Efficient Parallel Heuristic Algorithm for High-Utility
                  Itemset Mining in Large Transaction Datasets},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {652--667},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3290371},
	doi = {10.1109/TKDE.2023.3290371},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/FangJLSWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heuristic algorithms have been developed to find approximate solutions for high-utility itemset mining (HUIM) problems that compensate for the performance bottlenecks of exact algorithms. However, heuristic algorithms still face the problem of long runtime and insufficient mining quality, especially for large transaction datasets with thousands to tens of thousands of items and up to millions of transactions. To solve these problems, a novel GPU-based efficient parallel heuristic algorithm for HUIM (PHA-HUIM) is proposed in this paper. The iterative process of PHA-HUIM consists of three main steps: the search strategy, fitness evaluation, and ring topology communication. The search strategy and ring topology communication are designed to run in constant time on GPU. The parallelism of fitness evolution helps to substantially accelerate the algorithm. A new data structure with a sort-mapping strategy is proposed to enhance the search ability and reduce memory usage. To improve the mining quality, a multi-start strategy with an unbalanced allocation strategy is employed in the search process. Ring topology communication is adopted to maintain population diversity. A load balancing strategy is introduced to reduce the thread divergence to improve the parallel efficiency. The experimental results on nine large datasets show that PHA-HUIM outperforms state-of-the-art HUIM algorithms in terms of speedup performance, runtime, and mining quality.}
}


@article{DBLP:journals/tkde/XuWYX24,
	author = {Yuanbo Xu and
                  En Wang and
                  Yongjian Yang and
                  Hui Xiong},
	title = {{GS-RS:} {A} Generative Approach for Alleviating Cold Start and Filter
                  Bubbles in Recommender Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {668--681},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3290140},
	doi = {10.1109/TKDE.2023.3290140},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XuWYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender Systems (RSs) typically face the cold-start problem and the filter-bubble problem when users suffer the familiar, repeated, and even predictable recommendations, making them bored and unsatisfied. The key to solving these issues is learning users’ fine-grained preferences and recommending appealing and unexplored items deviating from users’ historical items. However, existing models consider cold-start or filter bubble problems separately and ignore that they can reinforce mutually and damage the models’ performance accuracy. To this end, we devise a novel serendipity-oriented recommender system (Generative Self-constrained Serendipitous Recommender System, GS$^{2}$2-RS) that generates users’ fine-grained preferences to enhance the recommendation performance. Specifically, GS$^{2}$-RS extracts users’ interest and satisfaction preferences and generates virtual but convincible neighbors’ preferences from themselves with a twin Conditional Generative Adversarial Nets (not from real neighbors). Then we introduce the serendipity item, which is low-interest but high-satisfaction among candidate items. We use the serendipity item to improve the diversity of recommended items, which relieves the filter-bubble problem. Along with this line, a gated mechanism is applied to their fine-grained preferences (interests, satisfactions) to obtain their serendipity items. Finally, these serendipity items are inversely injected into the original user-item rating matrix and build a relatively dense matrix as the input for backbone RS models. Note that GS$^{2}$-RS tackles cold-start and filter-bubble problems in a unified framework without any additional side information and enriches the interpretability of recommendation models. We comprehensively validate GS$^{2}$-RS for solving cold-start and filter bubble problems on four real-world benchmark datasets. Extensive experiments illustrate GS$^{2}$-RS's superiority in accuracy, serendipity, and interpretability over state-of-the-art models. Also, we can plug our model into existing recommender systems as a preprocessing procedure to enhance their performance.}
}


@article{DBLP:journals/tkde/YangZWLHHZC24,
	author = {Ling Yang and
                  Jiayi Zheng and
                  Heyuan Wang and
                  Zhongyi Liu and
                  Zhilin Huang and
                  Shenda Hong and
                  Wentao Zhang and
                  Bin Cui},
	title = {Individual and Structural Graph Information Bottlenecks for Out-of-Distribution
                  Generalization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {682--693},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3290792},
	doi = {10.1109/TKDE.2023.3290792},
	timestamp = {Tue, 11 Feb 2025 20:50:07 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangZWLHHZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Out-of-distribution (OOD) graph generalization are critical for many real-world applications. Existing methods neglect to discard spurious or noisy features of inputs, which are irrelevant to the label. Besides, they mainly conduct instance-level class-invariant graph learning and fail to utilize the structural class relationships between graph instances. In this work, we endeavor to address these issues in a unified framework, dubbed Individual and Structural Graph Information Bottlenecks (IS-GIB). To remove class spurious feature caused by distribution shifts, we propose Individual Graph Information Bottleneck (I-GIB) which discards irrelevant information by minimizing the mutual information between the input graph and its embeddings. To leverage the structural intra- and inter-domain correlations, we propose Structural Graph Information Bottleneck (S-GIB). Specifically for a batch of graphs with multiple domains, S-GIB first computes the pair-wise input-input, embedding-embedding, and label-label correlations. Then it minimizes the mutual information between input graph and embedding pairs while maximizing the mutual information between embedding and label pairs. The critical insight of S-GIB is to simultaneously discard spurious features and learn invariant features from a high-order perspective by maintaining class relationships under multiple distributional shifts. Notably, we unify the proposed I-GIB and S-GIB to form our complementary framework IS-GIB. Extensive experiments conducted on both node- and graph-level tasks consistently demonstrate the superior generalization ability of IS-GIB.}
}


@article{DBLP:journals/tkde/LiuDNW24,
	author = {Jiongnan Liu and
                  Zhicheng Dou and
                  Jian{-}Yun Nie and
                  Ji{-}Rong Wen},
	title = {Integrated Personalized and Diversified Search Based on Search Logs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {694--707},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3291006},
	doi = {10.1109/TKDE.2023.3291006},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuDNW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized search and search result diversification are two possible solutions to cope with the query ambiguity problem in search engines. In most existing studies, they have been investigated separately, but intuitively, they address the problem from two complementary perspectives and should be combined. Some recent work tried to combine them by restricting result diversification to the subtopics corresponding to the user’s personal profile. However, diversification can be required even when the subtopics are outside the user’s profile. In this paper, we propose a more general approach to integrate them based on users’ implicit feedback in query logs. The proposed approach PER+DIV aggregates a document’s novelty score and personal relevance score dynamically according to how much the query falls into the user’s interests. To train the model based on user clicks in the logs, we consider user click as a result of both personal relevance and result diversity and a new method is proposed to isolate and model these two factors. To evaluate the model, we design several diversified and personalized metrics in addition to the traditional click-based metrics. Experimental results on a large-scale query log dataset show that the proposed integrated method significantly outperforms the existing personalization and diversification approaches.}
}


@article{DBLP:journals/tkde/LengSZP24,
	author = {Yan Leng and
                  Tara Sowrirajan and
                  Yujia Zhai and
                  Alex Pentland},
	title = {Interpretable Stochastic Block Influence Model: Measuring Social Influence
                  Among Homophilous Communities},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {708--714},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3289848},
	doi = {10.1109/TKDE.2023.3289848},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LengSZP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decision-making on networks can be explained by both homophily and social influences. While homophily drives the formation of communities with similar characteristics, social influences occur both within and between communities. Social influences can be reasoned through role theory, which indicates that the influences among individuals depending on their roles and the behavior of interest. To operationalize these social science theories, we empirically identify the homophilous communities and use the community structures to capture such “roles”, affecting particular decision-making processes. We propose a generative model named the Stochastic Block Influence Model and jointly analyze both network formation and behavioral influences within and between different empirically-identified communities. To evaluate the performance and demonstrate the interpretability of our method, we study the adoption decisions for a microfinance product in Indian villages. We show that although individuals tend to form links within communities, there are strongly positive and negative social influences between communities, supporting the weak ties theory. Moreover, communities with shared characteristics are associated with positive influences. In contrast, communities that do not overlap are associated with negative influences. Our framework facilitates the quantification of the influences underlying decision communities and is thus a helpful tool for driving information diffusion, viral marketing, and technology adoption.}
}


@article{DBLP:journals/tkde/ZhuLWJSWXY24,
	author = {Xiangru Zhu and
                  Zhixu Li and
                  Xiaodan Wang and
                  Xueyao Jiang and
                  Penglei Sun and
                  Xuwu Wang and
                  Yanghua Xiao and
                  Nicholas Jing Yuan},
	title = {Multi-Modal Knowledge Graph Construction and Application: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {715--735},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2022.3224228},
	doi = {10.1109/TKDE.2022.3224228},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuLWJSWXY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the resurgence of knowledge engineering which is featured by the fast growth of knowledge graphs. However, most of existing knowledge graphs are represented with pure symbols, which hurts the machine's capability to understand the real world. The multi-modalization of knowledge graphs is an inevitable key step towards the realization of human-level machine intelligence. The results of this endeavor are Multi-modal Knowledge Graphs (MMKGs). In this survey on MMKGs constructed by texts and images, we first give definitions of MMKGs, followed with the preliminaries on multi-modal tasks and techniques. We then systematically review the challenges, progresses and opportunities on the construction and application of MMKGs respectively, with detailed analyses of the strengths and weaknesses of different solutions. We finalize this survey with open research problems relevant to MMKGs.}
}


@article{DBLP:journals/tkde/ZhangDZCW24,
	author = {Wei Zhang and
                  Zhaohong Deng and
                  Te Zhang and
                  Kup{-}Sze Choi and
                  Shitong Wang},
	title = {Multi-View Fuzzy Representation Learning With Rules Based Model},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {736--749},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3295874},
	doi = {10.1109/TKDE.2023.3295874},
	timestamp = {Tue, 08 Oct 2024 21:35:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangDZCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsupervised multi-view representation learning has been studied extensively for mining multi-view data. However, some critical challenges remain. On the one hand, the existing methods cannot explore multi-view data comprehensively since they usually learn a common representation between views and ignore the specific information within each view. On the other hand, to mine the nonlinear relationship between the data, kernel or neural network methods are commonly used for multi-view representation learning but they lack interpretability. To this end, this paper proposes a new multi-view fuzzy representation learning method based on the interpretable Takagi-Sugeno-Kang (TSK) fuzzy system (MVRL_FS). The method realizes multi-view representation learning from two aspects. First, multi-view data are transformed into a high-dimensional fuzzy feature space, while the common information between views and specific information of each view are explored simultaneously. Second, a new regularization method based on {L}_{2,1}\n-norm regression is proposed to mine the consistency information between views, while the geometric structure of the data is preserved through the Laplacian graph. Extensive experiments on many benchmark multi-view datasets are conducted to validate the superiority of the proposed method.}
}


@article{DBLP:journals/tkde/PugoyK24,
	author = {Reinald Adrian Pugoy and
                  Hung{-}Yu Kao},
	title = {{NEAR:} Non-Supervised Explainability Architecture for Accurate Review-Based
                  Collaborative Filtering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {750--765},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2022.3226189},
	doi = {10.1109/TKDE.2022.3226189},
	timestamp = {Fri, 26 Jan 2024 07:56:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/PugoyK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is a critical issue in explainable recommender systems that compounds the challenges of explainability yet is rarely tackled: the lack of ground-truth explanation texts for training. It is unrealistic to expect every user-item pair in a dataset to have a corresponding target explanation. Hence, we pioneer the first non-supervised explainability architecture for review-based collaborative filtering (called NEAR) as our novel contribution to the theory of explanation construction in recommender systems. While maintaining excellent recommendation performance, our approach reformulates explainability as a non-supervised (i.e., unsupervised and self-supervised) explanation generation task. We formally define two explanation types, both of which NEAR can produce. An invariant explanation, fixed for all users, is based on the unsupervised extractive summary of an item's reviews via embedding clustering. Meanwhile, a variant explanation, personalized for a specific user, is a sentence-level text generated by our customized Transformer conditioned on every user-item-rating tuple and artificial ground-truth (self-supervised label) from one of the invariant explanation's sentences. Our empirical evaluation illustrates that NEAR's rating prediction accuracy is better than the other state-of-the-art baselines. Moreover, experiments and assessments show that NEAR-generated variant explanations are more personalized and distinct than those from other Transformer-based models, and our invariant explanations are preferred over those from other contemporary models in real life.}
}


@article{DBLP:journals/tkde/TaoLL24,
	author = {Yuechen Tao and
                  Bo Li and
                  Baochun Li},
	title = {On Atomicity and Confidentiality Across Blockchains Under Failures},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {766--780},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3255842},
	doi = {10.1109/TKDE.2023.3255842},
	timestamp = {Sun, 19 Jan 2025 13:53:45 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/TaoLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed applications that utilize heterogeneous blockchain systems have the potential to be widely deployed. In such applications, users from different blockchains can transact with one another through cross-chain transactions. There are two essential features of particular relevance for those applications during cross-chain transactions: the atomicity in that either all or none of the blockchains involved confirm a cross-chain transaction, the confidentiality in that a blockchain involved in a cross-chain transaction is only accessible for designated users. Existing cross-chain proposals have largely relied on permissioned blockchains to ensure confidentiality. However, we found that failures could occur when reading or writing information during transaction confirmations across permissioned blockchains, namely read/write (r/w) failures, which can lead to the violation of atomicity. In this paper, we propose a novel mechanism, Unity, to ensure both atomicity and confidentiality of cross-chain transactions under r/w failures by leveraging permissioned blockchains. When failures occur in reading or writing data, Unity classifies the data into two categories based on its status - whether data is the latest version or not, and presents different solutions for atomicity. Specifically, when data is not the latest, we design a four-phase-commit protocol (\\text{4pc}\n), in which consensus on confirming or aborting a cross-chain transaction can be achieved. If data is the latest when r/w failures occur, we propose a smart contract based solution (\\text{SSC}\n). We examine the effectiveness of Unity theoretically and through experiments. With a failure probability of 0.7, Unity achieves 98% more atomic cross-chain transactions when compared with the state-of-the-art cross-chain platform, Hyperservice.}
}


@article{DBLP:journals/tkde/ZhangLMLRM24,
	author = {Man Zhang and
                  Xinghua Li and
                  Yinbin Miao and
                  Bin Luo and
                  Yanbing Ren and
                  Siqi Ma},
	title = {{PEAK:} Privacy-Enhanced Incentive Mechanism for Distributed -Anonymity
                  in {LBS}},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {781--794},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3295451},
	doi = {10.1109/TKDE.2023.3295451},
	timestamp = {Fri, 14 Feb 2025 20:58:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLMLRM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To motivate users’ assistance for protecting others’ location privacy by distributed K-anonymity in Location-Based Service (LBS), many incentive mechanisms have been proposed, where users obtain monetary compensation for their assistance. However, most existing distributed K-anonymity incentive mechanisms rely on trusted third parties and ignore users’ malicious strategies, which destroys LBS's distributed structure as well as leads to users’ privacy leakage and incentive ineffectiveness. To solve the above problems, we propose a Privacy-Enhanced incentive mechAnism for distributed K-anonymity (PEAK). With determining the monetary transaction relationship and location transmission between users, PEAK enables the anonymous cloaking region construction without the trusted server. Meanwhile, PEAK devises role identification mechanism and accountability mechanism to restrain and punish malicious users, which protects users’ location privacy and implements effective motivation on users’ assistance. Theoretical analysis based on the game theory shows that PEAK constrains users’ malicious strategies while satisfying individual rationality, computational efficiency, and satisfaction ratio. Extensive experiments based on the real-world dataset demonstrate that PEAK improves security and feasibility, especially reaching the success rate of anonymous cloaking region construction to more than 90\\% and decreasing the malicious users’ utilities significantly.}
}


@article{DBLP:journals/tkde/XieZSZZS24,
	author = {Hong Xie and
                  Mingze Zhong and
                  Xiaoyu Shi and
                  Xiaoying Zhang and
                  Jiang Zhong and
                  Mingsheng Shang},
	title = {Probabilistic Modeling of Assimilate-Contrast Effects in Online Rating
                  Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {795--808},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3292352},
	doi = {10.1109/TKDE.2023.3292352},
	timestamp = {Wed, 13 Nov 2024 15:45:07 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XieZSZZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online rating system serves as an indispensable building block for many web applications. Previous studies showed that due to assimilate-contrast effects, historical ratings could significantly distort users’ ratings, leading to low accuracy of product quality estimation and recommendation. To understand assimilate-contrast effects, an “accurate” model is still missing as previous models do not capture important factors like rating recency, selection bias, etc. Furthermore, an analytical framework to characterize product estimation accuracy under assimilate-contrast effects is also missing. This paper aims to fill in this gap. We propose a probabilistic model to quantify the aforementioned important factors on assimilate-contrast effects. We apply stochastic approximation theory to show that when the rating bias satisfies mild contraction conditions, the aggregate rating converges under aggregate opinion heterogeneity. We also apply non-stationary Markov chain theory to show that when the strength of assimilate-contrast satisfies mild stable conditions, the aggregate rating converges under rating recency. We also derive an equation to characterize the converged aggregate ratings. These conditions reveal important insights on how the aforementioned factors influence the convergence and guide the online rating system operator to design appropriate rating aggregation rules and rating displaying strategies. We apply it to rating prediction tasks and product recommendation tasks. Experiment results on four public datasets show that our model can improve the rating prediction and recommendation accuracy over previous models significantly, under various metrics like RMSE, NDCG, etc. We also demonstrate the flexibility of our model by showing that it can be applied to enhance other rating behavior models.}
}


@article{DBLP:journals/tkde/TangLHGXL24,
	author = {Yifu Tang and
                  Jianxin Li and
                  Nur Al Hasan Haldar and
                  Ziyu Guan and
                  Jiajie Xu and
                  Chengfei Liu},
	title = {Reliability-Driven Local Community Search in Dynamic Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {809--822},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3290295},
	doi = {10.1109/TKDE.2023.3290295},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/TangLHGXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community search over large dynamic graph has become an important research problem in modern complex networks, such as the online social network, collaboration network and biological networks. Network data in the time-varied environment has motivated several recent studies to identify the evolution of the communities. However, these studies mostly match communities of different snapshot or utilize the aggregation of the disjoint structural information and ignores the cohesion continuity. To fill this research gap, in this work, we propose a novel (\\theta,k)-core reliable community (CRC) and define the reliable community search problem which jointly considers member engagement, connection strength and cohesion continuity of the community in the dynamic network. We propose an online search algorithm based on eligible edge filtering and we further construct the Weighted Core Forest-Index (WCF-index) and develop efficient index-based querying algorithm with strong pruning properties. We also propose top-l reliable community search problem that couples query based distance to reduce the free rider effect in local community search and support flexible multiple query vertices. Extensive experiments are conducted to show the efficiency and effectiveness of the proposed algorithms.}
}


@article{DBLP:journals/tkde/NiGZLS24,
	author = {Li Ni and
                  Junnan Ge and
                  Yiwen Zhang and
                  Wenjian Luo and
                  Victor S. Sheng},
	title = {Semi-Supervised Local Community Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {823--839},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3290095},
	doi = {10.1109/TKDE.2023.3290095},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NiGZLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Owing to the lack of a universal definition of communities, some semi-supervised community detection approaches learn the concept of community structures from known communities, and then dig out communities using learned concepts of communities. In some cases, users are only interested in the community containing a given node. However, communities detected by these semi-supervised approaches may not contain a given node. Besides, these methods traverse the entire network to detect many communities and cost more resources than a local algorithm. Therefore, it is necessary and meaningful to find the local community that contains a given node with prior information on the local network around the given node. We call this a Semi-supervised Local Community Detection (SLCD) problem. In this paper, prior information refers to certain known communities. To address the SLCD problem, we propose the Semi-supervised Local community detection with the Structural Similarity algorithm, called SLSS, which uses some known communities instead of all known communities. The idea of SLSS is to use the structural similarity between the known communities and the detected community, calculated by the graph kernel, to guide the expansion of the community. Experimental results show that SLSS outperforms other algorithms on six real-world datasets.}
}


@article{DBLP:journals/tkde/LiuWWYDZW24,
	author = {Chunyu Liu and
                  Wei Wu and
                  Siyu Wu and
                  Lu Yuan and
                  Rui Ding and
                  Fuhui Zhou and
                  Qihui Wu},
	title = {Social-Enhanced Explainable Recommendation With Knowledge Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {840--853},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3292504},
	doi = {10.1109/TKDE.2023.3292504},
	timestamp = {Wed, 07 Aug 2024 07:51:02 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuWWYDZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation systems are of crucial importance due to their wide applications. Knowledge graph (KG) enabled recommendation schemes have attracted great attention due to their superior performance and interpretability. However, the rich social information is not exploited for those systems, which limits the recommendation performance (Wang et al., 2023). In this paper, a novel explainable recommendation scheme is proposed by exploiting our designed social enhanced knowledge graph attention network (SKGAN). The hidden relations among users and items are learned and used for recommendation with the collaborative KG (CKG) and the user social graph (USG). Moreover, the high-order semantic information in both CKG and USG are obtained by using the graph convolution networks (GCNs) and the node level attention algorithm. Furthermore, a graph level user-specific attention algorithm is proposed to capture the user personalized preference between CKG and USG. Extensive experiment results demonstrate that normalized discounted cumulative gain (NDCG), precision, recall and hits ratio (HR) achieved with our proposed recommendation system are the best among those obtained with the state-of-the-art benchmark recommendation systems.}
}


@article{DBLP:journals/tkde/YaoPT24,
	author = {Yuxin Yao and
                  Eann A. Patterson and
                  Richard J. Taylor},
	title = {The Influence of Digital Technologies on Knowledge Management in Engineering:
                  {A} Systematic Literature Review},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {854--867},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3285952},
	doi = {10.1109/TKDE.2023.3285952},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YaoPT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital technologies are gaining widespread acceptance in engineering and offer opportunities for collating and curating knowledge during and beyond the life cycle of engineering products. Knowledge is central to strategy and operations in most engineering organizations and digital technologies have been employed in attempts to improve current knowledge management practices. A systematic literature review was undertaken to address the question: how do digital technologies influence knowledge management in the engineering sector? Twenty-seven primary studies were identified from 3097 papers on these topics within the engineering literature published between 2010 and 2022. Four knowledge management processes supported by digital technologies were recognized: knowledge creation, storage and retrieval, sharing and application. In supporting knowledge management, digital technologies were found to have been acting in five roles: repositories, transactive memory systems, communication spaces, boundary objects and non-human actors. However, the ability of digital technologies to perform these roles simultaneously had not been considered and similarly knowledge management had not been addressed as a holistic process. Hence, it was concluded that a holistic approach to knowledge management combined with the deployment of digital technologies in multiple roles simultaneously would likely yield significant competitive advantage and organizational value for organizations in the engineering sector.}
}


@article{DBLP:journals/tkde/LiWLY24,
	author = {Wen{-}Zhi Li and
                  Chang{-}Dong Wang and
                  Jian{-}Huang Lai and
                  Philip S. Yu},
	title = {Towards Effective and Robust Graph Contrastive Learning With Graph
                  Autoencoding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {868--881},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3288280},
	doi = {10.1109/TKDE.2023.3288280},
	timestamp = {Thu, 13 Feb 2025 10:32:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiWLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph contrastive learning (GCL) has become the de-facto approach to conducting self-supervised learning on graphs for its superior performance. However, non-semantic graph augmentation methods prevent it from achieving better performance, and it suffers from vulnerability to graph attacks. To deal with these problems, we propose AEGCL to leverage graph AutoEncoder in Graph Contrastive Learning which directly targets graph property reconstruction to boost GCL effectiveness and robustness. Specifically, AEGCL has two distinctive characteristics, (1) a novel adaptive augmentation strategy based on motif centrality is proposed, which leverages semantic significant higher-order graph property; (2) the original attributed graph is decoupled into feature graph and topology graph to extract their dedicated information, and a simple AttnFuse is proposed to combine the two augmented graphs and the two decoupled graphs. Graph autoencoder can thus be applied to the topology domain and raw attribute domain. Empirically, extensive experiments on benchmark graph datasets show that AEGCL outperforms existing baseline methods in terms of classification accuracy and robustness.}
}


@article{DBLP:journals/tkde/DeraABR24,
	author = {Dimah Dera and
                  Sabeen Ahmed and
                  Nidhal Carla Bouaynaya and
                  Ghulam Rasool},
	title = {TRustworthy Uncertainty Propagation for Sequential Time-Series Analysis
                  in RNNs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {882--896},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3288628},
	doi = {10.1109/TKDE.2023.3288628},
	timestamp = {Fri, 08 Mar 2024 13:21:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DeraABR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The massive time-series production through the Internet of Things and digital healthcare requires novel data modeling and prediction. Recurrent neural networks (RNNs) are extensively used for analyzing time-series data. However, these models are unable to assess prediction uncertainty, which is particularly critical in heterogeneous and noisy environments. Bayesian inference allows reasoning about predictive uncertainty by estimating the posterior distribution of the parameters. The challenge remains in propagating the high-dimensional distribution through the sequential, non-linear layers of RNNs, resulting in mode collapse leading to erroneous uncertainty estimation and exacerbating the gradient explosion problem. This paper proposes a TRustworthy Uncertainty propagation for Sequential Time-series analysis (TRUST) in RNNs by introducing a Gaussian prior over network parameters and estimating the first two moments of the Gaussian variational distribution using the evidence lower bound. We propagate the variational moments through the sequential, non-linear layers of RNNs using the first-order Taylor approximation. The propagated covariance of the predictive distribution captures uncertainty in the output decision. The extensive experiments using ECG5000 and PeMS-SF classification and weather and power consumption prediction tasks demonstrate 1) significant robustness of TRUST-RNNs against noise and adversarial attacks and 2) self-assessment through the uncertainty that increases significantly with increasing noise.}
}


@article{DBLP:journals/tkde/ChenFLZKHX24,
	author = {Yonghao Chen and
                  Xiaozhao Fang and
                  Yuanyuan Liu and
                  Wei Zheng and
                  Peipei Kang and
                  Na Han and
                  Shengli Xie},
	title = {Two-Step Strategy for Domain Adaptation Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {897--912},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3289882},
	doi = {10.1109/TKDE.2023.3289882},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenFLZKHX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conventional hash-based retrieval method rely on the assumption that the query and database are of the identical domain. However, cross-domain problem often occurs in real-world applications, leading to the unsatisfactory performance of existing hashing methods. Recently, some researchers have put forward domain adaptation retrieval (DAR) under the perspective of domain adaptation (DA) and achieved promising results. But the following limitations still exist: 1) a single function is used to handle two challenges, i.e., domain adaptation and hashing, which is not flexible to explore enough underlying information for simultaneously accomplishing these challenges well; 2) non-dominant features in the sample are ignored; 3) the dissimilarity structure of dissimilar samples is not taken into account. To address the above problems, we propose a novel framework named two-step strategy (TSS) for domain adaptation retrieval, which advocates dividing DAR into two steps: DA step and hashing step. A DA function and a hash function are learned to handle the above two challenges, respectively, making the process more reasonable. Additionally, a discriminant semantic fusion loss is proposed to improve the discriminative ability among classes. Unlike other works that focus on discovering dominant features, we exploit the neglected non-dominant features and assign them attention with sinusoidal semantic embedding, actively creating a clear separation between classes. At last, we present an adaptive similarity preserving loss to preserve the similarity structure of the original data in all intra-domain and inter-domain hash codes. Extensive experiments on various datasets demonstrate that the proposed TSS achieves state-of-the-art performance.}
}


@article{DBLP:journals/tkde/YuXCCHY24,
	author = {Junliang Yu and
                  Xin Xia and
                  Tong Chen and
                  Lizhen Cui and
                  Nguyen Quoc Viet Hung and
                  Hongzhi Yin},
	title = {XSimGCL: Towards Extremely Simple Graph Contrastive Learning for Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {2},
	pages = {913--926},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3288135},
	doi = {10.1109/TKDE.2023.3288135},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YuXCCHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contrastive learning (CL) has recently been demonstrated critical in improving recommendation performance. The underlying principle of CL-based recommendation models is to ensure the consistency between representations derived from different graph augmentations of the user-item bipartite graph. This self-supervised approach allows for the extraction of general features from raw data, thereby mitigating the issue of data sparsity. Despite the effectiveness of this paradigm, the factors contributing to its performance gains have yet to be fully understood. This paper provides novel insights into the impact of CL on recommendation. Our findings indicate that CL enables the model to learn more evenly distributed user and item representations, which alleviates the prevalent popularity bias and promoting long-tail items. Our analysis also suggests that the graph augmentations, previously considered essential, are relatively unreliable and of limited significance in CL-based recommendation. Based on these findings, we put forward an eXtremely Simple Graph Contrastive Learning method (XSimGCL) for recommendation, which discards the ineffective graph augmentations and instead employs a simple yet effective noise-based embedding augmentation to generate views for CL. A comprehensive experimental study on four large and highly sparse benchmark datasets demonstrates that, though the proposed method is extremely simple, it can smoothly adjust the uniformity of learned representations and outperforms its graph augmentation-based counterparts by a large margin in both recommendation accuracy and training efficiency.}
}


@article{DBLP:journals/tkde/LuoLLGRMM24,
	author = {Bin Luo and
                  Xinghua Li and
                  Ximeng Liu and
                  Jingjing Guo and
                  Yanbing Ren and
                  Siqi Ma and
                  Jianfeng Ma},
	title = {{\textdollar}D{\^{}}\{2\}MTS{\textdollar}: Enabling Dependable Data
                  Collection With Multiple Crowdsourcers Trust Sharing in Mobile Crowdsensing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {927--942},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3294503},
	doi = {10.1109/TKDE.2023.3294503},
	timestamp = {Sun, 19 Jan 2025 13:53:46 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LuoLLGRMM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When enjoying mobile crowdsensing (MCS), it is vital to evaluate the trustworthiness of mobile users (MUs) without disclosing their sensitive information. However, the existing schemes ignore this requirement in the multiple crowdsourcers (CSs) scenario. The lack of a credible sharing about MUs’ trustworthiness results in an inaccurate trust evaluation, disabling allocating tasks to reliable MUs. To address it, based on the analysis of the desired properties, we propose a scheme enabling dependable data collection with multiple crowdsourcers trust sharing (D^{2}MTS\n). Specifically, we design the MU anonymous management. Two kinds of MU generated pseudonym systems without relationships are presented to mark each MU in trust evaluation and task execution, respectively. Through the devised pseudonym changes on these pseudonyms and the common token distribution algorithm, D^{2}MTS\nrealizes privacy-preserving trust sharing. Moreover, to guarantee credible sharing, based on the hash chain, D^{2}MTS\nrecords MUs’ trustworthiness with the unforgeable signature on the blockchain established by multiple CSs which do not trust each other naturally. Extensive experiments show that compared with the other works, D^{2}MTS\n's detection ratio of vicious MUs and the percentage of reliable MUs among the selected ones can increase by 208.61% and 28.27%. Both computational and communication delays are limited.}
}


@article{DBLP:journals/tkde/QuGXLWH24,
	author = {Xiaoye Qu and
                  Yingjie Gu and
                  Qingrong Xia and
                  Zechang Li and
                  Zhefeng Wang and
                  Baoxing Huai},
	title = {A Survey on Arabic Named Entity Recognition: Past, Recent Advances,
                  and Future Trends},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {943--959},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303136},
	doi = {10.1109/TKDE.2023.3303136},
	timestamp = {Tue, 27 Aug 2024 10:27:51 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QuGXLWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As more and more Arabic texts emerged on the Internet, extracting important information from these Arabic texts is especially useful. As a fundamental technology, Named entity recognition (NER) serves as the core component in information extraction technology, while also playing a critical role in many other Natural Language Processing (NLP) systems, such as question answering and knowledge graph building. In this paper, we provide a comprehensive review of the development of Arabic NER, especially the recent advances in deep learning and pre-trained language model. Specifically, we first introduce the background of Arabic NER, including the characteristics of Arabic and existing resources for Arabic NER. Then, we systematically review the development of Arabic NER methods. Traditional Arabic NER systems focus on feature engineering and designing domain-specific rules. In recent years, deep learning methods achieve significant progress by representing texts via continuous vector representations. With the growth of pre-trained language model, Arabic NER yields better performance. Finally, we conclude the method gap between Arabic NER and NER methods from other languages, which helps outline future directions for Arabic NER.}
}


@article{DBLP:journals/tkde/FettalLN24,
	author = {Chakib Fettal and
                  Lazhar Labiod and
                  Mohamed Nadif},
	title = {Boosting Subspace Co-Clustering via Bilateral Graph Convolution},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {960--971},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3300814},
	doi = {10.1109/TKDE.2023.3300814},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/FettalLN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subspace clustering seeks to cluster high-dimensional data lying in a union of low-dimensional subspaces. It has achieved state-of-the-art results in image clustering, but text clustering of document-term matrices, has proved more impervious to advances with this approach, even though text data satisfies the assumptions of subspace clustering. We hypothesize that this is because such matrices are generally sparser and higher-dimensional than images. This, combined with the complexity of subspace clustering, which is generally cubic in the number of inputs, makes its use impractical in the context of text. Here we address these issues with a view to leveraging subspace clustering for networked (or not) text data. We first extend the concept of subspace clustering to co-clustering, which is suitable to deal with document-term matrices because of the interplay engendered between the document and word representations. We then address the sparsity problem through bilateral graph convolution, which promotes the grouping effect that has been credited for the effectiveness of some subspace clustering models. The proposed formulation results in an algorithm that is computationally/spatially efficient. Experiments using real-world datasets demonstrate the superior performance, in terms of document clustering, word clustering, and computational efficiency, of our proposed approach over the baselines and comparable methods.}
}


@article{DBLP:journals/tkde/TanZHCLH24,
	author = {Qiaoyu Tan and
                  Xin Zhang and
                  Xiao Huang and
                  Hao Chen and
                  Jundong Li and
                  Xia Hu},
	title = {Collaborative Graph Neural Networks for Attributed Network Embedding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {972--986},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3298002},
	doi = {10.1109/TKDE.2023.3298002},
	timestamp = {Sat, 04 May 2024 10:55:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TanZHCLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborative graph Neural Networks–CONN, a tailored GNN architecture for attribute network embedding. It improves model capacity by 1) selectively diffusing messages from neighboring nodes and involved attribute categories, and 2) jointly reconstructing node-to-node and node-to-attribute-category interactions via cross-correlation. Experiments on real-world networks demonstrate that CONN excels state-of-the-art embedding algorithms with a great margin.}
}


@article{DBLP:journals/tkde/ZhangYY24,
	author = {Shijie Zhang and
                  Wei Yuan and
                  Hongzhi Yin},
	title = {Comprehensive Privacy Analysis on Federated Recommender System Against
                  Attribute Inference Attacks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {987--999},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3295601},
	doi = {10.1109/TKDE.2023.3295601},
	timestamp = {Mon, 26 Aug 2024 12:21:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, recommender systems are crucially important for the delivery of personalized services that satisfy users’ preferences. With personalized recommendation services, users can enjoy a variety of recommendations such as movies, books, ads, restaurants, and more. Despite the great benefits, personalized recommendations typically require the collection of personal data for user modelling and analysis, which can make users susceptible to attribute inference attacks. Specifically, the vulnerability of existing centralized recommenders under attribute inference attacks leaves malicious attackers a backdoor to infer users’ private attributes, as the systems remember information of their training data (i.e., interaction data and side information). An emerging practice is to implement recommender systems in the federated setting, which enables all user devices to collaboratively learn a shared global recommender while keeping all the training data on device. However, the privacy issues in federated recommender systems have been rarely explored. In this paper, we first design a novel attribute inference attacker to perform a comprehensive privacy analysis of the GCN-based federated recommender models. The experimental results show that the vulnerability of each model component against attribute inference attack is varied, highlighting the need for new defense approaches. Therefore, we propose a novel adaptive privacy-preserving approach to protect users’ sensitive data in the presence of attribute inference attacks and meanwhile maximize the recommendation accuracy. Extensive experimental results on two real-world datasets validate the superior performance of our model on both recommendation effectiveness and resistance to inference attacks.}
}


@article{DBLP:journals/tkde/TianZZ24,
	author = {Yao Tian and
                  Xi Zhao and
                  Xiaofang Zhou},
	title = {{DB-LSH} 2.0: Locality-Sensitive Hashing With Query-Based Dynamic
                  Bucketing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1000--1015},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3295831},
	doi = {10.1109/TKDE.2023.3295831},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/TianZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Locality-sensitive hashing (LSH) is a promising family of methods for the high-dimensional approximate nearest neighbor (ANN) search problem due to its sub-linear query time and strong theoretical guarantee. Existing LSH methods either suffer from large index sizes and hash boundary problems, or incur a linear cost for high-quality candidate identification. This dilemma is addressed in a novel method called DB-LSH proposed in this paper. It organizes the projected spaces with multi-dimensional indexes instead of fixed-width hash buckets, which significantly reduces space costs. High-quality candidates can be generated efficiently by dynamically constructing query-based hypercubic buckets with the required widths through index-based window queries. A novel incremental search strategy called DBI-LSH is also developed to further boost the query performance, which incrementally accesses the next best point for higher accuracy and efficiency. Considering the intermediate query information of each query, DBA-LSH is designed to adaptively tune termination conditions without scarifying the success probability. Our theoretical analysis proves that DB-LSH has a smaller query cost than the existing work while DBA-LSH and DBI-LSH have lower expected query costs than DB-LSH. An extensive range of experiments on real-world data show the superiority of our approaches over the state-of-the-art methods in both efficiency and accuracy.}
}


@article{DBLP:journals/tkde/ChenWCZZL24,
	author = {Han Chen and
                  Hanchen Wang and
                  Hongmei Chen and
                  Ying Zhang and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {Denoising Variational Graph of Graphs Auto-Encoder for Predicting
                  Structured Entity Interactions},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1016--1029},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3298490},
	doi = {10.1109/TKDE.2023.3298490},
	timestamp = {Mon, 29 Jul 2024 16:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenWCZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The interactions between structured entities play important roles in a wide range of applications such as chemistry, material science, biology, and medical science. Recently, graph-based methods have been exploited to effectively predict the interactions among structured entities. However, these methods usually only focus on structural information of the entities and are incapable of fully utilizing the interaction information between the entities. In this paper, we propose a Denoising Variational Graph of Graphs Auto-encoder (DVGGA) which follows the graph of graphs framework to capture both structural information in structured entities and interaction information among structured entities. With denoising criterion, DVGGA is able to capture the information from the useful structures of the local graph and address the overfitting issue caused by redundant substructures. Extensive experiments conducted on real-world datasets show that DVGGA outperforms the state-of-the-art structured entity interaction prediction methods.}
}


@article{DBLP:journals/tkde/ShiJLTLY24,
	author = {Chuan Shi and
                  Houye Ji and
                  Zhiyuan Lu and
                  Ye Tang and
                  Pan Li and
                  Cheng Yang},
	title = {Distance Information Improves Heterogeneous Graph Neural Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1030--1043},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3300879},
	doi = {10.1109/TKDE.2023.3300879},
	timestamp = {Fri, 31 Jan 2025 14:08:46 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ShiJLTLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous graph neural network (HGNN) has shown superior performance and attracted considerable research interest. However, HGNN inherits the limitation of expressive power from GNN via learning individual node embeddings based on their structural neighbors, largely ignoring the potential correlations between nodes and leading to sub-optimal performance. How to establish correlations among multiple node embeddings and improve the expressive power of HGNN is still an open problem. To solve the above problem, we propose a simple and effective technique called heterogeneous distance encoding (HDE) to fundamentally improve the expressive power of HGNN. Specifically, we define heterogeneous shortest path distance to describe the relative distance between nodes, and then jointly encode such distances for multiple nodes of interest to establish their correlation. By simply injecting the encoded correlation into the neighbor aggregating process, we can learn more expressive heterogeneous graph representations for downstream tasks. More importantly, the proposed HDE relies only on the graph structure and ensures the inductive ability of HGNN. We also propose an efficient HDE algorithm that can significantly reduce the computational overhead. Significant improvements on both transductive and inductive tasks over four real-world graphs demonstrate the effectiveness of HDE in improving the expressive power of HGNN.}
}


@article{DBLP:journals/tkde/DuG24,
	author = {Jiancheng Du and
                  Yang Gao},
	title = {Domain Adaptation and Summary Distillation for Unsupervised Query
                  Focused Summarization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1044--1055},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3296441},
	doi = {10.1109/TKDE.2023.3296441},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DuG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Text summarizing is the task of reducing a document's length while maintaining its essential information. In the age of information explosion, how to obtain the content that users needed from a large volume of information becomes particularly significant. Under such circumstances, query-focused abstractive summarization (qfs) becomes more dominant since it is able to focus on user needs while delivering fluent, concise, succinct paraphrased summaries. However, unlike generic summarization, which has achieved remarkable progress driven by a substantial amount of parallel data, the qfs struggles due to a deficiency of parallel corpus. Therefore, in this paper, we leverage a typical large generic summarization dataset to facilitate the pressing demands on unsupervised qfs. The large-scale query-free benchmark is automatically transformed into a query-focused dataset (Query-CNNDM) while preserving its informative summaries. We propose a simple yet effective unsupervised method, called Domain Adaptation and Summary Distillation method (DASD). In the model, to achieve the domain adaptation for unsupervised qfs, we design a query-aware gap sentence generation (q-GSG) strategy to equip the model with the capability of learning target textual knowledge and obtaining a good initialization at the target domain. As instance-specific regularization, we train a teacher model with the Query-CNNDM to generate pseudo-labels for summary distillation. Experimental results indicate that our DASD model achieves state-of-the-art performance on two benchmark datasets, Debatepedia and Wikiref, in a zero-shot setting and shows good generalization to the abstractive few-shot qfs.}
}


@article{DBLP:journals/tkde/ZhangZWY24,
	author = {Hongbin Zhang and
                  Qixin Zhang and
                  Feng Wu and
                  Yu Yang},
	title = {Dynamic Assortment Selection Under Inventory and Limited Switches
                  Constraints},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1056--1068},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3301649},
	doi = {10.1109/TKDE.2023.3301649},
	timestamp = {Tue, 05 Mar 2024 16:08:55 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optimizing the assortment of products to display to customers is key to increasing revenue for both offline and online retailers. To trade-off between exploring customers’ preferences and exploiting customers’ choices learned from data, in this article, by adopting the Multi-Nomial Logit (MNL) choice model to capture customers’ choices over products, we study the problem of optimizing assortments over a planning horizon T for maximizing the profit of the retailer. To make the problem setting more practical, we consider both the inventory constraint and the limited switches constraint, where the retailer is forced to stop the sales when the resources are depleted and is forbidden to switch the assortment shown to customers too many times. Such a setting suits the case when an online retailer wants to optimize the assortment selection for a population of customers dynamically. We develop an efficient UCB-like algorithm to optimize the assortments while learning customers’ choices from data. We prove that our algorithm can achieve a sub-linear regret bound \\tilde{O}(T^{\\max \\lbrace 2/3-\\alpha /3,1/2\\rbrace }) if O(T^\\alpha) switches are allowed. Extensive numerical experiments show that our algorithm outperforms baselines, and the gap between our algorithm's performance and the theoretical upper bound is small.}
}


@article{DBLP:journals/tkde/SunWWCZL24,
	author = {Renjie Sun and
                  Yanping Wu and
                  Xiaoyang Wang and
                  Chen Chen and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {Efficient Balanced Signed Biclique Search in Signed Bipartite Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1069--1083},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3296721},
	doi = {10.1109/TKDE.2023.3296721},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunWWCZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding bicliques is a fundamental problem in bipartite graph analysis, and can find numerous applications. However, previous studies only focus on unsigned bipartite graphs. Signed information, such as friend and enemy, naturally exists in real-world networks. It is critical to leverage signed information to better characterize biclique. To fill this gap, we propose a novel biclique model, named balanced signed biclique, by leveraging the property of balance theory. Specifically, given a signed bipartite graph G and two positive integers \\tau _{U}, \\tau _{V}, a subgraph S=(U_{S},V_{S},E_{S}) of G is a balanced signed biclique if i) S is a biclique without any unstable motif, i.e., unbalanced butterfly, and ii) |U_{S}| \\geq \\tau _{U} and |V_{S}| \\geq \\tau _{V}. In this paper, we propose and investigate two important problems, i.e., maximal balanced signed biclique enumeration and maximum balanced signed biclique identification. Due to the unique features of signed bipartite graphs, the previous works cannot be applied to our problems directly. For the enumeration task, to construct a reasonable baseline, we extend the existing biclique enumeration framework for unsigned bipartite graphs and integrate the developed balanced bipartite graph property. To scale for large networks, optimized strategies are proposed to overcome the three limitations in the baseline method. For the identification task, we first propose a baseline method by leveraging the proposed enumeration framework. Moreover, employing novel optimizations, an anchor balanced bipartite graph based search framework is introduced to accelerate the search. Finally, extensive experiments are conducted on 8 real-world datasets to demonstrate the efficiency and effectiveness of the proposed techniques and model.}
}


@article{DBLP:journals/tkde/CaoCXL24,
	author = {Fuyuan Cao and
                  Qingqiang Chen and
                  Ying Xing and
                  Jiye Liang},
	title = {Efficient Classification by Removing Bayesian Confusing Samples},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1084--1098},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303425},
	doi = {10.1109/TKDE.2023.3303425},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CaoCXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Improving the generalization performance of classifiers from data pre-processing perspective has recently received considerable attention in the machine learning community. Although many methods have been proposed in the past decades, most of them lack theoretical foundations and cannot guarantee better generalization performance of classifiers on processed datasets. To overcome this flaw, in this paper, we propose a method, which is supported by Bayesian decision theory and percolation theory, to improve generalization performance by removing Bayesian confusing samples (abbr. BCS). Specifically, for a training set, we define the samples that misclassified by the Bayesian optimal classifier as BCS and prove that a classifier trained on the training set after removing BCS can obtain better generalization performance. To find out BCS, we indicate that BCS can be identified according to the size of global homogeneous cluster, a set of samples with the same labels, based on percolation theory. Based on these analysis, we propose a method to construct global homogeneous clusters and remove BCS from the training set. Extensive experiments show that the proposed method is effective for a number of classical and state-of-the-art classifiers.}
}


@article{DBLP:journals/tkde/WangHCCT24,
	author = {Yu Wang and
                  Liang Hu and
                  Xiaofeng Cao and
                  Yi Chang and
                  Ivor W. Tsang},
	title = {Enhancing Locally Adaptive Smoothing of Graph Neural Networks Via
                  Laplacian Node Disagreement},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1099--1112},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303212},
	doi = {10.1109/TKDE.2023.3303212},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangHCCT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) are designed to perform inference on data described by graph-structured node features and topology information. From the perspective of graph signal denoising, the typical message passing schemes of GNNs act as a globally uniform smoothing that minimizes disagreements between embeddings of connected nodes. However, the level of smoothing over different regions of the graph should be different, especially for those inter-class regions. This deviation limits the expressiveness of GNNs, and then renders them fragile to over-smoothing, long-range dependencies, and non-homophily settings. In this paper, we find that the node disagreements of initial graph features can present more trustworthy constraints on node embeddings, thereby enhancing the locally adaptive smoothing of GNNs. To spread the inherent disagreements of nodes, we propose the Laplacian node disagreement to jointly measure the initial features and output embeddings. With such a measurement, we then present a new graph signal denoising objective deriving a more effective message passing scheme and further incorporate it into the GNN architecture, named Laplacian node disagreement-based GNN (LND-GNN). Learning from its output node representations, we integrate an auxiliary disagreement constraint into the overall classification loss. Experiments demonstrate the expressive ability of LND-GNN in the downstream semi-supervised node classification task.}
}


@article{DBLP:journals/tkde/YangQRC24,
	author = {Dingqi Yang and
                  Bingqing Qu and
                  Paolo Rosso and
                  Philippe Cudr{\'{e}}{-}Mauroux},
	title = {Fast and Slow Thinking: {A} Two-Step Schema-Aware Approach for Instance
                  Completion in Knowledge Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1113--1129},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3304137},
	doi = {10.1109/TKDE.2023.3304137},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangQRC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern Knowledge Graphs (KG) often suffer from an incompleteness issue (i.e., missing facts). By representing a fact as a triplet $(h,r,t)$ linking two entities $h$ and $t$ via a relation $r$, existing KG completion approaches mostly consider a link prediction task to solve this problem, i.e., given two elements of a triplet predicting the missing one, such as $(h,r,?)$. However, this task implicitly has a strong yet impractical assumption on the two given elements in a triplet, which have to be correlated, resulting otherwise in meaningless predictions, such as (Marie Curie, headquarters location, ?). Against this background, this paper studies an instance completion task suggesting $r$-$t$ pairs for a given $h$, i.e., $(h,?,?)$. Inspired by the human psychological principle “fast-and-slow thinking”, we propose a two-step schema-aware approach RETA++ to efficiently solve our instance completion problem. It consists of two components: a fast RETA-Filter efficiently filtering candidate $r$-$t$ pairs schematically matching the given $h$, and a deliberate RETA-Grader leveraging a KG embedding model scoring each candidate $r$-$t$ pair considering the plausibility of both the input triplet and its corresponding schema. RETA++ systematically integrates them by training RETA-Grader on the reduced solution space output by RETA-Filter via a customized negative sampling process, so as to fully benefit from the efficiency of RETA-Filter in solution space reduction and the deliberation of RETA-Grader in scoring candidate triplets. We evaluate our approach against a sizable collection of state-of-the-art techniques on three real-world KG datasets. Results show that RETA-Filter can efficiently reduce the solution space for the instance completion task, outperforming best baseline techniques by 10.61%–84.75% on the reduced solution space size, while also being 1.7×–29.6x faster than these techniques. Moreover, RETA-Grader trained on the reduced solution space also significantly outperforms the best state-of-the-art techniques on the instance completion task by 31.90%–105.02%.}
}


@article{DBLP:journals/tkde/ChenDKXCQ24,
	author = {Zhi Chen and
                  Jiang Duan and
                  Li Kang and
                  Hongyan Xu and
                  Rui Chen and
                  Guoping Qiu},
	title = {Generating Counterfactual Instances for Explainable Class-Imbalance
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1130--1144},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3302847},
	doi = {10.1109/TKDE.2023.3302847},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenDKXCQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing class imbalance learning paradigms focus on lifting the importance of minority instance, aiming to improve the model in terms of certain evaluation metrics (e.g., AUC and F_{1}\n-measure). One drawback of these methods is that they lack enough transparency, hence, cannot be fully trusted in vital domains. To this end, this paper deal with the class imbalance learning task with counterfactual instances. Given an instance and a classifier, a counterfactual is a fake instance which, while having smallest distance to the original instance, is classified as a different class by the classifier. Therefore, the most important features for a classifier can be identified by inspecting the difference between an instance and its counterfactual. To utilize counterfactuals, a novel Explainable Generative Adversarial Network (EXGAN) is proposed. EXGAN has a unique “two generators versus multiple discriminators” architecture where the generators are used to generate effective counterfactuals and discriminators are trained for the class imbalance learning task. In addition to the architecture, an innovative ensemble loss function ensuring each discriminator complementing each other is designed to overcome the class imbalance issue. Extensive experiments prove that the counterfactuals generated by EXGAN can be used to produce effective local explanation and provide significant better class imbalance learning ability than existing competitors.}
}


@article{DBLP:journals/tkde/YaoCQ24,
	author = {Kai Yao and
                  Lijun Chang and
                  Lu Qin},
	title = {Identifying Large Structural Balanced Cliques in Signed Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1145--1160},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3295803},
	doi = {10.1109/TKDE.2023.3295803},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YaoCQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Signed graphs have been used to capture the polarity of relationships through positive/negative edge signs. In this paper, we consider balanced cliques — a clique is balanced if its vertex set C\ncan be partitioned into C_{L}\nand C_{R}\nsuch that all negative edges are between C_{L}\nand C_{R}\n— and study the problems of maximum balanced clique computation and large balanced clique enumeration. Our main idea is a novel graph reduction that transforms a balanced clique problem over a signed graph G\nto problems over small subgraphs of G\n. Specifically, for each vertex u\nin G\n, we extract the subgraph G_{u}\nof G\ninduced by V_{L} \\cup V_{R}\n; V_{L}\nis u\nand u\n's positive neighbors while V_{R}\nis u\n's negative neighbors. Then, we remove from G_{u}\nall positive edges between V_{L}\nand V_{R}\nand all negative edges between vertices of the same set; denote the resulting graph of discarding edge signs as g_{u}\n. We show that all balanced cliques containing u\nin G\ncan be found by processing g_{u}\n. Due to the small size and no edge signs, large cliques containing u\nin g_{u}\ncan be efficiently identified. Experimental results on real signed graphs demonstrated the advantages of our techniques.}
}


@article{DBLP:journals/tkde/ZhangLLZTLX24,
	author = {Hanlin Zhang and
                  Shuai Lin and
                  Weiyang Liu and
                  Pan Zhou and
                  Jian Tang and
                  Xiaodan Liang and
                  Eric P. Xing},
	title = {Iterative Graph Self-Distillation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1161--1169},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303885},
	doi = {10.1109/TKDE.2023.3303885},
	timestamp = {Thu, 20 Jun 2024 15:06:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLLZTLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, there has been increasing interest in the challenge of how to discriminatively vectorize graphs. To address this, we propose a method called Iterative Graph Self-Distillation (IGSD) which learns graph-level representation in an unsupervised manner through instance discrimination using a self-supervised contrastive learning approach. IGSD involves a teacher-student distillation process that uses graph diffusion augmentations and constructs the teacher model using an exponential moving average of the student model. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and self-supervised contrastive loss. Finally, we show that fine-tuning the IGSD-trained models with self-training can further improve graph representation learning. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.}
}


@article{DBLP:journals/tkde/WangCWZ24,
	author = {Quanxiu Wang and
                  Xinlei Cao and
                  Jianyong Wang and
                  Wei Zhang},
	title = {Knowledge-Aware Collaborative Filtering With Pre-Trained Language
                  Model for Personalized Review-Based Rating Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1170--1182},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3301884},
	doi = {10.1109/TKDE.2023.3301884},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangCWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized review-based rating prediction aims at leveraging existing reviews to model user interests and item characteristics for rating prediction. Most of the existing studies mainly encounter two issues. First, the rich knowledge contained in the fine-grained aspects of each review and the knowledge graph is rarely considered to complement the pure text for better modeling user-item interactions. Second, the power of pre-trained language models is not carefully studied for personalized review-based rating prediction. To address these issues, we propose an approach named Knowledge-aware Collaborative Filtering with Pre-trained Language Model (KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a transformer network to model the interactions of the extracted aspects w.r.t. a user-item pair. For the second issue, to better represent users and items, KCF-PLM takes all the historical reviews of a user or an item as input to pre-trained language models. Moreover, KCF-PLM integrates the transformer network and the pre-trained language models through representation propagation on the knowledge graph and user-item guided attention of the aspect representations. Thus KCF-PLM combines review text, aspect, knowledge graph, and pre-trained language models together for review-based rating prediction. We conduct comprehensive experiments on several public datasets, demonstrating the effectiveness of KCF-PLM.}
}


@article{DBLP:journals/tkde/WeiLZZL24,
	author = {Lai Wei and
                  Shiteng Liu and
                  Rigui Zhou and
                  Changming Zhu and
                  Jin Liu},
	title = {Learning Idempotent Representation for Subspace Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1183--1197},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303343},
	doi = {10.1109/TKDE.2023.3303343},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WeiLZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The critical point for the success of spectral-type subspace clustering algorithms is to seek reconstruction coefficient matrices that can faithfully reveal the subspace structures of data sets. An ideal reconstruction coefficient matrix should have two properties: 1) it is block-diagonal with each block indicating a subspace; 2) each block is fully connected. We find that a normalized membership matrix naturally satisfies the above two conditions. Therefore, in this paper, we devise an idempotent representation (IDR) algorithm to pursue reconstruction coefficient matrices approximating normalized membership matrices. IDR designs a new idempotent constraint. And by combining the doubly stochastic constraints, the coefficient matrices which are close to normalized membership matrices could be directly achieved. We present an optimization algorithm for solving IDR problem and analyze its computation burden as well as convergence. The comparisons between IDR and related algorithms show the superiority of IDR. Plentiful experiments conducted on both synthetic and real-world datasets prove that IDR is an effective subspace clustering algorithm.}
}


@article{DBLP:journals/tkde/LiH24,
	author = {Shiou{-}Chi Li and
                  Jen{-}Wei Huang},
	title = {Let the Information Fly: Reconstructing Social Network After a Node
                  Deleted},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1198--1209},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303488},
	doi = {10.1109/TKDE.2023.3303488},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite considerable research into information diffusion, most models focus on static networks. Networks in the real world change over time and information is lost when a node disappears. We sought to resolve the problem of information loss by defining information and information loss in general terms. Experiments demonstrate the efficacy of the proposed scheme in resolving problems of information loss in real-world networks using only a few edges.}
}


@article{DBLP:journals/tkde/ZhangZZZ24,
	author = {Mei Zhang and
                  Yong{-}Dao Zhou and
                  Zheng Zhou and
                  Aijun Zhang},
	title = {Model-Free Subsampling Method Based on Uniform Designs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1210--1220},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3297167},
	doi = {10.1109/TKDE.2023.3297167},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subsampling or subdata selection is a useful approach in large-scale statistical learning. Most existing studies focus on model-based subsampling methods which significantly depend on the model assumption. In this article, we consider the model-free subsampling strategy for generating subdata from the original full data. In order to measure the goodness of representation of a subdata with respect to the original data, we propose a criterion, generalized empirical F-discrepancy (GEFD), and study its theoretical properties in connection with the classical generalized \\ell _{2}-discrepancy in the theory of uniform designs. These properties allow us to develop a kind of low-GEFD data-driven subsampling method based on the existing uniform designs. By simulation examples and a real case study, we show that the proposed subsampling method is superior to the random sampling method. Moreover, our method keeps robust under diverse model specifications while other popular model-based subsampling methods are under-performing. In practice, such a model-free property is more appealing than the model-based subsampling methods, where the latter may have poor performance when the model is misspecified, as demonstrated in our simulation studies. In addition, our method is orders of magnitude faster than other model-free subsampling methods, which makes it more applicable for subsampling of Big Data.}
}


@article{DBLP:journals/tkde/WangLLHY24,
	author = {Lianhong Wang and
                  Xiaoyao Li and
                  Zhihui Luo and
                  Zinan Hu and
                  Qing Yan},
	title = {Multivariate Cognitive Response Framework for Student Performance
                  Prediction on {MOOC}},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1221--1233},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3302848},
	doi = {10.1109/TKDE.2023.3302848},
	timestamp = {Fri, 15 Mar 2024 16:54:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangLLHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Based on student's cognitive structure, the cognitive diagnostic models (CDMs) can reveal the potential relationships among the student's knowledge level, test item features and the corresponding item scores, and then predict each student's future performance. However, due to the simplistic prior information and deficient cognitive mechanism, most of the existing CDMs have limited prediction performance. To address the issues, we propose the multivariate cognitive response framework (MvCRF). We first collect student's learning activity logs to calculate the corresponding effort trait. Considering both student's ability trait and effort trait, MvCRF then introduces the compensation mechanism to calculate student's knowledge level. In addition, we introduce not only the slip and guessing parameters in prediction but also the skill weakness parameter related with the student's knowledge level and the importance of each skill on solving specific item. Experimental results on both simulation study and real-data application on MOOC demonstrate that MvCRF achieves better prediction performance, robustness and interpretability than the baseline CDMs.}
}


@article{DBLP:journals/tkde/WangNWWL24,
	author = {Sisi Wang and
                  Feiping Nie and
                  Zheng Wang and
                  Rong Wang and
                  Xuelong Li},
	title = {Outliers Robust Unsupervised Feature Selection for Structured Sparse
                  Subspace},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1234--1248},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3297226},
	doi = {10.1109/TKDE.2023.3297226},
	timestamp = {Tue, 05 Mar 2024 17:26:56 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangNWWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature selection is one of the important topics of machine learning, and it has a wide range of applications in data preprocessing. At present, feature selection based on \\ell _{2,1}-norm regularization is a relatively mature method, but it is not enough to maximize the sparsity and parameter-tuning leads to increased costs. Later scholars found that the \\ell _{2,0}-norm constraint is more conductive to feature selection, but it is difficult to solve and lacks convergence guarantees. To address these problems, we creatively propose a novel Outliers Robust Unsupervised Feature Selection for structured sparse subspace (ORUFS), which utilizes \\ell _{2,0}-norm constraint to learn a structured sparse subspace and avoid tuning the regularization parameter. Moreover, by adding binary weights, outliers are directly eliminated and the robustness of model is improved. More importantly, a Re-Weighted (RW) algorithm is exploited to solve our \\ell _{p}-norm problem. For the NP-hard problem of \\ell _{2,0}-norm constraint, we develop an effective iterative optimization algorithm with strict convergence guarantees and closed-form solution. Subsequently, we provide theoretical analysis about convergence and computational complexity. Experimental results on real-world datasets illustrate that our method is superior to the state-of-the-art methods in clustering and anomaly detection tasks.}
}


@article{DBLP:journals/tkde/PibiriT24,
	author = {Giulio Ermanno Pibiri and
                  Roberto Trani},
	title = {Parallel and External-Memory Construction of Minimal Perfect Hash
                  Functions With PTHash},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1249--1259},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303341},
	doi = {10.1109/TKDE.2023.3303341},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/PibiriT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A function f : U \\to \\lbrace 0,\\ldots,n-1\\rbrace is a minimal perfect hash function for a set S \\subseteq U of size n, if f bijectively maps S into the first n natural numbers. These functions are important for many practical applications in computing, such as search engines, computer networks, and databases. Several algorithms have been proposed to build minimal perfect hash functions that: scale well to large sets, retain fast evaluation time, and take very little space, e.g., 2 – 3 bits/key. PTHash is one such algorithm, achieving very fast evaluation in compressed space, typically many times faster than other techniques. In this work, we propose a new construction algorithm for PTHash enabling: (1) multi-threading, to either build functions more quickly or more space-efficiently, and (2) external-memory processing, to scale to inputs much larger than the available internal memory. Only few other algorithms in the literature share these features, despite of their practical impact. We conduct an extensive experimental assessment on large real-world string collections and show that, with respect to other techniques, PTHash is competitive in construction time and space consumption, but retains 2 – 6× better lookup time.}
}


@article{DBLP:journals/tkde/ZhuZHD24,
	author = {Qiannan Zhu and
                  Haobo Zhang and
                  Qing He and
                  Zhicheng Dou},
	title = {Query-Aware Explainable Product Search With Reinforcement Knowledge
                  Graph Reasoning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1260--1273},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3297331},
	doi = {10.1109/TKDE.2023.3297331},
	timestamp = {Mon, 19 Aug 2024 07:47:30 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuZHD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Product search is one of the most effective tools for people to browse and purchase products on e-commerce platforms. Recent advances have mainly focused on ranking products by their likelihood to be purchased through retrieval models. However, they overlook the problem that users may not understand why certain products are retrieved for them. The lack of appropriate explanations can lead to an unsatisfactory user experience and further decrease user trust in the platforms. To address this problem, we propose a Query-aware Explainable Product Search with Reinforcement Knowledge Reasoning, namely QEPS, which uses search behaviors related to the current query to reinforce explanations. Specifically, with the aim of retrieving suitable products with explanations, QEPS takes full advantage of the user-product knowledge graph (KG) and develops a reinforcement learning approach, characterized by the demonstration-guided policy network and query-aware rewards, to perform explicit multi-step reasoning on the KG. The reasoning paths between users and products are automatically derived from the current query-related search behavior, which can provide valuable signals as to why the retrieved products are more likely to satisfy the user's search intent. Empirical experiments on four datasets show that our model achieves remarkable performance and is able to generate reasonable explanations for the search results.}
}


@article{DBLP:journals/tkde/LiuZWZLL24,
	author = {Huawen Liu and
                  Wenhua Zhou and
                  Zongda Wu and
                  Shichao Zhang and
                  Gang Li and
                  Xuelong Li},
	title = {Refining Codes for Locality Sensitive Hashing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1274--1284},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3297195},
	doi = {10.1109/TKDE.2023.3297195},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuZWZLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning to hash is of particular interest in information retrieval for large-scale data due to its high efficiency and effectiveness. Most studies in hashing concentrate on constructing new hashing models, but rarely touch the correlation and redundancy between hash bits derived. In this article, we first introduce a general schema of hash bit reduction to derive compact and informative binary codes for hashing techniques. Further, we take locality sensitive hashing, one of the most widely-used hashing methods, as an example and propose a novel and two-stage binary code refinement method under the reduction schema. Specifically, the proposed method includes two stages, i.e., bit evaluation and bit refinement. The former stage aims to initially extract a small portion of informative hash bits in terms of their importance and quality evaluated by bit balance and similarity preservation. Then, the representation capabilities of the reduced hash bits are strengthened further by refining their binary values. The purpose of refinement is to lessen the correlations and redundancies between the reduced bits, making themselves more discriminative. The experimental results on three widely-used data collections confirm the effectiveness of the proposed bit reduction method and its superiority over the state-of-the-art hashing methods, as well as a bit selection method.}
}


@article{DBLP:journals/tkde/GuanLPLX24,
	author = {Zeli Guan and
                  Yawen Li and
                  Zhenhui Pan and
                  Yuxin Liu and
                  Zhe Xue},
	title = {{RFDG:} Reinforcement Federated Domain Generalization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1285--1298},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3301036},
	doi = {10.1109/TKDE.2023.3301036},
	timestamp = {Mon, 17 Feb 2025 16:53:28 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GuanLPLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {During the training process of federated learning models, the domain information of the target test data on the server can differ greatly from the training data of each client, leading to a decrease in the performance of the federated model. Additionally, due to privacy protection during federated training, clients cannot see the target domain test data, and the distribution information of the target data cannot be used. This poses a new challenge for federated learning. Domain generalization techniques are often used in centralized frameworks to resolve such problems. In recent years, the domain generalization method based on feature decorrelation has enabled models to learn knowledge with a stronger generalization ability in unseen target domain data. However, existing methods require data centralization in the feature decorrelation process, which conflicts with data privacy protection in federated learning. To address these issues, we propose Reinforcement Federated Domain Generalization (RFDG), which incorporates domain generalization in federated learning via reinforcement learning. RFDG can improve the generalization ability of the federated model of unseen target domain test data. We design a reinforcement federated feature decorrelation policy that uses reinforcement learning technology to transform the sample reweight work into a parameterized sample reweight policy that can be shared among federated learning clients. We develop reinforcement federated experience replay techniques to supplement the feature information loss of local data due to the mini-batch mechanism during the policy learning process. When the policy is shared by each client, those features can be decorrelated from a global perspective, allowing the model to focus on capturing the fundamental association between features and labels to learn domain-invariant knowledge. We verified the effectiveness of our method through extensive experiments using four publicly available datasets.}
}


@article{DBLP:journals/tkde/GuoZCQCYC24,
	author = {Siyuan Guo and
                  Lixin Zou and
                  Hechang Chen and
                  Bohao Qu and
                  Haotian Chi and
                  Philip S. Yu and
                  Yi Chang},
	title = {Sample Efficient Offline-to-Online Reinforcement Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1299--1310},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3302804},
	doi = {10.1109/TKDE.2023.3302804},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GuoZCQCYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Offline reinforcement learning (RL) makes it possible to train the agents entirely from a previously collected dataset. However, constrained by the quality of the offline dataset, offline RL agents typically have limited performance and cannot be directly deployed. Thus, it is desirable to further finetune the pretrained offline RL agents via online interactions with the environment. Existing offline-to-online RL algorithms suffer from the low sample efficiency issue, due to two inherent challenges, i.e., exploration limitation and distribution shift. To this end, we propose a sample-efficient offline-to-online RL algorithm via Optimistic Exploration and Meta Adaptation (OEMA). Specifically, we first propose an optimistic exploration strategy according to the principle of optimism in the face of uncertainty. This allows agents to sufficiently explore the environment in a stable manner. Moreover, we propose a meta learning based adaptation method, which can reduce the distribution shift and accelerate the offline-to-online adaptation process. We empirically demonstrate that OEMA improves the sample efficiency on D4RL benchmark. Besides, we provide in-depth analyses to verify the effectiveness of both optimistic exploration and meta adaptation.}
}


@article{DBLP:journals/tkde/YinWYZY24,
	author = {Yanting Yin and
                  Yajing Wu and
                  Xuebing Yang and
                  Wensheng Zhang and
                  Xiaojie Yuan},
	title = {Super Resolution Graph With Conditional Normalizing Flows for Temporal
                  Link Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1311--1327},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3295367},
	doi = {10.1109/TKDE.2023.3295367},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YinWYZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal link prediction on dynamic graphs has attracted considerable attention. Most methods focus on the graph at each timestamp and extract features for prediction. As graphs are directly compressed into feature matrices, the important latent information at each timestamp has not been well revealed. Eventually, the acquisition of dynamic evolution-related patterns is rendered inadequately. In this paper, inspired by the process of Super-Resolution (SR), a novel deep generative model SRG (Super Resolution Graph) is proposed. We innovatively introduce the concepts of the Low-Resolution (LR) graph, which is a single adjacent matrix at a timestamp, and the High-Resolution (HR) graph, which includes the link status of surrounding snapshots. Specifically, two major aspects are considered regarding the construction of the HR graph. For edges, we endeavor to obtain an extensive information transmission description that affects the current link status. For nodes, similar to the SR process, the neighbor relationship among nodes is maintained. In this form, we could predict the link status from a new perspective: Under the supervision of the graph moving average strategy, the conditional normalizing flow effectively realizes the transformation between LR and HR graphs. Extensive experiments on six real-world datasets from different applications demonstrate the effectiveness of our proposal.}
}


@article{DBLP:journals/tkde/ZhangL24,
	author = {Delvin Ce Zhang and
                  Hady W. Lauw},
	title = {Topic Modeling on Document Networks With Dirichlet Optimal Transport
                  Barycenter},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1328--1340},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303465},
	doi = {10.1109/TKDE.2023.3303465},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Text documents are often interconnected in a network structure, e.g., academic papers via citations, Web pages via hyperlinks. On the one hand, though Graph Neural Networks (GNNs) have shown promising ability to derive effective embeddings for such networked documents, they do not assume a latent topic structure and result in uninterpretable embeddings. On the other hand, topic models can infer semantically interpretable topic distributions for documents by associating each topic with a group of understandable key words. However, most topic models mainly focus on plain text within documents and fail to leverage network structure across documents. Network connectivity reveals topic similarity between linked documents, and modeling it could uncover meaningful semantics. Motivated by above two challenges, in this paper, we propose a GNN-based neural topic model that both captures network connectivity and derives semantically interpretable topic distributions for networked documents. For network modeling, we build the model based on the theory of Optimal Transport Barycenter, which captures network structure by allowing the topic distribution of a document to generate the content of its linked neighbors. For semantic interpretability, we extend optimal transport by incorporating semantically related words in the embedding space. Since Dirichlet prior in Latent Dirichlet Allocation successfully improves topic quality, we also analyze Dirichlet as an optimal transport prior distribution to improve topic interpretability. We design rejection sampling to simulate Dirichlet distribution. Extensive experiments on document classification, clustering, link prediction, and topic analysis verify the effectiveness of our model.}
}


@article{DBLP:journals/tkde/TsitsigkosBLMT24,
	author = {Dimitrios Tsitsigkos and
                  Panagiotis Bouros and
                  Konstantinos Lampropoulos and
                  Nikos Mamoulis and
                  Manolis Terrovitis},
	title = {Two-Layer Space-Oriented Partitioning for Non-Point Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1341--1355},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3297975},
	doi = {10.1109/TKDE.2023.3297975},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/TsitsigkosBLMT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-point spatial objects (e.g., polygons, linestrings, etc.) are ubiquitous. We study the problem of indexing non-point objects in memory for range queries and spatial intersection joins. We propose a secondary partitioning technique for space-oriented partitioning indices (e.g., grids), which improves their performance significantly, by avoiding the generation and elimination of duplicate results. Our approach is easy to implement and can be used by any space-partitioning index to significantly reduce the cost of range queries and intersection joins. In addition, the secondary partitions can be processed independently, which makes our method appropriate for distributed and parallel indexing. Experiments on real datasets confirm the advantage of our approach against alternative duplicate elimination techniques and data-oriented state-of-the-art spatial indices. We also show that our partitioning technique, paired with optimized partition-to-partition join algorithms, typically reduces the cost of spatial joins by around 50%.}
}


@article{DBLP:journals/tkde/FengLKC24,
	author = {Wenjie Feng and
                  Shenghua Liu and
                  Danai Koutra and
                  Xueqi Cheng},
	title = {Unified Dense Subgraph Detection: Fast Spectral Theory Based Algorithms},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1356--1370},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3272574},
	doi = {10.1109/TKDE.2023.3272574},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FengLKC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How can we effectively detect fake reviews or fraudulent links on a website? How can we spot communities that suddenly appear based on users’ interactions? And how can we efficiently find the minimum cut in a large graph? All of these are related to the finding of dense subgraphs, a significant primitive problem in graph analysis with extensive applications across various domains. In this paper, we focus on formulating the problem of the densest subgraph detection and theoretically compare and contrast several correlated problems. Moreover, we propose a unified framework, GenDS, for the densest subgraph detection, provide some theoretical analysis based on the network flow and spectral graph theory, and devise simple and computationally efficient algorithms, SpecGDS and GepGDS, to solve it by leveraging the spectral properties and greedy search. We conduct thorough experiments on 40 real-world networks with up to 1.47 billion edges from various domains. We demonstrate that our SpecGDS yields up to 58.6 \\ \\times\nspeedup and achieves better or approximately equal-quality solutions for the densest subgraph detection compared to the baselines. GepGDS also reveals some properties of generalized eigenvalue problems for the GenDS. Also, our methods scale linearly with the graph size and are proven effective in applications such as finding collaborations that appear suddenly in an extensive, time-evolving co-authorship network.}
}


@article{DBLP:journals/tkde/ChenXLL24,
	author = {Yuyan Chen and
                  Yanghua Xiao and
                  Zhixu Li and
                  Bang Liu},
	title = {XMQAs: Constructing Complex-Modified Question-Answering Dataset for
                  Robust Question Understanding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {3},
	pages = {1371--1384},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303916},
	doi = {10.1109/TKDE.2023.3303916},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenXLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Question understanding is an important issue to the success of a Knowledge-based Question Answering (KBQA) system.However, the existing study does not pay enough attention to this issue given that the questions in the existing KBQA datasets are usually expressed in simple and straightforward way. This is not in line with the actual linguistic conventions, which often use a lot of modifiers. To facilitate the study on evaluating and enhancing the question understanding ability of the KBQA systems, this paper proposes to construct a complex-modified question-answering (XMQAs) dataset based on existing KBQA datasets. With the help of knowledge bases and dictionaries, three kinds of modifiers are defined and applied to original simple-expressed questions. These modifiers could make the expression of these questions complex without changing their semantics. Based on XMQAs, we then propose a novel question understanding algorithm upon existing KBQA models, which greatly improves the robustness of their question understanding abilities. We conduct extensive experiments on XMQAs and two widely acknowledged KBQA datasets. The empirical results demonstrate that our proposed algorithm can improve the performance of KBQA models on not only the complex-modified questions, but also simple-expressed questions.}
}


@article{DBLP:journals/tkde/LiuZZZ24,
	author = {Changan Liu and
                  Xiaotian Zhou and
                  Ahad N. Zehmakan and
                  Zhongzhi Zhang},
	title = {A Fast Algorithm for Moderating Critical Nodes via Edge Removal},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1385--1398},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3309987},
	doi = {10.1109/TKDE.2023.3309987},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuZZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Critical nodes in networks are extremely vulnerable to malicious attacks to trigger negative cascading events such as the spread of misinformation and diseases. Therefore, effective moderation of critical nodes is very vital for mitigating the potential damages caused by such malicious diffusions. The current moderation methods are computationally expensive. Furthermore, they disregard the fundamental metric of information centrality, which measures the dissemination power of nodes. We investigate the problem of removing k edges from a network to minimize the information centrality of a target node v while preserving the network's connectivity. We prove that this problem is computationally challenging: it is NP-complete and its objective function is not supermodular. However, we propose three approximation greedy algorithms using novel techniques such as random walk-based Schur complement approximation and fast sum estimation. One of our algorithms runs in nearly linear time in the number of edges. To complement our theoretical analysis, we conduct a comprehensive set of experiments on synthetic and real networks with over one million nodes. Across various settings, the experimental results illustrate the effectiveness and efficiency of our proposed algorithms.}
}


@article{DBLP:journals/tkde/NakisCLM24,
	author = {Nikolaos Nakis and
                  Abdulkadir {\c{C}}elikkanat and
                  Sune Lehmann and
                  Morten M{\o}rup},
	title = {A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph
                  Representations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1399--1412},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3304344},
	doi = {10.1109/TKDE.2023.3304344},
	timestamp = {Sun, 19 Jan 2025 13:53:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NakisCLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Representation Learning (GRL) has become central for characterizing structures of complex networks and performing tasks such as link prediction, node classification, network reconstruction, and community detection. Whereas numerous generative GRL models have been proposed, many approaches have prohibitive computational requirements hampering large-scale network analysis, fewer are able to explicitly account for structure emerging at multiple scales, and only a few explicitly respect important network properties such as homophily and transitivity. This paper proposes a novel scalable graph representation learning method named the Hierarchical Block Distance Model (HBDM). The HBDM imposes a multiscale block structure akin to stochastic block modeling (SBM) and accounts for homophily and transitivity by accurately approximating the latent distance model (LDM) throughout the inferred hierarchy. The HBDM naturally accommodates unipartite, directed, and bipartite networks whereas the hierarchy is designed to ensure linearithmic time and space complexity enabling the analysis of very large-scale networks. We evaluate the performance of the HBDM on massive networks consisting of millions of nodes. Importantly, we find that the proposed HBDM framework significantly outperforms recent scalable approaches in all considered downstream tasks. Surprisingly, we observe superior performance even imposing ultra-low two-dimensional embeddings facilitating accurate direct and hierarchical-aware network visualization and interpretation.}
}


@article{DBLP:journals/tkde/HuLZHNL24,
	author = {Linmei Hu and
                  Zeyi Liu and
                  Ziwang Zhao and
                  Lei Hou and
                  Liqiang Nie and
                  Juanzi Li},
	title = {A Survey of Knowledge Enhanced Pre-Trained Language Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1413--1430},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3310002},
	doi = {10.1109/TKDE.2023.3310002},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuLZHNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.}
}


@article{DBLP:journals/tkde/LinRLW24,
	author = {Yupian Lin and
                  Tong Ruan and
                  Jingping Liu and
                  Haofen Wang},
	title = {A Survey on Neural Data-to-Text Generation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1431--1449},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3304385},
	doi = {10.1109/TKDE.2023.3304385},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LinRLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data-to-text Generation (D2T) aims to generate textual natural language statements that can fluently and precisely describe the structured data such as graphs, tables, and meaning representations (MRs) in the form of key-value pairs. It is a typical and crucial task in natural language generation (NLG). Early D2T systems generated texts with the cost of human engineering in designing domain specific rules and templates, and achieved acceptable performance in coherence, fluency, and fidelity. In recent years, the data-driven D2T systems based on deep learning have reached state-of-the-art (SOTA) performance in more challenging datasets. In this paper, we provide a comprehensive review on existing neural data-to-text generation approaches. We first introduce available D2T resources, including systematically categorized D2T datasets and mainstream evaluation metrics. Next, we survey existing works based on the taxonomy along two axes: neural end-to-end D2T and neural modular D2T. We also discuss the potential applications and the adverse impacts. Finally, we present readers with the challenges faced by neural D2T and outline some potential future directions in this area.}
}


@article{DBLP:journals/tkde/ZhangXMLYL24,
	author = {Xiaokun Zhang and
                  Bo Xu and
                  Fenglong Ma and
                  Chenliang Li and
                  Liang Yang and
                  Hongfei Lin},
	title = {Beyond Co-Occurrence: Multi-Modal Session-Based Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1450--1462},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3309995},
	doi = {10.1109/TKDE.2023.3309995},
	timestamp = {Thu, 13 Feb 2025 18:39:09 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangXMLYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Session-based recommendation is devoted to characterizing preferences of anonymous users based on short sessions. Existing methods mostly focus on mining limited item co-occurrence patterns exposed by item ID within sessions, while ignoring what attracts users to engage with certain items is rich multi-modal information displayed on pages. Generally, the multi-modal information can be classified into two categories: descriptive information (e.g., item images and description text) and numerical information (e.g., price). In this paper, we aim to improve session-based recommendation by modeling the above multi-modal information holistically. There are mainly three issues to reveal user intent from multi-modal information: (1) How to extract relevant semantics from heterogeneous descriptive information with different noise? (2) How to fuse these heterogeneous descriptive information to comprehensively infer user interests? (3) How to handle probabilistic influence of numerical information on user behaviors? To solve above issues, we propose a novel multi-modal session-based recommendation (MMSBR) that models both descriptive and numerical information under a unified framework. Specifically, a pseudo-modality contrastive learning is devised to enhance the representation learning of descriptive information. Afterwards, a hierarchical pivot transformer is presented to fuse heterogeneous descriptive information. Moreover, we represent numerical information with Gaussian distribution and design a Wasserstein self-attention to handle the probabilistic influence mode. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed MMSBR. Further analysis also proves that our MMSBR can alleviate the cold-start problem in SBR effectively.}
}


@article{DBLP:journals/tkde/ZhouHYD24,
	author = {Peng Zhou and
                  Boao Hu and
                  Dengcheng Yan and
                  Liang Du},
	title = {Clustering Ensemble via Diffusion on Adaptive Multiplex},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1463--1474},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3311409},
	doi = {10.1109/TKDE.2023.3311409},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouHYD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing clustering ensemble methods often directly integrate multiple weak base results to obtain a consensus one which can improve the clustering performance. However, since the base results are weak and the clustering ensemble can improve the performance, why not refine the weak base results via the clustering ensemble, and then boost the clustering ensemble with the refined base results? To fulfill this idea, in this article, we propose a novel clustering ensemble method with an adaptive multiplex. We first use the multiplex to represent the multiple weak base results. Then, we learn an updated representation by diffusing the representation on the multiplex with a manifold ranking model. Since the multiplex characterizes the structure information of all base results, the learned representation can ensemble such structure information during diffusion. Next, the multiplex is refined by such representation, which is a process of refining base results via ensemble. We iteratively learn the representation (i.e., do ensemble) and update the multiplex (i.e., do refinement), which can make the ensemble and refinement be boosted by each other. At last, the final consensus result is obtained from the refined multiplex. The extensive experiments demonstrate the effectiveness and superiority of the proposed framework.}
}


@article{DBLP:journals/tkde/YuLSDLL24,
	author = {Le Yu and
                  Zihang Liu and
                  Leilei Sun and
                  Bowen Du and
                  Chuanren Liu and
                  Weifeng Lv},
	title = {Continuous-Time User Preference Modelling for Temporal Sets Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1475--1488},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3309982},
	doi = {10.1109/TKDE.2023.3309982},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuLSDLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a sequence of sets, where each set has a timestamp and contains an arbitrary number of elements, temporal sets prediction aims to predict the elements in the subsequent set. Previous studies for temporal sets prediction mainly focus on the modelling of elements and implicitly represent each user's preference based on his/her interacted elements. However, user preferences are often continuously evolving and the evolutionary trend cannot be fully captured with the indirect learning paradigm of user preferences. To this end, we propose a continuous-time user preference modelling framework for temporal sets prediction, which explicitly models the evolving preference of each user by maintaining a memory bank to store the states of all the users and elements. Specifically, we first construct a universal sequence by arranging all the user-set interactions in a non-descending temporal order, and then chronologically learn from each user-set interaction. For each interaction, we continuously update the memories of the related user and elements based on their currently encoded messages and past memories. Moreover, we present a personalized user behavior learning module to discover user-specific characteristics based on each user's historical sequence, which aggregates the previously interacted elements from dual perspectives according to the user and elements. Finally, we develop a set-batch algorithm to improve the model efficiency, which can create time-consistent batches in advance and achieve 3.5× and 3.0× speedups in the training and evaluation process on average. Experiments on four real-world datasets demonstrate the superiority of our approach over state-of-the-arts under both transductive and inductive settings. The good interpretability of our method is also shown.}
}


@article{DBLP:journals/tkde/DialloP24,
	author = {Alec F. Diallo and
                  Paul Patras},
	title = {Deciphering Clusters With a Deterministic Measure of Clustering Tendency},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1489--1501},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3306024},
	doi = {10.1109/TKDE.2023.3306024},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DialloP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustering, a key aspect of exploratory data analysis, plays a crucial role in various fields such as information retrieval. Yet, the sheer volume and variety of available clustering algorithms hinder their application to specific tasks, especially given their propensity to enforce partitions, even when no clear clusters exist, often leading to fruitless efforts and erroneous conclusions. This issue highlights the importance of accurately assessing clustering tendencies prior to clustering. However, existing methods either rely on subjective visual assessment, which hinders automation of downstream tasks, or on correlations between subsets of target datasets and random distributions, limiting their practical use. Therefore, we introduce the Proximal Homogeneity Index (PHI), a novel and deterministic statistic that reliably assesses the clustering tendencies of datasets by analyzing their internal structures via knowledge graphs. Leveraging PHI and the boundaries between clusters, we establish the Partitioning Sensitivity Index (PSI), a new statistic designed for cluster quality assessment and optimal clustering identification. Comparative studies using twelve synthetic and real-world datasets demonstrate PHI and PSI's superiority over existing metrics for clustering tendency assessment and cluster validation. Furthermore, we demonstrate the scalability of PHI to large and high-dimensional datasets, and PSI's broad effectiveness across diverse cluster analysis tasks.}
}


@article{DBLP:journals/tkde/WangJSZCHL24,
	author = {Haixin Wang and
                  Huiyu Jiang and
                  Jinan Sun and
                  Shikun Zhang and
                  Chong Chen and
                  Xian{-}Sheng Hua and
                  Xiao Luo},
	title = {{DIOR:} Learning to Hash With Label Noise Via Dual Partition and Contrastive
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1502--1517},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312109},
	doi = {10.1109/TKDE.2023.3312109},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangJSZCHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the excellent computing efficiency, learning to hash has acquired broad popularity for Big Data retrieval. Although supervised hashing methods have achieved promising performance recently, they presume that all training samples are appropriately annotated. Unfortunately, label noise is ubiquitous owing to erroneous annotations in real-world applications, which could seriously deteriorate the retrieval performance due to imprecise supervised guidance and severe memorization of noisy data. Here we propose a comprehensive method DIOR to handle the difficulties of learning to hash with label noise. DIOR performs partitions from two complementary levels, namely sample level and parameter level. On the one hand, DIOR divides the dataset into a labeled set with clean samples and an unlabeled set with noisy samples using an ensemble of perturbed views. Then we train the network in a contrastive semi-supervised manner by reconstructing label embeddings for both reliable supervision of clean data and sufficient exploration of noisy data. On the other hand, inspired by recent pruning techniques, DIOR divides the parameters in the hashing network into crucial parameters and non-crucial parameters, and then optimizes them separately to reduce the overfitting of noisy data. Extensive experiments on four popular benchmark datasets demonstrate the effectiveness of DIOR.}
}


@article{DBLP:journals/tkde/YeLDCW24,
	author = {Xiaowei Ye and
                  Rong{-}Hua Li and
                  Qiangqiang Dai and
                  Hongzhi Chen and
                  Guoren Wang},
	title = {Efficient {\textdollar}k{\textdollar}k-Clique Counting on Large Graphs:
                  The Power of Color-Based Sampling Approaches},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1518--1536},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3314643},
	doi = {10.1109/TKDE.2023.3314643},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YeLDCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {K-clique counting is a fundamental problem in network analysis which has attracted much attention in recent years. Computing the count of k-cliques in a graph for a large k (e.g., k=8) is often intractable as the number of k-cliques increases exponentially w.r.t. (with respect to) k. Existing exact k-clique counting algorithms are often hard to handle large dense graphs, while sampling-based solutions either require a huge number of samples or consume very high storage space to achieve a satisfactory accuracy. To overcome these limitations, we propose a new framework to estimate the number of k-cliques which integrates both the exact k-clique counting technique and three novel color-based sampling techniques. The key insight of our framework is that we only apply the exact algorithm to compute the k-clique counts in the sparse regions of a graph, and use the proposed color-based sampling approaches to estimate the number of k-cliques in the dense regions of the graph. Specifically, we develop three novel dynamic programming based k-color set sampling techniques to efficiently estimate the k-clique counts, where a k-color set contains k nodes with k different colors. Since a k-color set is often a good approximation of a k-clique in the dense regions of a graph, our sampling-based solutions are extremely efficient and accurate. Moreover, the proposed sampling techniques are space efficient which use near-linear space w.r.t. graph size. We conduct extensive experiments to evaluate our algorithms using 8 real-life graphs. The results show that our best algorithm is at least one order of magnitude faster than the state-of-the-art sampling-based solutions (with the same relative error 0.1%) and can be up to three orders of magnitude faster than the state-of-the-art exact algorithm on large graphs.}
}


@article{DBLP:journals/tkde/CongSLYHP24,
	author = {Zicun Cong and
                  Baoxu Shi and
                  Shan Li and
                  Jaewon Yang and
                  Qi He and
                  Jian Pei},
	title = {FairSample: Training Fair and Accurate Graph Convolutional Neural
                  Networks Efficiently},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1537--1551},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3306378},
	doi = {10.1109/TKDE.2023.3306378},
	timestamp = {Fri, 28 Feb 2025 17:23:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CongSLYHP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and more important concern as GCNs are adopted in many crucial applications. Societal biases against sensitive groups may exist in many real world graphs. GCNs trained on those graphs may be vulnerable to being affected by such biases. In this paper, we adopt the well-known fairness notion of demographic parity and tackle the challenge of training fair and accurate GCNs efficiently. We present an in-depth analysis on how graph structure bias, node attribute bias, and model parameters may affect the demographic parity of GCNs. Our insights lead to FairSample, a framework that jointly mitigates the three types of biases. We employ two intuitive strategies to rectify graph structures. First, we inject edges across nodes that are in different sensitive groups but similar in node features. Second, to enhance model fairness and retain model quality, we develop a learnable neighbor sampling policy using reinforcement learning. To address the bias in node features and model parameters, FairSample is complemented by a regularization objective to optimize fairness.}
}


@article{DBLP:journals/tkde/QinTWF24,
	author = {Yalan Qin and
                  Zhenjun Tang and
                  Hanzhou Wu and
                  Guorui Feng},
	title = {Flexible Tensor Learning for Multi-View Clustering With Markov Chain},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1552--1565},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3305624},
	doi = {10.1109/TKDE.2023.3305624},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QinTWF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view clustering has gained great progress recently, which employs the representations from different views for improving the final performance. In this paper, we focus on the problem of multi-view clustering based on the Markov chain by considering low-rank constraints. Since most existing methods fail to simultaneously characterize the relations among different entries in a tensor from the global perspective and describe local structures of similarity matrices of a tensor, we propose a novel Flexible Tensor Learning for Multi-view Clustering with the Markov chain (FTLMCM) to solve this problem. We also construct transition probability matrices based on the Markov chain to fully utilize the connection between the Markov chain and spectral clustering. Specifically, the low-rank constraints of the tensor, the frontal slices and the lateral slices of the tensor are imposed on the objective function of the proposed method to achieve these goals. Besides, these three constraints can be optimized jointly to achieve mutual refinement. FTLMCM also uses the tensor rotation to better explore the relationships among different views. We formulate FTLMCM as a problem of low-rank tensor recovery and solve it with the augmented Lagrangian multiplier. Experiments on six different benchmark data sets under six metrics demonstrate that the proposed method is able to achieve better clustering performance.}
}


@article{DBLP:journals/tkde/MaLMXLMD24,
	author = {Zhuoran Ma and
                  Yang Liu and
                  Yinbin Miao and
                  Guowen Xu and
                  Ximeng Liu and
                  Jianfeng Ma and
                  Robert H. Deng},
	title = {FlGan: GAN-Based Unbiased Federated Learning Under Non-IID Settings},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1566--1581},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3309858},
	doi = {10.1109/TKDE.2023.3309858},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MaLMXLMD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) suffers from low convergence and significant accuracy loss due to local biases caused by non-Independent and Identically Distributed (non-IID) data. To enhance the non-IID FL performance, a straightforward idea is to leverage the Generative Adversarial Network (GAN) to mitigate local biases using synthesized samples. Unfortunately, existing GAN-based solutions have inherent limitations, which do not support non-IID data and even compromise user privacy. To tackle the above issues, we propose a GAN-based unbiased FL scheme, called FlGan, to mitigate local biases using synthesized samples generated by GAN while preserving user-level privacy in the FL setting. Specifically, FlGan first presents a federated GAN algorithm using the divide-and-conquer strategy that eliminates the problem of model collapse in non-IID settings. To guarantee user-level privacy, FlGan then exploits Fully Homomorphic Encryption (FHE) to design the privacy-preserving GAN augmentation method for the unbiased FL. Extensive experiments show that FlGan achieves unbiased FL with 10\\%-60\\% accuracy improvement compared with two state-of-the-art FL baselines (i.e., FedAvg and FedSGD) trained under different non-IID settings. The FHE-based privacy guarantees only cost about 0.53% of the total overhead in FlGan.}
}


@article{DBLP:journals/tkde/ChanLYWL24,
	author = {Harry Kai{-}Ho Chan and
                  Cheng Long and
                  Da Yan and
                  Raymond Chi{-}Wing Wong and
                  Hua Lu},
	title = {Fraction-Score: {A} Generalized Support Measure for Weighted and Maximal
                  Co-Location Pattern Mining},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1582--1596},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3304365},
	doi = {10.1109/TKDE.2023.3304365},
	timestamp = {Tue, 22 Oct 2024 20:38:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChanLYWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Co-location patterns, which capture the phenomenon that objects with certain labels are often located in close geographic proximity, are defined based on a support measure which quantifies the prevalence of a pattern candidate in the form of a label set. Existing support measures share the idea of counting the number of instances of a given label set C as its support, where an instance of C is an object set whose objects collectively carry all labels in C and are located close to one another. However, they suffer from various weaknesses, e.g., fail to capture all possible instances, or overlook the cases when multiple instances overlap. In this paper, we propose a new measure called Fraction-Score which counts instances fractionally if they overlap. Fraction-Score captures all possible instances, and handles the cases where instances overlap appropriately (so that the supports defined are more meaningful and anti-monotonic). We develop efficient algorithms to solve the co-location pattern mining problem defined with Fraction-Score. Furthermore, to obtain representative patterns, we develop an efficient algorithm for mining the maximal co-location patterns, which are those patterns without proper superset patterns. We conduct extensive experiments using real and synthetic datasets, which verified the superiority of our proposals.}
}


@article{DBLP:journals/tkde/TaoCSWHC24,
	author = {Shuchang Tao and
                  Qi Cao and
                  Huawei Shen and
                  Yunfan Wu and
                  Liang Hou and
                  Xueqi Cheng},
	title = {Graph Adversarial Immunization for Certifiable Robustness},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1597--1610},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3311105},
	doi = {10.1109/TKDE.2023.3311105},
	timestamp = {Tue, 07 May 2024 20:19:23 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TaoCSWHC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite achieving great success, graph neural networks (GNNs) are vulnerable to adversarial attacks. Existing defenses focus on developing adversarial training or model modification. In this paper, we propose and formulate graph adversarial immunization, i.e., vaccinating part of graph structure to improve certifiable robustness of graph against any admissible adversarial attack. We first propose edge-level immunization to vaccinate node pairs. Unfortunately, such edge-level immunization cannot defend against emerging node injection attacks, since it only immunizes existing node pairs. To this end, we further propose node-level immunization. To avoid computationally intensive combinatorial optimization associated with adversarial immunization, we develop AdvImmune-Edge and AdvImmune-Node algorithms to effectively obtain the immune node pairs or nodes. Extensive experiments demonstrate the superiority of AdvImmune methods. In particular, AdvImmune-Node remarkably improves the ratio of robust nodes by 79\\%, 294\\%, and 100\\%, after immunizing only 5\\% of nodes. Furthermore, AdvImmune methods show excellent defensive performance against various attacks, outperforming state-of-the-art defenses. To the best of our knowledge, this is the first attempt to improve certifiable robustness from graph data perspective without losing performance on clean graphs, providing new insights into graph adversarial learning.}
}


@article{DBLP:journals/tkde/MashayekhiKLB24,
	author = {Yoosof Mashayekhi and
                  Bo Kang and
                  Jefrey Lijffijt and
                  Tijl De Bie},
	title = {{GREASE:} Graph Imbalance Reduction by Adding Sets of Edges},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1611--1623},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3304478},
	doi = {10.1109/TKDE.2023.3304478},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MashayekhiKLB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-world data can often be represented as a heterogeneous network relating nodes of different types. E.g., a job market can be represented as a job seeker-skill-vacancy network. It can be relevant to consider the imbalance between nodes of different types, in terms of whether they are similarly connected in the network. For example, it is desirable that job seekers and vacancies are mixed well. If they are not, then there is imbalance. We propose to quantify the imbalance between two sets of nodes in a network as the Earth Mover's Distance between the sets. Given this quantification, we introduce GREASE (Graph imbalance REduction by Adding Sets of Edges), a method that selects a fixed number of unconnected node-pairs, which—if links were added between them—aims to maximally reduce the imbalance. In the job market network, GREASE can be used to select skills that job seekers do not yet have, but could strive to acquire, to reduce the imbalance between job seekers and vacancies. GREASE may also be used in other applications, such as reducing controversy between opposing sides on a polarizing topic. We evaluated GREASE on several datasets and find that GREASE outperforms baselines in reducing network imbalance.}
}


@article{DBLP:journals/tkde/AbdousMB24,
	author = {Kamel Abdous and
                  Nairouz Mrabah and
                  Mohamed Bouguessa},
	title = {Hierarchical Aggregations for High-Dimensional Multiplex Graph Embedding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1624--1637},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3305809},
	doi = {10.1109/TKDE.2023.3305809},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/AbdousMB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate the problem of multiplex graph embedding, that is, graphs in which nodes interact through multiple types of relations (dimensions). In recent years, several methods have been developed to address this problem. However, the need for more effective and specialized approaches grows with the production of graph data with diverse characteristics. In particular, real-world multiplex graphs may exhibit a high number of dimensions, making it difficult to construct a single consensus representation. Furthermore, important information can be hidden in complex latent structures scattered in multiple dimensions. To address these issues, we propose HMGE, a novel embedding method based on hierarchical aggregation for high-dimensional multiplex graphs. Hierarchical aggregation consists in learning a hierarchical combination of the graph dimensions and refining the embeddings at each hierarchy level. Non-linear combinations are computed from previous ones, thus uncovering complex information and latent structures hidden in the multiplex graph dimensions. Moreover, we leverage mutual information maximization between local patches and global summaries to train the model without supervision. This allows to captures globally relevant information present in diverse locations of the graph. Detailed experiments on synthetic and real-world data illustrate the suitability of our approach on downstream supervised tasks, including link prediction and node classification.}
}


@article{DBLP:journals/tkde/WangLSCY24,
	author = {Dan Wang and
                  Bo Li and
                  Bin Song and
                  Chen Chen and
                  F. Richard Yu},
	title = {{HSMH:} {A} Hierarchical Sequence Multi-Hop Reasoning Model With Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1638--1649},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3303617},
	doi = {10.1109/TKDE.2023.3303617},
	timestamp = {Tue, 13 Aug 2024 08:01:17 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangLSCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The incompleteness of knowledge graphs (KGs) negatively impacts the performance of KGs in downstream applications (e.g., recommendation systems and information retrieval). This phenomenon has brought an increasing rise in research related to knowledge graph reasoning. Recently, emerged reinforcement learning-based multi-hop reasoning methods can infer missing information through multi-hop reasoning according to the existing information in KGs, which has better reasoning performance and interpretability. However, these methods always use relation-entity pairs that have been pre-cropped as the action space of agents for path reasoning, which leads to two problems: 1) insufficient learning and reasoning ability of reasoning models and 2) the hard convergence of the training process of agents. To address these problems, we propose a Hierarchical Sequence Multi Hop (HSMH) reasoning framework, which consists of the interactive search reasoning model, local-global knowledge fusion mechanism, and action optimization mechanism. We use interactive search reasoning models to select relations and entities independently, thus fully mining the semantic information of relations and entities and improving the learning and reasoning ability of reasoning models. In the HSMH framework, we design the local-global knowledge fusion mechanism to acquire the local knowledge of entities and neighboring relations and the global knowledge about KG structure, which can improve the learning ability of reasoning models. Meanwhile, we design the action optimization mechanism to combine the filtered action space and the additional action space for efficient path reasoning of agents. Experimental results on benchmark datasets show that our proposed HSMH framework comprehensively outperforms the state-of-the-art multi-hop reasoning model.}
}


@article{DBLP:journals/tkde/YuJBZHLZLZ24,
	author = {Ting Yu and
                  Ting Jiang and
                  Mohamed Jaward Bah and
                  Chen Zhao and
                  Hao Huang and
                  Mengchi Liu and
                  Shuigeng Zhou and
                  Zhao Li and
                  Ji Zhang},
	title = {Incremental Maximal Clique Enumeration for Hybrid Edge Changes in
                  Large Dynamic Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1650--1666},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3311398},
	doi = {10.1109/TKDE.2023.3311398},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuJBZHLZLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incremental maximal clique enumeration (IMCE), which maintains maximal cliques in dynamic graphs, is a fundamental problem in graph analysis. A maximal clique has a solid descriptive power of dense structures in graphs. Real-world graph data is often large and dynamic. Studies on IMCE face significant challenges in the efficiency of incremental batch computation and hybrid edge changes. Moreover, with growing graph sizes, new requirements occur on indexing global maximal cliques and obtaining maximal cliques under specific vertex scope constraints. This work presents a new data structure SOMEi to maintain intermediate maximal cliques during construction. SOMEi serves as a space-efficient index to retrieve scope-constrained maximal cliques on the fly. Based on SOMEi, we design a procedure-oriented IMCE algorithm to deal with hybrid edge changes within a unified algorithm framework. In particular, the algorithm is able to process a large batch of edge changes and significantly improve the average processing time of a single edge change through an efficient pruning strategy. Experimental results on real and synthetic graph data demonstrate that the proposed algorithm outperforms all the baselines and achieves good efficiency through pruning.}
}


@article{DBLP:journals/tkde/ZhangDTH24,
	author = {Qinggang Zhang and
                  Junnan Dong and
                  Qiaoyu Tan and
                  Xiao Huang},
	title = {Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1667--1682},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3310149},
	doi = {10.1109/TKDE.2023.3310149},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangDTH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graphs (KGs) can structurally organize large-scale information in the form of triples and significantly support many real-world applications. While most KG embedding algorithms hold the assumption that all triples are correct, considerable errors were inevitably injected during the construction process. It is urgent to develop effective error-aware KG embedding, since errors in KGs would lead to significant performance degradation in downstream applications. To this end, we propose a novel framework named Attributed Error-aware Knowledge Embedding (AEKE). It leverages the semantics contained in entity attributes to guide the KG embedding model learning against the impact of erroneous triples. We design two triple-level hypergraphs to model the topological structures of the KG and its attributes, respectively. The confidence score of each triple is jointly calculated based on self-contradictory within the triple, consistency between local and global structures, and homogeneity between structures and attributes. We leverage confidence scores to adaptively update the weighted aggregation in the multi-view graph learning framework and margin loss in KG embedding, such that potential errors will contribute little to KG learning. Experiments on three real-world KGs demonstrate that AEKE outperforms state-of-the-art KG embedding and error detection algorithms.}
}


@article{DBLP:journals/tkde/ZhangPLCLWLW24,
	author = {Yufeng Zhang and
                  Jialu Pan and
                  Wanwei Liu and
                  Zhenbang Chen and
                  Kenli Li and
                  Ji Wang and
                  Zhiming Liu and
                  Hongmei Wei},
	title = {Kullback-Leibler Divergence-Based Out-of-Distribution Detection With
                  Flow-Based Generative Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1683--1697},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3309853},
	doi = {10.1109/TKDE.2023.3309853},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangPLCLWLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent research has revealed that deep generative models including flow-based models and Variational Autoencoders may assign higher likelihoods to out-of-distribution (OOD) data than in-distribution (ID) data. However, we cannot sample OOD data from the model. This counterintuitive phenomenon has not been satisfactorily explained and brings obstacles to OOD detection with flow-based models. In this article, we prove theorems to investigate the Kullback-Leibler divergence in flow-based model and give two explanations for the above phenomenon. Based on our theoretical analysis, we propose a new method KLODS to leverage KL divergence and local pixel dependence of representations to perform anomaly detection. Experimental results on prevalent benchmarks demonstrate the effectiveness and robustness of our method. For group anomaly detection, our method achieves 98.1% AUROC on average with a small batch size of 5. On the contrary, the baseline typicality test-based method only achieves 64.6% AUROC on average due to its failure on challenging problems. Our method also outperforms the state-of-the-art method by 9.1% AUROC. For point-wise anomaly detection, our method achieves 90.7% AUROC on average and outperforms the baseline by 5.2% AUROC. Besides, our method has the least notable failures and is the most robust one.}
}


@article{DBLP:journals/tkde/XingCHLZ24,
	author = {Rong Xing and
                  Rui Cheng and
                  Jiwen Huang and
                  Qing Li and
                  Jingmei Zhao},
	title = {Learning to Understand the Vague Graph for Stock Prediction With Momentum
                  Spillovers},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1698--1712},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3310592},
	doi = {10.1109/TKDE.2023.3310592},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XingCHLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the realm of deep graph learning, our study uniquely addresses the under-explored area of vague graph learning. While the effectiveness of deep graph learning is recognized across various disciplines, the nuances of vague graph learning — whether its inherent vagueness should be incorporated or disregarded and its influence on deep graph learning efficiency — remain largely unexamined. We fill this gap by introducing a novel decoupled graph learning framework. This is achieved by proposing a matrix-based or a tensor-based fusion module to estimate unobservable node attributes, a hybrid attention mechanism to bridge nodes with both explicit and implicit relationships, and a message-passing mechanism for feature-sensitive transporting. The design principle of decoupling allows it to accommodate ambiguities in any or all of these aspects of node representation, linking, and message passing. Furthermore, we leverage an extensive stock dataset spanning 64 years across the entire U.S. market to assess our framework. This real-world data not only adds a practical dimension to our study but also highlights the effectiveness of vague graph learning. Remarkably, our framework demonstrates superiority over state-of-the-art algorithms, marking performance enhancements of at least 6.73%, 7.25%, and 11.39% in terms of Rank IC, R^{2}, and Rank ICIR, respectively.}
}


@article{DBLP:journals/tkde/ZhengCWZYZ24,
	author = {Shangfei Zheng and
                  Wei Chen and
                  Weiqing Wang and
                  Pengpeng Zhao and
                  Hongzhi Yin and
                  Lei Zhao},
	title = {Multi-Hop Knowledge Graph Reasoning in Few-Shot Scenarios},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1713--1727},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3304665},
	doi = {10.1109/TKDE.2023.3304665},
	timestamp = {Thu, 30 Jan 2025 15:15:46 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengCWZYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reinforcement learning (RL)-based multi-hop reasoning has become an interpretable way for knowledge graph reasoning owing to its persuasive explanations for the predicted results, but the reasoning performance of these methods drops significantly over few-shot relations (only contain few triplets). To address this problem, recent studies introduce meta-learning into RL-based reasoning methods. However, the performance of these studies is still limited due to the following points: (1) the overall reasoning accuracy is impaired due to the low reasoning accuracies over some hard relations; (2) the reasoning process becomes laborious and ineffective owing to the existence of noisy data; (3) the generalizability is negatively affected due to the lack of knowledge-sharing. To tackle these challenges, we propose a novel model HMLS consisting of two modules HHML (Hierarchical Hardness-aware Meta-reinforcement Learning) and HHS (Hierarchical Hardness-aware Sampling). Specifically, HHML contains the following two components: (1) a hardness-aware RL conducts multi-hop reasoning by training hardness-aware batches and reducing noise; (2) a knowledge-sharing meta-learning adapts to few-shot relations by exploiting common features in the hierarchical relation structure. The other module HHS generates hardness-aware batches from relation and relation-cluster levels. The experimental results demonstrate that this work notably outperforms the state-of-the-art approaches in few-shot scenarios.}
}


@article{DBLP:journals/tkde/LuRWNN24,
	author = {Chang Lu and
                  Chandan K. Reddy and
                  Ping Wang and
                  Dong Nie and
                  Yue Ning},
	title = {Multi-Label Clinical Time-Series Generation via Conditional {GAN}},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1728--1740},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3310909},
	doi = {10.1109/TKDE.2023.3310909},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuRWNN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, deep learning has been successfully adopted in a wide range of applications related to electronic health records (EHRs) such as representation learning and clinical event prediction. However, due to privacy constraints, limited access to EHR becomes a bottleneck for deep learning research. To mitigate these concerns, generative adversarial networks (GANs) have been successfully used for generating EHR data. However, there are still challenges in high-quality EHR generation, including generating time-series EHR data and imbalanced uncommon diseases. In this work, we propose a Multi-label Time-series GAN (MTGAN) to generate EHR and simultaneously improve the quality of uncommon disease generation. The generator of MTGAN uses a gated recurrent unit (GRU) with a smooth conditional matrix to generate sequences and uncommon diseases. The critic gives scores using Wasserstein distance to recognize real samples from synthetic samples by considering both data and temporal features. We also propose a training strategy to calculate temporal features for real data and stabilize GAN training. Furthermore, we design multiple statistical metrics and prediction tasks to evaluate the generated data. Experimental results demonstrate the quality of the synthetic data and the effectiveness of MTGAN in generating realistic sequential EHR data, especially for uncommon diseases.}
}


@article{DBLP:journals/tkde/WuZZFZ24,
	author = {Weichang Wu and
                  Xiaolu Zhang and
                  Shiwan Zhao and
                  Chilin Fu and
                  Jun Zhou},
	title = {Multi-Task Decouple Learning With Hierarchical Attentive Point Process},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1741--1757},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3305628},
	doi = {10.1109/TKDE.2023.3305628},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuZZFZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential data mining is ubiquitous in various scenarios. Modeling event sequence and predicting event occurrence is of vital importance in sequential data mining, and Temporal Point Processes (TPP) are widely used in this area. Conventional TPP use objective functions as sum of classification loss for event type and regression loss for occurrence time, leading to practical limitations that conventional TPP is unable to predict the occurrence of each type of event and distinguish the dependency within and between different event types. To tackle these defects, we propose a Multi-task Decouple Learning (MTDL) framework to model TPP from a novel perspective of Multi-task Learning (MTL), i.e., predicting the next-step occurrence time for all event types using a weighted multi-task regression loss. We experiment with three state-of-the-arts, showing that the proposed MTDL framework can improve the performance of original TPP models. Moreover, we develop a Hierarchical Attentive Point Process (HAPP) to further exploit the potential of the proposed MTDL framework, using a hierarchical attention mechanism to capture the inner-sequence time dependency within the same type of events and the inter-sequence dependency between different types of events. Experiments on real-world business dataset and public datasets show the efficacy of the proposed method.}
}


@article{DBLP:journals/tkde/WangKLWHWY24,
	author = {Haotian Wang and
                  Kun Kuang and
                  Long Lan and
                  Zige Wang and
                  Wanrong Huang and
                  Fei Wu and
                  Wenjing Yang},
	title = {Out-of-Distribution Generalization With Causal Feature Separation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1758--1772},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312255},
	doi = {10.1109/TKDE.2023.3312255},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangKLWHWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driven by empirical risk minimization, machine learning algorithm tends to exploit subtle statistical correlations existing in the training environment for prediction, while the spurious correlations are unstable across environments, leading to poor generalization performance. Accordingly, the problem of the Out-of-distribution (OOD) generalization aims to exploit an invariant/stable relationship between features and outcomes that generalizes well on all possible environments. To address the spurious correlation induced by the selection bias, in this article, we propose a novel Clique-based Causal Feature Separation (CCFS) algorithm by explicitly incorporating the causal structure to identify causal features of outcome for OOD generalization. Specifically, the proposed CCFS algorithm identifies the largest clique in the learned causal skeleton. Theoretically, we guarantee that either the largest clique or the rest of the causal skeleton is exactly the set of all causal features of the outcome. Finally, we separate the causal features from the non-causal ones with a sample-reweighting decorrelator for OOD prediction. Extensive experiments validate the effectiveness of the proposed CCFS method on both causal feature identification and OOD generalization tasks.}
}


@article{DBLP:journals/tkde/WangLZHWW24,
	author = {Liping Wang and
                  Qiang Liu and
                  Mengqi Zhang and
                  Yaxuan Hu and
                  Shu Wu and
                  Liang Wang},
	title = {Stage-Aware Hierarchical Attentive Relational Network for Diagnosis
                  Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1773--1784},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3310478},
	doi = {10.1109/TKDE.2023.3310478},
	timestamp = {Fri, 22 Mar 2024 09:01:03 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangLZHWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, Electronic Health Records (EHR) have become valuable for enhancing medical decision making, as well as online disease detection and monitoring. Meanwhile, deep learning-based methods have achieved great success in health risk prediction and diagnosis prediction based on EHR. Nevertheless, deep learning-based models usually require high volumes of data due to the vast amount of parameters. In addition, a considerable proportion of medical codes appear rarely in the EHR data which poses huge difficulties for clinical applications. Hence, some works propose to adopt medical ontologies to enhance the prediction performance and provide interpretable prediction results. However, these medical ontologies are often small-scale and coarse-grained, most of diagnoses and medical concepts are not included, lacking many diagnoses and medical concepts, let alone various relationships between these concepts. To overcome this limitation, we propose to incorporate existing large-scale medical knowledge graphs (KGs) into diagnosis prediction and devise a Stage-aware Hierarchical Attentive Relational Network, named HAR. Specifically, for each visit, a personalized sub-KG is extracted from the existing medical KG, on which HAR conducts relation-specific message passing and hierarchical message aggregation to refine representations of nodes that correspond to medical codes in visits. HAR takes the specific stage of a patient's disease progression into consideration, which participates in the computation of relation-level and node-level attention. Extensive experiments on two public datasets demonstrate the effectiveness of HAR in improving both the visit-level precision and code-level accuracy of the diagnosis prediction task.}
}


@article{DBLP:journals/tkde/WangZWJWZY24,
	author = {Huandong Wang and
                  Qizhong Zhang and
                  Yuchen Wu and
                  Depeng Jin and
                  Xing Wang and
                  Lin Zhu and
                  Li Yu},
	title = {Synthesizing Human Trajectories Based on Variational Point Processes},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1785--1799},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312209},
	doi = {10.1109/TKDE.2023.3312209},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangZWJWZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Synthesized human trajectories are instrumental for a large number of applications. However, existing trajectory synthesizing models are limited in either modeling variable-length trajectories with continuous temporal distribution or incorporating multi-dimensional context information. In this paper, we propose a novel probabilistic model based on the variational temporal point process to synthesize human trajectories. This model combines the classical temporal point process with the novel neural variational inference framework, leading to its strong ability to model human trajectories with continuous temporal distribution, variable length, and multi-dimensional context information. Extensive experimental results on two real-world trajectory datasets show that our proposed model can synthesize trajectories most similar to real-world human trajectories compared with four representative baseline algorithms in terms of a number of usability metrics, demonstrating its effectiveness.}
}


@article{DBLP:journals/tkde/MiaoZLZZQZJ24,
	author = {Hao Miao and
                  Xiaolong Zhong and
                  Jiaxin Liu and
                  Yan Zhao and
                  Xiangyu Zhao and
                  Weizhu Qian and
                  Kai Zheng and
                  Christian S. Jensen},
	title = {Task Assignment With Efficient Federated Preference Learning in Spatial
                  Crowdsourcing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1800--1814},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3311816},
	doi = {10.1109/TKDE.2023.3311816},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MiaoZLZZQZJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial Crowdsourcing (SC) is finding widespread application in today's online world. As we have transitioned from desktop crowdsourcing applications (e.g., Wikipedia) to SC applications (e.g., Uber), there is a sense that SC systems must not only provide effective task assignment but also need to ensure privacy. To achieve these often-conflicting objectives, we propose a framework, Task Assignment with Federated Preference Learning, that performs task assignment based on worker preferences while keeping the data decentralized and private in each platform center (e.g., each delivery center of an SC company). The framework includes a federated preference learning phase and a task assignment phase. Specifically, in the first phase, we build a local preference model for each platform center based on historical data. We provide means of horizontal federated learning that makes it possible to collaboratively train these local preference models under the orchestration of a central server. Specifically, we provide a practical method that accelerates federated preference learning based on stochastic controlled averaging and achieves low communication costs while considering data heterogeneity among clients. The task assignment phase aims to achieve effective and efficient task assignment by considering workers’ preferences. Extensive evaluations on real data offer insight into the effectiveness and efficiency of the paper's proposals.}
}


@article{DBLP:journals/tkde/LinZWZPCWWY24,
	author = {Xixun Lin and
                  Chuan Zhou and
                  Jia Wu and
                  Lixin Zou and
                  Shirui Pan and
                  Yanan Cao and
                  Bin Wang and
                  Shuaiqiang Wang and
                  Dawei Yin},
	title = {Towards Flexible and Adaptive Neural Process for Cold-Start Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1815--1828},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3304839},
	doi = {10.1109/TKDE.2023.3304839},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LinZWZPCWWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems have been widely adopted in various online personal e-commerce applications for improving user experience. A long-standing challenge in recommender systems is how to provide accurate recommendation to users in cold-start situations where only a few user-item interactions can be observed. Recently, meta learning methods provide a promising solution, and most of them follow a way of parameter initialization where predictions can be fast adapted via multiple gradient descent steps. While these meta-learning recommenders promote model performance, how to derive a fundamental paradigm that enables both flexible approximations of complex user interaction distributions and effective task adaptations of global knowledge still remains a critical yet under-explored problem. To this end, we present the Flow-based Adaptive Neural Process (FANP), a new probabilistic meta-learning model where estimating the preference of each user is governed by an underlying stochastic process. Following an encoder-decoder generative framework, FANP is an effective few-shot function estimator that directly maps limited user interactions to a predictive distribution without complicated gradient updates. Through introducing a conditional normalization flow-based encoder, FANP can get rid of the model bias on latent variables and thereby derive more flexible variational distributions. Meanwhile, we propose a task-adaptive mechanism capturing the relevance of different tasks for improving adaptation ability of global knowledge. The learned task-specific and task-relevant representations are simultaneously exploited to generate the decoder parameters via a novel modulation-augmented hypernetwork. FANP is evaluated on both scenario-specific and user-specific cold-start recommendations on various real-world datasets. Extensive experimental results and detailed model analyses demonstrate that our model yields superior performance compared with multiple state-of-the-art meta-learning recommenders.}
}


@article{DBLP:journals/tkde/ChangXWLS24,
	author = {Zhao Chang and
                  Dong Xie and
                  Sheng Wang and
                  Feifei Li and
                  Yulong Shen},
	title = {Towards Practical Oblivious Join Processing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {4},
	pages = {1829--1842},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3310038},
	doi = {10.1109/TKDE.2023.3310038},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChangXWLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In cloud computing, remote accesses over the cloud data inevitably bring the issue of trust. Despite strong encryption schemes, adversaries can still learn sensitive information from encrypted data by observing data access patterns. Oblivious RAMs (ORAMs) are proposed to protect against access pattern attacks. However, directly deploying ORAM constructions in an encrypted database brings large computational overhead. In this work, we focus on oblivious joins over a cloud database. Existing studies in the literature are restricted to either primary-foreign key joins or binary equi-joins. Our major contribution is to support general band joins and multiway equi-joins. For oblivious join without ORAMs, we extend the existing binary equi-join algorithm to support general band joins obliviously. For oblivious join with ORAMs, we integrate B\n-tree indices into ORAMs for each input table and retrieve blocks through the indices in join processing. The key point is to avoid retrieving tuples that make no contribution to the final join result and bound the number of accesses to each B\n-tree index. The effectiveness and efficiency of our algorithms are demonstrated through extensive evaluations over real-world datasets. Our method shows orders of magnitude speedup for oblivious multiway equi-joins in comparison with baseline algorithms.}
}


@article{DBLP:journals/tkde/YeZCXY24,
	author = {Ziming Ye and
                  Xiao Zhang and
                  Xu Chen and
                  Hui Xiong and
                  Dongxiao Yu},
	title = {Adaptive Clustering Based Personalized Federated Learning Framework
                  for Next {POI} Recommendation With Location Noise},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1843--1856},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312511},
	doi = {10.1109/TKDE.2023.3312511},
	timestamp = {Sat, 04 May 2024 10:55:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YeZCXY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next point-of-interest (POI) recommendation has been a hot research topic, which enables new paradigms for kinds of location-based services in real-world scenarios. Due to the privacy concerns and rigorous data regulations, federated learning provides a distributed learning framework to collaboratively train the recommendation model without sharing the highly sensitive POI data with others. However, there exist two main challenges, namely location noise, and balance between personalization and knowledge sharing, seriously restrict the development of the federated next POI recommendation. To this end, in this work, we propose an adaptive clustering based personalized federated learning framework for next POI recommendation with location noise, named CPF-POI, to address the above challenges. In detail, within the local client, a location recovery module can efficiently remove noises under the given assumption from the noisy POI data in which the recovery error bound can be theoretically proved. Then, within the parameter server, an adaptive clustering scheme is proposed to capture the internal relatedness among all clients to augment positive knowledge sharing. In order to make a balance between personalization and knowledge sharing under personalized federated learning framework, we design an alternative optimization process between clustering similar clients and minimizing local personalized loss functions. Finally, extensive experiments are conducted on two diverse real-world datasets to show the advantages of CPF-POI over state-of-the-art methods.}
}


@article{DBLP:journals/tkde/GaoFXGSC24,
	author = {Yunjun Gao and
                  Ziquan Fang and
                  Jiachen Xu and
                  Shenghao Gong and
                  Chunhui Shen and
                  Lu Chen},
	title = {An Efficient and Distributed Framework for Real-Time Trajectory Stream
                  Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1857--1873},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312319},
	doi = {10.1109/TKDE.2023.3312319},
	timestamp = {Sat, 04 May 2024 10:55:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GaoFXGSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive ubiquity of GPS-equipped devices, e.g., mobile phones, vehicles, and vessels, a massive amount of real-time, unbounded, and varying-sampling trajectory streams are being generated continuously. Clustering trajectory streams is useful in real-life applications, such as traffic congestion prediction, crowd flow detection, and moving behavior study. Although several sliding-window based algorithms (that adopt the classic two-phases online-offline processing framework) are proposed for trajectory stream clustering, three challenges exist to meet ever-increasing application demands for effective, efficient, and scalable online clustering: i) How to effectively model unbounded trajectory streams in the online settings for effective clustering? ii) How to achieve truly real-time online processing? iii) How to improve the scalable capability of the clustering algorithm to support large-scale moving trajectory streams? In this paper, we propose an efficient and distributed trajectory stream clustering framework that can: i) model trajectory streams dynamically and effectively in a self-adaptive manner, i.e., k\n-Segment, which considers both spatial and temporal aspects of trajectory streams, ii) support distributed indexing, processing, and workload balance, and iii) incrementally cluster trajectory streams in an efficient manner. Experiments on a wide range of real-world trajectory datasets show that our framework outperforms state-of-the-art baselines in terms of clustering quality, efficiency, and scalability.}
}


@article{DBLP:journals/tkde/LiSYHZZZC24,
	author = {Meng Li and
                  Yanzhe Shen and
                  Guixin Ye and
                  Jialing He and
                  Xin Zheng and
                  Zijian Zhang and
                  Liehuang Zhu and
                  Mauro Conti},
	title = {Anonymous, Secure, Traceable, and Efficient Decentralized Digital
                  Forensics},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1874--1888},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3321712},
	doi = {10.1109/TKDE.2023.3321712},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiSYHZZZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital forensics is crucial to fight crimes around the world. Decentralized Digital Forensics (DDF) promotes it to another level by channeling the power of blockchain into digital investigations. In this work, we focus on the privacy and security of DDF. Our motivations arise from (1) how to track an anonymous-and-malicious data user who leaks only a part of the previously requested data, (2) how to achieve access control while protecting data from untrusted data centers, and (3) how to enable efficient and secure search on the blockchain. To address these issues, we propose Themis: an anonymous and secure DDF scheme with traceable anonymity, private access control, and efficient search. Our framework is boosted by establishing a Trusted Execution Environment in each authority (blockchain node) for securing the uploading, requesting, and searching. To instantiate the framework, we design a secure and robust watermarking scheme in conjunction with decentralized anonymous authentication, a private and fine-grained access control scheme, and an efficient and secure search scheme based on a dynamically updated data structure. We formally define and prove the privacy and security of Themis. We build a prototype with Ethereum and Intel SGX2 to evaluate its performance, which supports processing data from a considerable number of data providers and investigators.}
}


@article{DBLP:journals/tkde/LuNDWL24,
	author = {Jitao Lu and
                  Feiping Nie and
                  Xia Dong and
                  Rong Wang and
                  Xuelong Li},
	title = {Bidirectional Attentive Multi-View Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1889--1901},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312794},
	doi = {10.1109/TKDE.2023.3312794},
	timestamp = {Sun, 19 Jan 2025 13:53:51 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LuNDWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The key challenge of multi-view graph-based clustering is to mine consistent clustering structures from multiple graphs. Existing works seek clustering decisions from either multiple spectral embeddings or multiple affinity matrices, ignoring the interactions among them. To address this problem, we propose a Bidirectional Attentive Multi-view Clustering (BAMC) model to explore a consensus space w.r.t. spectral embedding and affinity matrix simultaneously, where they can promote each other to mine richer structural information from multiple graphs. BAMC is composed of a Spectral Embedding Learning (SEL) module, an Affinity Matrix Learning (AML) module, and a Bidirectional Attentive Clustering (BAC) module. SEL seeks consensus spectral embeddings by aligning the distributions of elements sampled from subspaces spanned by multiple spectral embeddings. AML learns a consensus affinity matrix from input affinity matrices. BAC guarantees consistency between the learned consensus spectral embeddings and the affinity matrix. To balance their effects, it also assigns adaptive weights to SEL and AML's objective functions. To solve the optimization problem involved in BAMC, we propose an efficient algorithm based on the Majority-Minimization framework with an ingenious surrogate problem. Extensive experiments on several synthetic and real-world datasets demonstrate the superb performance of BAMC.}
}


@article{DBLP:journals/tkde/WanZZZZCZLCLPD24,
	author = {Weitao Wan and
                  Feng Zhang and
                  Chenyang Zhang and
                  Mingde Zhang and
                  Jidong Zhai and
                  Yunpeng Chai and
                  Huanchen Zhang and
                  Wei Lu and
                  Yuxing Chen and
                  Haixiang Li and
                  Anqun Pan and
                  Xiaoyong Du},
	title = {Compressed Data Direct Computing for Databases},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1902--1918},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3316274},
	doi = {10.1109/TKDE.2023.3316274},
	timestamp = {Tue, 28 Jan 2025 12:51:52 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WanZZZZCZLCLPD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Directly performing operations on compressed data has been proven to be a big success facing Big Data problems in modern data management systems. These systems have demonstrated significant compression benefits and performance improvement for data analytics applications. However, current systems only focus on data queries, while a complete Big Data system must support both data query and data manipulation. To solve this problem, we develop CompressDB, which is a new storage engine that can support data processing for databases without decompression. CompressDB has the following advantages. First, CompressDB utilizes context-free grammar to compress data, and supports both data query and data manipulation. Second, for adaptability, we integrate CompressDB to file systems so that a wide range of databases can directly use CompressDB without any change. Third, we enable operation pushdown to storage so that we can perform data query and manipulation in storage systems without bringing large data to memory for high efficiency. We validate the efficacy of CompressDB supporting various kinds of database systems, including SQLite, MySQL, LevelDB, MongoDB, ClickHouse, and Neo4j. We evaluate our method using seven real-world datasets with various lengths, structures, and content in both single node and cluster environments. Experiments show that CompressDB achieves 40% throughput improvement and 44% latency reduction, along with 1.75 compression ratio on average.}
}


@article{DBLP:journals/tkde/QinHFKLC24,
	author = {Hong Qin and
                  Debiao He and
                  Qi Feng and
                  Muhammad Khurram Khan and
                  Min Luo and
                  Kim{-}Kwang Raymond Choo},
	title = {Cryptographic Primitives in Privacy-Preserving Machine Learning: {A}
                  Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1919--1934},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3321803},
	doi = {10.1109/TKDE.2023.3321803},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QinHFKLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advances in machine learning have enabled a broad range of complex applications, such as image recognition, recommendation system and machine translation. Data plays an important role in our increasingly complex and diverse environments, and this also reinforces the importance of data privacy in machine learning-enabled applications. Although there are a number of literature survey articles on machine learning, only a few studies have investigated the cryptographic primitives used in privacy-preserving machine learning (PPML). In other words, there is no, or limited, systematization of knowledge (SoK) that provides a comprehensive introduction to cryptography that have been deployed in PPML. In this paper, we first introduce some basic concepts such as machine learning tasks and processes. Then, we review and systematize the cryptographic primitives used in PPML. We analyze these existing privacy-preserving schemes in their learning process, especially training and inference. Finally, we conclude our survey and provide an outlook on future trends and research directions in the field.}
}


@article{DBLP:journals/tkde/JiangCHXB24,
	author = {Jiaxin Jiang and
                  Byron Choi and
                  Xin Huang and
                  Jianliang Xu and
                  Sourav S. Bhowmick},
	title = {{DKWS:} {A} Distributed System for Keyword Search on Massive Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1935--1950},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3313726},
	doi = {10.1109/TKDE.2023.3313726},
	timestamp = {Sat, 04 May 2024 10:55:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiangCHXB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the unstructuredness and the lack of schemas of graphs, such as knowledge graphs, social networks, and RDF graphs, keyword search for querying such graphs has been proposed. As graphs have become voluminous, large-scale distributed processing has attracted much interest from the database research community. While there have been several distributed systems, distributed querying techniques for keyword search are still limited. This paper proposes a novel distributed keyword search system called \\mathsf {DKWS}. First, we present a monotonic property with keyword search algorithms that guarantees correct parallelization. Second, we present a keyword search algorithm as monotonic backward and forward search phases. Moreover, we propose new tight bounds for pruning nodes being searched. Third, we propose a notify-push paradigm and \\mathsf {PINE} programming model of \\mathsf {DKWS}. The notify-push paradigm allows asynchronously exchanging the upper bounds of matches across the workers and the coordinator in \\mathsf {DKWS}. The \\mathsf {PINE} programming model naturally fits keyword search algorithms, as they have distinguished phases, to allow preemptive searches to mitigate staleness in a distributed system. Finally, we investigate the performance and effectiveness of \\mathsf {DKWS} through experiments using real-world datasets. We find that \\mathsf {DKWS} is up to two orders of magnitude faster than related techniques, and its communication costs are 7.6 times smaller than those of other techniques.}
}


@article{DBLP:journals/tkde/ZouYSDWZ24,
	author = {Tao Zou and
                  Le Yu and
                  Leilei Sun and
                  Bowen Du and
                  Deqing Wang and
                  Fuzhen Zhuang},
	title = {Event-Based Dynamic Graph Representation Learning for Patent Application
                  Trend Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1951--1963},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312333},
	doi = {10.1109/TKDE.2023.3312333},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZouYSDWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modeling companies’ continuously evolving preferences and capturing the semantic correlations of classification codes. To fill this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies.}
}


@article{DBLP:journals/tkde/ChenGCZ24,
	author = {Jun{-}Xian Chen and
                  Yue{-}Jiao Gong and
                  Wei{-}Neng Chen and
                  Jun Zhang},
	title = {EvoS{\&}R: Evolving Multiple Seeds and Radii for Varying Density
                  Data Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1964--1978},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312760},
	doi = {10.1109/TKDE.2023.3312760},
	timestamp = {Tue, 28 May 2024 17:12:53 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenGCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Density clustering has shown advantages over other types of clustering methods for processing arbitrarily shaped datasets. In recent years, extensive research efforts has been made on the improvements of DBSCAN or the algorithms incorporating the concept of density peaks. However, these previous studies remain the problems of being sensitive to the parameter settings, and some of them will stuck in weak results when encountering the situations of varying-density distributions. To overcome these issues, we propose an evolution framework named EvoS&R that evolves multiple seeds and the corresponding radii for varying-density data clustering. Compared with the traditional methods, EvoS&R handles the parameter tuning and multi-density fitting problems in an integrated and straightforward manner. Note that, however, the underlying task in EvoS&R is a mixed-variable optimization problem that is challenging in nature. We specifically design a hybrid encoding differential evolution algorithm with novel encoding, mutation, etc., to solve the optimization problem efficiently. Extensive experiments on density-based datasets shows that our algorithm outperforms the other state-of-the-arts in most cases, which validates the effectiveness of the proposed method.}
}


@article{DBLP:journals/tkde/LiZCYWGHZ24,
	author = {Youru Li and
                  Zhenfeng Zhu and
                  Linxun Chen and
                  Bin Yang and
                  Yaxi Wu and
                  Xiaobo Guo and
                  Bing Han and
                  Yao Zhao},
	title = {Exploring Large-Scale Financial Knowledge Graph for SMEs Supply Chain
                  Mining},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1979--1990},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3317631},
	doi = {10.1109/TKDE.2023.3317631},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiZCYWGHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While large enterprises are benefiting from their global supply chains in these years, it is not easy for Small and Medium-sized Enterprises (SMEs) to find supply chain partners. Treating it as a supply chain mining problem, some deep learning methods, especially knowledge graph (KG) enhanced ones, can achieve workable performance by utilizing explicit structure information from KG while considering effectiveness. However, such improvement is limited when facing the challenges of scalability, complexity, and noisiness in large-scale KGs. To address these issues, we propose a novel Meta-tag Supported Connectivity representation Learning framework, also known as MSCL. Specifically, a Meta-tag Collaborative Filtering (MCF) method is proposed to highlight the representative schema from huge number of paths connecting two enterprises in large-scale KG. Furthermore, the DPPs-induced Hierarchical Path Sampling (DHPS), a novel sampling framework, is also developed to capture the latent connectivity pattern in KG more effectively. Moreover, the path-wise knowledge representations and the underlying information inherent in pairwise enterprises are aggregated by a connectivity representation learning (CRL) approach for SMEs supply chain mining. Experimental results from two real-world industries have illustrated that the proposed model can achieve competitive performance compared with other existing baselines.}
}


@article{DBLP:journals/tkde/LiZXHWXHDX24,
	author = {Shuangli Li and
                  Jingbo Zhou and
                  Tong Xu and
                  Liang Huang and
                  Fan Wang and
                  Haoyi Xiong and
                  Weili Huang and
                  Dejing Dou and
                  Hui Xiong},
	title = {GIaNt: Protein-Ligand Binding Affinity Prediction via Geometry-Aware
                  Interactive Graph Neural Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {1991--2008},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3314502},
	doi = {10.1109/TKDE.2023.3314502},
	timestamp = {Sat, 10 Aug 2024 14:34:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiZXHWXHDX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Drug discovery often relies on the successful prediction of protein-ligand binding affinity. Recent advances have shown great promise in applying graph neural networks (GNNs) for better affinity prediction by learning the representations of protein-ligand complexes. However, existing solutions usually treat protein-ligand complexes as topological graph data, thus the 3D geometry-based biomolecular structural information is not fully utilized. The essential intermolecular interactions with long-range dependencies, including type-wise interactions and molecule-wise interactions, are also neglected in GNN models. To this end, we propose a geometry-aware interactive graph neural network (GIaNt) which consists of two components: 3D geometric graph learning network (3DG-Net) and pairwise interactive learning network (Pi-Net). Specifically, 3DG-Net iteratively performs the node-edge interaction process to update embeddings of nodes and edges in a unified framework while preserving the 3D geometric factors among atoms, including spatial distance, polar angle and dihedral angle information in 3D space. Moreover, Pi-Net is adopted to incorporate both element type-level and molecule-level interactions. Specially, interactive edges are gathered with a subsequent reconstruction loss to reflect the global type-level interactions. Meanwhile, a pairwise attentive pooling scheme is designed to identify the critical interactive atoms for complex representation learning from a semantic view. An exhaustive experimental study on two benchmarks verifies the superiority of GIaNt.}
}


@article{DBLP:journals/tkde/WangJZWQG24,
	author = {Chenxu Wang and
                  Peijing Jiang and
                  Xiangliang Zhang and
                  Pinghui Wang and
                  Tao Qin and
                  Xiaohong Guan},
	title = {GTCAlign: Global Topology Consistency-Based Graph Alignment},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2009--2025},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312358},
	doi = {10.1109/TKDE.2023.3312358},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangJZWQG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph alignment aims to find correspondent nodes between two graphs. Most existing algorithms assume that correspondent nodes in different graphs have similar local structures. However, this principle may not apply to some real-world application scenarios when two graphs have different densities. Some correspondent node pairs may have very different local structures in these cases. Nevertheless, correspondent nodes are expected to have similar importance, inspiring us to exploit global topology consistency for graph alignment. This paper presents GTCAlign, an unsupervised graph alignment framework based on global topology consistency. An indicating matrix is calculated to show node pairs with consistent global topology based on a comprehensive centrality metric. A graph convolutional network (GCN) encodes local structural and attributive information into low-dimensional node embeddings. Then, node similarities are computed based on the obtained node embeddings under the guidance of the indicating matrix. Moreover, a pair of nodes are more likely to be aligned if most of their neighbors are aligned, motivating us to develop an iterative algorithm to refine the alignment results recursively. We conduct extensive experiments on real-world and synthetic datasets to evaluate the effectiveness of GTCAlign. The experimental results show that GTCAlign outperforms state-of-the-art graph alignment approaches.}
}


@article{DBLP:journals/tkde/HuHZLMWKY24,
	author = {Xuming Hu and
                  Zhaochen Hong and
                  Chenwei Zhang and
                  Aiwei Liu and
                  Shiao Meng and
                  Lijie Wen and
                  Irwin King and
                  Philip S. Yu},
	title = {Reading Broadly to Open Your Mind: Improving Open Relation Extraction
                  With Search Documents Under Self-Supervisions},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2026--2040},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3317139},
	doi = {10.1109/TKDE.2023.3317139},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuHZLMWKY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional dependency on external assumptions. However, these works can only obtain information signals from limited existing knowledge bases or datasets. In this work, we propose a self-supervised framework named Web-SelfORE, which exploits self-supervised signals by requiring a large pretrained language model to extensively read real-world relevant documents from the web, and obtain contextualized relational features by mixing contextualized representations of entities from different documents. We perform adaptive clustering on contextualized relational features and bootstrap the self-supervised signals by improving contextualized features in relation classification. We additionally compare the effectiveness of self-supervisions brought by different document sources, and introduce relevance and redundancy evaluation metrics to obtain higher-quality self-supervisions. Experimental results on four public datasets show the effectiveness and robustness of Web-SelfORE on open-domain relation extraction task when comparing with competitive baselines.}
}


@article{DBLP:journals/tkde/ZhouWL24,
	author = {Xiaoling Zhou and
                  Ou Wu and
                  Mengyang Li},
	title = {Investigating the Sample Weighting Mechanism Using an Interpretable
                  Weighting Framework},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2041--2055},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3316168},
	doi = {10.1109/TKDE.2023.3316168},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Training deep learning models with unequal sample weights has been shown to enhance model performance in various typical learning scenarios, particularly for imbalanced and noisy-label learning scenarios. A deep understanding of the weighting mechanism facilitates the application of existing weighting strategies and illuminates the design of new weighting strategies for real learning tasks. Scholars have focused on exploring existing weighting methods. However, their studies mainly establish how the weights of samples influence the model training. Little headway is made on the weighting mechanism, i.e., which and how the characteristics of a sample influence its weight. In this study, we adopt a data-driven approach to investigate the weighting mechanism by utilizing an interpretable weighting framework. First, a wide range of sample characteristics is extracted from the classifier network during training. Second, the extracted characteristics are fed into a new neural regression tree (NRT), which is a tree model implemented by a neural network, and its output is the weight of the input sample. Third, the NRT is trained using meta-learning within the whole training process. Once the NRT is learned, the weighting mechanism, including the importance of weighting characteristics, prior modes, and specific weighting rules, can be obtained. We conduct extensive experiments on benchmark noisy and imbalanced data corpora. A package of weighting mechanisms is derived from the learned NRT. Furthermore, our proposed interpretable weighting framework exhibits superior performance in comparison to existing weighting strategies.}
}


@article{DBLP:journals/tkde/FengMXWWZZ24,
	author = {Shibo Feng and
                  Chunyan Miao and
                  Ke Xu and
                  Jiaxiang Wu and
                  Pengcheng Wu and
                  Yang Zhang and
                  Peilin Zhao},
	title = {Multi-Scale Attention Flow for Probabilistic Time Series Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2056--2068},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3319672},
	doi = {10.1109/TKDE.2023.3319672},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FengMXWWZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The probability prediction of multivariate time series is a notoriously challenging but practical task. On the one hand, the challenge is how to effectively capture the cross-series correlations between interacting time series, to achieve accurate distribution modeling. On the other hand, we should consider how to capture the contextual information within time series more accurately to model multivariate temporal dynamics of time series. In this work, we proposed a novel non-autoregressive deep learning model, called Multi-scale Attention Normalizing Flow(MANF), where we combine multi-scale attention with relative position information and the multivariate data distribution is represented by the conditioned normalizing flow. Additionally, compared with autoregressive modeling methods, our model avoids the influence of cumulative error and does not increase the time complexity. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets.}
}


@article{DBLP:journals/tkde/SunWZCW24,
	author = {Peijie Sun and
                  Le Wu and
                  Kun Zhang and
                  Xiangzhi Chen and
                  Meng Wang},
	title = {Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative
                  Filtering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2069--2081},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3317068},
	doi = {10.1109/TKDE.2023.3317068},
	timestamp = {Mon, 26 Aug 2024 10:41:32 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SunWZCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While effective in recommendation tasks, collaborative filtering (CF) techniques face the challenge of data sparsity. Researchers have begun leveraging contrastive learning to introduce additional self-supervised signals to address this. However, this approach often unintentionally distances the target user/item from their collaborative neighbors, limiting its efficacy. In response, we propose a solution that treats the collaborative neighbors of the anchor node as positive samples within the final objective loss function. This paper focuses on developing two unique supervised contrastive loss functions that effectively combine supervision signals with contrastive loss. We analyze our proposed loss functions through the gradient lens, demonstrating that different positive samples simultaneously influence updating the anchor node's embeddings. These samples’ impact depends on their similarities to the anchor node and the negative samples. Using the graph-based collaborative filtering model as our backbone and following the same data augmentation methods as the existing contrastive learning model SGL, we effectively enhance the performance of the recommendation model. Our proposed Neighborhood-Enhanced Supervised Contrastive Loss (NESCL) model substitutes the contrastive loss function in SGL with our novel loss function, showing marked performance improvement. On three real-world datasets, Yelp2018, Gowalla, and Amazon-Book, our model surpasses the original SGL by 10.09%, 7.09%, and 35.36% on NDCG@20, respectively.}
}


@article{DBLP:journals/tkde/ZhangYM24,
	author = {Xianyong Zhang and
                  Zhong Yuan and
                  Duoqian Miao},
	title = {Outlier Detection Using Three-Way Neighborhood Characteristic Regions
                  and Corresponding Fusion Measurement},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2082--2095},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312108},
	doi = {10.1109/TKDE.2023.3312108},
	timestamp = {Mon, 29 Jul 2024 21:17:52 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangYM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Outliers carry significant information to reflect an anomaly mechanism, so outlier detection facilitates relevant data mining. In terms of outlier detection, the classical approaches from distances apply to numerical data rather than nominal data, while the recent methods on basic rough sets deal with nominal data rather than numerical data. Aiming at wide outlier detection on numerical, nominal, and hybrid data, this paper investigates three-way neighborhood characteristic regions and corresponding fusion measurement to advance outlier detection. First, neighborhood rough sets are deepened via three-way decision, so they derive three-way neighborhood structures on model boundaries, inner regions, and characteristic regions. Second, the three-way neighborhood characteristic regions motivate the information fusion and weight measurement regarding all features, and thus, a multiple neighborhood outlier factor emerges to establish a new method of outlier detection; furthermore, a relevant outlier detection algorithm (called 3WNCROD, available at https://github.com/BELLoney/3WNCROD) is designed to comprehensively process numerical, nominal, and mixed data. Finally, the 3WNCROD algorithm is experimentally validated, and it generally outperforms 13 contrast algorithms to perform better for outlier detection.}
}


@article{DBLP:journals/tkde/ZhouDLLJLS24,
	author = {Peng Zhou and
                  Liang Du and
                  Xinwang Liu and
                  Zhaolong Ling and
                  Xia Ji and
                  Xuejun Li and
                  Yi{-}Dong Shen},
	title = {Partial Clustering Ensemble},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2096--2109},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3321913},
	doi = {10.1109/TKDE.2023.3321913},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouDLLJLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustering ensemble often provides robust and stable results without accessing original features of data, and thus has been widely studied. The conventional clustering ensemble methods often take the full multiple base partitions as inputs and provide a consensus clustering result. However, in many real-world applications, full base partitions are hard to obtain because some data may be missing in some base partitions. To tackle this problem, in this paper, we propose a novel partial clustering ensemble method, which takes the partial multiple base partitions as inputs. In this method, we simultaneously fill the missing values in the base partitions and ensemble them by fully considering the consensus and diversity. Moreover, to address the unreliability issue in the partial data scenario, we seamlessly plug it into a self-paced learning framework. The extensive experiments on benchmark data sets demonstrate the effectiveness and efficiency of the proposed method when handling incomplete data.}
}


@article{DBLP:journals/tkde/ZhongZYXYZYG24,
	author = {Hao Zhong and
                  Yubo Zhang and
                  Chenggang Yan and
                  Zuxing Xuan and
                  Ting Yu and
                  Ji Zhang and
                  Shihui Ying and
                  Yue Gao},
	title = {Penalized Flow Hypergraph Local Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2110--2125},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3319019},
	doi = {10.1109/TKDE.2023.3319019},
	timestamp = {Sat, 27 Jul 2024 13:40:51 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhongZYXYZYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, hypergraph analysis have attracted increasing attention due to their ability to model complex data correlation, with hypergraph clustering being one of the most important tasks. However, when the scale of hypergraph is large enough, clustering is difficult based on global consistency. Existing flow-based hypergraph local clustering methods have good theoretical cut improvements and runtime guarantees. However, these methods exhibit poor performance when the initial reference node set is small and are prone to causing the output set to shrink into a small subset, resulting in local minima. To address this issue, we propose the Penalized Flow Hypergraph Local Clustering(PFHLC) and provide new conductance guarantees and runtime analyses for our method. First, we use the random walk method to grow the initial seed set, and introduce the random walk information of nodes as penalized flow into the flow-based framework to optimize the output. Second, we propose a generalized objective function containing random walk information, which takes full advantage of the semi-supervised information of the target cluster to protect important nodes. This feature can avoid the local minima of previous flow-based methods. Importantly, our method is strongly-local and can run efficiently on large-scale hypergraphs. We contribute a real-world dataset and the experiments on real-world large-scale datasets show that PFHLC achieves the state-of-the-art significantly.}
}


@article{DBLP:journals/tkde/GaoHXKZT24,
	author = {Qiang Gao and
                  Jinyu Hong and
                  Xovee Xu and
                  Ping Kuang and
                  Fan Zhou and
                  Goce Trajcevski},
	title = {Predicting Human Mobility via Self-Supervised Disentanglement Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2126--2141},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3317175},
	doi = {10.1109/TKDE.2023.3317175},
	timestamp = {Mon, 03 Feb 2025 16:10:30 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GaoHXKZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural networks have recently achieved considerable improvements in learning human behavioral patterns and individual preferences from massive spatial-temporal trajectory data. However, most of the existing research concentrates on fusing different semantics underlying sequential trajectories for mobility pattern learning which, in turn, yields a narrow perspective on comprehending human intrinsic motions. In addition, the inherent sparsity and under-explored heterogeneous collaborative items pertaining to human check-ins hinder the potential exploitation of human diverse periodic regularities as well as common interests. Motivated by recent advances in disentanglement learning, we propose a novel disentangled solution called SSDL for tackling the next POI prediction problem. SSDL primarily seeks to disentangle the potential time-invariant and time-varying factors into different latent spaces from massive trajectories, providing an interpretable view to understand the intricate semantics underlying human diverse mobility representations. To address the data sparsity issue, we present two realistic trajectory augmentation approaches to enhance the understanding of both the human intrinsic periodicity/habits and constantly-changing intents. In addition, we devise a POI-centric graph structure to explore heterogeneous collaborative signals underlying historical check-ins. Extensive experiments conducted on four real-world datasets demonstrate that SSDL significantly outperforms the state-of-the-art approaches–for example, it yields up to 8.57% averaged improvement on ACC@1.}
}


@article{DBLP:journals/tkde/TianLWW24,
	author = {Chris Xing Tian and
                  Haoliang Li and
                  Yufei Wang and
                  Shiqi Wang},
	title = {Privacy-Preserving Constrained Domain Generalization Via Gradient
                  Alignment},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2142--2150},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3315279},
	doi = {10.1109/TKDE.2023.3315279},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TianLWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural networks (DNN) have demonstrated unprecedented success for various applications. However, due to the issue of limited dataset availability and the strict legal and ethical requirements for data privacy protection, the broad applications of DNN (e.g., medical imaging classification) with large-scale training data have been largely hindered, greatly constraining the model generalization capability. In this paper, we aim to tackle this problem by developing the privacy-preserving constrained domain generalization method, aiming to improve the generalization capability under the privacy-preserving condition. In particular, we propose to improve the information aggregation process on the centralized server side with a novel gradient alignment loss, expecting that the trained model can be better generalized to the “unseen” but related data. The rationale and effectiveness of our proposed method can be explained by connecting our proposed method with the Maximum Mean Discrepancy (MMD) which has been widely adopted as the distribution distance measure. Experimental results on three domain generalization benchmark datasets indicate that our method can achieve better cross-domain generalization capability compared to the state-of-the-art federated learning methods.}
}


@article{DBLP:journals/tkde/CaiLSCYYZM24,
	author = {Taotao Cai and
                  Qi Lei and
                  Quan Z. Sheng and
                  Ningning Cui and
                  Shuiqiao Yang and
                  Jian Yang and
                  Wei Emma Zhang and
                  Adnan Mahmood},
	title = {Reconnecting the Estranged Relationships: Optimizing the Influence
                  Propagation in Evolving Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2151--2165},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3316268},
	doi = {10.1109/TKDE.2023.3316268},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CaiLSCYYZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence Maximization (IM), which aims to select a set of users from a social network to maximize the expected number of influenced users, has recently received significant attention for mass communication and commercial marketing. Existing research efforts dedicated to the IM problem depend on a strong assumption: the selected seed users are willing to spread the information after receiving benefits from a company or organization. In reality, however, some seed users may be reluctant to spread the information or need to be paid higher to be motivated. Furthermore, the existing IM works pay little attention to capture users’ influence propagation in the future period. In this paper, we target a new research problem named, Reconnecting Top-ll Relationships (RT l R) query, which aims to find l number of previous existing relationships but being estranged later such that reconnecting these relationships will maximize the expected number of influenced users by the given group in a future period. We prove that the RT l R problem is NP-hard. An efficient greedy algorithm is proposed to answer the RT l R queries with the influence estimation technique and the well-chosen link prediction method to predict the near future network structure. We also design a pruning method to reduce unnecessary probing from candidate edges. Further, a carefully designed order-based algorithm is proposed to accelerate the RT l R queries. Finally, we conduct extensive experiments on real-world datasets to demonstrate the effectiveness and efficiency of our proposed methods.}
}


@article{DBLP:journals/tkde/WangCXBZWZZ24,
	author = {Xin Wang and
                  Heng Chang and
                  Beini Xie and
                  Tian Bian and
                  Shiji Zhou and
                  Daixin Wang and
                  Zhiqiang Zhang and
                  Wenwu Zhu},
	title = {Revisiting Adversarial Attacks on Graph Neural Networks for Graph
                  Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2166--2178},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3313059},
	doi = {10.1109/TKDE.2023.3313059},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangCXBZWZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) have achieved tremendous success in the task of graph classification and its diverse downstream real-world applications. Despite the huge success in learning graph representations, current GNN models have demonstrated their vulnerability to potentially existent adversarial examples on graph-structured data. Existing approaches are either limited to structure attacks or restricted to local information, urging for the design of a more general attack framework on graph classification, which faces significant challenges due to the complexity of generating local-node-level adversarial examples using the global-graph-level information. To address this “global-to-local” attack challenge, we present a novel and general framework CAMA to generate adversarial examples via manipulating graph structure and node features. Specifically, we make use of Graph Class Activation Mapping and its variant to produce node-level importance corresponding to the graph classification task. Then through a heuristic design of algorithms, we can perform both feature and structure attacks under unnoticeable perturbation budgets with the help of both node-level and subgraph-level importance. Experiments towards attacking four state-of-the-art graph classification models on six real-world benchmarks verify the flexibility and effectiveness of our framework.}
}


@article{DBLP:journals/tkde/WeiQWMNC24,
	author = {Yinwei Wei and
                  Xiaoyang Qu and
                  Xiang Wang and
                  Yunshan Ma and
                  Liqiang Nie and
                  Tat{-}Seng Chua},
	title = {Rule-Guided Counterfactual Explainable Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2179--2190},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3322227},
	doi = {10.1109/TKDE.2023.3322227},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WeiQWMNC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To empower the trust of current recommender systems, the counterfactual explanation (CE) method is adopted to generate the counterfactual instance for each input and take their changes causing the different outcomes as the explanation. Although promising results have been achieved by existing CE-based methods, we propose to generate the attribute-oriented counterfactual explanation. Different from them, we aim to generate the counterfactual instance by performing the intervention on the attributes, and then build an attribute-oriented counterfactual explainable recommender system. Considering the correlation and categorical values of attributes, how to efficiently generate the reliable counterfactual instances on the attributes challenges us. To alleviate such a problem, we propose to extract the decision rules over the attributes to guide the attribute-oriented counterfactual generation. Specifically, we adopt the gradient boosting decision tree (GBDT) to pre-build the decision rules over the attributes and develop a Rule-guided Counterfactual Explainable Recommendation model (RCER) to predict the user-item interaction and generate the counterfactual instances for the user-item pairs. We finally conduct extensive experiments on four publicly datasets, including NYC, LON, Amazon, and Movielens datasets. Experimental results have qualitatively and quantitatively justified the superiority of our model over existing cutting-edge baselines.}
}


@article{DBLP:journals/tkde/HuCFFLG24,
	author = {Danlei Hu and
                  Lu Chen and
                  Hanxi Fang and
                  Ziquan Fang and
                  Tianyi Li and
                  Yunjun Gao},
	title = {Spatio-Temporal Trajectory Similarity Measures: {A} Comprehensive
                  Survey and Quantitative Study},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2191--2212},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3323535},
	doi = {10.1109/TKDE.2023.3323535},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuCFFLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatio-temporal trajectory analytics are useful in diversified applications such as urban planning, infrastructure development, and vehicular networks. Trajectory similarity measure, which aims to evaluate the distance between two trajectories, is a fundamental functionality of trajectory analytics. In this paper, we propose a comprehensive survey that investigates all the most common and representative spatio-temporal trajectory measures. First, we provide an overview of spatio-temporal trajectory measures in terms of three hierarchical perspectives: Non-learning versus Learning, Free Space versus Road Network, and Standalone versus Distributed. Next, we present an evaluation benchmark by designing five real-world transformation scenarios. Based on this benchmark, extensive experiments are conducted to study the effectiveness, robustness, efficiency, and scalability of each measure, which offers guidelines for trajectory measure selection among multiple techniques and applications such as trajectory data mining, deep learning, and distributed processing. Specifically, i) Effectiveness: In terms of trajectory length, DFD and Seg-Frechet are length-sensitive, while OWD and Hausdorff always return same results when varying query trajectory length. In terms of trajectory shape, LCRS and LORS are able to effectively find similar trajectories for query trajectories with different shapes; ii) Robustness: Learning based measures are more robust compared with non-learning based ones. Among non-learning based measures, DFD, Hausdorff, OWD and Seg-Frechet are relatively non-sensitive to noises and different sampling rates; and iii) Efficiency& Scalability: Compared to non-learning based measures, learning based and distributed based measures are more efficient and scalable.}
}


@article{DBLP:journals/tkde/LiangXSKQY24,
	author = {Langzhang Liang and
                  Zenglin Xu and
                  Zixing Song and
                  Irwin King and
                  Yuan Qi and
                  Jieping Ye},
	title = {Tackling Long-Tailed Distribution Issue in Graph Neural Networks via
                  Normalization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2213--2223},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3315284},
	doi = {10.1109/TKDE.2023.3315284},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiangXSKQY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have attracted much attention due to their superior learning capability. Despite the successful applications of GNNs in many areas, their performance suffers heavily from the long-tailed node degree distribution. Most prior studies tackle this issue by devising sophisticated model architectures. In this article, we aim to improve the performance of tail nodes (low-degree or hard-to-classify nodes) via a generic and light normalization method. In detail, we propose a novel normalization method for GNNs, termed as ResNorm, which Reshapes a long-tailed distribution into a normal-like distribution via Normalization. The ResNorm includes two operators. First, the scale operator reshapes the distribution of the node-wise standard deviation (NStd) so as to improve the accuracy of tail nodes. Second, the analysis of the behavior of the standard shift indicates that the standard shift serves as a preconditioner on the weight matrix, increasing the risk of over-smoothing. To address this issue, we design a new shift operator for ResNorm, which simulates the degree-specific parameter strategy in a low-cost manner. Extensive experiments on various node classification benchmark datasets have validated the effectiveness of ResNorm in improving the performance of tail nodes as well as the overall performance.}
}


@article{DBLP:journals/tkde/HuangCYZZTZHZ24,
	author = {Kai Huang and
                  Yue Cui and
                  Qingqing Ye and
                  Yan Zhao and
                  Xi Zhao and
                  Yao Tian and
                  Kai Zheng and
                  Haibo Hu and
                  Xiaofang Zhou},
	title = {TED{\textdollar}{\^{}}+{\textdollar}+: Towards Discovering Top-k Edge-Diversified
                  Patterns in a Graph Database},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2224--2238},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312566},
	doi = {10.1109/TKDE.2023.3312566},
	timestamp = {Sat, 30 Nov 2024 21:08:01 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuangCYZZTZHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With an exponentially growing number of graphs from disparate repositories, there is a strong need to analyze a graph database containing an extensive collection of small- or medium-sized data graphs (e.g., chemical compounds). Although subgraph enumeration and subgraph mining have been proposed to bring insights into a graph database by a set of subgraph structures, they often end up with similar or homogenous topologies, which is undesirable in many graph applications. To address this limitation, we propose the Top-k Edge-Diversified Patterns Discovery problem to retrieve a set of subgraphs that cover the maximum number of edges in a database. To efficiently process such query, we present a generic and extensible framework called \\textsc {Ted}^+ which achieves a guaranteed approximation ratio to the optimal result. Three optimization strategies are further developed to improve the performance, and a lightweight version called TedLite is designed for even larger graph databases. Experimental studies on real-world datasets demonstrate the superiority of \\textsc {Ted}^+ to traditional techniques.}
}


@article{DBLP:journals/tkde/QianZZCZZ24,
	author = {Weizhu Qian and
                  Yan Zhao and
                  Dalin Zhang and
                  Bowei Chen and
                  Kai Zheng and
                  Xiaofang Zhou},
	title = {Towards a Unified Understanding of Uncertainty Quantification in Traffic
                  Flow Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2239--2256},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3312261},
	doi = {10.1109/TKDE.2023.3312261},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QianZZCZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncertainty is an essential consideration for time series forecasting tasks. In this work, we focus on quantifying the uncertainty of traffic forecasting from a unified perspective. We develop a novel traffic forecasting framework, namely Deep Spatio-Temporal Uncertainty Quantification (DeepSTUQ), which can estimate both aleatoric and epistemic uncertainty. Specifically, we first leverage a spatio-temporal model to model the complex spatio-temporal correlations of traffic data. Subsequently, two independent sub-neural networks maximizing the heterogeneous log-likelihood are developed to estimate aleatoric uncertainty. To estimate epistemic uncertainty, we combine the merits of variational inference and deep ensembling by integrating the Monte Carlo dropout and the Adaptive Weight Averaging re-training methods, respectively. Furthermore, to relax the Gaussianity assumption, mitigate overfitting, and improve horizon-wise uncertainty quantification performance, we define a new calibration method called Multi-horizon Conformal Calibration (MHCC). Finally, we provide a theoretical analysis of the proposed unified approach based on the PAC-Bayes theory. Extensive experiments are conducted on four public datasets, and the empirical results suggest that the proposed method outperforms state-of-the-art methods in terms of both point prediction and uncertainty quantification.}
}


@article{DBLP:journals/tkde/PrenkajV24,
	author = {Bardh Prenkaj and
                  Paola Velardi},
	title = {Unsupervised Detection of Behavioural Drifts With Dynamic Clustering
                  and Trajectory Analysis},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2257--2270},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3320184},
	doi = {10.1109/TKDE.2023.3320184},
	timestamp = {Sat, 04 May 2024 10:55:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PrenkajV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time monitoring of human behaviours, especially in e-Health applications, has been an active area of research in the past decades. On top of IoT-based sensing environments, anomaly detection algorithms have been proposed for the early detection of abnormalities. Gradual change procedures, commonly referred to as drift anomalies, have received much less attention in the literature because they represent a much more challenging scenario than sudden temporary changes (point anomalies). In this article, we propose, for the first time, a fully unsupervised real-time drift detection algorithm named DynAmo, which can identify drift periods as they are happening. DynAmo comprises a dynamic clustering component to capture the overall trends of monitored behaviours and a trajectory generation component, which extracts features from the densest cluster centroids. Finally, we apply an ensemble of divergence tests on sliding reference and detection windows to detect drift periods in the behavioural sequence.}
}


@article{DBLP:journals/tkde/WenCCSGY24,
	author = {Hechuan Wen and
                  Tong Chen and
                  Li Kheng Chai and
                  Shazia Sadiq and
                  Junbin Gao and
                  Hongzhi Yin},
	title = {Variational Counterfactual Prediction Under Runtime Domain Corruption},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2271--2284},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3321893},
	doi = {10.1109/TKDE.2023.3321893},
	timestamp = {Mon, 27 Jan 2025 20:03:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WenCCSGY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To date, various neural methods have been proposed for causal effect estimation based on observational data, where a default assumption is the same distribution and availability of variables at both training and inference (i.e., runtime) stages. However, distribution shift (i.e., domain shift) could happen during runtime, and bigger challenges arise from the impaired accessibility of variables. This is commonly caused by increasing privacy and ethical concerns, which can make arbitrary variables unavailable in the entire runtime data and imputation impractical. We term the co-occurrence of domain shift and inaccessible variables runtime domain corruption, which seriously impairs the generalizability of a trained counterfactual predictor. To counter runtime domain corruption, we subsume counterfactual prediction under the notion of domain adaptation. Specifically, we upper-bound the error w.r.t. the target domain (i.e., runtime covariates) by the sum of source domain error and inter-domain distribution distance. In addition, we build an adversarially unified variational causal effect model, named VEGAN, with a novel two-stage adversarial domain adaptation scheme to reduce the latent distribution disparity between treated and control groups first, and between training and runtime variables afterwards. We demonstrate that VEGAN outperforms other state-of-the-art baselines on individual-level treatment effect estimation in the presence of runtime domain corruption on benchmark datasets.}
}


@article{DBLP:journals/tkde/LiuPTJWWPW24,
	author = {Qin Liu and
                  Yu Peng and
                  Ziyi Tang and
                  Hongbo Jiang and
                  Jie Wu and
                  Tian Wang and
                  Tao Peng and
                  Guojun Wang},
	title = {veffChain: Enabling Freshness Authentication of Rich Queries Over
                  Blockchain Databases},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {5},
	pages = {2285--2300},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3316127},
	doi = {10.1109/TKDE.2023.3316127},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuPTJWWPW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the wide adoption of blockchains in data-intensive applications, enabling verifiable queries over a blockchain database is urgently required. Aiming at reducing costs, previous solutions embed a small-sized authenticated data structure (ADS) in each block header, so that a user can verify search results without maintaining a full copy of blockchain databases. However, existing studies focus on exact queries with difficulty to guarantee the freshness of search results. In this article, we propose two frameworks, called \\mathsf{veffChain} and \\mathsf{veffChain++}, to realize freshness authentication of rich queries over blockchain databases. Specifically, \\mathsf{veffChain} concerns about verifiable latest-K exact queries and employs RSA accumulator to generate constant-size ADSs; \\mathsf{veffChain++} integrates RSA accumulator into the Trie tree to further authenticate latest-K fuzzy queries. For improved scalability, an adaptive keyword splitting (AKS) solution is proposed to enable ADSs to be incrementally updated. Compared with the state-of-the-art work, our frameworks have the following merits: (1) Freshness Guarantee. The user can efficiently retrieve the freshest data from a blockchain database in a verifiable way. (2) Flexibility. The user can specify different query patterns on demand to retrieve data as accurately as possible. The detailed security analysis and extensive experiments validate the practicality of our frameworks.}
}


@article{DBLP:journals/tkde/LiHJZ24,
	author = {Pengfei Li and
                  Yu Hua and
                  Jingnan Jia and
                  Pengfei Zuo},
	title = {A Fast Learned Key-Value Store for Concurrent and Distributed Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2301--2315},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3327009},
	doi = {10.1109/TKDE.2023.3327009},
	timestamp = {Mon, 06 May 2024 17:18:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiHJZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient key-value (KV) store becomes important for concurrent and distributed systems to deliver high performance. The promising learned indexes leverage deep-learning models to complement existing KV stores and obtain significant performance improvements. However, existing schemes show limited scalability in concurrent systems due to containing high dependency among data. The practical system performance decreases when inserting a large amount of new data due to triggering frequent and inefficient retraining operations. Moreover, existing learned indexes become inefficient in distributed systems, since different machines incur high overheads to guarantee the data consistency when the index structures dynamically change. To address these problems in concurrent and distributed systems, we propose a fine-grained learned index scheme with high scalability, called FineStore, which constructs independent models with a flattened data structure under the trained data array to concurrently process the requests with low overheads. FineStore processes the new requests in-place with the support of non-blocking retraining, hence adapting to the new distributions without blocking the systems. In the distributed systems, different machines efficiently leverage the extended RCU barrier to guarantee the data consistency. We evaluate FineStore via YCSB and real-world datasets, and extensive experimental results demonstrate that FineStore improves the performance respectively by up to 1.8× and 2.5× than state-of-the-art XIndex and Masstree. We have released the open-source codes of FineStore for public use in GitHub.}
}


@article{DBLP:journals/tkde/LiuJYLMYDLLM24,
	author = {Huijun Liu and
                  Bin Ji and
                  Jie Yu and
                  Shasha Li and
                  Jun Ma and
                  Zibo Yi and
                  Mengxue Du and
                  Miaomiao Li and
                  Jie Liu and
                  Zeyao Mo},
	title = {A More Context-Aware Approach for Textual Adversarial Attacks Using
                  Probability Difference-Guided Beam Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2316--2328},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3325315},
	doi = {10.1109/TKDE.2023.3325315},
	timestamp = {Wed, 28 Aug 2024 09:13:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuJYLMYDLLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Textual adversarial attacks expose the vulnerabilities of text classifiers and can be used to improve their robustness. Previous context-aware attack models suffer from several limitations. They generally rely on out-of-date substitutes, solely consider the gold label probability, and use the greedy search when generating adversarial examples, often limiting the attack efficiency. To tackle these issues, we propose MC-PDBS, a More Context-aware textual adversarial attack model using Probability Difference-guided Beam Search. MC-PDBS generates substitutes using the newest perturbed text sequences in each attack iteration, enabling the generation of more context-aware adversarial examples. The probability difference is an overall consideration of the probabilities of all class labels, which is more effective than the gold label probability in guiding the selection of attack paths. In addition, the beam search enables MC-PDBS to search attack paths from multiple search channels, thereby avoiding the limited search space problem. Extensive experiments and human evaluation demonstrate that MC-PDBS outperforms previous best models in a series of evaluation metrics, particularly bringing up to a +19.5% attack success rate. Extensive analyses further confirm the effectiveness of MC-PDBS.}
}


@article{DBLP:journals/tkde/RenFZMLZY24,
	author = {Zhiwen Ren and
                  Han Fang and
                  Jie Zhang and
                  Zehua Ma and
                  Ronghao Lin and
                  Weiming Zhang and
                  Nenghai Yu},
	title = {A Robust Database Watermarking Scheme That Preserves Statistical Characteristics},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2329--2342},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3324932},
	doi = {10.1109/TKDE.2023.3324932},
	timestamp = {Sun, 04 Aug 2024 19:47:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/RenFZMLZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Database watermarking can be used for copyright verification and leakage traceability, effectively protecting the security of the database. However, the existing watermarking schemes commonly embed watermarks by modifying the original data, which changes the statistical characteristics and affects the statistical analysis of the database. Therefore, this paper proposes SCPW, a Statistical Characteristics Preserving robust database Watermarking framework. First, we perform a theoretical analysis and propose a data modification scheme maintaining the statistical characteristics unchanged. Then, we establish the correspondence between the data and the watermarks that need to be embedded in it by grouping. Finally, the watermark message is embedded into the database through data verification and modification. Specifically, for data that needs to be watermarked, we first verify whether the potential watermark bits extracted from the data are the same as bits that need to be embedded. If they are the same, we regard this original data, usually a floating point number, as a “good number” and do not modify it. Otherwise, we modify the data until it becomes a “good number” using a data modification scheme that preserves the statistical characteristics proposed by the theoretical analysis. In addition, we also use the genetic algorithm to optimize the grouping results and increase the proportion of “good number”, thereby reducing the proportion of data that needs to be modified and further reducing distortion. To our best knowledge, SCPW is the first watermarking scheme that ensures the preservation of statistical characteristics, and the experimental results also prove its excellent ability to preserve statistical characteristics compared to existing schemes. Moreover, experiments also illustrate that our method is robust against a wide range of attacks. When under deletion attack (deletion rate = 90%), the bit error rate of watermark extraction is only 0.8%, which is more than 12% lower than the current best method.}
}


@article{DBLP:journals/tkde/LiLJ24,
	author = {Yongjun Li and
                  Xiangyu Li and
                  Wenli Ji},
	title = {A Trajectory-Oriented Locality-Sensitive Hashing Method for User Identification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2343--2356},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3324427},
	doi = {10.1109/TKDE.2023.3324427},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User identification across social sites, which benefits many applications, has recently been attracting considerable attention. Most existing methods focused more on the effectiveness of user identification, rather than on efficiency. Matching as many cross-site user accounts as possible, which causes very high computation overhead posed by the full-scale pairwise comparisons, remains unsolved, especially when the number of users reaches tens of millions or more. To address this issue, we present a novel locality-sensitive hashing method for user identification (UI-LSH), which consists of four components. 1) It involves embedding stay points into vectors, 2) and constructing locality-sensitive hashing families suitable for stay points. 3) It presents a method for projecting stay points into hash buckets that ensures the close stay points are placed in the same bucket with high probability. 4) It constructs the candidate user pairs based on the projection results. The experiments on three ground-truth datasets show that our method reduces the number of user pairs to be compared by as much as 81.87%, 67.68%, and 63.15%, respectively. Overall, UI-LSH holds great promise for significantly improving the efficiency of user identification.}
}


@article{DBLP:journals/tkde/ZhangSTZYCQ24,
	author = {Rui Zhang and
                  Yixin Su and
                  Bayu Distiawan Trisedya and
                  Xiaoyan Zhao and
                  Min Yang and
                  Hong Cheng and
                  Jianzhong Qi},
	title = {AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment
                  Enabled by Large Language Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2357--2371},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3325484},
	doi = {10.1109/TKDE.2023.3325484},
	timestamp = {Fri, 30 Aug 2024 09:16:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangSTZYCQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs’ entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity alignment can be done without manually crafted seed alignments. AutoAlign is not only fully automatic, but also highly effective. Experiments using real-world KGs show that AutoAlign improves the performance of entity alignment significantly compared to state-of-the-art methods.}
}


@article{DBLP:journals/tkde/WuMLGZFW24,
	author = {Youxi Wu and
                  Yufei Meng and
                  Yan Li and
                  Lei Guo and
                  Xingquan Zhu and
                  Philippe Fournier{-}Viger and
                  Xindong Wu},
	title = {COPP-Miner: Top-k Contrast Order-Preserving Pattern Mining for Time
                  Series Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2372--2387},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3321749},
	doi = {10.1109/TKDE.2023.3321749},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuMLGZFW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, order-preserving pattern (OPP) mining, a new sequential pattern mining method, has been proposed to mine frequent relative orders in a time series. Although frequent relative orders can be used as features to classify a time series, the mined patterns do not reflect the differences between two classes of time series well. To effectively discover the differences between time series, this paper addresses the top-k contrast OPP (COPP) mining and proposes a COPP-Miner algorithm to discover the top-k contrast patterns as features for time series classification, avoiding the problem of improper parameter setting. COPP-Miner is composed of three parts: extreme point extraction to reduce the length of the original time series, forward mining, and reverse mining to discover COPPs. Forward mining contains three steps: group pattern fusion strategy to generate candidate patterns, the support rate calculation method to efficiently calculate the support of a pattern, and two pruning strategies to further prune candidate patterns. Reverse mining uses one pruning strategy to prune candidate patterns and consists of applying the same process as forward mining. Experimental results validate the efficiency of the proposed algorithm and show that top-k COPPs can be used as features to obtain a better classification performance.}
}


@article{DBLP:journals/tkde/YuLWLX24,
	author = {Dianer Yu and
                  Qian Li and
                  Xiangmeng Wang and
                  Qing Li and
                  Guandong Xu},
	title = {Counterfactual Explainable Conversational Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2388--2400},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3322403},
	doi = {10.1109/TKDE.2023.3322403},
	timestamp = {Sun, 04 Aug 2024 19:47:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuLWLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conversational Recommender Systems (CRSs) fundamentally differ from traditional recommender systems by interacting with users in a conversational session to accurately predict their current preferences and provide personalized recommendations. Although current CRSs have achieved favorable recommendation performance, the explainability is still in its infancy stage. Most of the CRSs tend to provide coarse explanations and fail to explore the impact of minimal alterations on the recommendation decisions on items. In this paper, we are the first to incorporate the counterfactual techniques into CRS and propose a Counterfactual Explainable Conversational Recommender (CECR) to enhance the recommendation model from a counterfactual perspective. Counterfactual explanations can offer fine-grained reasons to explain users’ real-time intentions, meanwhile generating counterfactual samples for augmenting the training dataset to enhance recommendation performance. Specifically, CECR adaptively learns users’ preferences based on the conversation context and effectively responds to users’ real-time feedback during multiple rounds of conversation. Furthermore, CECR actively generates counterfactual samples to augment the training set and thus leading to a constant improvement in recommendation performance. Empirical experiments carried out on three benchmark datasets show that our CECR outperforms state-of-the-art CRSs in terms of recommendation performance and explainability.}
}


@article{DBLP:journals/tkde/ZhaoZLHWF24,
	author = {Chuang Zhao and
                  Hongke Zhao and
                  Xiaomeng Li and
                  Ming He and
                  Jiahui Wang and
                  Jianping Fan},
	title = {Cross-Domain Recommendation via Progressive Structural Alignment},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2401--2415},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3324912},
	doi = {10.1109/TKDE.2023.3324912},
	timestamp = {Thu, 30 Jan 2025 11:04:28 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoZLHWF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain recommendation, as a cutting-edge technology to settle data sparsity and cold start problems, is gaining increasingly popular. Existing research paradigms primarily focus on leveraging the representation of overlapping entities, such as representation aggregation or cross-domain consistency constraints, to facilitate knowledge transfer and enhance the performance of single-domain or dual-domain recommender systems. Even though these approaches bring significant promotions, they still suffer from optimization bottlenecks when faced with sparse overlapping users, which often occurs in reality. Unlocking the full potential of overlapping user information and exploring novel sources of cross-domain knowledge are pivotal in addressing this challenge effectively. On account of this, this paper proposes an innovative cross-domain recommendation framework, namely SEAGULL, to promote dual-target recommendation performance in line with these two perspectives. We bolster the utilization of overlapping user knowledge and extract non-overlapping user interests by refining the message passing mechanism in a unified heterogeneous cross-domain graph and facilitating the transfer of latent structural relationships among users. Specifically, we first construct the interaction of two domains as a unified cross-domain heterogeneous graph and design a novel attention mechanism to incorporate cross-domain collaboration signals between users and items. Second, we perform user structure alignment from global and local levels to extend semantic transfer and information augmentation. Finally, unlike previous work that directly incorporates mixed cross-domain knowledge, we employ a gentle and progressive cross-domain transfer strategy to reduce empirical risk loss. Extensive experiments on five tasks derived from three data sets fully demonstrate the effectiveness of SEAGULL.}
}


@article{DBLP:journals/tkde/ShaoWCZZ24,
	author = {Xinyue Shao and
                  Hongzhi Wang and
                  Xiang Chen and
                  Xiao Zhu and
                  Yan Zhang},
	title = {{CUBE:} Causal Intervention-Based Counterfactual Explanation for Prediction
                  Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2416--2429},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3322126},
	doi = {10.1109/TKDE.2023.3322126},
	timestamp = {Tue, 25 Feb 2025 09:34:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ShaoWCZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent several years have witnessed the rapid explosion of artificial intelligence applied in various domains with the surpassing human-level performance. Despite the success, these models’ underlying mechanisms remain a mystery, as their complicated representations make human understanding impossible. This mystery may cause discrimination and non-robustness in prediction. Making deep learning models more transparent and understandable is gaining popularity, but most of interpretation approaches provide spurious correlations leading to suboptimal, incorrect or even biased interpretations, which could be reduced by causal explanations. Motivated by this, we attempt to study the generation of causal explanations and propose CUBE, a causal intervention-based counterfactual interpretation method. To ensure that the generation process of counterfactual explanation conforms to causality, we model the counterfactual generation process as a causal graph and construct a counterfactual generation model based on the causal intervention; to generate counterfactuals that adhere to the causality, we introduce a causal director to capture the causal relationships in the distribution and guide the generation of counterfactuals; to improve the efficiency of the counterfactual generation when facing a large number of explanation queries, we model it as a sample generation problem and propose an explainable framework based on adversarial generation. The experimental results validate that CUBE outperforms other approaches in terms of both lower time costs and higher explanation quality.}
}


@article{DBLP:journals/tkde/JinHLRZ24,
	author = {Fengmei Jin and
                  Wen Hua and
                  Lei Li and
                  Boyu Ruan and
                  Xiaofang Zhou},
	title = {Efficient Frequency-Based Randomization for Spatial Trajectories Under
                  Differential Privacy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2430--2444},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3322471},
	doi = {10.1109/TKDE.2023.3322471},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JinHLRZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The uniqueness of trajectory data for user re-identification has received unprecedented attention as the increasing popularity of location-based services boosts the excessive collection of daily trajectories with sufficient spatiotemporal coverage. Consequently, leveraging or releasing personally-sensitive trajectories without proper protection severely threatens individual privacy despite simply removing IDs. Trajectory privacy protection is never a trivial task due to the trade-off between privacy protection, utility preservation, and computational efficiency. Furthermore, recovery attack, one of the most threatening attacks specific to trajectory data, has not been well studied in the current literature. To tackle these challenges, we propose a frequency-based randomization model with a rigorous differential privacy guarantee for privacy-preserving trajectory data publishing. In particular, two randomized mechanisms are introduced for perturbing the local/global frequency distributions of a limited number of significantly essential locations in trajectories by injecting special Laplace noises. To reflect the perturbed distributions on the trajectory level without losing privacy guarantee or data utility, we formulate the trajectory modification tasks as kNN search problems and design two hierarchical indices with powerful pruning strategies and a novel search algorithm to support efficient modification. Extensive experiments on a real-world dataset verify the effectiveness of our approaches in resisting individual re-identification and recovery attacks simultaneously while still preserving desirable data utility. The efficient performance on large-scale data demonstrates the feasibility and scalability in practice.}
}


@article{DBLP:journals/tkde/CuiWLZYXCZ24,
	author = {Ningning Cui and
                  Dong Wang and
                  Jianxin Li and
                  Huaijie Zhu and
                  Xiaochun Yang and
                  Jianliang Xu and
                  Jie Cui and
                  Hong Zhong},
	title = {Enabling Efficient, Verifiable, and Secure Conjunctive Keyword Search
                  in Hybrid-Storage Blockchains},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2445--2460},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3324128},
	doi = {10.1109/TKDE.2023.3324128},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CuiWLZYXCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain has emerged as a prevailing paradigm for decentralized applications due to its reliability and transparency. To scale up retrieval services, a common strategy is to use a hybrid storage model, where on-chain storage is responsible for small metadata and off-chain storage is for outsourced raw data. However, data security and result authenticity are ongoing challenges in this scenario, and little work has been done due to the difficulty of combining result verification and privacy preservation, especially for dynamic updates while supporting forward privacy. In this paper, we formally define the problem of efficient, verifiable, and secure conjunctive keyword search in hybrid-storage blockchains (vsChain) and propose a novel hybrid index that achieves efficient query and verification while supporting dynamic updates with forward privacy guarantee. We also design two optimized schemes to improve query and verification performance by using a partition-based method and an obfuscated counting Bloom filter mechanism. Finally, we provide a theoretical security analysis and empirical evaluations using real and synthetic datasets to demonstrate the feasibility of our proposed schemes.}
}


@article{DBLP:journals/tkde/SunYSXDCXW24,
	author = {Zhongxiang Sun and
                  Weijie Yu and
                  Zihua Si and
                  Jun Xu and
                  Zhenhua Dong and
                  Xu Chen and
                  Hongteng Xu and
                  Ji{-}Rong Wen},
	title = {Explainable Legal Case Matching via Graph Optimal Transport},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2461--2475},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3321935},
	doi = {10.1109/TKDE.2023.3321935},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SunYSXDCXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Providing human-understandable explanations for the matching predictions is still challenging for current legal case matching methods. One difficulty is that legal cases are semi-structured text documents with complicated case-case and case-law article correlations. To tackle the issue, we propose a novel graph optimal transport (GOT)-based legal case matching model that is able to provide not only the matching predictions but also plausible and faithful explanations for the prediction. The model, called GEIOT-Match, first constructs a heterogeneous graph to explicitly represent the semi-structured nature of legal cases and their associations with the law articles. Therefore, matching two legal cases amounts to identifying the rationales from the paired legal case sub-graphs in the heterogeneous graph and then aligning between them. An inverse optimal transport (IOT) model on graphs is learned to extract rationales from paired legal cases. The extracted rationales and the heterogeneous graph demonstrate the key legal characteristics of legal cases, which can be further used to conduct matching and generate explanations for the matching. Experimental results showed that GEIOT-Match outperformed state-of-the-art baselines in terms of matching prediction, rationale extraction, and natural language explanation generation.}
}


@article{DBLP:journals/tkde/ZhuTLLZ24,
	author = {Yulin Zhu and
                  Liang Tong and
                  Gaolei Li and
                  Xiapu Luo and
                  Kai Zhou},
	title = {FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-Based Node
                  Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2476--2489},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3322129},
	doi = {10.1109/TKDE.2023.3322129},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuTLLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) are vulnerable to data poisoning attacks, which will generate a poisoned graph as the input to the GNN models. We present FocusedCleaner as a poisoned graph sanitizer to effectively identify the poison injected by attackers. Specifically, FocusedCleaner provides a sanitation framework consisting of two modules: bi-level structural learning and victim node detection. In particular, the structural learning module will reverse the attack process to steadily sanitize the graph while the detection module provides the “focus” – a narrowed and more accurate search region – to structural learning. These two modules will operate in iterations and reinforce each other to sanitize a poisoned graph step by step. As an important application, we show that the adversarial robustness of GNNs trained over the sanitized graph for the node classification task is significantly improved. Extensive experiments demonstrate that FocusedCleaner outperforms the state-of-the-art baselines both on poisoned graph sanitation and improving robustness.}
}


@article{DBLP:journals/tkde/LiuZW24,
	author = {Hongzhi Liu and
                  Yao Zhu and
                  Zhonghai Wu},
	title = {Knowledge Graph-Based Behavior Denoising and Preference Learning for
                  Sequential Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2490--2503},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3325666},
	doi = {10.1109/TKDE.2023.3325666},
	timestamp = {Sun, 04 Aug 2024 19:47:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation seeks to predict users’ next behaviors and recommend related items over time. Existing research has mainly focused on modeling users’ dynamic preferences from their sequential behaviors. However, most of these studies have ignored the negative effects of noise behaviors in the given sequences, which may mislead the recommender. In addition, users’ behavior data is always sparse, which makes it difficult to effectively learn users’ preferences purely from their historical behaviors. Most recently, knowledge graphs (KGs) have been exploited by few researchers for sequential recommendation. However, they always assume all information in KGs or KG paths with limited length are useful for recommendation, which may bring irrelevant information from KGs into the recommender and further mislead the recommender. To address these issues, we propose a novel KG-based behavior denoising and preference learning model named KGDPL for sequential recommendation. We argue that the paths in KGs that reflect semantic relations between entities can not only help to remove noise behaviors and recommend successive items for users, but also provide relevant explanations. Therefore, we first devise a supervised knowledge path selection module to select effective paths between items from KGs for behavior prediction, which aims to filter out irrelevant information from KGs for the given recommendation task. Then, we design a knowledge-enhanced behavior denoising module to mitigate the negative effects of the noise behaviors contained in historical sequences by using the knowledge path information. After that, we propose a knowledge-enhanced preference learning module to better learn users’ personalized and dynamic preferences from their historical behavior sequences and related knowledge information, which can also help tag users and provide explanations for recommendation results. Experimental results on four real-world datasets demonstrate the effectiveness and interpretability of the proposed model KGDPL.}
}


@article{DBLP:journals/tkde/HeZWYNC24,
	author = {Hui He and
                  Qi Zhang and
                  Shoujin Wang and
                  Kun Yi and
                  Zhendong Niu and
                  Longbing Cao},
	title = {Learning Informative Representation for Fairness-Aware Multivariate
                  Time-Series Forecasting: {A} Group-Based Perspective},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2504--2516},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3323956},
	doi = {10.1109/TKDE.2023.3323956},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HeZWYNC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multivariate time series (MTS) forecasting penetrates various aspects of our economy and society, whose roles become increasingly recognized. However, often MTS forecasting is unfair, not only degrading their practical benefits but even incurring potential risk. Unfair MTS forecasting may be attributed to disparities relating to advantaged and disadvantaged variables, which has rarely been studied in the MTS forecasting. In this work, we formulate the MTS fairness modeling problem as learning informative representations attending to both advantaged and disadvantaged variables. Accordingly, we propose a novel framework, named FairFor, for fairness-aware MTS forecasting, i.e., fair MTS forecasting. FairFor uses adversarial learning to generate both group-irrelevant and -relevant representations for downstream forecasting. FairFor first adopts recurrent graph convolution to capture spatio-temporal variable correlations and to group variables by leveraging a spectral relaxation of the K-means objective. Then, it utilizes a novel filtering \\&\nfusion module to filter group-relevant information and generate group-irrelevant representations by orthogonality regularization. The group-irrelevant and -relevant representations form highly informative representations, facilitating to share the knowledge from advantaged variables to disadvantaged variables and guarantee the fairness of forecasting. Extensive experiments on four public datasets demonstrate the FairFor effectiveness for fair forecasting and significant performance improvement.}
}


@article{DBLP:journals/tkde/LiuFZZH24,
	author = {Zemin Liu and
                  Yuan Fang and
                  Wentao Zhang and
                  Xinming Zhang and
                  Steven C. H. Hoi},
	title = {Locality-Aware Tail Node Embeddings on Homogeneous and Heterogeneous
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2517--2532},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3313355},
	doi = {10.1109/TKDE.2023.3313355},
	timestamp = {Sun, 04 Aug 2024 19:47:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuFZZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While the state-of-the-art network embedding approaches often learn high-quality embeddings for high-degree nodes with abundant structural connectivity, the quality of the embeddings for low-degree or tail nodes is often suboptimal due to their limited structural connectivity. While many real-world networks are long-tailed, to date little effort has been devoted to tail node embeddings. In this article, we formulate the goal of learning tail node embeddings as a few-shot regression problem, given the few links on each tail node. In particular, since each node resides in its own local context, we personalize the regression model for each tail node. To reduce overfitting in the personalization, we propose a locality-aware meta-learning framework, called meta-tail2vec, which learns to learn the regression model for the tail nodes at different localities. Moreover, to address the heterogeneity in nodes and edges on heterogeneous information networks (HINs), we further extend the proposed model and formulate meta-tail2vec+, which is based on a dual-adaptation mechanism to facilitate the locality-aware tail node embeddings on HINs. Finally, we conduct extensive experiments and demonstrate the promising results of both meta-tail2vec and its extension meta-tail2vec+.}
}


@article{DBLP:journals/tkde/ZhaoMLJZ24,
	author = {Wei Zhao and
                  Jiali Mao and
                  Xingyi Lv and
                  Cheqing Jin and
                  Aoying Zhou},
	title = {Multi-Source Domain Adaptation Enhanced Warehouse Dwell Time Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2533--2547},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3324656},
	doi = {10.1109/TKDE.2023.3324656},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoMLJZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Warehouse dwell time (WDT) of a truck is a critical metric for evaluating plant-logistics efficiency, including the time of the truck's queuing outside and loading inside the warehouse. But WDT prediction is challenging as it is affected by diverse factors like loading distinct types and weights of the cargoes, and varying amounts of loading tasks in different time slots. Besides, each trucks’ WDT is transitively influenced by its preceding trucks’ loading time in the queue. In this paper, we propose a multi-block dwell time prediction framework consisting of LSTM model and self-attention mechanism, called SDP. In view of that low performance of SDP brought by sparse loading data of some warehouses, we further design a multi-source adaptation based block-to-block transfer learning module. We present a warehouse similarity measurement based on loading tasks allocated and loading ability of the warehouses, according to which we enhance overall prediction performance by learning from high-performance WDT prediction models of similar warehouses. Experimental results on a large-scale logistics data set demonstrate that our proposal can reduce Mean Absolute Percentage Error (MAPE) by an average of 10.0%, Mean Absolute Error(MAE) by an average of 16.5%, and Root Mean Square Error(RMSE) by an average of 17.0% as compared to the baselines.}
}


@article{DBLP:journals/tkde/ZouXLW24,
	author = {Kai Zou and
                  Xike Xie and
                  Haoyun Li and
                  X. Sean Wang},
	title = {Multithreading Heterogeneous Graph Aggregation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2548--2562},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3320127},
	doi = {10.1109/TKDE.2023.3320127},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZouXLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Towards building online analytical services on big heterogeneous graphs, we study the problem of the multithreading graph aggregation. The purpose is to exploit the thread-level parallelism for accelerating the graph aggregation process, which is both data and computation intensive. We identify the sources of parallelization latency caused by multifarious factors, including data distributions and contentions, uneven workload assignments, logical aggregation plan obstructions, etc. To cope with these problems, we investigate a parallelization solution for graph aggregation with a number of threads packaged as threadblocks, categorize the parallelization latency as the thread-level and threadblock-level latency, and propose a series of optimization techniques for alleviating or eliminating the latency on different levels. The solution supports different aggregate functions, scales up to large number of threads, and scales out to big heterogeneous graphs. Experiments on real datasets show that our solution achieves up to 60x acceleration with 256 threads compared to the non-parallelized solution.}
}


@article{DBLP:journals/tkde/YuanLG24,
	author = {Weiwei Yuan and
                  Xiang Li and
                  Donghai Guan},
	title = {Multi-View Attributed Network Embedding Using Manifold Regularization
                  Preserving Non-Negative Matrix Factorization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2563--2571},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3325461},
	doi = {10.1109/TKDE.2023.3325461},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuanLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Attributed network has more network information, so more and more attention is paid to the embedding of attributed network. A few existing works have considered the node attributes plays a crucial role in the quality of network embedding. They use the non-negative matrix factorization (NMF) method to mine the network information of network structure and node attributes respectively. Considering the reconstruction error of NMF method, the original network information will be lost when the final network embedding is generated. In this paper, we propose a novel multi-view attributed network embedding model with manifold regularization (Mane). The manifold regularization is added to the model to better reflect the Riemann geometry structure of the network in the feature space to enhance the information. And the problem of missing information of NMF is solved. Our approach uses the NMF to get the non-negative coefficient matrix corresponding to network structure and node attributes. Then cooperative regularization and manifold regularization is added to obtain more information in the final network embedding. The model proposed in this paper has been verified by experiments on several real data sets. The result shows that the model is superior to the state-of-the-art algorithm in node classification task.}
}


@article{DBLP:journals/tkde/LiuCDZYWY24,
	author = {Shuaiqi Liu and
                  Jiannong Cao and
                  Zhongfen Deng and
                  Wenting Zhao and
                  Ruosong Yang and
                  Zhiyuan Wen and
                  Philip S. Yu},
	title = {Neural Abstractive Summarization for Long Text and Multiple Tables},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2572--2586},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3324012},
	doi = {10.1109/TKDE.2023.3324012},
	timestamp = {Sun, 04 Aug 2024 19:47:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuCDZYWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Abstractive summarization aims to generate a concise summary covering the input document's salient information. Within a report document, the salient information can be scattered in the textual and non-textual content. However, existing document summarization datasets and methods usually focus on the text and filter out the non-textual content. Missing tabular data can limit produced summaries’ informativeness, especially when summaries require covering quantitative descriptions of critical metrics in tables. Existing datasets and methods cannot meet the requirements of summarizing long text and dozens of tables in each report document. To deal with the scarcity of available datasets, we propose FINDSum, the first large-scale dataset for long text and multi-table summarization. Built on 21,125 annual reports from 3,794 companies, FINDSum has two subsets for summarizing each company's results of operations and liquidity. Besides, we present four types of summarization methods to jointly consider text and table content when summarizing reports. Additionally, we propose a set of evaluation metrics to assess the usage of numerical information in produced summaries. Our summarization methods significantly outperform advanced baselines, which verifies the necessity of incorporating textual and tabular data when summarizing report documents. We also conduct extensive comparative experiments to identify vital model components and configurations that can improve summarization results.}
}


@article{DBLP:journals/tkde/GaoFXPN24,
	author = {Yunlong Gao and
                  Yuzhe Feng and
                  Youwei Xie and
                  Jinyan Pan and
                  Feiping Nie},
	title = {Normalized Robust {PCA} With Adaptive Reconstruction Error Minimization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2587--2599},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3325462},
	doi = {10.1109/TKDE.2023.3325462},
	timestamp = {Wed, 08 Jan 2025 20:33:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GaoFXPN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Principal component analysis (PCA) is one of the most versatile techniques for unsupervised dimension reduction, which is implemented as a fundamental preprocessing method in multiple tasks of statistics and machine learning research because of its efficiency. Nevertheless, researchers have concentrated on the identification of outliers that do not conform to the low-dimensional approximation through statistical methods, e.g., outlier rejection, without giving insights on each data point with a dynamic ratio of signal-to-noise components in the high-dimensional regimes. To characterize the dynamic nature of the principal component information, we propose a Normalized Robust PCA with Adaptive Reconstruction Error minimization model, which considers both the adaptive normalization technique and flexible weights learning simultaneously. With this configuration, the principal component information constantly adjusts the degree of sparsity for activated samples. In other words, the signal component's discrimination and noise information restriction could work cooperatively. Empirical studies on one synthetic dataset and several benchmarks demonstrate the effectiveness of our proposed method over existing outlier rejection methods.}
}


@article{DBLP:journals/tkde/YinQC24,
	author = {Kejing Yin and
                  Dong Qian and
                  William K. Cheung},
	title = {PATNet: Propensity-Adjusted Temporal Network for Joint Imputation
                  and Prediction Using Binary EHRs With Observation Bias},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2600--2613},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3321738},
	doi = {10.1109/TKDE.2023.3321738},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YinQC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predictive analysis of electronic health records (EHR) is a fundamental task that could provide actionable insights to help clinicians improve the efficiency and quality of care. EHR are commonly recorded in binary format and contain inevitable missing data. The nature of missingness may vary by patients, clinical features, and time, which incurs observation bias. It is essential to account for the binary missingness and observation bias or the predictive performance could be substantially compromised. In this paper, we develop a propensity-adjusted temporal network (PATNet) to conduct data imputation and predictive analysis simultaneously. PATNet contains three subnetworks: 1) an imputation subnetwork that generates the initial imputation based on historical observations, 2) a propensity subnetwork that infers the patient-, feature-, and time-dependent propensity scores, and 3) a prediction subnetwork that produces the missing-informative prediction using the propensity-adjusted imputations and the missing probabilities. To allow the propensity scores to be inferred from data, we use the expectation-maximization (EM) algorithm to learn the imputation and propensity subnetworks and incorporate a low-rank constraint via PARAFAC2 approximation. Extensive evaluation using the MIMIC-III and eICU datasets demonstrates that PATNet outperforms the state-of-the-art methods in terms of binary data imputation, disease progression modeling, and mortality prediction tasks.}
}


@article{DBLP:journals/tkde/LuYWRC24,
	author = {Yuhuan Lu and
                  Dingqi Yang and
                  Pengyang Wang and
                  Paolo Rosso and
                  Philippe Cudr{\'{e}}{-}Mauroux},
	title = {Schema-Aware Hyper-Relational Knowledge Graph Embeddings for Link
                  Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2614--2628},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3323499},
	doi = {10.1109/TKDE.2023.3323499},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuYWRC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graph (KG) embeddings have become a powerful paradigm to resolve link prediction tasks for KG completion. The widely adopted triple-based representation, where each triplet (h,r,t) links two entities h and t through a relation r, oversimplifies the complex nature of the data stored in a KG, in particular for hyper-relational facts, where each fact contains not only a base triplet (h,r,t), but also the associated key-value pairs (k,v). Even though a few recent techniques tried to learn from such data by transforming a hyper-relational fact into an n-ary representation (i.e., a set of key-value pairs only without triplets), they result in suboptimal models as they are unaware of the triplet structure, which serves as the fundamental data structure in modern KGs and preserves the essential information for link prediction. Moreover, as the KG schema information has been shown to be useful for resolving link prediction tasks, it is thus essential to incorporate the corresponding hyper-relational schema in KG embeddings. Against this background, we propose sHINGE, a schema-aware hyper-relational KG embedding model, which learns from hyper-relational facts directly (without the transformation to the n-ary representation) and their corresponding hyper-relational schema in a KG. Our extensive evaluation shows the superiority of sHINGE on various link prediction tasks over KGs. In particular, compared to a sizeable collection of 21 baselines, sHINGE consistently outperforms the best-performing triple-based KG embedding method, hyper-relational KG embedding method, and schema-aware KG embedding method by 19.1%, 1.8%, and 12.9%, respectively.}
}


@article{DBLP:journals/tkde/DooK24,
	author = {Woojin Doo and
                  Heeyoung Kim},
	title = {Simultaneous Deep Clustering and Feature Selection via K-Concrete
                  Autoencoder},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2629--2642},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3323580},
	doi = {10.1109/TKDE.2023.3323580},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DooK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing deep learning methods for clustering high-dimensional data perform feature selection and clustering separately, which can result in the exclusion of some important features for clustering. In this paper, we propose a method that performs deep clustering and feature selection simultaneously by inserting a concrete selector layer between the input layer and the first encoder layer of a modified autoencoder. The concrete selector layer performs feature selection, while the modified autoencoder performs clustering in the latent space by incorporating K-means loss and inter-cluster distances. The proposed method, called the K-concrete autoencoder, selects features important for clustering and uses only the selected features to learn K-means-friendly latent representations of the data. Moreover, we propose an extension of the K-concrete autoencoder to provide relative importance of each selected feature. We demonstrate the effectiveness of the proposed method using simulated and real datasets.}
}


@article{DBLP:journals/tkde/LiuGMZY24,
	author = {Yan Liu and
                  Bin Guo and
                  Jingxiang Meng and
                  Daqing Zhang and
                  Zhiwen Yu},
	title = {Spatio-Temporal Memory Augmented Multi-Level Attention Network for
                  Traffic Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2643--2658},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3322405},
	doi = {10.1109/TKDE.2023.3322405},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuGMZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic prediction is one of the fundamental spatio-temporal prediction tasks in urban computing, which is of great significance to a wide range of applications, e.g., traffic controlling, vehicle scheduling, etc. Recently, with the expansion of the city and the development of public transportation, long-range and long-term spatio-temporal correlations play a more important role in traffic prediction. However, it is challenging to model long-range spatial dependencies and long-term temporal dependencies simultaneously in two aspects: 1) complex influential factors, including spatial, temporal and external factors. 2) multiple spatio-temporal correlations, including long-range and short-range spatial correlations, as well as long-term and short-term temporal correlations. To solve these issues, we propose a spatio-temporal memory augmented multi-level attention network for fine-grained traffic prediction, entitled ST-MAN. Specifically, we design a spatio-temporal memory network to encode and memorize fine-grained spatial information and representative temporal patterns. Then, we propose a multi-level attention network to explicitly model both short-term local spatio-temporal dependencies and long-term global spatio-temporal dependencies at different spatial scales (i.e., grid and region levels) and temporal scales (i.e., daily and weekly levels). In addition, we design an external component that takes external factors and spatial embeddings as inputs to generate location-aware influence of the external factors much more efficiently. Finally, we design an end-to-end framework optimized with the contrastive objective and supervised objective to boost model performance. Empirical experiments over coarse-grained and fine-grained real-world datasets demonstrate the superiority of the ST-MAN model compared to several state-of-the-art baselines.}
}


@article{DBLP:journals/tkde/XuWZ24,
	author = {Cong Xu and
                  Jun Wang and
                  Wei Zhang},
	title = {StableGCN: Decoupling and Reconciling Information Propagation for
                  Collaborative Filtering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2659--2670},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3323458},
	doi = {10.1109/TKDE.2023.3323458},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Convolutional Networks (GCNs) have been widely applied to collaborative filtering, where each layer typically contains neighborhood aggregation and feature transformation. Recent studies have found that feature transformation contributes little to the final recommendation performance. They however eliminated it directly without further exploration, leading to a degradation of model expressive power. In this paper, we show that this problem arises from inconsistent information propagation process, in which the dominance of feature transformation prevents features from being properly smoothed by neighborhood aggregation. To this end, we present StableGCN to decouple and reconcile this contradictory process in an orderly rather than intertwined manner. The coarse-grained node features are first refined by an elaborate extractor, and then smoothed by a specific kind of GCN concerning feature denoising. Consequently, feature transformation and neighborhood aggregation can support each other without sacrificing expressive power. Extensive experiments on six public datasets demonstrate the effectiveness and state-of-the-art performance of StableGCN.}
}


@article{DBLP:journals/tkde/FangQLZZ24,
	author = {Yuchen Fang and
                  Yanjun Qin and
                  Haiyong Luo and
                  Fang Zhao and
                  Kai Zheng},
	title = {STWave{\textdollar}{\^{}}+{\textdollar}+: {A} Multi-Scale Efficient
                  Spectral Graph Attention Network With Long-Term Trends for Disentangled
                  Traffic Flow Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2671--2685},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3324501},
	doi = {10.1109/TKDE.2023.3324501},
	timestamp = {Mon, 12 Aug 2024 18:35:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FangQLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic forecasting is crucial for public safety and resource optimization, yet is very challenging due to the temporal changes and the dynamic spatial correlations. To capture these intricate dependencies, spatio-temporal networks, such as recurrent neural networks with graph convolution networks, are applied. However, traffic forecasting is still a non-trivial task because of three major challenges: 1) Previous spatio-temporal networks are based on end-to-end training and thus fail to handle the distribution shift in the non-stationary traffic time series. 2) Existing methods always utilize the one-hour input to forecast future traffic and the long-term historical trend knowledge is ignored. 3) The efficient and effective algorithm for modeling multi-scale spatial correlations is still lacking in prior networks. Therefore, in this paper, rather than proposing yet another end-to-end model, we provide a novel disentangle-fusion framework STWave^{+} to mitigate the distribution shift issue. The framework first decouples the complex one-hour traffic data into stable trends and fluctuating events, followed by a dual-channel spatio-temporal network to model trends and events, respectively. Moreover, long-term trends are used as a self-supervised signal in STWave^{+} to teach overall temporal information into one-hour trends through a contrastive loss. Finally, reasonable future traffic can be predicted through the adaptive fusion of one-hour trends and events. Additionally, we incorporate a novel query sampling strategy and multi-scale graph wavelet positional encoding into the full graph attention network to efficiently and effectively model dynamic hierarchical spatial correlations. Extensive experiments on four traffic datasets show the superiority of our approach, i.e., the higher forecasting accuracy with lower computational cost.}
}


@article{DBLP:journals/tkde/DangYGJWXSL24,
	author = {Yizhou Dang and
                  Enneng Yang and
                  Guibing Guo and
                  Linying Jiang and
                  Xingwei Wang and
                  Xiaoxiao Xu and
                  Qinghui Sun and
                  Hong Liu},
	title = {TiCoSeRec: Augmenting Data to Uniform Sequences by Time Intervals
                  for Effective Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2686--2700},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3324312},
	doi = {10.1109/TKDE.2023.3324312},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DangYGJWXSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation has now been more widely studied, characterized by its well-consistency with real-world recommendation situations. Most existing works model user preference as the transition pattern from the previous item to the next, ignoring the time interval between these two items. However, we find that the time intervals in different sequences may vary significantly and thus result in the ineffectiveness of user modeling due to the issue of preference drift. Thus we propose an assumption that a sequence with uniformly distributed time intervals (denoted as uniform sequence) is more beneficial for preference learning than that with greatly varying time intervals. We then conduct an empirical study on four real datasets and the results support this assumption. Therefore, we advocate to augment sequence data from the perspective of time intervals, which is not studied in the literature. Specifically, we design five operators (Ti-Crop, Ti-CateReorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original non-uniform sequence to uniform sequence with the consideration of time intervals. Then, we devise a control strategy to execute data augmentation on item sequences in different lengths and a looseness range to ensure the generalization (or diversity) of generated data. Finally, we implement these improvements on a state-of-the-art model CoSeRec and propose Time Interval Aware CoSeRec (TiCoSeRec). Experimental results on four datasets demonstrate that TiCoSeRec achieves significantly better performance than other 11 counterparts recommendation techniques.}
}


@article{DBLP:journals/tkde/RenPJLWYY24,
	author = {Jiaqian Ren and
                  Hao Peng and
                  Lei Jiang and
                  Zhiwei Liu and
                  Jia Wu and
                  Zhengtao Yu and
                  Philip S. Yu},
	title = {Uncertainty-Guided Boundary Learning for Imbalanced Social Event Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2701--2715},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3324510},
	doi = {10.1109/TKDE.2023.3324510},
	timestamp = {Mon, 10 Feb 2025 14:57:15 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/RenPJLWYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-world social events typically exhibit a severe class-imbalance distribution, which makes the trained detection model encounter a serious generalization challenge. Most studies solve this problem from the frequency perspective and emphasize the representation or classifier learning for tail classes. While in our observation, compared to the rarity of classes, the calibrated uncertainty estimated from well-trained evidential deep learning networks better reflects model performance. To this end, we propose a novel uncertainty-guided class imbalance learning framework—UCL_{SED}, and its variant—UCL-EC_{SED}, for imbalanced social event detection tasks. We aim to improve the overall model performance by enhancing model generalization to those uncertain classes. Considering performance degradation usually comes from misclassifying samples as their confusing neighboring classes, we focus on boundary learning in latent space and classifier learning with high-quality uncertainty estimation. First, we design a novel uncertainty-guided contrastive learning loss, namely UCL and its variant—UCL-EC, to manipulate distinguishable representation distribution for imbalanced data. During training, they force all classes, especially uncertain ones, to adaptively adjust a clear separable boundary in the feature space. Second, to obtain more robust and accurate class uncertainty, we combine the results of multi-view evidential classifiers via the Dempster-Shafer theory under the supervision of an additional calibration method. We conduct experiments on three severely imbalanced social event datasets including Events2012_100, Events2018_100, and CrisisLexT_7. Our model significantly improves social event representation and classification tasks in almost all classes, especially those uncertain ones.}
}


@article{DBLP:journals/tkde/ZhaoAXG24,
	author = {Xingyu Zhao and
                  Yuexuan An and
                  Ning Xu and
                  Xin Geng},
	title = {Variational Continuous Label Distribution Learning for Multi-Label
                  Text Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2716--2729},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3323401},
	doi = {10.1109/TKDE.2023.3323401},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoAXG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-label text classification (MLTC) refers to the problem of tagging a given document with the most relevant subset of labels. One of the biggest challenges for MLTC is the existence of class imbalance. Most advanced MLTC models suffer from this issue, which limits the performance of the models. In this paper, we propose a model-agnostic framework named variational continuous label distribution learning (VCLDL) to address this problem. VCLDL theoretically builds a corresponding relationship between the feature space and the label space to mine the information hidden in the observable logical labels. Specifically, VCLDL regards label distribution as a continuous density function in latent space and forms a flexible variational approach to approximate the density function of the labels with the collaboration of the feature space. Combined with VCLDL, MLTC models can pay more attention to the distribution of the whole label set, rather than specific labels with maximum response values, thus the class imbalance problem can be well overcome. Experimental results on multiple benchmark datasets demonstrate that VCLDL can bring significant performance improvements over the existing MLTC models.}
}


@article{DBLP:journals/tkde/ChenSG24,
	author = {Zhichao Chen and
                  Zhihuan Song and
                  Zhiqiang Ge},
	title = {Variational Inference Over Graph: Knowledge Representation for Deep
                  Process Data Analytics},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2730--2744},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3327415},
	doi = {10.1109/TKDE.2023.3327415},
	timestamp = {Tue, 19 Nov 2024 20:28:41 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenSG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advent of the industrial Big Data era, accurate estimation of product quality and monitoring of working conditions from historical data have become crucial in the process industry. However, the majority of data-driven approaches predominantly rely on observational data, overlooking the valuable empirical knowledge derived from experience or underlying mechanisms. In order to leverage this knowledge, researchers employ various graph neural network-based methods which introduce connections among process variables for feature extraction. Nevertheless, it is imperative to recognize that process knowledge undergoes changes due to internal or external concept drift. To address this challenge, we propose a novel deep learning module called “variational inference over graph” to effectively harness shifting knowledge. Building upon the self-attention mechanism, we design a probabilistic self-attention mechanism for encoding and reconciling prior knowledge. Instead of directly encoding the prior knowledge through graph neural network edges, we incorporate it as regularization term within the variational inference framework that accounts for knowledge shift. Furthermore, we introduce reparameterization estimator to control the variance resulting from knowledge uncertainty. To showcase the capability of our proposed method, we conduct various experiments on quality prediction task in real industrial processes.}
}


@article{DBLP:journals/tkde/GuanCZGQ24,
	author = {Wei Guan and
                  Jian Cao and
                  Haiyan Zhao and
                  Yang Gu and
                  Shiyou Qian},
	title = {{WAKE:} {A} Weakly Supervised Business Process Anomaly Detection Framework
                  via a Pre-Trained Autoencoder},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {6},
	pages = {2745--2758},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3322411},
	doi = {10.1109/TKDE.2023.3322411},
	timestamp = {Fri, 07 Feb 2025 15:52:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GuanCZGQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ability to detect anomalies in business processes is crucial for achieving success in business operations. While unsupervised anomaly detection approaches have gained popularity in recent years due to their label-free nature, in some cases, a limited number of labelled anomalies can be provided and using them can improve the performance of anomaly detection. To address this issue, we propose a novel framework for anomaly detection that uses a pre-trained autoencoder to extract feature representations of traces. An anomaly score generator based on a multi-layer perceptron is utilized to evaluate the extracted features. The entire framework is trained using a joint loss that ensures the generated anomaly scores satisfy a specific distribution without compromising the autoencoder's ability to reconstruct normal traces. The feature encoder is fine-tuned to provide insights into the cause of anomalies. Additionally, we design a novel technique for calculating anomaly scores to mitigate the effects of varying numbers of potential attribute values. We conduct extensive experiments on both synthetic and real-life logs, and our results demonstrate that our proposed method, WAKE, outperforms state-of-the-art unsupervised deep business process anomaly detection methods by a significant margin. Additionally, it outperforms other weakly supervised anomaly detection methods as well.}
}


@article{DBLP:journals/tkde/YuYKCL24,
	author = {Ziqiang Yu and
                  Xiaohui Yu and
                  Nick Koudas and
                  Yueting Chen and
                  Yang Liu},
	title = {A Distributed Solution for Efficient {K} Shortest Paths Computation
                  Over Dynamic Road Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2759--2773},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3346377},
	doi = {10.1109/TKDE.2023.3346377},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuYKCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of identifying the k-shortest paths (KSPs for short) in a dynamic road network is essential to many location-based services. Road networks are dynamic in the sense that the weights of the edges in the corresponding graph constantly change over time, representing evolving traffic conditions. Very often such services have to process numerous KSP queries over large road networks at the same time, thus there is a pressing need to identify distributed solutions for this problem. However, most existing approaches are designed to identify KSPs on a static graph in a sequential manner (i.e., the (i+1)\\text{th} shortest path is generated based on the i\\text{th} shortest path), restricting their scalability and applicability in a distributed setting. We therefore propose KSP-DG, a distributed algorithm for identifying k-shortest paths in a dynamic graph. It is based on partitioning the entire graph into smaller subgraphs, and reduces the problem of determining KSPs into the computation of partial KSPs in relevant subgraphs, which can execute in parallel on a cluster of servers. A distributed two-level index called DTLP is developed to facilitate the efficient identification of relevant subgraphs. A salient feature of DTLP is that it indexes a set of virtual paths that are insensitive to varying traffic conditions in an efficient and compact fashion, leading to very low maintenance cost in dynamic road networks. This is the first treatment of the problem of processing KSP queries over dynamic road networks. Extensive experiments conducted on real road networks confirm the superiority of our proposal over baseline methods.}
}


@article{DBLP:journals/tkde/ChillonKRM24,
	author = {Alberto Hern{\'{a}}ndez Chill{\'{o}}n and
                  Meike Klettke and
                  Diego Sevilla Ruiz and
                  Jes{\'{u}}s Garc{\'{\i}}a Molina},
	title = {A Generic Schema Evolution Approach for NoSQL and Relational Databases},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2774--2789},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3362273},
	doi = {10.1109/TKDE.2024.3362273},
	timestamp = {Sun, 19 Jan 2025 13:53:46 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChillonKRM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the same way as with relational systems, schema evolution is a crucial aspect of NoSQL systems. But providing approaches and tools to support NoSQL schema evolution is more challenging than for relational databases. Not only are most NoSQL systems schemaless, but different data models exist without a standard specification for them. Moreover, recent proposals fail to address some key aspects related to the kinds of relationships between entities, the definition of relationship types, and the support of structural variation. In this article, we present a generic schema evolution approach able to support the most popular NoSQL data models (columnar, document, key-value, and graph) and the relational model. The proposal is based on the Orion language that implements a schema change operation taxonomy defined for the U-Schema unified data model that integrates NoSQL and relational abstractions. The consistency of the taxonomy operations is formally evaluated with Alloy, and the Orion semantics is expressed by translating operations into native code to update data and schema. Several database systems are supported, and the engine built for each of them has been validated by testing each individual SCO and refactoring study cases. A study of relative execution time of operations is also shown.}
}


@article{DBLP:journals/tkde/ZeighamiSS24,
	author = {Sepanta Zeighami and
                  Raghav Seshadri and
                  Cyrus Shahabi},
	title = {A Neural Database for Answering Aggregate Queries on Incomplete Relational
                  Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2790--2802},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3310914},
	doi = {10.1109/TKDE.2023.3310914},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZeighamiSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-world datasets are often incomplete due to data collection cost, privacy considerations or as a side effect of data integration/preparation. We focus on answering aggregate queries on such datasets, where data incompleteness causes the answers to be inaccurate. To address this problem, assuming typical relational data, existing work generates synthetic data to complete the database, a challenging task, especially in the presence of bias in observed data. Instead, we propose a paradigm shift by learning to directly estimate query answers, circumventing the difficult data generation step. Our approach, dubbed NeuroComplete, learns to answer queries in three steps. First, NeuroComplete generates a set of queries for which accurate answers can be computed given the incomplete dataset. Next, it embeds queries in a feature space, through which each query is effectively represented with the portion of the database that contributes to the query answer. Finally, it trains a neural network in a supervised learning fashion: both query features (input) and correct answers (labels) are known. The learned model generates accurate answers to new queries at test time, exploiting the generalizability of the learned model in the embedding space. Extensive experimental results on real datasets show up to 4 times for AVG queries and 10 times for COUNT queries error reduction compared with the state-of-the-art.}
}


@article{DBLP:journals/tkde/KarapiperisTV24,
	author = {Dimitrios Karapiperis and
                  Christos Tjortjis and
                  Vassilios S. Verykios},
	title = {A Suite of Efficient Randomized Algorithms for Streaming Record Linkage},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2803--2813},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3361022},
	doi = {10.1109/TKDE.2024.3361022},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KarapiperisTV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Organizations leverage massive volumes of information and new types of data to generate unprecedented insights and improve their outcomes. Correctly identifying duplicate records that represent the same entity, such as user, customer, patient and so on, a process commonly known as record linkage, can improve service levels, accelerate sales, or elevate healthcare decision support. Towards this direction, blocking methods are used with the aim to group matching records in the same block using a combination of their attributes as blocking keys. This paper introduces a suite of randomized algorithms specifically crafted for streaming record linkage settings. Using a bounded in-memory data structure, in terms of the number of blocks and positions within each block, our algorithms guarantee that the most frequently accessed and the most recently used blocks remain in main memory and, additionally, the records within a block are renewed on a rolling basis. The operation of our algorithms rely on simple random choices, instead of utilizing cumbersome sorting data structures, which ensure that the probability of inactive blocks and older records to remain in main memory decays in order to free space for more promising blocks and fresher records, respectively. We also introduce an algorithm that performs approximate blocking to tackle the problem of misspellings and typos present in the blocking keys. The experimental evaluation showcases that our proposed algorithms scale efficiently to data streams by providing certain accuracy guarantees.}
}


@article{DBLP:journals/tkde/CaoTGXCHL24,
	author = {Hanqun Cao and
                  Cheng Tan and
                  Zhangyang Gao and
                  Yilun Xu and
                  Guangyong Chen and
                  Pheng{-}Ann Heng and
                  Stan Z. Li},
	title = {A Survey on Generative Diffusion Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2814--2830},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3361474},
	doi = {10.1109/TKDE.2024.3361474},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CaoTGXCHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artificial Intelligence for General Creativity (AIGC). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented here.}
}


@article{DBLP:journals/tkde/ZhongYZQLY24,
	author = {Ming Zhong and
                  Junyong Yang and
                  Yuanyuan Zhu and
                  Tieyun Qian and
                  Mengchi Liu and
                  Jeffrey Xu Yu},
	title = {A Unified and Scalable Algorithm Framework of User-Defined Temporal
                  {\textdollar}(k,{\textbackslash}mathcal \{X\}){\textdollar}(k,X)-Core
                  Query},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2831--2845},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3349310},
	doi = {10.1109/TKDE.2023.3349310},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhongYZQLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Querying cohesive subgraphs on temporal graphs (e.g., social network, finance network, etc.) with various conditions has attracted intensive research interests recently. In this paper, we study a novel Temporal (k,\\mathcal {X})-Core Query (TXCQ) that extends a fundamental Temporal k-Core Query (TCQ) proposed in our conference paper by optimizing or constraining an arbitrary metric \\mathcal {X} of k-core, such as size, engagement, interaction frequency, time span, burstiness, periodicity, etc. Our objective is to address specific TXCQ instances with conditions on different \\mathcal {X} in a unified algorithm framework that guarantees scalability. For that, this journal paper proposes a taxonomy of measurement \\mathcal {X}(\\cdot ) and achieve our objective using a two-phase framework while \\mathcal {X}(\\cdot ) is time-insensitive or time-monotonic. Specifically, Phase 1 still leverages the query processing algorithm of TCQ to induce all distinct k-cores during a given time range, and meanwhile locates the “time zones” in which the cores emerge. Then, Phase 2 conducts fast local search and \\mathcal {X} evaluation in each time zone with respect to the time insensitivity or monotonicity of \\mathcal {X}(\\cdot ). By revealing two insightful concepts named tightest time interval and loosest time interval that bound time zones, the redundant core induction and unnecessary \\mathcal {X} evaluation in a zone can be reduced dramatically. Our experimental results demonstrate that TXCQ can be addressed as efficiently as TCQ, which achieves the latest state-of-the-art performance, by using a general algorithm framework that leaves \\mathcal {X}(\\cdot ) as a user-defined function.}
}


@article{DBLP:journals/tkde/LiangCHCN24,
	author = {Yun Liang and
                  Yijin Chen and
                  Qiong Huang and
                  Haoming Chen and
                  Feiping Nie},
	title = {An Effective Optimization Method for Fuzzy {\textdollar}k{\textdollar}k-Means
                  With Entropy Regularization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2846--2861},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3329821},
	doi = {10.1109/TKDE.2023.3329821},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiangCHCN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fuzzy k-Means with Entropy Regularization method (ERFKM) is an extension to Fuzzy k-Means (FKM) by introducing a maximum entropy term to FKM, whose purpose is trading off fuzziness and compactness. However, ERFKM often converges to a poor local minimum, which affects its performance. In this paper, we propose an effective optimization method to solve this problem, called IRW-ERFKM. First a new equivalent problem for ERFKM is proposed; then we solve it through Iteratively Re-Weighted (IRW) method. Since IRW-ERFKM optimizes the problem with k\\times 1 instead of d\\times k intermediate variables, the space complexity of IRW-ERFKM is greatly reduced. Extensive experiments on clustering performance and objective function value show IRW-ERFKM can get a better local minimum than ERFKM with fewer iterations. Through time complexity analysis, it verifies IRW-ERFKM and ERFKM have the same linear time complexity. Moreover, IRW-ERFKM has advantages on evaluation metrics compared with other methods. What's more, there are two interesting findings. One is when we use IRW method to solve the equivalent problem of ERFKM with one factor \\mathbf{U}, it is equivalent to ERFKM. The other is when the inner loop of IRW-ERFKM is executed only once, IRW-ERFKM and ERFKM are equivalent in this case.}
}


@article{DBLP:journals/tkde/WangLT24,
	author = {Ru Wang and
                  Shangqi Lu and
                  Yufei Tao},
	title = {An Index for Set Intersection With Post-Filtering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2862--2876},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3329145},
	doi = {10.1109/TKDE.2023.3329145},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangLT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies how to design an index structure on a collection of sets S_{1}, S_{2},{\\ldots }, S_{n} to answer the following queries: given distinct set ids a, b \\in [1, n], report F(S_{a} \\cap S_{b}) where F(.) is a filtering function. We present a solution that can support a great variety of filtering functions — range research, skyline, convex hull, nearest neighbor search, quantile (to name just a few) — with attractive performance guarantees. The guarantees are sensitive to the set collection's pseudoarboricity, a new notion for quantifying the density of \\lbrace S_{1}, S_{2},{\\ldots }, S_{n}\\rbrace. Our index structures are simple to understand and implement.}
}


@article{DBLP:journals/tkde/QiaoZLYWWY24,
	author = {Pengpeng Qiao and
                  Zhiwei Zhang and
                  Yuntong Li and
                  Ye Yuan and
                  Shuliang Wang and
                  Guoren Wang and
                  Jeffrey Xu Yu},
	title = {AStore: Uniformed Adaptive Learned Index and Cache for RDMA-Enabled
                  Key-Value Store},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2877--2894},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3355100},
	doi = {10.1109/TKDE.2024.3355100},
	timestamp = {Sun, 19 Jan 2025 13:53:52 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/QiaoZLYWWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed key-value storage and computation are essential components of cloud services. As the demand for high-performance systems has increased significantly, a new architecture has been motivated to separate computing and storage nodes and connect them using RDMA-enabled networks. Existing RDMA-enabled systems use client-side cached indexes to reduce communication overhead and improve performance. However, such approaches could result in high server CPU contention due to heavy dynamic workloads (i.e., inserts), and cause a large accuracy gap because of the different indexes between client-side and server-side. These drawbacks limit the performance of RDMA-enabled systems. In this paper, to deal with these issues, we introduce AStore to achieve high performance with low memory footprint. AStore employs a new uniformed architecture, utilizing an adaptive learned index as both the server-side learned index and the client-side cached index, to handle dynamic and static workloads. We propose several optimization techniques to optimize dynamic and static workload procedures and design the leaf node lock mechanism to support high concurrent access. Extensive evaluations on YCSB, LGN, and OSM datasets demonstrate that AStore achieves competitive performance on read-only workloads by up to 75.2%, 107.3% and 57.7%, as well as improving performance on write-read workloads by up to 65.7%, 108.7% and 74.3% than XStore.}
}


@article{DBLP:journals/tkde/XiaYWYCY24,
	author = {Jiangnan Xia and
                  Yu Yang and
                  Senzhang Wang and
                  Hongzhi Yin and
                  Jiannong Cao and
                  Philip S. Yu},
	title = {Bayes-Enhanced Multi-View Attention Networks for Robust {POI} Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2895--2909},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3329673},
	doi = {10.1109/TKDE.2023.3329673},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XiaYWYCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {POI recommendation can facilitate various Location-Based Social Network services. Existing methods generally assume the available POI check-ins are the ground-truth depiction of user behaviors. However, in real scenarios, check-in data can be rather unreliable due to both subjective and objective causes including positioning errors and user privacy concerns. The data uncertainty issue may lead to significant negative impacts on POI recommendation, but has not been fully explored. To this end, we investigate a novel problem of robust POI recommendation by considering the uncertainty factors of user check-ins, and propose a Bayes-enhanced Multi-view Attention Network to effectively address it. Specifically, we construct three POI graphs to comprehensively model the dependencies among the POIs from different views, including the personal POI transition graph, the semantic-based and distance-based POI graphs. As the personal graph is usually sparse and sensitive to noise, we design a Bayes-enhanced spatial dependency learning module for data augmentation from the local view. A Bayesian posterior guided graph augmentation approach is adopted to generate a new graph with collaborative signals to increase the data diversity and thus counteract the data uncertainty issue. Next, a multi-view attention-based user preference learning module is proposed. By incorporating the semantic and distance correlations of POIs, the user preference can be effectively refined and finally achieve robust recommendations. We conduct extensive experiments over three datasets. The results show that our proposal significantly outperforms the state-of-the-art methods in POI recommendation when the available check-ins are incomplete and noisy.}
}


@article{DBLP:journals/tkde/HaldarLAJM24,
	author = {Nur Al Hasan Haldar and
                  Jianxin Li and
                  Naveed Akhtar and
                  Yan Jia and
                  Ajmal Mian},
	title = {Co-Engaged Location Group Search in Location-Based Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2910--2926},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3327405},
	doi = {10.1109/TKDE.2023.3327405},
	timestamp = {Sun, 04 Aug 2024 16:27:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HaldarLAJM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Searching for well-connected user communities in a Location-based Social Network (LBSN) has been extensively investigated. However, very few studies focus on finding a group of locations in an LBSN which are significantly engaged with socially cohesive user groups. In this work, we investigate the problem of Co-engaged Location group Search (CLS) from LBSNs where the selected locations are visited frequently by the members of the socially cohesive user groups, and the locations are reachable within a given distance threshold. To the best of our knowledge, this is the first work to search for socially co-engaged location groups in LBSNs. We devise a score function to measure the co-engagement of the location groups by combining social connectivity of the cohesive user groups and check-in density of the users to the selected locations. To solve the CLS problem, we propose a Filter-and-Verify algorithm that effectively filters out ineligible locations, and their corresponding check-in users. Further, we derive a lower bound on the number of check-ins to prune the insignificant locations and develop a novel greedy forward expansion algorithm (GFA). To accelerate the computation of CLS, we propose a ranking function and devise an incremental algorithm, GIA, that can filter the unqualified location groups. We establish the effectiveness of our solutions by conducting extensive experiments on three real-world datasets.}
}


@article{DBLP:journals/tkde/LiuCZSZZ24,
	author = {Shuncheng Liu and
                  Xu Chen and
                  Yan Zhao and
                  Han Su and
                  Xiaofang Zhou and
                  Kai Zheng},
	title = {Comfort-Aware Lane Change Planning With Exit Strategy for Autonomous
                  Vehicle},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2927--2941},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3348550},
	doi = {10.1109/TKDE.2023.3348550},
	timestamp = {Mon, 17 Feb 2025 12:13:59 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuCZSZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automation in road vehicles is an emerging technology that has developed rapidly over the last decade. There have been many inter-disciplinary challenges posed on existing transportation infrastructure by autonomous vehicles. In this paper, we conduct an algorithmic study on when and how an autonomous vehicle should change its lane, which is a fundamental problem in vehicle automation field and root cause of most ‘phantom’ traffic jams. We propose a prediction-and-decision framework, called Cheetah (Change lane smart for autonomous vehicle), which aims to optimize the lane changing maneuvers of autonomous vehicle while minimizing its impact on surrounding vehicles. In the prediction phase, Cheetah learns the spatio-temporal dynamics from historical trajectories of surrounding vehicles with a deep model (GAS-LED model) and predict their corresponding actions in the near future. A global attention mechanism and state sharing strategy are also incorporated to achieve higher accuracy and better convergence efficiency. Then in the decision phase, Cheetah looks for optimal lane change maneuvers for the autonomous vehicle by taking into account a few factors such as speed, impact on other vehicles and safety issues. A tree-based adaptive beam search algorithm is designed to reduce the search space and improve accuracy. In order to make our framework applicable to more scenarios, we further propose an improved Cheetah (Cheetah+) framework that makes the autonomous vehicle adapt for exiting a road and meet the requirement for driving comfort. Extensive experiments offer evidence that the proposed framework can advance the state of the art in terms of effectiveness and efficiency.}
}


@article{DBLP:journals/tkde/ZhangLLW24,
	author = {Liang Zhang and
                  Guannan Liu and
                  Xiaohui Liu and
                  Junjie Wu},
	title = {Denoising Item Graph With Disentangled Learning for Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2942--2955},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3361482},
	doi = {10.1109/TKDE.2024.3361482},
	timestamp = {Sun, 19 Jan 2025 13:53:51 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the growth of Graph-based Collaborative Filtering (GCF) for high-performance recommendations, but the widely adopted user-item bipartite graphs are subject to deeper layers’ over-smoothing effect and sparse user-item interactions when learning item representations. In this work, we introduce item graph, which regards items as nodes and connecting those that have ever co-occurred in transactions with edges, to preserve higher-order item relations while avoiding the drawbacks of bipartite graphs for item-based recommendation. To cope with the entangled semantics in the edges of an item graph, we first design a denoising scheme via a graph structure learning module with discrete sampling to drop noisy edges with respect to certain latent aspects, where multiple subgraphs can be yielded. We then implement graphical disentangled learning by imposing several structural regularizers that allow for macro conformity and micro divergence among the subgraphs. Finally, we propose a multi-graph fusion module to aggregate users’ preferences in different subgraphs with a user-graph attention mechanism. Extensive experiments on 5 real-world datasets demonstrate the superiority of our method over 16 competitive baseline methods including the recently proposed GCF ones. Particularly, our method shows evident advantages in recommendation under data sparsity conditions.}
}


@article{DBLP:journals/tkde/TaoYLWH24,
	author = {Yongfeng Tao and
                  Minqiang Yang and
                  Huiru Li and
                  Yushan Wu and
                  Bin Hu},
	title = {DepMSTAT: Multimodal Spatio-Temporal Attentional Transformer for Depression
                  Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2956--2966},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3350071},
	doi = {10.1109/TKDE.2024.3350071},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TaoYLWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Depression is one of the most common mental illnesses, but few of the currently proposed in-depth models based on social media data take into account both temporal and spatial information in the data for the detection of depression. In this paper, we present an efficient, low-covariance multimodal integrated spatio-temporal converter framework called DepMSTAT, which aims to detect depression using acoustic and visual features in social media data. The framework consists of four modules: a data preprocessing module, a token generation module, a Spatial-Temporal Attentional Transformer (STAT) module, and a depression classifier module. To efficiently capture spatial and temporal correlations in multimodal social media depression data, a plug-and-play STAT module is proposed. The module is capable of extracting unimodal spatio-temporal features and fusing unimodal information, playing a key role in the analysis of acoustic and visual features in social media data. Through extensive experiments on a depression database (D-Vlog), the method in this paper shows high accuracy (71.53%) in depression detection, achieving a performance that exceeds most models. This work provides a scaffold for studies based on multimodal data that assists in the detection of depression.}
}


@article{DBLP:journals/tkde/MaoHCFLMX24,
	author = {Yuren Mao and
                  Yu Hao and
                  Xin Cao and
                  Yixiang Fang and
                  Xuemin Lin and
                  Hua Mao and
                  Zhiqiang Xu},
	title = {Dynamic Graph Embedding via Meta-Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2967--2979},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3329238},
	doi = {10.1109/TKDE.2023.3329238},
	timestamp = {Fri, 02 Aug 2024 15:28:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MaoHCFLMX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs in real-world applications usually evolve constantly presenting dynamic behaviors such as social networks and transportation networks. Hence, dynamic graph embedding has gained much attention recently. In dynamic graphs, both the topology and node attributes could change over time, which pose great challenges for developing effective embedding models. Typically, the evolution process of a dynamic graph can be recorded as a series of snapshots. We observe that the evolution process inherently provides both prior information (previous snapshots) and validation information (the next snapshot). The prior information can be used to fit the evolution process, while the validation information can be used to improve the generalization ability of a graph embedding model. However, existing dynamic graph embedding models only utilize the prior information, but overlook the validation information. To tackle this issue, this paper proposes a novel dynamic graph embedding method via Model-Agnostic Meta-Learning, which utilizes both kinds of information to obtain better graph representation. The extensive experiments on eight real-world datasets demonstrate the superiority of our proposed method over state-of-the-art methods on various graph analysis tasks.}
}


@article{DBLP:journals/tkde/DenhamLSN24,
	author = {Benjamin Denham and
                  Edmund M.{-}K. Lai and
                  Roopak Sinha and
                  M. Asif Naeem},
	title = {Dynamic Quantification With Constrained Error Under Unknown General
                  Dataset Shift},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2980--2994},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3349286},
	doi = {10.1109/TKDE.2023.3349286},
	timestamp = {Sun, 19 Jan 2025 13:53:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DenhamLSN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantification research has sought to accurately estimate class distributions under dataset shift. While existing methods perform well under assumed conditions of shift, it is not always clear whether such assumptions will hold in a given application. This work extends the analysis and experimental evaluation of our Gain-Some-Lose-Some (GSLS) model for quantification under general dataset shift and incorporates it into a method for dynamically selecting the most appropriate quantification method. Selection by a Kolmogorov-Smirnov test for any shift followed by a newly proposed “Adjusted Kolmogorov-Smirnov” test for non-prior shift is found to best balance quantification and runtime performance. We also present a framework for constraining quantification prediction intervals to user-specified limits by requesting a smaller set of instance class labels from the user than required with confidence-based rejection.}
}


@article{DBLP:journals/tkde/GuoLWWL24,
	author = {Qintian Guo and
                  Dandan Lin and
                  Sibo Wang and
                  Raymond Chi{-}Wing Wong and
                  Wenqing Lin},
	title = {Efficient Algorithms for Group Hitting Probability Queries on Large
                  Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {2995--3008},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3349164},
	doi = {10.1109/TKDE.2023.3349164},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GuoLWWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a source node s and a target node t, the hitting probability tells us how likely an \\alpha-terminating random walk (which stops with probability \\alpha at each step) starting from s can hit t before it stops. This concept originates from the hitting time, a classic concept in random walks. In this paper, we focus on the group hitting probability (GHP) where the target is a set of nodes, measuring the node-to-group structural proximity. For this group version of the hitting probability, we present efficient algorithms for two types of GHP queries: the pairwise query which returns the GHP value of a target set T with respect to (w.r.t.) a source node s, and the top-k query which returns the top-k target sets with the largest GHP value w.r.t. a source node s. We first develop an efficient algorithm named SAMBA for the pairwise query, which is built on a group local push algorithm tailored for GHP, with rigorous analysis for correctness. Next, we show how to speed up SAMBA by combining the group local push algorithm with the Monte Carlo approach, where GHP brings new challenges as it might need to consider every hop of the random walk. We tackle this issue with a new formulation of the GHP and show how to provide approximation guarantees with a detailed theoretical analysis. With SAMBA as the backbone, we develop an iterative algorithm for top-k queries, which adaptively refines the bounds for the candidate target sets, and terminates as soon as it meets the stopping condition, thus saving unnecessary computational costs. We further present an optimization technique to accelerate the top-k query, improving its practical performance. Extensive experiments show that our solutions are orders of magnitude faster than their competitors.}
}


@article{DBLP:journals/tkde/HeHWW24,
	author = {Jingxuan He and
                  Xixian Han and
                  Xiaolong Wan and
                  Jinbao Wang},
	title = {Efficient Skyline Frequent-Utility Itemset Mining Algorithm on Massive
                  Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3009--3023},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3349454},
	doi = {10.1109/TKDE.2024.3349454},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HeHWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Frequent itemset mining (FIM) and high-utility itemset mining (HUIM) are two important branches of itemset mining which is a key technology of knowledge discovery in many applications. Nowadays, there have been extensive algorithms on FIM and HUIM, but few studies consider frequency and utility together, so skyline frequent-utility itemset mining (SFUIM) is proposed to find useful itemsets with both frequency and utility measurements. Nevertheless, SFUIM is more challenging than FIM and HUIM since the search space is large and the calculation cost is expensive without any threshold, especially on large-scale databases. To address it, this paper proposes a novel prefix-based algorithm PSI* to mine skyline frequent-utility itemsets on massive data. PSI* divides the huge database by prefix-based partitioning, so that the calculation of itemsets with a specific prefix-item only involves a partition instead of the database. A multilevel-index based list is presented to compactly maintain the maximal utility under the frequency constraint, and a novel grid-based structure is devised to organize partitions or items by a designed order. Moreover, four efficient pruning strategies are proposed to prune itemsets as early as possible. Substantial experiments show that the PSI* algorithm has better performance than the state-of-the-art algorithms, obviously on large-scale databases.}
}


@article{DBLP:journals/tkde/ZhangYBL24,
	author = {Haijun Zhang and
                  Xian Yang and
                  Liang Bai and
                  Jiye Liang},
	title = {Enhancing Drug Recommendations Via Heterogeneous Graph Representation
                  Learning in {EHR} Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3024--3035},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3329025},
	doi = {10.1109/TKDE.2023.3329025},
	timestamp = {Tue, 18 Jun 2024 09:25:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangYBL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electronic health records (EHRs) contain vast medical information like diagnosis, medication, and procedures, enabling personalized drug recommendations and treatment adjustments. However, current drug recommendation methods only model patients’ health conditions from EHR data, neglecting the rich relationships within the data. This paper seeks to utilize a heterogeneous information network (HIN) to represent EHR and develop a graph representation learning method for medication recommendation. However, three critical issues need to be investigated: (1) co-occurrence of diagnosis and drug for the same patient does not imply their relevance; (2) patients’ directly associated information may not be sufficient to reflect their health conditions; and (3) the cold start problem exists when patients have no historical EHRs. To tackle these challenges, we develop a bi-channel heterogeneous local structural encoder to decouple and extract the diverse information in HIN. Additionally, a global information capture and fusion module, aggregating meta-paths to form a global representation, is introduced to fill the information gaps in records. A longitudinal model using rich structural information available in EHR data is proposed for drug recommendations to new patients. Experimental results on real-world EHR data demonstrate significant improvements over existing approaches.}
}


@article{DBLP:journals/tkde/YanGWZH24,
	author = {Hanqi Yan and
                  Lin Gui and
                  Menghan Wang and
                  Kun Zhang and
                  Yulan He},
	title = {Explainable Recommender With Geometric Information Bottleneck},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3036--3046},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3350447},
	doi = {10.1109/TKDE.2024.3350447},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YanGWZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Explainable recommender systems can explain their recommendation decisions, enhancing user trust in the systems. Most explainable recommender systems either rely on human-annotated rationales to train models for explanation generation or leverage the attention mechanism to extract important text spans from reviews as explanations. The extracted rationales are often confined to an individual review and may fail to identify the implicit features beyond the review text. To avoid the expensive human annotation process and to generate explanations beyond individual reviews, we propose to incorporate a geometric prior learnt from user-item interactions into a variational network which infers latent factors from user-item reviews. The latent factors from an individual user-item pair can be used for both recommendation and explanation generation, which naturally inherit the global characteristics encoded in the prior knowledge. Experimental results on three e-commerce datasets show that our model significantly improves the interpretability of a variational recommender using the Wasserstein distance while achieving performance comparable to existing content-based recommender systems in terms of recommendation behaviours.}
}


@article{DBLP:journals/tkde/HeLZFTDAV24,
	author = {Jiayuan He and
                  Yuan Li and
                  Zenan Zhai and
                  Biaoyan Fang and
                  Camilo Thorne and
                  Christian Druckenbrodt and
                  Saber A. Akhondi and
                  Karin Verspoor},
	title = {Focused Contrastive Loss for Classification With Pre-Trained Language
                  Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3047--3061},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3327777},
	doi = {10.1109/TKDE.2023.3327777},
	timestamp = {Sun, 04 Aug 2024 16:27:45 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HeLZFTDAV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contrastive learning, which learns data representations by contrasting similar and dissimilar instances, has achieved great success in various domains including natural language processing (NLP). Recently, it has been demonstrated that incorporating class labels into contrastive learning, i.e., supervised contrastive learning (SCL), can further enhance the quality of the learned data representations. Although several works have shown empirically that incorporating SCL into classification models leads to better performance, the mechanism of how SCL works for classification is less studied. In this paper, we first investigate how SCL facilitates the classifier learning, where we show that the contrastive region, i.e., the data instances involved in each contrasting operation, has a crucial link to the mechanism of SCL. We reveal that the vanilla SCL is suboptimal since its behavior can be altered by variances in class distributions. Based on this finding, we propose a Focused Contrastive Loss (FoCL) for classification. Compared with SCL, FoCL defines a finer contrastive region, focusing on the data instances surrounding decision boundaries. We conduct extensive experiments on three NLP tasks: text classification, named entity recognition, and relation extraction. Experimental results show consistent and significant improvements of FoCL over strong baselines on various benchmark datasets, especially in few-shot scenarios.}
}


@article{DBLP:journals/tkde/NiSL24,
	author = {Mingze Ni and
                  Zhensu Sun and
                  Wei Liu},
	title = {Fraud's Bargain Attack: Generating Adversarial Text Samples via
                  Word Manipulation Process},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3062--3075},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3349708},
	doi = {10.1109/TKDE.2024.3349708},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NiSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent research has revealed that natural language processing (NLP) models are vulnerable to adversarial examples. However, the current techniques for generating such examples rely on deterministic heuristic rules, which fail to produce optimal adversarial examples. In response, this study proposes a new method called the Fraud's Bargain Attack (FBA), which uses a randomization mechanism to expand the search space and produce high-quality adversarial examples with a higher probability of success. FBA uses the Metropolis-Hasting sampler, a type of Markov Chain Monte Carlo sampler, to improve the selection of adversarial examples from all candidates generated by a customized stochastic process called the Word Manipulation Process (WMP). The WMP method modifies individual words in a contextually-aware manner through insertion, removal, or substitution. Through extensive experiments, this study demonstrates that FBA outperforms other methods in terms of attack success rate, imperceptibility and sentence quality.}
}


@article{DBLP:journals/tkde/ZhuoWZW24,
	author = {Xingrui Zhuo and
                  Gongqing Wu and
                  Zan Zhang and
                  Xindong Wu},
	title = {Geometric-Contextual Mutual Infomax Path Aggregation for Relation
                  Reasoning on Knowledge Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3076--3090},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3360258},
	doi = {10.1109/TKDE.2024.3360258},
	timestamp = {Sun, 19 Jan 2025 13:53:49 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuoWZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relation reasoning in Knowledge Graph Completion (KGC) aims at predicting missing relations between entities. Recently, effective KGC methods have usually focused on exploring the path pattern between entities, such as reward-based path walking and path context mining, to complete target relations. However, these methods typically suffer from two challenges: 1) They have difficulty in handling the individual representation limitation of candidate paths when there are no paths that directly represent latent relations between entities; 2) They overlook the biases of path context induction, which leads to unreasonable information interfering with the model's reasoning. To manage these challenges, a Geometric-Contextual Mutual Infomax (GCMI) path aggregator is proposed for relation reasoning. First, we design an attentive path aggregator with a shared Transformer encoder to capture the contexts from several candidate paths parallelly and integrate these contexts to sufficiently represent the latent relations of each entity pair for reasoning. Then, the GCMI modules are proposed to constrain the local and global biases of path context induction in the Transformer encoder and the path aggregator, respectively, by a straightforward geometric rule. Extensive experiments on 32 real-world relation reasoning tasks demonstrate that our method significantly outperforms 8 state-of-the-art baselines in terms of AP and AUC.}
}


@article{DBLP:journals/tkde/YangCLDW24,
	author = {Linyao Yang and
                  Hongyang Chen and
                  Zhao Li and
                  Xiao Ding and
                  Xindong Wu},
	title = {Give us the Facts: Enhancing Large Language Models With Knowledge
                  Graphs for Fact-Aware Language Modeling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3091--3110},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3360454},
	doi = {10.1109/TKDE.2024.3360454},
	timestamp = {Sun, 19 Jan 2025 13:53:51 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangCLDW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention. Due to their powerful emergent abilities, recent LLMs are considered as a possible alternative to structured knowledge bases like knowledge graphs (KGs). However, while LLMs are proficient at learning probabilistic language patterns and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance in generating texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes enhancing LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs’ factual reasoning ability, opening up new avenues for LLM research.}
}


@article{DBLP:journals/tkde/WangXZLLLRD24,
	author = {Pinghui Wang and
                  Dongdong Xie and
                  Junzhou Zhao and
                  Jinsong Li and
                  Zhicheng Li and
                  Rundong Li and
                  Yang Ren and
                  Jia Di},
	title = {Half-Xor: {A} Fully-Dynamic Sketch for Estimating the Number of Distinct
                  Values in Big Tables},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3111--3125},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3359710},
	doi = {10.1109/TKDE.2024.3359710},
	timestamp = {Wed, 12 Feb 2025 14:31:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangXZLLLRD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Calculating the number of distinct values (i.e., NDV) in a column of a big table is costly yet fundamental to a variety of database applications such as data compression and profiling. To reduce the high time and space cost, a number of sketch methods (e.g., HyperLogLog) have been proposed, which estimate the NDV from a constructed compact data summary of distinct values. However, these methods fail or are costly to manage fully-dynamic scenarios where data is often inserted into and deleted from the table. To solve this issue, we propose a novel sketch method, Half-Xor. Our Half-Xor sketch consists of a compact bit matrix and a small counter array, and it needs to set a few bits and update a counter when handling a data insertion/deletion. Compared with the state-of-the-art mergeable method, our experimental results demonstrate that our method Half-Xor is up to 6.6 times more accurate under the same memory usage and reduces the memory usage by up to 16 times to achieve the same estimation accuracy.}
}


@article{DBLP:journals/tkde/GaoWD24,
	author = {Jian Gao and
                  Jianshe Wu and
                  Jingyi Ding},
	title = {Heterogeneous Graph Condensation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3126--3138},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3362863},
	doi = {10.1109/TKDE.2024.3362863},
	timestamp = {Sun, 19 Jan 2025 13:53:47 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GaoWD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks greatly facilitate data processing in homogeneous and heterogeneous graphs. However, training GNNs on large-scale graphs poses a significant challenge to computing resources. It is especially prominent on heterogeneous graphs, which contain multiple types of nodes and edges, and heterogeneous GNNs are also several times more complex than the ordinary GNNs. Recently, Graph condensation (GCond) is proposed to address the challenge by condensing large-scale homogeneous graphs into small-scale informative graphs. Its label-based feature initialization and fully-connected design perform well on homogeneous graphs. While in heterogeneous graphs, label information generally only exists in specific types of nodes, making it difficult to be applied directly to heterogeneous graphs. In this article, we propose heterogeneous graph condensation (HGCond). HGCond uses clustering information instead of label information for feature initialization, and constructs a sparse connection scheme accordingly. In addition, we found that the simple parameter exploration strategy in GCond leads to insufficient optimization on heterogeneous graphs. This article proposes an exploration strategy based on orthogonal parameter sequences to address the problem. We experimentally demonstrate that the novel feature initialization and parameter exploration strategy is effective. Experiments show that HGCond significantly outperforms baselines on multiple datasets. On the dataset DBLP, HGCond can condense DBLP to 0.5% of its original scale to obtain DBLP-0.005. GNNs trained on DBLP-0.005 can retain nearly 99% accuracy compared to the GNNs trained on full-scale DBLP.}
}


@article{DBLP:journals/tkde/LinDSL24,
	author = {Chunming Lin and
                  Bowen Du and
                  Leilei Sun and
                  Linchao Li},
	title = {Hierarchical Context Representation and Self-Adaptive Thresholding
                  for Multivariate Anomaly Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3139--3150},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3360640},
	doi = {10.1109/TKDE.2024.3360640},
	timestamp = {Sun, 19 Jan 2025 13:53:51 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LinDSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection in multivariate time series is a critical research area, but it is also a challenging one due to its occurrence in various real-world scenarios, such as structural health monitoring and risk management. Traditional approaches for anomaly detection rely on deviating distribution and a static threshold that is set manually. However, static thresholds fail to detect contextual anomalies, leading to a high ratio of false anomalies. Therefore, a self-adaptive thresholding method is required to improve the accuracy of anomaly detection. In this study, we propose HCR-AdaAD, a multivariate anomaly detection framework that combines hierarchical context representation learning with deep learning methods. The core idea is to extract normal time-series patterns by transforming them into images, which can be used to extract spatial features and generate robust representations for normal time series. Next, we adopt Extreme Value Theory (EVT) to set self-adaptive thresholds in streaming time series, which can contribute to the ideal precision for anomaly detection and high interpretability with contextual information. We conducted evaluation experiments on three public datasets, and the results demonstrate the effectiveness and soundness of our proposed model. HCR-AdaAD offers a novel and effective approach to anomaly detection in multivariate time series that outperforms traditional methods, making it a promising solution for real-world applications in various domains.}
}


@article{DBLP:journals/tkde/ZhengMWWWMH24,
	author = {Jiping Zheng and
                  Fanxu Meng and
                  Yanhao Wang and
                  Xiaoyang Wang and
                  Sheng Wang and
                  Yuan Ma and
                  Zhiyang Hao},
	title = {Hybrid Regret Minimization: {A} Submodular Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3151--3165},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3328596},
	doi = {10.1109/TKDE.2023.3328596},
	timestamp = {Thu, 04 Jul 2024 22:05:06 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengMWWWMH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Regret minimization queries are important methods to extract representative tuples from databases. They have been extensively investigated in the last decade due to wide applications in multi-criteria decision making. For a given database D\nand a class \\mathcal {F}\nof utility functions (e.g., all nonnegative linear functions), two typical regret minimization queries considered in existing studies are maximum regret minimization (MRM) and average regret minimization (ARM) queries, whereby a subset of k\ntuples is selected from D\nto minimize the maximum or average of regret ratios among all utility functions in \\mathcal {F}\n, respectively. However, due to the different properties of maximum and average regret ratios, the result of one query cannot fulfill the requirement of the other. To the best of our knowledge, there has not yet been any attempt to combine both queries. In this paper, we first introduce the hybrid regret minimization (HRM) query, which simultaneously minimizes the maximum and average regret ratios. We show that finding the optimal result for an HRM query is NP-hard, but it is possible to exploit submodularity for approximate HRM query processing. We propose an efficient asymptotic approximation algorithm based on submodular maximization to process HRM queries and several optimization techniques, such as memoization, lazy evaluation, and stochastic subsampling, to improve query efficiency. Furthermore, we consider extending a multiplicative weights update (MWU) algorithm for multi-objective submodular maximization to provide higher-quality results for HRM queries. Finally, we demonstrate that our proposed algorithms achieve better performance for HRM queries than existing methods specific to MRM and ARM queries through extensive experiments on real-world and synthetic datasets. Meanwhile, our proposed algorithms are efficient and scalable to large datasets.}
}


@article{DBLP:journals/tkde/ZhouHLCM24,
	author = {Qihang Zhou and
                  Shibo He and
                  Haoyu Liu and
                  Jiming Chen and
                  Wenchao Meng},
	title = {Label-Free Multivariate Time Series Anomaly Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3166--3179},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3349613},
	doi = {10.1109/TKDE.2024.3349613},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouHLCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection in multivariate time series has been widely studied in one-class classification (OCC) setting. The training samples in this setting are assumed to be normal. In more practical situations, it is difficult to guarantee that all samples are normal. Meanwhile, preparing a completely clean training dataset is costly and laborious. Such a case may degrade the performance of OCC-based anomaly detection methods which fit the training distribution as the normal distribution. To overcome this limitation, in this paper, we propose MTGFlow, an unsupervised anomaly detection approach for Multivariate Time series anomaly detection via dynamic Graph and entity-aware normalizing Flow. MTGFlow first estimates the density of the entire training samples and then identifies anomalous instances based on the density of the test samples within the fitted distribution. This relies on a widely accepted assumption that anomalous instances exhibit more sparse densities than normal ones, with no reliance on the clean training dataset. However, it is intractable to directly estimate the density due to the complex dependencies among entities and their diverse inherent characteristics, not to mention detecting anomalies based on the estimated distribution. In order to address these problems, we utilize the graph structure learning model to learn interdependent and evolving relations among entities, which effectively captures the complex and accurate distribution patterns of multivariate time series. In addition, our approach incorporates the unique characteristics of individual entities by employing an entity-aware normalizing flow. This enables us to represent each entity as a parameterized normal distribution. Furthermore, considering that some entities present similar characteristics, we propose a cluster strategy that capitalizes on the commonalities of entities with similar characteristics, resulting in more precise and detailed density estimation. We refer to this cluster-aware extension as MTGFlow_cluster. Extensive experiments are conducted on six widely used benchmark datasets, in which MTGFlow and MTGFlow_cluster demonstrate their superior detection performance.}
}


@article{DBLP:journals/tkde/WengWQZCQZ24,
	author = {Siyang Weng and
                  Qingshuai Wang and
                  Luyi Qu and
                  Rong Zhang and
                  Peng Cai and
                  Weining Qian and
                  Aoying Zhou},
	title = {Lauca: {A} Workload Duplicator for Benchmarking Transactional Database
                  Performance},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3180--3194},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3360116},
	doi = {10.1109/TKDE.2024.3360116},
	timestamp = {Sun, 19 Jan 2025 13:53:49 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WengWQZCQZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generating synthetic workloads is essential and critical to the performance evaluation of database systems. When benchmarking database performance for a specific application, the similarity between synthetic workloads and real application workloads determines the credibility of the evaluation results. However, it meets a great challenge to catch workload characteristics for a target online transaction processing (OLTP) application considering the complexity of transaction executions. To address this problem, we propose a workload duplicator (Lauca) that can generate synthetic workloads with highly similar performance metrics compared to a specific application on both centralized and distributed databases. By carefully studying the application-driven workload generation problem, we present Transaction Logic, Data Access Distribution and Partition Access Distribution to characterize runtime workloads and propose novel generation algorithms to guarantee the high fidelity of synthetic workloads. To the best of our knowledge, Lauca is the first application-driven transactional workload generator. We conduct extensive experiments based on TPC-C, SmallBank and YCSB on both centralized and distributed databases. The experimental results show that Lauca consistently generates high-quality synthetic workloads.}
}


@article{DBLP:journals/tkde/HuangOYHZZZZ24,
	author = {Kai Huang and
                  Gaoya Ouyang and
                  Qingqing Ye and
                  Haibo Hu and
                  Bolong Zheng and
                  Xi Zhao and
                  Ruiyuan Zhang and
                  Xiaofang Zhou},
	title = {LDPGuard: Defenses Against Data Poisoning Attacks to Local Differential
                  Privacy Protocols},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3195--3209},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3358909},
	doi = {10.1109/TKDE.2024.3358909},
	timestamp = {Sun, 19 Jan 2025 13:53:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuangOYHZZZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The protocols that satisfy Local Differential Privacy (LDP) enable untrusted third parties to collect aggregate information about a population without disclosing each user's privacy. In particular, each user locally encodes and perturbs his private data before sending it to the data collector, who aggregates and estimates the statistics about the population based on the collected perturbed values from individuals. Owing to their growing importance, LDP protocols have been widely studied and deployed in real-world scenarios (e.g., Chrome and Windows). However, as data poisoning attacks may be injected by attackers who introduce many fake users, the utility of the statistics is heavily poisoned. In this paper, we present a generic and extensible framework called LDPGuard to address the problem. LDPGuard provides effective defenses against data poisoning attacks to LDP protocols for frequency estimation, a basic query of most data analytics tasks. In particular, it first precisely estimates the percentage of fake users and then provides adversarial schemes to defend against particular data poisoning attacks. Experimental study on real-world and synthetic datasets demonstrates the superiority of LDPGuard compared to existing techniques.}
}


@article{DBLP:journals/tkde/ChenLLZY24,
	author = {Hang Chen and
                  Bingyu Liao and
                  Jing Luo and
                  Wenjing Zhu and
                  Xinyu Yang},
	title = {Learning a Structural Causal Model for Intuition Reasoning in Conversation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3210--3223},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3352575},
	doi = {10.1109/TKDE.2024.3352575},
	timestamp = {Tue, 14 Jan 2025 09:49:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenLLZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reasoning, a crucial aspect of NLP research, has not been adequately addressed by prevailing models including Large Language Model. Conversation reasoning, as a critical component of it, remains largely unexplored due to the absence of a well-designed cognitive model. In this article, inspired by intuition theory on conversation cognition, we develop a conversation cognitive model (CCM) that explains how each utterance receives and activates channels of information recursively. Besides, we algebraically transformed CCM into a structural causal model (SCM) under some mild assumptions, rendering it compatible with various causal discovery methods. We further propose a probabilistic implementation of the SCM for utterance-level relation reasoning. By leveraging variational inference, it explores substitutes for implicit causes, addresses the issue of their unobservability, and reconstructs the causal representations of utterances through the evidence lower bounds. Moreover, we constructed synthetic and simulated datasets incorporating implicit causes and complete cause labels, alleviating the current situation where all available datasets are implicit-causes-agnostic. Extensive experiments demonstrate that our proposed method significantly outperforms existing methods on synthetic, simulated, and real-world datasets. Finally, we analyze the performance of CCM under latent confounders and propose theoretical ideas for addressing this currently unresolved issue.}
}


@article{DBLP:journals/tkde/QinJWLZ24,
	author = {Yifang Qin and
                  Wei Ju and
                  Hongjun Wu and
                  Xiao Luo and
                  Ming Zhang},
	title = {Learning Graph {ODE} for Continuous-Time Sequential Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3224--3236},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3349397},
	doi = {10.1109/TKDE.2024.3349397},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QinJWLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation aims at understanding user preference by capturing successive behavior correlations, which are usually represented as the item purchasing sequences based on their past interactions. Existing efforts generally predict the next item via modeling the sequential patterns. Despite effectiveness, there exist two natural deficiencies: (i) user preference is dynamic in nature, and the evolution of collaborative signals is often ignored; and (ii) the observed interactions are often irregularly-sampled, while existing methods model item transitions assuming uniform intervals. Thus, how to effectively model and predict the underlying dynamics for user preference becomes a critical research problem. To tackle the above challenges, in this paper, we focus on continuous-time sequential recommendation and propose a principled graph ordinary differential equation framework named GDERec. Technically, GDERec is characterized by an autoregressive graph ordinary differential equation consisting of two components, which are parameterized by two tailored graph neural networks (GNNs) respectively to capture user preference from the perspective of hybrid dynamical systems. On the one hand, we introduce a novel ordinary differential equation based GNN to implicitly model the temporal evolution of the user-item interaction graph. On the other hand, an attention-based GNN is proposed to explicitly incorporate collaborative attention to interaction signals when the interaction graph evolves over time. The two customized GNNs are trained alternately in an autoregressive manner to track the evolution of the underlying system from irregular observations, and thus learn effective representations of users and items beneficial to the sequential recommendation. Extensive experiments on five benchmark datasets demonstrate the superiority of our model over various state-of-the-art recommendation methods.}
}


@article{DBLP:journals/tkde/LiuJYN24,
	author = {Huafeng Liu and
                  Liping Jing and
                  Jian Yu and
                  Michael K. Ng},
	title = {Learning Hierarchical Preferences for Recommendation With Mixture
                  Intention Neural Stochastic Processes},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3237--3251},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3348493},
	doi = {10.1109/TKDE.2023.3348493},
	timestamp = {Thu, 07 Nov 2024 15:00:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuJYN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User preferences behind users’ decision-making processes are highly diverse and may range from lower-level concepts with more specific intentions and higher-level concepts with more general intentions. In this case, user preferences tend to be expressed hierarchically. However, learning such intentions with different levels from user behaviors is challenging, and remains largely neglected by the existing literature. Meanwhile, user behavior data tends to be sparse because of the limited user response and the vast combinations of users and items, which results in cold-start problems with unclear user intentions. In this paper, we propose a mixture intention neural stochastic process (MINSP), a new view of the stochastic processes family using a general meta-learning mechanism and mixture strategy for robust recommendation with hierarchical preferences modeling. By considering the recommendation process for each user as a stochastic process, MINSP defines distributions over functions and is capable of rapid adaptation to different users. To capture the user's intention on different levels, an iterative additive algorithm is proposed that minimizes the approximation error by backfitting the residuals of previous approximations. In this case, the induced tree intention hierarchies serve as an aggregated structured representation of the whole preference, summarizing the gist for convenient navigation and better generalization. Furthermore, we theoretically analyze the generalization error bound of the proposed MINSP to guarantee the model performance. Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines in terms of recommendation performance, and obtain an interpretable hierarchical intention structure.}
}


@article{DBLP:journals/tkde/DewulfSB24,
	author = {Pieter Dewulf and
                  Michiel Stock and
                  Bernard De Baets},
	title = {Link Prediction in Stagewise Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3252--3264},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3351732},
	doi = {10.1109/TKDE.2024.3351732},
	timestamp = {Sun, 19 Jan 2025 13:53:47 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DewulfSB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A stagewise graph has distinct edge types that represent the different stages. Two nodes can be connected by an edge of the current stage only if an edge of the preceding one is already connecting them. Stagewise graphs can represent many kinds of interactions. For example, a jobseeker-vacancy interaction can be labeled by the subsequent edge types click, apply, job interview, and, finally, hired. Also, biological and medical interactions, such as infection processes or administration of drugs, often occur in stages. In this work, we formalize link prediction problems on such graphs as ‘stagewise link prediction’. Though relevant and rapidly gaining attention, these types of problems are to date highly underexplored. We identify and address arising difficulties, such as competition in stagewise networks. We explore an activation function for stagewise modelling and an evaluation strategy that satisfies the stagewise constraints. We confirm our insights through a set of experiments on both well-chosen simulated data sets and real data related to job recommendation and synthetic biology.}
}


@article{DBLP:journals/tkde/NiLZLS24,
	author = {Li Ni and
                  Qiuyu Li and
                  Yiwen Zhang and
                  Wenjian Luo and
                  Victor S. Sheng},
	title = {{LSADEN:} Local Spatial-Aware Community Detection in Evolving Geo-Social
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3265--3280},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3348975},
	doi = {10.1109/TKDE.2023.3348975},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NiLZLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The identification of the local community structure in geo-social networks has been gaining increasing attention. The structure of geo-social networks evolves over time with the addition/deletion of edges/nodes and the update of node locations, which has motivated recent studies to mine local communities in dynamic geo-social networks. Mining communities in evolving geo-social networks is essential for understanding the evolution of group behaviors. However, in most previous studies on the community mining in dynamic networks, local spatial-aware communities were not identified in evolving geo-social networks. Therefore, in this study, the problem of determining local spatial-aware communities in evolving geo-social networks is proposed. To address this problem, we propose a parameter-free algorithm, called LSADEN. Specifically, LSADEN involves two main steps: i) selecting candidate nodes, where LSADEN defines the community dominance relation under dynamic environments to obtain candidate nodes that improve the community in terms of the community quality or the smoothness between communities at adjacent time stamps; ii) community expansion, where LSADEN designs the Manhattan distance of communities to add some candidate nodes to the local community. Experimental results on six real-world datasets and one synthetic dataset show that LSADEN performs well both in terms of the quality of communities and the smoothness between communities at adjacent time stamps.}
}


@article{DBLP:journals/tkde/HuHQFX24,
	author = {Jun Hu and
                  Bryan Hooi and
                  Shengsheng Qian and
                  Quan Fang and
                  Changsheng Xu},
	title = {{MGDCF:} Distance Learning via Markov Graph Diffusion for Neural Collaborative
                  Filtering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3281--3296},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3348537},
	doi = {10.1109/TKDE.2023.3348537},
	timestamp = {Tue, 19 Nov 2024 16:54:45 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuHQFX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have recently been utilized to build Collaborative Filtering (CF) models to predict user preferences based on historical user-item interactions. However, there is relatively little understanding of how GNN-based CF models relate to some traditional Network Representation Learning (NRL) approaches. In this paper, we show the equivalence between some state-of-the-art GNN-based CF models and a traditional 1-layer NRL model based on context encoding. Based on a Markov process that trades off two types of distances, we present Markov Graph Diffusion Collaborative Filtering (MGDCF) to generalize some state-of-the-art GNN-based CF models. Instead of considering the GNN as a trainable black box that propagates learnable user/item vertex embeddings, we treat GNNs as an untrainable Markov process that can construct constant context features of vertices for a traditional NRL model that encodes context features with a fully-connected layer. Such simplification can help us to better understand how GNNs benefit CF models. Especially, it helps us realize that ranking losses play crucial roles in GNN-based CF tasks. With our proposed simple yet powerful ranking loss InfoBPR, the NRL model can still perform well without the context features constructed by GNNs. We conduct experiments to perform detailed analysis on MGDCF.}
}


@article{DBLP:journals/tkde/LiuPXJWWPW24,
	author = {Qin Liu and
                  Yu Peng and
                  Mingzuo Xu and
                  Hongbo Jiang and
                  Jie Wu and
                  Tian Wang and
                  Tao Peng and
                  Guojun Wang},
	title = {{MPV:} Enabling Fine-Grained Query Authentication in Hybrid-Storage
                  Blockchain},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3297--3311},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3359173},
	doi = {10.1109/TKDE.2024.3359173},
	timestamp = {Tue, 16 Jul 2024 20:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuPXJWWPW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the large-scale data streams produced by distributed terminals, hybrid-storage blockchain (HSB) that combines on-chain and off-chain storages has emerged as a promising solution for secure data storage in decentralized applications. Because all the raw data is outsourced to an untrusted service provider (SP), existing solutions suggest to utilize an on-chain authenticated data structure (ADS) to verify query results retrieved off-chain. However, existing solutions support only coarse-grained authentication making a user abandon all the query results once the validation fails. In this article, we focus on realizing fine-grained authentication for range queries, enabling a user to distinguish authentic data from falsified results. Considering the heavy gas consumption of on-chain storage, we propose two multi-dimensional parity-based verification (MPV) schemes with a trade-off between off-chain and on-chain efficiencies. Our main idea is to design an accumulator-based ADS to summarize well-designed verifiable hypercubes, so that fake results can be quickly located by combining multi-dimensional faces failed validation. Compared with previous solutions, our MPV schemes allow a user to make efficient use of query results by filtering out errors, and thus have higher data utility. The detailed security analysis and extensive experiments demonstrate the security and effectiveness of our MPV schemes, respectively.}
}


@article{DBLP:journals/tkde/JiangLWZXO24,
	author = {Yangfan Jiang and
                  Xinjian Luo and
                  Yuncheng Wu and
                  Xiaochen Zhu and
                  Xiaokui Xiao and
                  Beng Chin Ooi},
	title = {On Data Distribution Leakage in Cross-Silo Federated Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3312--3328},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3349323},
	doi = {10.1109/TKDE.2023.3349323},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiangLWZXO24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged as a promising privacy-preserving machine learning paradigm, enabling data owners to collaboratively train a joint model by sharing model parameters instead of private training data. However, recent studies reveal the privacy risks in FL by inferring private training data from model parameters. Therefore, differential privacy (DP) is incorporated into FL to safeguard training data. Nevertheless, DP does not provide a strong theoretical guarantee for protecting data distribution, which is also highly sensitive in the cross-silo FL scenarios as it may reflect the business secrets of data owners. In this article, we develop two attack methods to investigate the potential risks of data distribution leakage in differentially private cross-silo FL. We highlight that an honest-but-curious server can successfully infer both the feature and label distributions of each party’s training data without any background knowledge. Specifically, the first attack applies when models are differentiable, while the second attack caters to non-differentiable classification models. Extensive experiments on six benchmark datasets validate the effectiveness of the proposed attacks. The results demonstrate that the state-of-the-art DP-SGD algorithm is still vulnerable to the inference attack on data distribution, emphasizing the necessity of designing more advanced privacy-preserving FL frameworks.}
}


@article{DBLP:journals/tkde/ChenGZWLWL24,
	author = {Jian Chen and
                  Hong Gao and
                  Kaiqi Zhang and
                  Jiachi Wang and
                  Yubo Luo and
                  Zhenqing Wu and
                  Jianzhong Li},
	title = {On Efficiently Processing {MIT} Queries in Trajectory Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3329--3347},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3361948},
	doi = {10.1109/TKDE.2024.3361948},
	timestamp = {Sun, 19 Jan 2025 13:53:46 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenGZWLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Maximizing Influence (Max-Inf) query is a fundamental operation in spatial data management. Given a set of weighted objects, this query aims to find an optimal location from a candidate set to maximize its influence, which is the total weight of its reverse nearest neighbors. Existing work commonly assumes that every object is in a fixed location. In real life, however, there are a wide variety of drive-in services (e.g., food joints, pharmacies, ATMs, etc.) that are widely accessed by mobile users (i.e., trajectories) instead of the fixed ones. In this paper, we first define the Maximizing Influence query over Trajectories, namely, MIT query, which aims to find an optimal location to maximize the total weight of influenced trajectories. We propose a novel index, QB-tree to hierarchically group trajectories with similar activity regions together for subsequent unified processing, and classify trajectories inside the same node into multiple buckets according to their motion patterns. For each bucket, we construct a rectilinear polygon using the trajectories in it to exclude some irrelevant areas in the minimum boundary rectangle. Moreover, we develop a branch-and-bound approach called BBM to efficiently solve the MIT query. The algorithm adaptively partitions the candidates into disjoint regions and prunes the regions without containing optimal results. Then, by exploiting the QB-tree, the upper and lower bounds are efficiently computed with three-level pruning technique. Practically, we also study a variant of the MIT query, called MDT query. We propose novel pruning bounds in cooperation with QB-tree to answer MDT queries efficiently. Finally, extensive experiments on real and synthetic datasets demonstrate that our index and algorithms have high performance in terms of efficiency, scalability, and genericity.}
}


@article{DBLP:journals/tkde/SunJYS24,
	author = {Jiahui Sun and
                  Haiming Jin and
                  Zhaoxing Yang and
                  Lu Su},
	title = {Optimizing Long-Term Efficiency and Fairness in Ride-Hailing Under
                  Budget Constraint via Joint Order Dispatching and Driver Repositioning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3348--3362},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3348491},
	doi = {10.1109/TKDE.2023.3348491},
	timestamp = {Mon, 26 Aug 2024 13:31:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SunJYS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ride-hailing platforms (e.g., Uber and Didi Chuxing) have become increasingly popular in recent years. Efficiency has always been an important metric for such platforms. However, only focusing on efficiency inevitably ignores the fairness of driver incomes, which could impair the sustainability of ride-hailing systems. To optimize such two essential objectives, order dispatching and driver repositioning play an important role, as they impact not only the immediate, but also the future order-serving outcomes of drivers. In practice, the platform offers monetary incentives to drivers for completing the repositioning and has a budget for the repositioning cost. Therefore, in this paper, we aim to exploit joint order dispatching and driver repositioning to optimize both long-term efficiency and fairness in ride-hailing under the budget constraint. To this end, we propose JDRCL, a novel multi-agent reinforcement learning framework, which integrates a group-based action representation that copes with the variable action space, and a primal-dual iterative training algorithm to learn a constraint-satisfying policy that maximizes both the worst and the overall incomes of drivers. Furthermore, we prove the asymptotic convergence rate of our training algorithm. Extensive experiments based on three real-world ride-hailing order datasets show that JDRCL outperforms state-of-the-art baselines on both efficiency and fairness.}
}


@article{DBLP:journals/tkde/ZhengQCZSY24,
	author = {Ruiqi Zheng and
                  Liang Qu and
                  Tong Chen and
                  Kai Zheng and
                  Yuhui Shi and
                  Hongzhi Yin},
	title = {Personalized Elastic Embedding Learning for On-Device Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3363--3375},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3361562},
	doi = {10.1109/TKDE.2024.3361562},
	timestamp = {Sun, 19 Jan 2025 13:53:47 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengQCZSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To address privacy concerns and reduce network latency, there has been a recent trend of compressing cumbersome recommendation models trained on the cloud and deploying compact recommender models to resource-limited devices for the real-time recommendation. Existing solutions generally overlook device heterogeneity and user heterogeneity. They require devices with the same budget to share the same model and assume the available device resources (e.g., memory) are constant, which is not reflective of reality. Considering device and user heterogeneities as well as dynamic resource constraints, this article proposes a Personalized Elastic Embedding Learning framework (PEEL) for the on-device recommendation, which generates Personalized Elastic Embeddings (PEEs) for devices with various memory budgets in a once-for-all manner, adapting to new or dynamic budgets, and addressing user preference diversity by assigning personalized embeddings for different groups of users. Specifically, it pretrains a global embedding table with collected user-item interaction instances and clusters users into groups. Then, it refines the embedding tables with local interaction instances within each group. PEEs are generated from the group-wise embedding blocks and their weights that indicate the contribution of each embedding block to the local recommendation performance. Given a memory budget, PEEL efficiently generates PEEs by selecting embedding blocks with the largest weights, making it adaptable to dynamic memory budgets on devices. Furthermore, a diversity-driven regularizer is implemented to encourage the expressiveness of embedding blocks, and a controller is utilized to optimize the weights. Extensive experiments are conducted on two public datasets, and the results show that PEEL yields superior performance on devices with heterogeneous and dynamic memory budgets.}
}


@article{DBLP:journals/tkde/WuXZZZLH24,
	author = {Yiqing Wu and
                  Ruobing Xie and
                  Yongchun Zhu and
                  Fuzhen Zhuang and
                  Xu Zhang and
                  Leyu Lin and
                  Qing He},
	title = {Personalized Prompt for Sequential Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3376--3389},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3357498},
	doi = {10.1109/TKDE.2024.3357498},
	timestamp = {Sun, 19 Jan 2025 13:53:51 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuXZZZLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pre-training models have shown their power in sequential recommendation. Recently, prompt has been widely explored and verified for tuning after pre-training in NLP, which helps to more effectively and parameter-efficiently extract useful knowledge from pre-training models for downstream tasks, especially in cold-start scenarios. However, it is challenging to bring prompt-tuning from NLP to recommendation, since the tokens of recommendation (i.e., items) are million-level and do not have concrete explainable semantics, and the sequence modeling in recommendation should be personalized. In this work, we first introduce prompt to recommendation models and propose a novel Personalized prompt-based recommendation (PPR) framework for cold-start recommendation. Specifically, we build personalized soft prompt via a prompt generator based on user profiles, and enable a sufficient training on prompts via a new prompt-oriented contrastive learning. PPR is effective, parameter-efficient, and universal in various tasks. In both few-shot and zero-shot recommendation tasks, PPR models achieve significant improvements over baselines in three large-scale datasets. We also verify PPR's universality in adopting different recommendation models as the backbone. Finally, we explore and confirm the capability of PPR on other tasks such as cross-domain recommendation and user profile prediction, shedding lights on the promising future directions of better using large-scale pre-trained recommendation models.}
}


@article{DBLP:journals/tkde/ZhenWHCWW24,
	author = {Yan Zhen and
                  Yunfei Wang and
                  Peng He and
                  Yaping Cui and
                  Ruyan Wang and
                  Dapeng Wu},
	title = {Popularity Balanced Multi-Task Bundling for Mobile Crowd Sensing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3390--3401},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3348796},
	doi = {10.1109/TKDE.2023.3348796},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhenWHCWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowd Sensing (MCS) is a data collection technology in which workers finish tasks and get payment. In MCS, some tasks are not preferred workers due to their remote locations or cheap prices, which leads to a huge proportion of unpopular tasks. Although increasing tasks payment is an effective to increase task popularity, however, it may decrease platform utility. In this work, we introduce bundling into MCS to solve this problem. Specially, a Task Bundling Reorganization Mechanism (TBRM) is proposed. In TBRM, unpopular tasks are properly bundled with popular tasks to maximize the minimum of both the number of task completions and expected profit. The TBRM is separated into two phases: the area selection phase and the rule selection phase. First, the randomly generated solution is input into the area selection phase, which selects the portion of the bundle that needs to be reorganized; then, the results of the area selection phase is regarded as input of the rule selection phase, which selects the appropriate task to reorganize; finally, the TBRM repeats this process until convergence. Experimental results demonstrate the effectiveness of the TBRM mechanism.}
}


@article{DBLP:journals/tkde/WangCWWL24,
	author = {Yuyao Wang and
                  Jie Cao and
                  Youquan Wang and
                  Jia Wu and
                  Yangyang Liu},
	title = {Position Matters: Play a Sequential Game to Detect Significant Communities},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3402--3416},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3323567},
	doi = {10.1109/TKDE.2023.3323567},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangCWWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting significant communities via an algorithmic game-theoretic model has recently shown great promise, which seeks to formulate community detection as a competitive game, enabling us to study the network's potential structure with a systematic tool. However, fully leveraging its potential to uncover the mechanism behind community formation remains a challenge. Here we propose SCG—a Sequential Community Game model to track and characterize the network's structural property. Unlike conventional formulations where individual nodes are treated as players, our model considers communities as players who strive to maximize their structural utility by strategically selecting member nodes. By prioritizing significant communities sequentially, SCG enables differentiation between uncovered communities. Importantly, we establish the existence of a strict Nash equilibrium in SCG, suggesting its ability to capture a stable community structure. We run extensive experiments on several synthetic and real-world networks to test SCG's performance. Results show that SCG can help us well track the network's structural properties and also give us reliable performance compared to related baselines.}
}


@article{DBLP:journals/tkde/FuWJXC24,
	author = {Fangcheng Fu and
                  Xuanyu Wang and
                  Jiawei Jiang and
                  Huanran Xue and
                  Bin Cui},
	title = {ProjPert: Projection-Based Perturbation for Label Protection in Split
                  Learning Based Vertical Federated Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3417--3428},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3349863},
	doi = {10.1109/TKDE.2024.3349863},
	timestamp = {Tue, 19 Nov 2024 08:45:52 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/FuWJXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the paradigms under which split learning (SL) is used is for the vertical federated learning (VFL) setting, where two or more parties build models over feature-partitioned data. However, to protect the private labels of one party, random noises are needed to perturb the backward derivatives (i.e., gradients w.r.t. forward activations), which incurs the privacy-utility tradeoff. In this work, we introduce ProjPert, a novel algorithm that searches for the optimal “perturbation knobs” for label protection in SL-based VFL. We formulate the problem of perturbation searching as how to minimize the impact on model quality given the desired privacy guarantee. Based on the problem, two solutions are introduced, where the first obtains the optimal perturbation via a simple but effective binary searching scheme, and the second heuristically approximates the optimality within a negligible error bound. Empirical results demonstrate that both our solutions are more effective in protecting the labels and achieve significantly better privacy-utility tradeoffs than state-of-the-art perturbation-based label protection methods. Furthermore, our heuristic solution is very efficient and incurs almost zero extra overhead in the overall running time, improving the usability in real-world applications.}
}


@article{DBLP:journals/tkde/YuQSMZZZX24,
	author = {Xiaoshan Yu and
                  Chuan Qin and
                  Dazhong Shen and
                  Haiping Ma and
                  Le Zhang and
                  Xingyi Zhang and
                  Hengshu Zhu and
                  Hui Xiong},
	title = {{RDGT:} Enhancing Group Cognitive Diagnosis With Relation-Guided Dual-Side
                  Graph Transformer},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3429--3442},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3352640},
	doi = {10.1109/TKDE.2024.3352640},
	timestamp = {Tue, 21 Jan 2025 12:48:52 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YuQSMZZZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cognitive diagnosis has been widely recognized as a crucial task in the field of computational education, which is capable of learning the knowledge profiles of students and predicting their future exercise performance. Indeed, considerable research efforts have been made in this direction over the past decades. However, most of the existing studies only focus on individual-level diagnostic modeling, while the group-level cognitive diagnosis still lacks an in-depth exploration, which is more compatible with realistic collaborative learning environments. To this end, in this paper, we propose a Relation-guided Dual-side Graph Transformer (RDGT) model for achieving effective group-level cognitive diagnosis. Specifically, we first construct the dual-side relation graphs (i.e., student-side and exercise-side) from the group-student-exercise heterogeneous interaction data for explicitly modeling associations between students and exercises, respectively. In particular, the edge weight between two nodes is defined based on the similarity of corresponding student-exercise interactions. Then, we introduce two relation-guided graph transformers to learn the representations of students and exercises by integrating the whole graph information, including both nodes and edge weights. Meanwhile, the inter-group information has been incorporated into the student-side relation graph to further enhance the representations of students. Along this line, we design a cognitive diagnosis module for learning the groups’ proficiency in specific knowledge concepts, which includes an attention-based aggregation strategy to obtain the final group representation and a hybrid loss for optimizing the performance prediction of both group and student. Finally, extensive experiments on 5 real-world datasets clearly demonstrate the effectiveness of our model as well as some interesting findings (e.g., the representative groups and potential collaborations among students).}
}


@article{DBLP:journals/tkde/WangLYLX24,
	author = {Xiangmeng Wang and
                  Qian Li and
                  Dianer Yu and
                  Qing Li and
                  Guandong Xu},
	title = {Reinforced Path Reasoning for Counterfactual Explainable Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3443--3459},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3354077},
	doi = {10.1109/TKDE.2024.3354077},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangLYLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Counterfactual explanations interpret the recommendation mechanism by exploring how minimal alterations on items or users affect recommendation decisions. Existing counterfactual explainable approaches face huge search space, and their explanations are either action-based (e.g., user click) or aspect-based (i.e., item description). We believe item attribute-based explanations are more intuitive and persuadable for users since they explain by fine-grained demographic features, e.g., brand. Moreover, counterfactual explanations could enhance recommendations by filtering out negative items. In this work, we propose a novel Counterfactual Explainable Recommendation (CERec) to generate item attribute-based counterfactual explanations meanwhile to boost recommendation performance. Our CERec optimizes an explanation policy upon uniformly searching candidate counterfactuals within a reinforcement learning environment. We reduce the huge search space with an adaptive path sampler by using rich context information of a given knowledge graph. We also deploy the explanation policy to a recommendation model to enhance the recommendation. Extensive explainability and recommendation evaluations demonstrate CERec's ability to provide explanations consistent with user preferences and maintain improved recommendations.}
}


@article{DBLP:journals/tkde/NieCYL24,
	author = {Feiping Nie and
                  Qiang Chen and
                  Weizhong Yu and
                  Xuelong Li},
	title = {Row-Sparse Principal Component Analysis via Coordinate Descent Method},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3460--3471},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3351851},
	doi = {10.1109/TKDE.2024.3351851},
	timestamp = {Sun, 19 Jan 2025 13:53:49 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NieCYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a novel algorithm to solve the row-sparse principal component analysis problem without relying on any data structure assumption. Sparse principal component analysis was proposed to improve the interpretability of principal component analysis by restricting the number of non-zero elements in each loading vector, but the varying sparsity patterns among different leading vectors may result in trouble on some occasions, such as feature selection. Then row-sparse principal component analysis was proposed, which demands the same sparse pattern among different loading vectors. However, the optimization of row-sparse principal component analysis problems is NP-hard. Although some algorithms were proposed to solve this problem, but they are only applicable to the specific data structure. In this paper, we transform the original row-sparse principal component analysis problem into a new equivalent problem that can be solved by coordinate descent method without relying on any data structure assumption. Then by carefully eliminating redundant structures to avoid repeating computation, we propose a more efficient coordinate descent method to solve this problem. Furthermore, no parameter needs to be tuned in our algorithm. Finally, extensive experiments are conducted on the real world data sets to demonstrate the superiority of our algorithm.}
}


@article{DBLP:journals/tkde/GuoTLPW24,
	author = {Jianhao Guo and
                  Siliang Tang and
                  Juncheng Li and
                  Kaihang Pan and
                  Lingfei Wu},
	title = {RustGraph: Robust Anomaly Detection in Dynamic Graphs by Jointly Learning
                  Structural-Temporal Dependency},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3472--3485},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3328645},
	doi = {10.1109/TKDE.2023.3328645},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GuoTLPW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic graph-based data are ubiquitous in the real world, such as social networks, finance systems, and traffic flow. Fast and accurately detecting anomalies in these dynamic graphs is of vital importance. However, despite promising results the current anomaly detection methods have achieved, there are two major limitations when coping with dynamic graphs. The first limitation is that the topological structures and the temporal dynamics have been modeled separately, resulting in less expressive features for detection. The second limitation is that the models have been trained by unreliable noisy labels generated by random negative sampling, rendering it severely vulnerable to subtle perturbations. To overcome the above limitations, we propose RustGraph, a robust anomaly detection framework by jointly learning structural-temporal dependency in dynamic graphs. To this end, we design a variational graph auto-encoder with informative prior that simultaneously encodes both graph structural and temporal information. Then we introduce a fine-grained contrastive learning method to learn better node representations by utilizing the temporal consistency between two snapshots. Furthermore, we formulate the noisy label learning problem for anomaly detection in dynamic graph, and then propose a robust anomaly detector to improve the model performance by leveraging learned graph structure signal. Our extensive experiments on six real-world datasets demonstrate the proposed RustGraph method achieves state-of-the-art performance with an average of 3.64% improvement on AUC-ROC metric compared with all baselines. The codes are available at https://github.com/aubreygjh/RustGraph.}
}


@article{DBLP:journals/tkde/YuZPY24,
	author = {Shenbao Yu and
                  Yifeng Zeng and
                  Yinghui Pan and
                  Fan Yang},
	title = {{SNMCF:} {A} Scalable Non-Negative Matrix Co-Factorization for Student
                  Cognitive Modeling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3486--3500},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3328730},
	doi = {10.1109/TKDE.2023.3328730},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuZPY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Student cognitive modeling plays an important role in the rapid development of educational data mining research. It aims to discover students’ proficiency in knowledge concepts as well as to predict students’ performance in conducting exercises. Studies in the past few years have been mainly centered around two types of techniques: cognitive diagnosis models and data mining approaches. Cognitive diagnosis models focus on students’ cognitive states and assess their knowledge concept proficiency through handcrafted features. The subjective features may trigger cascading errors in the students’ performance prediction. On the other hand, data mining techniques, e.g., matrix factorization methods, achieve high prediction accuracy by directly modeling the students’ exercising process. It lacks measuring the students’ knowledge concept proficiency. To address the dilemma of the aforementioned methods, in this paper, we propose a scalable non-negative matrix co-factorization (SNMCF) model by jointly modeling the students’ knowledge states and their exercising process. SNMCF can achieve high accuracy in predicting students’ exercise performance while modeling their states of knowledge concepts in a given domain. We conduct extensive experiments on several real-world datasets, including large sparse ones, and demonstrate the effectiveness of our new approach in terms of prediction accuracy, cognitive diagnostic ability, and scalability.}
}


@article{DBLP:journals/tkde/LiQYZC24,
	author = {Junyu Li and
                  Fei Qi and
                  Haoliang Yuan and
                  Cheng Zhong and
                  Hongmin Cai},
	title = {Stacked Network to Realize Spectral Clustering With Adaptive Graph
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3501--3513},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3327043},
	doi = {10.1109/TKDE.2023.3327043},
	timestamp = {Mon, 10 Feb 2025 10:29:52 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiQYZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectral clustering with graph learning usually performs eigen-decomposition on the adaptive graph to obtain embedded representation for clustering. In terms of adaptive graph learning, the embedded representation is usually treated as the principal component of the graph to help improve graph structure. However, most adaptive graph learning methods only use a single graph layer. Therefore, the extraction power of embedded representation is restricted to single graph layer and insufficient to explore the intrinsic information. To break through this limitation, this article proposes a stacked network to realize spectral clustering with adaptive graph learning (SCnet-AGL). Specifically, the network allows the development of latent embedded representation underlying the multiple graph layers to reveal the intrinsic information. Meanwhile, we have designed an adaptive graph learning scheme to exploit the latent embedded representation for graph learning. With the advantage of the network, an augmented graph is obtained by incorporating the representation information for graph learning layer by layer. Finally, an efficient algorithm with feedback training scheme is proposed for network training. Experiments on real datasets demonstrate the effectiveness of the proposed network, and show that it is feasible to develop latent embedded representation to improve clustering performance.}
}


@article{DBLP:journals/tkde/LiangLYCXLL24,
	author = {Meiyu Liang and
                  Yawen Li and
                  Yang Yu and
                  Xiaowen Cao and
                  Zhe Xue and
                  Ang Li and
                  Kangkang Lu},
	title = {Structures Aware Fine-Grained Contrastive Adversarial Hashing for
                  Cross-Media Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3514--3528},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3356258},
	doi = {10.1109/TKDE.2024.3356258},
	timestamp = {Mon, 17 Feb 2025 16:53:28 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiangLYCXLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep cross-media hashing provides an efficient semantic representation learning solution for large-scale cross-media retrieval. The existing methods only consider the inter-media or intra-media semantic association learning, ignore the guiding of semantic structure information, and have weak reasoning ability for implicit fine-grained semantic associations. To tackle this problem, we propose a novel structures aware fine-grained contrastive adversarial hashing method for cross-media retrieval. A novel cross-media contrastive adversarial hash network is constructed for the first time, which integrates the cross-media and intra-media contrastive learning and multi-modal adversarial learning, aiming at maximizing the semantic association between different modalities, and improving the semantic discrimination and consistency of cross-media unified hash representation, thereby the inter-media and intra-media semantic preserving ability can be well enhanced; A fine-grained cross-media semantic feature learning method based on fine-grained semantic reasoning with transformers is proposed, which captures fine-grained salient features of different modalities for semantic association learning, and enhances the reasoning ability of fine-grained implicit semantic association; A semantic label graph convolutional network guided cross-media semantic association learning strategy is proposed, which makes full use of semantic structure information to enhance the learning ability of implicit cross-media semantic associations. Extensive experiments on several large-scale cross-media benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art methods.}
}


@article{DBLP:journals/tkde/VMG24,
	author = {Venktesh V and
                  Mukesh K. Mohania and
                  Vikram Goyal},
	title = {TagRec++: Hierarchical Label Aware Attention Network for Question
                  Categorization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3529--3540},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3354504},
	doi = {10.1109/TKDE.2024.3354504},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/VMG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online learning systems have multiple data repositories in the form of transcripts, books and questions. To enable ease of access, such systems organize the content according to a well defined taxonomy of hierarchical nature (subject - chapter -topic). The task of categorizing inputs to the hierarchical labels is usually cast as a flat multi-class classification problem. Such approaches ignore the semantic relatedness between the terms in the input and the tokens in the hierarchical labels. Alternate approaches also suffer from class imbalance when they only consider leaf level nodes as labels. To tackle the issues, we formulate the task as a dense retrieval problem to retrieve the appropriate hierarchical labels for each content. In this paper, we deal with categorizing questions and learning content. We model the hierarchical labels as a composition of their tokens and use an efficient cross-attention mechanism to fuse the information with the term representations of the content. We also adopt an adaptive in-batch hard negative sampling approach which samples better negatives as the training progresses. We demonstrate that the proposed approach TagRec++ outperforms existing state-of-the-art approaches on question and learning content datasets as measured by Recall@k. In addition, we demonstrate zero-shot capabilities of TagRec++ and preliminary analysis of it's ability to adapt to label changes.}
}


@article{DBLP:journals/tkde/XuZLLS24,
	author = {Yang Xu and
                  Lei Zhu and
                  Jingjing Li and
                  Fengling Li and
                  Heng Tao Shen},
	title = {Temporal Social Graph Network Hashing for Efficient Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3541--3555},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3352255},
	doi = {10.1109/TKDE.2024.3352255},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuZLLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hashing-based recommender systems that represent users and items as binary hash codes are recently proposed to significantly improve time and space efficiency. However, the highly developed social media presents two major challenges to hashing-based recommendation algorithms. First, the boundary between information producers and consumers becomes blurred, resulting in the rapid emergence of massive online content. Meanwhile, users’ limited information consumption capacity inevitably causes further interaction sparsity. The inherent high sparsity of data leads to insufficient hash learning. Second, a considerable amount of online content becomes fast-moving consumer goods, such as short videos and news commentary, causing frequent changes in user interests and item popularity. To address the above problems, we propose a Temporal Social Graph Network Hashing (TSGNH) method for efficient recommendation, which generates binary hash codes of users and items through dynamic-adaptive aggregation on a constructed temporal social graph network. Specifically, we build a temporal social graph network to fully capture the social information widely existing in practical recommendation scenarios and propose a dynamic-adaptive aggregation method to capture long-term and short-term characters of users and items. Furthermore, different from the discrete optimization approaches used by existing hashing-based recommendation methods, we devise an end-to-end hashing learning approach that incorporates balanced and de-correlated constraints to learn compact and informative binary hash codes tailored for recommendation scenarios. Extensive experiments on three widely evaluated recommendation datasets demonstrate the superiority of the proposed method.}
}


@article{DBLP:journals/tkde/ZouZWMLCFF24,
	author = {Ding Zou and
                  Sen Zhao and
                  Wei Wei and
                  Xian{-}Ling Mao and
                  Ruixuan Li and
                  Dangyang Chen and
                  Rui Fang and
                  Yuanyuan Fu},
	title = {Towards Hierarchical Intent Disentanglement for Bundle Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3556--3567},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3329175},
	doi = {10.1109/TKDE.2023.3329175},
	timestamp = {Mon, 26 Aug 2024 10:57:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZouZWMLCFF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bundle recommendation aims to recommend a bundle of items for the user to purchase together, for which two scenarios (i.e., Next-bundle recommendation and Within-bundle recommendation) are explored to recommend a specific bundle of items for the user and a specific item to fill the user's current bundle, respectively. Previous works largely model the user's preference with a uniform intent, without considering the diversity of intents when adopting the items within the bundle. In the real scenario of bundle recommendation, user intents modeling actually needs to be considered from three hierarchical levels, for that: a user's intents may be naturally distributed in different bundles (user level), one bundle may contain multiple intents of a user (bundle level), and an item in different bundles may also present different user intents (item level). To this end, we develop a novel model, Hierarchical Intent Disentangle Graph Networks (HIDGN) for bundle recommendation. HIDGN is capable of capturing the diversity of the user's intent precisely and comprehensively from the hierarchical structure with an cross-task intent contrastive learning, which is unified with the supervised next-/within-bundle recommendation sub-tasks as a multi-task framework. Extensive experiments on three benchmark datasets demonstrate that HIDGN outperforms the state-of-the-art methods by 43.0\\%, 13.2\\%, and 73.3\\%, respectively.}
}


@article{DBLP:journals/tkde/LiWLYW24,
	author = {Yanming Li and
                  Shiye Wang and
                  Changsheng Li and
                  Ye Yuan and
                  Guoren Wang},
	title = {Towards Very Deep Representation Learning for Subspace Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3568--3579},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3362984},
	doi = {10.1109/TKDE.2024.3362984},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiWLYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep subspace clustering based on the self-expressive layer has attracted increasing attention in recent years. Due to the self-expressive layer, these methods need to load the whole dataset into one batch for learning the self-expressive coefficients. Such a learning strategy puts a great burden on memory, which severely prevents from the usage of deeper network architectures (e.g., ResNet), and becomes a bottleneck for applying to large-scale data. In this paper, we propose a new deep subspace clustering framework, in order to address the above challenges. In contrast to previous approaches taking the weights of a fully connected layer as the self-expressive coefficients, we attempt to obtain the self-expressive coefficients by learning an energy based network in a mini-batch training manner. By this means, it is no longer necessary to load all data into one batch for learning, thus avoiding the above issue. Considering the powerful representation ability of the recently popular self-supervised learning, we leverage self-supervised representation learning to learn the dictionary for representing data. Finally, we propose a joint framework to learn both the self-expressive coefficients and the dictionary simultaneously. Extensive experiments on three publicly available datasets demonstrate the effectiveness of our method.}
}


@article{DBLP:journals/tkde/PanLWCWW24,
	author = {Shirui Pan and
                  Linhao Luo and
                  Yufei Wang and
                  Chen Chen and
                  Jiapu Wang and
                  Xindong Wu},
	title = {Unifying Large Language Models and Knowledge Graphs: {A} Roadmap},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3580--3599},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3352100},
	doi = {10.1109/TKDE.2024.3352100},
	timestamp = {Sun, 19 Jan 2025 13:53:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/PanLWCWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.}
}


@article{DBLP:journals/tkde/ZhengLXLZSQ24,
	author = {Hangyu Zheng and
                  Youhuan Li and
                  Fang Xiong and
                  Xiaosen Li and
                  Lei Zou and
                  Peifan Shi and
                  Zheng Qin},
	title = {Vertex Encoding for Edge Nonexistence Determination With {SIMD} Acceleration},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3600--3614},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3350919},
	doi = {10.1109/TKDE.2024.3350919},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengLXLZSQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose to design vertex encoding for determinations of no-result edge queries that should not be executed. Edge query is one of the core operations in mainstream graph databases, which is to retrieve edges connecting two given vertices. Real-world graphs may be too large to be stored in memory and frequently accessing edge data on disk usually incurs much overhead. The average degree of real-world graph tends to be much less than the vertex number, and edges may not exist in most pairs of vertices. Efficiently avoiding no-result edge query executions will certainly improve the performance of graph database. In this article, we propose a new and important problem for determining no-result edge queries: vertex encoding for edge nonexistence determination (VEND, for short). We build a low dimensional vertex encoding for all vertices, and we can efficiently determine most vertex pairs that are connected by no edges just with their corresponding codes. The encoding can be efficiently adjusted when data updates happen. With VEND, we can utilize in-memory efficient operations to filter no-result disk accesses for edge query. We also design SIMD-oriented compression optimizations to further improve performance. Extensive experiments on real-world datasets confirm the effectiveness of our solution.}
}


@article{DBLP:journals/tkde/LiuKZPHYOZY24,
	author = {Yang Liu and
                  Yan Kang and
                  Tianyuan Zou and
                  Yanhong Pu and
                  Yuanqin He and
                  Xiaozhou Ye and
                  Ye Ouyang and
                  Ya{-}Qin Zhang and
                  Qiang Yang},
	title = {Vertical Federated Learning: Concepts, Advances, and Challenges},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {7},
	pages = {3615--3634},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3352628},
	doi = {10.1109/TKDE.2024.3352628},
	timestamp = {Sun, 19 Jan 2025 13:53:46 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuKZPHYOZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.}
}


@article{DBLP:journals/tkde/LiXWXLCZW24,
	author = {Xiaocan Li and
                  Kun Xie and
                  Xin Wang and
                  Gaogang Xie and
                  Kenli Li and
                  Jiannong Cao and
                  Dafang Zhang and
                  Jigang Wen},
	title = {A Light-Weight and Robust Tensor Convolutional Autoencoder for Anomaly
                  Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4346--4360},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3332784},
	doi = {10.1109/TKDE.2023.3332784},
	timestamp = {Wed, 18 Sep 2024 14:53:45 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiXWXLCZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Robust PCA is a popular anomaly detection technique and has been widely used in many applications. Although Robust PCA is promising, it is usually designed in a two-order matrix form, which is inferior to the tensor that can capture multilinearity features of data. Moreover, the detection accuracy under Robust PCA further suffers due to its sensitivity to the rank parameter which is hard to set in practice and the limitation of PCA method in capturing the non-linear feature in the data. To address the issues, we propose a Robust Tensor Convolutional Autoencoder (RTCAE) where the autoencoder instead of SVD is exploited to recover the normal data from the corrupted measurement tensor data. However, directly exploiting deep autoencoder may suffer from the problem of high memory consumption and computation overhead due to the large number of parameters used in autoencoder. To make our anomaly detection lightweight, we further design a Light Convolutional Autoencoder (LightCAE) which contains a compressed autoencoder by exploiting tensor factorization to largely compress the parameters while significantly reducing the computation complexity. We conduct extensive experiments on three real data traces to compare the performance of our proposed schemes (RTCAE and lightCAE) with that of seven baseline algorithms. The experiment results demonstrate that our proposed RTCAE achieves the highest anomaly detection accuracy. Moreover, our LightCAE requires over 60 times smaller memory storage than that required in RTCAE while achieving the similar anomaly detection accuracy.}
}


@article{DBLP:journals/tkde/LiWYZ24,
	author = {Junyi Li and
                  Lizhen Wang and
                  Peizhong Yang and
                  Lihua Zhou},
	title = {A Novel Algorithm for Efficiently Mining Spatial Multi-Level Co-Location
                  Patterns},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4361--4374},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3381178},
	doi = {10.1109/TKDE.2024.3381178},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiWYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The spatial co-location pattern is a collection of spatial features in which instances of features prevalently appear in neighboring spatial regions. Due to the heterogeneity of spatial data distribution, the instances of some patterns appear prevalently in the global region (i.e., Global Prevalent Co-location Patterns, GPCPs), while some patterns are not prevalent globally, and their instances are clustered only in some local regions (i.e., Local Prevalent Co-location Patterns, LPCPs). Multi-level co-location pattern mining aims to mine these two types of patterns simultaneously, but existing methods cannot accurately judge the spatial distribution of patterns in a certain region, leading to unsuitable judgment of both GPCPs and LPCPs. To overcome this problem, this paper firstly proposes the relative distribution coefficient to identify the spatial distribution form of patterns, and provides a more refined way for discovering both GPCPs and LPCPs. Secondly, a novel multi-level co-location pattern mining algorithm is proposed by using the relative distribution coefficient as the interest metrics, and some pruning strategies are suggested to improve the mining efficiency. Finally, extensive experiments are conducted on both real and synthetic datasets to verify the effectiveness and efficiency of the proposed method.}
}


@article{DBLP:journals/tkde/WuLGZL24,
	author = {Lirong Wu and
                  Haitao Lin and
                  Zhangyang Gao and
                  Guojiang Zhao and
                  Stan Z. Li},
	title = {A Teacher-Free Graph Knowledge Distillation Framework With Dual Self-Distillation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4375--4385},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3374773},
	doi = {10.1109/TKDE.2024.3374773},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuLGZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for such an academic-industry gap is the neighborhood-fetching latency incurred by data dependency in GNNs. To reduce their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a standard teacher-student architecture, to distill knowledge from a large teacher GNN into a lightweight student GNN or MLP. However, we found in this paper that neither teachers nor GNNs are necessary for graph knowledge distillation. We propose a Teacher-Free Graph Self-Distillation (TGS) framework that does not require any teacher model or GNNs during both training and inference. More importantly, the proposed TGS framework is purely based on MLPs, where structural information is only implicitly used to guide dual knowledge self-distillation between the target node and its neighborhood. As a result, TGS enjoys the benefits of graph topology awareness in training but is free from data dependency in inference. Extensive experiments have shown that the performance of vanilla MLPs can be greatly improved with dual self-distillation, e.g., TGS improves over vanilla MLPs by 15.54% on average and outperforms state-of-the-art GKD algorithms on six real-world datasets. In terms of inference speed, TGS infers 75×-89× faster than existing GNNs and 16×-25× faster than classical inference acceleration methods.}
}


@article{DBLP:journals/tkde/WangTF24,
	author = {Yuan Wang and
                  Zhiqiang Tao and
                  Yi Fang},
	title = {A Unified Meta-Learning Framework for Fair Ranking With Curriculum
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4386--4397},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3377644},
	doi = {10.1109/TKDE.2024.3377644},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangTF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent information retrieval systems, it is observed that the datasets used to train machine learning models can be biased, leading to systematic discrimination against certain demographic groups, which means the ranking utility of specific groups is often lower than others in a biased dataset. Training models on these datasets will further decrease the exposure of the minority groups. To address this problem, we propose a Meta Curriculum-based Fair Ranking framework (MCFR) which could alleviate the data bias issue through the weighted loss using gradient-based learning to learn. Specifically, we optimize a meta learner from a sampled dataset (meta-dataset), and meanwhile train a ranking model on the whole (biased) dataset. The meta-dataset is sampled with a curriculum learning scheduler to guide the meta learner's training to gradually mitigate the skewness towards biased attributes. The meta learner serves as a weighting function to make the ranking loss focus more on the minority group. We formulate the proposed MCFR as a bilevel optimization problem and solve it using gradients through gradients. Extensive experiments on real-world datasets demonstrate that our approach can be used as a generic framework to work with various ranking losses and fairness metrics.}
}


@article{DBLP:journals/tkde/WangWLZCC24,
	author = {Heyuan Wang and
                  Tengjiao Wang and
                  Shun Li and
                  Jiayi Zheng and
                  Weijun Chen and
                  Wei Chen},
	title = {Agree to Disagree: Personalized Temporal Embedding and Routing for
                  Stock Forecast},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4398--4410},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3374373},
	doi = {10.1109/TKDE.2024.3374373},
	timestamp = {Tue, 04 Feb 2025 07:44:29 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangWLZCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stock forecast is a crucial yet challenging task in modern quantitative trading. Given theoretical and investment merits, recently a variety of deep learning methods have been proposed for automatically simulating stock movements from historical time series. However, these methods typically follow the i.i.d. assumption that actually contradicts the complex trading environment. In reality, individual stocks often exhibit diverse volatility patterns, while macro market scenarios may also change over time, jointly resulting in distribution shifts and weak generalization. To combat these bottlenecks, in this paper we propose a new learning architecture called Personalized Temporal Embedding and Routing (PTER) to improve stock forecast by forming a relaxed weight-sharing paradigm. The key of PTER is introducing hypernetworks to guide tailoring target network parameters, such that stock time series are embedded adapting to multi-object multi-scenario data disparities. Specifically, in the encoding stage, PTER first captures hyper-knowledge characterizing the similarity and peculiarity of different stocks and market scenarios. The knowledge space is then projected onto the temporal parameter space, enabling the customization of protruded features from chaotic observation signals. In the inference stage, each sample is dispatched to orthogonal predictor heads to dynamically output expected returns based on market conditions. Through experiments on benchmark datasets spanning over five years on four of the world's largest exchange markets, we show that PTER improves the cumulative and risk-adjusted revenue performance by a significant margin.}
}


@article{DBLP:journals/tkde/YouLWZCY24,
	author = {Linlin You and
                  Sheng Liu and
                  Tao Wang and
                  Bingran Zuo and
                  Yi Chang and
                  Chau Yuen},
	title = {AiFed: An Adaptive and Integrated Mechanism for Asynchronous Federated
                  Data Mining},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4411--4427},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3332770},
	doi = {10.1109/TKDE.2023.3332770},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YouLWZCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the growing concerns on datasecurity and user privacy, a decentralized mechanism is implemented for federated data mining (FDM), which can bridge data silos and collaborate diverse devices in ubiquitous IoT (Internet of Things) systems and services to extract global and shareable knowledge, i.e., encoded in deep neural networks (DDNs). Moreover, compared with FDM in synchronous mode, asynchronous FDM (AFDM) is more suitable to accommodate devices with diversified computing resources and distinguishable working statuses. However, as AFDM is still in its infancy, how to harness heterogeneous resources and biased knowledge of learning participants within the asynchronous context remains to be addressed. Such that, this paper proposes an adaptive and integrated mechanism, named AiFed, in which, a layer-wise optimization of AFDM is implemented based on the integration of two dedicated strategies, i.e., an adaptive local model uploading strategy (ALMU), and an adaptive global model aggregation strategy (AGMA). As shown by the evaluation results, AiFed can outperform five state-of-the-art methods to reduce communication costs by about 61.76% and 56.88%, improve learning accuracy by about 1.66% and 3.05%, and accelerate learning speed by about 22.16% and 37.81% under IID (independent and identically distributed) and Non-IID settings of four standard datasets, respectively.}
}


@article{DBLP:journals/tkde/YangH24,
	author = {Benyuan Yang and
                  Hesuan Hu},
	title = {An Efficient Verification Approach to Separation of Duty in Attribute-Based
                  Access Control},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4428--4442},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3373562},
	doi = {10.1109/TKDE.2024.3373562},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem considered in this paper is the verification and enforcement of separation of duty (SoD) constraints in attribute based access control (ABAC) systems. We propose an efficient algorithm for checking the satisfiability of SoD constraints. It is based on the idea of partitioning all permissions of SoD constraints into two classes so as to compute the minimal number of users to accomplish each class of permissions, respectively. As a result, several SoD constraints with certain class of permissions can be verified in polynomial time. Experimental results show that our method performs well compared with existing ones. When SoD violations occur, a 0–1 integer programming (IP) based enforcement solution is presented such that SoD violations can be solved once for all and it is provably shown that such solution does not result in the violation of other SoD constraints.}
}


@article{DBLP:journals/tkde/LiZWZZT24,
	author = {Shuyuan Li and
                  Yuxiang Zeng and
                  Yuxiang Wang and
                  Yiman Zhong and
                  Zimu Zhou and
                  Yongxin Tong},
	title = {An Experimental Study on Federated Equi-Joins},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4443--4457},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3375028},
	doi = {10.1109/TKDE.2024.3375028},
	timestamp = {Sun, 06 Oct 2024 21:41:29 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiZWZZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data federation has emerged as a novel database system enabling collaborative queries across mutually distrusted data owners. Federated equi-join, a commonly used operation in data federation, combines relations from distinct data owners while preserving their data privacy. Due to the wide applications of this query, many solutions to federated equi-joins have been proposed. However, it is still challenging for practitioners to choose the most appropriate algorithm due to various reasons, including incomplete evaluation protocols (e.g., lack of evaluating multi-way equi-joins), under-explored performance metric (main memory usage), and absence of a standardized comparison. Motivated by this reason, this paper conducts a comprehensive experimental study and builds a new benchmark, called {\\sf FEJ-Bench}, for federated equi-joins. The experimental study and the benchmark consist of eight state-of-the-art algorithms and five datasets. Our evaluation reveals the query efficiency ranking, its impact factors, and potential research opportunities. Finally, we open-source {\\sf FEJ-Bench} on GitHub, which is the first benchmark for federated equi-joins. Our findings aim to guide researchers and practitioners in deploying federated equi-joins in practice.}
}


@article{DBLP:journals/tkde/LiSCZYX24,
	author = {Yicong Li and
                  Xiangguo Sun and
                  Hongxu Chen and
                  Sixiao Zhang and
                  Yu Yang and
                  Guandong Xu},
	title = {Attention Is Not the Only Choice: Counterfactual Reasoning for Path-Based
                  Explainable Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4458--4471},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3373608},
	doi = {10.1109/TKDE.2024.3373608},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiSCZYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compared with only pursuing recommendation accuracy, the explainability of a recommendation model has drawn more attention in recent years. Many graph-based recommendations resort to informative paths with the attention mechanism for the explanation. Unfortunately, these attention weights are intentionally designed for model accuracy but not explainability. Recently, some researchers have started to question attention-based explainability because the attention weights are unstable for different reproductions, and they may not always align with human intuition. Inspired by the counterfactual reasoning from causality learning theory, we propose a novel explainable framework targeting path-based recommendations, wherein the explainable weights of paths are learned to replace attention weights. Specifically, we design two counterfactual reasoning algorithms from both path representation and path topological structure perspectives. Moreover, unlike traditional case studies, we also propose a package of explainability evaluation solutions with both qualitative and quantitative methods. We conduct extensive experiments on four real-world datasets, the results of which further demonstrate the effectiveness and reliability of our method.}
}


@article{DBLP:journals/tkde/MaLLXJSM24,
	author = {Chengyan Ma and
                  Di Lu and
                  Chaoyue Lv and
                  Ning Xi and
                  Xiaohong Jiang and
                  Yulong Shen and
                  Jianfeng Ma},
	title = {BiTDB: Constructing {A} Built-in {TEE} Secure Database for Embedded
                  Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4472--4485},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3380367},
	doi = {10.1109/TKDE.2024.3380367},
	timestamp = {Tue, 04 Mar 2025 20:33:44 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MaLLXJSM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose BiTDB, a built-in Trusted Execution Environment (TEE) database for embedded systems, to realize higher system availability while ensuring data confidentiality. With BiTDB, dilemmas that the state-of-the-art research work on secure embedded databases has to face can be significantly reduced and eliminated, including (i) complicated research and realization on searchable encryption algorithms (SEA), (ii) limited support to all database operations, and (iii) almost none of specific design and optimizations toward build-in TEE embedded databases. Through BiTDB, all database operations can process plaintext in TEE instead of retrieving ciphertext by developing complicated SEAs. To enable BiTDB to handle database files in Rich Execution Environment (REE) as local ones, we extend the TEE OS with generic file I/O libraries. Then, we contribute three critical optimizations to significantly reduce redundant memory and file operations between TEE and REE, and BiTDB achieve better system performance and availability in embedded systems. Finally, we have implemented the prototype system based on OP-TEE and SQLite for several typical platforms, including virtualization and hardware environments. The TPC-H test shows BiTDB can achieve 85% (on average) of the original database performance while guaranteeing data confidentiality and integrity.}
}


@article{DBLP:journals/tkde/YuanDXTL24,
	author = {Lixiang Yuan and
                  Mingxing Duan and
                  Guoqing Xiao and
                  Zhuo Tang and
                  Kenli Li},
	title = {{BM-FL:} {A} Balanced Weight Strategy for Multi-Stage Federated Learning
                  Against Multi-Client Data Skewing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4486--4498},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3372708},
	doi = {10.1109/TKDE.2024.3372708},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuanDXTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) combined with Differential Privacy (DP) is widespread in healthcare, finance, and IoT due to its advantages in multi-client data distribution. However, existing FL approaches overlook the differential impact levels among clients and data redundancy issues, resulting in high computational overhead and limited real-time applicability. Additionally, non-independent identical distribution (Non-IID) and imbalanced datasets in multi-clients pose challenges in privacy preservation and model overfitting. Therefore, we propose a balanced weight strategy for multi-stage federated learning against multi-client data skewing, called BM-FL, which involves clients, intermediate trust servers (ITSs), and the central server (CS). First, to protect data privacy, an improved Laplace \\epsilon\n-differential privacy method is employed. Second, a novel generative adversarial network (GAN) called BC-GAN is introduced. It is used to generate realistic fake samples and maintain a balanced proportion of samples across different categories. Then, to make full use of each client's valuable data, we designe a balanced weight strategy. Moreover, extensive experimental results clearly demonstrate the effectiveness of BM-FL in efficiently handling classification tasks involving Non-IID and imbalanced datasets while maintaining privacy and security. Furthermore, our method attains superior classification accuracy with fewer training epochs compared to relevant classical algorithms.}
}


@article{DBLP:journals/tkde/ZhaoXXFCZ24,
	author = {Jing Zhao and
                  Jiajie Xu and
                  Yuan Xu and
                  Junhua Fang and
                  Pingfu Chao and
                  Xiaofang Zhou},
	title = {{CCML:} Curriculum and Contrastive Learning Enhanced Meta-Learner
                  for Personalized Spatial Trajectory Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4499--4514},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3376539},
	doi = {10.1109/TKDE.2024.3376539},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoXXFCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial trajectory prediction is a fundamental problem for diverse location-based applications. However, existing methods fall short in learning and generalization, and cannot sufficiently capture users’ spatiotemporal preferences, especially for cold-start users. Moreover, these methods do not explicitly consider the diversity of moving patterns among users and trajectories, i.e., the learning difficulty of different user and trajectory samples, thus hindering the improvement of prediction accuracy. To solve these problems, we propose a novel Curriculum and Contrastive Learning Enhanced Meta-Learner (CCML) that transfers knowledge from users with rich data to cold-start users. Specifically, a Contrastive-based Trajectory Predictor (CTP) is designed as the base model, which utilizes contrastive learning technique on both user-level and trajectory-level, aiming to facilitate a more profound understanding and differentiation of the varied travel behaviors and preferences exhibited by individuals. Meanwhile, CCML also incorporates the curriculum learning and the hard sample mining strategies. It simultaneously considers the learning difficulty of both user and trajectory samples, and presents the learning tasks by an easy-to-hard curriculum. By learning more challenging combinations of user and trajectory samples in each meta-learning iteration, the meta-learner can converge to a better status. Extensive experiments on two real-world datasets demonstrate the superiority of our models.}
}


@article{DBLP:journals/tkde/SongGLQLJY24,
	author = {Yumeng Song and
                  Yu Gu and
                  Tianyi Li and
                  Jianzhong Qi and
                  Zhenghao Liu and
                  Christian S. Jensen and
                  Ge Yu},
	title = {{CHGNN:} {A} Semi-Supervised Contrastive Hypergraph Learning Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4515--4530},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3380643},
	doi = {10.1109/TKDE.2024.3380643},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SongGLQLJY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also includes basic and cross-validation contrastive losses, associated with an enhanced contrastive loss training process. Experimental results on nine real datasets offer insight into the effectiveness of CHGNN, showing that it outperforms 19 competitors in terms of classification accuracy consistently.}
}


@article{DBLP:journals/tkde/ZhangZLZGCPD24,
	author = {Yu Zhang and
                  Feng Zhang and
                  Hourun Li and
                  Shuhao Zhang and
                  Xiaoguang Guo and
                  Yuxing Chen and
                  Anqun Pan and
                  Xiaoyong Du},
	title = {Data-Aware Adaptive Compression for Stream Processing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4531--4549},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3377710},
	doi = {10.1109/TKDE.2024.3377710},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZLZGCPD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stream processing has been in widespread use, and one of the most common application scenarios is SQL query on streams. By 2021, the global deployment of IoT endpoints reached 12.3 billion, indicating a surge in data generation. However, the escalating demands for high throughput and low latency in stream processing systems have posed significant challenges due to the increasing data volume and evolving user requirements. We present a compression-based stream processing engine, called CompressStreamDB, which enables adaptive fine-grained stream processing directly on compressed streams, to significantly enhance the performance of existing stream processing solutions. CompressStreamDB utilizes nine diverse compression methods tailored for different stream data types and integrates a cost model to automatically select the most efficient compression schemes. CompressStreamDB provides high throughput with low latency in stream SQL processing by identifying and eliminating redundant data among streams. Our evaluation demonstrates that CompressStreamDB improves average performance by 3.84× and reduces average delay by 68.0% compared to the state-of-the-art stream processing solution for uncompressed streams, along with 68.7% space savings. Besides, our edge trials show an average throughput/price ratio of 9.95× and a throughput/power ratio of 7.32× compared to the cloud design.}
}


@article{DBLP:journals/tkde/NajafiQSJ24,
	author = {Mohammadreza Najafi and
                  Thamir M. Qadah and
                  Mohammad Sadoghi and
                  Hans{-}Arno Jacobsen},
	title = {{DIBA:} {A} Re-Configurable Stream Processor},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4550--4566},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3381192},
	doi = {10.1109/TKDE.2024.3381192},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NajafiQSJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stream processing acceleration is driven by the continuously increasing volume and velocity of data generated on the Web and the limitations of storage, computation, and power consumption. Hardware solutions provide better performance and power consumption, but they are hindered by the high research and development costs and the long time to market. In this work, we propose our re-configurable stream processor (Diba), a complete rethinking of a previously proposed customized and flexible query processor that targets real-time stream processing. Diba uses a unidirectional dataflow not dedicated to any specific type of query (operator) on streams, allowing a straightforward placement of processing components on a general data path that facilitates query mapping. In Diba, the concepts of the distribution network and processing components are implemented as two separate entities connected using generic interfaces. This approach allows the adoption of a versatile architecture for a family of queries rather than forcing a rigid chain of processing components to implement such queries. Our experimental evaluations of representative queries from TPC-H yielded processing times of 300, 1220, and 3520 milliseconds for data streams with scale factor sizes of one, four, and ten gigabytes, respectively.}
}


@article{DBLP:journals/tkde/ChenDBZJ24,
	author = {Lin Chen and
                  Xiaofeng Ding and
                  Zhifeng Bao and
                  Pan Zhou and
                  Hai Jin},
	title = {Differentially Private Federated Learning on Non-iid Data: Convergence
                  Analysis and Adaptive Optimization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4567--4581},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3379001},
	doi = {10.1109/TKDE.2024.3379001},
	timestamp = {Mon, 09 Dec 2024 22:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenDBZJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has attracted increasing attention in recent years due to its data privacy preservation and great applicability to large-scale user scenarios. However, when FL faces numerous clients, it is inevitable to emerge the non-independent and identically distributed (non-iid) data between clients, which brings an enormous challenge for model training and performance analysis like convergence. Besides, due to the non-iid data, the participating clients of FL tend to be extremely heterogeneous so the number of samplings among clients causes a sampling variance problem, which induces a huge variation in convergence. More importantly, although FL can foster privacy security via locally retaining the training data, if local data is secret and sensitive, FL should have more powerful privacy protection to resist the cloud server or third party to infer private information from shared models or intermediate gradients. Facing the non-iid and privacy challenges, we propose a differential privacy (DP) based non-iid FL algorithm called DPNFL to jointly tackle these two issues. Specifically, motivated by the DP and its variants, we are the first to adopt the truncated concentrated differential privacy technique under the FL scenario to more tightly track end-to-end privacy loss, while requiring less noise injection for the same level of DP. To avoid the sampling variance problem, we enable the server to sample the partial clients uniformly without replacement, which also guarantees unbiased sampling. To further improve the algorithm performance, we also propose an adaptive version of DPNFL named AdDPNFL, which adopts the adaptive optimization on the server-side to simultaneously alleviate the impact of non-iid data and DP noise on model utility. Finally, we perform extensive experiments to validate the effectiveness and superiority of our algorithms.}
}


@article{DBLP:journals/tkde/YangWWWW24,
	author = {Mingji Yang and
                  Hanzhi Wang and
                  Zhewei Wei and
                  Sibo Wang and
                  Ji{-}Rong Wen},
	title = {Efficient Algorithms for Personalized PageRank Computation: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4582--4602},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3376000},
	doi = {10.1109/TKDE.2024.3376000},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangWWWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized PageRank (PPR) is a traditional measure for node proximity on large graphs. For a pair of nodes \\boldsymbol{s} and \\boldsymbol{t}, the PPR value {\\boldsymbol{\\pi }_{s}(t)} equals the probability that an \\boldsymbol{\\alpha }-discounted random walk from \\boldsymbol{s} terminates at \\boldsymbol{t} and reflects the importance between \\boldsymbol{s} and \\boldsymbol{t} in a bidirectional way. As a generalization of Google's celebrated PageRank centrality, PPR has been extensively studied and has found multifaceted applications in many fields, such as network analysis, graph mining, and graph machine learning. Despite numerous studies devoted to PPR over the decades, efficient computation of PPR remains a challenging problem, and there is a dearth of systematic summaries and comparisons of existing algorithms. In this paper, we recap several frequently used techniques for PPR computation and conduct a comprehensive survey of various recent PPR algorithms from an algorithmic perspective. We classify these approaches based on the types of queries they address and review their methodologies and contributions. We also discuss some representative algorithms for computing PPR on dynamic graphs and in parallel or distributed environments.}
}


@article{DBLP:journals/tkde/ZengHHLL24,
	author = {Shuchang Zeng and
                  Ching{-}Fang Hsu and
                  Lein Harn and
                  Yi{-}Ning Liu and
                  Yang Liu},
	title = {Efficient and Privacy-Preserving Skyline Queries Over Encrypted Data
                  Under a Blockchain-Based Audit Architecture},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4603--4617},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3373602},
	doi = {10.1109/TKDE.2024.3373602},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZengHHLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Skyline queries is an advanced data mining algorithm suitable for multi-criteria decision-making scenarios (i.e., medical pre-diagnosis). Privacy-preserving skyline queries schemes are usually constructed by certain methods of cryptography such as additive homomorphic cryptosystem, secret sharing technology, etc. Interestingly, these secure skyline queries schemes require that skyline computations do not reveal any message details, including encrypted inter-tuple domination relations, among which privacy schemes based on homomorphic cryptosystems are the most popular due to their strong security. However, existing secure skyline queries schemes not only suffer from low computational efficiency, but also do not have sufficient security for privacy-key management in the system. To address the above issues, this paper designs an efficient and privacy-preserving skyline queries over encrypted data under a blockchain-based audit architecture. Firstly, we propose a blockchain-based audit architecture that not only provides error auditing functionality but also makes our scheme suitable for (distributed) multi-user scenarios while providing secure key management in the system. Secondly, we implement a series of secure sub-protocols using the CRT-Based Paillier encryption algorithm and construct a privacy sparse matrix elimination protocol to reduce the size of the dataset, leading to a significant reduction in computational cost without compromising privacy. Finally, we put forward our secure skyline queries protocol and prove its security. The performance evaluation shows that our proposed method our proposed method is significantly more efficient (at least 7.4 times faster) compared to current methods.}
}


@article{DBLP:journals/tkde/WangYGOTL24,
	author = {Jianhua Wang and
                  Jianye Yang and
                  Zhaoquan Gu and
                  Dian Ouyang and
                  Zhihong Tian and
                  Xuemin Lin},
	title = {Efficient Maximal Biclique Enumeration on Large Signed Bipartite Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4618--4631},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3373654},
	doi = {10.1109/TKDE.2024.3373654},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangYGOTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the problem of maximal biclique enumeration on large signed bipartite graphs. Given a signed bipartite graph G=(U,V,E,s), a parameter \\theta \\in [0.5, 1.0], our goal is to efficiently enumerate all maximal \\theta-bicliques in G, where a maximal \\theta-biclique B(L,R) is a complete subgraph of G with (1) the proportion of positive neighbors for each vertex in B is at least \\theta, and (2) B is not contained in another biclique B^{\\prime }, while B^{\\prime } also satisfies (1). This problem has many applications, such as biclustering for genes, recommendation of similar groups, collaboration in communities, etc. However, it is computationally challenging due to its #P-completeness. Besides, we prove that even determining the maximality of a \\theta-biclique is NP-hard. To the best of our knowledge, there is no efficient and scalable solution to this problem in the literature. In this paper, we first propose a branch-and-bound framework, namely {\\sf MSiBE}, which enumerates all maximal \\theta-bicliques in a depth-first manner. Then, we develop three effective optimizations to improve the performance of {\\sf MSiBE}. (1) The local information of each search space is utilized to enhance the pruning capacity. (2) When expanding the partial biclique, we always focus on the side with fewer candidates first, by which fruitless search branches can be skipped early. (3) We implement {\\sf MSiBE} with efficient array reordering techniques and set intersection strategy. To further accelerate the computation, we introduce useful graph reduction techniques. Comprehensive performance studies on 10 real datasets demonstrate that our proposals can significantly outperform the baseline methods by up to 3 orders of magnitude.}
}


@article{DBLP:journals/tkde/ZhangZS24,
	author = {Nan Zhang and
                  Xiaoqin Zhang and
                  Shiliang Sun},
	title = {Efficient Multiview Representation Learning With Correntropy and Anchor
                  Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4632--4645},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3332682},
	doi = {10.1109/TKDE.2023.3332682},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-based multiview clustering methods have attracted much attention because of their ability to mine nonlinear structural information among instances. Although they perform well in many scenarios, they consume a lot of computational resources when dealing with large-scale multiview scenarios. To address this issue, we present a new insight into the anchor graph mechanism and propose a novel Nonnegative Anchor Graph Reconstruction (NAGR) model. NAGR introduces the sparse similarity graph into the symmetric matrix factorization and gets the nonnegative representation that retains the graph structural information. Thereafter, we develop a novel Efficient Multiview nonnegative Representation learning framework with Correntropy and Anchor graph (EMR-CA), which integrates multiview anchor graph reconstruction and consensus nonnegative representation learning into a unified framework. EMR-CA uses multiview anchor graph reconstruction to learn consensus nonnegative representation, where correntropy rather than F-norm is used as the approximation measurement criterion. Specifically, normalized anchor graphs of different views are decomposed into a consensus nonnegative representation and multiple view-specific representations, where the consensus representation retains the neighbor graph information between multiview instances and representative anchors on different views. Finally, the effectiveness of the proposed EMR-CA framework is verified by theoretical analysis and experimental results on large-scale realistic multiview scenarios.}
}


@article{DBLP:journals/tkde/ZhongLCP24,
	author = {Haodi Zhong and
                  Grigorios Loukides and
                  Alessio Conte and
                  Solon P. Pissis},
	title = {Ego-Network Segmentation via (Weighted) Jaccard Median},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4646--4663},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3373712},
	doi = {10.1109/TKDE.2024.3373712},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhongLCP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An ego-network is a graph representing the interactions of a node (ego) with its neighbors and the interactions among those neighbors. A sequence of ego-networks having the same ego can thus model the evolution of these interactions over time. We introduce the problem of segmenting a sequence of ego-networks into k segments. Each segment is represented by a summary network, and the goal is to minimize the total loss of representing k segments by k summaries. The main challenge is to construct a summary with minimum loss. To address it, we employ the Jaccard Median (JM) problem, for which, however, no effective and efficient algorithms are known. We develop several algorithms for JM: (I) an exact algorithm, based on Mixed Integer Linear Programming; (II) exact and approximation polynomial-time algorithms for minimizing an upper bound of the objective function of JM; and (III) efficient heuristics. We also study a generalization of the segmentation problem, in which there may be multiple edges between a pair of nodes in an ego-network, and develop a series of algorithms (exact algorithms and heuristics) for it, based on a more general problem than JM, called Weighted Jaccard Median (WJM). By building upon the above results, we design algorithms for segmenting a sequence of ego-networks. Extensive experiments show that our algorithms produce (near)-optimal solutions to JM or to WJM and that they substantially outperform state-of-the-art methods which can be employed for ego-network segmentation.}
}


@article{DBLP:journals/tkde/LongZCRL24,
	author = {Zhen Long and
                  Ce Zhu and
                  Pierre Comon and
                  Yazhou Ren and
                  Yipeng Liu},
	title = {Feature Space Recovery for Efficient Incomplete Multi-View Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4664--4677},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3333522},
	doi = {10.1109/TKDE.2023.3333522},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LongZCRL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The tensor singular value decomposition (t-SVD) based incomplete multi-view clustering (IMVC) has received wide attention due to its ability to capture high-order correlations. However, t-SVD suffers from rotation sensitivity, failing to fully explore both inter- and intra-view consistencies. Besides, current methods mainly consider inter- or intra-view correlations, ignoring the low-rank information of sample features within views. To address these weaknesses, we first propose a feature space recovery based IMVC (FSR-IMVC) method, where low-rank feature space recovery and low-rank tensor ring based consistency learning are considered into a unified framework. Furthermore, we extend FSR-IMVC by incorporating anchor learning on the latent feature space, resulting in a scalable FSR-IMVC (sFSR-IMVC) approach that is well-suited to large-scale data. In an iterative way, the learned inter- and intra-view correlations will guide the recovery of missing features, while the explored low-rank information from feature spaces will in turn facilitate consistency exploration, eventually achieving outstanding clustering performance. Experimental results show that FSR-IMVC provides a significant improvement over known state-of-the-art algorithms in terms of ACC, NMI and Purity. Compared with FSR-IMVC, sFSR-IMVC performs slightly worse in clustering accuracy, but offers a notable advantage in computational efficiency, particularly for large-scale datasets.}
}


@article{DBLP:journals/tkde/DuLLXYC24,
	author = {Xinqi Du and
                  Ziyue Li and
                  Cheng Long and
                  Yongheng Xing and
                  Philip S. Yu and
                  Hechang Chen},
	title = {FELight: Fairness-Aware Traffic Signal Control via Sample-Efficient
                  Reinforcement Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4678--4692},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3376745},
	doi = {10.1109/TKDE.2024.3376745},
	timestamp = {Tue, 22 Oct 2024 20:38:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DuLLXYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic congestion is becoming an increasingly prominent problem, and intelligent traffic signal control methods can effectively alleviate it. Recently, there has been a growing trend of applying reinforcement learning to traffic signal control for adaptive signal scheduling. However, most existing methods focus on improving traffic performance while neglecting the issue of scheduling fairness, resulting in long waiting time for some vehicles. Some works attempt to address fairness issues but often sacrifice transport performance. Furthermore, existing methods overlook the challenge of sample efficiency, especially when dealing with diversity-limited traffic data. Therefore, we propose a Fairness-aware and sample-Efficient traffic signal control method called FELight. Specifically, we first design a novel fairness metric and integrate it into decision process to penalize cases with high latency by setting a threshold for activating the fairness mechanism. Theoretical comparison with other fairness works proves why and when our fairness could bring advantages. Moreover, counterfactual data augmentation is employed to enrich interaction data, enhancing the sample efficiency of FELight. Self-supervised state representation is introduced to extract informative features from raw states, further improving sample efficiency. Experiments on real traffic datasets demonstrate that FELight provides relatively fairer traffic signal control without compromising performance compared to state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/NingWRL24,
	author = {Nianwen Ning and
                  Bin Wu and
                  Haoqing Ren and
                  Qiuyue Li},
	title = {Graph Alignment Neural Network Model With Graph to Sequence Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4693--4706},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3329380},
	doi = {10.1109/TKDE.2023.3329380},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NingWRL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network alignment aims at detecting the corresponding entities across multiple networks, which is an essential basis for the fusion and analysis of multiple network information. Moreover, embedding-based network alignment has gradually become one of the promising methods. However, existing methods ignore the confusing selection problem caused by the similarity-orientated principle of network embedding and over-dependence on the hypothesis of structural consistency. In this paper, we propose an end-to-end Graph Alignment Neural Network (GANN) model with graph-to-sequence learning. GANN mainly consists of two modules: Graph encoder and Sequence decoder. In graph encoder module, we present a restricted network embedding method, which can not only capture the local structure and attribute information of nodes but also realize the constraint of node embedding and space reconciliation. In sequence decoder module, we propose a graph-to-sequence learning model to address large graphs’ structural consistency hypothesis problem. In this model, an attention-based LSTM mechanism is introduced to infer a node in the source network corresponding to the candidate node sequence in target networks. In this candidate sequence, the correct aligned node is placed at the top. We demonstrate that GANN outperforms the state-of-the-art methods in network alignment tasks on various real-world datasets.}
}


@article{DBLP:journals/tkde/TangC24,
	author = {Xing Tang and
                  Ling Chen},
	title = {{GTRL:} An Entity Group-Aware Temporal Knowledge Graph Representation
                  Learning Method},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4707--4721},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3334165},
	doi = {10.1109/TKDE.2023.3334165},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TangC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal Knowledge Graph (TKG) representation learning embeds entities and event types into a continuous low-dimensional vector space by integrating the temporal information, which is essential for downstream tasks, e.g., event prediction and question answering. Existing methods stack multiple graph convolution layers to model the influence of distant entities, leading to the over-smoothing problem. To alleviate the problem, recent studies infuse reinforcement learning to obtain paths that contribute to modeling the influence of distant entities. However, due to the limited number of hops, these studies fail to capture the correlation between entities that are far apart and even unreachable. To this end, we propose GTRL, an entity Group-aware Temporal knowledge graph Representation Learning method. GTRL is the first work that incorporates the entity group modeling to capture the correlation between entities by stacking only a finite number of layers. Specifically, the entity group mapper is proposed to generate entity groups from entities in a learning way. Based on entity groups, the implicit correlation encoder is introduced to capture implicit correlations between any pairwise entity groups. In addition, the hierarchical GCNs are exploited to accomplish the message aggregation and representation updating on the entity group graph and the entity graph. Finally, GRUs are employed to capture the temporal dependency in TKGs. Extensive experiments on six real-world datasets demonstrate that GTRL achieves the state-of-the-art performances on the event prediction task, outperforming the best baseline by an average of 7.35%, 6.09%, 8.31%, and 11.21% in MRR, Hits@1, Hits@3, and Hits@10, respectively.}
}


@article{DBLP:journals/tkde/ChenJLLLCYH24,
	author = {Jie Chen and
                  Licheng Jiao and
                  Xu Liu and
                  Lingling Li and
                  Fang Liu and
                  Puhua Chen and
                  Shuyuan Yang and
                  Biao Hou},
	title = {Hierarchical Dynamic Graph Clustering Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4722--4735},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3333529},
	doi = {10.1109/TKDE.2023.3333529},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenJLLLCYH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Connections between visual components are ubiquitous. Graphs, as a highly flexible data structure, not only allow imposing relational induction bias on data, but can provide a completely distinct learning perspective for regular image data. In this article, we propose a hierarchical dynamic graph clustering network (HDGCN) for visual feature learning. We construct hierarchical graph representations in graph domain in an adaptive, data-adaptive and task-adaptive manner. First, the initial graph is constructed in high-dimensional feature domain of images. To mine the hierarchical geometric features in latent graph space, adaptive clustering network (ClusterNet) is performed to learn discriminative clusters and generates cluster-based coarse graph. Then, graph convolutional networks (GCNs) are used to diffuse, transform and aggregate information among clusters. So, the intra-class and inter-class information is fully explored to increase the discriminativity of graph representations. Next, coarsened graph representations are mapped to grid based on its affinity with linear projection features. To further improve the task adaptation of clusters and hierarchical graph representations, ClusterNet and GCNs are fused in the same framework for end-to-end training and clusters is updated dynamically. We have conducted extensive experiments on classification and segmentation tasks. The experimental results fully validate the robustness of the proposed algorithm.}
}


@article{DBLP:journals/tkde/YaoLHSWXC24,
	author = {Yao Yao and
                  Bin Liu and
                  Haoxun He and
                  Dakui Sheng and
                  Ke Wang and
                  Li Xiao and
                  Huanhuan Cao},
	title = {I-Razor: {A} Differentiable Neural Input Razor for Feature Selection
                  and Dimension Search in DNN-Based Recommender Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4736--4749},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3332671},
	doi = {10.1109/TKDE.2023.3332671},
	timestamp = {Wed, 04 Sep 2024 16:50:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YaoLHSWXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Input features play a crucial role in DNN-based recommender systems with thousands of categorical and continuous fields from users, items, contexts, and interactions. Noisy features and inappropriate embedding dimension assignments can deteriorate the performance of recommender systems and introduce unnecessary complexity in model training and online serving. Optimizing the input configuration of DNN models, including feature selection and embedding dimension assignment, has become one of the essential topics in feature engineering. However, in existing industrial practices, feature selection and dimension search are optimized sequentially, i.e., feature selection is performed first, followed by dimension search to determine the optimal dimension size for each selected feature. Such a sequential optimization mechanism increases training costs and risks generating suboptimal input configurations. To address this problem, we propose a differentiable neural input razor (i-Razor) that enables joint optimization of feature selection and dimension search. Concretely, we introduce an end-to-end differentiable model to learn the relative importance of different embedding regions of each feature. Furthermore, a flexible pruning algorithm is proposed to achieve feature filtering and dimension derivation simultaneously. Extensive experiments on two large-scale public datasets in the Click-Through-Rate (CTR) prediction task demonstrate the efficacy and superiority of i-Razor in balancing model complexity and performance.}
}


@article{DBLP:journals/tkde/HeXCSBGRJ24,
	author = {Erhu He and
                  Yiqun Xie and
                  Weiye Chen and
                  Sergii Skakun and
                  Han Bao and
                  Rahul Ghosh and
                  Praveen Ravirathinam and
                  Xiaowei Jia},
	title = {Learning With Location-Based Fairness: {A} Statistically-Robust Framework
                  and Acceleration},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4750--4765},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3371460},
	doi = {10.1109/TKDE.2024.3371460},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HeXCSBGRJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fairness related to locations (i.e., “where”) is critical for the use of machine learning in a variety of societal domains involving spatial datasets (e.g., agriculture, disaster response, urban planning). Spatial biases incurred by learning, if left unattended, may cause or exacerbate unfair distribution of resources, social division, spatial disparity, etc. The goal of this work is to develop statistically-robust formulations and model-agnostic learning strategies to understand and promote spatial fairness. The problem is challenging as locations are often from continuous spaces with no well-defined categories (e.g., gender), and statistical conclusions from spatial data are fragile to changes in spatial partitionings and scales. Existing studies in fairness-driven learning have generated valuable insights related to non-spatial factors including race, gender, education level, etc., but research to mitigate location-related biases still remains in its infancy, leaving the main challenges unaddressed. To bridge the gap, we first propose a robust space-as-distribution (SPAD) representation of spatial fairness to reduce statistical sensitivity related to partitionings and scales in continuous space. Furthermore, we propose a new SPAD-based stochastic strategy to efficiently optimize over an extensive distribution of fairness criteria, and a bi-level training framework to enforce fairness via adaptive adjustment of priorities among locations. Finally, we extend this framework with a similarity-based training strategy to improve the computational efficiency. Experiments conducted on two real-world problems, crop monitoring in the US and palm oil plantation mapping in Indonesia, show that SPAD can effectively reduce sensitivity in fairness evaluation and the stochastic bi-level training framework can greatly improve the fairness. Controlled experiments also show that similarity-based acceleration can greatly reduce the training time while keeping the prediction performance and fairness results at the same level.}
}


@article{DBLP:journals/tkde/ZhangYZQ24,
	author = {Yikai Zhang and
                  Jeffrey Xu Yu and
                  Ying Zhang and
                  Lu Qin},
	title = {Maintaining Top-{\textdollar}t{\textdollar} Cores in Dynamic Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4766--4780},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3332638},
	doi = {10.1109/TKDE.2023.3332638},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangYZQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs have been widely used in many applications. One important graph analytics is to explore cohesive subgraphs in a large graph. Among several cohesive subgraphs studied, k-core is one that can be computed in linear time for a static graph. Since graphs are evolving in real applications, in this paper, we study core maintenance which is to reduce the computational cost to compute k-cores for a graph when graphs are updated from time to time dynamically. We identify drawbacks of the existing efficient algorithm, which needs a large search space to find the vertices that need to be updated, and has high overhead to maintain the index built, when a graph is updated. We propose a new order-based approach to maintain an order, called k-order, among vertices, while a graph is updated. Our new algorithm can significantly outperform the state-of-the-art algorithm up to 3 orders of magnitude for the 11 large real graphs tested. In addition, we also study the problem of partial core maintenance, which is to maintain the top-t cores of the graph for a given positive integer t. By instead maintaining only a small subset of cores, further improvement in performance can be obtained.}
}


@article{DBLP:journals/tkde/YeSJ24,
	author = {Hui Ye and
                  Rajshekhar Sunderraman and
                  Shihao Ji},
	title = {MatchXML: An Efficient Text-Label Matching Framework for Extreme Multi-Label
                  Text Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4781--4793},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3374750},
	doi = {10.1109/TKDE.2024.3374750},
	timestamp = {Mon, 09 Dec 2024 16:44:20 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YeSJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The eXtreme Multi-label text Classification (XMC) refers to training a classifier that assigns a text sample with relevant labels from an extremely large-scale label set (e.g., millions of labels). We propose MatchXML, an efficient text-label matching framework for XMC. We observe that the label embeddings generated from the sparse Term Frequency-Inverse Document Frequency (TF–IDF) features have several limitations. We thus propose label2vec to effectively train the semantic dense label embeddings by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical Label Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we formulate the multi-label text classification as a text-label matching problem in a bipartite graph. We then extract the dense text representations from the fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also extract the static dense sentence embeddings from a pre-trained Sentence Transformer. Finally, a linear ranker is trained by utilizing the sparse TF–IDF features, the fine-tuned dense text representations, and static dense sentence features. Experimental results demonstrate that MatchXML achieves the state-of-the-art accuracies on five out of six datasets. As for the training speed, MatchXML outperforms the competing methods on all the six datasets.}
}


@article{DBLP:journals/tkde/ZhuZFYWH24,
	author = {Xinyuan Zhu and
                  Yang Zhang and
                  Fuli Feng and
                  Xun Yang and
                  Dingxian Wang and
                  Xiangnan He},
	title = {Mitigating Hidden Confounding Effects for Causal Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4794--4805},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3378482},
	doi = {10.1109/TKDE.2024.3378482},
	timestamp = {Thu, 14 Nov 2024 17:26:18 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuZFYWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems suffer from confounding biases when there exist confounders affecting both item features and user feedback (e.g., like or not). Existing causal recommendation methods typically assume confounders are fully observed and measured, forgoing the possible existence of hidden confounders in real applications. For instance, product quality is a confounder since it affects both item prices and user ratings, but is hidden for the third-party e-commerce platform due to the difficulty of large-scale quality inspection; ignoring it could result in the bias effect of over-recommending high-price items. This work analyzes and addresses the problem from a causal perspective. The key lies in modeling the causal effect of item features on a user's feedback. To mitigate hidden confounding effects, it is compulsory but challenging to estimate the causal effect without measuring the confounder. Towards this goal, we propose a Hidden Confounder Removal (HCR) framework that leverages front-door adjustment to decompose the causal effect into two partial effects, according to the mediators between item features and user feedback. The partial effects are independent from the hidden confounder and identifiable. During training, HCR performs multi-task learning to infer the partial effects from historical interactions. We instantiate HCR for two scenarios and conduct experiments on three real-world datasets. Empirical results show that the HCR framework provides more accurate recommendations, especially for less-active users. We will release the code once accepted.}
}


@article{DBLP:journals/tkde/ZhuoQWH24,
	author = {Shengda Zhuo and
                  Jin{-}Jie Qiu and
                  Chang{-}Dong Wang and
                  Shu{-}Qiang Huang},
	title = {Online Feature Selection With Varying Feature Spaces},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4806--4819},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3377243},
	doi = {10.1109/TKDE.2024.3377243},
	timestamp = {Thu, 13 Feb 2025 10:32:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuoQWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature selection, an essential technique in data mining, is often confined to batch learning or online idealization of data scenarios despite its significance. Existing online feature selection methods have specific assumptions regarding the data stream, such as requiring a fixed feature space with an explicit pattern and complete labeling of samples. Unfortunately, data streams generated in many real scenarios commonly exhibit arbitrarily incomplete feature spaces and scarcity labels, making existing approaches unsuitable for real applications. To fill these gaps, this study proposes a new problem called Online Feature Selection with Varying Features Spaces (OFSVF). OFSVF has a three-fold main idea: 1) it leverages Gaussian Copula to model the incomplete feature correlation in a complete latent space, encoded by continuous variables, 2) it employs a novel tree-ensemble-based approach to select the most informative features on-the-fly, and 3) it develops the underlying geometric structure of instances to establish the relationship between unlabeled and labels. Experimental results are documented to demonstrate the feasibility and effectiveness of our proposed method.}
}


@article{DBLP:journals/tkde/YouYXCWSW24,
	author = {Dianlong You and
                  Huigui Yan and
                  Jiawei Xiao and
                  Zhen Chen and
                  Di Wu and
                  Limin Shen and
                  Xindong Wu},
	title = {Online Learning for Data Streams With Incomplete Features and Labels},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4820--4834},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3374357},
	doi = {10.1109/TKDE.2024.3374357},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YouYXCWSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online learning is critical for handling complex data streams in Big Data-related applications. This study explores a new online learning problem where both the features and labels are incomplete. Such incompleteness poses a critical challenge in determining the latent relationship between incomplete features and labels. Unfortunately, existing online learning methods only consider a few cases of incomplete feature spaces, such as trapezoidal, evolvable, and capricious data streams, limiting their applicability to this problem. To bridge this gap, this study proposes a novel algorithm of Online Learning for Data Streams with Incomplete Features and Labels (OLIFL). OLIFL imposes no constraints on changing patterns of feature space and does not require all instances to be labeled with two-fold ideas. First, OLIFL explores the informativeness of individual features to update the classifier by dynamically maintaining global feature space and updating the informativeness matrix. Second, it estimates the label confidence of unlabeled instances to control their negative effects by limiting the error upper bound. Extensive experiments on benchmark datasets are conducted in five scenarios: three incomplete feature (trapezoidal, evolvable, and capricious) spaces, and two incomplete labels (only missing labels and missing both features and labels). In addition, we explore the sensitivity of the model to parameters, and its usability and response efficiency in handling concept drifts. The results show that OLIFL significantly outperforms its rivals. Moreover, we use OLIFL to classify a movie review task as real application verification.}
}


@article{DBLP:journals/tkde/ZhongDLDT24,
	author = {Qihuang Zhong and
                  Liang Ding and
                  Juhua Liu and
                  Bo Du and
                  Dacheng Tao},
	title = {PanDa: Prompt Transfer Meets Knowledge Distillation for Efficient
                  Model Adaptation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4835--4848},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3376453},
	doi = {10.1109/TKDE.2024.3376453},
	timestamp = {Sun, 19 Jan 2025 13:53:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhongDLDT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Prompt Transfer (PoT) is a recently-proposed approach to improve prompt-tuning, by initializing the target prompt with the existing prompt trained on similar source tasks. However, such a vanilla PoT approach usually achieves sub-optimal performance, as (i) the PoT is sensitive to the similarity of source-target pair and (ii) directly fine-tuning the prompt initialized with source prompt on target task might lead to forgetting of the useful general knowledge learned from source task. To tackle these issues, we propose a new metric to accurately predict the prompt transferability (regarding (i)), and a novel PoT approach (namely PanDa) that leverages the knowledge distillation technique to alleviate the knowledge forgetting effectively (regarding (ii)). Extensive and systematic experiments on 189 combinations of 21 source and 9 target datasets across 5 scales of PLMs demonstrate that: 1) our proposed metric works well to predict the prompt transferability; 2) our PanDaconsistently outperforms the vanilla PoT approach by 2.3% average score (up to 24.1%) among all tasks and model sizes; 3) with our PanDaapproach, prompt-tuning can achieve competitive and even better performance than model-tuning in various PLM scales scenarios.}
}


@article{DBLP:journals/tkde/GaoLWHFZ24,
	author = {Yuan Gao and
                  Jinghan Li and
                  Xiang Wang and
                  Xiangnan He and
                  Huamin Feng and
                  Yongdong Zhang},
	title = {Revisiting Attack-Caused Structural Distribution Shift in Graph Anomaly
                  Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4849--4861},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3380709},
	doi = {10.1109/TKDE.2024.3380709},
	timestamp = {Mon, 26 Aug 2024 13:31:51 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GaoLWHFZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph anomaly detection (GAD) under semi-supervised setting poses a significant challenge due to the distinct structural distribution between anomalous and normal nodes. Specifically, anomalous nodes constitute a minority and exhibit high heterophily and low homophily compared to normal nodes. The distribution of neighbors of the two types of nodes is close, making them difficult to distinguish during aggregation. Furthermore, we discover that apart from various time factors and annotation preferences, graph adversarial attacks can amplify the heterophily difference across training and testing data, namely distribution shift (SDS) in this paper. Current methods for GAD tend to overlook SDS, resulting in poor generalization and limited effectiveness. This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence the key lies in (1) resisting high heterophily for anomalies and (2) benefiting the learning of normals from homophily. To this end, we design a Graph Decomposition Network (GDN), which not only teases out the anomaly features that make great contributions to GAD to mitigate the effect of heterophilous neighbors and make them invariant, but also constrain the remaining features for normal nodes to preserve the connectivity of nodes and reinforce the influence of the homophilous neighborhood. To further validate the effectiveness of our method, we illustrate the feature decomposition process in spectral domain, and we also conduct an adversarial attack to incur different heterophily degrees under SDS. Extensive experimental results demonstrate that our framework achieves both accuracy and robustness enhancement.}
}


@article{DBLP:journals/tkde/MilicevS24,
	author = {Dragan Milicev and
                  Zivojin Sustran},
	title = {Rewriting Queries for Hyper-Relations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4862--4873},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3374076},
	doi = {10.1109/TKDE.2024.3374076},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MilicevS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hyper-relation is a recently proposed formal concept for denormalization of relational databases. Hyper-relations are structures of prejoined relations the schema of which can be represented as a hyper-graph. Instead of entire relations of the tables in the initial normalized schema, hyper-relations prejoin their atomic constituents called base relations, and do that with only those base relations that are really used in the queries being optimized. In this paper, we derive a simple and efficient algorithm for rewriting queries for hyper-relations. The algorithm is based on incremental rewriting procedure that traverses the query expression tree and builds up the transformed query by seamlessly propagating the proposed \\Sigma\noperator, composed of selection and nullification of the tuples in a hyper-relation. The algorithm is applicable to a wide range of multiblock (nested) select-project-(outer) join queries.}
}


@article{DBLP:journals/tkde/GengWLLFZW24,
	author = {Meng Geng and
                  Youxi Wu and
                  Yan Li and
                  Jing Liu and
                  Philippe Fournier{-}Viger and
                  Xingquan Zhu and
                  Xindong Wu},
	title = {RNP-Miner: Repetitive Nonoverlapping Sequential Pattern Mining},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4874--4889},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3334300},
	doi = {10.1109/TKDE.2023.3334300},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GengWLLFZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential pattern mining (SPM) is an important branch of knowledge discovery that aims to mine frequent sub-sequences (patterns) in a sequential database. Various SPM methods have been investigated, and most of them are classical SPM methods, since these methods only consider whether or not a given pattern occurs within a sequence. Classical SPM can only find the common features of sequences, but it ignores the number of occurrences of the pattern in each sequence, i.e., the degree of interest of specific users. To solve this problem, this paper addresses the issue of repetitive nonoverlapping sequential pattern (RNP) mining and proposes the RNP-Miner algorithm. To reduce the number of candidate patterns, RNP-Miner adopts an itemset pattern join strategy. To improve the efficiency of support calculation, RNP-Miner utilizes the candidate support calculation algorithm based on the position dictionary. To validate the performance of RNP-Miner, 10 competitive algorithms and 20 sequence databases were selected. The experimental results verify that RNP-Miner outperforms the other algorithms, and using RNPs can achieve a better clustering performance than raw data and classical frequent patterns.}
}


@article{DBLP:journals/tkde/ZhangLLLSXG24,
	author = {Hao Zhang and
                  Yang Liu and
                  Xiaoyan Liu and
                  Tianming Liang and
                  Gaurav Sharma and
                  Liang Xue and
                  Maozu Guo},
	title = {Sentence Bag Graph Formulation for Biomedical Distant Supervision
                  Relation Extraction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4890--4903},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3377229},
	doi = {10.1109/TKDE.2024.3377229},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLLLSXG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce a novel graph-based framework for alleviating key challenges in distantly-supervised relation extraction and demonstrate its effectiveness in the challenging and important domain of biomedical data. Specifically, we propose a graph view of sentence bags referring to an entity pair, which enables message-passing based aggregation of information related to the entity pair over the sentence bag. The proposed framework alleviates the common problem of noisy labeling in distantly supervised relation extraction and also effectively incorporates inter-dependencies between sentences within a bag. Extensive experiments on two large-scale biomedical relation datasets and the widely utilized NYT dataset demonstrate that our proposed framework significantly outperforms the state-of-the-art methods for biomedical distant supervision relation extraction while also providing excellent performance for relation extraction in the general text mining domain.}
}


@article{DBLP:journals/tkde/BelleW24,
	author = {Rafa{\"{e}}l Van Belle and
                  Jochen De Weerdt},
	title = {{SHINE:} {A} Scalable Heterogeneous Inductive Graph Neural Network
                  for Large Imbalanced Datasets},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4904--4915},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3381240},
	doi = {10.1109/TKDE.2024.3381240},
	timestamp = {Sun, 08 Sep 2024 16:07:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BelleW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Research interest in machine learning (ML) for graphs has skyrocketed in recent years. However, non-euclidean graph structures inhibit the application of traditional ML algorithms. Consequently, scholars introduced graph learning algorithms tailored to network data, such as graph neural networks (GNNs). Most GNNs are designed for homogeneous and homophilous graphs and are evaluated on small, static, and balanced datasets, deviating from real-world conditions and industry applications. This paper introduces SHINE, a scalable heterogeneous inductive GNN for large imbalanced datasets. SHINE addresses four key challenges: scalability, network heterogeneity, inductive learning on dynamic graphs, and imbalanced node classification. SHINE comprises three core components: 1) a sampler based on nearest-neighbor (NN) search, 2) a heterogeneous GNN (HGNN) layer with a novel relationship aggregator, and 3) aggregator functions tailored to skewed class distributions. The components of SHINE are evaluated on benchmark datasets, while the integrated benefits of SHINE are demonstrated on two fraud detection datasets.}
}


@article{DBLP:journals/tkde/LiLSL24,
	author = {Lina Li and
                  Yang Liu and
                  Guodong Sun and
                  Nianfeng Li},
	title = {Smart Contract Vulnerability Detection Based on Automated Feature
                  Extraction and Feature Interaction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4916--4929},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3333371},
	doi = {10.1109/TKDE.2023.3333371},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiLSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart contract is the core of blockchain operation, and contract vulnerability will cause huge economic losses. Therefore, effective smart contract vulnerability detection is of vital importance and attracts more and more attention. In this paper, we propose a vulnerability detection model (VDM-AEI) based on automatic feature extraction and feature interaction. For the first time, this model converts smart contracts into gray images and uses VGG16 and GRU models to automatically extract vulnerability features and filter effective features, respectively. Then, a contract graph and an expert knowledge feature vector are constructed by using commonly used methods as part of feature construction. Next, AutoInt and DCN networks are used to build a dual feature interaction network to obtain more abundant vulnerability feature information, which extracts high-dimensional nonlinear features from the low and sparse features of the contract graph feature vector and the expert knowledge-defined feature vector. Finally, all output features of GRU, AutoInt and DCN networks are integrated to obtain vulnerability classification results through fully connected neural networks. We conducted extensive experiments on the ESC and VSC datasets for reentrancy vulnerabilities, timestamp dependency vulnerabilities, and infinite loop vulnerabilities. The experimental results prove the effectiveness and accuracy of the VDM-AEI model. Compared with the latest vulnerability detection model CGE, the accuracy rates of the 3 types of vulnerability detection are improved by 10.85%, 6.18%, and 12.34%, respectively. In addition, the predicted F1 scores of VDM-AEI are all greater than 95%, and the recall rate is no less than 94%.}
}


@article{DBLP:journals/tkde/ZhangNWL24,
	author = {Canyu Zhang and
                  Feiping Nie and
                  Rong Wang and
                  Xuelong Li},
	title = {Supervised Feature Selection via Multi-Center and Local Structure
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4930--4942},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3372657},
	doi = {10.1109/TKDE.2024.3372657},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangNWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature selection has achieved unprecedented success in obtaining sparse discriminative features. However, the existing methods almost use the \\ell _{2,p}-norm constraint on transformation matrix to obtain sparse features, which introduces extra parameters and cannot obtain the features directly. In addition, existing algorithms only focused on the global structure and ignored the local structure, leading to poor performance when solving data with non-Gaussian distributions which a single center point cannot describe precisely. Based on above considerations, we propose a supervised feature selection via multi-center and local structure learning. We further introduce trace ratio criterion into our model in favor of improving the discriminant of features selected. In order to address the overlap problem, we use multiple center points to match the distribution of data and construct a k-Nearest Neighbor graph to explore the local structure of the data. In addition, we also propose an efficient method to optimize the transformation matrix with the \\ell _{2,0}-norm constraint and can directly obtain the sparse features. We evaluate our method on Toy datasets and several real-world datasets, show improvement over state-of-the-art feature selection methods, and demonstrate the effectiveness of our model in dealing with non-Gaussian distributed data problems.}
}


@article{DBLP:journals/tkde/ZhaoCYGZZ24,
	author = {Yan Zhao and
                  Xuanlei Chen and
                  Guanyu Ye and
                  Fangda Guo and
                  Kai Zheng and
                  Xiaofang Zhou},
	title = {Task Allocation in Spatial Crowdsourcing: An Efficient Geographic
                  Partition Framework},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4943--4955},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3374086},
	doi = {10.1109/TKDE.2024.3374086},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoCYGZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed a revolution in Spatial Crowdsourcing (SC), in which people with mobile connectivity can perform spatio-temporal tasks that involve traveling to specified locations. In this paper, we identify and study in depth a new multi-center-based task allocation problem in the context of SC, where multiple allocation centers exist. In particular, we aim to maximize the total number of the allocated tasks while minimizing the allocated task number difference. To solve the problem, we propose a two-phase framework, called Task Allocation with Geographic Partition, consisting of a geographic partition and a task allocation phase. The first phase divides the whole study area based on the allocation centers by using both a basic Voronoi diagram-based algorithm and an adaptive weighted Voronoi diagram-based algorithm. In the allocation phase, we utilize a Reinforcement Learning method to achieve the task allocation, where a graph neural network with the attention mechanism is used to learn the embeddings of allocation centers, delivery points, and workers. To further improve the efficiency, we propose an early stopping optimization strategy for the adaptive weighted Voronoi diagram-based algorithm in the geographic partition phase and give a distance-constrained graph pruning strategy for the Reinforcement Learning method in the task allocation phase. Extensive experiments give insight into the effectiveness and efficiency of the proposed solutions.}
}


@article{DBLP:journals/tkde/LiYH24,
	author = {Yawen Li and
                  Zhizhi Yu and
                  Dongxiao He},
	title = {Text-Rich Graph Neural Networks With Subjective-Objective Semantic
                  Modeling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4956--4967},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3378914},
	doi = {10.1109/TKDE.2024.3378914},
	timestamp = {Mon, 17 Feb 2025 16:53:28 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiYH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs), which obtain node embeddings by attribute propagates along graph topology, exhibit significant power in graph-structured data mining. However, graphs in the real world are usually text-rich, where the text can not only be represented as node attributes, but also contains valuable objective semantic structures. Moreover, the graph topology also exhibits complex subjective semantic structures, especially the heterophily where nodes from different classes are prone to build connections, making existing GNNs that work under the assumption of homophily incapable to realize generalization. To tackle aforementioned limitations, we design a new text-rich graph neural network from a unified perspective, namely SO-GNN. It can effectively enhance the expressive power of GNNs by modeling the implicit but informative subjective-objective semantics underlying the text-rich graphs. Specifically, we first introduce a new constrained Markov matrix with well-defined probabilistic diffusion dynamics to guide information propagation, where the neighbors are more appropriate and indicative in providing both local and global subjective semantics. We then construct a flexible heterogeneous text graph to gain a deeper insight into objective semantics, providing indispensable information for learning node embedding. Finally, we unite subjective and objective semantics in an end-to-end manner, so that the model can fully utilize the most relevant information for downstream tasks. Extensive experiments across various text-rich graphs with low-to-high homophily demonstrate the effectiveness and flexibility of the proposed SO-GNN over state-of-the-arts.}
}


@article{DBLP:journals/tkde/ChenLHCWX24,
	author = {Jia{-}Yao Chen and
                  Shao{-}Yuan Li and
                  Sheng{-}Jun Huang and
                  Songcan Chen and
                  Lei Wang and
                  Ming{-}Kun Xie},
	title = {{UNM:} {A} Universal Approach for Noisy Multi-Label Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4968--4980},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3373500},
	doi = {10.1109/TKDE.2024.3373500},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenLHCWX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-label image classification relies on a large-scale, well-maintained dataset, which may easily be mislabeled due to various subjective reasons. Existing methods for coping with noise usually focus on improving the model robustness in the case of single-label noise. However, compared with noisy single-label learning, noisy multi-label learning is more practical and challenging. To reduce the negative impact of noisy multi-annotations, we propose a universal approach for noisy multi-label learning (UNM). In UNM, we propose the label-wise embedding network which investigates the semantic alignment between label embeddings and their corresponding output features to learn robust feature representations. Meanwhile, mining the co-occurrence of multi-labels is also added to regularize the noisy network predictions. We cyclically change the fitting status of our label-wise embedding network to distinguish the noisy samples and generate pseudo labels for them. As a result, UNM provides an effective way to exploit the label-wise features and semantic label embeddings in noisy scenarios. To verify the generalizability of our method, we also test our method on Partial Multi-label Learning (PML) and Multi-label Learning with Missing Labels (MLML). Extensive experiments on benchmark datasets including Microsoft COCO, Pascal VOC, and Visual Genome explicitly validate the proposed method.}
}


@article{DBLP:journals/tkde/LiuWYCH24,
	author = {Tong Liu and
                  Xulong Wang and
                  Po Yang and
                  Sheng Chen and
                  Chris J. Harris},
	title = {Unsupervised Transfer Aided Lifelong Regression for Learning New Tasks
                  Without Target Output},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4981--4995},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3372462},
	doi = {10.1109/TKDE.2024.3372462},
	timestamp = {Fri, 07 Feb 2025 13:01:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuWYCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an emerging learning paradigm, lifelong learning solves multiple consecutive tasks based upon previously accumulated knowledge. When facing with a new task, existing lifelong learning approaches need both input and desired output data to construct task models before knowledge transfer can succeed. However, labeling each task requires extensive labors and time, which can be prohibitive for real-world lifelong regression problems. To reduce this burden, we propose to incorporate unsupervised feature into lifelong regression via coupled dictionary learning, enabling to learn new tasks without target output data. Specifically, the input data for each task is encoded as unsupervised feature while both input and output data are used to construct task predictor. The unsupervised feature is linked with task predictor through two dictionaries that are coupled by a joint sparse representation. Because of the learned coupling between the two spaces, the task predictor for the new coming task can be recovered given only the input data. We further incorporate active task selection into this framework, enabling actively choosing tasks to learn in a task-efficient manner. Three case studies are used to evaluate the effectiveness of our method, in comparison with existing lifelong learning approaches. Results show that our method is able to accurately predict new tasks through unsupervised transfer, eliminating the need to label tasks before constructing the predictor.}
}


@article{DBLP:journals/tkde/ZhouYWDW24,
	author = {Yujia Zhou and
                  Jing Yao and
                  Ledell Wu and
                  Zhicheng Dou and
                  Ji{-}Rong Wen},
	title = {WebUltron: An Ultimate Retriever on Webpages Under the Model-Centric
                  Paradigm},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {9},
	pages = {4996--5006},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3332858},
	doi = {10.1109/TKDE.2023.3332858},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouYWDW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Document retrieval has been extensively studied within the index-retrieve framework for decades, which has withstood the test of time. However, this approach inherently segregates the indexing and retrieval processes, preventing a cohesive, end-to-end optimization. To bridge this divide, we introduce WebUltron, a revolutionary model-centric indexer for document retrieval. This system embeds the entirety of document knowledge within the model, striving for seamless end-to-end retrieval. Two primary challenges with this indexer are the representation of document identifiers (docids) and the model's training. Current methods grapple with docids that lack semantic depth and the constraints of limited supervised data, making scaling up to larger datasets challenging. Addressing this, we’ve engineered two novel docid types imbued with richer semantics that also streamline model inference. Further enhancing WebUltron's capabilities, we’ve developed a three-stage training regimen, leveraging deeper corpus insights and fortifying query-docid relationships. Experiments on two public datasets demonstrate the superiority of WebUltron over advanced baselines for document retrieval.}
}


@article{DBLP:journals/tkde/ChaiWYZCY24,
	author = {Di Chai and
                  Leye Wang and
                  Liu Yang and
                  Junxue Zhang and
                  Kai Chen and
                  Qiang Yang},
	title = {A Survey for Federated Learning Evaluations: Goals and Measures},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5007--5024},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3382002},
	doi = {10.1109/TKDE.2024.3382002},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChaiWYZCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.}
}


@article{DBLP:journals/tkde/WenLYW24,
	author = {Liangliang Wen and
                  Jiye Liang and
                  Kaixuan Yao and
                  Zhiqiang Wang},
	title = {Black-Box Adversarial Attack on Graph Neural Networks With Node Voting
                  Mechanism},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5025--5038},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3380750},
	doi = {10.1109/TKDE.2024.3380750},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WenLYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have attracted significant research interest in various graph data modeling tasks. To advance trustworthy, reliable, and safe Artificial Intelligence (AI) systems for practical applications, adversarial robustness learning on GNNs has drawn widespread attention among researchers. Numerous attack methods, including white-box attacks, gray-box attacks, and black-box attacks, have been proposed, but black-box attacks are widely considered to be the most challenging and practical in real-world applications. In this paper, we focus on the challenging and realistic black-box attack scenario on GNNs, where the attacker has no information about the structure and parameters of the target model. We first theoretically demonstrate that the loss changes of the GNNs are related to the node voting matrix, which is subject to the graph topology information and is independent to the structures of GNNs. Then, we propose a novel black-box attack strategy for GNNs based on the theoretical results, i.e., node voting influence-based GNNs black-box adversarial attack, named VoteAttack. Specifically, the VoteAttack algorithm iteratively chooses a group of significant nodes based on mutual voting among nodes (the node voting matrix) and considers the voting weights among nodes. Furthermore, the VoteAttack algorithm modifies the attributes of the selected nodes to create a perturbed graph and ultimately utilizes the perturbed graph to attack GNNs. Experimental results on popular GNNs and graph datasets indicate that the proposed attack strategy outperforms baseline strategies.}
}


@article{DBLP:journals/tkde/XuHSDLX24,
	author = {Yi Xu and
                  Liangzhe Han and
                  Leilei Sun and
                  Bowen Du and
                  Chuanren Liu and
                  Hui Xiong},
	title = {Bootstrapping on Continuous-Time Dynamic Graphs for Crowd Flow Modeling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5039--5052},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3383663},
	doi = {10.1109/TKDE.2024.3383663},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuHSDLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Numerous spatial-temporal learning methods have been proposed for crowd flow modeling, which is an important problem in Intelligent Transportation Systems (ITSs). However, most of the existing methods were designed to use data in one specific form to solve one particular task of crowd flow modeling and the shared patterns among different tasks have been largely ignored. In this paper, we investigate how to learn generic node representations that can simultaneously support various downstream tasks of crowd flow modeling. Along this line, we develop a continuous-time dynamic graph representation learning method based on Bootstrapping for Crowd Flow modeling (BootCF). Our approach follows a training procedure with two phases. In the pre-training phase, the continuous-time dynamic encoder converts edges with timestamps into messages to update the representations of the related traffic nodes. Inspired by the recent progress of contrastive learning, a bootstrapping framework for continuous-time dynamic graphs is designed to calculate pre-training loss and update the model in a self-supervised way, and thus enabling the node representation learning to be task-agnostic. Moreover, a context-aware data augmentation on continuous-time dynamic graphs is proposed to generate the augmented view of input data. Once the general node representations are obtained, the second phase can learn an effective model for any downstream task. Experiments on two real-world datasets show that our approach can achieve significant performance gain on four downstream tasks, which demonstrates that the proposed method has the powerful generalization capability for learning task-agnostic node representations.}
}


@article{DBLP:journals/tkde/KimKML24,
	author = {HyunGi Kim and
                  Siwon Kim and
                  Seonwoo Min and
                  Byunghan Lee},
	title = {Contrastive Time-Series Anomaly Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5053--5065},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3335317},
	doi = {10.1109/TKDE.2023.3335317},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KimKML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In addition to its success in representation learning, contrastive learning is effective in image anomaly detection. Although contrastive learning depends significantly on data augmentation methods, time-series data augmentation for time-series anomaly detection is not investigated sufficiently. Additionally, although time-series data share a temporal context, the existing contrastive loss contrasts temporally related samples, in which deteriorated anomaly detection performance is observed on time-series data. Herein, we propose contrastive multivariate time-series anomaly detection (CTAD), a multivariate time-series anomaly detection framework that addresses these challenges by incorporating a one-class learning scheme into the contrastive loss based on meticulously designed time-series data augmentations. Specifically, we propose seven types of general time-series data augmentations to be applied variable- and point-wise, and provide guidance on data augmentation methods for contrastive time-series anomaly detection. The superiority of the one-class contrastive loss and the appropriate selection of time-series data augmentation allow CTAD to achieve outstanding performance in multiple datasets, even using a simple long short-term memory network. Furthermore, CTAD is robust to noise as it trains a noise-invariant network. This enables up to 47× faster and 20× more memory-efficient anomaly detection performance compared with existing methods while affording robustness, which are essential considerations in real-world applications.}
}


@article{DBLP:journals/tkde/MuPZ24,
	author = {Xin Mu and
                  Ming Pang and
                  Feida Zhu},
	title = {Data Provenance via Differential Auditing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5066--5079},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3334821},
	doi = {10.1109/TKDE.2023.3334821},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MuPZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rising awareness of data assets, data governance, which is to understand where data comes from, how it is collected, and how it is used, has been assuming ever-growing importance. One critical component of data governance gaining increasing attention is auditing machine learning models to determine if specific data has been used for training. Existing auditing techniques, like shadow auditing methods, have shown feasibility under specific conditions such as having access to label information and knowledge of training protocols. However, these conditions are often not met in most real-world applications. In this paper, we introduce a practical framework for auditing data provenance based on a differential mechanism, i.e., after carefully designed transformation, perturbed input data from the target model's training set would result in much more drastic changes in the output than those from the model's non-training set. Our framework is data-dependent and does not require distinguishing training data from non-training data or training additional shadow models with labeled output data. Furthermore, our framework extends beyond point-based data auditing to group-based data auditing, aligning with the needs of real-world applications. Our theoretical analysis of the differential mechanism and the experimental results on real-world data sets verify the proposal's effectiveness.}
}


@article{DBLP:journals/tkde/JingSNLS24,
	author = {Peiguang Jing and
                  Haoyi Sun and
                  Liqiang Nie and
                  Yun Li and
                  Yuting Su},
	title = {Deep Multi-Modal Hashing With Semantic Enhancement for Multi-Label
                  Micro-Video Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5080--5091},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3337077},
	doi = {10.1109/TKDE.2023.3337077},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JingSNLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The pressing need for low storage and high efficiency has significantly propelled the advancement of deep hashing techniques in the realm of large-scale search and retrieval tasks. As one of the most prevailing forms of user-generated contents, micro-videos usually represent more complicated multi-modal behaviors that are further challenged in multi-label retrieval. Existing multi-modal hashing methods tend to prioritize the complementarity and consistency in multi-modal fusion, while neglecting the completeness problem. In this paper, we propose a deep multi-modal hashing with semantic enhancement (DMHSE) method that effectively integrates complete multi-modal representation learning with discriminative binary coding by means of collaboration between two distinct encoders, FoldCoder and HashCoder. FoldCoder translates latent multi-modal representation learning to a degradation process through mimicking data transmitting. Further, it incorporates a prompt learning paradigm to maximize the utilization of multi-label semantics for guiding representation learning. HashCoder combines pairwise and central constraints to ensure more discriminative hashing results. Pairwise constraint preserves the original local relevance structure, while central constraint tackles the problem of semantic ambiguity in multi-label data by leveraging the global label distribution. Experimental results demonstrate that DMHSE achieves superior performance in multi-label micro-video retrieval tasks.}
}


@article{DBLP:journals/tkde/LiSW24,
	author = {Lingli Li and
                  Wenjing Sun and
                  Baohua Wu},
	title = {DForest: {A} Minimal Dimensionality-Aware Indexing for High-Dimensional
                  Exact Similarity Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5092--5105},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3381111},
	doi = {10.1109/TKDE.2024.3381111},
	timestamp = {Sun, 06 Oct 2024 21:41:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of similarity search in high-dimensional space is a fundamental problem with numerous applications in computer science, yet it remains challenging due to the curse of dimensionality. This paper introduces DForest, a novel indexing approach designed to address this challenge for both range and kNN queries on high-dimensional data. Unlike previous similarity search approaches that apply a fixed dimensionality reduction to all objects uniformly, our approach determines the minimal dimensionality required for each object within a specified loss threshold and then reduces the dimensionality for each object individually. Furthermore, the query performance is also optimized by deriving the upper and lower bounds of retrieved blocks and computing distances in a low-embedding space preferentially. Theoretical analysis is provided to support our search strategy. Extensive experiments on a variety of datasets verify the superiority of DForest over the state-of-the-art methods.}
}


@article{DBLP:journals/tkde/HouDZL24,
	author = {Jingyi Hou and
                  Zhen Dong and
                  Jiayu Zhou and
                  Zhijie Liu},
	title = {Discovering Predictable Latent Factors for Time Series Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5106--5119},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3335240},
	doi = {10.1109/TKDE.2023.3335240},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HouDZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern temporal modeling methods, such as Transformer and its variants, have demonstrated remarkable capabilities in handling sequential data from specific domains like language and vision. Though achieving high performance with large-scale data, they often have redundant or unexplainable structures. When encountering some real-world datasets with limited observable variables that can be affected by many unknown factors, these methods may struggle to identify meaningful patterns and dependencies inherent in data, and thus, the modeling becomes unstable and unpredictable. To tackle this critical issue, in this article, we develop a novel algorithmic framework for inferring latent factors implied by the observed temporal data. The inferred factors are used to form multiple predictable and independent signal components that enable not only the reconstruction of future time series for accurate prediction but also sparse relation reasoning for long-term efficiency. To achieve this, we introduce three characteristics, i.e., predictability, sufficiency, and identifiability, and model these characteristics of latent factors via powerful deep latent dynamics models to infer the predictable signal components. Empirical results on multiple real datasets show the efficiency of our method for different kinds of time series forecasting tasks. Statistical analyses validate the predictability and interpretability of the learned latent factors.}
}


@article{DBLP:journals/tkde/ShenYZZDW24,
	author = {Zhihao Shen and
                  Kang Yang and
                  Xi Zhao and
                  Jianhua Zou and
                  Wan Du and
                  Junjie Wu},
	title = {{DMM:} {A} Deep Reinforcement Learning Based Map Matching Framework
                  for Cellular Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5120--5137},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3383881},
	doi = {10.1109/TKDE.2024.3383881},
	timestamp = {Fri, 07 Feb 2025 10:06:02 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ShenYZZDW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a novel map matching framework that adopts deep learning techniques to map a sequence of cell tower locations to a trajectory on a road network. Map matching is an essential pre-processing step for many applications, such as traffic optimization and human mobility analysis. However, most recent approaches are based on hidden Markov models (HMMs) or neural networks that are hard to consider high-order location information or heuristics observed from real driving scenarios. In this paper, we develop a deep reinforcement learning based map matching framework for cellular data, named as DMM, which adopts a recurrent neural network (RNN) coupled with a reinforcement learning scheme to identify the most-likely trajectory of roads given a sequence of cell towers. To transform DMM into a practical system, several challenges are addressed by developing a set of techniques, including spatial-aware representation of input cell tower sequences, an encoder-decoder based RNN network for map matching model with variable-length input and output, and a global heuristics-driven reinforcement learning based scheme for optimizing the parameters of the encoder-decoder map matching model. Extensive experiments on a large-scale anonymized cellular dataset reveal that DMM provides high map matching accuracy and fast inference time.}
}


@article{DBLP:journals/tkde/LiuWLCL24,
	author = {Yuhan Liu and
                  Tianhao Wang and
                  Yixuan Liu and
                  Hong Chen and
                  Cuiping Li},
	title = {Edge-Protected Triangle Count Estimation Under Relationship Local
                  Differential Privacy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5138--5152},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3381832},
	doi = {10.1109/TKDE.2024.3381832},
	timestamp = {Tue, 01 Oct 2024 22:42:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuWLCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Triangle count estimation is a fundamental task in federated graph analysis. Yet, directly collecting local counts from users exposes individuals to severe privacy risks, as the local reports may reveal sensitive social connections. Protecting edge privacy in triangle count estimation is extremely challenging due to the strong data correlation among distinct users and large data sensitivity. Though many efforts have been put into addressing this issue, the existing works fail to provide a stringent privacy guarantee as well as a promising data utility. Motivated by this, we first propose an enhanced privacy notion namely Edge Relationship Local Differential Privacy (Edge-RLDP) that formally considers data correlations and provides a stringent privacy guarantee by hiding multiple edges in the global graph. Based on Edge-RLDP, we further propose a PRIvacy-preserved federated Estimator for Triangle count (Privet) with three perturbation algorithms, which enhances the estimation accuracy by designing specialized noise calibration schemes and leveraging a triangle-subsample trick. Theoretically, we prove that Privet achieves (\\varepsilon,\\delta)-Edge-RLDP. Empirically, we verify that Privet provides promising estimation accuracy in terms of mean relative error.}
}


@article{DBLP:journals/tkde/ShaikTLXCZL24,
	author = {Thanveer Shaik and
                  Xiaohui Tao and
                  Lin Li and
                  Haoran Xie and
                  Taotao Cai and
                  Xiaofeng Zhu and
                  Qing Li},
	title = {{FRAMU:} Attention-Based Machine Unlearning Using Federated Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5153--5167},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3382726},
	doi = {10.1109/TKDE.2024.3382726},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShaikTLXCZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine Unlearning, a pivotal field addressing data privacy in machine learning, necessitates efficient methods for the removal of private or irrelevant data. In this context, significant challenges arise, particularly in maintaining privacy and ensuring model efficiency when managing outdated, private, and irrelevant data. Such data not only compromises model accuracy but also burdens computational efficiency in both learning and unlearning processes. To mitigate these challenges, we introduce a novel framework: Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strengths include its adaptability in fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution without compromising privacy. Our experiments, conducted on both single-modality and multi-modality datasets, revealed that FRAMU significantly outperformed baseline models. Additional assessments of convergence behavior and optimization strategies further validate the framework's utility in federated learning applications. Overall, FRAMU advances Machine Unlearning by offering a robust, privacy-preserving solution that optimizes model performance while also addressing key challenges in dynamic data environments.}
}


@article{DBLP:journals/tkde/HeDYC24,
	author = {Guoliang He and
                  Lifang Dai and
                  Zhiwen Yu and
                  C. L. Philip Chen},
	title = {GAN-Based Temporal Association Rule Mining on Multivariate Time Series
                  Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5168--5180},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3335049},
	doi = {10.1109/TKDE.2023.3335049},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HeDYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature mining is a challenging work in the field of multivariate time series (MTS) data mining. Traditional methods suffer from three major issues. 1) Learned shapelets may seriously diverge from original subsequences since learning methods do not restrain the learned ones similar to raw sequences, which reduces interpretability. 2) Existing rule mining methods just generate association rules based on feature combination of different variables without considering temporal relations among features, which could not adequately express the essential characteristics of MTS data. 3) Most deep learning methods only mine global and high-level features of MTS data, which affects interpretability. To address these issues, we propose a temporal association rule mining method based on Generative Adversarial Network (GAN) called TAR-GAN. First, a shapelet mining method based on GAN (SGAN) is advanced to discover dataset-level and sample-level shapelets of all variables in MTS data. Second, a Temporal Graph based Rule Mining method (TGRM) is introduced to discover temporal association rules based on the temporal relationships among shapelets of different variables. Meanwhile, a Fast Convolution-based Similarity Measure methods (FCSM) is introduced to measure the similarity between MTS samples and temporal association rules. Furthermore, an adversarial training strategy is introduced to ensure the effectiveness and stability of generated temporal association rules, which could reflect the essential characteristics of MTS data. Extensive experiments on 12 datasets show the effectiveness and efficiency of our method.}
}


@article{DBLP:journals/tkde/YuGLZ24,
	author = {Jianxiang Yu and
                  Qingqing Ge and
                  Xiang Li and
                  Aoying Zhou},
	title = {Heterogeneous Graph Contrastive Learning With Meta-Path Contexts and
                  Adaptively Weighted Negative Samples},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5181--5193},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3377431},
	doi = {10.1109/TKDE.2024.3377431},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuGLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous graph contrastive learning has received wide attention recently. Some existing methods use meta-paths, which are sequences of object types that capture semantic relationships between objects, to construct contrastive views. However, most of them ignore the rich meta-path context information that describes how two objects are connected by meta-paths. Further, they fail to distinguish negative samples, which could adversely affect the model performance. To address the problems, we propose MEOW, which considers both meta-path contexts and weighted negative samples. Specifically, MEOW constructs a coarse view and a fine-grained view for contrast. The former reflects which objects are connected by meta-paths, while the latter uses meta-path contexts and characterizes details on how the objects are connected. Then, we theoretically analyze the InfoNCE loss and recognize its limitations for computing gradients of negative samples. To better distinguish negative samples, we learn hard-valued weights for them based on node clustering and use prototypical contrastive learning to pull close embeddings of nodes in the same cluster. In addition, we propose a variant model AdaMEOW that adaptively learns soft-valued weights of negative samples to further improve node representation. Finally, we conduct extensive experiments to show the superiority of MEOW and AdaMEOW against other state-of-the-art methods.}
}


@article{DBLP:journals/tkde/XuYC24,
	author = {Yuhong Xu and
                  Zhiwen Yu and
                  C. L. Philip Chen},
	title = {Improved Contraction-Expansion Subspace Ensemble for High-Dimensional
                  Imbalanced Data Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5194--5205},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3384274},
	doi = {10.1109/TKDE.2024.3384274},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Imbalanced data biases the classifier towards the majority class. Accompanied with high-dimensional characteristics, classification performance is further degraded. Existing researches for skewed data mainly involve resampling, cost-sensitive learning, and classifier ensemble. However, these approaches have some limitations: 1) resampling suffers from noisy and redundant features in high-dimensional skewed data; 2) cost-sensitive learning is hard to construct an optimal cost matrix for sample misclassification; 3) ensemble with random feature subspace easily leads to information loss; 4) ensemble with sample subspace on small-size data easily leads to insufficient description of sample space and suffers from negative impacts of high-dimensional data. This paper proposes an improved contraction-expansion subspace ensemble (ICESE) for high-dimensional imbalanced data classification. First, a contraction-expansion subspace optimization (CESO) is designed to perform subspace selection and transformation, which is beneficial for enhancing the discrimination and diversity of subspace. Then, to strengthen classification capabilities, a CESO-based multilayer optimization structure is developed to construct the improved subspace. Finally, to mitigate the effects of skewed data, ICESE performs a resampling scheme on the improved subspace for constructing a rebalanced subset to base classifier. Experimental results on 24 high-dimensional imbalanced data sets demonstrate that our ICESE outperforms different mainstream ensemble systems in terms of F-score and G-mean.}
}


@article{DBLP:journals/tkde/ZhouWLYY24,
	author = {Menghui Zhou and
                  Xulong Wang and
                  Tong Liu and
                  Yun Yang and
                  Po Yang},
	title = {Integrating Visualised Automatic Temporal Relation Graph into Multi-Task
                  Learning for Alzheimer's Disease Progression Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5206--5220},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3385712},
	doi = {10.1109/TKDE.2024.3385712},
	timestamp = {Fri, 07 Feb 2025 13:01:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouWLYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Alzheimer's disease (AD), the most prevalent dementia, gradually reduces the cognitive abilities of patients while also posing a significant financial burden on the healthcare system. A variety of multi-task learning methods have recently been proposed in order to identify potential MRI-related biomarkers and accurately predict the progression of AD. These methods, however, all use a predefined task relation structure that is rigid and insufficient to adequately capture the intricate temporal relations among tasks. Instead, we propose a novel mechanism for directly and automatically learning the temporal relation and constructing it as an Automatic Temporal relation Graph (AutoTG). We use the sparse group Lasso to select a universal MRI feature set for all tasks and particular sets for various tasks in order to find biomarkers that are useful for predicting the progression of AD. To solve the biconvex and non-smooth objective function, we adopt the alternating optimization and show that the two related sub-optimization problems are amenable to closed-form solution of the proximal operator. To solve the two problems efficiently, the accelerated proximal gradient method is used, which has the fastest convergence rate of any first-order method. We have preprocessed three latest AD datasets, and the experimental results verify our proposed novel multi-task approach outperforms several baseline methods. To demonstrate the high interpretability of our approach, we visualise the automatically learned temporal relation graph and investigate the temporal patterns of the important MRI features.}
}


@article{DBLP:journals/tkde/YangYGL24,
	author = {Kang Yang and
                  Ruiyun Yu and
                  Bingyang Guo and
                  Jie Li},
	title = {Interaction Subgraph Sequential Topology-Aware Network for Transferable
                  Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5221--5233},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3384965},
	doi = {10.1109/TKDE.2024.3384965},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangYGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation systems have primarily been limited to research on a single dataset compared to natural language processing and computer vision, which have seen tremendous growth in transferable tasks. Existing approaches for recommendation systems need to be more scalable to arbitrary tasks, given that previous research efforts on transferable recommendations have only yielded brief explorations and neglected systematic studies of sequential tasks. In this regard, we propose the interaction subgraph sequential topology-aware network (ISTN), which overcomes this limitation, enabling transferable sequence recommendations. ISTN performs subgraph sampling and node labeling of user interactions, captures the topological features of the user interaction sequences with the sequential topology auto-encoder, and employs the sequential preference decoupling module to decouple user interaction sequences for transferable adaptive granularity modeling of user preferences. ISTN requires no fine-tuning, and its knowledge transfer capability from the training dataset to the new dataset delivers accurate, individualized recommendation results. ISTN outperforms state-of-the-art performance in transferable contexts with only minor performance degradation compared to the traditional baseline, as shown in Yelp, MovieLens, and Foursquare experiments.}
}


@article{DBLP:journals/tkde/YeLHLS24,
	author = {Chang Ye and
                  Yuchen Li and
                  Bingsheng He and
                  Zhao Li and
                  Jianling Sun},
	title = {Large-Scale Graph Label Propagation on GPUs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5234--5248},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3336329},
	doi = {10.1109/TKDE.2023.3336329},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YeLHLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph label propagation (LP) is a core component in many downstream applications such as fraud detection, recommendation and image segmentation. In this paper, we propose GLP, a GPU-based framework to enable efficient LP processing on large-scale graphs. By investigating the data processing pipeline in a large e-commerce platform, we have identified two key challenges on integrating GPU-accelerated LP processing to the pipeline: (1) programmability for evolving application logics; (2) demand for real-time performance. Motivated by these challenges, we offer a set of expressive APIs that data engineers can customize and deploy efficient LP algorithms on GPUs with ease. To achieve better performance, we propose novel GPU-centric optimizations by leveraging the community as well as power-law properties of large graphs. Further, we significantly reduce the expensive data transfer cost between CPUs and GPUs by enabling LP processing on compressed graphs. Extensive experiments have confirmed the effectiveness of our proposed approaches over the state-of-the-art GPU methods. Furthermore, our proposed solution supports a real billion-scale graph workload for fraud detection and achieves 13.2x speedup to the current in-house solution running on a high-end multicore machine with compressed graphs.}
}


@article{DBLP:journals/tkde/ZhangXWCLWJ24,
	author = {Shijie Zhang and
                  Jiang Xiao and
                  Enping Wu and
                  Feng Cheng and
                  Bo Li and
                  Wei Wang and
                  Hai Jin},
	title = {MorphDAG: {A} Workload-Aware Elastic DAG-Based Blockchain},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5249--5264},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3382743},
	doi = {10.1109/TKDE.2024.3382743},
	timestamp = {Mon, 09 Dec 2024 22:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangXWCLWJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Directed Acyclic Graph (DAG)-based blockchain represents a paradigm shift from conventional blockchains, which has the potential to drastically improve throughput performance through concurrent storage and executions. In practice, however, existing DAG-based blockchains fail to deliver such promises, often with limited throughput, high conflicts, and security vulnerabilities under dynamic workloads. The root causes are their unawareness of the workload characteristics of different workload sizes and skewed access patterns. In this article, we propose MorphDAG, the first workload-aware DAG-based blockchain that can significantly enhance throughput without compromising security and achieve elastic scaling under realistic workloads. We derive the theoretically optimal degree of storage concurrency to achieve high throughput while retaining system security as the workload size changes, while enabling fine-grained concurrency adjustment that accommodates a Proof-of-Stake (PoS)-based consensus protocol. We develop a dual-mode transaction processing mechanism that effectively resolves the conflicts brought by skewed access. We implement a prototype of MorphDAG and evaluate under real-world workloads. Extensive evaluations demonstrate that MorphDAG improves end-to-end throughput by up to 2.3× and 2.4× over state-of-the-art DAG-based blockchain systems AdaptChain and OHIE, respectively.}
}


@article{DBLP:journals/tkde/LiSWHCY24,
	author = {Shuxian Li and
                  Liyan Song and
                  Xiaoyu Wu and
                  Zheng Hu and
                  Yiu{-}ming Cheung and
                  Xin Yao},
	title = {Multi-Class Imbalance Classification Based on Data Distribution and
                  Adaptive Weights},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5265--5279},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3384961},
	doi = {10.1109/TKDE.2024.3384961},
	timestamp = {Mon, 09 Dec 2024 22:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiSWHCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {AdaBoost approaches have been used for multi-class imbalance classification with an imbalance ratio measured on class sizes. However, such ratio would assign each training sample of the same class with the same weight, thus failing to reflect the data distribution within a class. We propose to incorporate the density information of training samples into the class imbalance ratio so that samples of the same class could have different weights. As one could use the entire training set to calculate the imbalance and density factors, the weight of a training sample resulting from the two factors remains static throughout the training epochs. However, static weights could not reflect the up-to-date training status of base learners. To deal with this, we propose to design an adaptive weighting mechanism by making use of up-to-date training status to further alleviate the multi-class imbalance issue. Ultimately, we incorporate the class imbalance ratio, the density-based factor, and the adaptive weighting mechanism into a single variable, based on which the adaptive weights of all training samples are computed. Experimental studies are carried out to investigate the effectiveness of the proposed approach and each of the three components in dealing with multi-class imbalance classification problem.}
}


@article{DBLP:journals/tkde/LiCXRHS24,
	author = {Xingyi Li and
                  Xiang Cheng and
                  Min Xia and
                  Qiyu Ren and
                  Zhaofeng He and
                  Sen Su},
	title = {Multi-Passage Machine Reading Comprehension Through Multi-Task Learning
                  and Dual Verification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5280--5293},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3383103},
	doi = {10.1109/TKDE.2024.3383103},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiCXRHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-passage machine reading comprehension (MRC) aims to answer a question by multiple passages. Existing multi-passage MRC approaches have shown that employing passages with and without golden answers (i.e., labeled and unlabeled passages) for model training can improve prediction accuracy. However, when using the unlabeled passages, they either incur the wrong labeling problem or treat the labeled and unlabeled passages equally. In addition, they ignore the original passage information to verify the correctness of the answer. In this paper, we present MLDV-MRC, a novel approach for multi-passage MRC via Multi-task Learning and Dual Verification. MLDV-MRC adopts the extract-then-select framework, where an extractor is first used to predict answer candidates, then a selector is used to choose the final answer. For the extractor, we adopt multi-task learning with generative adversarial training to train it by using both labeled and unlabeled passages. To train the extractor by backpropagation, we propose a hybrid method which combines boundary-based and content-based extracting methods to produce the answer candidate set and its representation. For the selector, we propose to leverage both the information from answer candidates and original passages to verify the final answer. In particular, we propose a global-local memory-augmented neural network to build the representations of original passages, which fuses the passage-level information and word-level information. The experimental results on three open-domain QA datasets confirm the effectiveness of our approach.}
}


@article{DBLP:journals/tkde/WuZGM24,
	author = {Wenming Wu and
                  Wensheng Zhang and
                  Maoguo Gong and
                  Xiaoke Ma},
	title = {Noised Multi-Layer Networks Clustering With Graph Denoising and Structure
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5294--5307},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3335223},
	doi = {10.1109/TKDE.2023.3335223},
	timestamp = {Wed, 11 Dec 2024 17:20:53 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuZGM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-layer networks treat various types of interactions at each level to model complex systems in nature and society, and clustering of them is of great significance for revealing mechanisms of systems. Vast majority of current algorithms focus on identifying the common communities in clear multi-layer networks, and few attempt has been devoted to the detection of layer-specific communities in noised ones. To address these issues, a joint learning algorithm with Graph Denoising and Structure Learning (called GDSL) for the detection of layer-specific communities in noised multi-layer networks is proposed, which simultaneously integrates graph denoising, structure learning, and module detection. To remove noise of networks, GDSL re-constructs affinity graphs for the original ones by preserving community structure. To enhance robustness and discriminative of features, GDSL explores the relations of features among various layers with the Hilbert-Schmidt Independence Criterion and structure learning. Finally, GDSL joins all these procedures with an objective function, and deduces optimization rules. The results show that GDSL not only significantly outperforms baselines but also enhances the robustness of the algorithm, providing an effective model for community detection in noised multi-layer networks.}
}


@article{DBLP:journals/tkde/PappachanZHM24,
	author = {Primal Pappachan and
                  Shufan Zhang and
                  Xi He and
                  Sharad Mehrotra},
	title = {Preventing Inferences Through Data Dependencies on Sensitive Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5308--5327},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3336630},
	doi = {10.1109/TKDE.2023.3336630},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PappachanZHM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Simply restricting the computation to non-sensitive part of the data may lead to inferences on sensitive data through data dependencies. Prior work on preventing inference control through data dependencies detect and deny queries which may lead to leakage, or only protect against exact reconstruction of the sensitive data. These solutions result in poor utility, and poor security respectively. In this paper, we present a novel security model called full deniability. Under this stronger security model, any information inferred about sensitive data from non-sensitive data is considered as a leakage. We describe algorithms for efficiently implementing full deniability on a given database instance with a set of data dependencies and sensitive cells. Using experiments on two different datasets, we demonstrate that our approach protects against realistic adversaries while hiding only minimal number of additional non-sensitive cells and scales well with database size and sensitive data.}
}


@article{DBLP:journals/tkde/ZhuWQW24,
	author = {Yi Zhu and
                  Ye Wang and
                  Jipeng Qiang and
                  Xindong Wu},
	title = {Prompt-Learning for Short Text Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5328--5339},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3332787},
	doi = {10.1109/TKDE.2023.3332787},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuWQW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the short text, the extremely short length, feature sparsity, and high ambiguity pose huge challenges to classification tasks. Recently, as an effective method for tuning Pre-trained Language Models for specific downstream tasks, prompt-learning has attracted a vast amount of attention and research. The main intuition behind the prompt-learning is to insert the template into the input and convert the tasks into equivalent cloze-style tasks. However, most prompt-learning methods only consider the class name and monotonous strategy for knowledge incorporating in cloze-style prediction, which will inevitably incur omissions and bias in short text classification tasks. In this paper, we propose a short text classification method with prompt-learning. Specifically, the top M\nconcepts related to the entity in the short text are retrieved from the open Knowledge Graph like Probase, these concepts are first selected by the distance with class labels, which takes both the short text itself and the class name into consideration during expanding label word space. Then, we conducted four additional strategies for the integration of the expanded concepts, and the union of these concepts are adopted finally in the verbalizer of prompt-learning. Experimental results show that the obvious improvement is obtained compared with other state-of-the-art methods on five well-known datasets.}
}


@article{DBLP:journals/tkde/TuLZPMLLCH24,
	author = {Wenxuan Tu and
                  Qing Liao and
                  Sihang Zhou and
                  Xin Peng and
                  Chuan Ma and
                  Zhe Liu and
                  Xinwang Liu and
                  Zhiping Cai and
                  Kunlun He},
	title = {{RARE:} Robust Masked Graph Autoencoder},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5340--5353},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3335222},
	doi = {10.1109/TKDE.2023.3335222},
	timestamp = {Thu, 14 Nov 2024 14:45:54 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/TuLZPMLLCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Masked graph autoencoder (MGAE) has emerged as a promising self-supervised graph pre-training (SGP) paradigm due to its simplicity and effectiveness. However, existing efforts perform the mask-then-reconstruct operation in the raw data space as is done in computer vision (CV) and natural language processing (NLP) areas, while neglecting the important non-Euclidean property of graph data. As a result, the highly unstable local structures largely increase the uncertainty in inferring masked data and decrease the reliability of the exploited self-supervision signals, leading to inferior representations for downstream evaluations. To address this issue, we propose a novel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the certainty in inferring masked data and the reliability of the self-supervision mechanism by further masking and reconstructing node samples in the high-order latent feature space. Through both theoretical and empirical analyses, we have discovered that performing a joint mask-then-reconstruct strategy in both latent feature and raw data spaces could yield improved stability and performance. To this end, we elaborately design a masked latent feature completion scheme, which predicts latent features of masked nodes under the guidance of high-order sample correlations that are hard to be observed from the raw data perspective. Specifically, we first adopt a latent feature predictor to predict the masked latent features from the visible ones. Next, we encode the raw data of masked samples with a momentum graph encoder and subsequently employ the resulting representations to improve the predicted results through latent feature matching. Extensive experiments on seventeen datasets have demonstrated the effectiveness and robustness of RARE against state-of-the-art (SOTA) competitors across three downstream tasks.}
}


@article{DBLP:journals/tkde/WuZMLHML24,
	author = {Haolun Wu and
                  Yansen Zhang and
                  Chen Ma and
                  Fuyuan Lyu and
                  Bowei He and
                  Bhaskar Mitra and
                  Xue Liu},
	title = {Result Diversification in Search and Recommendation: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5354--5373},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3382262},
	doi = {10.1109/TKDE.2024.3382262},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuZMLHML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Diversifying return results is an important research topic in retrieval systems in order to satisfy both the various interests of customers and the equal market exposure of providers. There has been growing attention on diversity-aware research during recent years, accompanied by a proliferation of literature on methods to promote diversity in search and recommendation. However, diversity-aware studies in retrieval systems lack a systematic organization and are rather fragmented. In this survey, we are the first to propose a unified taxonomy for classifying the metrics and approaches of diversification in both search and recommendation, which are two of the most extensively researched fields of retrieval systems. We begin the survey with a brief discussion of why diversity is important in retrieval systems, followed by a summary of the various diversity concerns in search and recommendation, highlighting their relationship and differences. For the survey's main body, we present a unified taxonomy of diversification metrics and approaches in retrieval systems, from both the search and recommendation perspectives. In the later part of the survey, we discuss the open research questions of diversity-aware research in search and recommendation in an effort to inspire future innovations and encourage the implementation of diversity in real-world systems.}
}


@article{DBLP:journals/tkde/JiangYXW24,
	author = {Yiheng Jiang and
                  Yongjian Yang and
                  Yuanbo Xu and
                  En Wang},
	title = {Spatial-Temporal Interval Aware Individual Future Trajectory Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5374--5387},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3332929},
	doi = {10.1109/TKDE.2023.3332929},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiangYXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we present STiSAN^+ as an end-to-end mobility trajectory prediction framework. It is a Self-Attention Network (SAN)-based model augmented with two lightweight approaches: Rotary Time Aware Position Encoder (RoTAPE) and multi-head Interval Aware Attention Block (IAAB). These methods allow for the explicit, efficient and effective processing of spatial-temporal intervals among the user's historical trajectory, without the need for additional parameters or a substantial computational burden. On the one hand, RoTAPE simultaneously encodes day- and hour-level timestamps into the sequence representation via a sinusoidal encoding matrix. Notably, the multi-level temporal differences operate in a mutually independent manner to reflect the periodical pattern, and collectively measure the absolute time interval. On the other hand, IAAB, point-wise injecting the historical spatial-temporal intervals into the attention map, can promote SAN attaching importance to the spatial relations under the constraints of time conditions. Moreover, we equip STiSAN^+ with a novel MLP-based module, namely Spatial-Temporal Relation Memory (STR Memory). It endows the interactions inside historical intervals along different directions, and converts them into spatial-temporal relations in future trajectories for accurate predictions. The empirical study on six public LBSN shows that from Next Location Recommendation to Multi-location Future Trajectory Prediction, our STiSAN^+ gains average 15.05% and 18.35% improvements against several state-of-the-art sequential models, respectively. We demonstrate the effectiveness of proposed module with an ablation study, and validate the extensibility and interpretability of RoTAPE and IAAB through non-sampled metric evaluation and visualization.}
}


@article{DBLP:journals/tkde/JinLFSHZZ24,
	author = {Guangyin Jin and
                  Yuxuan Liang and
                  Yuchen Fang and
                  Zezhi Shao and
                  Jincai Huang and
                  Junbo Zhang and
                  Yu Zheng},
	title = {Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban
                  Computing: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5388--5408},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3333824},
	doi = {10.1109/TKDE.2023.3333824},
	timestamp = {Fri, 15 Nov 2024 15:28:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/JinLFSHZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With recent advances in sensing technologies, a myriad of spatio-temporal data has been generated and recorded in smart cities. Forecasting the evolution patterns of spatio-temporal data is an important yet demanding aspect of urban computing, which can enhance intelligent management decisions in various fields, including transportation, environment, climate, public safety, healthcare, and others. Traditional statistical and deep learning methods struggle to capture complex correlations in urban spatio-temporal data. To this end, Spatio-Temporal Graph Neural Networks (STGNN) have been proposed, achieving great promise in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. In this manuscript, we provide a comprehensive survey on recent progress on STGNN technologies for predictive learning in urban computing. Firstly, we provide a brief introduction to the construction methods of spatio-temporal graph data and the prevalent deep-learning architectures used in STGNNs. We then sort out the primary application domains and specific predictive learning tasks based on existing literature. Afterward, we scrutinize the design of STGNNs and their combination with some advanced technologies in recent years. Finally, we conclude the limitations of existing research and suggest potential directions for future work.}
}


@article{DBLP:journals/tkde/LiangJZZZ24,
	author = {Xin Liang and
                  Yanli Ji and
                  Wei{-}Shi Zheng and
                  Wangmeng Zuo and
                  Xiaofeng Zhu},
	title = {SV-Learner: Support-Vector Contrastive Learning for Robust Learning
                  With Noisy Labels},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5409--5422},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3386829},
	doi = {10.1109/TKDE.2024.3386829},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiangJZZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Noisy-label data inevitably gives rise to confusion in various perception applications. In this work, we revisit the theory of support vector machines (SVMs) which mines support vectors to build the maximum-margin hyperplane for robust classification, and propose a robust-to-noise deep learning framework, SV-Learner, including the Support Vector Contrastive Learning (SVCL) and Support Vector-based Noise Screening (SVNS). The SV-Learner mines support vectors to solve the learning problem with noisy labels (LNL) reliably. SVCL adopts support vectors as positive and negative samples, driving robust contrastive learning to enlarge the feature distribution margin for learning convergent feature distributions. SVNS uses support vectors with valid labels to assist in screening noisy ones from confusable samples for reliable clean-noisy sample screening. Finally, Semi-Supervised classification is performed to realize the recognition of noisy samples. Extensive experiments are evaluated on CIFAR-10, CIFAR-100, Clothing1M, and Webvision datasets, and results demonstrate the effectiveness of our proposed approach.}
}


@article{DBLP:journals/tkde/JiangZLDXLZL24,
	author = {Zigui Jiang and
                  Weilin Zheng and
                  Bo Liu and
                  Hong{-}Ning Dai and
                  Haoran Xie and
                  Xiapu Luo and
                  Zibin Zheng and
                  Qing Li},
	title = {Unravelling Token Ecosystem of {EOSIO} Blockchain},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5423--5439},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3378381},
	doi = {10.1109/TKDE.2024.3378381},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiangZLDXLZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Being the largest Initial Coin Offering project, EOSIO has attracted great interest in cryptocurrency markets. Despite its popularity and prosperity (e.g., 26,311,585,008 token transactions occurred from June 8, 2018 to Aug. 5, 2020), there is almost no work investigating the EOSIO token ecosystem. To fill this gap, we are the first to conduct a systematic investigation of the EOSIO token ecosystem by conducting a comprehensive graph analysis of the entire on-chain EOSIO data (nearly 135 million blocks). We construct token-creator graphs, token-contract creator graphs, token-holder graphs, and token-transfer graphs to characterize token creators, holders, and transfer activities. Through graph analysis, we have obtained many insightful findings and observed some abnormal trading patterns. Moreover, we propose a fake-token detection algorithm to identify tokens generated by fake users or fake transactions and analyze their corresponding manipulation behaviors. Evaluation results also demonstrate the effectiveness of our algorithm.}
}


@article{DBLP:journals/tkde/KuangWWLD24,
	author = {Weirui Kuang and
                  Zhen Wang and
                  Zhewei Wei and
                  Yaliang Li and
                  Bolin Ding},
	title = {When Transformer Meets Large Graphs: An Expressive and Efficient Two-View
                  Architecture},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {10},
	pages = {5440--5452},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3381125},
	doi = {10.1109/TKDE.2024.3381125},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KuangWWLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The successes of applying Transformer to graphs have been witnessed on small graphs (e.g., molecular graphs), yet two barriers prevent its adoption on large graphs (e.g., citation networks). First, despite the benefit of the global receptive field, enormous distant nodes might distract the necessary attention of each target node from its neighborhood. Second, training a Transformer model on large graphs is costly due to the node-to-node attention mechanism's quadratic computational complexity. To break down these barriers, we propose a two-view architecture Coarformer, wherein a GNN-based module captures fine-grained local information from the original graph, and a Transformer-based module captures coarse yet long-range information on the coarse graph. We further design a cross-view propagation scheme so that these two views can enhance each other. Our graph isomorphism analysis shows the complementary natures of GNN and Transformer, justifying the motivation and design of Coarformer. We conduct extensive experiments on real-world datasets, where Coarformer surpasses any single-view method that solely applies a GNN or Transformer. As an ablation, Coarformer outperforms straightforward combinations of a GNN model and a Transformer-based model, verifying the effectiveness of our coarse global view and the cross-view propagation scheme. Meanwhile, Coarformer consumes the least runtime and GPU memory than those combinations.}
}


@article{DBLP:journals/tkde/YueLJWA24,
	author = {Linan Yue and
                  Qi Liu and
                  Binbin Jin and
                  Han Wu and
                  Yanqing An},
	title = {A Circumstance-Aware Neural Framework for Explainable Legal Judgment
                  Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5453--5467},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3387580},
	doi = {10.1109/TKDE.2024.3387580},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YueLJWA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive legal documents have promoted the application of legal intelligence. Among them, Legal Judgment Prediction (LJP) has emerged as a critical task, garnering significant attention. LJP aims to predict judgment results for multiple subtasks, including charges, law articles, and terms of penalty. Existing studies primarily focus on utilizing the entire factual description to produce judgment results, overlooking the practical judicial scenario where judges consider various crime circumstances to decide verdicts and sentencing. To this end, in this article, we propose a circumstance-aware LJP framework (i.e., NeurJudge) by exploring the circumstances of crime. Specifically, NeurJudge first separates the factual description into different circumstances with the predicted results of intermediate subtasks and then employs them to yield results of other subtasks. Besides, as confusing verdicts may degrade the performance of LJP, we further develop a variant of NeurJudge (NeurJudge+) that incorporates the semantics of labels (charges and law articles) into facts to yield more expressive and distinguishable fact representations. Finally, to provide explanations for LJP, we extend NeurJudge to an explainable LJP framework E-NeurJudge with a cooperative teacher-student system. The teacher system is NeurJudge which exploits legal particularities well but lacks explanation capability. The student system is a rationalization method that provides explainability but fails to utilize legal particularities. To combine the advantages of the above methods, we use a transferring function to transfer legal particularities from the teacher to the student, making a trade-off between yielding LJP results and rendering them explainable. Extensive experimental results on real-world datasets validate the effectiveness of our proposed frameworks.}
}


@article{DBLP:journals/tkde/ZhangXWLG24,
	author = {Hanlei Zhang and
                  Hua Xu and
                  Xin Wang and
                  Fei Long and
                  Kai Gao},
	title = {A Clustering Framework for Unsupervised and Semi-Supervised New Intent
                  Discovery},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5468--5481},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3340732},
	doi = {10.1109/TKDE.2023.3340732},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangXWLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {New intent discovery is of great value to natural language processing, allowing for a better understanding of user needs and providing friendly services. However, most existing methods struggle to capture the complicated semantics of discrete text representations when limited or no prior knowledge of labeled data is available. To tackle this problem, we propose a novel clustering framework, USNID, for unsupervised and semi-supervised new intent discovery, which has three key technologies. First, it fully utilizes of unsupervised or semi-supervised data to mine shallow semantic similarity relations and provide well-initialized representations for clustering. Second, it designs a centroid-guided clustering mechanism to address the issue of cluster allocation inconsistency and provide high-quality self-supervised targets for representation learning. Third, it captures high-level semantics in unsupervised or semi-supervised data to discover fine-grained intent-wise clusters by optimizing both cluster-level and instance-level objectives. We also propose an effective method for estimating the cluster number in open-world scenarios without knowing the number of new intents beforehand. USNID performs exceptionally well on several benchmark intent datasets, achieving new state-of-the-art results in unsupervised and semi-supervised new intent discovery, and demonstrating robust performance with different cluster numbers.}
}


@article{DBLP:journals/tkde/AiXMDL24,
	author = {Wei Ai and
                  CanHao Xie and
                  Tao Meng and
                  Jiayi Du and
                  Keqin Li},
	title = {A D-Truss-Equivalence Based Index for Community Search Over Large
                  Directed Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5482--5494},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3393864},
	doi = {10.1109/TKDE.2024.3393864},
	timestamp = {Fri, 22 Nov 2024 08:28:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/AiXMDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community Search (CS) aims to enable online and personalized discovery of communities. Recently, attention to the CS problem in directed graphs (di-graph) needs to be improved despite the extensive study conducted on undirected graphs. Nevertheless, the existing studies are plagued by several shortcomings, e.g., Achieving high-performance CS while ensuring the retrieved community is cohesive is challenging. This paper uses the D-truss model to address the limitations of investigating the CS problem in large di-graphs. We aim to implement millisecond-level D-truss CS in di-graphs by building a summarized graph index. To capture the interconnectedness of edges within D-truss communities, we propose an innovative equivalence relation known as D-truss-equivalence, which allows us to divide the edges in a di-graph into a sequence of super nodes (s-nodes). These s-nodes form the D-truss-equivalence-based index, DEBI, an index structure that preserves the truss properties and ensures efficient space utilization. Using DEBI, CS can be performed without time-consuming access to the original graph. The experiments indicate that our method can achieve millisecond-level D-truss community query while ensuring high community quality. In addition, dynamic maintenance of indexes can also be achieved at a lower cost.}
}


@article{DBLP:journals/tkde/FanGW24,
	author = {Zongwen Fan and
                  Jin Gou and
                  Shaoyuan Weng},
	title = {A Feature Importance-Based Multi-Layer CatBoost for Student Performance
                  Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5495--5507},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3393472},
	doi = {10.1109/TKDE.2024.3393472},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FanGW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Student performance prediction is vital for identifying at-risk students and providing support to help them succeed academically. In this paper, we propose a feature importance-based multi-layer CatBoost approach to predict the students’ grade in the period exam. The idea is to construct a multi-layer structure with increasingly important features layer by layer. Specifically, the feature importance are first calculated and sorted in ascending order. In each layer, features with the least importance are accumulated until reaching a given threshold. Then, these selected features are used to construct the first layer by training the CatBoost. Next, this trained CatBoost is utilized to generate a feature that adds to the feature set with their importance within a threshold. After that, all these feature are used to train the CatBoost in the next layer. This process is repeated until all the features are used. The results show that the proposed model has the best performance. Moreover, the statistical test conducted based on 20-runs of experiments validates the significant superiority of our proposed model over the compared models and demonstrates the efficacy of the multi-layer structure in enhancing the proposed model. This indicates our proposed model can help decision makers in enhancing educational quality.}
}


@article{DBLP:journals/tkde/LiSXYLH24,
	author = {Yuchen Li and
                  Shixuan Sun and
                  Hanhua Xiao and
                  Chang Ye and
                  Shengliang Lu and
                  Bingsheng He},
	title = {A Survey on Concurrent Processing of Graph Analytical Queries: Systems
                  and Algorithms},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5508--5528},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3393936},
	doi = {10.1109/TKDE.2024.3393936},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiSXYLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph analytical queries (GAQs) are becoming increasingly important in various domains, including social networks, recommendation systems, and bioinformatics, among others. GAQs typically require iterative processing of the graph data to compute various metrics and identify patterns or anomalies. Parallel to the burgeoning demand for graph analytics, the need for Concurrent Graph Analytical Queries (CGAQs), allowing simultaneous execution of multiple graph queries, is increasing. Within social networks, CGAQs bolster real-time analytics, concurrently investigate various network properties, such as community detection, path analysis, and influence propagation. In transportation, CGAQs concurrently optimize multiple routes and manage real-time traffic data, contributing significantly to efficient supply chain strategies and traffic management. The key property of CGAQs lies in their capacity for shared processing, exploiting the synergies between concurrent queries, which in return opens opportunities for improved system scalability and throughput. In this survey, we present a comprehensive review of system-level and algorithm-level efforts to support CGAQ processing. We introduce a novel survey framework based on three aspects: 1) What are the sharing opportunities exploited? 2) What are the scheduling techniques proposed to maximize sharing? 3) What are the optimizations employed? We also identify important gaps and promising research directions for CGAQ processing.}
}


@article{DBLP:journals/tkde/WangWYYL24,
	author = {Hengzhi Wang and
                  En Wang and
                  Yongjian Yang and
                  Bo Yang and
                  Jiangchuan Liu},
	title = {A Truthful Pricing-Based Defending Strategy Against Adversarial Attacks
                  in Budgeted Combinatorial Multi-Armed Bandits},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5529--5543},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3335248},
	doi = {10.1109/TKDE.2023.3335248},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangWYYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study defending strategies against adversarial attacks on Combinatorial Multi-Armed Bandits (CMAB) algorithms. CMAB is an effective sequence decision making tool that has been broadly applied in online real-world applications. We consider a realistic CMAB setting, budgeted CMAB, in which multiple arms associated with pulling costs and unknown rewards are pulled per round, aiming to maximize the cumulative reward under a budget constraint. However, the adversarial attack against budgeted CMAB is rarely studied, posing a very important security issue. Specifically, a suboptimal arm that is not pulled (i.e., attacker) can hijack the budgeted CMAB algorithm's behavior, forcing itself to be pulled frequently by manipulating other arms’ rewards. Existing strategies cannot prevent such attacks. Motivated by this, we closely study the adversarial attack against a popular budgeted CMAB algorithm, exposing a significant security threat to real-world applications. The attack extends to other algorithms with certain customization. To address this, we incorporate a truthful pricing-based defending strategy that prevents such attacks effectively and ensures arms share pulling costs truthfully. Extensive simulations have illustrated the proposed attack strategy can hijack the algorithm efficiently, while the defending strategy provides attack prevention, individual rationality, and asymptotic truthfulness guarantees.}
}


@article{DBLP:journals/tkde/LiuWWYYZTC24,
	author = {Zirui Liu and
                  Xiangyuan Wang and
                  Yuhan Wu and
                  Tong Yang and
                  Kaicheng Yang and
                  Hailin Zhang and
                  Yaofeng Tu and
                  Bin Cui},
	title = {A Unified Framework for Mining Batch and Periodic Batch in Data Streams},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5544--5561},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3399024},
	doi = {10.1109/TKDE.2024.3399024},
	timestamp = {Thu, 13 Feb 2025 09:03:27 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuWWYYZTC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Batch is an important pattern in data streams, which refers to a group of identical items that arrive closely. We find that some special batches that arrive periodically are of great value. In this paper, we formally define a new pattern, namely periodic batches. A group of periodic batches refers to several batches of the same item, where these batches arrive periodically. Studying periodic batches is important in many applications, such as caches, financial markets, online advertisements, networks, etc. This paper proposes a unified framework, namely the HyperCalm sketch, to detect batch and periodic batch in data streams. HyperCalm sketch takes two phases to detect periodic batches. In phase 1, we propose a time-aware Bloom filter, called HyperBloomFilter (HyperBF), to detect batches. In phase 2, we propose an enhanced top-k algorithm, called Calm Space-Saving (CalmSS), to report top-k periodic batches. Extensive experiments show HyperCalm outperforms the strawman solutions 4× in term of average relative error and 98.1× in term of speed. All related codes are open-sourced.}
}


@article{DBLP:journals/tkde/LiuCLOLZ24,
	author = {Yipeng Liu and
                  Jie Chen and
                  Yingcong Lu and
                  Weiting Ou and
                  Zhen Long and
                  Ce Zhu},
	title = {Adaptively Topological Tensor Network for Multi-View Subspace Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5562--5575},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3391627},
	doi = {10.1109/TKDE.2024.3391627},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuCLOLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view subspace clustering employs learned self-representation from multiple tensor decompositions to exploit the low-rank information. However, the data structures embedded with self-representation tensors may vary in different multi-view datasets. Therefore, a pre-defined decomposition may not fully exploit low-rank information from various data, resulting in sub-optimal multi-view clustering performance. To alleviate this, we proposed the adaptively topological tensor network (ATTN). ATTN can learn a suitable decomposition structure that can represent the low-rank structure and high-order correlation of the self-representation tensors better in a data-driven way, which can capture the intra-view and inter-view information better. Firstly, instead of connecting the tensor network blindly, ATTN utilizes the correlation between adjacent factors to prune redundant connections from the fully connected tensor network, making the tensor network more expressive. Furthermore, a greedy adaptive rank-increasing strategy is applied to optimize the pruned tensor network structure, which improves the capacity of capturing low-rank structure. We apply ATTN on a multi-view subspace clustering task and utilize the alternating direction method of multipliers(ADMM) method to optimize it. Experiments show that multi-view subspace clustering based on ATTN has better performance on nine multi-view datasets.}
}


@article{DBLP:journals/tkde/LiXCW24,
	author = {Fan Li and
                  Zhiyu Xu and
                  Dawei Cheng and
                  Xiaoyang Wang},
	title = {AdaRisk: Risk-Adaptive Deep Reinforcement Learning for Vulnerable
                  Nodes Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5576--5590},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3409869},
	doi = {10.1109/TKDE.2024.3409869},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiXCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vulnerable node detection in uncertain graphs is a typical graph mining problem that seeks to identify nodes at a high risk of breakdown under the joint effect from both the self and contagion risk probability. This is an NP-hard problem that is crucial for risk management in many real-world applications such as networked loans and smart grids. Monte Carlo (MC) simulation and its optimized algorithms are commonly used to approximate the breakdown probability, but these methods require a large number of samples to ensure accuracy, which is computationally expensive for large-scale networks. Although recent studies employ Graph Neural Networks (GNNs) to model the contagion process and accelerate the inference, many of these methods suffer from the over-smoothing problem, leading to suboptimal performance under the long-distance risk contagion process. To this end, we propose a novel risk-adaptive deep reinforcement learning-based framework (AdaRisk) for vulnerable nodes detection in uncertain graphs. In particular, we design the Markov Decision Process (MDP) of the vulnerability estimation process in which our agent would approach the risk adaptively based on contagion probability accumulated in prior iterations. To encode state embeddings that incorporate multi-hop contagion information, the agent utilizes a long-distance adaptable policy network to process the input graph and output actions as the vulnerable probability of nodes. We conducted extensive experiments on four benchmark networks and three real-world financial networks to evaluate our proposed framework's performance. Our results demonstrate that AdaRisk outperforms state-of-the-art baselines in terms of detection performance, and also offers significant running time reductions compared to MC simulation.}
}


@article{DBLP:journals/tkde/WuXLWW24,
	author = {Junfei Wu and
                  Weizhi Xu and
                  Qiang Liu and
                  Shu Wu and
                  Liang Wang},
	title = {Adversarial Contrastive Learning for Evidence-Aware Fake News Detection
                  With Graph Neural Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5591--5604},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3341640},
	doi = {10.1109/TKDE.2023.3341640},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuXLWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prevalence and perniciousness of fake news have been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on the evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on different attention mechanisms. Despite their effectiveness, they still suffer from three weaknesses. First, due to the inherent drawbacks of sequential models, they fail to integrate the relevant information that is scattered far apart in evidences for veracity checking. Second, they underestimate much redundant information contained in evidences that may be useless or even harmful. Third, insufficient data utilization limits the separability and reliability of representations captured by the model, which are sensitive to local evidence. To solve these problems, we propose a unified Graph-based sEmantic structure mining framework with ConTRAstive Learning, namely GETRAL in short. Specifically, different from the existing work that treats claims and evidences as sequences, we first model them as graph-structured data and capture the long-distance semantic dependency among dispersed relevant snippets via neighborhood propagation. After obtaining contextual semantic information, our model reduces information redundancy by performing graph structure learning. Then the fine-grained semantic representations are fed into the downstream claim-evidence interaction module for predictions. Finally, the supervised contrastive learning accompanied with adversarial augmented instances is applied to make full use of data and strengthen the representation learning. Comprehensive experiments have demonstrated the superiority of GETRAL over the state-of-the-arts and validated the efficacy of semantic mining with graph structure and contrastive learning.}
}


@article{DBLP:journals/tkde/SuTWY24,
	author = {Qinliang Su and
                  Bowen Tian and
                  Hai Wan and
                  Jian Yin},
	title = {Anomaly Detection Under Contaminated Data With Contamination-Immune
                  Bidirectional GANs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5605--5620},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3404027},
	doi = {10.1109/TKDE.2024.3404027},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SuTWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection aims to detect instances that deviate significantly from the majority. Due to the difficulties of collecting a large amount of anomalies in practice, existing methods generally assume the availability of a clean normal dataset and leverage it to detect anomalies by characterizing the normality of normal samples. However, for many application scenarios, collecting a normal dataset that is sufficiently clean is not easy. What is often observed is that a small amount of anomalies are often falsely mixed into the normal dataset, resulting in a contaminated dataset. Obviously, the contamination in the normal dataset could significantly compromise the model's ability to detect anomalies. To alleviate this issue, two contamination-immune bidirectional generative adversarial networks (BiGAN) are developed, which can learn the probability distribution of normal samples from a contaminated dataset under some mild conditions. Rigorous proofs are provided to guarantee the theoretical correctness of the proposed models. Thanks to the removing of negative influences from the contamination samples, the proposed contamination-immune models can thus be applied to detect anomalies accurately for the scenarios with contaminated datasets. Extensive experimental results show that the proposed method outperforms the current state-of-the-art (SOTA) ones significantly under the scenarios with contaminated training datasets.}
}


@article{DBLP:journals/tkde/ZhangHZLP24,
	author = {Huimin Zhang and
                  Xingchen Hu and
                  Xiubin Zhu and
                  Xinwang Liu and
                  Witold Pedrycz},
	title = {Application of Gradient Boosting in the Design of Fuzzy Rule-Based
                  Regression Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5621--5632},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3392247},
	doi = {10.1109/TKDE.2024.3392247},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangHZLP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study is devoted to the design of gradient boosted fuzzy rule-based models for regression problems. Fuzzy rule-based models are built on the basis of information granules formed in the input and output spaces whose structure involves a family of conditional ‘if-then’ statements. The architecture of fuzzy rule-based models contributes to the realization of a sound tradeoff between modeling accuracy and interpretability and computing overhead. Gradient boosting paradigm has emerged as a powerful learning method realized through sequentially fitting additive base learners to current residuals in the steepest descent way. However, surprisingly, studies on the design and analysis of gradient boosted fuzzy rule-based models are still lacking. In this study, fuzzy rule-based model is regarded as a base learner. Different loss functions and their influence on the performance of the final models are explored. We also thoroughly investigate an impact of the initial quality of the rule-based model (implied by the number of rules) on the process of gradient boosting. The performance of the proposed approach is illustrated by a series of experimental studies concerning synthetic and publicly available datasets.}
}


@article{DBLP:journals/tkde/LiuLWSY24,
	author = {Yao Liu and
                  Binghao Li and
                  Xianzhi Wang and
                  Claude Sammut and
                  Lina Yao},
	title = {Attention-Aware Social Graph Transformer Networks for Stochastic Trajectory
                  Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5633--5646},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3390765},
	doi = {10.1109/TKDE.2024.3390765},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLWSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory prediction is fundamental to various intelligent technologies, such as autonomous driving and robotics. The motion prediction of pedestrians and vehicles helps emergency braking, reduces collisions, and improves traffic safety. Current trajectory prediction research faces problems of complex social interactions, high dynamics and multi-modality. Especially, it still has limitations in long-time prediction. We propose Attention-aware Social Graph Transformer Networks for multi-modal trajectory prediction. We combine Graph Convolutional Networks and Transformer Networks by generating stable resolution pseudo-images from Spatio-temporal graphs through a designed stacking and interception method. Furthermore, we design the attention-aware module to handle social interaction information in scenarios involving mixed pedestrian-vehicle traffic. Thus, we maintain the advantages of the Graph and Transformer, i.e., the ability to aggregate information over an arbitrary number of neighbors and the ability to perform complex time-dependent data processing. We conduct experiments on datasets involving pedestrian, vehicle, and mixed trajectories, respectively. Our results demonstrate that our model minimizes displacement errors across various metrics and significantly reduces the likelihood of collisions. It is worth noting that our model effectively reduces the final displacement error, illustrating the ability of our model to predict for a long time.}
}


@article{DBLP:journals/tkde/WangWWF24,
	author = {Chunnan Wang and
                  Hongzhi Wang and
                  Junzhe Wang and
                  Guosheng Feng},
	title = {AutoSR: Automatic Sequential Recommendation System Design},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5647--5660},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3400031},
	doi = {10.1109/TKDE.2024.3400031},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangWWF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential Recommendation (SR) System emerged recently as a powerful tool for suggesting users with the next item of interest. Despite their great success, the design of SR systems requires heavy manual work and domain knowledge. In this paper, we present \\mathbf {AutoSR}, an effective Automated Machine Learning (AutoML) tool that enables automatic design of powerful SR systems based on Graph Neural Network (GNN) and Reinforcement Learning (RL). In \\mathbf {AutoSR}, we summarize the design process of the SR systems and extract effective operations from the existing SR systems to construct our search space. Such an experience-based search space generates diverse SR systems by integrating effective operations of different systems, providing a basic condition for the implementation of AutoML. Besides, we propose a graph-based RL method to efficiently explore the SR search space, where operations have complex and diverse application conditions. Compared with the existing AutoML methods, which ignore potential relations among operations, \\mathbf {AutoSR} can greatly avoid invalid SR system design and efficiently discover more powerful SR systems by analyzing the relation graph of various operations. Extensive experimental results show that \\mathbf {AutoSR} can gain powerful SR systems, superior to the existing \\mathbf {AutoSR} systems used for search space construction. Besides, \\mathbf {AutoSR} is more efficient than the existing AutoML algorithms in SR system design, which demonstrate the superiority of \\mathbf {AutoSR}.}
}


@article{DBLP:journals/tkde/ZhangLXLWWD24,
	author = {Haiyan Zhang and
                  Xinghua Li and
                  Mengfan Xu and
                  Ximeng Liu and
                  Tong Wu and
                  Jian Weng and
                  Robert H. Deng},
	title = {{BADFL:} Backdoor Attack Defense in Federated Learning From Local
                  Model Perspective},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5661--5674},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3420778},
	doi = {10.1109/TKDE.2024.3420778},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLXLWWD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is substantial attention to federated learning with its ability to train a powerful global model collaboratively while protecting data privacy. Despite its many advantages, federated learning is vulnerable to backdoor attacks, where an adversary injects malicious weights into the global model, making the global model's targeted predictions incorrect. Existing defenses based on identifying and eliminating malicious weights ignore the similarity variation of the local weights during iterations in the malicious model detection and the presence of benign weights in the malicious model during the malicious local weight elimination, resulting in a poor defense and a degradation of global model accuracy. In this paper, we defend against backdoor attacks from the perspective of local models. First, a malicious model detection method based on interpretability techniques is proposed. The method appends a sampling check after clustering to identify malicious models accurately. We further design a malicious local weight elimination method based on local weight contributions. This method preserves the benign weights in the malicious model to maintain their contributions to the global model. Finally, we analyze the security of the proposed method in terms of model closeness and then verify the effectiveness of the proposed method through experiments. In comparison with existing defenses, the results show that BADFL improves the global model accuracy by 23.14% while reducing the attack success rate to 0.04% in the best case.}
}


@article{DBLP:journals/tkde/YangZWWLN24,
	author = {Xiaojun Yang and
                  Tuoji Zhu and
                  Danyang Wu and
                  Penglei Wang and
                  Yujia Liu and
                  Feiping Nie},
	title = {Bidirectional Fusion With Cross-View Graph Filter for Multi-View Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5675--5680},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3413682},
	doi = {10.1109/TKDE.2024.3413682},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangZWWLN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most existing multi-view graph clustering models either seek consistent clustering results from similarity matrices and spectral embeddings respectively or follow direct bidirectional integration of them, which ignores the interaction between them. To make up for this flaw, this paper designs a novel multi-view clustering model that performs Bidirectional Fusion with Cross-view Graph Filter (BF-CGF). To be specific, BF-CGF first learns a consistent graph embedding via performing the interaction between multi-view graphs and spectral embeddings with the perspective of the graph spectral domain and then considers seeking a consistent indicator matrix via the graph cut model from the consistent graph embedding and the similarity matrices. To solve the optimization problem of BF-CGF, we propose an efficient iterative algorithm and provide the corresponding convergence and complexity analyses. Extensive experimental results demonstrate that the proposed BF-CGF outperforms state-of-the-art competitors in most benchmark datasets.}
}


@article{DBLP:journals/tkde/ZhengHQCGSZX24,
	author = {Zhi Zheng and
                  Xiao Hu and
                  Zhaopeng Qiu and
                  Yuan Cheng and
                  Shanshan Gao and
                  Yang Song and
                  Hengshu Zhu and
                  Hui Xiong},
	title = {Bilateral Multi-Behavior Modeling for Reciprocal Recommendation in
                  Online Recruitment},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5681--5694},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397705},
	doi = {10.1109/TKDE.2024.3397705},
	timestamp = {Tue, 22 Oct 2024 21:09:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengHQCGSZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the rapid development of online recruitment platforms, which provide a convenient way for matching job seekers and recruiters. Indeed, this is a reciprocal recommendation problem which needs to consider the preferences of both job seekers and recruiters simultaneously. Existing studies mainly focus on building recommendation models based on text matching or collaborative filtering methods. However, we propose that these methods are limited and insufficient, since the abundant multi-typed bilateral behaviors among users have been largely ignored. Therefore, in this paper, we propose a novel BilAteral Multi-BehaviOr mOdeling (BAMBOO) method for reciprocal recommendation in online recruitment, which can model the multi-typed interactions from expectation perspective and competitiveness perspective. Specifically, for the expectation perspective, we propose to format the historical behaviors of different users as bilateral multi-behavior sequences, and we utilize a transformer-based model to learn the representations of what the users want to obtain. For the competitiveness perspective, we propose to construct a bilateral interaction heterogeneous graph to describe the entire recruitment market, and further utilize a heterogeneous graph transformer-based model to learn the representations of what the users can obtain. Moreover, we utilize contrastive learning methods to enhance these two modules. Furthermore, we propose to decompose the matching probability into the product of two parts, and we train our model based on a multi-task learning strategy. We conduct both offline experiments on real-world datasets and online A/B test, and the experiment results validate the effectiveness of our BAMBOO model compared with several state-of-the-art baseline methods.}
}


@article{DBLP:journals/tkde/WangWLZTZW24,
	author = {Liang Wang and
                  Shu Wu and
                  Qiang Liu and
                  Yanqiao Zhu and
                  Xiang Tao and
                  Mengdi Zhang and
                  Liang Wang},
	title = {Bi-Level Graph Structure Learning for Next {POI} Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5695--5708},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397683},
	doi = {10.1109/TKDE.2024.3397683},
	timestamp = {Tue, 24 Dec 2024 22:38:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangWLZTZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next point-of-interest (POI) recommendation aims to predict a user's next destination based on sequential check-in history and a set of POI candidates. Graph neural networks (GNNs) have demonstrated a remarkable capability in this endeavor by exploiting the extensive global collaborative signals present among POIs. However, most of the existing graph-based approaches construct graph structures based on pre-defined heuristics, failing to consider inherent hierarchical structures of POI features such as geographical locations and visiting peaks, or suffering from noisy and incomplete structures in graphs. To address the aforementioned issues, this paper presents a novel Bi-level Graph Structure Learning ({\\sf BiGSL}) for next POI recommendation. {\\sf BiGSL} first learns a hierarchical graph structure to capture the fine-to-coarse connectivity between POIs and prototypes, and then uses a pairwise learning module to dynamically infer relationships between POI pairs and prototype pairs. Based on the learned bi-level graphs, our model then employs a multi-relational graph network that considers both POI- and prototype-level neighbors, resulting in improved POI representations. Our bi-level structure learning scheme is more robust to data noise and incompleteness, and improves the exploration ability for recommendation by alleviating sparsity issues. Experimental results on three real-world datasets demonstrate the superiority of our model over existing state-of-the-art methods, with a significant improvement in recommendation accuracy and exploration performance.}
}


@article{DBLP:journals/tkde/XingZ24,
	author = {Zheng Xing and
                  Weibing Zhao},
	title = {Block-Diagonal Guided {DBSCAN} Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5709--5722},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3401075},
	doi = {10.1109/TKDE.2024.3401075},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XingZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cluster analysis constitutes a pivotal component of database mining, with DBSCAN being one of the most extensively employed algorithms in this domain. Nevertheless, DBSCAN is encumbered by several limitations, including challenges in processing high-dimensional datasets, a pronounced sensitivity to input parameters, and inconsistencies in generating reliable clustering outcomes. This paper presents a refined version of DBSCAN that utilizes the block-diagonal property of similarity graphs to enhance the clustering process. The core concept involves the construction of a graph that assesses the similarity among high-dimensional data points, capable of transformation into a block-diagonal form via an unknown permutation. This is followed by a cluster-ordering procedure that establishes the requisite permutation, thereby facilitating the straightforward identification of clustering structures through the recognition of diagonal blocks in the permuted graph. The principal obstacle addressed in this study is the construction of a graph that inherently possesses a block-diagonal structure, the permutation of this graph to actualize such a structure, and the autonomous identification of diagonal blocks within the permuted graph. To surmount these challenges, we initially devise a block-diagonal constrained self-representation model to create a similarity graph that exhibits a block-diagonal form post-permutation. A gradient descent-based methodology is proposed to resolve this problem effectively. Concurrently, we engineer a traversal algorithm, inspired by DBSCAN, that discerns clusters of high density within the graph and generates an enhanced cluster ordering. The attainment of a block-diagonal structure is then realized through permutation aligned with the traversal sequence, laying a robust foundation for both automated and interactive cluster analysis. Moreover, a novel split-and-refine algorithm is introduced to autonomously identify all diagonal blocks within the permuted graph, offering theoretical optimality under specific conditions. Extensive evaluations of our method across twelve rigorous real-world benchmark datasets affirm its superiority over contemporary state-of-the-art clustering techniques.}
}


@article{DBLP:journals/tkde/XuWJLWP24,
	author = {Hongzuo Xu and
                  Yijie Wang and
                  Songlei Jian and
                  Qing Liao and
                  Yongjun Wang and
                  Guansong Pang},
	title = {Calibrated One-Class Classification for Unsupervised Time Series Anomaly
                  Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5723--5736},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3393996},
	doi = {10.1109/TKDE.2024.3393996},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuWJLWP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series anomaly detection is instrumental in maintaining system availability in various domains. Current work in this research line mainly focuses on learning data normality deeply and comprehensively by devising advanced neural network structures and new reconstruction/prediction learning objectives. However, their one-class learning process can be misled by latent anomalies in training data (i.e., anomaly contamination) under the unsupervised paradigm. Their learning process also lacks knowledge about the anomalies. Consequently, they often learn a biased, inaccurate normality boundary. To tackle these problems, this paper proposes calibrated one-class classification for anomaly detection, realizing contamination-tolerant, anomaly-informed learning of data normality via uncertainty modeling-based calibration and native anomaly-based calibration. Specifically, our approach adaptively penalizes uncertain predictions to restrain irregular samples in anomaly contamination during optimization, while simultaneously encouraging confident predictions on regular samples to ensure effective normality learning. This largely alleviates the negative impact of anomaly contamination. Our approach also creates native anomaly examples via perturbation to simulate time series abnormal behaviors. Through discriminating these dummy anomalies, our one-class learning is further calibrated to form a more precise normality boundary. Extensive experiments on ten real-world datasets show that our model achieves substantial improvement over sixteen state-of-the-art contenders.}
}


@article{DBLP:journals/tkde/ShenZYYZZT24,
	author = {Qiaomu Shen and
                  Chaozu Zhang and
                  Xiao Yan and
                  Chuan Yang and
                  Dan Zeng and
                  Wei Zeng and
                  Bo Tang},
	title = {{\textdollar}{\textbackslash}mathsf \{CheetahTraj\}{\textdollar}CheetahTraj:
                  Efficient Visualization for Large Trajectory Dataset With Quality
                  Guarantee},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5737--5752},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3387480},
	doi = {10.1109/TKDE.2024.3387480},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShenZYYZZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Visualizing large-scale trajectory dataset is a core subroutine for many applications. However, rendering all trajectories could result in severe visual clutter and incur long visualization delays due to large data volume. Naively sampling the trajectories reduces visualization time but usually harms visual quality, i.e., the generated visualizations may look substantially different from the exact ones without sampling. In this paper, we propose \\mathsf {CheetahTraj}\n, a principled sampling framework that achieves both high visualization quality and low visualization latency. We first define the visual quality function measuring the similarity between two visualizations, based on which we formulate the quality optimal sampling problem ({\\sf QOSP}\n). To solve {\\sf QOSP}\n, we design the Visual Quality Guaranteed Sampling algorithms, which reduce visual clutter while guaranteeing visual quality by considering both trajectory data distribution and human perception properties. We also develop a quad-tree-based index (\\mathsf {InvQuad}\n) that allows using trajectory samples computed offline for interactive online visualization. Extensive experiments including case-, user-, and quantitative-studies are conducted on three real-world trajectory datasets, and the results show that \\mathsf {CheetahTraj}\nconsistently provides higher visual quality and better efficiency than baseline methods. Compared with visualizing all trajectories, \\mathsf {CheetahTraj}\nreduces the visualization latency by up to 3 orders of magnitude while avoiding visual clutter.}
}


@article{DBLP:journals/tkde/ChenZXXLZWZ24,
	author = {Weilong Chen and
                  Shaoliang Zhang and
                  Ruobing Xie and
                  Feng Xia and
                  Leyu Lin and
                  Xinran Zhang and
                  Yan Wang and
                  Yanru Zhang},
	title = {{CIPPO:} Contrastive Imitation Proximal Policy Optimization for Recommendation
                  Based on Reinforcement Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5753--5767},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3402649},
	doi = {10.1109/TKDE.2024.3402649},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenZXXLZWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation systems, widely adopted in social networks, personalize user experiences through advanced technologies such as Reinforcement Learning (RL), known for producing high-performance, list-wise recommendations. However, RL-based recommendation methods exhibit biases, specifically: 1) Online bias, which stems from a complex real-world online policy composed of various rules and models rather than a single policy; 2) Training bias, a distributional shift resulting from differences between the target policy and the behavior policy. To address these issues, we introduce a novel framework named Contrastive Imitation Proximal Policy Optimization (CIPPO) for recommendation based on RL. This approach leverages extensively labeled feedback data and incorporates a Masked Imitation Network (MIN) that closely emulates the online policy, thus reducing discrepancies between online and offline environments. Additionally, the clipping function in Proximal Policy Optimization, combined with a specially designed contrastive module, effectively reduces the distributional shift between the behavior and target policies. We conduct offline and online experiments to show the improvements of CIPPO, providing details including ablation tests and parameter analysis to validate the effectiveness and robustness. CIPPO gains 12.79% on ACN and in WeChat Top Stories, a large media platform with over 50 million users.}
}


@article{DBLP:journals/tkde/PangZDPT24,
	author = {Ying Pang and
                  Haibo Zhang and
                  Jeremiah D. Deng and
                  Lizhi Peng and
                  Fei Teng},
	title = {Collaborative Learning With Heterogeneous Local Models: {A} Rule-Based
                  Knowledge Fusion Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5768--5783},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3341808},
	doi = {10.1109/TKDE.2023.3341808},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PangZDPT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has emerged as a promising collaborative learning paradigm that enables to train machine learning models across decentralized devices, while keeping the training data localized to preserve user privacy. However, the heterogeneity in both decentralized training data and distributed computing resources has posed significant challenges to the design of effective and efficient FL schemes. Most existing solutions either focus on tackling a single type of heterogeneity, or are unable to fully support model heterogeneity with low communication overhead, fast convergence, and good interpretability. In this paper, we present CloREF, a novel rule-based collaborative learning framework that allows devices in FL to use completely different local learning models to cater to both data and resource heterogeneity. In CloREF, each rule is represented as a linear model, which provides good interpretability. Each participating device chooses a local model and trains it using its local data. The decision boundary of each trained local model is then approximated using a set of rules, which effectively bridges the gap arising from model heterogeneity. All participating devices collaborate to select the optimal set of rules as the global model, employing evolutionary optimization to effectively fuse the knowledge acquired from all local models. Experimental results on both synthesized and real-world datasets demonstrate that the rules generated by our proposed method can mimic the behaviors of various learning models with high fidelity (> 0.95 in most tests), and CloREF gives competitive performance in accuracy, AUC, and communication overhead, compared with both the best-performing model trained centrally and several state-of-the-art model-heterogeneous federated learning schemes.}
}


@article{DBLP:journals/tkde/ChenWHLY24,
	author = {Man{-}Sheng Chen and
                  Chang{-}Dong Wang and
                  Dong Huang and
                  Jian{-}Huang Lai and
                  Philip S. Yu},
	title = {Concept Factorization Based Multiview Clustering for Large-Scale Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5784--5796},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3392209},
	doi = {10.1109/TKDE.2024.3392209},
	timestamp = {Thu, 13 Feb 2025 10:32:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenWHLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most existing large-scale multiview clustering algorithms attempt to capture data distribution in multiple views by selecting view-wise anchor representations beforehand with k\n-means, or by direct matrix factorization on the original observations. Despite impressive performance, few of them have paid attention to the semantic correlations between anchor bases and cluster centroids, or even the underlying relations between clusters and data samples. In view of this, we propose a Concept Factorization based Multiview Clustering for Large-scale Data (CFMC) method with nearly linear complexity. The anchor bases learning, coefficient expression with clear semantic cues and partitioning are integrated together in this unified model. Meanwhile, explicit connections among multiview data, anchor bases and clusters are modeled via coefficient representations with semantic meanings. A four-step alternate minimizing algorithm is designed to handle the optimization problem, which is proved to have linear time complexity w.r.t. the sample size. Extensive experiments conducted on several challenging large-scale datasets confirm the superiority of the method compared with the state-of-the-art methods.}
}


@article{DBLP:journals/tkde/QianLYXHD24,
	author = {Wenbin Qian and
                  Yihui Li and
                  Qianzhi Ye and
                  Shuyin Xia and
                  Jintao Huang and
                  Weiping Ding},
	title = {Confidence-Induced Granular Partial Label Feature Selection via Dependency
                  and Similarity},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5797--5810},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3405489},
	doi = {10.1109/TKDE.2024.3405489},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QianLYXHD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Partial label learning (PLL) tackles scenarios where the unique ground-truth label of each sample is concealed within a candidate label set. Dimensionality reduction, considering labeling confidence estimation, has become a promising strategy to enhance the generalization performance of PLL models. However, current studies achieve dimensionality reduction, often relying on kNN-based labeling confidence estimation or disregarding potential labeling information. To address this issue, this paper proposes a novel Confidence-induced granular Partial label feature selection method using Dependency and Similarity (CPDS), which consists of two phases: Labeling Confidence Estimation (LCE) and Feature Selection (FS). For LCE, through granular ball computing, the feature space's similarity and the label space's correlation between the training data and the granular ball can be fused simultaneously, thereby effectively reconstructing more credible labeling confidence from candidate labels with more diverse semantic representation information. In the FS stage, by leveraging the LC with more diverse information, the proposed PLL neighborhood decision system further effectively combines feature dependency and label similarity to identify a feature subset with more discriminative capabilities, thereby achieving better performance for classification tasks. Among them, feature dependency effectively utilizes the dependency between neighborhoods and equivalence relations, while label similarity fully exploits the similarity between each sample and its neighbors. Extensive experiments show that CPDS significantly outperforms the compared approaches in most cases on nine controlled UCI datasets and five real-world datasets, demonstrating the superiority of the proposed method.}
}


@article{DBLP:journals/tkde/ApostolTP24,
	author = {Elena Simona Apostol and
                  Ciprian{-}Octavian Truica and
                  Adrian Paschke},
	title = {ContCommRTD: {A} Distributed Content-Based Misinformation-Aware Community
                  Detection System for Real-Time Disaster Reporting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5811--5822},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3417232},
	doi = {10.1109/TKDE.2024.3417232},
	timestamp = {Mon, 09 Dec 2024 22:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ApostolTP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time social media data can provide useful information on evolving hazards. Alongside traditional methods of disaster detection, the integration of social media data can considerably enhance disaster management. In this paper, we investigate the problem of detecting geolocation-content communities on Twitter and propose a novel distributed system that provides in near real-time information on hazard-related events and their evolution. We show that content-based community analysis can lead to better and faster dissemination of hazard-related reports than using only traditional methods, such as satellite or airborne sensing platforms. Our distributed disaster reporting system analyzes the social relationship among worldwide geolocated tweets and applies topic modeling to group tweets by topics. Considering for each tweet the following information: user, timestamp, geolocation, retweets, and replies, we create a publisher-subscriber distribution model for topics. We use content similarity and the proximity of nodes to create a new model for geolocation-content based communities. Users can subscribe to different topics in specific geographical areas or worldwide and receive real-time reports regarding these topics. As misinformation can lead to increased damage if propagated in hazards-related tweets, we propose a new deep learning model to detect fake news. The misinformed tweets are then removed from display. We also show empirically the scalability capabilities of the proposed system.}
}


@article{DBLP:journals/tkde/LuoZN24,
	author = {Haoyang Luo and
                  Zheng Zhang and
                  Liqiang Nie},
	title = {Contrastive Incomplete Cross-Modal Hashing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5823--5834},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3410388},
	doi = {10.1109/TKDE.2024.3410388},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuoZN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The success of current deep cross-modal hashing admits a default assumption of the fully-observed cross-modal data. However, such a rigorous common policy is hardly guaranteed for practical large-scale cases, which directly disable the training of prevalent cross-modal retrieval methods with incomplete cross-modal instances and unpaired relations. The main challenges come from the collapsed semantic- and modality-level similarity learning as well as uncertain cross-modal correspondence. In this paper, we propose a Contrastive Incomplete Cross-modal Hashing (CICH) network, which simultaneously determines the cross-modal semantic coordination, unbalanced similarity calibration, and contextual correspondence alignment. Specifically, we design a prototypical semantic similarity coordination module to globally rebuild partially-observed cross-modal similarities under an asymmetric learning scheme. Meanwhile, a semantic-aware contrastive hashing module is established to adaptively perceive and remedy the unbalanced similarities across different modalities with the semantic transition for generating discriminative hash codes. Additionally, a contextual correspondence alignment module is conceived to maximally capture shared knowledge across modalities and eliminate the correspondence uncertainty via a dual contextual information bottleneck formula. To the best of our knowledge, this is the first successful attempt of enabling contrastive learning to incomplete deep cross-modal hashing. Extensive experiments validate the superiority of our CICH against state-of-the-art methods.}
}


@article{DBLP:journals/tkde/WuZLZN24,
	author = {Qingpeng Wu and
                  Zheng Zhang and
                  Yishu Liu and
                  Jingyi Zhang and
                  Liqiang Nie},
	title = {Contrastive Multi-Bit Collaborative Learning for Deep Cross-Modal
                  Hashing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5835--5848},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419577},
	doi = {10.1109/TKDE.2024.3419577},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuZLZN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep cross-modal hashing, as a promising fast similarity search technique, has attracted broad interest and obtained great success owing to its outstanding representation capability and computational efficiency. Since the inconsistent feature representations and distributions of different modalities (i.e., image and text), prior studies primarily focus on preserving pairwise similarity with global embedding, but fail to further utilize detailed local representations to effectively align such heterogeneous data to jointly bridge the heterogeneous and semantic gaps across modalities. Meanwhile, typical learning networks can learn only one fixed-length hash code rather than multi-length ones, leading to extremely limited flexibility and scalability. To tackle these issues, this paper proposes a novel Contrastive Multi-bit Collaborative Learning (CMCL) network, which hierarchically aligns both global and local features among different modalities and simultaneously generates multi-length hash codes (i.e., 16-, 32-, 64-bits) in one unified transformer-based framework. Specifically, we design a novel cross-modal contrastive alignment module to simultaneously bridge the heterogeneous and semantic gaps across modalities via global and local contrastive learning. Moreover, we propose a multi-bit collaborative optimization module to synchronously produce multi-length hash codes under the explicit guidance of one auxiliary online hash learner with a longer length (i.e., 128-bit). As such, our CMCL framework can jointly alleviate the heterogeneity among modalities from a hierarchical perspective and collaboratively explore the correlations between multi-bit hash codes, thereby yielding multi-length discriminative hash codes in a one-stop learning manner. Comprehensive experiments demonstrate the consistent superiority of our CMCL in multi-bit hash code learning over the state-of-the-art cross-modal hashing baselines.}
}


@article{DBLP:journals/tkde/SunCDQQL24,
	author = {Xiangguo Sun and
                  Hong Cheng and
                  Hang Dong and
                  Bo Qiao and
                  Si Qin and
                  Qingwei Lin},
	title = {Counter-Empirical Attacking Based on Adversarial Reinforcement Learning
                  for Time-Relevant Scoring System},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5849--5859},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3341430},
	doi = {10.1109/TKDE.2023.3341430},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SunCDQQL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scoring systems are commonly seen for platforms in the era of Big Data. From credit scoring systems in financial services to membership scores in E-commerce shopping platforms, platform managers use such systems to guide users towards the encouraged activity pattern, and manage resources more effectively and efficiently. To establish such scoring systems, several “empirical criteria” are first determined, followed by a dedicated top-down design for each score factor, which usually requires enormous effort to adjust and tune the scoring function in the new application scenario. What's worse, many fresh projects usually have no ground truth or any experience to evaluate a reasonable scoring system, making the designing even harder. To reduce the effort of manual adjustment of the scoring function in every new scoring system, we innovatively study the scoring system from the preset empirical criteria without any ground truth and propose a novel framework to improve the system from scratch. In this paper, we propose a “counter-empirical attacking” mechanism that can generate “attacking” behavior traces and try to break the empirical rules of the scoring system. Then an adversarial “enhancer” is applied to evaluate the scoring system and find the improvement strategy. By training the adversarial learning problem, a proper scoring function can be learned to be robust to the attacking activity traces that are trying to violate the empirical criteria. Extensive experiments have been conducted on two scoring systems, including a shared computing resource platform and a financial credit system. The experimental results have validated the effectiveness of our proposed framework.}
}


@article{DBLP:journals/tkde/CaiDLLWYWW24,
	author = {Rui Cai and
                  Jianfeng Dong and
                  Tianxiang Liang and
                  Yonghui Liang and
                  Yabing Wang and
                  Xun Yang and
                  Xun Wang and
                  Meng Wang},
	title = {Cross-Lingual Cross-Modal Retrieval With Noise-Robust Fine-Tuning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5860--5873},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3400060},
	doi = {10.1109/TKDE.2024.3400060},
	timestamp = {Sat, 30 Nov 2024 21:08:01 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CaiDLLWYWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-lingual cross-modal retrieval aims at leveraging human-labeled annotations in a source language to construct cross-modal retrieval models for a new target language, due to the lack of manually-annotated dataset in low-resource languages (target languages). Contrary to the growing developments in the field of monolingual cross-modal retrieval, there has been less research focusing on cross-modal retrieval in the cross-lingual scenario. A straightforward method to obtain target-language labeled data is translating source-language datasets utilizing Machine Translations (MT). However, as MT is not perfect, it tends to introduce noise during translation, rendering textual embeddings corrupted and thereby compromising the retrieval performance. To alleviate this, we propose Noise-Robust Fine-tuning (NRF) which tries to extract clean textual information from a possibly noisy target-language input with the guidance of its source-language counterpart. Besides, contrastive learning involving different modalities are performed to strengthen the noise-robustness of our model. Different from traditional cross-modal retrieval methods which only employ image/video-text paired data for fine-tuning, in NRF, selected parallel data plays a key role in improving the noise-filtering ability of our model. Extensive experiments are conducted on three video-text and image-text retrieval benchmarks across different target languages, and the results demonstrate that our method significantly improves the overall performance without using any image/video-text paired data on target languages.}
}


@article{DBLP:journals/tkde/SongZYWW24,
	author = {Yixin Song and
                  Lihua Zhou and
                  Peizhong Yang and
                  Jialong Wang and
                  Lizhen Wang},
	title = {{CS-DAHIN:} Community Search Over Dynamic Attribute Heterogeneous
                  Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5874--5888},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3402258},
	doi = {10.1109/TKDE.2024.3402258},
	timestamp = {Mon, 03 Mar 2025 22:25:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SongZYWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community search (CS) is an important research topic in network analysis, which aims to find a subgraph that satisfies the given conditions. A dynamic attribute heterogeneous information network (DAHIN) is a sequence of attribute heterogeneous information network (AHIN) snapshots, where each snapshot consists of multiple types of vertices as well as edges, and each vertex is associated a set of attribute keywords. CS over DAHIN faces many challenges. In this paper, we study the CS problem over DAHINs, aiming to search for cohesive subgraphs containing query vertex and simultaneously satisfying the connectivity, attribute cohesiveness and interaction stability. To this end, we propose a deep learning model with a three-level attention mechanism and the concept of interaction frequency with respect to multiple semantic relationships to measure the similarity of attributes and the stability of interactions between vertices respectively. In addition, we design three search algorithms to locate the target community by optimizing the degree of interaction stability and attribute similarity between vertices. Extensive experiments, including comparison with existing algorithms, ablation analysis, parameter sensitivity examination, and case studies, are conducted on four real-world datasets to validate the effectiveness and efficiency of the proposed model and search algorithms. The code and model of CS-DAHIN will be open source on GitHub.}
}


@article{DBLP:journals/tkde/ZengZ24,
	author = {Xianzhi Zeng and
                  Shuhao Zhang},
	title = {CStream: Parallel Data Stream Compression on Multicore Edge Devices},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5889--5904},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3386862},
	doi = {10.1109/TKDE.2024.3386862},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZengZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the burgeoning realm of Internet of Things (IoT) applications on edge devices, data stream compression has become increasingly pertinent. The integration of added compression overhead and limited hardware resources on these devices calls for a nuanced software-hardware co-design. This paper introduces CStream, a pioneering framework crafted for parallelizing stream compression on multicore edge devices. CStream grapples with the distinct challenges of delivering a high compression ratio, high throughput, low latency, and low energy consumption. Notably, CStream distinguishes itself by accommodating an array of stream compression algorithms, a variety of hardware architectures and configurations, and an innovative set of parallelization strategies, some of which are proposed herein for the first time. Our evaluation showcases the efficacy of a thoughtful co-design involving a lossy compression algorithm, asymmetric multicore processors, and our novel, hardware-conscious parallelization strategies. This approach achieves a 2.8 \\times compression ratio with only marginal information loss, 4.3 \\times throughput, 65% latency reduction and 89% energy consumption reduction, compared to designs lacking such strategic integration.}
}


@article{DBLP:journals/tkde/YowL24,
	author = {Kai Siong Yow and
                  Chunbo Li},
	title = {D{\&}A: Resource Optimization in Personalized PageRank Computations
                  Using Multi-Core Machines},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5905--5910},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3417264},
	doi = {10.1109/TKDE.2024.3417264},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YowL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resource optimization is commonly used in workload management, ensuring efficient and timely task completion utilising available resources. It serves to minimise costs, prompting the development of numerous algorithms tailored to this end. The majority of these techniques focus on scheduling and executing workloads effectively within the provided resource constraints. In this paper, we tackle this problem using another approach. We propose a novel framework D&A to determine the number of cores required in completing a workload under time constraint. We first preprocess a small portion of queries to derive the number of required slots, allowing for the allocation of the remaining workloads into each slot. We introduce a scaling factor in handling the time fluctuation issue caused by random functions. We further establish a lower bound of the number of cores required under this scenario, serving as a baseline for comparison purposes. We examine the framework by computing personalized PageRank values involving intensive computations. Our experimental results show that D&A surpasses the baseline, achieving reductions in the required number of cores ranging from 38.89\\% to 73.68\\% across benchmark datasets comprising millions of vertices and edges.}
}


@article{DBLP:journals/tkde/LiNDQCSZR24,
	author = {Conggai Li and
                  Wei Ni and
                  Ming Ding and
                  Youyang Qu and
                  Jianjun Chen and
                  David B. Smith and
                  Wenjie Zhang and
                  Thierry Rakotoarivelo},
	title = {Decentralized Privacy Preservation for Critical Connections in Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5911--5925},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3406641},
	doi = {10.1109/TKDE.2024.3406641},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiNDQCSZR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many real-world interconnections among entities can be characterized as graphs. Collecting local graph information with balanced privacy and data utility has garnered notable interest recently. This paper delves into the problem of identifying and protecting critical information of entity connections for individual participants in a graph based on cohesive subgraph searches. This problem has not been addressed in the literature. To address the problem, we propose to extract the critical connections of a queried vertex using a fortress-like cohesive subgraph model known as p-cohesion. A user's connections within a fortress are obfuscated when being released, to protect critical information about the user. Novel merit and penalty score functions are designed to measure each participant's critical connections in the minimal p-cohesion, facilitating effective identification of the connections. We further propose to preserve the privacy of a vertex enquired by only protecting its critical connections when responding to queries raised by data collectors. We prove that, under the decentralized differential privacy (DDP) mechanism, one's response satisfies (\\varepsilon, \\delta )-DDP when its critical connections are protected while the rest remains unperturbed. The effectiveness of our proposed method is demonstrated through extensive experiments on real-life graph datasets.}
}


@article{DBLP:journals/tkde/HuoQZHN24,
	author = {Yadong Huo and
                  Qibing Qin and
                  Wenfeng Zhang and
                  Lei Huang and
                  Jie Nie},
	title = {Deep Hierarchy-Aware Proxy Hashing With Self-Paced Learning for Cross-Modal
                  Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5926--5939},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3401050},
	doi = {10.1109/TKDE.2024.3401050},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuoQZHN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to its low storage cost and high retrieval efficiency, hashing technology is popularly applied in both academia and industry, which provides an interesting solution for cross-modal similarity retrieval. However, most existing supervised cross-modal hashing methods typically view the fixed-level semantic affinity defined by manual labels as supervised signals to guide hash learning, which only represents a small subset of complex semantic relations between multi-modal samples, thus impeding the hash function learning and degrading the obtained hash codes. In the paper, by learning shared hierarchy proxies, a novel deep cross-modal hashing framework, called Deep Hierarchy-aware Proxy Hashing (DHaPH), is proposed to construct the semantic hierarchy in a data-driven manner, thereby capturing the accurate fine-grained semantic relationships and achieving small intra-class scatter and big inter-class scatter. Specifically, by regarding the hierarchical proxies as learnable ancestors, a novel hierarchy-aware proxy loss is designed to model the latent semantic hierarchical structures from different modalities without prior hierarchy knowledge, in which similar samples share the same Lowest Common Ancestor (LCA) and dissimilar points have different LCA. Meanwhile, to adequately capture valuable semantic information from hard pairs, a multi-modal self-paced loss is introduced into cross-modal hashing to reweight multi-modal pairs dynamically, which enables the model to gradually focus on hard pairs while simultaneously learning universal patterns from multi-modal pairs. Extensive experiments on three available benchmark databases demonstrate that our proposed DHaPH framework outperforms the compared baselines with different evaluation metrics.}
}


@article{DBLP:journals/tkde/MaW24,
	author = {Fei Ma and
                  Ping Wang},
	title = {Determining Mean First-Passage Time for Random Walks on Stochastic
                  Uniform Growth Tree Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5940--5953},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3392786},
	doi = {10.1109/TKDE.2024.3392786},
	timestamp = {Mon, 11 Nov 2024 17:30:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MaW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As known, the commonly-utilized ways to determine mean first-passage time \\overline{\\mathcal{F}} for random walk on networks are mainly based on Laplacian spectra. However, methods of this type can become prohibitively complicated and even fail to work when the Laplacian matrix of network under consideration is difficult to describe in the first place. In this paper, we propose an effective approach to determining quantity \\overline{\\mathcal{F}} on some widely-studied tree networks. To this end, we first build up a general formula between Wiener index \\mathcal{W} and \\overline{\\mathcal{F}} on a tree. This enables us to convert issues to answer into calculation of \\mathcal{W} on networks in question. As opposed to most of previous work focusing on deterministic growth trees, our goal is to consider stochastic case. Towards this end, we establish a principled framework where randomness is introduced into the process of growing trees. As an immediate consequence, the previously published results upon deterministic cases are thoroughly covered by formulas established in this paper. Additionally, it is also straightforward to obtain Kirchhoff index on our tree networks using the proposed approach. Most importantly, our approach is more manageable than some other methods including spectral technique in situations considered herein.}
}


@article{DBLP:journals/tkde/ChuZZLZXZ24,
	author = {Deming Chu and
                  Fan Zhang and
                  Wenjie Zhang and
                  Xuemin Lin and
                  Ying Zhang and
                  Yinglong Xia and
                  Chenyi Zhang},
	title = {Discovering and Maintaining the Best {\textdollar}k{\textdollar}k
                  in Core Decomposition},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5954--5971},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3389989},
	doi = {10.1109/TKDE.2024.3389989},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChuZZLZXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The mode of k-core and its hierarchical decomposition have been applied in many areas, such as sociology, the world wide web, and biology. Algorithms on related studies often need an input value of parameter k, while there is no existing solution other than manual selection. In this paper, given a graph and a scoring metric, we aim to find the best value of k such that the score of the k-core (or k-core set) is the highest. The problem is challenging because there are various community scoring metrics and the computation is costly on large datasets. With the well-designed vertex ordering, we propose time-and-space-optimal algorithms to compute the best k, which are applicable to most community metrics. As real-world networks are often fast-evolving, we also design a novel framework to maintain the best k-core (set) against graph dynamics. We prove the dynamic algorithms are bounded, i.e., the update cost is decided by the changes of input and output. The proposed algorithms can benefit the solutions to k-core-related problems and their dynamic counterparts. Extensive experiments are conducted on 10 real-world networks with size up to billion-scale, which validates the efficiency of our algorithms and the effectiveness of the resulting k-cores.}
}


@article{DBLP:journals/tkde/GongDY24,
	author = {Chaoyu Gong and
                  Jim Demmel and
                  Yang You},
	title = {Distributed and Joint Evidential K-Nearest Neighbor Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5972--5985},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3341098},
	doi = {10.1109/TKDE.2023.3341098},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GongDY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The performance of K-nearest neighbor (K-NN) classification depends significantly on the searched neighborhoods of test samples, namely, the neighborhood size K and the used distance metric. For these two issues, many methods either to acquire the adaptive K or to learn a variant metric have been proposed and yielded appropriate performances. However, most of the existing methods ignore the fact that these two factors can be jointly learned. In this paper, we propose a Joint Evidential K-NN algorithm (JEKNN), which learns the adaptive K of each sample and distance metric jointly based on the feedback of error function. To break the computational bottleneck of handling large datasets, a distributed version of JEKNN (JEKNN_{\\mathrm{{dis}}}) is implemented under Apache Spark, i.e., an optimization algorithm based on distributed gradient descent and data parallelism is proposed to accelerate the training stage. Ablation and comparison experiments on small-scale datasets shows the performance improvement from the joint learning and the state-of-the-art accuracy of JEKNN, respectively. Compared to other KNN-based methods designed for Big Data, experimental results on big datasets demonstrate that JEKNN_{\\mathrm{{dis}}} achieves better scaling efficiency without significant loss of accuracy. Besides, the generalization error bound of the proposed algorithm is also analyzed theoretically.}
}


@article{DBLP:journals/tkde/WangZWCG24,
	author = {Ranran Wang and
                  Yin Zhang and
                  Wenchao Wan and
                  Min Chen and
                  Mohsen Guizani},
	title = {Distributed Rumor Source Detection via Boosted Federated Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {5986--6001},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3390238},
	doi = {10.1109/TKDE.2024.3390238},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangZWCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How to localize the rumor source is an extremely important matter for all sectors of the society. Many researchers have tried to use deep-learning-based graph models to detect rumor sources, but they have neglected how to train their deep-learning-based graph models in the noisy social network environment efficiently. Especially for deep learning models, the performance relies on the data scale. However, even though it is known that a substantial amount of rumor data distributed across multiple edge servers (e.g., cross-platform), due to conflicting business interests, its challenging to coordinate all parties to train a model driven by many samples while avoiding moving data. Federated learning, is an effective technique to bridge this gap. Therefore, this paper proposes a Distributed Rumor Source Detection via Boosted Federated Learning (DRSDBFL). Specifically, this paper proposes an effective rumor source detection method based on a deep-learning-based graph model with a denoising module. To the best of our knowledge, we are the first to attempt the use of a denoising module to reduce the noisy effects of social networks. Then, we propose a novel boosted federated learning mechanism through boosting the high-quality edge worker to improve the training efficiency. Finally, the effectiveness of the proposed method is verified by extensive experiments.}
}


@article{DBLP:journals/tkde/ZhangZZDY24,
	author = {Yi Zhang and
                  Yiwen Zhang and
                  Yuchuan Zhao and
                  Shuiguang Deng and
                  Yun Yang},
	title = {Dual Variational Graph Reconstruction Learning for Social Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6002--6015},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3386895},
	doi = {10.1109/TKDE.2024.3386895},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZZDY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a new recommendation pattern combining collaborative filtering and social network, social recommender system strives to introduce auxiliary user relations to alleviate data sparsity problems. Considering the graph structure characteristics of user historical interactions and social network, there have been emerged several innovative works that utilize Graph Neural Network (GNN) for social recommendation to show impressive performance. However, existing works seem to be restricted to exploiting social network as auxiliary information for main recommendation tasks, with little attention on the social network itself at the fine-grained level. From empirical perspective, the effectiveness of directly applying social network to social recommendation via GNNs may be limited since the social information that can be used for training is actually sparser than user interactions, and most of observable social information is not valid. To resolve this problem, we propose a Dual Variational Graph Reconstruction Learning (DVGRL) framework for social recommendation. It treats user interaction graph and social network as equivalent and aims to learn both variational distributions of user preferences from historical interactions and social connections, which are trained simultaneously and used to guide the reconstruction of historical interaction graph and social network. To effectively exploit the social information gleaned from reconstruction learning for enhancing recommendation, we design two inter-domain fusion mechanisms to achieve knowledge transfer from the perspectives of attention features and prior distributions, respectively. Extensive experiments on four real-world datasets validate the effectiveness of DVGRL for social recommendation tasks.}
}


@article{DBLP:journals/tkde/ZhouLLZ24,
	author = {Ming Zhou and
                  Jie Lu and
                  Pengqian Lu and
                  Guangquan Zhang},
	title = {Dynamic Graph Regularization for Multi-Stream Concept Drift Self-Adaptation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6016--6028},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3401156},
	doi = {10.1109/TKDE.2024.3401156},
	timestamp = {Mon, 03 Mar 2025 22:25:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouLLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Concept drift is an inevitable problem in non-stationary stream environments, due to evolving data distributions. In practical applications, multi-stream is more complex than single-stream, yet they have received little attention. Addressing concept drift while mining correlations between streams poses a significant challenge. Research focuses on capturing correlations between streams using graph neural networks (GNNs), providing valuable insights. However, these methods fix the correlation graph structure after training, failing to adapt to the new data distributions with dynamic correlations during testing. To bridge this gap, we propose a novel framework named Multi-stream Self-adaptation based on Graph Regularization (MSGR). A new GNN architecture is proposed to capture deep spatio-temporal correlations and learn a correlation graph structure without pre-defined graphs. Each graph node represents a stream and the graph is constructed through Gumbel sampling and an adaptive matrix from stream pairs. Thus we attain a base high-performance GNN for multi-stream multi-step prediction. To adapt to the new data distribution during testing, we design a self-adaptation mechanism by assigning dynamic learning weights for newly arriving samples. Larger learning weights are assigned to relevant samples when drift occurs. The self-adaptation is accomplished by the sub-graph updating and the proposed graph regularization. Error-based drift detection is integrated, and when drift occurs, the weight for sub-graph updating increases by adjusting the regularization coefficient. Thus, MSGR maintains high self-adaptation performance and accurate prediction results consistently regardless of the type and degree of concept drift. Comprehensive testing on real-world and synthetic datasets shows that MSGR achieves state-of-the-art performance.}
}


@article{DBLP:journals/tkde/WuMNZHY24,
	author = {Yangyang Wu and
                  Xiaoye Miao and
                  Zi{-}ang Nan and
                  Jinshan Zhang and
                  Jianhu He and
                  Jianwei Yin},
	title = {Effective and Efficient Multi-View Imputation With Optimal Transport},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6029--6041},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3387439},
	doi = {10.1109/TKDE.2024.3387439},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuMNZHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The multi-view data with incomplete information hinder effective data analysis. Existing multi-view imputation methods, which learn the mapping between a complete view and a completely missing view, are not able to deal with the typical multi-view data with missing feature information. In this paper, we propose a unified generative imputation model named {\\sf UGit} with optimal transport theory to simultaneously impute the missing features/values of all incomplete views. This imputation is conditional on all the observed values from the multi-view data. {\\sf UGit} consists of two modules, i.e., a unified multi-view generator (UMG) and a masking energy discriminator (MED). To effectively and efficiently impute missing features across all views, the generator UMG employs a unified autoencoder in conjunction with the cross-view attention mechanism to learn the data distribution from all observed multi-view data. The discriminator MED leverages a novel masking energy divergence function to make {\\sf UGit} differentiable for imputation accuracy enhancement. Extensive experiments on several real-world multi-view data sets demonstrate that, {\\sf UGit} speeds up the model training by 4.28x with more than 41% accuracy gain on average, compared to the state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/LiuQZW24,
	author = {Jinlu Liu and
                  Jing Qin and
                  Xi Zhang and
                  Huaxiong Wang},
	title = {Efficient Key-Aggregate Cryptosystem With User Revocation for Selective
                  Group Data Sharing in Cloud Storage},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6042--6055},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397721},
	doi = {10.1109/TKDE.2024.3397721},
	timestamp = {Sun, 19 Jan 2025 13:53:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuQZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud computing has become prevalent due to its extensive storage resources and robust computational capacities. To protect data security and privacy, data owners opt for uploading encrypted data to the cloud. Flexible sharing of these encrypted data in a group of users is a critical functionality in cloud storage. In addition, given that users may exit the group, revocation becomes a crucial requirement in group data-sharing systems. The Key-Aggregate Cryptosystem (KAC) has become a promising mechanism for group data sharing. The decryption rights for any set of ciphertexts can be efficiently delegated by distributing a constant-size aggregate key, while the confidentiality of other ciphertexts outside the set is maintained. However, in previous KAC schemes, revocation remains a challenging task regarding key update, ciphertext re-encryption, and collision resistance. In this paper, we propose a Key-Aggregate Cryptosystem with User Revocation (KAC-UR) scheme to overcome this challenge. The KAC-UR scheme not only achieves flexible data sharing, but also can perform secure and efficient user revocation with properties including collision resistance, revocation without data owner-user communication, and constant ciphertext size. The KAC-UR scheme also enables the cloud server to perform partial decryption, thereby significantly alleviating the computational burden for users. The KAC-UR scheme is chosen plaintext attack secure under the decisional Bilinear Diffie-Hellman Exponent assumption.}
}


@article{DBLP:journals/tkde/WuZJL24,
	author = {Dingming Wu and
                  Zhaofen Zhang and
                  Christian S. Jensen and
                  Kezhong Lu},
	title = {Efficient Skyline Keyword-Based Tree Retrieval on Attributed Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6056--6070},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3388988},
	doi = {10.1109/TKDE.2024.3388988},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuZJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Attributed graphs are graphs, where the vertices have attributes. Such graphs encompass, e.g., social network graph, citation graphs, and knowledge graphs, which have numerous real-world applications. Keyword-based search is a prominent and user-friendly way of querying attributed graphs. One widely used approach to keyword search adopts tree-based query semantics that relies on scoring functions that aggregate distances from a root to keyword-matched vertices. However, it is non-trivial to design scoring functions that capture different users’ keyword preferences. This study defines and solves the skyline KTree retrieval problem that combines keyword querying with skyline functionality on attributed graphs. The result of a skyline KTree query is independent of scoring functions. Hence, no matter which keywords are preferred, users can always find their favorite KTrees in a result. To enable efficient skyline KTree retrieval, we propose algorithm \\mathsf {FilterRefine} that first identifies candidate results and then uses them for search space pruning. Computing distances between keywords and vertices is expensive and dominates the computational cost of \\mathsf {FilterRefine}. Inspired by subspace skyline query techniques, we convert the skyline KTree retrieval problem into a multi-dimensional subspace skyline problem and propose algorithm \\mathsf {MultiDiSkylineOpt}. This algorithm is able to reuse skylines in subspaces and uses bounds on all dimensions to accelerate distance computation. Experimental results on real datasets show that a baseline algorithm cannot report results within a 500 second cut-off time, while the proposed algorithms are able to compute results in reasonable time. In particular, \\mathsf {MultiDiSkylineOpt} is able to efficiently retrieve skyline KTrees on large graphs with millions of nodes and hundreds of millions of edges.}
}


@article{DBLP:journals/tkde/LiLFWLTL24,
	author = {Jiatong Li and
                  Yunqing Liu and
                  Wenqi Fan and
                  Xiao{-}Yong Wei and
                  Hui Liu and
                  Jiliang Tang and
                  Qing Li},
	title = {Empowering Molecule Discovery for Molecule-Caption Translation With
                  Large Language Models: {A} ChatGPT Perspective},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6071--6083},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3393356},
	doi = {10.1109/TKDE.2024.3393356},
	timestamp = {Thu, 17 Oct 2024 12:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiLFWLTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs, which contributes to the development of society and human well-being. Specifically, molecule-caption translation is an important task for molecule discovery, aligning human understanding with molecular space. However, most of the existing methods heavily rely on domain experts, require excessive computational cost, or suffer from sub-optimal performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their powerful capabilities in natural language understanding, generalization, and in-context learning (ICL), which provides unprecedented opportunities to advance molecule discovery. Despite several previous works trying to apply LLMs in this task, the lack of domain-specific corpus and difficulties in training specialized LLMs still remain challenges. In this work, we propose a novel LLM-based framework (MolReGPT) for molecule-caption translation, where an In-Context Few-Shot Molecule Learning paradigm is introduced to empower molecule discovery with LLMs like ChatGPT to perform their in-context learning capability without domain-specific pre-training and fine-tuning. MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to enable LLMs to learn the task knowledge from context examples. We evaluate the effectiveness of MolReGPT on molecule-caption translation, including molecule understanding and text-based molecule generation. Experimental results show that compared to fine-tuned models, MolReGPT outperforms MolT5-base and is comparable to MolT5-large without additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs via in-context learning in molecule-caption translation for advancing molecule discovery. Our work expands the scope of LLM applications, as well as providing a new paradigm for molecule discovery and design.}
}


@article{DBLP:journals/tkde/WangYHM24,
	author = {Leixia Wang and
                  Qingqing Ye and
                  Haibo Hu and
                  Xiaofeng Meng},
	title = {EPS{\textdollar}{\^{}}\{2\}{\textdollar}2: Privacy Preserving Set-Valued
                  Data Analysis in the Shuffle Model},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6084--6098},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3341171},
	doi = {10.1109/TKDE.2023.3341171},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangYHM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collecting and analyzing users’ set-valued data with privacy-preserving is a common scenario in real life. However, the existing solutions in LDP are not efficient enough, where users perturbing their data locally introduces a large amount of noise. The shuffle model, which adds a shuffler in LDP to shuffle all perturbed values, can amplify privacy, then improve utility. Inspired by this, we study the frequency estimation and top-k frequent item estimation of set-valued data in the shuffle model. To solve the challenges of different item quantities of users and further improve the utility, we combine sampling and shuffling together, and propose the Encoding, Padding, Sampling, and Shuffling framework, i.e., EPS^{2}. Based on this framework, we propose three protocols for frequency estimation in different application scenarios, then assemble them into multi-phase protocols for the top-k frequent item estimation. Theoretically, we identify all three protocols gain dual privacy amplification from sampling and shuffling. And by setting the size of users’ set to 1, we can extend this amplified bound to the single-valued frequency estimation scenario, producing a tighter privacy bound than existing works. Finally, we perform experiments on both synthetic and real-world datasets to demonstrate the effectiveness of our protocols.}
}


@article{DBLP:journals/tkde/AdesunkanmiK24,
	author = {Rahmat Adesunkanmi and
                  Ratnesh Kumar},
	title = {Expectation Distance-Based Distributional Clustering for Noise-Robustness},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6099--6110},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3386401},
	doi = {10.1109/TKDE.2024.3386401},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/AdesunkanmiK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a clustering technique that reduces the susceptibility to data noise by learning and clustering the data-distribution and then assigning the data to the cluster of its distribution. In the process, it reduces the impact of noise on clustering results. This method involves introducing a new distance among distributions, namely the expectation distance (denoted, ED), that goes beyond the state-of-art distribution distance of optimal mass transport, also called 2-Wasserstein (denoted, W_{2}): The latter essentially depends only on the marginal distributions while the former also employs the information about the joint distributions, making it more powerful. Using the ED, the paper extends the classical K-means and K-medoids clustering to those over data-distributions (rather than raw-data) and further introduces K-medoids using W_{2}. The paper also presents the closed-form expressions of the W_{2} and ED distance measures. The implementation results of the proposed ED and the W_{2} distance measures to cluster real-world weather data as well as stock data are also presented, which involves efficiently extracting and using the underlying data distributions—Gaussians for weather data versus lognormals for stock data. The results show striking performance improvement over classical clustering of raw-data, with higher accuracy realized for ED. Also, not only does the distribution-based clustering offer higher accuracy, but it also lowers the computation time due to reduced time-complexity.}
}


@article{DBLP:journals/tkde/LiuCGWJXXW24,
	author = {Jingping Liu and
                  Tao Chen and
                  Hao Guo and
                  Chao Wang and
                  Haiyun Jiang and
                  Yanghua Xiao and
                  Xiang Xu and
                  Baohua Wu},
	title = {Exploiting Duality in Aspect Sentiment Triplet Extraction With Sequential
                  Prompting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6111--6123},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3391381},
	doi = {10.1109/TKDE.2024.3391381},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuCGWJXXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aspect sentiment triplet extraction is an important task in natural language processing. Previous work tends to focus on the interaction between the aspect and opinion, while ignoring the positive impact of sentiment on interaction within the triplet. In this paper, we propose a novel aspect sentiment triplet extraction model based on dual learning with sequential prompting. This model is designed as a bidirectional extraction framework that fully takes sentiment polarity into account in the interaction process of aspect and opinion. Besides, we introduce a dual loss as a regularization term for the extraction model to promote better learning in both directions. We further design a sequential prompting strategy to determine aspect, opinion, and sentiment polarity more accurately, which utilizes the results extracted in the previous step as prior knowledge to guide the prediction of the next target. We conduct experiments on three public datasets and the results show the effectiveness of our method. More importantly, we deploy our method on Fliggy application and the 14-day online A/B testing indicates that Page View Click-Through Rate and Page View Conversion Rate increase by 1.17% and 1.08% when user short reviews are used for tagging items with the help of our method.}
}


@article{DBLP:journals/tkde/LiYYLC24,
	author = {Guojie Li and
                  Zhiwen Yu and
                  Kaixiang Yang and
                  Mianfen Lin and
                  C. L. Philip Chen},
	title = {Exploring Feature Selection With Limited Labels: {A} Comprehensive
                  Survey of Semi-Supervised and Unsupervised Approaches},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6124--6144},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397878},
	doi = {10.1109/TKDE.2024.3397878},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiYYLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature selection is a highly regarded research area in the field of data mining, as it significantly enhances the efficiency and performance of high-dimensional data analysis by eliminating redundant and irrelevant features. Despite the ease of data acquisition, labeling data remains a laborious and expensive task. To leverage the abundance of unlabeled data, researchers have proposed various feature selection methods that operate with limited labels, including semi-supervised feature selection and unsupervised feature selection. However, a comprehensive review encompassing a thorough overview of feature selection algorithms with limited labels is lacking. To bridge this gap, this paper conducts a comprehensive exploration of feature selection methods specifically tailored to limited-label scenarios. These methods are systematically classified into two primary categories: semi-supervised and unsupervised feature selection. Additionally, by introducing a novel taxonomy and discussing future challenges, this survey aims to provide researchers with a comprehensive and in-depth understanding of feature selection in limited-label scenarios. Moreover, it aims to offer valuable insights that can guide further research and development in this domain.}
}


@article{DBLP:journals/tkde/ZhangQSG24,
	author = {Jian Zhang and
                  Lei Qi and
                  Yinghuan Shi and
                  Yang Gao},
	title = {Exploring Flat Minima for Domain Generalization With Large Learning
                  Rates},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6145--6158},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3392980},
	doi = {10.1109/TKDE.2024.3392980},
	timestamp = {Sun, 19 Jan 2025 13:53:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangQSG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Domain Generalization (DG) aims to generalize to arbitrary unseen domains. A promising approach to improve model generalization in DG is the identification of flat minima. One typical method for this task is SWAD, which involves averaging weights along the training trajectory. However, the success of weight averaging depends on the diversity of weights, which is limited when training with a small learning rate. Instead, we observe that leveraging a large learning rate can simultaneously promote weight diversity and facilitate the identification of flat regions in the loss landscape. However, employing a large learning rate suffers from the convergence problem, which cannot be resolved by simply averaging the training weights. To address this issue, we introduce a training strategy called Lookahead which involves the weight interpolation, instead of average, between fast and slow weights. The fast weight explores the weight space with a large learning rate, which is not converged while the slow weight interpolates with it to ensure the convergence. Besides, weight interpolation also helps identify flat minima by implicitly optimizing the local entropy loss that measures flatness. To further prevent overfitting during training, we propose two variants to regularize the training weight with weighted averaged weight or with accumulated history weight. Taking advantage of this new perspective, our methods achieve state-of-the-art performance on both classification and semantic segmentation domain generalization benchmarks.}
}


@article{DBLP:journals/tkde/PanL24,
	author = {James Jie Pan and
                  Guoliang Li},
	title = {Fast and Scalable Ridesharing Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6159--6170},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3418433},
	doi = {10.1109/TKDE.2024.3418433},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PanL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the next few decades, it is estimated that a quarter of all trips worldwide will be served by shared mobility supported in part by lower carbon footprint compared to private mobility. In particular, on-demand ridesharing is appealing due to its convenience, matching passengers needing rides to vehicles in real time while optimizing the matching. While this matching problem is computationally challenging, the state-of-art greedy search algorithm assigns passengers one at a time to the locally best vehicle and has been shown to perform well in practice. However, in order to scale the algorithm, how to parallelize searches for multiple requests remains challenging due to contention for vehicle tours. Moreover, the request latency may still be too high for on-demand requests. In this paper, we give several techniques to speed up and scale out ridesharing search. To deal with data contention while scaling out greedy search, we introduce a “map-release” and ticketing system that sacrifices read-write consistency to achieve high concurrency, even under high contention, and while avoiding expensive aborts incurred by optimistic approaches. To address high request latency, we give a caching technique to speed up the tour expansion subroutine of greedy search, and we also give a pruning technique to reduce the tour candidates even further compared to existing techniques. Together, these techniques deliver around 7x the throughput and order of magnitude lower latency on a real instance compared to the “embarassingly parallel” parallelized map approach and with better scalability.}
}


@article{DBLP:journals/tkde/WangTZLZZZ24,
	author = {Jun Wang and
                  Chang Tang and
                  Xiao Zheng and
                  Xinwang Liu and
                  Wei Zhang and
                  En Zhu and
                  Xinzhong Zhu},
	title = {Fast Approximated Multiple Kernel K-Means},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6171--6180},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3340743},
	doi = {10.1109/TKDE.2023.3340743},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangTZLZZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multiple Kernel Clustering (MKC) has emerged as a prominent research domain in recent decades due to its capacity to exploit diverse information from multiple views by learning an optimal kernel. Despite the successes achieved by various MKC methods, a significant challenge lies in the computational complexity associated with generating a consensus partition from the optimal kernel matrix, typically of size n \\times n, where n represents the number of samples. This computational bottleneck restricts the practical applicability of these methods when confronted with large-scale datasets. Furthermore, certain existing MKC algorithms derive the consensus partition matrix by fusing all base partitions. However, this fusion process may inadvertently overlook critical information embedded in individual base kernels, potentially leading to inferior clustering performance. In light of these challenges, we introduce an innovative and efficient multiple kernel k-means approach, denoted as FAMKKM. Notably, FAMKKM incorporates two approximated partition matrices instead of the original individual partition matric for each base kernel. This strategic substitution significantly reduces computational complexity. Additionally, FAMKKM leverages the original kernel information to guide the fusion of all base partitions, thereby enhancing the quality of the resulting consensus partition matrix. Finally, we substantiate the efficacy and efficiency of the proposed FAMKKM through extensive experiments conducted on six benchmark datasets. Our results demonstrate its superiority over state-of-the-art methods.}
}


@article{DBLP:journals/tkde/ZhangZW24,
	author = {Zhiqiang Zhang and
                  Dandan Zhang and
                  Yun Wang},
	title = {Fast Long Sequence Time-Series Forecasting for Edge Service Running
                  State Based on Data Drift and Non-Stationarity},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6181--6194},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3390135},
	doi = {10.1109/TKDE.2024.3390135},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The operational state of edge services in remote areas and complex geographical work environments is influenced by scarce hardware resources, unstable communication state and external dynamic environment. This leads to the continuous evolution of reliability data streams of edge services in the form of data drift, producing non-stationary data streams with substantial amounts of noise data, resulting in the difficulty of service operational status prediction and low efficiency. Previous studies mainly utilize stationarity-based methods to attenuate the non-stationarity of original sequence, aiming to enhance predictability using deep learning models. However, stationary series deprived of their inherent non-stationarity struggle to accurately forecast practical emergencies. Furthermore, the size and computational complexity of deep learning models are unsuitable for deployment at the edge. To accurately predict the reliability of edge services in dynamic environments, we propose a lightweight and fast long sequence time-series forecasting method based on data drift and non-stationarity, named FSNet. FSNet introduces a non-stationary information sampling factor to extract external factors influencing data flows and incorporates Moore-Penrose inverse matrix to swiftly update the model weights during runtime. Combining unmanned aerial vehicles as mobile edge servers with edge computing offloading achieves collaborative computation of the model, thereby alleviating the scarcity of resources at the edge, enhancing computational efficiency, and enabling fast and reliable prediction of service operations. Extensive experimental results validate the effectiveness and efficiency of the FSNet.}
}


@article{DBLP:journals/tkde/XieWGP24,
	author = {Juanying Xie and
                  Mingzhao Wang and
                  Philip W. Grant and
                  Witold Pedrycz},
	title = {Feature Selection With Discernibility and Independence Criteria},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6195--6209},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3388526},
	doi = {10.1109/TKDE.2024.3388526},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XieWGP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature selection plays a significant role in data mining and machine learning. It is challenging to determine how many features are necessary to form an optimal feature subset. To address this challenge, an innovative visual 2D feature selection framework is introduced, in which the feature discernibility and independence are defined to evaluate its capability for classification and its relevance to other features, respectively. All features are represented in 2D space with discernibility as x-axis and independence as y-axis. The features located in the upper right corner represent high discernibility and high independence, so comprise the optimal feature subset. This leads to the formation of a family of feature selection algorithms. Three such algorithms are proposed in this paper referred to as FSDIE, FSDIR, and FSDIS (Feature Selection based on the Discernibility and the Independence, respectively, of Exponent, Reciprocal, and anti-Similarity). To speed-up these three algorithms, a clustering based feature preselection first eliminates some unrelated and redundant features. Extensive experiments on UCI datasets, face datasets and gene expression datasets demonstrate that these three 2D feature selection algorithms are superior to the state-of-the-art methods indicating the power of our 2D feature selection framework.}
}


@article{DBLP:journals/tkde/GuoXWWWJ24,
	author = {Yu Guo and
                  Yuxin Xi and
                  Haodi Wang and
                  Mingyue Wang and
                  Cong Wang and
                  Xiaohua Jia},
	title = {FedEDB: Building a Federated and Encrypted Data Store via Consortium
                  Blockchains},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6210--6224},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3341149},
	doi = {10.1109/TKDE.2023.3341149},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GuoXWWWJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decentralized storage platforms based on consortium blockchains have emerged in the spotlight of research and industry communities because they are flexible, transparent, and eliminated trust in contrast to the traditional centralized data-sharing model. However, due to wide attacking surfaces in a blockchain network, this decentralized data-sharing paradigm is subject to malicious data breaches. Untrusted blockchain nodes can directly obtain sensitive information from the query processing and their local storage. Several studies have been made for solving this dilemma, but they only focus on single-user settings and cannot be directly applied to multi-owners blockchain-based data sharing scenarios. In this paper, we introduce FedEDB, a federated and encrypted data store by using consortium blockchains. Unlike existing solutions that focus on single-user settings, our proposed schemes can efficiently support privacy-preserving and reliable multi-owner queries in the decentralized setting. We start from the practical key aggregation technique to construct the multi-owner search schemes and further refine the underling building blocks to enhance the security. Besides, we integrate the smart contract with our tailored zero-knowledge proof to enforce secure and reliable result verification protocol with fairness. We implement a prototype and thorough security analysis and comprehensive evaluation results confirm the practicability of our design.}
}


@article{DBLP:journals/tkde/LiBPYLJH24,
	author = {Jun Li and
                  Yi Bin and
                  Liang Peng and
                  Yang Yang and
                  Yangyang Li and
                  Hao Jin and
                  Zi Huang},
	title = {Focusing on Relevant Responses for Multi-Modal Rumor Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6225--6236},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3389694},
	doi = {10.1109/TKDE.2024.3389694},
	timestamp = {Tue, 24 Dec 2024 22:38:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiBPYLJH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the absence of an official statement about a rumor, people may expose the truth behind such rumor through their responses on social media. Due to the varying relevance of responses in exposing hidden suspicious points within a rumor claim, it is crucial to prioritize those with higher relevance, rather than considering every responding tweets. As for the multi-modal rumor detection, an effective approach for evaluating relevance is aligning responses with the different modalities of the rumor claim in a fine-grained manner. However, owing to the substantial volume of response tweets, it is both costly and redundant to align all responses with the multi-modal claim. In this paper, we propose a novel two-stage model, termed Focal Reasoning Model (FoRM), to select critical responses for multi-modal rumor detection. More specifically, our FoRM consists of two primary elements: coarse-grained selection and fine-grained reasoning. The coarse-grained selection component employs post-level features of responses to initialize a relevant score for each. Based on these scores, we preserve the responses with higher scores as the candidate ones for subsequent reasoning. Within the fine-grained reasoning component, we develop a relation attention module to investigate fine-grained relationships, specifically token-to-token and token-to-object connections, between the preserved responses and the multi-modal claim, with the goal of discovering valuable clues. Extensive experiments have been conducted on three real-world datasets, and the results demonstrate that our proposed model outperforms all the baselines.}
}


@article{DBLP:journals/tkde/YuLFLCZ24,
	author = {Xingtong Yu and
                  Zhenghao Liu and
                  Yuan Fang and
                  Zemin Liu and
                  Sihong Chen and
                  Xinming Zhang},
	title = {Generalized Graph Prompt: Toward a Unification of Pre-Training and
                  Downstream Tasks on Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6237--6250},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419109},
	doi = {10.1109/TKDE.2024.3419109},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuLFLCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks (GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily relies on a large amount of task-specific supervision. To reduce labeling requirement, the “pre-train, fine-tune” and “pre-train, prompt” paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. In particular, GraphPrompt adopts simple yet effective designs in both pre-training and prompt tuning: During pre-training, a link prediction-based task is used to materialize the task template; during prompt tuning, a learnable prompt vector is applied to the ReadOut layer of the graph encoder. To further enhance GraphPrompt in these two stages, we extend it into GraphPrompt+ with two major enhancements. First, we generalize a few popular graph pre-training tasks beyond simple link prediction to broaden the compatibility with our task template. Second, we propose a more generalized prompt design that incorporates a series of prompt vectors within every layer of the pre-trained graph encoder, in order to capitalize on the hierarchical information across different layers beyond just the readout layer. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt and GraphPrompt+.}
}


@article{DBLP:journals/tkde/ChangLS24,
	author = {Zhao Chang and
                  Feifei Li and
                  Yulong Shen},
	title = {Generalized Measure-Biased Sampling and Priority Sampling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6251--6265},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3340673},
	doi = {10.1109/TKDE.2023.3340673},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChangLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query with aggregates is one of the most important classes of ad-hoc queries. Since query response time is critical in many scenarios, small errors are usually tolerable for query processing. In this work, we adopt sampling to provide fast approximate answers to distribution query and subset-sum query. On the one hand, uniform sampler is sub-optimal. On the other hand, both measure-biased sampler and priority sampler need to create a sample for each measure column. It leads to expensive storage cost, when there are dozens or hundreds of measure columns in the table. To address this issue, we generalize both measure-biased sampler and priority sampler, which can compress the samples but still provide fast approximate answers to both distribution query and subset-sum query within a user-specified error bound. Besides, we establish the relationship between measure-biased sampler and priority sampler by constructing a measure-biased sample from a priority sample. We also extend the priority sampler to support multiple types of aggregates for arbitrary subset. In the extensive experimental evaluation, our generalized samplers achieve a remarkable improvement over the original samplers in terms of the error metrics.}
}


@article{DBLP:journals/tkde/JiangTL24,
	author = {Xin Jiang and
                  Hao Tang and
                  Zechao Li},
	title = {Global Meets Local: Dual Activation Hashing Network for Large-Scale
                  Fine-Grained Image Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6266--6279},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3393512},
	doi = {10.1109/TKDE.2024.3393512},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiangTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Internet era, the exponential growth of fine-grained image databases poses a considerable challenge for efficient information retrieval. Hashing-based approaches gained traction for their computational and storage efficiency, yet fine-grained hashing retrieval presents unique challenges due to small inter-class and large intra-class variations inherent to fine-grained entities. Thus, traditional hashing algorithms falter in discerning these subtle, yet critical, visual differences and fail to generate compact yet semantically rich hash codes. To address this, we introduce a Dual Activation Hashing Network (DAHNet) designed to convert high-dimensional image data into optimized binary codes via an innovative feature activation paradigm. The architecture consists of dual branches specifically tailored for global and local semantic activation, thereby establishing direct correspondences between hash codes and distinguishable object parts through a hierarchical activation pipeline. Specifically, our spatial-oriented semantic activation module modulates dominant visual regions while amplifying the activations of subtle yet semantically rich areas in a controlled manner. Building on these activated visual representations, the proposed inter-region semantic enrichment module further enriches them by unearthing semantically complementary cues. Concurrently, DAHNet integrates a channel-oriented semantic activation module that exploits channel-specific correlations to distill contextual cues from spatially-activated visual features, thereby reinforcing robust learning to hash. To maintain the similarity of the original entities, we amalgamate final hash codes from both activation branches, capturing both local textural details and global structural information. Comprehensive evaluations on five fine-grained image retrieval benchmarks demonstrate DAHNet's superior performance over existing state-of-the-art hashing solutions, especially on 12-bit, improving performance by 4%–15% compared to the current best results on the five benchmarks. Moreover, generalization studies validate the efficacy of our dual-activation framework in the domain of content-based fine-grained image retrieval.}
}


@article{DBLP:journals/tkde/ChenLYL24,
	author = {Yibi Chen and
                  Kenli Li and
                  Chai Kiat Yeo and
                  Keqin Li},
	title = {Global-Local Feature Learning via Dynamic Spatial-Temporal Graph Neural
                  Network in Meteorological Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6280--6292},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397840},
	doi = {10.1109/TKDE.2024.3397840},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenLYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The meteorological environment has a profound impact on global health (e.g., air quality), science and technology (e.g., rocket launches), and economic development (e.g., poverty reduction) etc. Meteorological prediction presents numerous challenges to both academia and industry due to its multifaceted nature which encompasses real-time observations and complex modeling. Recent research adopt graph convolutional recurrent network and establish coordinate information to obtain local spatial-temporal pattern. However, the model only utilizes the local spatial-temporal information and fail to fully consider the dynamic meteorological situation. To address the above limitations, we propose a Dynamic Spatial-Temporal Graph Neural Network (DSTGNN) to learn global-local meteorological features. Specifically, we divide the global spatial-temporal information along the timeline to obtain local spatial-temporal information. For the global aspect, we design a random throwedge module during the neighborhood propagation process in graph neural network (GNN) to extract the features and adapt to the dynamic situation. We also establish convolution operation module to learn the features. Next, we perform information fusion on the two modules to capture sufficient features. In addition, we employ graph ordinary differential equation (ODE) network and utilize the coordinate information to obtain the long-term features and coordinate relationships. In the local aspect, we first construct a GNN to conduct graph embedding. Then, we integrate another GNN into a gated recurrent unit (GRU) and also use the coordinate information to explore the features and coordinate relationships. Finally, we combine the global and local features via a global-local features learning layer for meteorological prediction. Experimental results on the four real-world meteorological datasets show that DSTGNN outperforms the baseline models.}
}


@article{DBLP:journals/tkde/XiaLWGHS24,
	author = {Shuyin Xia and
                  Xiaoyu Lian and
                  Guoyin Wang and
                  Xinbo Gao and
                  Qinghua Hu and
                  Yabin Shao},
	title = {Granular-Ball Fuzzy Set and Its Implement in {SVM}},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6293--6304},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419184},
	doi = {10.1109/TKDE.2024.3419184},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XiaLWGHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional fuzzy set methods, designed around the finest granularity of inputs-individual points and their membership degrees-often struggle with inefficiencies and label noise. To overcome these challenges, we introduce granular-ball computing into the fuzzy set, creating the new granular-ball fuzzy set framework. This approach uses granular-ball inputs rather than single points, significantly reducing the number of entities and minimizing susceptibility to the noise affecting individual sample points. As a result, our framework enhances both efficiency and robustness compared to traditional methods and is applicable across various domains of fuzzy data processing. Furthermore, we apply this framework to fuzzy support vector machines (FSVMs), developing the Granular-ball Fuzzy Support Vector Machine (GBFSVM). Experimental tests on UCI benchmark datasets show that GBFSVM surpasses traditional models in efficiency and robustness.}
}


@article{DBLP:journals/tkde/ZhangTHL24,
	author = {Xin Zhang and
                  Qiaoyu Tan and
                  Xiao Huang and
                  Bo Li},
	title = {Graph Contrastive Learning With Personalized Augmentation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6305--6316},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3388728},
	doi = {10.1109/TKDE.2024.3388728},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangTHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph contrastive learning (GCL) has emerged as an effective tool to learn representations for whole graphs in the absence of labels. The key idea is to maximize the agreement between two augmented views of each graph via data augmentation. Existing GCL models mainly focus on applying identical augmentation strategies for all graphs within a given scenario. However, real-world graphs are often not monomorphic but abstractions of diverse natures. Even within the same scenario (e.g., macromolecules and online communities), different graphs might need diverse augmentations to perform effective GCL. Thus, blindly augmenting all graphs without considering their individual characteristics may undermine the performance of GCL arts. However, it is non-trivial to achieve personalized allocation for all graphs since the search space is exponential to the number of graphs. To bridge the gap, we propose the first principled framework, termed as Graph contrastive learning with Personalized Augmentation (GPA). It advances conventional GCL by allowing each graph to choose its own suitable augmentation operations. To cope with the huge search space, we design a tailored augmentation selector by converting the discrete space into a continuous one, which is a plug-and-play module and can be effectively trained with downstream GCL models end to end. Extensive experiments across 10 benchmark datasets demonstrate the superiority of GPA against state-of-the-art competitors. Moreover, by visualizing the learned augmentation distributions across different types of datasets, we show that GPA can effectively identify the most suitable augmentations for each graph based on its characteristics.}
}


@article{DBLP:journals/tkde/LuZGLFWZ24,
	author = {Bin Lu and
                  Ze Zhao and
                  Xiaoying Gan and
                  Shiyu Liang and
                  Luoyi Fu and
                  Xinbing Wang and
                  Chenghu Zhou},
	title = {Graph Out-of-Distribution Generalization With Controllable Data Augmentation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6317--6329},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3393109},
	doi = {10.1109/TKDE.2024.3393109},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuZGLFWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe hybrid structure distribution shift of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose OOD-GMixup to jointly manipulate the training distribution with controllable data augmentation in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Second, we generate virtual samples with perturbation on graph rationale representation domain to obtain potential OOD training samples. Finally, we propose OOD calibration to measure the distribution deviation of virtual samples by leveraging Extreme Value Theory, and further actively control the training distribution by emphasizing the impact of virtual OOD samples. Extensive studies on several real-world datasets on graph classification demonstrate the superiority of our proposed method over state-of-the-art baselines.}
}


@article{DBLP:journals/tkde/ShenLYYZP24,
	author = {Xu Shen and
                  Pietro Li{\`{o}} and
                  Lintao Yang and
                  Ru Yuan and
                  Yuyang Zhang and
                  Chengbin Peng},
	title = {Graph Rewiring and Preprocessing for Graph Neural Networks Based on
                  Effective Resistance},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6330--6343},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397692},
	doi = {10.1109/TKDE.2024.3397692},
	timestamp = {Wed, 22 Jan 2025 15:48:14 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ShenLYYZP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) are powerful models for processing graph data and have demonstrated state-of-the-art performance on many downstream tasks. However, existing GNNs can generally suffer from two limitations: over-smoothing and over-squashing, which can significantly undermine their learning ability for large graphs. To overcome these issues simultaneously, by utilizing the concept of effective resistances, we focus on minimizing total constrained resistance while identifying problematic edges using topological redundancy and bottleneck sparsity coefficients. We introduce a novel graph rewiring and preprocessing method guided by effective resistance (GPER), capable of edge addition or removal. Theoretical analysis validates our method's efficacy in mitigating over-smoothing and over-squashing. In the experiments, we conduct node and graph classifications on the benchmark datasets and can achieve an average improvement of 7.8% and 2.0%, respectively. We also conduct scalability analysis on large graphs with GCN and demonstrate that the proposed preprocess approach can reduce graph size by over 50% while improve the performance.}
}


@article{DBLP:journals/tkde/WangZCWPLWY24,
	author = {Haibo Wang and
                  Chuan Zhou and
                  Xin Chen and
                  Jia Wu and
                  Shirui Pan and
                  Zhao Li and
                  Jilong Wang and
                  Philip S. Yu},
	title = {Graph Structure Reshaping Against Adversarial Attacks on Graph Neural
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6344--6357},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3403925},
	doi = {10.1109/TKDE.2024.3403925},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangZCWPLWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have achieved impressive performance in many tasks on graph data. Recent studies show that they are vulnerable to adversarial attacks. Deliberate and unnoticeable perturbations on topology structure could render them near-useless in applications. How to design effective methods to improve the robustness of GNNs is a crucial problem. To solve this problem, some works attempt to design more robust GNN models, while others attempt to remove perturbations from the poisoned graph. Different from the previous works, this paper proposes a general framework termed as GraphReshape to enhance the robustness of GNNs via directly correcting the shifted classification boundary of GNN models in the presence of adversarial attacks. GraphReshape consists of two modules: locating tractive nodes that could correct GNNs and reshaping local structure to improve their representations in the latent space. Extensive experiments on four real-world datasets show that GraphReshape achieves significant performance gain compared with state-of-the-art baselines against different adversarial attacks.}
}


@article{DBLP:journals/tkde/YangHZLHZYCZ24,
	author = {Ling Yang and
                  Zhilin Huang and
                  Zhilong Zhang and
                  Zhongyi Liu and
                  Shenda Hong and
                  Wentao Zhang and
                  Wenming Yang and
                  Bin Cui and
                  Luxia Zhang},
	title = {Graphusion: Latent Diffusion for Graph Generation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6358--6369},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3389783},
	doi = {10.1109/TKDE.2024.3389783},
	timestamp = {Tue, 11 Feb 2025 20:50:07 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangHZLHZYCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph generation is a fundamental task in machine learning with broad impacts on numerous real-world applications such as biomedical discovery and social science. Most recently, generative models, especially diffusion models (DMs), have shown great promise in synthesizing realistic graphs. However, existing DMs methods typically conduct diffusion processes directly in complex graph space (i.e., node feature, adjacency matrix, or both), resulting in high modeling complexity and poor multimodal distribution coverage. In this paper, we propose Graphusion, a novel and unified latent-based graph generative framework to address the problems. Specifically, Graphusion is composed of a variational graph autoencoder mapping raw graphs with high-dimensional discrete space to low-dimensional topology-injected latent space, and latent DMs running there, producing a smoother, faster, and more expressive graph generation procedure. Thanks to the latest space modeling, we further develop principled latent self-guidance to sufficiently cover the whole semantical distribution of the unlabeled graph set. Experiments show that our Graphusion framework can consistently outperform previous graph generation baselines on both generic and molecular graph datasets, demonstrating the generality and extensibility along with further analytical justifications.}
}


@article{DBLP:journals/tkde/BaiCWLLYH24,
	author = {Lu Bai and
                  Lixin Cui and
                  Yue Wang and
                  Ming Li and
                  Jing Li and
                  Philip S. Yu and
                  Edwin R. Hancock},
	title = {{HAQJSK:} Hierarchical-Aligned Quantum Jensen-Shannon Kernels for
                  Graph Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6370--6384},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3389966},
	doi = {10.1109/TKDE.2024.3389966},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BaiCWLLYH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we propose two novel quantum walk kernels, namely the Hierarchical Aligned Quantum Jensen-Shannon Kernels (HAQJSK), between un-attributed graph structures. Different from most classical graph kernels, the proposed HAQJSK kernels can incorporate hierarchical aligned structure information between graphs and transform graphs of random sizes into fixed-size aligned graph structures, i.e., the Hierarchical Transitive Aligned Adjacency Matrix of vertices and the Hierarchical Transitive Aligned Density Matrix of the Continuous-Time Quantum Walks (CTQW). With pairwise graphs to hand, the resulting HAQJSK kernels are defined by computing the Quantum Jensen-Shannon Divergence (QJSD) between their transitive aligned graph structures. We show that the proposed HAQJSK kernels not only reflect richer intrinsic whole graph characteristics in terms of the CTQW, but also address the drawback of neglecting structural correspondence information that arises in most R-convolution graph kernels. Moreover, unlike the previous QJSD based graph kernels associated with the QJSD and the CTQW, the proposed HAQJSK kernels can simultaneously guarantee the properties of permutation invariant and positive definiteness, explaining the theoretical advantages of the HAQJSK kernels. The experiment indicates the effectiveness of the new proposed kernels.}
}


@article{DBLP:journals/tkde/LiLWSSLYWGZ24,
	author = {Jiaqi Li and
                  Yuanhao Lai and
                  Rui Wang and
                  Changjian Shui and
                  Sabyasachi Sahoo and
                  Charles X. Ling and
                  Shichun Yang and
                  Boyu Wang and
                  Christian Gagn{\'{e}} and
                  Fan Zhou},
	title = {Hessian Aware Low-Rank Perturbation for Order-Robust Continual Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6385--6396},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419449},
	doi = {10.1109/TKDE.2024.3419449},
	timestamp = {Wed, 05 Mar 2025 20:49:29 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiLWSSLYWGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continual learning aims to learn a series of tasks sequentially without forgetting the knowledge acquired from the previous ones. In this work, we propose the Hessian Aware Low-Rank Perturbation algorithm for continual learning. By modeling the parameter transitions along the sequential tasks with the weight matrix transformation, we propose to apply the low-rank approximation on the task-adaptive parameters in each layer of the neural networks. Specifically, we theoretically demonstrate the quantitative relationship between the Hessian and the proposed low-rank approximation. The approximation ranks are then globally determined according to the marginal change of the empirical loss estimated by the layer-specific gradient and low-rank approximation error. Furthermore, we control the model capacity by pruning less important parameters to diminish the parameter growth. We conduct extensive experiments on various benchmarks, including a dataset with large-scale tasks, and compare our method against some recent state-of-the-art methods to demonstrate the effectiveness and scalability of our proposed method. Empirical results show that our method performs better on different benchmarks, especially in achieving task order robustness and handling the forgetting issue.}
}


@article{DBLP:journals/tkde/LiuFLLLZTJ24,
	author = {Jiang Liu and
                  Hao Fei and
                  Fei Li and
                  Jingye Li and
                  Bobo Li and
                  Liang Zhao and
                  Chong Teng and
                  Donghong Ji},
	title = {{TKDP:} Threefold Knowledge-Enriched Deep Prompt Tuning for Few-Shot
                  Named Entity Recognition},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6397--6409},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3389650},
	doi = {10.1109/TKDE.2024.3389650},
	timestamp = {Sun, 19 Jan 2025 13:53:49 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuFLLLZTJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Few-shot named entity recognition (NER) exploits limited annotated instances to identify named mentions. Effectively transferring the internal or external resources thus becomes the key to few-shot NER. While the existing prompt tuning methods have shown remarkable few-shot performances, they still fail to make full use of knowledge. In this work, we investigate the integration of rich knowledge to prompt tuning for stronger few-shot NER. We propose incorporating the deep prompt tuning framework with threefold knowledge (namely TKDP), including the internal 1) context knowledge and the external 2) label knowledge & 3) sememe knowledge. TKDP encodes the three feature sources and incorporates them into soft prompt embeddings, which are further injected into an existing pre-trained language model to facilitate predictions. On five benchmark datasets, the performance of our knowledge-enriched model was boosted by at most 11.53% F1 over the raw deep prompt method, and it significantly outperforms 9 strong-performing baseline systems in 5-/10-/20-shot settings, showing great potential in few-shot NER. Our TKDP framework can be broadly adapted to other few-shot tasks without much effort.}
}


@article{DBLP:journals/tkde/ZhangLZZF24,
	author = {Chao Zhang and
                  Guoliang Li and
                  Jintao Zhang and
                  Xinning Zhang and
                  Jianhua Feng},
	title = {{HTAP} Databases: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6410--6429},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3389693},
	doi = {10.1109/TKDE.2024.3389693},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLZZF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since Gartner coined the term, Hybrid Transactional and Analytical Processing (HTAP), numerous HTAP databases have been proposed to combine transactions with analytics in order to enable real-time data analytics for various data-intensive applications. HTAP databases typically process the mixed workloads of transactions and analytical queries in a unified system by leveraging both a row store and a column store. As there are different storage architectures and processing techniques to satisfy various requirements of diverse applications, it is critical to summarize the pros and cons of these key techniques. This paper offers a comprehensive survey of HTAP databases. We mainly classify state-of-the-art HTAP databases according to four storage architectures: (a) Primary Row Store and In-Memory Column Store; (b) Distributed Row Store and Column Store Replica; (c) Primary Row Store and Distributed In-Memory Column Store; and (d) Primary Column Store and Delta Row Store. We then review the key techniques in HTAP databases, including hybrid workload processing, data organization, data synchronization, query optimization, and resource scheduling. We also discuss existing HTAP benchmarks. Finally, we provide the research challenges and opportunities for HTAP techniques.}
}


@article{DBLP:journals/tkde/ZhouHLZZ24,
	author = {Xinjie Zhou and
                  Kai Huang and
                  Lei Li and
                  Mengxuan Zhang and
                  Xiaofang Zhou},
	title = {I/O-Efficient Multi-Criteria Shortest Paths Query Processing on Large
                  Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6430--6446},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3386906},
	doi = {10.1109/TKDE.2024.3386906},
	timestamp = {Mon, 09 Dec 2024 22:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouHLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shortest path computation is a basic operation for many graph-based applications and has been extensively studied. However, most existing works only consider the optimal path of a single criterion but ignore real-world situations involving multiple criteria. This paper investigates a new Multi-Criteria Shortest Paths (MCSPs) problem, aiming to compute the shortest paths of all criteria between a vertex pair. It is significant for real-world applications such as GPS navigation and social network analysis. Nevertheless, the rapid growth of graph size or memory-limited devices poses a memory-constraint challenge, making the adaptation of existing methods extremely time-consuming. To solve the memory-constraint MCSPs problem, we propose a general STOP & SHARE scheme to synchronize the search speeds of all criteria for sharing partition accesses. Two algorithms called OHP and MHP, adopting the one-hop strategy and partition exhaustive strategy, respectively, are proposed for implementing our scheme. Moreover, we develop two optimized algorithms, BMHP and BMHPS, to improve query efficiency by combining MHP with the bidirectional technique and a novel in-partition shortcut optimization. We also investigate partition-oriented I/O management. Experimental studies on large real-world graphs demonstrate the effectiveness of the proposed methods over the multi-pass adaptations of the existing methods.}
}


@article{DBLP:journals/tkde/WangJGZW24,
	author = {Lingzhi Wang and
                  Shafiq Joty and
                  Wei Gao and
                  Xingshan Zeng and
                  Kam{-}Fai Wong},
	title = {Improving Conversational Recommender System Via Contextual and Time-Aware
                  Modeling With Less Domain-Specific Knowledge},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6447--6461},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397321},
	doi = {10.1109/TKDE.2024.3397321},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangJGZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conversational Recommender Systems (CRS) has become an emerging research topic seeking to perform recommendations through interactive conversations, which generally consist of generation and recommendation modules. Prior work on CRS tends to incorporate more external and domain-specific knowledge like item reviews to enhance performance. Despite the fact that the collection and annotation of the external domain-specific information needs much human effort and degenerates the generalizability, too much extra knowledge introduces more difficulty to balance among them. Therefore, we propose to fully discover and extract the internal knowledge from the context. We capture both entity-level and contextual-level representations to jointly model user preferences for the recommendation, where a time-aware attention is designed to emphasize the recently appeared items in entity-level representations. We further use the pre-trained BART to initialize the generation module to alleviate the data scarcity and enhance the context modeling. In addition to conducting experiments on a popular dataset (ReDial), we also include a multi-domain dataset (OpenDialKG) to show the effectiveness of our model. Experiments on both datasets show that our model achieves better performance on most evaluation metrics with less external knowledge and generalizes well to other domains. Additional analyses on the recommendation and generation tasks demonstrate the effectiveness of our model in different scenarios.}
}


@article{DBLP:journals/tkde/FengWHNL24,
	author = {Wenjie Feng and
                  Li Wang and
                  Bryan Hooi and
                  See{-}Kiong Ng and
                  Shenghua Liu},
	title = {Interrelated Dense Pattern Detection in Multilayer Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6462--6476},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3398683},
	doi = {10.1109/TKDE.2024.3398683},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FengWHNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a heterogeneous multilayer network with various connections in pharmacology, how can we detect components with intensive interactions and strong dependencies? Can we accurately capture suspicious groups in a multi-lot transaction network under camouflage? These challenges related to dense subgraph detection have been extensively studied in simple graphs (such as bipartite graph, multi-view network) but remain under-explored on complex networks. Existing methods struggle to effectively handle the intricate dependencies, let alone accurately identify the interrelated dense connected patterns within a series of complex heterogeneous networks. In this paper, we propose InDuen, a novel algorithm designed to detect interrelated densest subgraphs in multilayer networks through joint optimization of coupled factorization and local search for an elaborate-designed joint density measure. It is (a) effective for both large synthetic and real networks, (b) resistant to camouflage for anomaly detection, and (c) linearly scalable. Experimental results demonstrate that InDuen outperforms the state-of-the-art baselines in accurately detecting interrelated densest subgraphs under various settings. Furthermore, InDuen uncovers some intriguing patterns in real-world data, i.e., closely cooperated academic groups and interrelated dependent functional components in biology-net. InDuen achieves more than 35 \\times speedup compared to the SOTA method Destine.}
}


@article{DBLP:journals/tkde/ShaoLGYX24,
	author = {Feifei Shao and
                  Yawei Luo and
                  Fei Gao and
                  Yi Yang and
                  Jun Xiao},
	title = {Knowledge-Guided Causal Intervention for Weakly-Supervised Object
                  Localization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6477--6489},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3389668},
	doi = {10.1109/TKDE.2024.3389668},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShaoLGYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Previous weakly-supervised object localization (WSOL) methods aim to expand activation map discriminative areas to cover the whole objects, yet neglect two inherent challenges when relying solely on image-level labels. First, the “entangled context” issue arises from object-context co-occurrence (e.g., fish and water), making the model inspection hard to distinguish object boundaries clearly. Second, the “C-L dilemma” issue results from the information decay caused by the pooling layers, which struggle to retain both the semantic information for precise classification and those essential details for accurate localization, leading to a trade-off in performance. In this paper, we propose a knowledge-guided causal intervention method, dubbed KG-CI-CAM, to address these two under-explored issues in one go. More specifically, we tackle the co-occurrence context confounder problem via causal intervention, which explores the causalities among image features, contexts, and categories to eliminate the biased object-context entanglement in the class activation maps. Based on the disentangled object feature, we introduce a multi-source knowledge guidance framework to strike a balance between absorbing classification knowledge and localization knowledge during model training. Extensive experiments conducted on several benchmark datasets demonstrate the effectiveness of KG-CI-CAM in learning distinct object boundaries amidst confounding contexts and mitigating the dilemma between classification and localization performance.}
}


@article{DBLP:journals/tkde/ChenCCSS24,
	author = {Xinyu Chen and
                  Zhanhong Cheng and
                  HanQin Cai and
                  Nicolas Saunier and
                  Lijun Sun},
	title = {Laplacian Convolutional Representation for Traffic Time Series Imputation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6490--6502},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419698},
	doi = {10.1109/TKDE.2024.3419698},
	timestamp = {Fri, 21 Feb 2025 10:22:55 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenCCSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatiotemporal traffic data imputation is of great significance in intelligent transportation systems and data-driven decision-making processes. To perform efficient learning and accurate reconstruction from partially observed traffic data, we assert the importance of characterizing both global and local trends in time series. In the literature, substantial works have demonstrated the effectiveness of utilizing the low-rank property of traffic data by matrix/tensor completion models. In this study, we first introduce a Laplacian kernel to temporal regularization for characterizing local trends in traffic time series, which can be formulated as a circular convolution. Then, we develop a low-rank Laplacian convolutional representation (LCR) model by putting the circulant matrix nuclear norm and the Laplacian kernelized temporal regularization together, which is proved to meet a unified framework that has a fast Fourier transform (FFT) solution in log-linear time complexity. Through extensive experiments on several traffic datasets, we demonstrate the superiority of LCR over several baseline models for imputing traffic time series of various time series behaviors (e.g., data noises and strong/weak periodicity) and reconstructing sparse speed fields of vehicular traffic flow. The proposed LCR model is also an efficient solution to large-scale traffic data imputation over the existing imputation models.}
}


@article{DBLP:journals/tkde/YangJLWGZ24,
	author = {Hao Yang and
                  Youzhi Jin and
                  Ziyin Li and
                  Deng{-}Bao Wang and
                  Xin Geng and
                  Min{-}Ling Zhang},
	title = {Learning From Noisy Labels via Dynamic Loss Thresholding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6503--6516},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3313604},
	doi = {10.1109/TKDE.2023.3313604},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangJLWGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Numerous researches have proved that deep neural networks (DNNs) can fit almost everything even given data with noisy labels, and result in poor generalization performance. However, recent studies suggest that DNNs tend to gradually memorize the data, moving from correct data to mislabeled data. Inspired by this finding, we propose a novel method named Dynamic Loss Thresholding (DLT). During the training process, DLT records the loss value of each sample and calculates dynamic loss thresholds. Specifically, DLT compares the loss value of each sample with the current loss threshold. Samples with smaller losses can be considered as clean samples with higher probability and vice versa. Then, DLT discards the potentially corrupted labels and further leverages self-training semi-supervised learning techniques. Experiments on CIFAR-10/100, WebVision and Clothing1M demonstrate substantial improvements over recent state-of-the-art methods. In addition, we investigate two real-world problems. First, we propose a novel approach to estimate the noise rates of datasets based on the loss difference between the early and late training stages of DNNs. Second, we explore the effect of hard samples (which are difficult to be distinguished) on the process of learning from noisy labels.}
}


@article{DBLP:journals/tkde/XieDZZGW24,
	author = {Kun Xie and
                  Xiangyu Dong and
                  Yusong Zhang and
                  Xingyi Zhang and
                  Qintian Guo and
                  Sibo Wang},
	title = {Learning-Based Attribute-Augmented Proximity Matrix Factorization
                  for Attributed Network Embedding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6517--6531},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3385847},
	doi = {10.1109/TKDE.2024.3385847},
	timestamp = {Mon, 10 Feb 2025 09:31:15 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XieDZZGW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a graph \\mathcal{G} with a set of attributes, the attributed network embedding (ANE) aims to learn low-dimensional representations of nodes that preserve both graph topology and node attribute proximity. ANE is shown to be more effective than plain network embedding methods (using only graph topology) on many graph mining tasks. However, existing ANE solutions still provide inferior performance on tasks like node classification and link prediction, as will be shown in our experiments. The key issue is that when combining graph topology and attribute information, most existing solutions take attributes with equal importance, while in real scenarios, different attribute exerts distinct influence over the network due to the heterogeneous nature among attributes. Motivated by this, we present LATAM, a learning-based framework for ANE via trainable proximity matrix factorization. To capture the node-attribute relationships, we first construct the attribute-augmented graph by adding attribute nodes (resp. edges) to the original graph. Then, we define the attribute-augmented random walk and proximity on the attribute-augmented graph, where the weights of different attributes can be learned automatically by our designed loss functions so that more indicative attributes tend to have higher weights, imposing a higher impact on the node connectivity. To achieve this, we incorporate a differentiable SVD to back-propagate gradients of attribute weights in an end-to-end process. To scale our LATAM to large graphs, we further propose sampling techniques to learn attribute weights and an efficient attribute-augmented push algorithm to compute the proximity matrix. Extensive experiments on 8 public attributed networks against 11 existing methods show the effectiveness of our LATAM.}
}


@article{DBLP:journals/tkde/ZhangJHZW24,
	author = {Hanbing Zhang and
                  Yinan Jing and
                  Zhenying He and
                  Kai Zhang and
                  X. Sean Wang},
	title = {Learning-Based Sample Tuning for Approximate Query Processing in Interactive
                  Data Exploration},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6532--6546},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3341451},
	doi = {10.1109/TKDE.2023.3341451},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangJHZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For interactive data exploration, approximate query processing (AQP) is a useful approach that usually uses samples to provide a timely response for queries by trading query accuracy. Existing AQP systems often materialize samples in the memory for reuse to speed up query processing. How to tune the samples according to the workload is one of the key problems in AQP. However, since the data exploration workload is so complex that it cannot be accurately predicted, existing sample tuning approaches cannot adapt to the changing workload very well. To address this problem, this paper proposes a deep reinforcement learning-based sample tuner, RL-STuner. When tuning samples, RL-STuner considers the workload changes from a global perspective and uses a Deep Q-learning Network (DQN) model to select an optimal sample set that has the maximum utility for the current workload. In addition, this paper proposes a set of optimization mechanisms to reduce the sample tuning cost. Experimental results on both real-world and synthetic datasets show that RL-STuner outperforms the existing sample tuning approaches and achieves 1.6×-5.2× improvements on query accuracy with a low tuning cost.}
}


@article{DBLP:journals/tkde/YuanLZZLZX24,
	author = {Zixuan Yuan and
                  Junming Liu and
                  Haoyi Zhou and
                  Denghui Zhang and
                  Hao Liu and
                  Nengjun Zhu and
                  Hui Xiong},
	title = {{LEVER:} Online Adaptive Sequence Learning Framework for High-Frequency
                  Trading},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6547--6559},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3336185},
	doi = {10.1109/TKDE.2023.3336185},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuanLZZLZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the fast development of deep learning techniques in quantitative trading. It still remains unclear how to exploit deep learning techniques to improve high-frequency trading (HFT). Indeed, there are two emerging challenges for the use of deep learning for HFT: (i) how to quantify fast-changing market conditions for tick-level signal prediction; (ii) how to establish a unified trading paradigm for different securities of diverse market conditions and severe signal sparsity. To this end, in this paper, we propose an Online Adaptive Sequence Learning (LEVER) framework, which consists of two distinct components to predict the HFT signals at the tick level for a variety of securities simultaneously. Specifically, we start with a single learner that adopts an encoder-decoder architecture for each security-based HFT signal prediction. In this single learner, an ordered encoder module first captures the variability patterns of the security's price curve by encoding the input indicator sequence from different time ranges. An unordered decoder module then outlines the pivot points of the price curve as support and resistance levels to quantify the market status. Based on the measured market condition, a prediction module further approximates the impacts of upcoming security data as the potential market momentum to detect the tick-level trading signals. To overcome the computational challenges and signal sparsity posed by online HFT for multiple securities, we develop a competitive active-meta learning paradigm to enhance the signal learners’ learning efficiency for online implementation. Finally, extensive experiments on real-world stock market data demonstrate the effectiveness of our deployed LEVER for improving the performances of the existing industry method by 0.27 in the Sharpe ratio and by 0.09% in a transaction-based return.}
}


@article{DBLP:journals/tkde/LiSLLSYZ24,
	author = {Xin{-}Chun Li and
                  Shaoming Song and
                  Yinchuan Li and
                  Bingshuai Li and
                  Yunfeng Shao and
                  Yang Yang and
                  De{-}Chuan Zhan},
	title = {{MAP:} Model Aggregation and Personalization in Federated Learning
                  With Incomplete Classes},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6560--6573},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3390041},
	doi = {10.1109/TKDE.2024.3390041},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiSLLSYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In some real-world applications, data samples are usually distributed on local devices, where federated learning (FL) techniques are proposed to coordinate decentralized clients without directly sharing users’ private data. FL commonly follows the parameter server architecture and contains multiple personalization and aggregation procedures. The natural data heterogeneity across clients, i.e., Non-I.I.D. data, challenges both the aggregation and personalization goals in FL. In this paper, we focus on a special kind of Non-I.I.D. scene where clients own incomplete classes, i.e., each client can only access a partial set of the whole class set. The server aims to aggregate a complete classification model that could generalize to all classes, while the clients are inclined to improve the performance of distinguishing their observed classes. For better model aggregation, we point out that the standard softmax will encounter several problems caused by missing classes and propose “restricted softmax” as an alternative. For better model personalization, we point out that the hard-won personalized models are not well exploited and propose “inherited private model” to store the personalization experience. Our proposed algorithm named MAP could simultaneously achieve the aggregation and personalization goals in FL. Abundant experimental studies verify the superiorities of our algorithm.}
}


@article{DBLP:journals/tkde/WeiLLGZW24,
	author = {Tonglong Wei and
                  Youfang Lin and
                  Yan Lin and
                  Shengnan Guo and
                  Lan Zhang and
                  Huaiyu Wan},
	title = {Micro-Macro Spatial-Temporal Graph-Based Encoder-Decoder for Map-Constrained
                  Trajectory Recovery},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6574--6587},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3396158},
	doi = {10.1109/TKDE.2024.3396158},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WeiLLGZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recovering intermediate missing GPS points in a sparse trajectory, while adhering to the constraints of the road network, could offer deep insights into users’ moving behaviors in intelligent transportation systems. Although recent studies have demonstrated the advantages of achieving map-constrained trajectory recovery via an end-to-end manner, they still face two significant challenges. First, existing methods are mostly sequence-based models. It is extremely hard for them to comprehensively capture the micro-semantics of individual trajectory, including the information of each GPS point and the movement between two GPS points. Second, existing approaches ignore the impact of the macro-semantics, i.e., the road conditions and the people's shared travel preferences reflected by a group of trajectories. To address the above challenges, we propose a Micro-Macro Spatial-Temporal Graph-based Encoder-Decoder (MM-STGED). Specifically, we model each trajectory as a graph to efficiently describe the micro-semantics of trajectory and design a novel message-passing mechanism to learn trajectory representations. Additionally, we extract the macro-semantics of trajectories and further incorporate them into a well-designed graph-based decoder to guide trajectory recovery. Extensive experiments conducted on sparse trajectories with three different sampling intervals that are respectively constructed from two real-world trajectory datasets demonstrate the superiority of our proposed model.}
}


@article{DBLP:journals/tkde/LanHMM24,
	author = {Zixun Lan and
                  Binjie Hong and
                  Ye Ma and
                  Fei Ma},
	title = {More Interpretable Graph Similarity Computation via Maximum Common
                  Subgraph Inference},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6588--6599},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3387044},
	doi = {10.1109/TKDE.2024.3387044},
	timestamp = {Mon, 04 Nov 2024 17:21:29 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LanHMM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph similarity measurement is a fundamental task in various graph-related applications. However, recent learning-based approaches lack interpretability as they directly transform interaction information between two graphs into a hidden vector, making it difficult to understand how the similarity score is derived. To address this issue, we propose an end-to-end paradigm for graph similarity learning called Similarity Computation via Maximum Common Subgraph Inference (INFMCS), which is more interpretable. Our key insight is that the similarity score has a strong correlation with the Maximum Common Subgraph (MCS). We implicitly infer the MCS to obtain the normalized MCS size, with only the similarity score being used as supervision information during training. To capture more global information, we stack vanilla transformer encoder layers with graph convolution layers and propose a novel permutation-invariant node Positional Encoding. Our entire model is simple yet effective. Comprehensive experiments demonstrate that INFMCS consistently outperforms state-of-the-art baselines for graph-graph classification and graph-graph regression tasks. Ablation experiments verify the effectiveness of our proposed computation paradigm and other components. Additionally, visualization and statistical analysis of results demonstrate the interpretability of INFMCS.}
}


@article{DBLP:journals/tkde/HanLL24,
	author = {Shuai Han and
                  Xianmin Liu and
                  Jianzhong Li},
	title = {MulRF: {A} Multi-Dimensional Range Filter for Sublinear Time Range
                  Query Processing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6600--6613},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397313},
	doi = {10.1109/TKDE.2024.3397313},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HanLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Range query is an important operation on big multi-dimensional data. This paper studies the problem of multi-dimensional range query filtering for speeding up the range query processing by avoiding reading the useless data. To solve the problem, a novel multi-dimensional range filter is proposed to filter the multi-dimensional range queries, while the existing one-dimensional range filters can not provide efficient filtering. Based on the multi-dimensional range filter, an efficient range query processing algorithm is presented. It can directly return the locations of the I/O units that contain the data in the query result without any access to the input dataset. The time complexity of the algorithm is O(3^{m}h), where h is the number of I/O units partially overlapping with a range query, and m is the dimension number. Since m is usually o(\\sqrt{\\log n}), it is a sublinear time algorithm if V=O(n), where n is the size of the input dataset, V=\\prod _{i=1}^{m}d_{i}, and d_{i} is the number of distinct values on the i-th dimension of the dataset for 1\\leq i\\leq m. Experimental results show that the multi-dimensional range filter has low false positive rate and good filtering efficiency. The proposed range query processing algorithm achieves at least 3\\sim7 times improvement compared to the one-dimensional filter based algorithms on different datasets.}
}


@article{DBLP:journals/tkde/YeBOATPA24,
	author = {Eric Ye and
                  Xiao Bai and
                  Neil O'Hare and
                  Eliyar Asgarieh and
                  Kapil Thadani and
                  Francisco Perez{-}Sorrosal and
                  Sujyothi Adiga},
	title = {Multilingual Taxonomic Web Page Categorization Through Ensemble Knowledge
                  Distillation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6614--6627},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3406368},
	doi = {10.1109/TKDE.2024.3406368},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YeBOATPA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Web page categorization has been extensively studied in the literature and has been successfully used to improve information retrieval, recommendation, personalization and ad targeting. With the new industry trend of not tracking users’ online behavior without their explicit permission, using contextual targeting to accurately understand web pages in order to display ads that are topically relevant to the pages becomes more important. This is challenging, however, because an ad request only contains the URL of a web page. As a result, there is very limited available text for making accurate classifications. In this paper, we propose a unified multilingual model that can seamlessly classify web pages in 5 high-impact languages using either their full content or just their URLs with limited text. We adopt multiple data sampling techniques to increase coverage for rare categories in our training corpus, and modify the loss using class-based re-weighting to smooth the influence of frequent versus rare categories. We also propose using an ensemble of teacher models for knowledge distillation and explore different ways to create a teacher ensemble. Offline evaluation shows at least 2.6% improvement in mean average precision across 5 languages compared to a URL classification model trained with single-teacher knowledge distillation. The unified model for both full-content and URL-only input further improves the mean average precision of the dedicated URL classification model by 0.6%. We launched the proposed models, which achieve at least 37% better mean average precision than the legacy tree-based models, for contextual targeting in the Yahoo Demand Side Platform, leading to a significant ad delivery and revenue increase.}
}


@article{DBLP:journals/tkde/XuGLTZ24,
	author = {Ning Xu and
                  Yifei Gao and
                  An{-}An Liu and
                  Hongshuo Tian and
                  Yongdong Zhang},
	title = {Multi-Modal Validation and Domain Interaction Learning for Knowledge-Based
                  Visual Question Answering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6628--6640},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3384270},
	doi = {10.1109/TKDE.2024.3384270},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuGLTZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge-based Visual Question Answering (KB-VQA) aims to answer the image-aware question via the external knowledge, which requires an agent to not only understand images but also explicitly retrieve and integrate knowledge facts. Intuitively, to accurately answer the question, we humans can validate the retrieved knowledge based on our memory, and then align the knowledge facts with the image regions to infer answers. However, most existing methods ignore the process of knowledge validation and alignment. In this paper, we propose the Multi-Modal Validation and Domain Interaction Learning method, which consists of two components: 1) Multi-modal validation for knowledge retrieval. We propose the multi-modal validation module (MMV) to evaluate the confidence of each retrieved knowledge fact via images and questions, which preserves knowledge candidates effective for inferring answers. 2) Domain interaction for knowledge integration. We propose the Domain Interaction TRansformer module (DI-TR) to align visual regions with knowledge facts by the interaction learning in the improved transformer. Specifically, the inter-domain and intra-domain masks are injected into each self-attention layer to control the integration scope. The proposed method outperforms several strong baselines on three widely-used knowledge-based datasets: KRVQA, OK-VQA and VQA2.0. Extensive experiments and ablation studies demonstrate the effectiveness of multi-modal knowledge validation and domain interaction learning.}
}


@article{DBLP:journals/tkde/WangLTLWL24,
	author = {Jun Wang and
                  Zhenglai Li and
                  Chang Tang and
                  Suyuan Liu and
                  Xinhang Wan and
                  Xinwang Liu},
	title = {Multiple Kernel Clustering With Adaptive Multi-Scale Partition Selection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6641--6652},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3399738},
	doi = {10.1109/TKDE.2024.3399738},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangLTLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multiple kernel clustering (MKC) enhances clustering performance by deriving a consensus partition or graph from a predefined set of kernels. Despite many advanced MKC methods proposed in recent years, the prevalent approaches involve incorporating all kernels by default to capture diverse information within the data. However, learning from all kernels may not be better than one of a few kernels, particularly since some kernels exhibit a higher proportion of noise than semantic content. Additionally, existing MKC methods, whether based on early-fusion or late-fusion approaches, predominantly rely on pairwise relationships among samples or cluster structures, neglecting potential correlations between these two aspects. To this end, we propose a multiple kernel clustering with an adaptive multi-scale partition selection method (MPS), which exploits multiple-dimensional representations and the pairwise cluster structure for clustering. By the proposed kernel selection framework, potentially harmful kernels are dynamically excluded during the kernel fusion process, and then the multi-scale partitions and similarity graphs derived from the retained kernels are utilized to facilitate the improved consensus partition generation. Finally, extensive experiments are conducted to demonstrate the effectiveness of MPS on eight benchmark datasets.}
}


@article{DBLP:journals/tkde/LiZZC24,
	author = {Dong Li and
                  Shuisheng Zhou and
                  Tieyong Zeng and
                  Raymond H. Chan},
	title = {Multi-Prototypes Convex Merging Based K-Means Clustering Algorithm},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6653--6666},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3342209},
	doi = {10.1109/TKDE.2023.3342209},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiZZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {K-Means algorithm is a popular clustering method. However, it has two limitations: 1) it gets stuck easily in spurious local minima, and 2) the number of clusters k has to be given a priori. To solve these two issues, a multi-prototypes convex merging based K-Means clustering algorithm (MCKM) is presented. First, based on the structure of the spurious local minima of the K-Means problem, a multi-prototypes sampling (MPS) is designed to select the appropriate number of multi-prototypes for data with arbitrary shapes. Then, a merging technique, called convex merging (CM), merges the multi-prototypes to get a better local minima without k being given a priori. Specifically, CM can obtain the optimal merging and estimate the correct k. By integrating these two techniques with K-Means algorithm, the proposed MCKM is an efficient and explainable clustering algorithm for escaping the undesirable local minima of K-Means problem without given k first. Two theoretical proofs are given to guarantee that the cost of MCKM (MPS+CM) can achieve a constant factor approximation to the optimal cost of the K-Means problem. Experimental results performed on synthetic and real-world data sets have verified the effectiveness of the proposed algorithm.}
}


@article{DBLP:journals/tkde/YanLWW24,
	author = {Kaiwen Yan and
                  Chen Long and
                  Huisi Wu and
                  Zhenkun Wen},
	title = {Multi-Resolution Expansion of Analysis in Time-Frequency Domain for
                  Time Series Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6667--6680},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3396785},
	doi = {10.1109/TKDE.2024.3396785},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YanLWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series forecasting plays a crucial role in various real-world applications, such as finance, energy, traffic, and healthcare, providing valuable insights for decision-making processes. The aggregation of information windows with different resolutions has proven effective in time series forecasting tasks and provides the model diverse contextual information. As a result, the network can better capture and model the heterogeneity present in the data, thereby improving performance. However, most of the current work focuses on extracting multilevel-resolution information without considering the possibility that important information can be supplemented. Meanwhile, these methods also tend to ignore the effect of resolution on frequency. To address these challenges, we introduce the Time-Frequency Domain Multi-Resolution Expansion Network (TFMRN) for long-series forecasting using multi-resolution time-frequency data. The proposed TFMRN aims to expand the data in both the time and frequency domains, enabling the model to capture finer details that may not be evident in the original data. In addition, we also propose an Information Gating Unit (IGU) to enhance the selection and guidance of rich information from the expanded time-frequency multi-resolution data. Experimental results demonstrate that the proposed method yields better performance compared with the state-of-the-art methods in both univariate and multivariate time forecasting tasks.}
}


@article{DBLP:journals/tkde/HongWLGDWWLZ24,
	author = {Zhiqing Hong and
                  Guang Wang and
                  Wenjun Lyu and
                  Baoshen Guo and
                  Yi Ding and
                  Haotian Wang and
                  Shuai Wang and
                  Yunhuai Liu and
                  Desheng Zhang},
	title = {Nationwide Behavior-Aware Coordinates Mining From Uncertain Delivery
                  Events},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6681--6698},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3411562},
	doi = {10.1109/TKDE.2024.3411562},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HongWLGDWWLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geocoding, associating textual addresses with corresponding GPS coordinates, is vital for many location-based services (e.g., logistics, ridesharing, and social networks). One of the most common Geocoding solutions is using commercial map services such as Google Maps. However, this is typically not practical for some location-based service providers due to real-world challenges like commercial competition and high costs (recurring fees). In this paper, we design a new cost-effective Geocoding framework to automatically infer the geographic coordinates from textual addresses. To achieve this, we take the E-Commerce logistics service as a concrete scenario and design CoMiner, an unsupervised coordinate inference framework based on textual address data, delivery event data, and courier trajectory data. CoMiner includes three main components, (1) A POI-level clustering model, (2) A Delivery Mobility Graph (DMG), and (3) A behavior-driven address ranking model. Furthermore, we design CoMiner-W, a coordinates mining algorithm based on WiFi data, to further enhance the effectiveness of CoMiner. We conduct extensive experiments on three large-scale datasets where CoMiner outperforms the state-of-the-art methods by 20.3%. Moreover, we have designed an abnormal delivery event detection system based on CoMiner and deployed it at JD Logistics, which brings a significant reduction in abnormal delivery event rates.}
}


@article{DBLP:journals/tkde/ZhangWSWTQCWY24,
	author = {Weixu Zhang and
                  Yifei Wang and
                  Yuanfeng Song and
                  Victor Junqiu Wei and
                  Yuxing Tian and
                  Yiyan Qi and
                  Jonathan H. Chan and
                  Raymond Chi{-}Wing Wong and
                  Haiqin Yang},
	title = {Natural Language Interfaces for Tabular Data Querying and Visualization:
                  {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6699--6718},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3400824},
	doi = {10.1109/TKDE.2024.3400824},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangWSWTQCWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.}
}


@article{DBLP:journals/tkde/HuangSSL24,
	author = {Zhan ao Huang and
                  Yongsheng Sang and
                  Yanan Sun and
                  Jiancheng Lv},
	title = {Neural Networks Learn Specified Information for Imbalanced Data Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6719--6730},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3392953},
	doi = {10.1109/TKDE.2024.3392953},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuangSSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Imbalanced data problem is a classic topic in artificial intelligence. Neural network approaches to solve this problem mostly rely on resampling or reweighting strategies. However, these methods severely suffer from the learning bias in most cases when the empirical representation of known samples is insufficient. One-class learning can provide an ideal classification property to alleviate this critical issue. However, extending one-class learning to imbalanced data presents problems of hypersphere collapse, ambiguous interclass relations, and compact representations. In this paper, a new one-class learning paradigm is proposed for binary imbalanced data classification. Specifically, a neural network is employed to map known samples to a specified attribute space to solve the problems of hypersphere collapse and ambiguous interclass relations. Then, to alleviate the compact representation problem, a dynamic information potential energy is developed to disperse the mapped majority samples to fill the specified region as much as possible. The proposed method is validated on 34 imbalanced datasets with imbalanced ratios ranging from 16.90 to 100.14. The test results show that the proposed method achieves the best performance on more than half of the test datasets.}
}


@article{DBLP:journals/tkde/DingXRB24,
	author = {Shizhe Ding and
                  Boyang Xia and
                  Milong Ren and
                  Dongbo Bu},
	title = {{NIERT:} Accurate Numerical Interpolation Through Unifying Scattered
                  Data Representations Using Transformer Encoder},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6731--6744},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3402444},
	doi = {10.1109/TKDE.2024.3402444},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DingXRB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interpolation for scattered data is a classical problem in numerical analysis, with a long history of theoretical and practical contributions. Recent advances have utilized deep neural networks to construct interpolators, exhibiting excellent and generalizable performance. However, they still fall short in two aspects: 1) inadequate representation learning, resulting from separate embeddings of observed and target points in popular encoder-decoder frameworks and 2) limited generalization power, caused by overlooking prior interpolation knowledge shared across different domains. To overcome these limitations, we present a Numerical Interpolation approach using Encoder Representation of Transformers (called NIERT). On one hand, NIERT utilizes an encoder-only framework rather than the encoder-decoder structure. This way, NIERT can embed observed and target points into a unified encoder representation space, thus effectively exploiting the correlations among them and obtaining more precise representations. On the other hand, we propose to pre-train NIERT on large-scale synthetic mathematical functions to acquire prior interpolation knowledge, and transfer it to multiple interpolation domains with consistent performance gain. On both synthetic and real-world datasets, NIERT outperforms the existing approaches by a large margin, i.e., 4.3\\sim 14.3\\times lower MAE on TFRD subsets, and 1.7/1.8/8.7× lower MSE on Mathit/PhysioNet/PTV datasets.}
}


@article{DBLP:journals/tkde/BouAK24,
	author = {Savong Bou and
                  Toshiyuki Amagasa and
                  Hiroyuki Kitagawa},
	title = {O(1)-Time Complexity for Fixed Sliding-Window Aggregation Over Out-of-Order
                  Data Streams},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6745--6757},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419566},
	doi = {10.1109/TKDE.2024.3419566},
	timestamp = {Mon, 09 Dec 2024 22:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/BouAK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sliding-window aggregation is one of the core operations in processing and analyzing data streams, but it seriously suffers from the unordered events or elements from data streams. Unordered streams or out-of-order data streams contain events whose order based on their timestamps (called event time) is different from the order based on their arriving times to the system (called ingestion time). Out-of-order data streams typically occur in a distributed environment due to many factors, such as network disruptions and delays. Out-of-order data streams drastically make the processing speed slower and existing works, that can handle out-of-order streams, do not address this problem well and can be further improved. The time complexities of existing approaches are not efficient because they are dependent on n, which is the number of slides in the window. In addition, they ignore the past windows affected by the late-arrival records. In many applications, updating and reporting the results of the past windows affected by the late-arrival records in real time is strongly needed. This paper proposes two solutions: (1) A Maximum-allowed lateness-based IndeXing algorithm with a Constant time complexity (CMiX) for computing the current window, and (2) A Past Window Indexing algorithm (PWiX) for efficient updating the past windows. Experimental results show that CMiX and PWiX can deal with out-of-order data streams significantly better than other existing approaches. CMiX is about 3.21 times faster than the state-of-the-art approach by significantly using less memory. It is important to emphasize that all approaches mentioned in the paper have the following limitations: (1) Aggregation can be both distributive and algebraic, which must be commutative due to the out-of-order of data streams, and (2) The window and slide sizes are assumed to be fixed, and if they are changed, the indices must be reconstructed.}
}


@article{DBLP:journals/tkde/YuYZCLL24,
	author = {Ziqiang Yu and
                  Xiaohui Yu and
                  Tao Zhou and
                  Yueting Chen and
                  Yang Liu and
                  Bohan Li},
	title = {{ODIN:} Object Density Aware Index for C{\textdollar}k{\textdollar}kNN
                  Queries Over Moving Objects on Road Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6758--6772},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3344662},
	doi = {10.1109/TKDE.2023.3344662},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuYZCLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of processing continuous k\nnearest neighbor (Ck\nNN) queries over moving objects on road networks, which is an essential operation in a variety of applications. We are particularly concerned with scenarios where the object densities in different parts of the road network evolve over time as the objects move. Existing methods on Ck\nNN query processing are ill-suited for such scenarios as they utilize index structures with fixed granularities and are thus unable to keep up with the evolving object densities. In this paper, we directly address this problem and propose an object density aware index structure called ODIN that is an elastic tree built on a hierarchical partitioning of the road network. It is equipped with the unique capability of dynamically folding/unfolding its nodes, thereby adapting to varying object densities. We further present the ODIN-KNN-Init and ODIN-KNN-Inc algorithms for the initial identification of the k\nNNs and the incremental update of query result as objects move. Thorough experiments on both real and synthetic datasets confirm the superiority of our proposal over several baseline methods.}
}


@article{DBLP:journals/tkde/LiCFLP24,
	author = {Huijia Li and
                  Haobin Cao and
                  Yuhao Feng and
                  Xiaoyan Li and
                  Jian Pei},
	title = {Optimization of Graph Clustering Inspired by Dynamic Belief Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6773--6785},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3274547},
	doi = {10.1109/TKDE.2023.3274547},
	timestamp = {Sat, 01 Mar 2025 10:58:20 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiCFLP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph clustering is essential to understand the nature and behavior of real world such as social network, technical network and transportation network. Different from the existing studies, we propose a new Markov clustering method inspired by belief dynamical system which can be used in general for optimization of different quality measures. By a rigorous theoretical proof, it has been shown that the quality function's global maximum is a dynamical system's asymptotically stable fixed point. Under specified conditions, the trajectory of the dynamical converges to the cluster labels of corresponding nodes. Particularly, a general formulation can unite well-known methodologies and the quality functions that correspond to them. The algorithm is fast and its computational complexity is nearly linear with the scale of sparse networks. Finally, we thoroughly evaluate our methodology on a variety of synthetic and real-world networks with various network properties, particularly on the dynamical networks. The results demonstrate that when compared to the current state-of-the-art algorithms, our method performs better on these networks.}
}


@article{DBLP:journals/tkde/FuCCWH24,
	author = {Yujian Fu and
                  Cheng Chen and
                  Xiaohui Chen and
                  Weng{-}Fai Wong and
                  Bingsheng He},
	title = {Optimizing the Number of Clusters for Billion-Scale Quantization-Based
                  Nearest Neighbor Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6786--6800},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3408815},
	doi = {10.1109/TKDE.2024.3408815},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FuCCWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate nearest neighbor search (ANNS) is crucial in various real-world applications, including recommendation systems, data mining, and image retrieval. To date, quantization-based algorithms have emerged as one of the most efficient solutions for ANNS on billion-scale datasets. However, the determination of the optimal number of clusters, a critical factor for peak data performance in quantization-based systems, remains inadequately explored. Previous works often propose numbers of clusters that are not optimal, and the absence of effective methodologies for tuning this parameter leads to suboptimal search performance due to the vast configuration space. In response to this challenge, this paper introduces a novel algorithm that automatically identifies the optimal number of clusters for billion-scale, quantization-based ANNS systems to maximize search efficiency. We propose an analytical model for evaluating retrieval performance, serving as the benchmark for optimizing cluster numbers in quantization-based indexes. Our algorithm applies iterative local adjustments to the ANNS index being constructed, progressively refining the number of clusters. We demonstrate the efficacy of our approach using the popular inverted index structure in quantization-based ANNS systems. Our findings indicate that: (1) By optimizing the number of clusters, the vanilla inverted index exhibits improved retrieval performance on billion-scale datasets when compared to existing state-of-the-art quantization-based methods; and (2) The additional computational overhead introduced by our optimization algorithm is minimal, even when applied to billion-scale datasets.}
}


@article{DBLP:journals/tkde/LiuWWW24,
	author = {Qiang Liu and
                  Junfei Wu and
                  Shu Wu and
                  Liang Wang},
	title = {Out-of-Distribution Evidence-Aware Fake News Detection via Dual Adversarial
                  Debiasing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6801--6813},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3390431},
	doi = {10.1109/TKDE.2024.3390431},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuWWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Evidence-aware fake news detection aims to conduct reasoning between news and evidences, which are retrieved based on news content, to find uniformity or inconsistency. However, we find evidence-aware detection models suffer from biases, i.e., spurious correlations between news/evidence contents and true/fake news labels, and are hard to be generalized to Out-Of-Distribution (OOD) situations. To deal with this, we propose a novel Dual Adversarial Learning (DAL) approach. We incorporate news-aspect and evidence-aspect debiasing discriminators, whose targets are both true/fake news labels, in DAL. Then, DAL reversely optimizes news-aspect and evidence-aspect debiasing discriminators to mitigate the impact of news and evidence content biases. At the same time, DAL also optimizes the main fake news predictor, so that the news-evidence interaction module can be learned. This process allows us to teach evidence-aware fake news detection models to better conduct news-evidence reasoning, and minimize the impact of content biases. To be noted, our proposed DAL approach is a plug-and-play module that works well with existing backbones. We conduct comprehensive experiments under two OOD settings, and plug DAL in four evidence-aware fake news detection backbones. Results demonstrate that, DAL significantly and stably outperforms the original backbones and some competitive debiasing methods.}
}


@article{DBLP:journals/tkde/GermainTO24,
	author = {Thibaut Germain and
                  Charles Truong and
                  Laurent Oudre},
	title = {Persistence-Based Motif Discovery in Time Series},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6814--6827},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3417303},
	doi = {10.1109/TKDE.2024.3417303},
	timestamp = {Mon, 03 Mar 2025 22:25:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GermainTO24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motif Discovery consists of finding repeated patterns and locating their occurrences in a time series without prior knowledge about their shape or location. Most state-of-the-art algorithms rely on three core parameters: the number of motifs to discover, the length of the motifs, and a similarity threshold between motif occurrences. Setting these parameters is difficult in practice and often results from a trial-and-error strategy. In this paper, we propose a new algorithm that discovers motifs of variable length given a single motif length and without requiring a similarity threshold. At its core, the algorithm maps a time series onto a graph, summarizes it with persistent homology - a tool from topological data analysis - and identifies the most relevant motifs from the graph summary. We propose two versions of the algorithm, one requiring the number of motifs to discover and another, adaptive, that infers the number of motifs from the graph summary. Empirical evaluation on 9 labeled datasets, including 6 real-world datasets, shows that both algorithm versions significantly outperform state-of-the-art algorithms.}
}


@article{DBLP:journals/tkde/BaoYY24,
	author = {Peng Bao and
                  Rong Yan and
                  Caipiao Yang},
	title = {Popularity Prediction via Modeling Temporal Dependencies on Dynamic
                  Evolution Process},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6828--6838},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3409737},
	doi = {10.1109/TKDE.2024.3409737},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BaoYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting the future popularity of individual information cascades has attracted much attention in various application fields. It is significantly important for online advertising, viral marketing, rumor detection, and social recommendation. Most approaches target modeling forwarding path or learning important information from discrete static graph. These methods either extract complicated hand-crafted features that rely on domain knowledge and have lower generality, or devote to modeling the arriving intensity function of each message and cannot be optimized for the final popularity. Despite some approaches trying to utilize the underlying structural information in discrete snapshots, they neglect to model the temporal information that implicitly underlying abundant diffusion patterns. Meanwhile, they ignore the inherent dependencies among forwarding behaviors of users. In this paper, we propose a novel learning framework for popularity prediction via modeling temporal dependencies on dynamic evolution process, called TEDDY. Our framework not only models the temporal evolution in a separate snapshot via multiple sequences temporal encoder, but also captures the inherent temporal dependencies among different snapshots. We have conducted extensive experiments on two real-world datasets, i.e. Sina Weibo and American Physical Society. Experimental results demonstrate that our proposed TEDDY significantly improves the prediction accuracy and is superior to the state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/RanbadugeVD24,
	author = {Thilina Ranbaduge and
                  Dinusha Vatsalan and
                  Ming Ding},
	title = {Privacy-Preserving Deep Learning Based Record Linkage},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6839--6850},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3342757},
	doi = {10.1109/TKDE.2023.3342757},
	timestamp = {Mon, 09 Dec 2024 22:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/RanbadugeVD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning-based linkage of records across different databases is becoming increasingly useful in data integration and mining applications to discover new insights from multiple data sources. However, due to privacy and confidentiality concerns, organisations often are unwilling or allowed to share their sensitive data with any external parties, thus making it challenging to build/train deep learning models for record linkage across different organisations’ databases. To overcome this limitation, we propose the first deep learning-based multi-party privacy-preserving record linkage (PPRL) protocol that can be used to link sensitive databases held by multiple different organisations. In our approach, each database owner first trains a local deep learning model, which is then uploaded to a secure environment and securely aggregated to create a global model. The global model is then used by a linkage unit to distinguish unlabelled record pairs as matches and non-matches. We utilise differential privacy to achieve provable privacy protection against re-identification attacks. We evaluate the linkage quality and scalability of our approach using several large real-world databases, showing that it can achieve high linkage quality while providing sufficient privacy protection against existing attacks.}
}


@article{DBLP:journals/tkde/XueS24,
	author = {Hao Xue and
                  Flora D. Salim},
	title = {PromptCast: {A} New Prompt-Based Learning Paradigm for Time Series
                  Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6851--6864},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3342137},
	doi = {10.1109/TKDE.2023.3342137},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XueS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes three real-world forecasting scenarios. We evaluate different SOTA numerical-based forecasting methods and language generation models. The benchmark results with various forecasting settings demonstrate the proposed PromptCast with language generation models is a promising research direction. Additionally, in comparison to conventional numerical-based forecasting, PromptCast shows a much better generalization ability under the zero-shot setting.}
}


@article{DBLP:journals/tkde/ZhangXCMY24,
	author = {Chen Zhang and
                  Yu Xie and
                  Tingbin Chen and
                  Wenjie Mao and
                  Bin Yu},
	title = {Prototype Similarity Distillation for Communication-Efficient Federated
                  Unsupervised Representation Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6865--6876},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3386712},
	doi = {10.1109/TKDE.2024.3386712},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangXCMY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated unsupervised representation learning aims at leveraging unlabeled data from multiple parties to learn visual representations without compromising the data privacy and tackle the non-IID challenge by aligning diverse representation spaces. However, model heterogeneity and communication overhead will directly impact the convergence rate and model accuracy of federated unsupervised learning. And it is challenging to learn visual features for downstream tasks under the premise of compatibility with heterogeneous models and reducing communication overhead. To address these issues, we propose a novel communication-efficient federated unsupervised representation learning framework based on prototype similarity distillation (FLPD). In this framework, the global model builds the feature representation space based on the global dataset and steers the optimization of the prototype relations of the client models. In addition to employing discriminative self-supervised learning for model training, each client fine-tunes the local representation space with global prototype similarity via knowledge distillation, which facilitates local models to fit both the local data distribution and the global representation space. In order to maintain the compactness of prototypes within the same category and enhance the separability between prototypes of different categories, a prototype-based consistency constraint is introduced to alleviate the conflict between local and global representation space. Experimental results demonstrate that our framework outperforms other alternative approaches in terms of communication efficiency and accuracy in the federated settings with statistical heterogeneity and model heterogeneity.}
}


@article{DBLP:journals/tkde/ChenDZW24,
	author = {Haonan Chen and
                  Zhicheng Dou and
                  Yutao Zhu and
                  Ji{-}Rong Wen},
	title = {Query-Oriented Data Augmentation for Session Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6877--6888},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419131},
	doi = {10.1109/TKDE.2024.3419131},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenDZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modeling contextual information in a search session has drawn more and more attention when understanding complex user intents. Recent methods are all data-driven, i.e., they train different models on large-scale search log data to identify the relevance between search contexts and candidate documents. The common training paradigm is to pair the search context with different candidate documents and train the model to rank the clicked documents higher than the unclicked ones. However, this paradigm neglects the symmetric nature of the relevance between the session context and document, i.e., the clicked documents can also be paired with different search contexts when training. In this work, we propose query-oriented data augmentation to enrich search logs and empower the modeling. We generate supplemental training pairs by altering the most important part of a search context, i.e., the current query, and train our model to rank the generated sequence along with the original sequence. This approach enables models to learn that the relevance of a document may vary as the session context changes, leading to a better understanding of users’ search patterns. We develop several strategies to alter the current query, resulting in new training data with varying degrees of difficulty. Through experimentation on two extensive public search logs, we have successfully demonstrated the effectiveness of our model.}
}


@article{DBLP:journals/tkde/ZhaoFLLMWWWZTL24,
	author = {Zihuai Zhao and
                  Wenqi Fan and
                  Jiatong Li and
                  Yunqing Liu and
                  Xiaowei Mei and
                  Yiqi Wang and
                  Zhen Wen and
                  Fei Wang and
                  Xiangyu Zhao and
                  Jiliang Tang and
                  Qing Li},
	title = {Recommender Systems in the Era of Large Language Models (LLMs)},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6889--6907},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3392335},
	doi = {10.1109/TKDE.2024.3392335},
	timestamp = {Mon, 03 Mar 2025 22:25:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoFLLMWWWZTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an indispensable and important component, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have achieved significant advancements in enhancing recommender systems, these DNN-based methods still exhibit some limitations, such as inferior capabilities to effectively capture textual side information about users and items, difficulties in generalization to various recommendation scenarios, and reasoning on their predictions, etc. Meanwhile, the development of Large Language Models (LLMs), such as ChatGPT and GPT-4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems. Therefore, in this survey, we comprehensively review LLM-empowered recommender systems from various perspectives including pre-training, fine-tuning, and prompting paradigms. More specifically, we first introduce the representative methods to learn user and item representations, leveraging LLMs as feature encoders. Then, we systematically review the emerging advanced techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss the promising future directions in this emerging field.}
}


@article{DBLP:journals/tkde/LinXLY24,
	author = {Guoliang Lin and
                  Yongheng Xu and
                  Hanjiang Lai and
                  Jian Yin},
	title = {Revisiting Few-Shot Learning From a Causal Perspective},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6908--6919},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397689},
	doi = {10.1109/TKDE.2024.3397689},
	timestamp = {Tue, 22 Oct 2024 21:09:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LinXLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Few-shot learning with N-way K-shot scheme is an open challenge in machine learning. Many metric-based approaches have been proposed to tackle this problem, e.g., the Matching Networks and CLIP-Adapter. Despite that these approaches have shown significant progress, the mechanism of why these methods succeed has not been well explored. In this paper, we try to interpret these metric-based few-shot learning methods via causal mechanism. We show that the existing approaches can be viewed as specific forms of front-door adjustment, which can alleviate the effect of spurious correlations and thus learn the causality. This causal interpretation could provide us a new perspective to better understand these existing metric-based methods. Further, based on this causal interpretation, we simply introduce two causal methods for metric-based few-shot learning, which considers not only the relationship between examples but also the diversity of representations. Experimental results demonstrate the superiority of our proposed methods in few-shot classification on various benchmark datasets.}
}


@article{DBLP:journals/tkde/LiLFYWZ24,
	author = {Hanjie Li and
                  Changsheng Li and
                  Kaituo Feng and
                  Ye Yuan and
                  Guoren Wang and
                  Hongyuan Zha},
	title = {Robust Knowledge Adaptation for Dynamic Graph Neural Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6920--6933},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3388453},
	doi = {10.1109/TKDE.2024.3388453},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiLFYWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph structured data often possess dynamic characters in nature, such as the addition of links and nodes, in many real-world applications. Recent years have witnessed the increasing attentions paid to dynamic graph neural networks for modelling graph data. However, almost all existing approaches operate under the assumption that, upon the establishment of a new link, the embeddings of the neighboring nodes should undergo updates to learn temporal dynamics. Nevertheless, these approaches face the following limitation: If the node introduced by a new connection contains noisy information, propagating its knowledge to other nodes becomes unreliable and may even lead to the collapse of the model. In this paper, we propose Ada-DyGNN: a robust knowledge Adaptation framework via reinforcement learning for Dynamic Graph Neural Networks. In contrast to previous approaches, which update the embeddings of the neighbor nodes immediately after adding a new link, Ada-DyGNN adaptively determines which nodes should be updated. Considering that the decision to update the embedding of one neighbor node can significantly impact other neighbor nodes, we conceptualize the node update selection as a sequence decision problem and employ reinforcement learning to address it effectively. By this means, we can adaptively propagate knowledge to other nodes for learning robust node embedding representations. To the best of our knowledge, our approach constitutes the first attempt to explore robust knowledge adaptation via reinforcement learning specifically tailored for dynamic graph neural networks. Extensive experiments on three benchmark datasets demonstrate that Ada-DyGNN achieves the state-of-the-art performance. In addition, we conduct experiments by introducing different degrees of noise into the dataset, quantitatively and qualitatively illustrating the robustness of Ada-DyGNN.}
}


@article{DBLP:journals/tkde/LiangWZZG24,
	author = {Cheng Liang and
                  Daoyuan Wang and
                  Huaxiang Zhang and
                  Shichao Zhang and
                  Fei Guo},
	title = {Robust Tensor Subspace Learning for Incomplete Multi-View Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6934--6948},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3399707},
	doi = {10.1109/TKDE.2024.3399707},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiangWZZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incomplete multi-view clustering has represented a significant role in grouping real images. In this study, a novel robust tensor subspace learning (RTSL) is proposed for incomplete multi-view clustering. Specifically, the missing samples within views are first recovered by matrix factorization. The recovered information is utilized for latent representations learning. And then, the obtained latent representations are organized from all views into a third-order tensor and the intrinsic sample relations are captured with tensor linear representation. Moreover, a low-rank sample coefficient tensor is sought to capture high-order connections among views by imposing the tensor nuclear norm. Compared with traditional learning paradigms in the vector space, the sample relations within each view as well as across views could be preserved with the aid of robust tensor subspace learning. As a result, our model can simultaneously handle the missing samples and exploit the intrinsic correlations, leading to enhanced representation capability and better quality of the recovered data. We design an efficient iterative optimization strategy to solve the proposed method. Experimental results on eight datasets show that our model outperforms other competing approaches.}
}


@article{DBLP:journals/tkde/Yu24,
	author = {James Jianqiao Yu},
	title = {Scalable and Sustainable Graph-Based Traffic Prediction With Adaptive
                  Deep Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6949--6961},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419036},
	doi = {10.1109/TKDE.2024.3419036},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/Yu24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-based deep learning models are becoming prevalent for data-driven traffic prediction in the past years, due to their competence in exploiting the non-euclidean spatial-temporal traffic data. Nonetheless, these models are approaching a limit where drastically increasing model complexity in terms of trainable parameters cannot notably improve the prediction accuracy. Furthermore, the diversity of transportation networks requires traffic predictors to be scalable to various data sizes and quantities, and ever-changing traffic dynamics also call for capacity sustainability. To this end, we propose a novel adaptive deep learning scheme for boosting graph-based traffic predictor performance. The proposed scheme utilizes domain knowledge to decompose the traffic prediction task into sub-tasks, each of which is handled by deep models with low complexity and training difficulty. Further, a stream learning algorithm based on the empirical Fisher information loss is devised to enable predictors to incrementally learn from new data without re-training from scratch. Comprehensive case studies on five real-world traffic datasets indicate outstanding performance improvement of the proposed scheme when equipped to six state-of-the-art predictors. Additionally, the scheme also provides impressive autoregressive long-term predictions and incremental learning efficacy with traffic data streams.}
}


@article{DBLP:journals/tkde/SongSLLSXZ24,
	author = {Yaoxian Song and
                  Penglei Sun and
                  Haoyu Liu and
                  Zhixu Li and
                  Wei Song and
                  Yanghua Xiao and
                  Xiaofang Zhou},
	title = {Scene-Driven Multimodal Knowledge Graph Construction for Embodied
                  {AI}},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6962--6976},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3399746},
	doi = {10.1109/TKDE.2024.3399746},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SongSLLSXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Embodied AI is one of the most popular studies in artificial intelligence and robotics, which can effectively improve the intelligence of real-world agents (i.e. robots) serving human beings. Scene knowledge is important for an agent to understand the surroundings and make correct decisions in the varied open world. Currently, knowledge base for embodied tasks is missing and most existing work use general knowledge base or pre-trained models to enhance the intelligence of an agent. For conventional knowledge base, it is sparse, insufficient in capacity and cost in data collection. For pre-trained models, they face the uncertainty of knowledge and hard maintenance. To overcome the challenges of scene knowledge, we propose a scene-driven multimodal knowledge graph (Scene-MMKG) construction method combining conventional knowledge engineering and large language models. A unified scene knowledge injection framework is introduced for knowledge representation. To evaluate the advantages of our proposed method, we instantiate Scene-MMKG considering typical indoor robotic functionalities (Manipulation and Mobility), named ManipMob-MMKG. Comparisons in characteristics indicate our instantiated ManipMob-MMKG has broad superiority on data-collection efficiency and knowledge quality. Experimental results on typical embodied tasks show that knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the performance obviously without re-designing model structures complexly.}
}


@article{DBLP:journals/tkde/LiuWLYWY24,
	author = {Diju Liu and
                  Yalin Wang and
                  Chenliang Liu and
                  Xiaofeng Yuan and
                  Kai Wang and
                  Chunhua Yang},
	title = {Scope-Free Global Multi-Condition-Aware Industrial Missing Data Imputation
                  Framework via Diffusion Transformer},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6977--6988},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3392897},
	doi = {10.1109/TKDE.2024.3392897},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuWLYWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Missing data is a common phenomenon in the industrial field. The recovery of missing data is crucial to enhance the reliability of subsequent data-driven monitoring and control of industrial processes. Most existing methods are limited by the confined scope of feature extraction, which makes it impossible to rely on global information to impute missing data. In addition, they usually assume that industrial data is a uniform distribution across all working conditions, ignoring the differences in data evolution patterns across different conditions. To address these issues, this paper proposes an innovative scope-free global multi-condition-aware imputation framework based on diffusion transformer (SGMCAI-DiT). First, it extends the diffusion model by introducing conditional probability to capture the condition distribution of the entire data. Then, a noise prediction model is designed based on a novel double-weighted attention mechanism (DW-SA) to broaden the horizons of feature extraction. By discerning the inter-conditional interactions and the intra-conditional local information, the missing data imputation performance can be improved. Finally, the effectiveness and suitability of the proposed SGMCAI-DiT are verified on four real datasets sourced from industrial processes and two public non-industrial datasets. Extensive experimental results demonstrate that the proposed method outperforms several state-of-the-art methods in different missing data scenarios.}
}


@article{DBLP:journals/tkde/BerahmandBALX24,
	author = {Kamal Berahmand and
                  Sondos Bahadori and
                  Maryam Nooraei Abadeh and
                  Yuefeng Li and
                  Yue Xu},
	title = {{SDAC-DA:} Semi-Supervised Deep Attributed Clustering Using Dual Autoencoder},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {6989--7002},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3389049},
	doi = {10.1109/TKDE.2024.3389049},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BerahmandBALX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Attributed graph clustering aims to group nodes into disjoint categories using deep learning to represent node embeddings and has shown promising performance across various applications. However, two main challenges hinder further performance improvement. First, reliance on unsupervised methods impedes the learning of low-dimensional, clustering-specific features in the representation layer, thus impacting clustering performance. Second, the predominant use of separate approaches leads to suboptimal learned embeddings that are insufficient for subsequent clustering steps. To address these limitations, we propose a novel method called Semi-supervised Deep Attributed Clustering using Dual Autoencoder (SDAC-DA). This approach enables semi-supervised deep end-to-end clustering in attributed networks, promoting high structural cohesiveness and attribute homogeneity. SDAC-DA transforms the attribute network into a dual-view network, applies a semi-supervised autoencoder layering approach to each view, and integrates dimensionality reduction matrices by considering complementary views. The resulting representation layer contains high clustering-friendly embeddings, which are optimized through a unified end-to-end clustering process for effectively identifying clusters. Extensive experiments on both synthetic and real networks demonstrate the superiority of our proposed method over seven state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/TuMLJWH24,
	author = {Rong{-}Cheng Tu and
                  Xian{-}Ling Mao and
                  Jinyu Liu and
                  Yatai Ji and
                  Wei Wei and
                  Heyan Huang},
	title = {Similarity Transitivity Broken-Aware Multi-Modal Hashing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7003--7014},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3396492},
	doi = {10.1109/TKDE.2024.3396492},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TuMLJWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the low storage cost and fast retrieval speed, multi-modal hashing, which maps the instances with different modal data-views into hash codes, has earned increasing research attention. Most existing supervised multi-modal hashing methods exploit the label information to define the similarities between instance pairs to supervise their training process. However, such methods ignore that the transitivity of their defined similarity has been broken in the multi-label scenarios, i.e., the instance x is similar to the instance y, and the instance z is also similar to the instance y, but x may be not similar to z, which will lead to fluctuations in the model optimization process and damage their retrieval performance. For example, in the first batch with instances x and y but without z, the model will be optimized to make the hash codes of x and y similar to each other; In the second batch with instances z and y but without x, the model will be optimized to make the hash codes of z and y similar to each other; In the third batch with the instances x and z but without y, the model will be optimized to make the hash codes of z and x dissimilar to each other, meanwhile in this process, the hash codes of z and x may be dissimilar to that of y which damage the optimizing results of the first two batches. Therefore, we propose a novel Similarity Transitivity Broken-aware Multi-modal Hashing, called STBMH, to solve this problem by adding a novel regularization loss into the original pair-wise loss. For each instance x in a training batch, the regularization loss will take all instances in the training set into account. Extensive experiments on four widely used datasets show that the proposed method achieves better performance than the state-of-the-art baselines on multi-modal retrieval task.}
}


@article{DBLP:journals/tkde/LeiWLJZG24,
	author = {Runze Lei and
                  Pinghui Wang and
                  Rundong Li and
                  Peng Jia and
                  Junzhou Zhao and
                  Xiaohong Guan},
	title = {Sketching Data Distribution by Rotation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7015--7029},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3342747},
	doi = {10.1109/TKDE.2023.3342747},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LeiWLJZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Kernel density estimation is a useful method for estimating the probability distribution of data. It is a challenge to achieve efficient kernel density estimation, especially for large-scale and high-dimension stream data. We propose rotation kernel, a novel kernel function for density estimation. The rotation kernel density can be fast estimated by a data structure named Rotation Kernel Density Sketch (RKDS). RKDS is a time- and memory-efficient method for kernel density estimation, even over data streams and distributed systems. RKDS is applicable for estimating density at specific points and also for representing data distribution. We provide theoretical analysis for rotation kernel and RKDS. Furthermore, we apply RKDS to outlier detection, concept drift detection, and personalized federated learning. Experiments show that our method improves time efficiency by up to 3\\times 10^{3} times compared with baselines. RKDS also provides comparable detecting precision and better delay on outlier detection and concept drift detection tasks.}
}


@article{DBLP:journals/tkde/LuZLGLYZYT24,
	author = {Weigang Lu and
                  Yibing Zhan and
                  Binbin Lin and
                  Ziyu Guan and
                  Liu Liu and
                  Baosheng Yu and
                  Wei Zhao and
                  Yaming Yang and
                  Dacheng Tao},
	title = {SkipNode: On Alleviating Performance Degradation for Deep Graph Convolutional
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7030--7043},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3374701},
	doi = {10.1109/TKDE.2024.3374701},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuZLGLYZYT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Convolutional Networks (GCNs) suffer from performance degradation when models go deeper. However, earlier works only attributed the performance degeneration to over-smoothing. In this paper, we conduct theoretical and experimental analysis to explore the fundamental causes of performance degradation in deep GCNs: over-smoothing and gradient vanishing have a mutually reinforcing effect that causes the performance to deteriorate more quickly in deep GCNs. On the other hand, existing anti-over-smoothing methods all perform full convolutions up to the model depth. They could not well resist the exponential convergence of over-smoothing due to model depth increasing. In this work, we propose a simple yet effective plug-and-play module, SkipNode, to overcome the performance degradation of deep GCNs. It samples graph nodes in each convolutional layer to skip the convolution operation. In this way, both over-smoothing and gradient vanishing can be effectively suppressed since (1) not all nodes’features propagate through full layers and, (2) the gradient can be directly passed back through “skipped” nodes. We provide both theoretical analysis and empirical evaluation to demonstrate the efficacy of SkipNode and its superiority over SOTA baselines.}
}


@article{DBLP:journals/tkde/YangRF24,
	author = {Jiawei Yang and
                  Susanto Rahardja and
                  Pasi Fr{\"{a}}nti},
	title = {Smoothing Outlier Scores Is All You Need to Improve Outlier Detectors},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7044--7057},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3332757},
	doi = {10.1109/TKDE.2023.3332757},
	timestamp = {Mon, 09 Dec 2024 22:46:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangRF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We hypothesize that similar objects should have similar outlier scores. To the best of our knowledge, all existing outlier detectors calculate the outlier score for each object independently regardless of the outlier scores of the other objects. Therefore, they do not guarantee that similar objects have similar outlier scores. To verify our proposed hypothesis, we propose an outlier score post-processing technique for outlier detectors, called neighborhood averaging (NA) for neighborhood smoothing in outlier score space. It pays attention to objects and their neighbors and guarantees them to have more similar outlier scores than their original scores. Given an object and its outlier score from any outlier detector, NA modifies its outlier score by combining it with its k nearest neighbors’ scores. We demonstrate the effectivity of NA by using the well-known k nearest neighbors (k-NN). Experimental results show that NA improves all 10 tested baseline detectors by 13% on average relative to the original results (from 0.70 to 0.79 AUC) evaluated on nine real-world datasets. Moreover, deep-learning-based detectors and even outlier detectors that are already based on k-NN are also improved. The experiments also show that in some applications, the choice of detector is no more significant when detectors are jointly used with NA. This may pose a challenge to the generally considered idea that the data model is the most important factor.}
}


@article{DBLP:journals/tkde/JiangCHCC24,
	author = {Jiaxin Jiang and
                  Yuhang Chen and
                  Bingsheng He and
                  Min Chen and
                  Jia Chen},
	title = {Spade+: {A} Generic Real-Time Fraud Detection Framework on Dynamic
                  Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7058--7073},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3394155},
	doi = {10.1109/TKDE.2024.3394155},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiangCHCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time fraud detection remains a pressing issue for many financial and e-commerce platforms. \\mathsf {Grab}, a prominent technology company in Southeast Asia, addresses this by constructing a transactional graph. This graph aids in pinpointing dense subgraphs, possibly indicative of fraudster networks. Notably, prevalent methods are designed for static graphs, neglecting the evolving nature of transaction graphs. This static approach is ill-suited to the real-time necessities of modern industries. In our earlier work, \\mathsf {Spade}, the focus was mainly on edge insertions. However, \\mathsf {Grab}'s operational demands necessitated managing outdated transactions. Persistently adding edges without a deletion mechanism might inadvertently lead to densely connected legitimate communities. To resolve this, we present \\mathsf {Spade+}, a refined real-time fraud detection system at \\mathsf {Grab}. Contrary to \\mathsf {Spade}, \\mathsf {Spade+} manages both edge additions and removals. Leveraging an incremental approach, \\mathsf {Spade+} promptly identifies suspicious communities in large graphs. Moreover, \\mathsf {Spade+} efficiently handles batch updates and employs edge packing to diminish latency. A standout feature of \\mathsf {Spade+} is its user-friendly APIs, allowing for tailored fraud detection methods. Developers can easily integrate their specific metrics, which \\mathsf {Spade+} autonomously refines. Rigorous evaluations validate the prowess of \\mathsf {Spade+}; fraud detection mechanisms powered by \\mathsf {Spade+} were up to a million times faster than their static counterparts.}
}


@article{DBLP:journals/tkde/WangZLFCHS24,
	author = {Yu Wang and
                  Tongya Zheng and
                  Shunyu Liu and
                  Zunlei Feng and
                  Kai{-}Xuan Chen and
                  Yunzhi Hao and
                  Mingli Song},
	title = {Spatiotemporal-Augmented Graph Neural Networks for Human Mobility
                  Simulation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7074--7086},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3409071},
	doi = {10.1109/TKDE.2024.3409071},
	timestamp = {Wed, 12 Feb 2025 20:32:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangZLFCHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal correspondences and builds a novel dwell branch to simulate the varying durations in locations, which is finally optimized in an adversarial manner. The comprehensive experiments over four real datasets for the human mobility simulation have verified the superiority of STAR to state-of-the-art methods.}
}


@article{DBLP:journals/tkde/ZhangXYHXCPC24,
	author = {Wen Zhang and
                  Yajing Xu and
                  Peng Ye and
                  Zhiwei Huang and
                  Zezhong Xu and
                  Jiaoyan Chen and
                  Jeff Z. Pan and
                  Huajun Chen},
	title = {Start From Zero: Triple Set Prediction for Automatic Knowledge Graph
                  Completion},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7087--7101},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3399832},
	doi = {10.1109/TKDE.2024.3399832},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangXYHXCPC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graph (KG) completion aims to find out missing triples in a KG. Some tasks, such as link prediction and instance completion, have been proposed for KG completion. They are triple-level tasks with some elements in a missing triple given to predict the missing element of the triple. However, knowing some elements of the missing triple in advance is not always a realistic setting. In this paper, we propose a novel graph-level automatic KG completion task called Triple Set Prediction (TSP) which assumes none of the elements in the missing triples is given. TSP is to predict a set of missing triples given a set of known triples. To properly and accurately evaluate this new task, we propose 4 evaluation metrics including 3 classification metrics and 1 ranking metric, considering both the partial-open-world and the closed-world assumptions. Furthermore, to tackle the huge candidate triples for prediction, we propose a novel and efficient subgraph-based method GPHT that can predict the triple set fast. To fairly compare the TSP results, we also propose two types of methods RuleTensor-TSP and KGE-TSP applying the existing rule- and embedding-based methods for TSP as baselines. During experiments, we evaluate the proposed methods on two datasets extracted from Wikidata following the relation-similarity partial-open-world assumption proposed by us, and also create a complete family data set to evaluate TSP results following the closed-world assumption. Results prove that the methods can successfully generate a set of missing triples and achieve reasonable scores on the new task, and GPHTperforms better than the baselines with significantly shorter prediction time.}
}


@article{DBLP:journals/tkde/SunG24,
	author = {Aimin Sun and
                  Zhiguo Gong},
	title = {Temporal Graph Multi-Aspect Embeddings},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7102--7114},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397491},
	doi = {10.1109/TKDE.2024.3397491},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SunG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, graph embedding techniques have exhibited great potential for various downstream tasks, which can leverage both topological structures and the temporal dependencies of nodes in their representations, leading to remarkable achievements. However, the multi-role nature of nodes during their temporally interacting is neglected. To tackle this problem, we propose a novel model, Temporal graph Multi-Aspect Embedding (TMAE), to capture the latent multi-aspect characteristics of nodes in temporal graphs, thereby enhancing the quality of graph embeddings. Specifically, we propose to learn the aspect embeddings of nodes and their weights at different timestamps separately for a better adaptation. In contrast to the conventional fixed aspect number assumption, a Hierarchical Dirichlet Process-based approach is employed to dynamically determine the weight of aspects for nodes at different times. Through this framework, we effectively learn the multi-aspect information through Time-reversed Temporal Walks (TTWs). Extensive experiments performed across eight publicly accessible datasets have demonstrated the significant improvements of the proposed TMAE model over state-of-the-art algorithms by taking advantage of the multi-aspect nature.}
}


@article{DBLP:journals/tkde/ZhangZZZWZ24,
	author = {Fuwei Zhang and
                  Zhao Zhang and
                  Fuzhen Zhuang and
                  Yu Zhao and
                  Deqing Wang and
                  Hongwei Zheng},
	title = {Temporal Knowledge Graph Reasoning With Dynamic Memory Enhancement},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7115--7128},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3390683},
	doi = {10.1109/TKDE.2024.3390683},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZZZWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal Knowledge Graph (TKG) reasoning involves predicting future facts based on historical information by learning correlations between entities and relations. Recently, many models have been proposed for the TKG reasoning task. However, most existing models cannot efficiently utilize historical information, which can be summarized in two aspects: 1) Many models only consider the historical information in a fixed time range, resulting in a lack of useful information; 2) some models use all the historical facts, thus some noise or invalid facts are introduced during reasoning. In this regard, we propose a novel TKG reasoning model with dynamic memory enhancement (DyMemR). Inspired by human memory, we introduce memory capacity, memory loss, and repetition stimulation to design a human-like memory pool that could remember potentially useful historical facts. To fully leverage the memory pool, we utilize a two-stage training strategy. The first stage is guided by the memory-based encoding module which learns embeddings from memory-based subgraphs generated through the memory pool. The second stage is the memory-based scoring module that emphasizes the historical facts in the memory pool. Finally, we extensively validate the superiority of DyMemR against various state-of-the-art baselines.}
}


@article{DBLP:journals/tkde/HanYZ24,
	author = {Lu Han and
                  Han{-}Jia Ye and
                  De{-}Chuan Zhan},
	title = {The Capacity and Robustness Trade-Off: Revisiting the Channel Independent
                  Strategy for Multivariate Time Series Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7129--7142},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3400008},
	doi = {10.1109/TKDE.2024.3400008},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HanYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multivariate time series data comprises various channels of variables. The multivariate forecasting models need to capture the relationship between the channels to accurately predict future values. However, recently, there has been an emergence of methods that employ the Channel Independent (CI) strategy. These methods view multivariate time series data as separate univariate time series and disregard the correlation between channels. Surprisingly, our empirical results have shown that models trained with the CI strategy outperform those trained with the Channel Dependent (CD) strategy, usually by a significant margin. Nevertheless, the reasons behind this phenomenon have not yet been thoroughly explored in the literature. This paper provides comprehensive empirical and theoretical analyses of the characteristics of multivariate time series datasets and the CI/CD strategy. Our results conclude that the CD approach has higher capacity but often lacks robustness to accurately predict distributionally drifted time series. In contrast, the CI approach trades capacity for robust prediction. Practical measures inspired by these analyses are proposed to address the capacity and robustness dilemma, including a modified CD method called Predict Residuals with Regularization (PRReg) that can surpass the CI strategy. We hope our findings can raise awareness among researchers about the characteristics of multivariate time series and inspire the construction of better forecasting models.}
}


@article{DBLP:journals/tkde/TsoukanaraKPT24,
	author = {Evangelia Tsoukanara and
                  Georgia Koloniari and
                  Evaggelia Pitoura and
                  Peter Triantafillou},
	title = {The GraphTempo Framework for Exploring the Evolution of a Graph Through
                  Pattern Aggregation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7143--7156},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3410647},
	doi = {10.1109/TKDE.2024.3410647},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TsoukanaraKPT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When the focus is on the relationships or interactions between entities, graphs offer an intuitive model for many real-world data. Such graphs are usually large and change over time, thus, requiring models and strategies that explore their evolution. We study the evolution of aggregate graphs and introduce the GraphTempo model that allows temporal and graph aggregation not only on node level by grouping individual nodes, but on a pattern level as well, where subgraphs are grouped together. Furthermore, we propose an efficient strategy for exploring the evolution of the graph based on identifying time intervals of significant growth, shrinkage, or stability. Finally, we evaluate the efficiency and effectiveness of the proposed approach using four real graphs.}
}


@article{DBLP:journals/tkde/YanLW24,
	author = {Surong Yan and
                  Kwei{-}Jay Lin and
                  Haosen Wang},
	title = {Toward Lightweight End-to-End Semantic Learning of Real-Time Human
                  Activity Recognition for Enabling Ambient Intelligence},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7157--7170},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3386794},
	doi = {10.1109/TKDE.2024.3386794},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YanLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Building accurate human behavior models is necessary for ambient intelligence. However, human activity recognition (HAR) in continuously monitored physical space meets many challenges to achieve a good performance when using only simple computing resources. In this work, we model HAR as an edge classification problem for a collaborative event graph of context entities in a sequential bipartite graph form. We design a semantic learning framework, called KGAR, to perform HAR by mining, encoding, and exploiting deep semantic knowledge of activities in an end-to-end fashion. KGAR has three components: preprocessor, KGEncoder, and predictor. The preprocessor builds offline a tiny knowledge graph of activities, to model and capture multidimensional semantic relationships between activities and core context entities. KGEncoder encodes the knowledge graph of activities using improved graph neural networks (GNNs) models, to handle confusing context patterns. The predictor may be deployed using some lightweight deep neural network to produce real-time labels. Experimental results show that using KGEncoder in KGAR improves the performance of original deep neural networks by 25% - 439% on five datasets. The time of labeling each sensor event during testing with event streams is less than 0.5 ms. We have also conducted extensive experimental study to show that KGAR outperforms different types of models in more complex activity scenarios. We believe KGAR can be used for real-time HAR in real life with its high prediction performance and a low computing requirement.}
}


@article{DBLP:journals/tkde/LiHL24,
	author = {Yakun Li and
                  Lei Hou and
                  Juanzi Li},
	title = {Towards Knowledge-Aware and Deep Reinforced Cross-Domain Recommendation
                  Over Collaborative Knowledge Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7171--7187},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3391268},
	doi = {10.1109/TKDE.2024.3391268},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain recommendations (CDRs), which can leverage the relatively abundant information from a richer domain to improve the recommendation performance in a sparser domain, have attracted great attention due to their flexible recommendation strategies. Nevertheless, existing CDR approaches still suffer from severe data sparsity and low semantic sampling efficiency issues, and hardly employ existing reinforcement learning models to improve cross-domain recommendation accuracy. To this end, we propose a new Knowledge-aware and Deep Reinforced Cross-Domain Recommendation framework over Collaborative Knowledge Graph (KRCDR). Specifically, we formalize the cross-domain recommendation task as a Markov Decision Process, and propose a knowledge-aware dual state representation approach to enhance state representations within and across domains for target users by leveraging knowledge graph information. Then, to improve the training performance, we propose a Constrained Self-supervised Actor-Critic network (CSAC) model, in which a constrained neighbor pruning strategy is devised to narrow the exploration space and improve the sampling efficiency, and the CSAC is developed to improve the recommendation policy. Additionally, in our proposed CSAC model, a self-supervised output layer within domains is used as an actor network to generate the recommendation policy, and a Q-learning output layer across domains is used as a critic network to feedback reward signals. Finally, based on the KRCDR approach, we design a new algorithm to assist in generating cross-domain recommendation results. Extensive experiments have been conducted on several real-world datasets, which demonstrate the superiority of our proposed approach compared with state-of-the-art baseline methods.}
}


@article{DBLP:journals/tkde/YangLGLLLL24,
	author = {Zhenguo Yang and
                  Jiajie Lin and
                  Zhiwei Guo and
                  Yang Li and
                  Xiaoping Li and
                  Qing Li and
                  Wenyin Liu},
	title = {Towards Rumor Detection With Multi-Granularity Evidences: {A} Dataset
                  and Benchmark},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7188--7200},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3401700},
	doi = {10.1109/TKDE.2024.3401700},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangLGLLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social media serves as a real-time collecting and disseminating center of users’ ideas, opinions, and experiences. The deliberate disinformation and rumors propagate rapidly online due to their exaggerated facts, controversial opinions, divisive perspectives, and stunning expressions. Rumor detection approaches typically use social media posts with rumor or non-rumor labels for training and testing without disclosing the rationale behind decision-makings. On one hand, collecting evidence data to verify claims relies on expert efforts. On the other hand, verifying the truthfulness of confusing claims with distracting and lengthy evidences is still challenging. In this paper, we contribute a rumor detection dataset with multi-granularity evidences, denoted as the RD-E dataset, which includes response, fact-check, article, sourcing data and generated evidence by large language models, supporting models to verify the truthfulness of claims on social media. A number of 32,892 claims from 4,525 public individuals and organizations are annotated to 6 kinds of labels, including true, mostly true, half true, mostly false, false, pants on fire, covering a wide range of topics, e.g., politics, economy, society, technology, and health. In the experiments, seven rumor detection models have been investigated and customized on four predefined subtasks for comparisons.}
}


@article{DBLP:journals/tkde/XieWZLYZLL24,
	author = {Yuan Xie and
                  Fan Wu and
                  Xu Zhou and
                  Wensheng Luo and
                  Yifang Yin and
                  Roger Zimmermann and
                  Keqin Li and
                  Kenli Li},
	title = {Trajectory-Aware Task Coalition Assignment in Spatial Crowdsourcing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7201--7216},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3336642},
	doi = {10.1109/TKDE.2023.3336642},
	timestamp = {Sat, 14 Dec 2024 21:39:14 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XieWZLYZLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularity of GPS-equipped smart devices, spatial crowdsourcing (SC) techniques have attracted growing attention in both academia and industry. A fundamental problem in SC is assigning location-based tasks to workers under spatial-temporal constraints. In many real-life applications, workers choose tasks on the basis of their preferred trajectories. However, by existing trajectory-aware task assignment approaches, tasks assigned to a worker may be far apart from each other, resulting in a higher detour cost as the worker needs to deviate from the original trajectory more often than necessary. Motivated by the above observations, we investigate a trajectory-aware task coalition assignment (TCA) problem and prove it to be NP-hard. The goal is to maximize the number of assigned tasks by assigning task coalitions to workers based on their preferred trajectories. For tackling the TCA problem, we develop a batch-based three-stage framework consisting of task grouping, planning, and assignment. First, we design greedy and spanning grouping approaches to generate task coalitions. Second, to gain candidate task coalitions for each worker efficiently, we design task-based and trajectory-based pruning strategies to reduce the search space. Furthermore, a 2-approximate algorithm, termed MST-Euler, is proposed to obtain a route among each worker and task coalition with a minimal detour cost. Third, the MST-Euler Greedy (MEG) algorithm is presented to compute an assignment that results in the maximal number of tasks assigned and a parallel strategy is introduced to boost its efficiency. Extensive experiments on real and synthetic datasets demonstrate the effectiveness and efficiency of the proposed algorithms.}
}


@article{DBLP:journals/tkde/QuCGCTZ24,
	author = {Bohao Qu and
                  Xiaofeng Cao and
                  Qing Guo and
                  Yi Chang and
                  Ivor W. Tsang and
                  Chengqi Zhang},
	title = {Transductive Reward Inference on Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7217--7228},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3398208},
	doi = {10.1109/TKDE.2024.3398208},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/QuCGCTZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data. Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum. Empirical evaluations on locomotion and robotic manipulation tasks validate the effectiveness of our approach. The application of our inferred rewards improves the performance in offline reinforcement learning tasks.}
}


@article{DBLP:journals/tkde/LiXPWCY24,
	author = {Guorui Li and
                  Pengfei Xu and
                  Sancheng Peng and
                  Cong Wang and
                  Yi Cai and
                  Shui Yu},
	title = {{TTSR:} Tensor-Train Subspace Representation Method for Visual Domain
                  Adaptation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7229--7241},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3391019},
	doi = {10.1109/TKDE.2024.3391019},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiXPWCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most existing methods for visual domain adaptation need to convert high-order tensors into one-order high-dimensional vectors through naive vectorization operations. However, they not only destroy the internal spatial structure within the original high-order tensors, but also result in exponentially increasing model parameters. To address these problems, we propose a novel method for visual domain adaptation by representing tensorial features in tensor-train subspace in this paper. Specifically, we firstly provide a theoretical deduction by constructing a tensor-train subspace and proving its linearity and left-orthogonality. Secondly, to extract common tensorial features between source and target domains, we formulate the visual domain adaptation problem into an optimization problem that models the aforementioned common tensor-train subspace between two domains, as well as their corresponding projections. Thirdly, we design a tensor-train subspace representation algorithm (TTSR) to solve the multiple variables optimization problem by optimizing its sub-problems iteratively, so as to process high-order tensorial features. Finally, we evaluate the performance of our proposed TTSR algorithm by conducting extensive experiments on three popular public datasets. The experimental results demonstrate that the TTSR algorithm can improve the classification accuracy of unlabeled target domain than that of baseline algorithms.}
}


@article{DBLP:journals/tkde/WangXY24,
	author = {Zengzhi Wang and
                  Rui Xia and
                  Jianfei Yu},
	title = {Unified {ABSA} via Annotation-Decoupled Multi-Task Instruction Tuning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7242--7254},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3392836},
	doi = {10.1109/TKDE.2024.3392836},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangXY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aspect-Based Sentiment Analysis (ABSA) aims to provide fine-grained aspect-level sentiment information. Different ABSA tasks are designed for different real-world applications. However, application scenarios of ABSA tasks are often diverse, typically requiring training separate systems for each task on the task-specific labeled data and making separate predictions. Second, different tasks often contain shared sentiment elements. Training task-specific models either fail to exploit the shared knowledge among multiple ABSA tasks effectively or neglect the complementarity between tasks. Third, despite the existence of the compound ABSA task such as quadruple extraction and triple extraction, it is not easy to obtain satisfactory performance due to the coupling of multiple elements. To tackle these issues, we present UnifiedABSA, a general-purpose ABSA framework based on multi-task instruction tuning, aiming at “one-model-for-all-tasks”. We also introduce a new annotation-decoupled multi-task learning mechanism that only depends on annotation on the compound task rather than all tasks. This mechanism not only fully utilizes the existing annotations from the compound task, but also alleviates the complicated coupling relationship among multiple elements, making the learning more effective. Extensive experiments show that UnifiedABSA can consistently outperform dedicated models in fully-supervised and low-resource settings for almost all 11 ABSA tasks. We also conduct further experiments to demonstrate the general applicability of our framework.}
}


@article{DBLP:journals/tkde/YeZCYZX24,
	author = {Yuyang Ye and
                  Hengshu Zhu and
                  Tianyi Cui and
                  Runlong Yu and
                  Le Zhang and
                  Hui Xiong},
	title = {University Evaluation Through Graduate Employment Prediction: An Influence
                  Based Graph Autoencoder Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7255--7267},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3402234},
	doi = {10.1109/TKDE.2024.3402234},
	timestamp = {Tue, 11 Feb 2025 12:52:24 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YeZCYZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is always challenging task for students to select right universities. For students, graduate job placement is the most important component of university quality. However, existing university evaluation methods predominantly depend on either subjective criteria, such as the perceived quality of the learning environment and academic prestige, or on factors like faculty excellence, which may not provide a precise indication of graduate job placement. Indeed, there is still a lack of a data-driven approach to accurately measure university quality based on the employment situation of graduates. Moreover, the inherently unsupervised nature of university evaluation, compounded by the absence of a reasonable ground truth, necessitates the development of a reliable supervised methodology to precisely quantify university quality. Our basic assumption is that highly influential companies would attract graduates from high-ranking universities. To this end, in this paper, we formulate university evaluation problem into the graduate flow prediction problem, and propose an Influence based Graph Autoencoder (IGAE) method to learn the representation of universities based on the employment of their graduates. Specifically, we first build a talent transition graph based on the massive resume information. This graph reveals the flow of talent between institutions. Then, considering the asymmetric and heterogeneous properties of talent flow, an unidirectional aggregation process with a heterogeneous attention mechanism is designed to encode the nodes in the directed graph and preserve the influence terms at the same time. Afterwards, a novel dual self-attention module is exploited to capture the dynamic pattern of institutions to forecast future employment. Furthermore, we design an influence based decoder to predict the existence of talent flows and estimate the frequency of employment, which can be learnt in a joint learning framework. Finally, we conduct extensive experiments on a real-world dataset for performance evaluation. The experimental results clearly validate the effectiveness of our approach compared to the state-of-the-art baselines, and we provide a case study on university influence analysis.}
}


@article{DBLP:journals/tkde/GuoKLZKSTW24,
	author = {Jiannan Guo and
                  Yangyang Kang and
                  Xiaolin Li and
                  Wenqiao Zhang and
                  Kun Kuang and
                  Changlong Sun and
                  Siliang Tang and
                  Fei Wu},
	title = {Unleash the Power of Inconsistency-Based Semi-Supervised Active Learning
                  by Dynamic Programming of Curriculum Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7268--7282},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3417235},
	doi = {10.1109/TKDE.2024.3417235},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GuoKLZKSTW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the training procedures of many real-world learning models, gathering and annotating decent amounts of labeled data can be cost-prohibitive. To mitigate this data-hungry problem, active learning (AL) and semi-supervised learning (SSL) are frequently adopted as two effective but often isolated means. Some recent studies explored the potential of combining AL and SSL to better probe the unlabeled data. However, almost all these contemporary SSL-AL works use a simple combination strategy, ignoring SSL and AL's inherent relation. Further, other methods suffer from high computational costs when dealing with large-scale, high-dimensional datasets. Motivated by the industry practice of labeling data, we first propose an innovative Inconsistency-based virtual aDvErsarial Active Learning (IDEAL) algorithm to further investigate SSL-AL's potential superiority and achieve mutual enhancement of AL and SSL, i.e., SSL propagates label information to unlabeled samples and provides smoothed embeddings for AL, while AL excludes samples with inconsistent predictions and considerable uncertainty for SSL. We estimate unlabeled samples’ inconsistency by augmentation strategies of different granularities, including fine-grained continuous perturbation exploration and coarse-grained data transformations. Moreover, to solve the problem that the utilization efficiency of unlabeled samples is still insufficient in the process of semi-supervised training, we extend our IDEAL to a curriculum-guided version, namely SPL-IDEAL algorithm. The SPL-IDEAL algorithm can regularize the training process towards better regions in parameter space and denoise the pseudo labels with low confidence, achieving better performance. The extensive experiments, in both text and image benchmark datasets, validate the effectiveness of our proposed IDEAL and SPL-IDEAL algorithms, comparing them against state-of-the-art baselines. Two real-world case studies visualize the practical industrial value of applying and deploying the proposed data sampling algorithms.}
}


@article{DBLP:journals/tkde/SilvaLKL24,
	author = {Amila Silva and
                  Ling Luo and
                  Shanika Karunasekera and
                  Christopher Leckie},
	title = {Unsupervised Domain-Agnostic Fake News Detection Using Multi-Modal
                  Weak Signals},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7283--7295},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3392788},
	doi = {10.1109/TKDE.2024.3392788},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SilvaLKL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of social media as one of the main platforms for people to access news has enabled the wide dissemination of fake news, having serious impacts on society. Thus, it is really important to identify fake news with high confidence in a timely manner, which is not feasible using manual analysis. This has motivated numerous studies on automating fake news detection. Most of these approaches are supervised, which requires extensive time and labour to build a labelled dataset. Although there have been limited attempts at unsupervised fake news detection, their performance suffers due to not exploiting the knowledge from various modalities related to news records and due to the presence of various latent biases in the existing news datasets (e.g., unrealistic real and fake news distributions). To address these limitations, this work proposes an effective framework for unsupervised fake news detection, which first embeds the knowledge available in four modalities (i.e., source credibility, textual content, propagation speed, and user credibility) in news records and then proposes (UMD)^{2}, a novel noise-robust self-supervised learning technique, to identify the veracity of news records from the multi-modal embeddings. Also, we propose a novel technique to construct news datasets minimizing the latent biases in existing news datasets. Following the proposed approach for dataset construction, we produce a Large-scale Unlabelled News Dataset consisting 419,351 news articles related to COVID-19, acronymed as LUND-COVID. We trained the proposed unsupervised framework using LUND-COVID to exploit the potential of large datasets, and evaluate it using a set of existing labelled datasets. Our results show that the proposed unsupervised framework largely outperforms existing unsupervised baselines for different tasks such as multi-modal fake news detection, fake news early detection and few-shot fake news detection, while yielding notable improvements for unseen domains during training.}
}


@article{DBLP:journals/tkde/ZhaoYWDT24,
	author = {Han Zhao and
                  Xu Yang and
                  Kun Wei and
                  Cheng Deng and
                  Dacheng Tao},
	title = {Unsupervised Graph Transformer With Augmentation-Free Contrastive
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7296--7307},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3386984},
	doi = {10.1109/TKDE.2024.3386984},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoYWDT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transformers, having the superior ability to capture both adjacent and long-range dependencies, have been applied to the graph representation learning field. Existing methods are permanently established in the supervised setting with several high-quality labels to optimize the graph Transformers effectively. However, such labels are difficult to be obtained in real-world applications, and it remains largely unexplored in unsupervised representation learning that is essential for graph Transformers to be practical. This article first proposes an unsupervised graph Transformer and makes several technical contributions. 1) We first study various typical augmentations on graph contrastive Transformers, and conclude that such augmentations can lead to model degradation due to their domain-agnostic property. On this basis, we propose an Augmentation-free Graph Contrastive Transformer optimized through nearest neighbors to avoid model degradation; 2) Different similarity measures are designed for positive (mutual information) and negative samples (cosine) to improve the contrastive effectiveness; 3) We derive a novel way to precisely maximize mutual information, capturing more discriminative information with an additional entropy maximization. Finally, by performing the augmentation-free graph contrastive learning at different-scale representations, our graph Transformer can learn discriminative representations without supervision. Extensive experiments conducted on various datasets can demonstrate the superiority of our method.}
}


@article{DBLP:journals/tkde/HuangHHWLLC24,
	author = {Chang{-}Qin Huang and
                  Qionghao Huang and
                  Xiaodi Huang and
                  Hua Wang and
                  Ming Li and
                  Kwei{-}Jay Lin and
                  Yi Chang},
	title = {{XKT:} Toward Explainable Knowledge Tracing Model With Cognitive Learning
                  Theories for Questions of Multiple Knowledge Concepts},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7308--7325},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3418098},
	doi = {10.1109/TKDE.2024.3418098},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuangHHWLLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL) based knowledge tracing (KT) models have challenges for uninterpretable prediction and parameter representation in educational applications, though they achieved remarkable outcomes in predicting the exercise performance of students. This paper proposes a novel knowledge tracing model of high precision and interpretability (named XKT) for questions with multiple knowledge concepts based on cognitive learning theories and multidimensional item response theory (MIRT). The XKT consists of three differentiable network components: multi-feature embedding, cognition processing network, and MIRT-based neural predictor, which aim to provide an explainable prediction of student exercise performance. Specifically, in XKT, multi-feature embedding learns the rich semantic representation (e.g., knowledge distribution information) to enhance knowledge tracing using a cognition processing network. The cognition processing network performs selective perception, ability memory processing, and long-term knowledge memory processing to ensure the explainable factor representation for the MIRT-based neural predictor. Lastly, the MIRT-based neural predictor employs psychometric parameters to interpret student exercise predictions better. Extensive experiments on four real-world datasets show that XKT outperforms existing KT methods in predicting future learner responses. Moreover, ablation studies further show that XKT offers good interpretability of student performance predictions with multiple knowledge concepts, indicating excellent potential in real-world educational applications.}
}


@article{DBLP:journals/tkde/YuanMPDZ24,
	author = {Kehua Yuan and
                  Duoqian Miao and
                  Witold Pedrycz and
                  Weiping Ding and
                  Hongyun Zhang},
	title = {Ze-HFS: Zentropy-Based Uncertainty Measure for Heterogeneous Feature
                  Selection and Knowledge Discovery},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {11},
	pages = {7326--7339},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419215},
	doi = {10.1109/TKDE.2024.3419215},
	timestamp = {Tue, 22 Oct 2024 21:09:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuanMPDZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge discovery of heterogeneous data is an active topic in knowledge engineering. Feature selection for heterogeneous data is an important part of effective data analysis. Although there have been many attempts to study the feature selection for heterogeneous data, there are still some challenges, such as the unbalanced problem between the stability and validity of the designed model. Hence, this paper focuses on how to design an effective and robust heterogeneous feature selection method, namely a zentropy-based uncertainty measure for heterogeneous feature selection(Ze-HFS). Different from other entropy-based uncertainty measures, the proposed method does not consider single-level information measures but systematically analyzes and integrates the information between different granular levels, which has an obvious advantage in the study of heterogeneous data knowledge discovery. Specifically, a heterogeneous distance metric is first introduced to construct heterogeneous neighborhood granules and heterogeneous neighborhood rough sets(HNRS). Then, the zentropy-based uncertainty measure is developed by analyzing the granular level structure in the HNRS model. Finally, two significant measures based on the above research are designed for heterogeneous feature selection. Compared with other state-of-the-art methods, the experimental results on 18 public datasets demonstrate the robustness and effectiveness of the proposed method.}
}


@article{DBLP:journals/tkde/WangS24,
	author = {Zhikai Wang and
                  Yanyan Shen},
	title = {A Framework for Elastic Adaptation of User Multiple Intents in Sequential
                  Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7340--7352},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3354796},
	doi = {10.1109/TKDE.2024.3354796},
	timestamp = {Sun, 19 Jan 2025 13:53:51 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, substantial research has been conducted on sequential recommendation, with the objective of forecasting the subsequent item by leveraging a user's historical sequence of interacted items. Prior studies employ both capsule networks and self-attention techniques to effectively capture diverse underlying intents within a user's interaction sequence, thereby achieving the most advanced performance in sequential recommendation. However, users could potentially form novel intents from fresh interactions as the lengths of user interaction sequences grow. Consequently, models need to be continually updated or even extended to adeptly encompass these emerging user intents, referred as incremental multi-intent sequential recommendation. In this paper, we propose an effective Incremental learning framework for user Multi-intent Adaptation in sequential recommendation called IMA, which augments the traditional fine-tuning strategy with the existing-intents retainer, new-intents detector, and projection-based intents trimmer to adaptively expand the model to accommodate user's new intents and prevent it from forgetting user's existing intents. Furthermore, we upgrade the IMA into an Elastic Multi-intent Adaptation (EMA) framework which can elastically remove inactive intents and compress user intent vectors under memory space limit. Extensive experiments on real-world datasets verify the effectiveness of the proposed IMA and EMA on incremental multi-intent sequential recommendation, compared with various baselines.}
}


@article{DBLP:journals/tkde/LiSZKCBH24,
	author = {Ziyu Li and
                  Wenbo Sun and
                  Danning Zhan and
                  Yan Kang and
                  Lydia Yiyu Chen and
                  Alessandro Bozzon and
                  Rihan Hai},
	title = {Amalur: The Convergence of Data Integration and Machine Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7353--7367},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3357389},
	doi = {10.1109/TKDE.2024.3357389},
	timestamp = {Sun, 19 Jan 2025 13:53:52 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiSZKCBH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) training data is often scattered across disparate collections of datasets, called data silos. This fragmentation poses a major challenge for data-intensive ML applications: integrating and transforming data residing in different sources demand a lot of manual work and computational resources. With data privacy constraints, data often cannot leave the premises of data silos; hence model training should proceed in a decentralized manner. In this work, we present a vision of bridging traditional data integration (DI) techniques with the requirements of modern machine learning systems. We explore the possibilities of utilizing metadata obtained from data integration processes for improving the effectiveness, efficiency, and privacy of ML models. Towards this direction, we analyze ML training and inference over data silos. Bringing data integration and machine learning together, we highlight new research opportunities from the aspects of systems, representations, factorized learning, and federated learning.}
}


@article{DBLP:journals/tkde/GuhaKSS24,
	author = {Shubha Guha and
                  Falaah Arif Khan and
                  Julia Stoyanovich and
                  Sebastian Schelter},
	title = {Automated Data Cleaning can Hurt Fairness in Machine Learning-Based
                  Decision Making},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7368--7379},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3365524},
	doi = {10.1109/TKDE.2024.3365524},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GuhaKSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we interrogate whether data quality issues track demographic group membership (based on sex, race and age) and whether automated data cleaning — of the kind commonly used in production ML systems — impacts the fairness of predictions made by these systems. To the best of our knowledge, the impact of data cleaning on fairness in downstream tasks has not been investigated in the literature. We first analyse the tuples flagged by common error detection strategies in five research datasets. We find that, while specific data quality issues, such as higher rates of missing values, are associated with membership in historically disadvantaged groups, poor data quality does not generally track demographic group membership. As a follow-up, we conduct a large-scale empirical study on the impact of automated data cleaning on fairness, involving more than 26,000 model evaluations. We observe that, while automated data cleaning is unlikely to worsen accuracy, it is more likely to worsen fairness than to improve it, especially when the cleaning techniques are not carefully chosen. Furthermore, we find that the positive or negative impact of a particular cleaning technique often depends on the choice of fairness metric and group definition (single-attribute or intersectional). We make our code and experimental results publicly available. The analysis we conducted in this paper is difficult, primarily because it requires that we think holistically about disparities in data quality, disparities in the effectiveness of data cleaning methods, and impacts of such disparities on ML model performance for different demographic groups. Such holistic analysis can and should be supported by data engineering tools, and requires substantial data engineering research. Towards this goal, we discuss open research questions, envision the development of fairness-aware data cleaning methods, and their integration into complex pipelines for ML-based decision making.}
}


@article{DBLP:journals/tkde/BreveCCDP24,
	author = {Bernardo Breve and
                  Loredana Caruccio and
                  Stefano Cirillo and
                  Vincenzo Deufemia and
                  Giuseppe Polese},
	title = {Decentralized and Incremental Discovery of Relaxed Functional Dependencies
                  Using Bitwise Similarity},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7380--7398},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3403928},
	doi = {10.1109/TKDE.2024.3403928},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/BreveCCDP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past decade, there have been numerous extensions to the definition of Functional Dependency (fd), culminating in the introduction of Relaxed Functional Dependency (rfd), offering more flexible constraints compared to traditional fds. This increased flexibility makes rfds well-suited for exploring and profiling data in datasets with lower data quality. However, efficiently identifying rfds within dynamic data sources presents a significant challenge, as it requires processing an entire dataset from scratch whenever modifications occur. To tackle this problem, incremental discovery algorithms have been defined, but they often suffer when the frequency and the size of batches of updates increase. This article presents a new algorithm, namely D-IndiBits, relying on a new decentralized architecture to balance the workload that drives the incremental discovery process of IndiBits, which is based on bitwise operators for computing attribute similarities. Experiments demonstrate D-IndiBits's effectiveness compared to fd and rfd discovery algorithms on both static and dynamic real-world data. With batches of modifications of sizes 10 k and 100 k, D-IndiBits is capable of updating the set of rfds in a few seconds, whereas all other approaches often employ more than 3 hours.}
}


@article{DBLP:journals/tkde/SancaA24,
	author = {Viktor Sanca and
                  Anastasia Ailamaki},
	title = {Efficient Model-Relational Data Management: Challenges and Opportunities},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7399--7409},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3384276},
	doi = {10.1109/TKDE.2024.3384276},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SancaA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As modern data pipelines continue to collect, produce, and store various data formats, extracting and combining value from traditional and context-rich sources becomes unsuitable for RDBMS. To tap into the dark data, domain experts analyze and extract insights and integrate them into various data repositories. This can involve out-of-DBMS processing with high manual effort and suboptimal performance. While AI systems based on ML models can automate the analysis, they can further generate context-rich answers. Using multiple data sources and models further exacerbates the problem of consolidating and analyzing the data of interest. We envision an analytical engine co-optimized with components that enable context-rich analysis. First, as all the data from different sources is expensive to clean ahead of time, we propose using online data integration via model-assisted similarity operations. Second, we aim for a holistic pipeline cost- and rule-based optimization across relational and model-based operators. Third, with increasingly heterogeneous hardware and workloads ranging from relational analytics to generative model inference, we envision a system that adapts to the complex query requirements at runtime. Composing ML-driven insights with established approaches aims to expand decades of research and systems-building effort in making complex functionality and performance effortless for the end user.}
}


@article{DBLP:journals/tkde/PaponMKRHSDMA24,
	author = {Tarikul Islam Papon and
                  Ju Hyoung Mun and
                  Konstantinos Karatsenidis and
                  Shahin Roozkhosh and
                  Denis Hoornaert and
                  Ahmed Sanaullah and
                  Ulrich Drepper and
                  Renato Mancuso and
                  Manos Athanassoulis},
	title = {Effortless Locality on Data Systems Using Relational Fabric},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7410--7422},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3386827},
	doi = {10.1109/TKDE.2024.3386827},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/PaponMKRHSDMA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A key design decision for data systems is whether they follow the row-store or the column-store paradigm. The former supports transactional workloads, while the latter is better for analytical queries. This decision has a significant impact on the entire data system architecture. The multiple-decade-long journey of these two designs has led to a new family of hybrid transactional/analytical processing (HTAP) architectures. Several efforts have been proposed to reap the benefits of both worlds by proposing systems that maintain multiple copies of data (in different physical layouts) and convert them into the desired layout as required. Due to data duplication, the additional necessary bookkeeping, and the cost of converting data between different layouts, these systems compromise between efficient analytics and data freshness. We depart from existing designs by proposing a radically new approach. We ask the question: “What if we could access any layout and ship only the relevant data through the memory hierarchy by transparently converting rows to (arbitrary groups of) columns?” To achieve this functionality, we capitalize on the reinvigorated trend of hardware specialization (that has been accelerated due to the tapering of Moore's law) to propose Relational Fabric, a near-data vertical partitioner that allows memory or storage components to perform on-the-fly transparent data transformation. By exposing an intuitive API, Relational Fabric pushes vertical partitioning to the hardware, which profoundly impacts the process of designing and building data systems. (A) There is no need for data duplication and layout conversion, making HTAP systems viable using a single layout. (B) It simplifies the memory and storage manager that needs to maintain and update a single data layout. (C) It reduces unnecessary data movement through the memory hierarchy allowing for better hardware utilization and, ultimately, better performance. In this paper, we present Relational Fabric for both memory and storage. We present our initial results on Relational Fabric for in-memory systems and discuss the challenges of building this hardware and the opportunities it brings for simplicity and innovation in the data system software stack, including physical design, query optimization, query evaluation, and concurrency control.}
}


@article{DBLP:journals/tkde/ZhangCOSTTXYZ24,
	author = {Bingxue Zhang and
                  Gang Chen and
                  Beng Chin Ooi and
                  Mike Zheng Shou and
                  Kian{-}Lee Tan and
                  Anthony K. H. Tung and
                  Xiaokui Xiao and
                  James Wei Luen Yip and
                  Meihui Zhang},
	title = {Managing Metaverse Data Tsunami: Actionable Insights},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7423--7441},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3354960},
	doi = {10.1109/TKDE.2024.3354960},
	timestamp = {Sun, 19 Jan 2025 13:53:46 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangCOSTTXYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the metaverse the physical space and the virtual space co-exist, and interact simultaneously. While the physical space is virtually enhanced with information, the virtual space is continuously refreshed with real-time, real-world information. To allow users to process and manipulate information seamlessly between the real and digital spaces, novel technologies must be developed. These include smart interfaces, new augmented realities, and efficient data storage, management, and dissemination techniques. In this paper, we first discuss some promising co-space applications. These applications offer opportunities that neither of the spaces can realize on its own. Then, we further discuss several emerging technologies that empower the construction of metaverse. After that, we discuss comprehensively the data centric challenges. Finally, we discuss and envision what are likely to be required from the database and system perspectives.}
}


@article{DBLP:journals/tkde/ZhangWCLDWLR24,
	author = {Weiyan Zhang and
                  Jiacheng Wang and
                  Chuang Chen and
                  Wanpeng Lu and
                  Wen Du and
                  Haofen Wang and
                  Jingping Liu and
                  Tong Ruan},
	title = {A Bidirectional Extraction-Then-Evaluation Framework for Complex Relation
                  Extraction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7442--7454},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3435765},
	doi = {10.1109/TKDE.2024.3435765},
	timestamp = {Mon, 03 Mar 2025 22:25:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangWCLDWLR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relation extraction is an important task in the field of natural language processing. Previous works mainly focus on adopting pipeline methods or joint methods to model relation extraction in general scenarios. However, these existing methods face challenges when adapting to complex relation extraction scenarios, such as handling overlapped triplets, multiple triplets, and cross-sentence triplets. In this paper, we revisit the advantages and disadvantages of the aforementioned methods in complex relation extraction. Based on the in-depth analysis, we propose a novel two-stage bidirectional extract-then-evaluate framework named BeeRe. In the extraction stage, we first obtain the subject set, relation set, and object set. Then, we design subject- and object-oriented triplet extractors to iteratively recurrent obtain candidate triplets, ensuring high recall. In the evaluation stage, we adopt a relation-oriented triplet filter to determine subject-object pairs based on relations in triplets obtained in the first stage, ensuring high precision. We conduct extensive experiments on three public datasets to show that BeeRe achieves state-of-the-art performance in both complex and general relation extraction scenarios. Even when compared to large language models like closed-source/open-source LLMs, BeeRe still has significant performance gains.}
}


@article{DBLP:journals/tkde/GuoLZZLS24,
	author = {Yixiu Guo and
                  Yong Li and
                  Sisi Zhou and
                  Zhenyu Zhang and
                  Zuyi Li and
                  Mohammad Shahidehpour},
	title = {A Data-Driven Three-Stage Adaptive Pattern Mining Approach for Multi-Energy
                  Loads},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7455--7467},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3462770},
	doi = {10.1109/TKDE.2024.3462770},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GuoLZZLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-depth understanding of the multi-energy consumption behavior pattern is the essential to improve the management of multi-energy system (MES). This paper proposes a data-driven three-stage adaptive pattern mining approach for multi-energy loads, which addresses the issues of complex multi-dimensional time-series mining, uncommon daily loads discovery, typical load classification and parameter setting requiring user intervention. In the first stage, the relative state changes over time between different energy loads are excavated based on Autoplait, which realizes time pattern discovery, segmentation and match for multi-dimensional loads. In the second stage, adaptive affinity propagation (AAP) considering trend similarity distance (TSD) is proposed to classify loads into common and uncommon clusters, where uncommon loads are eliminated and daily pattern is obtained by taking average of common loads. In the third stage, AAP with windows dynamic time warping (WDTW) identifies various profiles to obtain typical pattern of daily loads. Specifically, pattern mining provides the key information of multi-energy loads, which is significant to the applications for the demand side, such as load scene compression, load forecasting and demand response analysis. A case study uses MES data from Arizona State University to verify the effectiveness and practicality of the proposed approach.}
}


@article{DBLP:journals/tkde/LiXZWDY24,
	author = {Qian Li and
                  Yunpeng Xiao and
                  Xinming Zhou and
                  Rong Wang and
                  Sirui Duan and
                  Xiang Yu},
	title = {A Derivative Topic Dissemination Model Based on Representation Learning
                  and Topic Relevance},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7468--7482},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3484496},
	doi = {10.1109/TKDE.2024.3484496},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiXZWDY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In social networks, topics often demonstrate a “fission” trend, where new topics arise from existing ones. Effectively predicting collective behavioral patterns during the dissemination of derivative topics is crucial for public opinion management. Addressing the symbiotic, antagonistic nature of “native-derived” topics, a derivative topic propagation model based on representation learning, topic relevance is proposed herein. First, considering the transition in user interest levels, cognitive accumulation at different evolutionary stages of native-derivative topics, a user content representation method, namely DTR2vec, is introduced, based on topic-related feature associations, for learning user content features. Then, evolutionary game theory is introduced by recognizing the symbiotic, antagonistic nature of “native-derived” topics during their propagation. Moreover, implicit relationships between users are explored, user influence is quantified for learning user structural features. Finally, considering the graph convolutional network’s ability to process non-euclidean structured data, the proposed model integrates user content, structural features to predict user forwarding behavior. Experimental results indicate that the proposed model not only effectively predicts the dissemination trends of derivative topics but also more authentically reflects the association, game relationships between native, derivative topics during their dissemination.}
}


@article{DBLP:journals/tkde/ZhuCWXS24,
	author = {Huijuan Zhu and
                  Xilong Chen and
                  Liangmin Wang and
                  Zhicheng Xu and
                  Victor S. Sheng},
	title = {A Dynamic Analysis-Powered Explanation Framework for Malware Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7483--7496},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3436891},
	doi = {10.1109/TKDE.2024.3436891},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuCWXS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning has been widely adopted in Android malicious software (malware) detection. However, poor explanation in deep learning-based detection models severely undermines user trusts and poses a significant obstacle to their practical promotion in critical security domains. Some studies strive to uncover the rationale behind a model's decision. Unfortunately, these efforts are often hindered by the limitations of feature extraction methods, such as primarily relying on static analysis to derive separate and approximate behavioral descriptions of applications (apps). As a result, establishing a reliable interpretation for deep learning-based malware detection models remains an open issue. In this work, we propose a novel framework XDeepMal to interpret deep learning-based malware detection models. Specifically, in XDeepMal, we formulate a dynamic analysis tool XTracer+ to capture runtime behaviors of apps and automatically generate their continuous behavior trajectories. Then, we propose a novel interpreter to pinpoint certainty behavior fragments that are crucial for deep learning models to make their decisions. This approach regards the identification of the most critical fragments as an optimization problem and leverages heuristic algorithms for implementation. We conduct extensive experiments on a real-world dataset to investigate the effectiveness and reliability of XDeepMal. These experiments cover intuitive case studies (malware family and individual app) and in-depth quantitative analysis. Additionally, we evaluate its coverage and efficiency. Our experimental results demonstrate that XDeepMal is capable of generating convincing interpretations for deep learning (e.g., Transformer) based models within feasible inference time, which greatly benefits security analysts in accurately comprehending why an app is identified as malware by deep learning-based detection models.}
}


@article{DBLP:journals/tkde/ZhangZLCWKYD24,
	author = {Yi Zhang and
                  Yuying Zhao and
                  Zhaoqing Li and
                  Xueqi Cheng and
                  Yu Wang and
                  Olivera Kotevska and
                  Philip S. Yu and
                  Tyler Derr},
	title = {A Survey on Privacy in Graph Neural Networks: Attacks, Preservation,
                  and Applications},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7497--7515},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3454328},
	doi = {10.1109/TKDE.2024.3454328},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZLCWKYD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.}
}


@article{DBLP:journals/tkde/WenLWMCHGLJZZYW24,
	author = {Haomin Wen and
                  Youfang Lin and
                  Lixia Wu and
                  Xiaowei Mao and
                  Tianyue Cai and
                  Yunfeng Hou and
                  Shengnan Guo and
                  Yuxuan Liang and
                  Guangyin Jin and
                  Yiji Zhao and
                  Roger Zimmermann and
                  Jieping Ye and
                  Huaiyu Wan},
	title = {A Survey on Service Route and Time Prediction in Instant Delivery:
                  Taxonomy, Progress, and Prospects},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7516--7535},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3441309},
	doi = {10.1109/TKDE.2024.3441309},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WenLWMCHGLJZZYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Instant delivery services, such as food delivery and package delivery, have achieved explosive growth in recent years by providing customers with daily-life convenience. An emerging research area within these services is service Route&Time Prediction (RTP), which aims to estimate the future service route as well as the arrival time of a given worker. As one of the most crucial tasks in those service platforms, RTP stands central to enhancing user satisfaction and trimming operational expenditures on these platforms. Despite a plethora of algorithms developed to date, there is no systematic, comprehensive survey to guide researchers in this domain. To fill this gap, our work presents the first comprehensive survey that methodically categorizes recent advances in service route and time prediction. We start by defining the RTP challenge and then delve into the metrics that are often employed. Following that, we scrutinize the existing RTP methodologies, presenting a novel taxonomy of them. We categorize these methods based on three criteria: (i) type of task, subdivided into only-route prediction, only-time prediction, and joint route&time prediction; (ii) model architecture, which encompasses sequence-based and graph-based models; and (iii) learning paradigm, including Supervised Learning (SL) and Deep Reinforcement Learning (DRL). Conclusively, we highlight the limitations of current research and suggest prospective avenues. We believe that the taxonomy, progress, and prospects introduced in this paper can significantly promote the development of this field.}
}


@article{DBLP:journals/tkde/MaLZHZYK24,
	author = {Qianli Ma and
                  Zhen Liu and
                  Zhenjing Zheng and
                  Ziyang Huang and
                  Siying Zhu and
                  Zhongzhong Yu and
                  James T. Kwok},
	title = {A Survey on Time-Series Pre-Trained Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7536--7555},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3475809},
	doi = {10.1109/TKDE.2024.3475809},
	timestamp = {Fri, 31 Jan 2025 08:10:55 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MaLZHZYK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, pre-trained models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments involving 27 methods, 434 datasets, and 679 transfer learning scenarios are conducted to analyze the advantages and disadvantages of transfer learning strategies, Transformer-based models, and representative TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future work.}
}


@article{DBLP:journals/tkde/LiXCLC24,
	author = {Xueqi Li and
                  Guoqing Xiao and
                  Yuedan Chen and
                  Kenli Li and
                  Gao Cong},
	title = {Accurate and Scalable Graph Convolutional Networks for Recommendation
                  Based on Subgraph Propagation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7556--7568},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3467333},
	doi = {10.1109/TKDE.2024.3467333},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiXCLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recommendation systems, Graph Convolutional Networks (GCNs) often suffer from significant computational and memory cost when propagating features across the entire user-item graph. While various sampling strategies have been introduced to reduce the cost, the challenge of neighbor explosion persists, primarily due to the iterative nature of neighbor aggregation. This work focuses on exploring subgraph propagation for scalable recommendation by addressing two primary challenges: efficient and effective subgraph construction and subgraph sparsity. To address these challenges, we propose a novel GCN model for recommendation based on Subgraph propagation, called SubGCN. One key component of SubGCN is BiPPR, a technique that fuses both source- and target-based Personalized PageRank (PPR) approximations, to overcome the challenge of efficient and effective subgraph construction. Furthermore, we propose a source-target contrastive learning scheme to mitigate the impact of subgraph sparsity for SubGCN. We conduct extensive experiments on two large and two medium-sized datasets to evaluate the scalability, efficiency, and effectiveness of SubGCN. On medium-sized datasets, compared to full-graph GCNs, SubGCN achieves competitive accuracy while using only 23.79% training time on Gowalla and 16.3% on Yelp2018. On large datasets, where full-graph GCNs ran out of the GPU memory, our proposed SubGCN outperforms widely used sampling strategies in terms of training efficiency and recommendation accuracy.}
}


@article{DBLP:journals/tkde/WangZYZW24,
	author = {Huimin Wang and
                  Yunyan Zhang and
                  Yifan Yang and
                  Yefeng Zheng and
                  Kam{-}Fai Wong},
	title = {Acquiring New Knowledge Without Losing Old Ones for Effective Continual
                  Dialogue Policy Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7569--7584},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3344727},
	doi = {10.1109/TKDE.2023.3344727},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangZYZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dialogue policy learning is the core decision-making module of a task-oriented dialogue system. Its primary objective is to assist users to achieve their goals effectively in as few turns as possible. A practical dialogue-policy agent must be able to expand its knowledge to handle new scenarios efficiently without affecting its performance. Nevertheless, when adapting to new tasks, existing dialogue-policy agents often fail to retain their existing (old) knowledge. To overcome this predicament, we propose a novel continual dialogue-policy model which tackles the issues of “not forgetting the old” and “acquiring the new” from three different aspects: (1) For effective old-task preservation, we introduce the forgetting preventor which uses a behavior cloning technique to force the agent to take actions consistent with the replayed experience to retain the policy trained on historic tasks. (2) For new-task acquisition, we introduce the adaption accelerator which employs an invariant risk minimization mechanism to produce a stable policy predictor to avoid spurious corrections in training data. (3) For reducing the storage cost of the replayed experience, we introduce a replay manager which helps regularly clean up the old data. The effectiveness of the proposed model is evaluated both theoretically and experimentally and demonstrated favorable results.}
}


@article{DBLP:journals/tkde/YunYYC24,
	author = {Fan Yun and
                  Zhiwen Yu and
                  Kaixiang Yang and
                  C. L. Philip Chen},
	title = {AdaBoost-Stacking Based on Incremental Broad Learning System},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7585--7599},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3433587},
	doi = {10.1109/TKDE.2024.3433587},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YunYYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the advantages of fast training speed and competitive performance, Broad Learning System (BLS) has been widely used for classification tasks across various domains. However, the random weight generation mechanism in BLS makes the model unstable, and the performance of BLS may be limited when dealing with some complex datasets. On the other hand, the instability of BLS brings diversity to ensemble learning, and ensemble methods can also reduce the variance and bias of the single BLS. Therefore, we propose an ensemble learning algorithm based on BLS, which includes three modules. To improve the stability and generalization ability of BLS, we utilize BLS as the base classifier in an AdaBoost framework first. Taking advantage of the incremental learning mechanism of BLS, we then propose a selective ensemble method to raise the accuracy and diversity of the BLS ensemble method. In addition, based on the former selective Adaboost framework, we suggest a hierarchical ensemble algorithm, which combines sample and feature dimensions to further improve the fitting ability of the ensemble BLS. Extensive experiments have demonstrated that the proposed method performs better than the original BLS and other state-of-the-art models, proving the effectiveness and versatility of our proposed approaches.}
}


@article{DBLP:journals/tkde/ZhaoYYXZLLC24,
	author = {Ziwei Zhao and
                  Yu Yang and
                  Zikai Yin and
                  Tong Xu and
                  Xi Zhu and
                  Fake Lin and
                  Xueying Li and
                  Enhong Chen},
	title = {Adversarial Attack and Defense on Discrete Time Dynamic Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7600--7611},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3438238},
	doi = {10.1109/TKDE.2024.3438238},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoYYXZLLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph learning methods have achieved remarkable performance in various domains such as social recommendation, financial fraud detection, and so on. In real applications, the underlying graph is often dynamically evolving and thus, some recent studies focus on integrating the temporal topology information of graphs into the GNN for learning graph embedding. However, the robustness of training GNNs for dynamic graphs has not been discussed so far. The major reason is how to attack dynamic graph embedding still remains largely untouched, let alone how to defend against the attacks. To enable robust training of GNNs for dynamic graphs, in this paper, we investigate the problem of how to generate attacks and defend against attacks for dynamic graph embedding. Attacking dynamic graph embedding is more challenging than attacking static graph embedding as we need to understand the temporal dynamics of graphs as well as its impact on the embedding and the injected perturbations should be distinguished from the natural evolution. In addition, the defense is very challenging as the perturbations may be hidden within the natural evolution. To tackle these technical challenges, in this paper, we first develop a novel gradient-based attack method from an optimization perspective to generate perturbations to fool dynamic graph learning methods, where a key idea is to use gradient dynamics to attack the natural dynamics of the graph. Further, we borrow the idea of the attack method and integrate it with adversarial training to train a more robust dynamic graph learning method to defend against hand-crafted attacks. Finally, extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed attack and defense method, where our defense method not only achieves comparable performance on clean graphs but also significantly increases the defense performance on attacked graphs.}
}


@article{DBLP:journals/tkde/ZhengMZZZYYZJ24,
	author = {Bolong Zheng and
                  Lingfeng Ming and
                  Kai Zeng and
                  Mengtao Zhou and
                  Xinyong Zhang and
                  Tao Ye and
                  Bin Yang and
                  Xiaofang Zhou and
                  Christian S. Jensen},
	title = {Adversarial Graph Neural Network for Multivariate Time Series Anomaly
                  Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7612--7626},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419891},
	doi = {10.1109/TKDE.2024.3419891},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengMZZZYYZJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection is one of the most significant tasks in multivariate time series analysis, while it remains challenging to model complex patterns for improving detection accuracy and to interpret the root causes of anomalies. However, existing studies either consider only the temporal dependencies, or simply reconstruct the original input for detection, both neglecting the hidden relationships among multivariate. We propose an adversarial graph neural network based anomaly detection model, called SGAT-AE, which consists of a Self-learning Graph ATtention network (SGAT), an Auto-Encoder (AE), and an adversarial training component. Specifically, SGAT is a prediction model that discovers the graph dependency relationships among multivariate and acts as a sample generator to confuse AE, while AE reconstructs the samples and acts as a discriminator that distinguishes a real sample from a generated one. A novel adversarial training between SGAT and AE is applied to amplify the errors of anomalies such that the prediction performance of SGAT is improved and the overfitting of AE is avoided. In addition, we aggregate the prediction error, the reconstruction error, and the adversarial error for anomaly detection, and develop a graph based anomaly interpretation method that locates the root causes from both local and global perspectives. Extensive experiments with five real-world data offer evidence that the proposed solution SGAT-AE is capable of achieving better performance when compared with the state-of-the-art proposals.}
}


@article{DBLP:journals/tkde/ZhangWCJW24,
	author = {Xinyin Zhang and
                  Ran Wang and
                  Shuyue Chen and
                  Yuheng Jia and
                  Debby Dan Wang},
	title = {{AME-LSIFT:} Attention-Aware Multi-Label Ensemble With Label Subset-SpecIfic
                  FeaTures},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7627--7642},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3447878},
	doi = {10.1109/TKDE.2024.3447878},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangWCJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-label ensemble can achieve superior performance on multi-label learning problems by integrating a number of base classifiers. In existing multi-label ensemble methods, the base classifiers are usually trained with the same original features; it is difficult for each base classifier to capture label-relevant or label subset-relevant information. Meanwhile, the manually designed integrating strategies cannot automatically distinguish the importance of the base classifiers, which also lack flexibility and scalability. In order to resolve these problems, this paper proposes a new multi-label ensemble framework, named Attention-aware Multi-label Ensemble with Label Subset-specIfic FeaTures (AME-LSIFT). It utilizes c\n-means clustering to produce Label Subset-specIfic FeaTures (LSIFT), constructs a neural network based model for each label subset, and integrates the base models with a dynamic and automatic attention-aware mechanism. Moreover, an objective function that considers both the label subset accuracy and ensemble accuracy is developed for training the proposed AME-LSIFT. Experiments conducted on ten benchmark datasets demonstrate the superior performance of the proposed method compared with state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/ZhaoNWWL24,
	author = {Zihua Zhao and
                  Feiping Nie and
                  Rong Wang and
                  Zheng Wang and
                  Xuelong Li},
	title = {An Balanced, and Scalable Graph-Based Multiview Clustering Method},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7643--7656},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3443534},
	doi = {10.1109/TKDE.2024.3443534},
	timestamp = {Mon, 03 Mar 2025 22:25:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoNWWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, graph-based multiview clustering methods have become a research hotspot in the clustering field. However, most existing methods lack consideration of cluster balance in their results. In fact, cluster balance is crucial in many real-world scenarios. Additionally, graph-based multiview clustering methods often suffer from high time consumption and cannot handle large-scale datasets. To address these issues, this paper proposes a novel graph-based multiview clustering method. The method is built upon the bipartite graph. Specifically, it employs a label propagation mechanism to update the smaller anchor label matrix rather than the sample label matrix, significantly reducing the computational cost. The introduced balance constraint in the proposed model contributes to achieving balanced clustering results. The entire clustering model combines information from multiple views through graph fusion. The joint graph and view weight parameters in the model are obtained through task-driven self-supervised learning. Moreover, the model can directly obtain clustering results without the need for the two-stage processing typically used in general spectral clustering. Finally, extensive experiments on toy datasets and real-world datasets are conducted to validate the superiority of the proposed method in terms of clustering performance, clustering balance, and time expenditure.}
}


@article{DBLP:journals/tkde/WuZZL24,
	author = {Yang Wu and
                  Xuanhe Zhou and
                  Yong Zhang and
                  Guoliang Li},
	title = {Automatic Index Tuning: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7657--7676},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3422006},
	doi = {10.1109/TKDE.2024.3422006},
	timestamp = {Sun, 22 Dec 2024 15:49:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Index tuning plays a crucial role in facilitating the efficiency of data retrieval within database systems, which adjusts index settings to optimize the database performance. Recently, with the growth of data volumes, the complexity of workloads, and the diversification of database applications, various Automatic Index Tuning (AIT) methods have been proposed to address these challenges. In this paper, we provide a comprehensive survey on Automatic Index Tuning. First, we overview the AIT techniques from multiple aspects, including i) problem definition, ii) workflow, iii) framework, iv) index types, v) index interaction, vi) changing factors, vii) automation level, and show the development history. Second, we summarize techniques in the main modules of AIT, including preprocessing, index benefit estimation, and index selection. Preprocessing involves workload compression, index candidate generation, feature representation of workloads and databases, and workload reduction. Index benefit estimation approaches are categorized into empirical methods and machine learning based methods. Index selection involves algorithms of offline AIT and online AIT. Moreover, we summarize the commonly-used datasets in AIT and discuss the applications of index tuning in commercial and opensource database products. Finally, we outline potential future research directions. Our survey aims to enhance both general knowledge and in-depth insights into AIT, and inspire researchers to address the ongoing challenges.}
}


@article{DBLP:journals/tkde/ZhangYZYZL24,
	author = {Shiding Zhang and
                  Jianye Yang and
                  Wenjie Zhang and
                  Shiyu Yang and
                  Ying Zhang and
                  Xuemin Lin},
	title = {BigSet: An Efficient Set Intersection Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7677--7691},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3432595},
	doi = {10.1109/TKDE.2024.3432595},
	timestamp = {Tue, 24 Dec 2024 22:38:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangYZYZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Set intersection is a fundamental operation in many applications, such as common neighbor computation in graph-based algorithms, set similarity computation, item recommendation, etc. In the literature, many set intersection methods are proposed. We observe that the state-of-the-art algorithm {\\sf RCode}\nbears several limitations, such as high index time complexity, inefficient for large-sized sets, and not friendly to the generic set intersection. In this paper, we introduce the Bucket Signature for Set ({\\sf BigSet}\n), an efficient generic set intersection algorithm. {\\sf BigSet}\nconsists of two phases, namely the preprocessing phase and the query phase. In the preprocessing phase, {\\sf BigSet}\npartitions the elements of a record into O(2^{k})\nbuckets and uses a bitmap to indicate the status of the buckets where n\nis the record length and k\nis the number of bits in the signature. In the query phase, {\\sf BigSet}\ncalculates the results using a candidate generating-and-verification framework. Specifically, a set of candidate elements is identified as those falling in the same buckets. Then, for each bucket, {\\sf BigSet}\ncollects the common elements using a merge-based method. To improve the performance, we introduce two optimizations, including bucket sharing and size-aware signature construction techniques. We conduct experiments on 10 real graph datasets and 5 real generic set datasets to evaluate the performance of our proposals. The experiment results show that {\\sf BigSet}\nis 20× faster than the leading generic set intersection algorithms. Besides it outperforms the {\\sf RCode}\nwith 5× speedup, and while uses up to 8× less memory.}
}


@article{DBLP:journals/tkde/HsuLCTS24,
	author = {Bay{-}Yuan Hsu and
                  Chia{-}Hsun Lu and
                  Ming{-}Yi Chang and
                  Chih{-}Ying Tseng and
                  Chih{-}Ya Shen},
	title = {Budget-Constrained Ego Network Extraction With Maximized Willingness},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7692--7707},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3446169},
	doi = {10.1109/TKDE.2024.3446169},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HsuLCTS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many large-scale machine learning approaches and graph algorithms are proposed recently to address a variety of problems in online social networks (OSNs). To evaluate and validate these algorithms and models, the data of ego-centric networks (ego networks) are widely adopted. Therefore, effectively extracting large-scale ego networks from OSNs becomes an important issue, particularly when privacy policies become increasingly strict nowadays. In this paper, we study the problem of extracting ego network data by considering jointly the user willingness, crawling cost, and structure of the network. We formulate a new research problem, named Structure and Willingness Aware Ego Network Extraction (SWAN) and analyze its NP-hardness. We first propose a (1-\\frac{1}{e})-approximation algorithm, named Tristar-Optimized Ego Network Identification with Maximum Willingness (TOMW). In addition to the deterministic approximation algorithm, we also propose to automatically learn an effective heuristic approach with machine learning, to avoid the huge efforts for human to devise a good algorithm. The learning approach is named Willingness-maximized and Structure-aware Ego Network Extraction with Reinforcement Learning (WSRL), in which we propose a novel constrastive learning strategy, named Contrastive Learning with Performance-boosting Graph Augmentation. We recruited 1,810 real-world participants and conducted an evaluation study to validate our problem formulation and proposed approaches. Moreover, experimental results on real social network datasets show that the proposed approaches outperform the other baselines significantly.}
}


@article{DBLP:journals/tkde/DingCGHJ24,
	author = {Weiping Ding and
                  Xiaotian Cheng and
                  Yu Geng and
                  Jiashuang Huang and
                  Hengrong Ju},
	title = {C2F-Explainer: Explaining Transformers Better Through a Coarse-to-Fine
                  Strategy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7708--7724},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3443888},
	doi = {10.1109/TKDE.2024.3443888},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DingCGHJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transformer interpretability research is a hot topic in the area of deep learning. Traditional interpretation methods mostly use the final layer output of the Transformer encoder as masks to generate an explanation map. However, These approaches overlook two crucial aspects. At the coarse-grained level, the mask may contain uncertain information, including unreliable and incomplete object location data; at the fine-grained level, there is information loss on the mask, resulting in spatial noise and detail loss. To address these issues, in this paper, we propose a two-stage coarse-to-fine strategy (C2F-Explainer) for improving Transformer interpretability. Specifically, we first design a sequential three-way mask (S3WM) module to handle the problem of uncertain information at the coarse-grained level. This module uses sequential three-way decisions to process the mask, preventing uncertain information on the mask from impacting the interpretation results, thus obtaining coarse-grained interpretation results with accurate position. Second, to further reduce the impact of information loss at the fine-grained level, we devised an attention fusion (AF) module inspired by the fact that self-attention can capture global semantic information, AF aggregates the attention matrix to generate a cross-layer relation matrix, which is then used to optimize detailed information on the interpretation results and produce fine-grained interpretation results with clear and complete edges. Experimental results show that the proposed C2F-Explainer has good interpretation results on both natural and medical image datasets, and the mIoU is improved by 2.08% on the PASCAL VOC 2012 dataset.}
}


@article{DBLP:journals/tkde/CaoWYL24,
	author = {Fuyuan Cao and
                  Yunxia Wang and
                  Kui Yu and
                  Jiye Liang},
	title = {Causal Discovery From Unknown Interventional Datasets Over Overlapping
                  Variable Sets},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7725--7742},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3443997},
	doi = {10.1109/TKDE.2024.3443997},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CaoWYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inferring causal structures from experimentation is a challenging task in many fields. Most causal structure learning algorithms with unknown interventions are proposed to discover causal relationships over an identical variable set. However, often due to privacy, ethical, financial, and practical concerns, the variable sets observed by multiple sources or domains are not entirely identical. While a few algorithms are proposed to handle the partially overlapping variable sets, they focus on the case of known intervention targets. Therefore, to be close to the real-world environment, we consider discovering causal relationships over overlapping variable sets under the unknown intervention setting and exploring a scenario where a problem is studied across multiple domains. Here, we propose an algorithm for discovering the causal relationships over the integrated set of variables from unknown interventions, mainly handling the entangled inconsistencies caused by the incomplete observation of variables and unknown intervention targets. Specifically, we first distinguish two types of inconsistencies and then deal with respectively them by presenting some lemmas. Finally, we construct a fusion rule to combine learned structures of multiple domains, obtaining the final structures over the integrated set of variables. Theoretical analysis and experimental results on synthetic, benchmark, and real-world datasets have verified the effectiveness of the proposed algorithm.}
}


@article{DBLP:journals/tkde/CaoSVB24,
	author = {Zhenxiang Cao and
                  Nick Seeuws and
                  Maarten De Vos and
                  Alexander Bertrand},
	title = {Change Point Detection in Multi-Channel Time Series via a Time-Invariant
                  Representation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7743--7756},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3347356},
	doi = {10.1109/TKDE.2023.3347356},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CaoSVB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Change Point Detection (CPD) refers to the task of identifying abrupt changes in the characteristics or statistics of time series data. Recent advancements have led to a shift away from traditional model-based CPD approaches, which rely on predefined statistical distributions, toward neural network-based and distribution-free methods using autoencoders. However, many state-of-the-art methods in this category often neglect to explicitly leverage spatial information across multiple channels, making them less effective at detecting changes in cross-channel statistics. In this paper, we introduce an unsupervised, distribution-free CPD method that explicitly incorporates both temporal and spatial (cross-channel) information in multi-channel time series data based on the so-called Time-Invariant Representation (TIRE) autoencoder. Our evaluation, conducted on both simulated and real-life datasets, illustrates the significant advantages of our proposed multi-channel TIRE (MC-TIRE) method, which consistently delivers more accurate CPD results.}
}


@article{DBLP:journals/tkde/NiranjanR24,
	author = {Ranjani Niranjan and
                  Sachit Rao},
	title = {Classification With Trust: {A} Supervised Approach Based on Sequential
                  Ellipsoidal Partitioning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7757--7771},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3345658},
	doi = {10.1109/TKDE.2023.3345658},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NiranjanR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Standard metrics of performance of classifiers, such as accuracy and sensitivity, do not reveal the trust or confidence in the predicted labels of data. While other metrics such as the computed probability of a label or the signed distance from a hyperplane can act as a trust measure, these are subjected to heuristic thresholds. This paper presents a convex optimization-based supervised classifier that sequentially partitions a dataset into several ellipsoids, where each ellipsoid contains nearly all points of the same label. By stating classification rules based on this partitioning, Bayes’ formula is then applied to calculate a trust score to a label assigned to a test datapoint determined from these rules. The proposed Sequential Ellipsoidal Partitioning Classifier (SEP-C) exposes dataset irregularities, such as degree of overlap, without requiring a separate exploratory data analysis. The rules of classification, which are free of hyperparameters, are also not affected by class-imbalance, the underlying data distribution, or number of features. SEP-C does not require the use of non-linear kernels when the dataset is not linearly separable. The performance, and comparison with other methods, of SEP-C is demonstrated on the XOR-problem, circle dataset, and other open-source datasets.}
}


@article{DBLP:journals/tkde/DongZLZ24,
	author = {Haowen Dong and
                  Chao Zhang and
                  Guoliang Li and
                  Huanchen Zhang},
	title = {Cloud-Native Databases: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7772--7791},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3397508},
	doi = {10.1109/TKDE.2024.3397508},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DongZLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud databases have been widely accepted and deployed due to their unique advantages, such as high elasticity, high availability, and low cost. Many new techniques, such as compute-storage disaggregation and the log is the database, have been proposed recently to seek for higher elasticity and lower cost. To better harness the power of cloud databases, it is crucial to study and compare the pros and cons of their key techniques. In this paper, we offer a comprehensive survey of cloud-native databases. Particularly, we investigate and summarize the state-of-the-art cloud-native OLTP and OLAP databases, respectively. In the first part, we discuss three types of architectures of cloud-native OLTP database. Then we introduce their key techniques including data placement strategy, storage layer consistency, compute layer consistency, multi-layer recovery, and HTAP optimization. In the second part, we present two kinds of architectures of cloud-native OLAP databases. Then we take a deep dive into their key techniques regarding storage management, query processing, serverless computing, data protection, and machine learning in databases. Finally, we discuss the research challenges and opportunities.}
}


@article{DBLP:journals/tkde/ChansonLMT24,
	author = {Alexandre Chanson and
                  Nicolas Labroche and
                  Patrick Marcel and
                  Vincent T'kindt},
	title = {Comparison Queries Generation Using Mathematical Programming for Exploratory
                  Data Analysis},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7792--7804},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3474828},
	doi = {10.1109/TKDE.2024.3474828},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChansonLMT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Exploratory Data Analysis (EDA) is the interactive process of gaining insights from a dataset. Comparisons are popular insights that can be specified with comparison queries, i.e., specifications of the comparison of subsets of data. In this work, we consider the problem of automatically computing sequences of comparison queries that are coherent, significant and whose overall cost is bounded. Such an automation is usually done by either generating all insights and solving a multi-criteria optimization problem, or using reinforcement learning. In the first case, a large search space has to be explored using exponential algorithms or dedicated heuristics. In the second case, a dataset-specific, time and energy-consuming training, is necessary. We contribute with a novel approach, consisting of decomposing the optimization problem in two: the original problem, that is solved over a smaller search space, and a new problem of generating comparison queries, aiming at generating only queries improving existing solutions of the first problem. This allows to explore only a portion of the search space, without resorting to reinforcement learning. We show that this approach is effective, in that it finds good solutions to the original multi-criteria optimization problem, and efficient, allowing to generate sequences of comparisons in reasonable time.}
}


@article{DBLP:journals/tkde/YangLZTL24,
	author = {Li Yang and
                  Zhipeng Luo and
                  Shiming Zhang and
                  Fei Teng and
                  Tianrui Li},
	title = {Continual Learning for Smart City: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7805--7824},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3447123},
	doi = {10.1109/TKDE.2024.3447123},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangLZTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning. 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3) Challenges. We discuss current problems and challenges and envision several promising research directions. We believe this survey can help relevant researchers quickly familiarize themselves with the current state of continual learning research used in smart city development and direct them to future research trends.}
}


@article{DBLP:journals/tkde/DaiWXLL24,
	author = {Xiangxiang Dai and
                  Zhiyong Wang and
                  Jize Xie and
                  Xutong Liu and
                  John C. S. Lui},
	title = {Conversational Recommendation With Online Learning and Clustering
                  on Misspecified Users},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7825--7838},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3423442},
	doi = {10.1109/TKDE.2024.3423442},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DaiWXLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the domain of conversational recommendation systems (CRSs), the development of recommenders capable of eliciting user preferences through conversation has marked a significant advancement. These systems have been enhanced by incorporating conversational key-terms related to items, which streamline the recommendation process by reducing the extensive exploration that traditional interactive recommenders necessitate. Despite these advancements, CRSs still face significant challenges. The vast number of users and the difficulty in accurately capturing preferences lead to persistent inaccuracies, even when direct user interactions are employed to refine the understanding of user preferences. To tackle these challenges, we propose two innovative bandit algorithms: RCLUMB (Robust Clustering of Misspecified Bandits) and RSCLUMB (Robust Set-based Clustering of Misspecified Bandits). These algorithms employ dynamic graphs and evolving cluster sets, respectively, to represent the changing structure of user preferences, thus leveraging collaborative user preferences to accelerate the learning process. Our algorithms are designed to be resilient against errors in preference modeling and the resulting inaccuracies in clustering. We rigorously analyze the performance of our algorithms and establish regret upper bounds of O(\\epsilon _*T\\sqrt{md\\log T} + d\\sqrt{mT}\\log T)\nunder milder assumptions than previous works, matching the state-of-the-art results in several degenerate cases. Through extensive experiments on synthetic and real-world datasets, our algorithms demonstrate superior performance over existing algorithms.}
}


@article{DBLP:journals/tkde/LiLSL24a,
	author = {Zhong Li and
                  Sheng Liang and
                  Jiayang Shi and
                  Matthijs van Leeuwen},
	title = {Cross-Domain Graph Level Anomaly Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7839--7850},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3462442},
	doi = {10.1109/TKDE.2024.3462442},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiLSL24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing graph level anomaly detection methods are predominantly unsupervised due to high costs for obtaining labels, yielding sub-optimal detection accuracy when compared to supervised methods. Moreover, they heavily rely on the assumption that the training data exclusively consists of normal graphs. Hence, even the presence of a few anomalous graphs can lead to substantial performance degradation. To alleviate these problems, we propose a cross-domain graph level anomaly detection method, aiming to identify anomalous graphs from a set of unlabeled graphs (target domain) by using easily accessible normal graphs from a different but related domain (source domain). Our method consists of four components: a feature extractor that preserves semantic and topological information of individual graphs while incorporating the distance between different graphs; an adversarial domain classifier to make graph level representations domain-invariant; a one-class classifier to exploit label information in the source domain; and a class aligner to align classes from both domains based on pseudolabels. Experiments on seven benchmark datasets show that the proposed method largely outperforms state-of-the-art methods.}
}


@article{DBLP:journals/tkde/YeYSYZ24,
	author = {Mang Ye and
                  Yi Yu and
                  Ziqin Shen and
                  Wei Yu and
                  Qingyan Zeng},
	title = {Cross-Feature Interactive Tabular Data Modeling With Multiplex Graph
                  Neural Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7851--7864},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3440654},
	doi = {10.1109/TKDE.2024.3440654},
	timestamp = {Mon, 03 Mar 2025 22:25:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YeYSYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rising popularity of tabular data in data science applications has led to a surge of interest in utilizing deep neural networks (DNNs) to address tabular problems. Existing deep neural network methods are not effective in handling two fundamental challenges that are inherent in tabular data: permutation invariance (where the labels remain unchanged regardless of element order) and local dependency (where predictive labels are solely determined by local features). Furthermore, given the inherent heterogeneity among elements in tabular data, effectively capturing heterogeneous feature interactions remains unresolved. In this paper, we propose a novel Multiplex Cross-Feature Interaction Network (MPCFIN) by explicitly and systematically modeling feature relations with interactive graph neural networks. Specifically, MPCFIN first learns the most relevant features associated with individual features, and merges them to form cross-feature embedding. Subsequently, we design a multiplex graph neural network to learn enhanced representation for each sample. Comprehensive experiments on seven datasets demonstrate that MPCFIN exhibits superior performance over deep neural network methods in modeling the tabular data, showcasing consistent interpretability in its cross-feature embedding module for medical diagnosis applications.}
}


@article{DBLP:journals/tkde/LiYGWZL24,
	author = {Yujie Li and
                  Xin Yang and
                  Qiang Gao and
                  Hao Wang and
                  Junbo Zhang and
                  Tianrui Li},
	title = {Cross-Regional Fraud Detection via Continual Learning With Knowledge
                  Transfer},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7865--7877},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3451161},
	doi = {10.1109/TKDE.2024.3451161},
	timestamp = {Mon, 03 Feb 2025 16:10:30 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiYGWZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fraud detection poses a fundamental yet challenging problem to mitigate various risks associated with fraudulent activities. However, existing methods are limited by their reliance on static data within single geographical regions, thereby restricting the trained model’s adaptability across different regions. Practically, when enterprises expand their business into new cities or countries, training a new model from scratch can incur high computational costs and lead to catastrophic forgetting (CF). To address these limitations, we propose cross-regional fraud detection as an incremental learning problem, enabling the development of a unified model capable of adapting across diverse regions without suffering from CF. Subsequently, we introduce Cross-Regional Continual Learning (CCL), a novel paradigm that facilitates knowledge transfer and maintains performance when incrementally training models from previously learned regions to new ones. Specifically, CCL utilizes prototype-based knowledge replay for effective knowledge transfer while implementing a parameter smoothing mechanism to alleviate forgetting. Furthermore, we construct heterogeneous trade graphs (HTGs) and leverage graph-based backbones to enhance knowledge representation and facilitate knowledge transfer by uncovering intricate semantics inherent in cross-regional datasets. Extensive experiments demonstrate the superiority of our proposed method over baseline approaches and its substantial improvement in cross-regional fraud detection performance.}
}


@article{DBLP:journals/tkde/LiuLW24,
	author = {Bin Liu and
                  Qin Luo and
                  Bang Wang},
	title = {Debiased Pairwise Learning for Implicit Collaborative Filtering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7878--7892},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3479240},
	doi = {10.1109/TKDE.2024.3479240},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning representations from pairwise comparisons has achieved significant success in various fields, including computer vision and information retrieval. In recommendation systems, collaborative filtering algorithms based on pairwise learning are also rooted in this approach. However, a major challenge in collaborative filtering is the lack of labels for negative instances in implicit feedback data, leading to the inclusion of false negatives among randomly selected instances. This issue causes biased optimization objectives and results in biased parameter estimation. In this paper, we propose a novel method to address learning biases arising from implicit feedback data and introduce a modified loss function for pairwise learning, called debiased pairwise loss (DPL). The core idea of DPL is to correct the biased probability estimates caused by false negatives, thereby adjusting the gradients to more closely approximate those of fully supervised data. Implementing DPL requires only a small modification to the existing codebase. Experimental studies on public datasets demonstrate the effectiveness of the proposed method.}
}


@article{DBLP:journals/tkde/YangWYQZL24,
	author = {Peilun Yang and
                  Hanchen Wang and
                  Jianye Yang and
                  Zhengping Qian and
                  Ying Zhang and
                  Xuemin Lin},
	title = {Deep Learning Approaches for Similarity Computation: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7893--7912},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3422484},
	doi = {10.1109/TKDE.2024.3422484},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangWYQZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The requirement for appropriate ways to measure the similarity between data objects is a common but vital task in various domains, such as data mining, machine learning and so on. Driven by abundant real-world applications, many well-known similarity (distance) metrics are proposed to measure the pairwise similarity of data pairs, e.g., graph edit distance for graphs and dynamic time warping for time series. However, many similarity metrics suffer from the high time complexity. More specifically, most of the well-known similarity metrics often need quadratic time or even much more time to compute the ground truth similarity and some of them are proven to be NP-hard. With the development of deep learning techniques, there is an emerging research trend on the learning for similarity computation on various data types in the field of database (DB) and data mining, which is quite different with the metric learning studies in the machine learning (ML) literature. Specifically, the studies in the ML focus on the learning for semantic similarity in specific tasks, which is implicitly indicated by the training data, on the data in the feature space. While the studies in the DB literature usually consider the learning for well-defined similarity metrics (e.g., graph edit distance) on the data objects (e.g., graphs), such that it can benefit the similarity computation on data in terms of multiple aspects, such as computation time, metric quality and search heuristic, and the learned representation of data can also be naturally fed to downstream tasks. This survey paper provides a comprehensive review of similarity computation learning on several data types, including set, sequence and graph. Moreover, we first classify the learning-based approaches in terms of their learning target into three categories, i.e., similarity learning, cost matrix learning and search heuristic learning. Then we detail some representative approaches for each category on every data type, and analyze some key features that are utilized by these approaches. Finally, we discuss some challenges and future directions towards the learning for similarity learning on these data types.}
}


@article{DBLP:journals/tkde/ChowdhuryGZ24,
	author = {Tanmoy Chowdhury and
                  Yuyang Gao and
                  Liang Zhao},
	title = {Deep Multi-Task Learning for Spatio-Temporal Incomplete Qualitative
                  Event Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7913--7926},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3460539},
	doi = {10.1109/TKDE.2024.3460539},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChowdhuryGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Forecasting spatiotemporal social events has significant benefits for society to provide the proper amounts and types of resources to manage catastrophes and any accompanying societal risks. Nevertheless, forecasting event subtypes are far more complex than merely extending binary prediction to cover multiple subtypes because of spatial heterogeneity, experiencing a partial set of event subtypes, subtle discrepancy among different event subtypes, nature of the event subtype, spatial correlation of event subtypes. We present Deep multi-task learning for spatio-temporal incomplete qualitative event forecasting (DETECTIVE) framework to effectively forecast the subtypes of future events by addressing all these issues. This formulates spatial locations into tasks to handle spatial heterogeneity in event subtypes and learns a joint deep representation of subtypes across tasks. This has the adaptability to be used for different types of problem formulation required by the nature of the events. Furthermore, based on the “first law of geography”, spatially-closed tasks share similar event subtypes or scale patterns so that adjacent tasks can share knowledge effectively. To optimize the non-convex and strongly coupled problem of the proposed model, we also propose algorithms based on the Alternating Direction Method of Multipliers (ADMM). Extensive experiments on real-world datasets demonstrate the model’s usefulness and efficiency.}
}


@article{DBLP:journals/tkde/ZhangWDWZM24,
	author = {Xin Zhang and
                  Zengmao Wang and
                  Bo Du and
                  Jia Wu and
                  Xiao Zhang and
                  Erli Meng},
	title = {Deep Session Heterogeneity-Aware Network for Click Through Rate Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7927--7939},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3421594},
	doi = {10.1109/TKDE.2024.3421594},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangWDWZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {CTR (Click-Through Rate) prediction plays an essential role in online advertising systems. Most existing works attempt to capture users’ interests from sessions by assuming that behaviors within a session are homogeneous. However, user interest may change frequently. Thus it is hard to guarantee that behaviors in a session are homogeneous, resulting in users’ interests extracted from sessions being biased. In this paper, we propose a model named Deep Session Heterogeneity-aware Network (DSHN) by learning the relationships of behaviors within sessions and the relevance between the session and target item to alleviate the influence of irrelevant or heterogeneous sessions. We design a heterogeneity-aware mechanism to learn the heterogeneity of items within a session. Then we further design two modules: the Session Heterogeneity Learning module and the Relevance Inference module. The Session Heterogeneity Learning module weighs each session by summarizing the variation of session interest with and without any behavior. The relevance Inference module learns the relevance between the target item and each session in a similar way by learning session interest with and without the target item. Extensive experiments on four datasets demonstrate that our proposed DSHN achieves better results compared to the state-of-the-art.}
}


@article{DBLP:journals/tkde/WeiLGLHXBW24,
	author = {Tonglong Wei and
                  Youfang Lin and
                  Shengnan Guo and
                  Yan Lin and
                  Yiheng Huang and
                  Chenyang Xiang and
                  Yuqing Bai and
                  Huaiyu Wan},
	title = {Diff-RNTraj: {A} Structure-Aware Diffusion Model for Road Network-Constrained
                  Trajectory Generation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7940--7953},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3460051},
	doi = {10.1109/TKDE.2024.3460051},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WeiLGLHXBW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory data is essential for various applications. However, publicly available trajectory datasets remain limited in scale due to privacy concerns, which hinders the development of trajectory mining and applications. Although some trajectory generation methods have been proposed to expand dataset scale, they generate trajectories in the geographical coordinate system, posing two limitations for practical applications: 1) failing to ensure that the generated trajectories are road-constrained. 2) lacking road-related information. In this paper, we propose a new problem, road network-constrained trajectory (RNTraj) generation, which can directly generate trajectories on the road network with road-related information. Specifically, RNTraj is a hybrid type of data, in which each point is represented by a discrete road segment and a continuous moving rate. To generate RNTraj, we design a diffusion model called Diff-RNTraj, which can effectively handle the hybrid RNTraj using a continuous diffusion framework by incorporating a pre-training strategy to embed hybrid RNTraj into continuous representations. During the sampling stage, a RNTraj decoder is designed to map the continuous representation generated by the diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj introduces a novel loss function to enhance trajectory’s spatial validity. Extensive experiments conducted on two datasets demonstrate the effectiveness of Diff-RNTraj.}
}


@article{DBLP:journals/tkde/ChenXZZL24,
	author = {Hongyang Chen and
                  Can Xu and
                  Lingyu Zheng and
                  Qiang Zhang and
                  Xuemin Lin},
	title = {Diffusion-Based Graph Generative Methods},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7954--7972},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3466301},
	doi = {10.1109/TKDE.2024.3466301},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenXZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Being the most cutting-edge generative methods, diffusion methods have shown great advances in wide generation tasks. Among them, graph generation attracts significant research attention for its broad application in real life. In our survey, we systematically and comprehensively review on diffusion-based graph generative methods. We first make a review on three mainstream paradigms of diffusion methods, which are denoising diffusion probabilistic models, score-based genrative models, and stochastic differential equations. Then we further categorize and introduce the latest applications of diffusion models on graphs. In the end, we point out some limitations of current studies and future directions of future explorations.}
}


@article{DBLP:journals/tkde/LiuXHLBLL24,
	author = {Yang Liu and
                  Chang Xu and
                  Min Hou and
                  Weiqing Liu and
                  Jiang Bian and
                  Qi Liu and
                  Tie{-}Yan Liu},
	title = {Digger-Guider: High-Frequency Factor Extraction for Stock Trend Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7973--7985},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3424475},
	doi = {10.1109/TKDE.2024.3424475},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuXHLBLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed increasing attention being paid to AI-based quantitative investment. Compared to traditional low-frequency data (e.g., daily, weekly), high-frequency data (e.g., minute-level) is often underutilized for low-frequency stock trend prediction, leaving the vast potential for improvement. However, valuable and noisy information coexist in high-frequency data. The learning process of high-frequency factor extractors can easily be overwhelmed by noise, leading to overfitting. Moreover, common techniques used to prevent overfitting often result in poor performance on this task since they usually roughly restrict the model’s capacity, making it challenging to model complex trading signals in high-frequency data. When designing high-frequency factor extractors, we face a tough dilemma. A high-capacity model may easily overfit to noise, while a simple but robust model may not capture complex high-frequency patterns. To address these problems, we propose maintaining model capacity while preventing overfitting by constructing two components that balance information and noise through interactions between them. Specifically, we propose a novel learning framework called Digger-Guider to extract informative stock representations from noisy high-frequency data. We develop a high-capacity model called Digger to extract local and detailed features from the high-frequency data, and we design a robust model called Guider to capture global tendency features and help the Digger overcome the noise. The Digger and Guider enhance each other through mutual distillation during training, serving as data-driven regularizations that work well on this task. Extensive experiments on real-world datasets demonstrate that our framework can produce powerful high-frequency stock factors that significantly improve stock trend prediction performance and our understanding of the finance market.}
}


@article{DBLP:journals/tkde/PiaggesiKPA24,
	author = {Simone Piaggesi and
                  Megha Khosla and
                  Andr{\'{e}} Panisson and
                  Avishek Anand},
	title = {{DINE:} Dimensional Interpretability of Node Embeddings},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7986--7997},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3425460},
	doi = {10.1109/TKDE.2024.3425460},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/PiaggesiKPA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph representation learning methods, such as node embeddings, are powerful approaches to map nodes into a latent vector space, allowing their use for various graph learning tasks. Despite their success, these techniques are inherently black-boxes and few studies have focused on investigating local explanations of node embeddings for specific instances. Moreover, explaining the overall behavior of unsupervised embedding models remains an unexplored problem, limiting global interpretability and debugging potentials. We address this gap by developing human-understandable explanations for latent space dimensions in node embeddings. Towards that, we first develop new metrics that measure the global interpretability of embeddings based on the marginal contribution of the latent dimensions to predicting graph structure. We say an embedding dimension is more interpretable if it can faithfully map to an understandable sub-structure in the input graph - like community structure. Having observed that standard node embeddings have low interpretability, we then introduce Dine (Dimension-based Interpretable Node Embedding). This novel approach can retrofit existing node embeddings by making them more interpretable without sacrificing their task performance. We conduct extensive experiments on synthetic and real-world graphs and show that we can simultaneously learn highly interpretable node embeddings with effective performance in link prediction and node classification.}
}


@article{DBLP:journals/tkde/WuLL24,
	author = {Fang Wu and
                  Siyuan Li and
                  Stan Z. Li},
	title = {Discovering the Representation Bottleneck of Graph Neural Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {7998--8008},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3446584},
	doi = {10.1109/TKDE.2024.3446584},
	timestamp = {Tue, 28 Jan 2025 08:07:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) rely mainly on the message-passing paradigm to propagate node features and build interactions, and different graph learning problems require different ranges of node interactions. In this work, we explore the capacity of GNNs to capture node interactions under contexts of different complexities. We discover that GNNs usually fail to capture the most informative kinds of interaction styles for diverse graph learning tasks, and thus name this phenomenon as GNNs’ representation bottleneck. As a response, we demonstrate that the inductive bias introduced by existing graph construction mechanisms can result in this representation bottleneck, i.e., preventing GNNs from learning interactions of the most appropriate complexity. To address that limitation, we propose a novel graph rewiring approach based on interaction patterns learned by GNNs to adjust each node's receptive fields dynamically. Extensive experiments on both real-world and synthetic datasets prove the effectiveness of our algorithm in alleviating the representation bottleneck and its superiority in enhancing the performance of GNNs over state-of-the-art graph rewiring baselines.}
}


@article{DBLP:journals/tkde/LiYLLYW24,
	author = {Rongqing Li and
                  Jiaqi Yu and
                  Changsheng Li and
                  Wenhan Luo and
                  Ye Yuan and
                  Guoren Wang},
	title = {{DREAM:} Domain-Agnostic Reverse Engineering Attributes of Black-Box
                  Model},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8009--8022},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3460806},
	doi = {10.1109/TKDE.2024.3460806},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiYLLYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning models are usually black boxes when deployed on machine learning platforms. Prior works have shown that the attributes (e.g., the number of convolutional layers) of a target black-box model can be exposed through a sequence of queries. There is a crucial limitation: these works assume the training dataset of the target model is known beforehand and leverage this dataset for model attribute attack. However, it is difficult to access the training dataset of the target black-box model in reality. Therefore, whether the attributes of a target black-box model could be still revealed in this case is doubtful. In this paper, we investigate a new problem of black-box reverse engineering, without requiring the availability of the target model’s training dataset. We put forward a general and principled framework DREAM, by casting this problem as out-of-distribution (OOD) generalization. In this way, we can learn a domain-agnostic meta-model to infer the attributes of the target black-box model with unknown training data. This makes our method one of the kinds that can gracefully apply to an arbitrary domain for model attribute reverse engineering with strong generalization ability. Extensive experimental results demonstrate the superiority of our proposed method over the baselines.}
}


@article{DBLP:journals/tkde/GuoZW24,
	author = {Husheng Guo and
                  Yang Zhang and
                  Wenjian Wang},
	title = {Dynamical Targeted Ensemble Learning for Streaming Data With Concept
                  Drift},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8023--8036},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3460404},
	doi = {10.1109/TKDE.2024.3460404},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GuoZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Concept drift is an important characteristic and inevitable difficult problem in streaming data mining. Ensemble learning is commonly used to deal with concept drift. However, most ensemble methods cannot balance the accuracy and diversity of base learners after drift occurs, and cannot adjust adaptively according to the drift type. To solve these problems, this paper proposes a targeted ensemble learning (Targeted EL) method to improve the accuracy and diversity of ensemble learning for streaming data with abrupt and gradual concept drift. First, to improve the accuracy of the base learners, the method adopts different sample weighting strategies for different types of drift to realize bidirectional transfer of new and old distributed samples. Second, the difference matrix is constructed by the prediction results of the base learners on the current samples. According to the drift type, the submatrix with appropriate size and maximum difference sum is extracted adaptively to select appropriate, accuracy and diverse base learners for ensemble. The experimental results show that the proposed method can achieve good generalization performance when dealing with the streaming data with abrupt and gradual concept drift.}
}


@article{DBLP:journals/tkde/ZhongDLDT24a,
	author = {Qihuang Zhong and
                  Liang Ding and
                  Juhua Liu and
                  Bo Du and
                  Dacheng Tao},
	title = {{E2S2:} Encoding-Enhanced Sequence-to-Sequence Pretraining for Language
                  Understanding and Generation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8037--8050},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3341917},
	doi = {10.1109/TKDE.2023.3341917},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhongDLDT24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequence-to-sequence (seq2seq) learning is a popular fashion for large-scale pretraining language models. However, the previous seq2seq pretraining models generally focus on reconstructive objectives on the decoder side and neglect the effect of encoder-side supervision, which we argue may lead to sub-optimal performance. To verify our hypothesis, we first empirically study the functionalities of the encoder and decoder in seq2seq pretrained language models, and find that the encoder takes an important but under-exploitation role than the decoder regarding the downstream performance and neuron activation. Therefore, we propose an encoding-enhanced seq2seq pretraining strategy, namely E2S2, which improves the seq2seq models via integrating more efficient self-supervised information into the encoders. Specifically, E2S2 adopts two self-supervised objectives on the encoder side from two aspects: 1) locally denoising the corrupted sentence (denoising objective); and 2) globally learning better sentence representations (contrastive objective). With the help of both objectives, the encoder can effectively distinguish the noise tokens and capture high-level (i.e., syntactic and semantic) knowledge, thus strengthening the ability of seq2seq model to accurately achieve the conditional generation. On a large diversity of downstream natural language understanding and generation tasks, E2S2 dominantly improves the performance of its powerful backbone models, e.g., BART and T5. For example, upon BART backbone, we achieve +1.1% averaged gain on the general language understanding evaluation (GLUE) benchmark and +1.75% F_{0.5} score improvement on CoNLL2014 dataset. We also provide in-depth analyses to show the improvement stems from better linguistic representation. We hope that our work will foster future self-supervision research on seq2seq language model pretraining.}
}


@article{DBLP:journals/tkde/YiYTLW24,
	author = {Qianxin Yi and
                  Yiyang Yang and
                  Shaojie Tang and
                  Jiapeng Liu and
                  Yao Wang},
	title = {Effective Generalized Low-Rank Tensor Contextual Bandits},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8051--8065},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3469782},
	doi = {10.1109/TKDE.2024.3469782},
	timestamp = {Mon, 03 Mar 2025 22:25:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YiYTLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus is represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action’s feature tensor and a fixed but unknown parameter tensor with low-rank structure. To effectively achieve the trade-off between exploration and exploitation, we introduce an algorithm called “Generalized Low-Rank Tensor Exploration Subspace then Refine” (G-LowTESTR). This algorithm first collects data to explore the intrinsic low-rank tensor subspace information embedded in the scenario, and then converts the original problem into a lower-dimensional generalized linear contextual bandits problem. Rigorous theoretical analysis shows that the regret bound of G-LowTESTR is superior to those in vectorization and matricization cases. We conduct a series of synthetic and real data experiments to further highlight the effectiveness of G-LowTESTR, leveraging its ability to capitalize on the low-rank tensor structure for enhanced learning.}
}


@article{DBLP:journals/tkde/ZengWZL24,
	author = {Aoting Zeng and
                  Liping Wang and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {Efficient and Effective Augmentation Framework With Latent Mixup and
                  Label-Guided Contrastive Learning for Graph Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8066--8078},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3471659},
	doi = {10.1109/TKDE.2024.3471659},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZengWZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) with data augmentation obtain promising results among existing solutions for graph classification. Mixup-based augmentation methods for graph classification have already achieved state-of-the-art performance. However, existing mixup-based augmentation methods either operate in the input space and thus face the challenge of balancing efficiency and accuracy, or directly conduct mixup in the latent space without similarity guarantee, thus leading to lacking semantic validity and limited performance. To address these limitations, this paper proposes \\mathcal {G}-MixCon, a novel framework leveraging the strengths of Mixup-based augmentation and supervised Contrastive learning (SCL). To the best of our knowledge, this is the first attempt to develop an SCL-based approach for learning graph representations. Specifically, the mixup-based strategy within the latent space named GDA_{gl} and GDA_{nl} are proposed, which efficiently conduct linear interpolation between views of the node or graph level. Furthermore, we design a dual-objective loss function named SupMixCon that can consider both the consistency among graphs and the distances between the original and augmented graph. SupMixCon can guide the training process for SCL in \\mathcal {G}-MixCon while achieving a similarity guarantee. Comprehensive experiments are conducted on various real-world datasets, the results show that \\mathcal {G}-MixCon demonstrably enhances performance, achieving an average accuracy increment of 6.24%, and significantly increases the robustness of GNNs against noisy labels.}
}


@article{DBLP:journals/tkde/WangZLZZT24,
	author = {Yuxiang Wang and
                  Yuxiang Zeng and
                  Shuyuan Li and
                  Yuanyuan Zhang and
                  Zimu Zhou and
                  Yongxin Tong},
	title = {Efficient and Private Federated Trajectory Matching},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8079--8092},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3424411},
	doi = {10.1109/TKDE.2024.3424411},
	timestamp = {Mon, 03 Mar 2025 22:25:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangZLZZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Trajectory Matching (FTM) is gaining increasing importance in big trajectory data analytics, supporting diverse applications such as public health, law enforcement, and emergency response. FTM retrieves trajectories that match with a query trajectory from a large-scale trajectory database, while safeguarding the privacy of trajectories in both the query and the database. A naive solution to FTM is to process the query through Secure Multi-party Computation (SMC) across the entire database, which is inherently secure yet inevitably slow due to the massive secure operations. A promising acceleration strategy is to filter irrelevant trajectories from the database based on the query, thus reducing the SMC operations. However, a key challenge is how to publish the query in a way that both preserves privacy and enables efficient trajectory filtering. In this paper, we design {\\sf GIST}\n, a novel framework for efficient Federated Trajectory Matching. {\\sf GIST}\nis grounded in Geo-Indistinguishability, a privacy criterion dedicated to locations. It employs a new privacy mechanism for the query that facilitates efficient trajectory filtering. We theoretically prove the privacy guarantee of the mechanism and the accuracy of the filtering strategy of {\\sf GIST}\n. Extensive evaluations on five real datasets show that {\\sf GIST}\nis significantly faster and incurs up to 2 orders of magnitude lower communication cost than the state-of-the-arts.}
}


@article{DBLP:journals/tkde/HuHH24,
	author = {Jun Hu and
                  Bryan Hooi and
                  Bingsheng He},
	title = {Efficient Heterogeneous Graph Learning via Random Projection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8093--8107},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3434956},
	doi = {10.1109/TKDE.2024.3434956},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuHH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous Graph Neural Networks (HGNNs) are powerful tools for deep learning on heterogeneous graphs. Typical HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Recent pre-computation-based HGNNs use one-time message passing to transform a heterogeneous graph into regular-shaped tensors, enabling efficient mini-batch training. Existing pre-computation-based HGNNs can be mainly categorized into two styles, which differ in how much information loss is allowed and efficiency. We propose a hybrid pre-computation-based HGNN, named Random Projection Heterogeneous Graph Neural Network (RpHGNN), which combines the benefits of one style's efficiency with the low information loss of the other style. To achieve efficiency, the main framework of RpHGNN consists of propagate-then-update iterations, where we introduce a Random Projection Squashing step to ensure that complexity increases only linearly. To achieve low information loss, we introduce a Relation-wise Neighbor Collection component with an Even-odd Propagation Scheme, which aims to collect information from neighbors in a finer-grained way. Experimental results indicate that our approach achieves state-of-the-art results on seven small and large benchmark datasets while also being 230% faster compared to the most effective baseline. Surprisingly, our approach not only surpasses pre-processing-based baselines but also outperforms end-to-end methods.}
}


@article{DBLP:journals/tkde/NapolitanoVC24,
	author = {Davide Napolitano and
                  Lorenzo Vaiani and
                  Luca Cagliero},
	title = {Efficient Neural Network-Based Estimation of Interval Shapley Values},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8108--8119},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3420180},
	doi = {10.1109/TKDE.2024.3420180},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NapolitanoVC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The use of Shapley Values (SVs) to explain machine learning model predictions is established. Recent research efforts have been devoted to generating efficient Neural Network-based SVs estimates. However, the variability of the generated estimates, which depend on the selected data sampling, model, and training parameters, brings the reliability of such estimates into question. By leveraging the concept of Interval SVs, we propose to incorporate SVs uncertainty directly into the learning process. Specifically, we explain ensemble models composed of multiple predictors, each one generating potentially different outcomes. Unlike all existing approaches, the explainer design is tailored to Interval SVs learning instead of SVs only. We present three new Network-based explainers relying on different ISV paradigms, i.e., a Multi-Task Learning network inspired by the Shapley value's weighted least squares characterization and two Interval Shapley-Like Value Neural estimators. The experiments thoroughly evaluate the new approaches on ten benchmark datasets, looking for the best compromise between intervals’ accuracy and explainers’ efficiency.}
}


@article{DBLP:journals/tkde/LiuWFC24,
	author = {Ziyang Liu and
                  Chaokun Wang and
                  Hao Feng and
                  Ziyang Chen},
	title = {Efficient Unsupervised Graph Embedding With Attributed Graph Reduction
                  and Dual-Level Loss},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8120--8134},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3436076},
	doi = {10.1109/TKDE.2024.3436076},
	timestamp = {Mon, 03 Mar 2025 22:25:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuWFC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph embedding aims to extract low-dimensional representation vectors, commonly referred to as embeddings, from graph data. The generated embeddings simplify subsequent data analysis and machine learning tasks. Recently, researchers have proposed the use of contrastive learning on graphs to extract node embeddings in an unsupervised manner. Although existing graph contrastive learning methods have significantly advanced this field, there is still potential for further exploration, particularly in optimizing training efficiency and enhancing embedding quality. In this paper, we propose an efficient unsupervised graph embedding method named GEARED. First, the method involves an attributed graph reduction module that converts the raw graph into a reduced graph, greatly improving model training efficiency. Second, GEARED employs a dual-level loss with adaptive scaling factors to ensure the acquisition of high-quality embeddings. Finally, we conduct a partial derivative analysis to elucidate the specific mechanisms through which GEARED is capable of generating high-quality embeddings. Extensive experimental evaluations on 14 benchmark datasets show that GEARED consistently outperforms state-of-the-art methods in terms of training efficiency and classification accuracy. For instance, GEARED achieves a training speedup of over 40 times on both the CS and Physics datasets while maintaining superior classification accuracy.}
}


@article{DBLP:journals/tkde/YangTZZL24,
	author = {Wenqi Yang and
                  Chang Tang and
                  Xiao Zheng and
                  Xinzhong Zhu and
                  Xinwang Liu},
	title = {Eigenvalue Ratio Inspired Partition Learning and Fusion for Multiple
                  Kernel Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8135--8147},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3425393},
	doi = {10.1109/TKDE.2024.3425393},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangTZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multiple kernel clustering (MKC) aims to extract and integrate the clustering information from a set of pre-defined kernels for handling data which cannot be linearly separated well. More precisely, existing MKC methods generally devote to learn the complementary information from a set of kernel partitions, whose feature dimensions are commonly fixed as the upper bound n\nor lower bound c\n, where n\nand c\nrepresents the number of samples and clusters, respectively. However, the adopting of the lower bound or upper bound generally leads to poor clustering performance caused by the lack or redundancy of clustering information carried by kernel partitions. To tackle this issue, we propose a novel late fusion multiple kernel clustering method, termed as Eigenvalue Ratio Inspired Partition Learning and Fusion for Multiple Kernel Clustering (ERMKC), in this paper. Specifically, we propose an eigenvalue ratio based criterion to guide the kernel partition learning for each single kernel matrix, which ensures more suitable feature dimensions for the learnt kernel partitions. In addition, we also propose a novel late fusion model for fusing the learnt kernel partitions optimally. Furthermore, we conduct extensive experiments on numerous benchmark datasets to evaluate the proposed ERMKC method, whose results verify the effectiveness and advantage of the proposed method compared to the other state-of-the-art methods.}
}


@article{DBLP:journals/tkde/CuiWZLXY24,
	author = {Ningning Cui and
                  Dong Wang and
                  Huaijie Zhu and
                  Jianxin Li and
                  Jianliang Xu and
                  Xiaochun Yang},
	title = {Enabling Verifiable and Secure Range Query in Multi-User Setting Under
                  Cloud Environments},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8148--8163},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419930},
	doi = {10.1109/TKDE.2024.3419930},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CuiWZLXY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data outsourcing to the cloud has become increasingly popular for high-speed storage and retrieval. However, privacy and security are pressing concerns that hinder the further development of cloud computing. A common approach is to encrypt data before outsourcing, assuming the cloud is semi-honest. However, in reality, the cloud may be malicious and forge query results unexpectedly. Moreover, most previous schemes are designed for single-user setting, where different users share the same secret key, leading to potential privacy leaks. Therefore, ensuring confidentiality and verifiability in multi-user setting is crucial but has not been well-addressed. In this paper, we formally define the notion of Verifiable and Secure Range Query in Multi-User Setting (VSRQM) and propose a prefix-aware encoding (Pcode) scheme to encode spatial data for query processing. Next, we design a Tree-Aided Verifiable and Secure Index (SATree) on top of the Pcode and symmetric re-encryption scheme. SATree preserves data privacy, provides a mechanism to verify query results’ integrity and achieves sub-linear search time. Additionally, we propose two compression schemes to reduce the space cost of storage and transmission. Finally, we present formal complexity and security analyses and conduct empirical evaluations on real and synthetic datasets to demonstrate our proposed approaches’ practical performance.}
}


@article{DBLP:journals/tkde/ZhaoZZL24,
	author = {Chuang Zhao and
                  Hongke Zhao and
                  Xiaofang Zhou and
                  Xiaomeng Li},
	title = {Enhancing Precision Drug Recommendations via In-Depth Exploration
                  of Motif Relationships},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8164--8178},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3437775},
	doi = {10.1109/TKDE.2024.3437775},
	timestamp = {Thu, 30 Jan 2025 11:04:28 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Making accurate and safe clinical decisions for patients has long been a challenging task. With the proliferation of electronic health records and the rapid advancement of technology, drug recommender systems have emerged as invaluable aids for healthcare professionals, offering precise and secure prescriptions. Among prevailing methods, the exploration of motifs, defined as substructures with specific biological functions, has largely been overlooked. Nevertheless, the substantial impact of the motifs on drug efficacy and patient diseases implies that a more extensive incorporation could potentially improve the recommender systems. In light of this, we introduce DEPOT, an innovative drug recommendation framework developed from a motif-aware perspective. In our approach, we employ chemical decomposition to partition drug molecules into semantic motif-trees and design a structure-aware graph transformer to capture motif collaboration. This innovative practice preserves the topology knowledge and facilitates perception of drug functionality. To delve into the dynamic correlation between motifs and disease progression, we conduct a meticulous investigation from two perspectives: repetition and exploration. This comprehensive analysis allows us to gain valuable insights into the drug turnover, with the former focusing on reusability and the latter on discovering new requirements. We further formulate a historical weighting strategy for drug-drug interaction (DDI) objective, enabling adaptive control over the trade-off between accuracy and safety criteria throughout the training process. Extensive experiments conducted on four data sets validate the effectiveness and robustness of DEPOT.}
}


@article{DBLP:journals/tkde/TanLWHXTL24,
	author = {Yuze Tan and
                  Yixi Liu and
                  Hongjie Wu and
                  Shudong Huang and
                  Zenglin Xu and
                  Ivor W. Tsang and
                  Jiancheng Lv},
	title = {Euclidean Distance is Not Your Swiss Army Knife},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8179--8191},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3424511},
	doi = {10.1109/TKDE.2024.3424511},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/TanLWHXTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-based multi-view learning, which has hitherto been used to discover the intrinsic patterns of graph data giving the credit to its convenience of implementation and effectiveness. Note that even though these approaches have been increasingly adopted in multi-view clustering and have generated promising outcomes, they are still faced with the sub-optimal solution. For one thing, multi-view data can be corrupted in the raw feature space. For the other, most existing approaches normally utilize euclidean distance to obtain the similarity between two samples, which can not be the best option for all types of real-world data and leads to inferior results. Therefore, to overcome the aforementioned issues, we integrate multi-metric learning, graph filtering, and subspace learning into a collaborative learning framework for multi-view clustering. Particularly, we prefer to recover a smooth representation of data by graph filtering, which can reserve the geometric structure of the original multi-view data and discard the corruptions simultaneously. Furthermore, instead of using euclidean distance as a Swiss army knife, multiple metrics are utilized to fully exploit the correlation of data based on the smooth representation, hence finally facilitating the downstream clustering task. Extensive experiments on multi-view clustering tasks validate our theoretical findings of ours and prove the improvement of our method over the SOTA approaches.}
}


@article{DBLP:journals/tkde/DuYZWSLLZ24,
	author = {Hanwen Du and
                  Huanhuan Yuan and
                  Pengpeng Zhao and
                  Deqing Wang and
                  Victor S. Sheng and
                  Yanchi Liu and
                  Guanfeng Liu and
                  Lei Zhao},
	title = {Feature-Aware Contrastive Learning With Bidirectional Transformers
                  for Sequential Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8192--8205},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3343345},
	doi = {10.1109/TKDE.2023.3343345},
	timestamp = {Thu, 30 Jan 2025 15:14:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DuYZWSLLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contrastive learning with Transformer-based sequence encoder has gained predominance for sequential recommendation due to its ability to mitigate the data noise and the data sparsity issue. However, existing contrastive learning approaches for sequential recommendation still suffer from two limitations. First, they mainly center on left-to-right unidirectional Transformers as base encoders, which are suboptimal for sequential recommendation because user behaviors may not be a rigid left-to-right sequence. Second, they devise contrastive learning objectives only from the sequence level, neglecting the rich self-supervision signals from the feature level. To address these limitations, we propose a novel framework called Feature-aware Contrastive Learning with bidirectional Transformers for sequential Recommendation (FCLRec) to effectively leverage feature information for sequential recommendation. Specifically, we first augment bidirectional Transformers with a novel feature-aware self-attention module that is able to simultaneously model the complex relationships between sequences and features. Next, we propose a novel feature-aware contrastive learning objective that generates a collection of positive samples via three types of augmentations from three different levels. Finally, we adopt feature prediction as an auxiliary task to strengthen the connections between items and features. Our experimental results on four public benchmark datasets show that FCLRec outperforms the state-of-the-art methods for sequential recommendation.}
}


@article{DBLP:journals/tkde/MengLYLZLL24,
	author = {Lingyuan Meng and
                  Ke Liang and
                  Hao Yu and
                  Yue Liu and
                  Sihang Zhou and
                  Meng Liu and
                  Xinwang Liu},
	title = {FedEAN: Entity-Aware Adversarial Negative Sampling for Federated Knowledge
                  Graph Reasoning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8206--8219},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3464516},
	doi = {10.1109/TKDE.2024.3464516},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MengLYLZLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated knowledge graph reasoning (FedKGR) aims to perform reasoning over different clients while protecting data privacy, drawing increasing attention to its high practical value. Previous works primarily focus on data heterogeneity, ignoring challenges from limited data scale and primitive negative sample strategies, i.e., random entity replacement, which yield low-quality negatives and zero loss issues. Meanwhile, generative adversarial networks (GANs) are widely used in different fields to generate high-quality negative samples, but no work has been developed for FedKGR. To this end, we propose a plug-and-play Entity-aware Adversarial Negative sampling strategy for FedKGR, termed FedEAN. Specifically, we are the first to adopt GANs to generate high-quality negative samples in different clients. It takes the target triplet in each batch as input and outputs high-quality negative samples, which guaranteed by the joint training of the generator and discriminator. Moreover, we design an entity-aware adaptive negative sampling mechanism based on the similarity of entity representations before and after server aggregation, which can persevere the entity global consistency across clients during training. Extensive experiments demonstrate that FedEAN excels with various FedKGR backbones, demonstrating its ability to construct high-quality negative samples and address the zero-loss issue.}
}


@article{DBLP:journals/tkde/ShenLL24,
	author = {Sheng Shen and
                  Chi Liu and
                  Teng Joon Lim},
	title = {Federated Learning With Heterogeneous Client Expectations: {A} Game
                  Theory Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8220--8237},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3464488},
	doi = {10.1109/TKDE.2024.3464488},
	timestamp = {Tue, 18 Feb 2025 15:42:07 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ShenLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In federated learning (FL), local models are trained independently by clients, local model parameters are shared with a global aggregator or server, and then the updated model is used to initialize the next round of local training. FL and its variants have become synonymous with privacy-preserving distributed machine learning. However, most FL methods have maximization of model accuracy as their sole objective, and rarely are the clients’ needs and constraints considered. In this paper, we consider that clients have differing performance expectations and resource constraints, and we assume local data quality can be improved at a cost. In this light, we treat FL in the training phase as a game in satisfaction form that seeks to satisfy all clients’ expectations. We propose two novel FL methods, a deep reinforcement learning method and a stochastic method, that embrace this design approach. We also account for the scenario where certain clients can adjust their actions even after being satisfied, by introducing probabilistic parameters in both of our methods. The experimental results demonstrate that our proposed methods converge quickly to a lower cost solution than competing methods. Furthermore, it was found that the probabilistic parameters facilitate the attainment of satisfaction equilibria (SE), addressing scenarios where reaching SEs may be challenging within the confines of traditional games in satisfaction form.}
}


@article{DBLP:journals/tkde/LiuSCZ24,
	author = {Youming Liu and
                  Lin Shu and
                  Chuan Chen and
                  Zibin Zheng},
	title = {Fine-Grained Semantics Enhanced Contrastive Learning for Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8238--8250},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3466990},
	doi = {10.1109/TKDE.2024.3466990},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuSCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph contrastive learning defines a contrastive task to pull similar instances close and push dissimilar instances away. It learns discriminative node embeddings without supervised labels, which has aroused increasing attention in the past few years. Nevertheless, existing methods of graph contrastive learning ignore the differences between diverse semantics existed in graphs, which learn coarse-grained node embeddings and lead to sub-optimal performances on downstream tasks. To bridge this gap, we propose a novel Fine-grained Semantics enhanced Graph Contrastive Learning (FSGCL) in this paper. Concretely, FSGCL first introduces a motif-based graph construction, which employs graph motifs to extract diverse semantics existed in graphs from the perspective of input data. Then, the semantic-level contrastive task is explored to further enhance the utilization of fine-grained semantics from the perspective of model training. Experiments on five real-world datasets demonstrate the superiority of our proposed FSGCL over state-of-the-art methods. To make the results reproducible, we will make our codes public on GitHub after this paper is accepted.}
}


@article{DBLP:journals/tkde/LiLCWX24,
	author = {Shangzhe Li and
                  Yingke Liu and
                  Xueyuan Chen and
                  Junran Wu and
                  Ke Xu},
	title = {Forecasting Turning Points in Stock Price by Integrating Chart Similarity
                  and Multipersistence},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8251--8266},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3444814},
	doi = {10.1109/TKDE.2024.3444814},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiLCWX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Forecasting financial data plays a crucial role in financial market. Relying solely on prices or price trends as prediction targets often leads to a vast of invalid transactions. As a result, researchers have increasingly turned their attention to turning points as the prediction target. Surprisingly, existing methods have largely overlooked the role of technical charts, despite turning points being closely related to the technical charts. Recently, several researchers have attempted to utilize chart information via converting price sequences into images for turning point forecasting, but robustness and convergence problems arise. To address these challenges and enhance the turning point predictions, this article introduces a new method known as MPCNet. Specifically, we first transform the price series into a graph structure using chart similarity to robustly extract valuable information from technical charts. Additionally, we introduce the multipersistence topology tool to accurately predict stock turning points and provide convergence guarantee. Experimental results demonstrate the significant superiority of our proposed model over existing methods. Furthermore, based on additional performance evaluations using real stock data, MPCNet consistently achieves the highest average return during the transaction backtesting period. Meanwhile, we provide empirical validation of robustness and theoretical analysis to confirm its convergence, establishing it as a superior tool for financial forecasting.}
}


@article{DBLP:journals/tkde/ZhangLWWYWL24,
	author = {Lingling Zhang and
                  Yifei Li and
                  Qianying Wang and
                  Yun Wang and
                  Hang Yan and
                  Jiaxin Wang and
                  Jun Liu},
	title = {FPrompt-PLM: Flexible-Prompt on Pretrained Language Model for Continual
                  Few-Shot Relation Extraction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8267--8282},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419117},
	doi = {10.1109/TKDE.2024.3419117},
	timestamp = {Mon, 17 Feb 2025 12:13:59 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLWWYWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relation extraction (RE) aims to identify the relation between two entities within a sentence, which plays a crucial role in information extraction. Traditional supervised setting on RE does not fit the actual scenario, due to the continuous emergence of new relations and the unavailability of massive labeled examples. Continual few-shot relation extraction (CFS-RE) is proposed as a potential solution to the above situation, which requires the model to learn new relations sequentially from a few examples. Apparently, CFS-RE is more challenging than previous RE, as the catastrophic forgetting of old knowledge and few-shot overfitting on a handful of examples. To this end, we propose a novel flexible-prompt framework on pretrained language model named FPrompt-PLM for CFS-RE, which includes flexible-prompt embedding, pretrained-language understanding, and nearest-prototype learning modules. Note that two pools in FPrompt-PLM, i.e., prompt and prototype pools, are continual updated and applied for prediction of all seen relations at current time-step. The former pool records the distinctive prompt embedding in each time period, and the latter records all learned relation prototypes. Besides, three progressive stages are introduced to learn FPrompt-PLM's parameters and apply this model for CFS-RE testing, which includes meta-training, continual meta-finetuning, and testing stages. And we improve the CFS-RE loss by incorporating multiple distillation losses as well as a novel prototype-diversity loss in these stages to alleviate the catastrophic forgetting and few-shot overfitting problems. Comprehensive experiments on two widely-used datasets show that FPrompt-PLM achieves significant performance improvements over the SOTA baselines.}
}


@article{DBLP:journals/tkde/WeiCM24,
	author = {Tianjun Wei and
                  Tommy W. S. Chow and
                  Jianghong Ma},
	title = {{FPSR+:} Toward Robust, Efficient, and Scalable Collaborative Filtering
                  With Partition-Aware Item Similarity Modeling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8283--8296},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3418080},
	doi = {10.1109/TKDE.2024.3418080},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WeiCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative filtering (CF) has been extensively studied in recommendation, spawning various solutions. While graph convolution networks (GCNs) are effective at representation learning, their efficiency is lacking. Comparatively, item similarity model efficiently establishes direct relationships between items. In spite of this, the modeling problem grows quadratically as the number of items increases. This poses critical scalability issues. In this paper, through an investigation of the latest GCN model, we reveal the feasibility of optimizing the process of similarity modeling using the underlying group structure in the item set. Based on these findings, we propose a novel model which introduces graph partitioning to reduce the scale of similarity modeling problem, dubbed FPSR+. Specifically, we divide similarity modeling of items into sub-problems within each partition, and incorporate global and local prior knowledge to alleviate information loss. Following an analysis of the properties of different items in partitioning, we propose a new hub set selection strategy that improves the robustness of FPSR+ in the small partition case. Extensive experiments on four real-world datasets demonstrate the superior performance of FPSR+ compared with state-of-the-art GCN models and item similarity models, as well as several-fold speedups and reductions in parameter storage.}
}


@article{DBLP:journals/tkde/HuangXCL24,
	author = {Yingcheng Huang and
                  Fuyuan Xiao and
                  Zehong Cao and
                  Chin{-}Teng Lin},
	title = {Fractal Belief R{\'{e}}nyi Divergence With its Applications in
                  Pattern Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8297--8312},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3342907},
	doi = {10.1109/TKDE.2023.3342907},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuangXCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multisource information fusion is a comprehensive and interdisciplinary subject. Dempster-Shafer (D-S) evidence theory copes with uncertain information effectively. Pattern classification is the core research content of pattern recognition, and multisource information fusion based on D-S evidence theory can be effectively applied to pattern classification problems. However, in D-S evidence theory, highly-conflicting evidence may cause counterintuitive fusion results. Belief divergence theory is one of the theories that are proposed to address problems of highly-conflicting evidence. Although belief divergence can deal with conflict between evidence, none of the existing belief divergence methods has considered how to effectively measure the discrepancy between two pieces of evidence with time evolutionary. In this study, a novel fractal belief Rényi (FBR) divergence is proposed to handle this problem. We assume that it is the first divergence that extends the concept of fractal to Rényi divergence. The advantage is measuring the discrepancy between two pieces of evidence with time evolution, which satisfies several properties and is flexible and practical in various circumstances. Furthermore, a novel algorithm for multisource information fusion based on FBR divergence, namely FBReD-based weighted multisource information fusion, is developed. Ultimately, the proposed multisource information fusion algorithm is applied to a series of experiments for pattern classification based on real datasets, where our proposed algorithm achieved superior performance.}
}


@article{DBLP:journals/tkde/ZhouSXLZ24,
	author = {Xiaotian Zhou and
                  Haoxin Sun and
                  Wanyue Xu and
                  Wei Li and
                  Zhongzhi Zhang},
	title = {Friedkin-Johnsen Model for Opinion Dynamics on Signed Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8313--8327},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3424974},
	doi = {10.1109/TKDE.2024.3424974},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouSXLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A signed graph offers richer information than an unsigned graph, since it describes both collaborative and competitive relationships in social networks. In this paper, we study opinion dynamics on a signed graph, based on the Friedkin-Johnsen model. We first interpret the equilibrium opinion in terms of a defined random walk on an augmented signed graph, by representing the equilibrium opinion of every node as a combination of all nodes’ internal opinions, with the coefficient of the internal opinion for each node being the difference of two absorbing probabilities. We then quantify some relevant social phenomena and express them in terms of the \\ell _{2}\nnorms of vectors. We also design a nearly-linear time signed Laplacian solver for assessing these quantities, by establishing a connection between the absorbing probability of random walks on a signed graph and that on an associated unsigned graph. We further study the opinion optimization problem by changing the initial opinions of a fixed number of nodes, which can be optimally solved in cubic time. We provide a nearly-linear time algorithm with an error guarantee to approximately solve the problem. Finally, we execute extensive experiments on sixteen real-life signed networks, which show that both of our algorithms are effective and efficient, and are scalable to massive graphs with over 20 million nodes.}
}


@article{DBLP:journals/tkde/ZhangBLZW24,
	author = {Yipeng Zhang and
                  Zhifeng Bao and
                  Yuchen Li and
                  Baihua Zheng and
                  Xiaoli Wang},
	title = {From a Timeline Contact Graph to Close Contact Tracing and Infection
                  Diffusion Intervention},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8328--8340},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3423476},
	doi = {10.1109/TKDE.2024.3423476},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangBLZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a novel graph structure to address the problems of information spreading in a real-world, frequently updating graph, with two main contributions at hand: accurately tracing infection diffusion according to fine-grained user movements and finding vulnerable vertices under the virus immunization scenario to mitigate infection diffusion. Unlike previous work that primarily predicts the long-term epidemic trend at the census level, this study aims to intervene in the short-term at the individual level. Therefore, two downstream tasks are formulated to illustrate practicalities: Epidemic Mitigating in Public Area problem (EMA) and Epidemic Maximized Spread in Public Area problem (ESA), where EMA aims to find intervention strategies, and ESA is an adversarial solution against the intervention strategy to test the robustness. Comprehensive experiments are conducted using two real-world datasets with millions of public transport trips, which demonstrate the effectiveness of our approach and highlight the importance of considering the dynamic nature of close contacts in epidemic modelling.}
}


@article{DBLP:journals/tkde/CaiXGWZJL24,
	author = {Borui Cai and
                  Yong Xiang and
                  Longxiang Gao and
                  Di Wu and
                  He Zhang and
                  Jiong Jin and
                  Tom H. Luan},
	title = {From Wide to Deep: Dimension Lifting Network for Parameter-Efficient
                  Knowledge Graph Embedding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8341--8348},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3437479},
	doi = {10.1109/TKDE.2024.3437479},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CaiXGWZJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream applications. Conventional KGE methods require high-dimensional representations to learn the complex structure of knowledge graph, but lead to oversized model parameters. Recent advances reduce parameters by low-dimensional entity representations, while developing techniques (e.g., knowledge distillation or reinvented representation forms) to compensate for reduced dimension. However, such operations introduce complicated computations and model designs that may not benefit large knowledge graphs. To seek a simple strategy to improve the parameter efficiency of conventional KGE models, we take inspiration from that deeper neural networks require exponentially fewer parameters to achieve expressiveness comparable to wider networks for compositional structures. We view all entity representations as a single-layer embedding network, and conventional KGE methods that adopt high-dimensional entity representations equal widening the embedding network to gain expressiveness. To achieve parameter efficiency, we instead propose a deeper embedding network for entity representations, i.e., a narrow entity embedding layer plus a multi-layer dimension lifting network (LiftNet). Experiments on three public datasets show that by integrating LiftNet, four conventional KGE methods with 16-dimensional representations achieve comparable link prediction accuracy as original models that adopt 512-dimensional representations, saving 68.4% to 96.9% parameters.}
}


@article{DBLP:journals/tkde/LiZSM24,
	author = {Mengyuan Li and
                  Xiaohong Zhang and
                  Jiaoyan Shang and
                  Yingcang Ma},
	title = {General Quasi Overlap Functions and Fuzzy Neighborhood Systems-Based
                  Fuzzy Rough Sets With Their Applications},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8349--8361},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3474728},
	doi = {10.1109/TKDE.2024.3474728},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiZSM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fuzzy rough sets are important mathematical tool for processing data using existing knowledge. Fuzzy rough sets have been widely studied and used into various fields, such as data reduction and image processing, etc. In extensive literature we have studied, general quasi overlap functions and fuzzy neighborhood systems are broader than other all fuzzy operators and knowledge used in existing fuzzy rough sets, respectively. In this article, a novel fuzzy rough sets model (shortly (I, Q, NS)-fuzzy rough sets) is proposed using fuzzy implications, general quasi overlap functions and fuzzy neighborhood systems, which contains almost all existing fuzzy rough sets. Then, a novel feature selection algorithm (called IQNS-FS algorithm) is proposed and implemented using (I, Q, NS)-fuzzy rough sets, dependency and specificity measure. The results of 12 datasets indicate that IQNS-FS algorithm performs better than others. Finally, we input the results of IQNS-FS algorithm into single hidden layer neural networks and other classification algorithms, the results illustrate that the IQNS-FS algorithm can be better connected with neural networks than other classification algorithms. The high classification accuracy of single hidden layer neural networks (a very simple structure) further shows that the attributes selected by the IQNS-FS algorithm are important which can express the features of the datasets.}
}


@article{DBLP:journals/tkde/PanCXZNY24,
	author = {Xuan Pan and
                  Xiangrui Cai and
                  Sihan Xu and
                  Ying Zhang and
                  Peng Nie and
                  Xiaojie Yuan},
	title = {GeoCo: Geographical Correlation Enhanced Network for {POI} Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8362--8376},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3425151},
	doi = {10.1109/TKDE.2024.3425151},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/PanCXZNY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User mobility behaviors frequently exhibit a spatial clustering phenomenon, wherein points of interest (POIs) visited by the same user tend to be in close proximity. Consequently, leveraging geographical influences for user preference modeling remains a prevalent approach in POI recommendation tasks. However, existing studies often overlook users’ hidden geographical habits for the following reasons: (1) Geographical features are commonly approximated by manually partitioned regions or fixed distributions, inadequately capturing the nuanced spatial proximity among POIs. (2) POIs with high geographical correlations are not explicitly incorporated as feedback signals during the training process, resulting in a lack of spatial clustering pattern learning within users’ preference representations. This paper introduces GeoCo, a Geographical Correlation enhanced network for POI recommendation. First, we model POIs’ geographical features using fine-grained hierarchical sequences to capture multilevel spatial relations. Subsequently, we propose a pre-training network that employs the sentence similarity assessment technique to comprehend the semantics of geographical correlations. Second, we introduce a novel multi-objective training process that intuitively learns spatial clustering patterns through user mobility behaviors. Extensive experiments conducted on two location-based social network (LBSN) datasets, Gowalla and Foursquare, demonstrate the superiority of our proposed model over fourteen state-of-the-art baseline models in POI recommendation tasks. Compared with the baselines, GeoCo has achieved a performance improvement of at least 5\\%\nin Rec@5 and HR@5 on both datasets. Furthermore, we verify the effectiveness of pre-trained location vectors and the multi-objective training process in enhancing the model's understanding of geographical correlations for user preference construction.}
}


@article{DBLP:journals/tkde/XuLZXZ24,
	author = {Yehong Xu and
                  Lei Li and
                  Mengxuan Zhang and
                  Zizhuo Xu and
                  Xiaofang Zhou},
	title = {Global Optimal Travel Planning for Massive Travel Queries in Road
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8377--8394},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3439409},
	doi = {10.1109/TKDE.2024.3439409},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XuLZXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Travel planning plays an increasingly important role in our society. The travel plans, which consist of the paths each vehicle is suggested to follow and its corresponding departure time, influence the traffic conditions naturally. However, existing travel planning algorithms cannot consider the planning results and their influences simultaneously, so traffic congestion could be created when many vehicles are directed to adopt similar travel plans. In this paper, we propose the Global Optimal Travel Planning (GOTP) problem that aims to minimize traffic congestion by continuously evaluating traffic conditions for a set of planning tasks. Achieving this global optimization goal is non-trivial because travel planning and traffic evaluation are time-consuming and interdependent. To break this dependency, we first propose a GOTP paradigm that interleaves travel planning and traffic evaluation for queries, where the planning consists of departure time planning and travel path planning. To implement the paradigm, we propose the serial model that optimizes travel plans one by one, followed by the batch model that improves processing efficiency, and the iterative model that further optimizes planning quality. Extensive experiments on large real-world networks with synthetic and real workloads validate the effectiveness and efficiency of our methods.}
}


@article{DBLP:journals/tkde/WangZWZLYT24,
	author = {Zhaobo Wang and
                  Yanmin Zhu and
                  Chunyang Wang and
                  Xuhao Zhao and
                  Bo Li and
                  Jiadi Yu and
                  Feilong Tang},
	title = {Graph Diffusion-Based Representation Learning for Sequential Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8395--8407},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3477621},
	doi = {10.1109/TKDE.2024.3477621},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangZWZLYT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation is a critical part of the flourishing online applications by suggesting appealing items on users’ next interactions, where global dependencies among items have proven to be indispensable for enhancing the quality of item representations toward a better understanding of user dynamic preferences. Existing methods rely on pre-defined graphs with shallow Graph Neural Networks to capture such necessary dependencies due to the constraint of the over-smoothing problem. However, this graph representation learning paradigm makes them difficult to satisfy the original expectation because of noisy graph structures and the limited ability of shallow architectures for modeling high-order relations. In this paper, we propose a novel Graph Diffusion Representation-enhanced Attention Network for sequential recommendation, which explores the construction of deeper networks by utilizing graph diffusion on adaptive graph structures for generating expressive item representations. Specifically, we design an adaptive graph generation strategy via leveraging similarity learning between item embeddings, automatically optimizing the input graph topology under the guidance of downstream recommendation tasks. Afterward, we propose a novel graph diffusion paradigm with robustness to over-smoothing, which enriches the learned item representations with sufficient global dependencies for attention-based sequential modeling. Moreover, extensive experiments demonstrate the effectiveness of our approach over state-of-the-art baselines.}
}


@article{DBLP:journals/tkde/BaiZLZLYZ24,
	author = {Jie Bai and
                  Kang Zhao and
                  Linjing Li and
                  Daniel Dajun Zeng and
                  Qiudan Li and
                  Fan Yang and
                  Quannan Zu},
	title = {Graph Representation Learning Based on Cognitive Spreading Activations},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8408--8420},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3437781},
	doi = {10.1109/TKDE.2024.3437781},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/BaiZLZLYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph representation learning is an emerging area for graph analysis and inference. However, existing approaches for large-scale graphs either sample nodes in sequential walks or manipulate the adjacency matrices of graphs. The former approach can cause sampling bias against less-connected nodes, whereas the latter may suffer from sparsity that exists in many real-world graphs. To learn from structural information in a graph more efficiently and comprehensively, this paper proposes a new graph representation learning approach inspired by the cognitive model of spreading-activation mechanisms in human memory. This approach learns node embeddings by adopting a graph activation model that allows nodes to “activate” their neighbors and spread their own structural information to other nodes through the paths simultaneously. Comprehensive experiments demonstrate that the proposed model performs better than existing methods on several empirical datasets for multiple graph inference tasks. Meanwhile, the spreading-activation-based model is computationally more efficient than existing approaches–the training process converges after only a small number of iterations, and the training time is linear in the number of edges in a graph. The proposed method works for both homogeneous and heterogeneous graphs.}
}


@article{DBLP:journals/tkde/LiuLZ24,
	author = {Peiyao Liu and
                  Junpeng Lin and
                  Chen Zhang},
	title = {Heterogeneous Multivariate Functional Time Series Modeling: {A} State
                  Space Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8421--8433},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3472906},
	doi = {10.1109/TKDE.2024.3472906},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Functional data have been gaining increasing popularity in the field of time series analysis. However, so far modeling heterogeneous multivariate functional time series remains a research gap. To fill it, this paper proposes a time-varying functional state space model (TV-FSSM). It uses functional decomposition to extract features of the functional observations, where the decomposition coefficients are regarded as latent states that evolve according to a tensor autoregressive model. This two-layer structure can on the one hand efficiently extract continuous functional features, and on the other provide a flexible and generalized description of data heterogeneity among different time points. An expectation maximization (EM) framework is developed for parameter estimation, where regularization and constraints are incorporated for better model interoperability. As the sample size grows, an incremental learning version of the EM algorithm is given to efficiently update the model parameters. Some model properties, including model identifiability conditions, convergence issues, time complexities, and bounds of its one-step-ahead prediction errors, are also presented. Extensive experiments on both real and synthetic datasets are performed to evaluate the predictive accuracy and efficiency of the proposed framework.}
}


@article{DBLP:journals/tkde/LuoGHWHL24,
	author = {Zhipeng Luo and
                  Qiang Gao and
                  Yazhou He and
                  Hongjun Wang and
                  Milos Hauskrecht and
                  Tianrui Li},
	title = {Hierarchical Active Learning With Label Proportions on Data Regions},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8434--8446},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419588},
	doi = {10.1109/TKDE.2024.3419588},
	timestamp = {Mon, 03 Feb 2025 16:10:30 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LuoGHWHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning classification models from real-world data often requires substantial human effort devoted to instance annotation. As the instance-based annotating process can be very time-consuming and costly, we propose a novel active learning framework that builds classification models from human-annotated regions. A region is defined by a set of conjunctive patterns that are formed by value ranges over the input features. A region label is a human assessment of the class proportion in the data population covered by the region. By leveraging learning from label proportions algorithms, regions and their class proportions can be used to train instance-based classification models. However, the key challenge is that in practice, very few regions are defined already. Therefore, to identify regions important for model learning, we design a hierarchical active learning (HAL) framework, which actively builds a hierarchy of regions. Similar to the decision-tree learning process, our approach progressively divides the input data space into smaller sub-regions, solicits labels for the new regions, and retrains the base classification model with all the leaf regions. And we further develop a multi-hierarchy (forest) solution, which builds multiple shallower hierarchies that have more informative, diverse, and simpler regions. We evaluate our HAL framework on numerous impactful classification datasets as well as on a real user study - on the survival analysis of colorectal cancer patients. The results demonstrate that region-based active learning methods can learn high-quality classifiers from very few labeled regions. Hence, our framework is shown very effective in reducing the human annotation effort needed for building classification models.}
}


@article{DBLP:journals/tkde/WangXQLSH24,
	author = {Na Wang and
                  Shuxi Xu and
                  Chuan Qin and
                  Sian{-}Jheng Lin and
                  Shuo Shao and
                  Yunghsiang S. Han},
	title = {High-Capacity Framework for Reversible Data Hiding Using Asymmetric
                  Numeral Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8447--8461},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3438943},
	doi = {10.1109/TKDE.2024.3438943},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangXQLSH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reversible data hiding (RDH) has been extensively studied in the field of multimedia security. Embedding capacity is an important metric for RDH performance evaluation. However, the embedding capacity of existing methods for independent and identically distributed (i.i.d.) gray-scale signals is still not good enough. In this paper, we propose a high-capacity RDH code construction method that employs asymmetric numeral systems (ANS) coding as the underlying coding framework. Based on the proposed framework, two RDH methods are presented. First, we propose a static RDH method that takes the constant host probability mass function (PMF) as input parameters and offers high embedding performance. Then, we give a dynamic RDH method that can eliminate the need for transmitting the host PMF in advance by designing a reversible dynamic probability calculator. The simulation results on discrete normally distributed signals demonstrate that the performance of the proposed static method is very close to the expected rate-distortion bound, and the proposed dynamic method can achieve satisfactory embedding capacity without prior knowledge of host PMF at the cost of slightly sacrificing steganographic data quality. Moreover, the experimental results on gray-scale images show that the proposed static method provides higher peak signal-to-noise ratio (PSNR) values and larger embedding capacities than some state-of-the-art methods, e.g., the embedding capacity of image Lena is as high as 3.571 bits per pixel.}
}


@article{DBLP:journals/tkde/ChenCFHZ24,
	author = {Weijian Chen and
                  Yixin Cao and
                  Fuli Feng and
                  Xiangnan He and
                  Yongdong Zhang},
	title = {HoGRN: Explainable Sparse Knowledge Graph Completion via High-Order
                  Graph Reasoning Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8462--8475},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3422226},
	doi = {10.1109/TKDE.2024.3422226},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenCFHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graphs (KGs) are becoming increasingly essential infrastructures in many applications while suffering from incompleteness issues. The KG Completion (KGC) task automatically predicts missing facts based on an incomplete KG. However, existing methods perform unsatisfactorily in real-world scenarios. On the one hand, their performance will dramatically degrade along with the increasing sparsity of KGs. On the other hand, the inference procedure for prediction is an untrustworthy black box. This paper proposes a novel explainable model for sparse KGC, compositing high-order reasoning into a Graph Convolutional Network (GCN), namely HoGRN. It can not only improve the generalization ability to mitigate the information insufficiency issue but also provide interpretability while maintaining the model's effectiveness and efficiency. Two main components are seamlessly integrated for joint optimization. First, the high-order reasoning component learns high-quality relation representations by capturing endogenous correlation among relations. This can reflect logical rules to justify a broader range of missing facts. Second, the entity updating component leverages a weight-free GCN to efficiently model KG structures with interpretability. For evaluation, we conduct extensive experiments–the results of HoGRN on several sparse KGs present considerable improvements. Further ablation and case studies demonstrate the effectiveness of the main components.}
}


@article{DBLP:journals/tkde/SunZC24,
	author = {Tianao Sun and
                  Kai Zhao and
                  Meng Chen},
	title = {Human-AI Interaction: Human Behavior Routineness Shapes {AI} Performance},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8476--8487},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3480317},
	doi = {10.1109/TKDE.2024.3480317},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A crucial area of research in Human-AI Interaction focuses on understanding how the integration of AI into social systems influences human behavior, for example, how news-feeding algorithms affect people’s voting decisions. But little attention has been paid to how human behavior shapes AI performance. We fill this research gap by introducing routineness to measure human behavior for the AI system, which assesses the degree of routine in a person’s activity based on their past activities. We apply the proposed routineness metric to two extensive human behavior datasets: the human mobility dataset with over 700 million data samples and the social media dataset with over 3.8 million data samples. Our analysis reveals routineness can effectively detect behavioral changes in human activities. The performance of AI algorithms is profoundly determined by human routineness, which provides valuable guidance for the selection of AI algorithms.}
}


@article{DBLP:journals/tkde/YangWZHZZZW24,
	author = {Yonghui Yang and
                  Le Wu and
                  Kun Zhang and
                  Richang Hong and
                  Hailin Zhou and
                  Zhiqiang Zhang and
                  Jun Zhou and
                  Meng Wang},
	title = {Hyperbolic Graph Learning for Social Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8488--8501},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3343402},
	doi = {10.1109/TKDE.2023.3343402},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangWZHZZZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social recommendation provides an auxiliary social network structure to enhance recommendation performances. By formulating user-user social network and user-item interaction graph, modern social recommendation architecture is built on learning user and item embeddings into Euclidean space with graph convolution operations. However, the Euclidean space suffers structure distortion when representing the nature power-law distribution of graphs, leading to sub-optimal results for graph based social recommendation. Recently, some studies have explored the alternative of graph embedding learning into hyperbolic space, which can preserve the hierarchy of real-world graphs. However, directly applying current hyperbolic graph embedding models for social recommendation is non-trivial as two challenges: network heterogeneity and social diffusion noise. First, due to the semantic gap existing between social networks and user-item interactions, how to tackle the heterogeneity issue of social recommendation under hyperbolic formulation? Second, explicit modeling of social diffusion easily introduces noise for user preference learning, especially for those active users with amounts of interactions. To tackle the above challenges, in this paper, we propose a Hyperbolic Graph Learning based Social Recommendation (HGSR) model. First, we exploit social structure with hyperbolic social embedding pre-training, which could preserve the hierarchical properties of social networks. Second, we construct the heterogeneous graph based on user-item interactions and social networks, then treat the pre-trained social embeddings as an additional feature input for user preference learning. Such that, we combine explicit heterogeneous graph learning and implicit feature enhancement for the hyperbolic social recommendation, which can well tackle heterogeneity and social noise issues. We conduct empirical studies on four datasets, and extensive experiments demonstrate the effectiveness of our proposed model compared to state-of-the-art baselines.}
}


@article{DBLP:journals/tkde/ZhangZTZ24,
	author = {Junfeng Zhang and
                  Weixin Zeng and
                  Jiuyang Tang and
                  Xiang Zhao},
	title = {Hyperedge Graph Contrastive Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8502--8514},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3435861},
	doi = {10.1109/TKDE.2024.3435861},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZTZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although various graph contrastive learning (GCL) techniques have been employed to generate augmented views and maximize their mutual information, current solutions only consider the pairwise relationships based on edges, neglecting the high-order information that can help generate more informative augmented views and make better contrast. To fill in this gap, we propose to leverage hyperedge to facilitate GCL, as it connects two or more nodes and can model high-order relationships among multiple nodes. More specifically, hyperedges are constructed based on the original graph. Then, we conduct node-level PageRank based on hyperedges and hyperedge-level PageRank based on nodes to generate augmented views. As to the contrasting stage, different from existing GCL methods that simply treat the corresponding nodes of the anchor in different views as positives and overlook certain nodes strongly associated with the anchor, we build the positives and negatives based on hyperedges, where whether a node is a positive is determined by the number of hyperedges it coexists with the anchor. We compare our hyperedge GCL with state-of-the-art methods on downstream tasks, and the empirical results validate the superiority of our proposal. Further experiments on graph augmentation and graph contrastive loss also demonstrate the effectiveness of the proposed modules.}
}


@article{DBLP:journals/tkde/ZhaoZW24,
	author = {Tianxiang Zhao and
                  Xiang Zhang and
                  Suhang Wang},
	title = {Imbalanced Node Classification With Synthetic Over-Sampling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8515--8528},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3443160},
	doi = {10.1109/TKDE.2024.3443160},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, graph neural networks (GNNs) have achieved state-of-the-art performance for node classification. However, most existing GNNs would suffer from the graph imbalance problem. In many real-world scenarios, node classes are imbalanced, with some majority classes making up most parts of the graph. The message propagation mechanism in GNNs would further amplify the dominance of those majority classes, resulting in sub-optimal classification performance. In this work, we seek to address this problem by generating pseudo instances of minority classes to balance the training data, extending previous over-sampling-based techniques. This task is non-trivial, as those techniques are designed with the assumption that instances are independent. Neglection of relation information would complicate this oversampling process. Furthermore, the node classification task typically takes the semi-supervised setting with only a few labeled nodes, providing insufficient supervision for the generation of minority instances. Generated new nodes of low quality would harm the trained classifier. In this work, we address these difficulties by synthesizing new nodes in a constructed embedding space, which encodes both node attributes and topology information. Furthermore, an edge generator is trained simultaneously to model the graph structure and provide relations for new samples. To further improve the data efficiency, we also explore synthesizing mixed “in-between” nodes to utilize nodes from the majority class in this over-sampling process. Experiments on real-world datasets validate the effectiveness of our proposed framework.}
}


@article{DBLP:journals/tkde/WuZLSQ24,
	author = {Feng Wu and
                  Guoshuai Zhao and
                  Tengjiao Li and
                  Jialie Shen and
                  Xueming Qian},
	title = {Improving Conversational Recommendation System Through Personalized
                  Preference Modeling and Knowledge Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8529--8540},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3421580},
	doi = {10.1109/TKDE.2024.3421580},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuZLSQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conversational recommendation systems (CRS) can actively discover users’ preferences and perform recommendations during conversations. The majority of works on CRS tend to focus on a single conversation and dig it using knowledge graphs, language models, etc. However, they often overlook the abundant and rich preference information that exists in the user's historical conversations. Meanwhile, end-to-end generation of recommendation results may lead to a decrease in recommendation quality. In this work, we propose a personalized conversational recommendation system infused with historical interaction information. This framework leverages users’ preferences extracted from their historical conversations and integrates them with the users’ preferences in current conversations. We find that this contributes to higher accuracy in recommendations and fewer recommendation turns. Moreover, we improve the interactive pattern between the recommendation module and the dialogue generation module by utilizing the slot filling method. This enables the results inferred by the recommendation module to be integrated into the conversation naturally and accurately. Our experiments on the benchmark dataset demonstrate that our model significantly outperforms the state-of-the-art methods in the evaluation of recommendations and dialogue generation.}
}


@article{DBLP:journals/tkde/ChengZXZTZY24,
	author = {Zhangtao Cheng and
                  Fan Zhou and
                  Xovee Xu and
                  Kunpeng Zhang and
                  Goce Trajcevski and
                  Ting Zhong and
                  Philip S. Yu},
	title = {Information Cascade Popularity Prediction via Probabilistic Diffusion},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8541--8555},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3465241},
	doi = {10.1109/TKDE.2024.3465241},
	timestamp = {Thu, 06 Feb 2025 16:04:57 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChengZXZTZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information cascade popularity prediction is an important problem in social network content diffusion analysis. Various facets have been investigated (e.g., diffusion structures and patterns, user influence) and, recently, deep learning models based on sequential architecture and graph neural network (GNN) have been leveraged. However, despite the improvements attained in predicting the future popularity, these methodologies fail to capture two essential aspects inherent to information diffusion: (1) the temporal irregularity of cascade event – i.e., users’ re-tweetings at random and non-periodic time instants; and (2) the inherent uncertainty of the information diffusion. To address these challenges, in this work, we present CasDO – a novel framework for information cascade popularity prediction with probabilistic diffusion models and neural ordinary differential equations (ODEs). We devise a temporal ODE network to generalize the discrete state transitions in RNNs to continuous-time dynamics. CasDO introduces a probabilistic diffusion model to consider the uncertainties in information diffusion by injecting noises in the forwarding process and reconstructing cascade embedding in the reversing process. Extensive experiments that we conducted on three large-scale datasets demonstrate the advantages of the CasDO model over baselines.}
}


@article{DBLP:journals/tkde/SunWZCL24,
	author = {Qingqiang Sun and
                  Kai Wang and
                  Wenjie Zhang and
                  Peng Cheng and
                  Xuemin Lin},
	title = {Interdependence-Adaptive Mutual Information Maximization for Graph
                  Contrastive Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8556--8567},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3423409},
	doi = {10.1109/TKDE.2024.3423409},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunWZCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite remarkable advancements in graph contrastive learning techniques, the identification of interdependent relationships when maximizing cross-view mutual information remains a challenging issue, primarily due to the complexity of graph topology. In this study, we propose to formulate cross-view interdependence from the innovative perspective of information flow. Accordingly, IDEAL, a simple yet effective framework, is proposed for interdependence-adaptive graph contrastive learning. Compared with existing methods, IDEAL concurrently addresses same-node and distinct-node interdependence, circumvents the reliance on additional distribution mining techniques, and is augmentation-aware. Besides, the objective of IDEAL takes advantage of both contrastive and generative learning objectives and is thus capable of learning a uniform embedding distribution while retaining essential semantic information. The effectiveness of IDEAL is validated by extensive empirical evidence. It consistently outperforms state-of-the-art self-supervised methods by considerable margins across seven benchmark datasets with diverse scales and properties and, at the same time, showcases promising training efficiency.}
}


@article{DBLP:journals/tkde/YaoWXLKCD24,
	author = {Liuyi Yao and
                  Zhen Wang and
                  Yuexiang Xie and
                  Yaliang Li and
                  Weirui Kuang and
                  Daoyuan Chen and
                  Bolin Ding},
	title = {Is Sharing Neighbor Generator in Federated Graph Learning Safe?},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8568--8579},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3482448},
	doi = {10.1109/TKDE.2024.3482448},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YaoWXLKCD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, as privacy concerns continue to rise, federated graph learning (FGL) which generalizes the classic federated learning to graph data has attracted increasing attention. However, while the focus has been on designing collaborative learning algorithms, the potential risks of privacy leakage through the sharing of necessary graph-related information in FGL, such as node embeddings and neighbor generators, have been largely neglected. In this paper, we verify the potential risks of privacy leakage in FGL, and provide insights about the cautions in FGL algorithm design. Specifically, we propose a novel privacy attack algorithm named Privacy Attack on federated Graph learning (PAG) towards reconstructing participants’ private node attributes and the linkage relationships. The participant performing the PAG attack is able to reconstruct the node attributes of the victim by matching the received gradients of the generator, and then train a link prediction model based on its local sub-graph to inductively infer the linkages connected to these reconstructed nodes. We theoretically and empirically demonstrate that under PAG attack, directly sharing the neighbor generators makes the FGL vulnerable to the data reconstruction attack. Furthermore, an investigation into the key factors that can hinder the success of the PAG attack provides insights into corresponding defense strategies and inspires future research into privacy-preserving FGL.}
}


@article{DBLP:journals/tkde/ZhuWQW24a,
	author = {Yi Zhu and
                  Shuqin Wang and
                  Jipeng Qiang and
                  Xindong Wu},
	title = {Iterative Soft Prompt-Tuning for Unsupervised Domain Adaptation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8580--8592},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3483903},
	doi = {10.1109/TKDE.2024.3483903},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuWQW24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsupervised domain adaptation aims to facilitate learning tasks in unlabeled target domain with knowledge in the related source domain, which has achieved awesome performance with the pre-trained language models (PLMs). Recently, inspired by GPT, the prompt-tuning model has been widely explored in stimulating rich knowledge in PLMs for language understanding. However, existing prompt-tuning methods still directly applied the model that was learned in the source domain into the target domain to minimize the discrepancy between different domains, e.g., the prompts or the template are trained separately to learn embeddings for transferring to the target domain, which is actually the intuition of end-to-end deep-based approach. In this paper, we propose an Iterative Soft Prompt-Tuning method (ItSPT) for better unsupervised domain adaptation. On the one hand, the prompt-tuning model learned in the source domain is converted into an iterative model to find the true label information in the target domain, the domain adaptation method is then regarded as a few-shot learning task. On the other hand, instead of hand-crafted templates, ItSPT adopts soft prompts for both considering the automatic template generation and classification performance. Experiments on both English and Chinese datasets demonstrate that our method surpasses the performance of SOTA methods.}
}


@article{DBLP:journals/tkde/ZhangYYMW24,
	author = {Zhongyun Zhang and
                  Lei Yang and
                  Jiajun Yao and
                  Chao Ma and
                  Jianguo Wang},
	title = {Joint Optimization of Pricing, Dispatching and Repositioning in Ride-Hailing
                  With Multiple Models Interplayed Reinforcement Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8593--8606},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3464563},
	doi = {10.1109/TKDE.2024.3464563},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangYYMW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Popular ride-hailing products, such as DiDi, Uber and Lyft, provide people with transportation convenience. Pricing, order dispatching and vehicle repositioning are three tasks with tight correlation and complex interactions in ride-hailing platforms, significantly impacting each other’s decisions and demand distribution or supply distribution. However, no past work considered combining the three tasks to improve platform efficiency. In this paper, we exploit to optimize pricing, dispatching and repositioning strategies simultaneously. Such a new multi-stage decision-making problem is quite challenging because it involves complex coordination and lacks a unified problem model. To address this problem, we propose a novel Joint optimization framework of Pricing, Dispatching and Repositioning (JPDR) integrating contextual bandit and multi-agent deep reinforcement learning. JPDR consists of two components, including a Soft Actor-Critic (SAC)-based centralized policy for dispatching and repositioning and a pricing strategy learned by a multi-armed contextual bandit algorithm based on the feedback from the former. The two components learn in a mutually guided way to achieve joint optimization because their updates are highly interdependent. Based on real-world data, we implement a realistic environment simulator. Extensive experiments conducted on it show our method outperforms state-of-the-art baselines in terms of both gross merchandise volume and success rate.}
}


@article{DBLP:journals/tkde/GazaB24,
	author = {Haifa Gaza and
                  Jaewook Byun},
	title = {Kairos: Enabling Prompt Monitoring of Information Diffusion Over Temporal
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8607--8621},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3347621},
	doi = {10.1109/TKDE.2023.3347621},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GazaB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analyses of temporal graphs provide valuable insights into temporal data through the use of two analytical approaches: temporal evolution and temporal information diffusion. The former shows how a network evolves over time; the latter explains how information spreads throughout a network over time. Systems have been mainly proposed to efficiently handle graph snapshots, which are suitable for temporal evolution but inappropriate for temporal information diffusion. For analyses of temporal information diffusion, temporal graph traversal platforms have recently been proposed; however, it is still infeasible to handle infinitely evolving temporal data, especially for monitoring applications. In this paper, we propose an incremental approach and its graph processing engine, Kairos, to enable prompt monitoring of temporal information diffusion. This approach makes it possible to immediately process diffusion results for sources of interest by traversing a part of the whole network, which avoids full traversals influenced by a small change in the network, thus making monitoring applications feasible. The recipes for implementing incremental versions of existing temporal graph traversal algorithms and metrics will make it easier for users to build their ad-hoc programs.}
}


@article{DBLP:journals/tkde/JinLHJJH24,
	author = {Bowen Jin and
                  Gang Liu and
                  Chi Han and
                  Meng Jiang and
                  Heng Ji and
                  Jiawei Han},
	title = {Large Language Models on Graphs: {A} Comprehensive Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8622--8642},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3469578},
	doi = {10.1109/TKDE.2024.3469578},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/JinLHJJH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field.}
}


@article{DBLP:journals/tkde/KhanAMM24,
	author = {Abdul Atif Khan and
                  Mohammad Maksood Akhter and
                  Rashmi Maheshwari and
                  Sraban Kumar Mohanty},
	title = {{L-ASCRA:} {A} Linearithmic Time Approximate Spectral Clustering Algorithm
                  Using Topologically-Preserved Representatives},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8643--8654},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3483572},
	doi = {10.1109/TKDE.2024.3483572},
	timestamp = {Mon, 03 Mar 2025 22:25:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/KhanAMM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate spectral clustering (ASC) algorithms work on the representative points of the data for discovering intrinsic groups. The existing ASC methods identify fewer representatives as compared to the number of data points to reduce the cubic computational overhead of the spectral clustering technique. However, identifying such representative points without any domain knowledge to capture the shapes and topology of the clusters remains a challenge. This work proposes an ASC method that suitably computes enough well-scattered representatives to efficiently capture the topology of the data, making the ASC faster without the requirement of tuning any external parameters. The proposed ASC algorithm first applies two-level partitioning using both boundary points and centroids-based partitioning to identify quality representatives in less time. In the next step, we calculate the proximity between the neighboring representatives using k-rounds of minimum spanning tree (MST) by considering the distribution of edge weights in each round to find k. The proposed method effectively utilizes the number of representatives in a way that the overall computational time is bounded by O(N\\lg N). The experimental results suggest that the proposed ASC method outperforms the competing ASC methods in terms of both running time and clustering quality.}
}


@article{DBLP:journals/tkde/LiuLCLWYW24,
	author = {Cheng Liu and
                  Rui Li and
                  Hangjun Che and
                  Man{-}Fai Leung and
                  Si Wu and
                  Zhiwen Yu and
                  Hau{-}San Wong},
	title = {Latent Structure-Aware View Recovery for Incomplete Multi-View Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8655--8669},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3445992},
	doi = {10.1109/TKDE.2024.3445992},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLCLWYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incomplete multi-view clustering (IMVC) presents a significant challenge due to the need for effectively exploring complementary and consistent information within the context of missing views. One promising strategy to tackle this challenge is to recover missing views by inferring the missing samples. However, such approaches often fail to fully utilize discriminative structural information or adequately address consistency, as it requires such information to be known or learnable in advance, which contradicts the incomplete data setting. In this study, we propose a novel approach called Latent Structure-Aware view recovery (LaSA) for the IMVC task. Our objective is to recover missing views through discriminative latent representations by leveraging structural information. Specifically, our method offers a unified closed-form formulation that simultaneously performs missing data inference and latent representation learning, using a learned intrinsic graph as structural information. This formulation, incorporating graph structure information, enhances the inference of missing data while facilitating discriminative feature learning. Even when intrinsic graph is initially unknown due to incomplete data, our formulation allows for effective view recovery and intrinsic graph learning through an iterative optimization process. To further enhance performance, we introduce an iterative consistency diffusion process, which effectively leverages the consistency and complementary information across multiple views. Extensive experiments demonstrate the effectiveness of the proposed method compared to state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/ChengCSL24,
	author = {Yao Cheng and
                  Minjie Chen and
                  Caihua Shan and
                  Xiang Li},
	title = {Learning Prioritized Node-Wise Message Propagation in Graph Neural
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8670--8681},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3436909},
	doi = {10.1109/TKDE.2024.3436909},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChengCSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) have recently received significant attention. Learning node-wise message propagation in GNNs aims to set personalized propagation steps for different nodes in the graph. Despite the success, existing methods ignore node priority that can be reflected by node influence and heterophily. In this paper, we propose a versatile framework PriPro, which can be integrated with most existing GNN models and aim to learn prioritized node-wise message propagation in GNNs. Specifically, the framework consists of three components: a backbone GNN model, a propagation controller to determine the optimal propagation steps for nodes, and a weight controller to compute the priority scores for nodes. We design a mutually enhanced mechanism to compute node priority, optimal propagation step and label prediction. We also propose an alternative optimization strategy to learn the parameters in the backbone GNN model and two parametric controllers. We conduct extensive experiments to compare our framework with other 12 state-of-the-art competitors on 10 benchmark datasets. Experimental results show that our framework can lead to superior performance in terms of propagation strategies and node representations.}
}


@article{DBLP:journals/tkde/MaCTZLPLWWSZY24,
	author = {Tengfei Ma and
                  Yujie Chen and
                  Wen Tao and
                  Dashun Zheng and
                  Xuan Lin and
                  Patrick Cheong{-}Iao Pang and
                  Yiping Liu and
                  Yijun Wang and
                  Longyue Wang and
                  Bosheng Song and
                  Xiangxiang Zeng and
                  Philip S. Yu},
	title = {Learning to Denoise Biomedical Knowledge Graph for Robust Molecular
                  Interaction Prediction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8682--8694},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3471508},
	doi = {10.1109/TKDE.2024.3471508},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MaCTZLPLWWSZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Molecular interaction prediction plays a crucial role in forecasting unknown interactions between molecules, such as drug-target interaction (DTI) and drug-drug interaction (DDI), which are essential in the field of drug discovery and therapeutics. Although previous prediction methods have yielded promising results by leveraging the rich semantics and topological structure of biomedical knowledge graphs (KGs), they have primarily focused on enhancing predictive performance without addressing the presence of inevitable noise and inconsistent semantics. This limitation has hindered the advancement of KG-based prediction methods. To address this limitation, we propose BioKDN (Biomedical Knowledge Graph Denoising Network) for robust molecular interaction prediction. BioKDN refines the reliable structure of local subgraphs by denoising noisy links in a learnable manner, providing a general module for extracting task-relevant interactions. To enhance the reliability of the refined structure, BioKDN maintains consistent and robust semantics by smoothing relations around the target interaction. By maximizing the mutual information between reliable structure and smoothed relations, BioKDN emphasizes informative semantics to enable precise predictions. Experimental results on real-world datasets show that BioKDN surpasses state-of-the-art models in DTI and DDI prediction tasks, confirming the effectiveness and robustness of BioKDN in denoising unreliable interactions within contaminated KGs.}
}


@article{DBLP:journals/tkde/LaiZLJLZ24,
	author = {Zhichen Lai and
                  Dalin Zhang and
                  Huan Li and
                  Christian S. Jensen and
                  Hua Lu and
                  Yan Zhao},
	title = {LightCTS*: Lightweight Correlated Time Series Forecasting Enhanced
                  With Model Distillation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8695--8710},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3424451},
	doi = {10.1109/TKDE.2024.3424451},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LaiZLJLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Correlated time series (CTS) forecasting is essential in many practical applications, such as traffic management and server load control. Various deep learning based solutions have been proposed to improve forecasting accuracy. However, while models have become increasingly computationally intensive, they struggle to improve accuracy. This study aims instead to enable more lightweight, accurate models suitable for resource-constrained devices. To achieve this goal, we characterize popular CTS forecasting models, yielding two observations for developing lightweight CTS forecasting. On this basis, we propose the LightCTS framework that adopts plain stacking of temporal and spatial operators instead of alternate stacking which is much more computationally expensive. Moreover, LightCTS features light temporal and spatial operators, L-TCN and GL-Former, offering improved computational efficiency without compromising their feature extraction capabilities. LightCTS also encompasses a last-shot compression scheme to reduce redundant temporal features and speed up subsequent computations. Next, we equip LightCTS with two knowledge distillation modules, Tafd and Caad, that result in LightCTS^\\star retaining the original benefits of LightCTS, while also being able to adapt to varying levels of ultra-constrained resources. Experimental studies offer detailed insight into these proposals and provide evidence that both LightCTS and LightCTS^\\star are capable of nearly state-of-the-art accuracy at substantially reduced computational costs.}
}


@article{DBLP:journals/tkde/WanAKV24,
	author = {Li Wan and
                  Tansu Alpcan and
                  Margreta Kuijper and
                  Emanuele Viterbo},
	title = {Lightweight Conceptual Dictionary Learning for Text Classification
                  Using Information Compression},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8711--8717},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3421255},
	doi = {10.1109/TKDE.2024.3421255},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WanAKV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a novel supervised dictionary learning framework for text classification, integrating the Lempel-Ziv-Welch (LZW) algorithm for data compression and dictionary construction. This two-phase approach refines dictionaries by optimizing dictionary atoms for discriminative power using mutual information and class distribution. Our method facilitates classifier training, such as SVMs and neural networks. We introduce the information plane area rank (IPAR) to evaluate the information-theoretic performance of our algorithm. Tested on six benchmark text datasets, our model performs nearly as well as top models in limited-vocabulary settings, lagging by only about 2% while using just 10% of the parameters. However, its performance drops in diverse-vocabulary contexts due to the LZW algorithm's limitations with low-repetition data. This contrast highlights its efficiency and limitations across different dataset types.}
}


@article{DBLP:journals/tkde/ZhangZZWW24,
	author = {Huanyu Zhang and
                  Yifan Zhang and
                  Zhang Zhang and
                  Qingsong Wen and
                  Liang Wang},
	title = {LogoRA: Local-Global Representation Alignment for Robust Time Series
                  Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8718--8729},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3459908},
	doi = {10.1109/TKDE.2024.3459908},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZZWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsupervised domain adaptation (UDA) of time series aims to teach models to identify consistent patterns across various temporal scenarios, disregarding domain-specific differences, which can maintain their predictive accuracy and effectively adapt to new domains. However, existing UDA methods struggle to adequately extract and align both global and local features in time series data. To address this issue, we propose the Local-Global Representation Alignment framework (LogoRA), which employs a two-branch encoder–comprising a multi-scale convolutional branch and a patching transformer branch. The encoder enables the extraction of both local and global representations from time series. A fusion module is then introduced to integrate these representations, enhancing domain-invariant feature alignment from multi-scale perspectives. To achieve effective alignment, LogoRA employs strategies like invariant feature learning on the source domain, utilizing triplet loss for fine alignment and dynamic time warping-based feature alignment. Additionally, it reduces source-target domain gaps through adversarial training and per-class prototype alignment. Our evaluations on four time-series datasets demonstrate that LogoRA outperforms strong baselines by up to 12.52%, showcasing its superiority in time series UDA tasks.}
}


@article{DBLP:journals/tkde/WangYSZCHL24,
	author = {Haixin Wang and
                  Xinlong Yang and
                  Jinan Sun and
                  Shikun Zhang and
                  Chong Chen and
                  XianSheng Hua and
                  Xiao Luo},
	title = {Look Into Gradients: Learning Compact Hash Codes for Out-of-Distribution
                  Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8730--8743},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3425268},
	doi = {10.1109/TKDE.2024.3425268},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangYSZCHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hashing aims to compress raw data into compact binary descriptors, which has drawn increasing interest for efficient large-scale image retrieval. Current deep hashing often employs evaluation protocols where usually query data and training data are from similar distributions. However, more realistic evaluations should take into account a broad spectrum of distribution shifts with varying degrees. Therefore, we study the problem of out-of-distribution generalization in image retrieval, which seeks to learn a retrieval model from a source domain and generalize to unseen target domains. However, this problem is challenging owing to data scarcity in target domains and the potential overfitting of domain-specific patterns. Here, we propose a novel hashing model named Looking-into-gradients (LOG) for image retrieval under out-of-distribution shifts, which comprehensively explores gradients for both data generation and model optimization. Specifically, to overcome data deficiency in target domains, we formalize the worst-case problem to generate challenging virtue samples via adversarial gradient ascend. Besides, to further enhance model generalization capability, we not only identify non-crucial parameters with minor gradients and values and shrink them to zero, but also modify the inconsistent gradients across domains to prevent learning domain-specific patterns. Extensive experiments on various datasets demonstrate that LOG outperforms state-of-the-art methods by up to 8.54%.}
}


@article{DBLP:journals/tkde/BiDFWHZ24,
	author = {Wendong Bi and
                  Lun Du and
                  Qiang Fu and
                  Yanlin Wang and
                  Shi Han and
                  Dongmei Zhang},
	title = {Make Heterophilic Graphs Better Fit {GNN:} {A} Graph Rewiring Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8744--8757},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3441766},
	doi = {10.1109/TKDE.2024.3441766},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/BiDFWHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have shown superior performance in modeling graph data. Existing studies have shown that a lot of GNNs perform well on homophilic graphs while performing poorly on heterophilic graphs. Recently, researchers have turned their attention to design GNNs for heterophilic graphs by specific model design. Different from existing methods that mitigate heterophily by model design, we propose to study heterophilic graphs from an orthogonal perspective by rewiring the graph to reduce heterophily and make GNNs perform better. Through comprehensive empirical analysis, we verify the potential of graph rewiring methods. Then we propose a method named Deep Heterophily Graph Rewiring (DHGR) to rewire graphs by adding homophilic edges and pruning heterophilic edges. The rewiring operation is implemented by comparing the similarity of neighborhood label/feature distribution of node pairs. Besides, we design a scalable implementation for DHGR to guarantee a high efficiency. DHRG can be easily used as a plug-in module, i.e., a graph pre-processing step, for any GNNs, including both GNNs for homophily and heterophily, to boost their performance on the node classification task. To the best of our knowledge, it is the first work studying graph rewiring for heterophilic graphs. Extensive experiments on 11 public graph datasets demonstrate the superiority of our proposed methods.}
}


@article{DBLP:journals/tkde/ZhengLCSLHT24,
	author = {Xiaolin Zheng and
                  Weiming Liu and
                  Chaochao Chen and
                  Jiajie Su and
                  Xinting Liao and
                  Mengling Hu and
                  Yanchao Tan},
	title = {Mining User Consistent and Robust Preference for Unified Cross Domain
                  Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8758--8772},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3446581},
	doi = {10.1109/TKDE.2024.3446581},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengLCSLHT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-Domain Recommendation has been popularly studied to resolve data sparsity problem via leveraging knowledge transfer across different domains. In this paper, we focus on the Unified Cross-Domain Recommendation (Unified CDR) problem. That is, how to enhance the recommendation performance within and cross domains when users are partially overlapped. It has two main challenges, i.e., 1) how to obtain robust matching solution among the whole users and 2) how to exploit consistent and accurate results across domains. To address these two challenges, we propose MUCRP, a cross-domain recommendation framework for the Unified CDR problem. MUCRP contains three modules, i.e., variational rating reconstruction module, robust variational embedding alignment module, and cycle-consistent preference extraction module. To solve the first challenge, we propose fused Gromov-Wasserstein distribution co-clustering optimal transport to obtain more robust matching solution via considering both semantic and structure information. To tackle the second challenge, we propose embedding-consistent and prediction-consistent losses via dual autoencoder framework to achieve consistent results. Our empirical study on Douban and Amazon datasets demonstrates that MUCRP significantly outperforms the state-of-the-art models.}
}


@article{DBLP:journals/tkde/HeWGWHY24,
	author = {Xiaxia He and
                  Boyue Wang and
                  Junbin Gao and
                  Qianqian Wang and
                  Yongli Hu and
                  Baocai Yin},
	title = {Mixed-Modality Clustering via Generative Graph Structure Matching},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8773--8786},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3434556},
	doi = {10.1109/TKDE.2024.3434556},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HeWGWHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The goal of mixed-modality clustering, which differs from typical multi-modality/view clustering, is to divide samples derived from various modalities into several clusters. This task has to solve two critical semantic gap problems: i) how to generate the missing modalities without the pairwise-modality data; and ii) how to align the representations of heterogeneous modalities. To tackle the above problems, this paper proposes a novel mixed-modality clustering model, which integrates the missing-modality generation and the heterogeneous modality alignment into a unified framework. During the missing-modality generation process, a bidirectional mapping is established between different modalities, enabling generation of preliminary representations for the missing-modality using information from another modality. Then the intra-modality bipartite graphs are constructed to help generate better missing-modality representations by weighted aggregating existing intra-modality neighbors. In this way, a pairwise-modality representation for each sample can be obtained. In the process of heterogeneous modality alignment, each modality is modelled as a graph to capture the global structure among intra-modality samples and is aligned against the heterogeneous modality representations through the adaptive heterogeneous graph matching module. Experimental results on three public datasets show the effectiveness of the proposed model compared to multiple state-of-the-art multi-modality/view clustering methods.}
}


@article{DBLP:journals/tkde/MuNWXZL24,
	author = {Jie Mu and
                  Feiping Nie and
                  Wei Wang and
                  Jian Xu and
                  Jing Zhang and
                  Han Liu},
	title = {MOCOLNet: {A} Momentum Contrastive Learning Network for Multimodal
                  Aspect-Level Sentiment Analysis},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8787--8800},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3345022},
	doi = {10.1109/TKDE.2023.3345022},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MuNWXZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multimodal aspect-level sentiment analysis has attracted increasing attention in recent years. However, existing methods have two unaddressed limitations: (1) due to the lack of labelled pre-training data of dedicated sentiment analysis, the methods with a pre-training manner produce suboptimal prediction results; (2) most existing methods employ a self-attention encoder to fuse multimodal tokens, which not only ignores the alignment relationship between different modal tokens but also makes the model unable to capture the semantic links between images and texts. In this paper, we propose a momentum contrastive learning network (MOCOLNet) to overcome above limitations. First, we merge the pre-training stage with the training stage to design an end-to-end training manner which uses less labelled data dedicated to sentiment analysis to obtain better prediction results. Second, we propose a multimodal contrastive learning method to align the different modal representations before data fusing, and design a cross-modal matching strategy to provide semantic interactive information between texts and images. Moreover, we introduce an auxiliary momentum strategy to increase the robustness of model. We also analyse the effectiveness of the proposed multimodal contrastive learning method using a mutual information theory. Experiments verify that the proposed MOCOLNet is superior to other strong baselines.}
}


@article{DBLP:journals/tkde/ZhangSXDW24,
	author = {Xiao Zhang and
                  Teng Shi and
                  Jun Xu and
                  Zhenhua Dong and
                  Ji{-}Rong Wen},
	title = {Model-Agnostic Causal Embedding Learning for Counterfactually Group-Fair
                  Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8801--8813},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3424906},
	doi = {10.1109/TKDE.2024.3424906},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangSXDW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Group-fair recommendation aims at ensuring the equality of recommendation results across user groups categorized by sensitive attributes (e.g., gender, occupation, etc.). Existing group-fair recommendation models traditionally employ original user embeddings for both training and testing, primarily focusing on statistical learning while imposing group fairness constraints under the I.I.D. assumption. However, these models encounter limitations when addressing out-of-distribution (OOD) sensitive attributes. The fundamental issue of unfairness within user embeddings arises from a causal perspective, where each embedding vector comprises an exogenous component devoid of correlations with sensitive attributes and an endogenous component strongly correlated with these attributes. Overlooking the distinction between these two components during model training renders models sensitive to shifts in the distribution of sensitive attributes. This paper introduces the concept of Counterfactual Group Fairness (CGF) along with a corresponding metric to evaluate group fairness in scenarios involving OOD sensitive attributes in recommender systems. Building on this foundation, we propose a model-agnostic causal embedding learning framework named MACE. MACE effectively disentangles user embedding vectors into their exogenous and endogenous parts, thus ensuring group fairness, even in the presence of OOD sensitive attributes in embeddings. Specifically, MACE identifies the exogenous part of each user's embedding using mutual information minimization, treating it as instrumental variables. Subsequently, under the constraint of CGF, MACE reconstructs the endogenous and exogenous parts using the instrumental variable regression, combines the obtained parts into novel user embeddings using deep neural networks, and uses the combined embeddings for fair recommendation. Experimental results demonstrated that MACE can outperform the state-of-the-art baselines in terms of the metric of CGF while maintaining a comparable recommendation accuracy.}
}


@article{DBLP:journals/tkde/LiaoWSWZ24,
	author = {Zihan Liao and
                  Xiaodong Wu and
                  Shuo Shang and
                  Jun Wang and
                  Wei Zhang},
	title = {Modeling Dynamic Item Tendency Bias in Sequential Recommendation With
                  Causal Intervention},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8814--8828},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3427719},
	doi = {10.1109/TKDE.2024.3427719},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiaoWSWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation is a critical but challenging task in capturing users’ potential preferences due to inherent biases in the data. Existing debiasing recommendation methods aim to eliminate biases from historical interaction data collected by recommender systems and have shown promising results. However, there is another significant bias that hinders the improvement of sequential recommendation models: dynamic item tendency bias. This bias arises because a period might have some unique tendencies consisting of items interacted with by users with the same intent, leading to a dynamic tendency distribution that biases the model training towards these tendencies. To address this issue, we propose a causal approach to model dynamic item tendency bias in sequential recommendation. We first extract tendencies on carefully designed item-item graphs through community detection. We then use causal intervention to conduct deconfounded training to capture true user preferences and introduce the beneficial item tendency bias to the inference process through optimal transport techniques. Experimental results on four real-world datasets demonstrate that our proposed method consistently outperforms state-of-the-art debiasing recommendation methods, confirming that our model is effective in reducing dynamic item tendency bias and dealing with tendency drifts.}
}


@article{DBLP:journals/tkde/YangWZCYLX24,
	author = {Haoran Yang and
                  Yuhao Wang and
                  Xiangyu Zhao and
                  Hongxu Chen and
                  Hongzhi Yin and
                  Qing Li and
                  Guandong Xu},
	title = {Multi-Level Graph Knowledge Contrastive Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8829--8841},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3466530},
	doi = {10.1109/TKDE.2024.3466530},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangWZCYLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Contrastive Learning (GCL) stands as a potent framework for unsupervised graph representation learning that has gained traction across numerous graph learning applications. The effectiveness of GCL relies on generating high-quality contrasting samples, enhancing the model’s ability to discern graph semantics. However, the prevailing GCL methods face two key challenges: 1) introducing noise during graph augmentations and 2) requiring additional storage for generated samples, which degrade the model performance. In this paper, we propose novel approaches, GKCL (i.e., Graph Knowledge Contrastive Learning) and DGKCL (i.e., Distilled Graph Knowledge Contrastive Learning), that leverage multi-level graph knowledge to create noise-free contrasting pairs. This framework not only addresses the noise-related challenges but also circumvents excessive storage demands. Furthermore, our method incorporates a knowledge distillation component to optimize the trained embedding tables, reducing the model’s scale while ensuring superior performance, particularly for the scenarios with smaller embedding sizes. Comprehensive experimental evaluations on three public benchmark datasets underscore the merits of our proposed method and elucidate its properties, which primarily reflect the performance of the proposed method equipped with different embedding sizes and how the distillation weight affects the overall performance.}
}


@article{DBLP:journals/tkde/LiXLGH24,
	author = {Shuaiyang Li and
                  Feng Xue and
                  Kang Liu and
                  Dan Guo and
                  Richang Hong},
	title = {Multimodal Graph Causal Embedding for Multimedia-Based Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8842--8858},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3424268},
	doi = {10.1109/TKDE.2024.3424268},
	timestamp = {Sat, 01 Mar 2025 11:23:14 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiXLGH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multimedia-based recommendation (MMRec) models typically rely on observed user-item interactions and the multimodal content of items, such as visual images and textual descriptions, to predict user preferences. Among these, the user's preference for the displayed multimodal content of items is crucial for interacting with a particular item. We argue that users' preference behaviors (i.e., user-item interactions) for the modality content of items, beyond stemming from their real interest in the modality content, may also be influenced by their conformity to the popularity of items' modality-specific content (e.g., a user might be motivated to interact with a lipstick due to enthusiastic discussions among other users regarding textual reviews of the product). In essence, user-item interactions are jointly triggered by real interest and conformity. However, most existing MMRec models primarily concentrate on modeling users' interest preferences when capturing multimodal user preferences, neglecting the modeling of their conformity preferences, which results in sub-optimal recommendation performance. In this work, we resort to causal theory to propose a novel MMRec model, termed Multimodal Graph Causal Embedding (MGCE), revealing insights into the crucial causal relations of users' modality-specific interest and conformity in interaction behaviors within MMRec scenarios. Inspired by the colliding effect in causal inference and integrating the characteristics of real interest and conformity, we devise multimodal causal embedding learning networks to facilitate the learning of high-quality causal embeddings (multimodal interest and multimodal conformity embeddings) from both the structure-level and feature-level, yielding state-of-the-art performance. Extensive experimental results on three datasets demonstrate the effectiveness of MGCE.}
}


@article{DBLP:journals/tkde/OuyangWZYYL24,
	author = {Dian Ouyang and
                  Zhuoran Wang and
                  Fan Zhang and
                  Shiyu Yang and
                  Jianye Yang and
                  Xuemin Lin},
	title = {Multi-Source Shortest Path Query With Assembly Points on Large Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8859--8875},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3424947},
	doi = {10.1109/TKDE.2024.3424947},
	timestamp = {Sun, 22 Dec 2024 15:49:05 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/OuyangWZYYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computing Multi-source Shortest Path query with Assembly points (\\mathsf {MSPA}\n) is a fundamental graph problem. The \\mathsf {MSPA}\nproblem locates a set of assembly points to minimize the overall distance for transporting objects from different sources to a destination, where we can assemble objects at assembly points to reduce the total cost. We prove that the \\mathsf {MSPA}\nproblem is NP-hard. The intuitive method for computing the optimal set of assembly points and the corresponding set of paths is by Branch-and-Bound. However, the combination of different assembly points is exponential. By analyzing the structure of the path set based on the proposed distance graph, we find that the used paths can be combined into a tree. Hence, by defining the state of subtrees and the state transition equation, we propose a dynamic programming (DP) algorithm by pruning the redundant computation of subtrees. The experiment shows that the DP algorithm can achieve three orders of magnitude speedup in query processing time compared with the optimized Branch-and-Bound algorithm. Moreover, we reduce the transition candidates of the DP algorithm from the entire vertex set to certain neighbors. Extensive experiments are conducted on different types of real-world networks to demonstrate the performance of our DP algorithm.}
}


@article{DBLP:journals/tkde/LiLSLZWS24,
	author = {Tian{-}Bao Li and
                  An{-}An Liu and
                  Dan Song and
                  Wenhui Li and
                  Jing Zhang and
                  Zhiqiang Wei and
                  Yu{-}Ting Su},
	title = {Multi-Task Spatial-Temporal Transformer for Multi-Variable Meteorological
                  Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8876--8888},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3432599},
	doi = {10.1109/TKDE.2024.3432599},
	timestamp = {Mon, 03 Feb 2025 10:44:20 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiLSLZWS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study delves into multi-variable meteorological spatial-temporal prediction, focusing on the simultaneous forecasting of key meteorological parameters such as temperature, wind speed, and atmospheric pressure. The core challenge of this task lies in identifying commonalities across different variables while capturing their unique features and the interactions among them. To address this, we propose a novel multi-task learning framework tailored for multi-variable meteorological forecasting. Our framework integrates a convolutional variable-specific visual representation module and a variable-interactive spatial-temporal inference module. The former extracts distinct variable information independently for each variable, while the latter employs a tri-level attention mechanism across space, time, and variables to uncover both commonalities and interactions among the variables. An adaptive multi-loss optimization strategy and a local information aggregation module are introduced to balance task optimization complexities and enhance representation stability. Comprehensive experiments across various meteorological prediction tasks confirm the effectiveness of our methods, showcasing superior performance over existing approaches.}
}


@article{DBLP:journals/tkde/ZhuHTLLH24,
	author = {Yanran Zhu and
                  Xiao He and
                  Chang Tang and
                  Xinwang Liu and
                  Yuanyuan Liu and
                  Kunlun He},
	title = {Multi-View Adaptive Fusion Network for Spatially Resolved Transcriptomics
                  Data Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8889--8900},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3450333},
	doi = {10.1109/TKDE.2024.3450333},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuHTLLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial transcriptomics technology fully leverages spatial location and gene expression information for spatial clustering tasks. However, existing spatial clustering methods primarily concentrate on utilizing the complementary features between spatial and gene expression information, while overlooking the discriminative features during the integration process. Consequently, the discriminative capability of node representation in the gene expression features is limited. Besides, most existing methods lack a flexible combination mechanism to adaptively integrate spatial and gene expression information. To this end, we propose an end-to-end deep learning method named MAFN for spatially resolved transcriptomics data clustering via a multi-view adaptive fusion network. Specifically, we first adaptively learn inter-view complementary features from spatial and gene expression information. To improve the discriminative capability of gene expression nodes by utilizing spatial information, we employ two GCN encoders to learn intra-view specific features and design a Cross-view Correlation Reduction (CCR) strategy to filter the irrelevant information. Moreover, considering the distinct characteristics of each view, a Cross-view Attention Module (CAM) is utilized to adaptively fuse the multi-view features. Extensive experimental results demonstrate that the proposed MAFN achieves competitive performance in spatial domain identification compared to other state-of-the-art ones.}
}


@article{DBLP:journals/tkde/ZhouG24,
	author = {Mengting Zhou and
                  Zhiguo Gong},
	title = {Neighbor Distribution Learning for Minority Class Augmentation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8901--8913},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3447014},
	doi = {10.1109/TKDE.2024.3447014},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have achieved remarkable success in graph-based tasks. However, learning unbiased node representations under class-imbalanced training data remains challenging. Existing solutions may face overfitting due to extensive reuse of those limited labeled data in minority classes. Furthermore, many works address the class-imbalanced issue based on the embeddings generated from the biased GNNs, which make models intrinsically biased towards majority classes. In this paper, we propose a novel data augmentation strategy GraphGLS for semi-supervised class-imbalanced node classification, which aims to select informative unlabeled nodes to augment minority classes with consideration of both global and local information. Specifically, we first design a Global Selection module to learn global information (pseudo-labels) for unlabeled nodes and then select potential ones from them for minority classes. The Local Selection module further conducts filtering over those potential nodes by comparing their neighbor distributions with minority classes. To achieve this, we further design a neighbor distribution auto-encoder to learn a robust node-level neighbor distribution for each node. Then, we define class-level neighbor distribution to capture the overall neighbor characteristics of nodes within the same class. We conduct extensive experiments on multiple datasets, and the results demonstrate the superiority of GraphGLS over state-of-the-art baselines.}
}


@article{DBLP:journals/tkde/XieZZLTH24,
	author = {Zhiwen Xie and
                  Yi Zhang and
                  Guangyou Zhou and
                  Jin Liu and
                  Xinhui Tu and
                  Jimmy Xiangji Huang},
	title = {One Subgraph for All: Efficient Reasoning on Opening Subgraphs for
                  Inductive Knowledge Graph Completion},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8914--8927},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3432767},
	doi = {10.1109/TKDE.2024.3432767},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XieZZLTH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graph Completion (KGC) has garnered massive research interest recently, and most existing methods are designed following a transductive setting where all entities are observed during training. Despite the great progress on the transductive KGC, these methods struggle to conduct reasoning on emerging KGs involving unseen entities. Thus, inductive KGC, which aims to deduce missing links among unseen entities, has become a new trend. Many existing studies transform inductive KGC as a graph classification problem by extracting enclosing subgraphs surrounding each candidate triple. Unfortunately, they still face certain challenges, such as the expensive time consumption caused by the repeat extraction of enclosing subgraphs, and the deficiency of entity-independent feature learning. To address these issues, we propose a global-local anchor representation (GLAR) learning method for inductive KGC. Unlike previous methods that utilize enclosing subgraphs, we extract a shared opening subgraph for all candidates and perform reasoning on it, enabling the model to perform reasoning more efficiently. Moreover, we design some transferable global and local anchors to learn rich entity-independent features for emerging entities. Finally, a global-local graph reasoning model is applied on the opening subgraph to rank all candidates. Extensive experiments show that our GLAR outperforms most existing state-of-the-art methods.}
}


@article{DBLP:journals/tkde/LiuH24,
	author = {Zeyi Liu and
                  Xiao He},
	title = {Online Dynamic Hybrid Broad Learning System for Real-Time Safety Assessment
                  of Dynamic Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8928--8938},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3475028},
	doi = {10.1109/TKDE.2024.3475028},
	timestamp = {Mon, 27 Jan 2025 20:03:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time safety assessment of dynamic systems is of paramount importance in industrial processes since it provides continuous monitoring and evaluation to prevent potential harm to the environment and individuals. However, there are still several challenges to be resolved due to the requirements of time consumption and the non-stationary nature of real-world environments. In this paper, a novel online dynamic hybrid broad learning system, termed ODH-BLS, is proposed to more fully utilize the co-design advantages of active adaptation and passive adaptation. It makes effective use of limited annotations with the proposed sample value function. Simultaneously, anchor points can be dynamically adjusted to accommodate changes of the underlying distribution, thereby leveraging the value of unlabeled samples. An iterative update rule is also derived to ensure adaptation of the assessment model to real-time data at low computational costs. We also provide theoretical analyses to illustrate its practicality. Several experiments regarding the JiaoLong deep-sea manned submersible are carried out. The results demonstrate that the proposed ODH-BLS method achieves a performance improvement of approximately 8% over the baseline method on the benchmark dataset, showing its effectiveness in solving real-time safety assessment tasks for dynamic systems.}
}


@article{DBLP:journals/tkde/DaiWXYL24,
	author = {Xiangxiang Dai and
                  Zhiyong Wang and
                  Jize Xie and
                  Tong Yu and
                  John C. S. Lui},
	title = {Online Learning and Detecting Corrupted Users for Conversational Recommendation
                  Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8939--8953},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3448250},
	doi = {10.1109/TKDE.2024.3448250},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DaiWXYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conversational recommendation systems (CRSs) are increasingly prevalent, but they are susceptible to the influence of corrupted user behaviors, such as deceptive click ratings. These behaviors can skew the recommendation process, resulting in suboptimal results. Traditional bandit algorithms, which are typically oriented to single users, do not capitalize on implicit social connections between users, which could otherwise enhance learning efficiency. Furthermore, they cannot identify corrupted users in a real-time, multi-user environment. In this paper, we propose a novel bandit problem, Online Learning and Detecting Corrupted Users (OLDCU), to learn and utilize unknown user relations from disrupted behaviors to speed up learning and detect corrupted users in an online setting. This problem is non-trivial due to the dynamic nature of user behaviors and the difficulty of online detection. To robustly learn and leverage the unknown relations among potentially corrupted users, we propose a novel bandit algorithm RCLUB-WCU, incorporating a conversational mechanism. This algorithm is designed to handle the complexities of disrupted behaviors and to make accurate user relation inferences. To detect corrupted users with bandit feedback, we further devise a novel online detection algorithm, OCCUD, which is based on RCLUB-WCU’s inferred user relations and designed to adapt over time. We prove a sub-linear regret bound for RCLUB-WCU, demonstrating its efficiency. We also analyze the detection accuracy of OCCUD, showing its effectiveness in identifying corrupted users. Through extensive experiments, we validate the performance of our methods. Our results show that RCLUB-WCU and OCCUD outperform previous bandit algorithms and achieve high corrupted user detection accuracy, providing robust and efficient solutions in the field of CRSs.}
}


@article{DBLP:journals/tkde/HeJDXYC24,
	author = {Guoliang He and
                  Dawei Jin and
                  Lifang Dai and
                  Xin Xin and
                  Zhiwen Yu and
                  C. L. Philip Chen},
	title = {Online Learning of Temporal Association Rule on Dynamic Multivariate
                  Time Series Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8954--8966},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3438259},
	doi = {10.1109/TKDE.2024.3438259},
	timestamp = {Tue, 14 Jan 2025 21:23:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HeJDXYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, rule-based classification on multivariate time series (MTS) data has gained lots of attention, which could improve the interpretability of classification. However, state-of-the-art approaches suffer from three major issues. 1) few existing studies consider temporal relations among features in a rule, which could not adequately express the essential characteristics of MTS data. 2) due to the concept drift and time warping of MTS data, traditional methods could not mine essential characteristics of MTS data. 3) existing online learning algorithms could not effectively update shapelet-based temporal association rules of MTS data due to its temporal relationships among features of different variables. To handle these issues, we propose an online learning method for temporal association rule on dynamically collected MTS data (OTARL). First, a new type of rule named temporal association rule is defined and mined to represent temporal relationships among features in a rule. Second, an online learning mechanism with a probability correlation-based evaluation criterion is proposed to realize the online learning of temporal association rules on dynamically collected MTS data. Finally, an ensemble classification approach based on maximum-likelihood estimation is advanced to further enhance the classification performance. We conduct experiments on ten real-world datasets to verify the effectiveness and efficiency of our approach.}
}


@article{DBLP:journals/tkde/CaoYXWL24,
	author = {Xuemei Cao and
                  Xin Yang and
                  Shuyin Xia and
                  Guoyin Wang and
                  Tianrui Li},
	title = {Open Continual Feature Selection via Granular-Ball Knowledge Transfer},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8967--8980},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3428485},
	doi = {10.1109/TKDE.2024.3428485},
	timestamp = {Sun, 19 Jan 2025 13:53:51 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CaoYXWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a novel framework for continual feature selection (CFS) in data preprocessing, particularly in the context of an open and dynamic environment where unknown classes may emerge. CFS encounters two primary challenges: the discovery of unknown knowledge and the transfer of known knowledge. To this end, we propose a GBCFS method, which combines the strengths of continual learning (CL) with granular-ball computing (GBC). The GBCFS method focuses on constructing a granular-ball knowledge base to detect unknown classes and facilitate the transfer of previously learned knowledge for further feature selection. GBCFS consists of two stages: initial learning and open learning. The former aims to establish an initial knowledge base through multi-granularity representation using granular balls. The latter utilizes prior granular-ball knowledge to identify unknowns, updates the knowledge base for granular-ball knowledge transfer, reinforces old knowledge, and integrates new knowledge. Subsequently, we devise an optimal feature subset mechanism that incorporates minimal new features into the existing optimal subset, often yielding superior results during each period. Extensive experimental results on public benchmark datasets demonstrate our method's superiority in terms of both effectiveness and efficiency compared to state-of-the-art feature selection methods.}
}


@article{DBLP:journals/tkde/LiMGWLWW24,
	author = {Yan Li and
                  Chenyu Ma and
                  Rong Gao and
                  Youxi Wu and
                  Jinyan Li and
                  Wenjian Wang and
                  Xindong Wu},
	title = {OPF-Miner: Order-Preserving Pattern Mining With Forgetting Mechanism
                  for Time Series},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8981--8995},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3438274},
	doi = {10.1109/TKDE.2024.3438274},
	timestamp = {Fri, 29 Nov 2024 23:42:41 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiMGWLWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Order-preserving pattern (OPP) mining is a type of sequential pattern mining method in which a group of ranks of time series is used to represent an OPP. This approach can discover frequent trends in time series. Existing OPP mining algorithms consider data points at different time to be equally important; however, newer data usually have a more significant impact, while older data have a weaker impact. We therefore introduce the forgetting mechanism into OPP mining to reduce the importance of older data. This paper explores the mining of OPPs with forgetting mechanism (OPF) and proposes an algorithm called OPF-Miner that can discover frequent OPFs. OPF-Miner performs two tasks, candidate pattern generation and support calculation. In candidate pattern generation, OPF-Miner employs a maximal support priority strategy and a group pattern fusion strategy to avoid redundant pattern fusions. For support calculation, we propose an algorithm called support calculation with forgetting mechanism, which uses prefix and suffix pattern pruning strategies to avoid redundant support calculations. The experiments are conducted on nine datasets and 12 alternative algorithms. The results verify that OPF-Miner is superior to other competitive algorithms. More importantly, OPF-Miner yields good clustering performance for time series, since the forgetting mechanism is employed.}
}


@article{DBLP:journals/tkde/LiLFW24,
	author = {Zhenyu Li and
                  Xiuxing Li and
                  Sunqi Fan and
                  Jianyong Wang},
	title = {Optimization Techniques for Unsupervised Complex Table Reasoning via
                  Self-Training Framework},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {8996--9010},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3439405},
	doi = {10.1109/TKDE.2024.3439405},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiLFW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Structured tabular data is a fundamental data type in numerous fields, and the capacity to reason over tables is crucial for answering questions and validating hypotheses. However, constructing labeled data for complex reasoning tasks is labor-intensive, and the quantity of annotated data remains insufficient to support the intricate demands of real-world applications. To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a “Program-Management” component, and we bridge the gap between programs and text with a powerful “Program-Transformation” module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using “Table-Text Manipulator” to handle joint table-text reasoning scenarios. The entire framework utilizes self-training techniques to leverage the unlabeled training data, which results in significant performance improvements when tested on real-world data. Experimental results demonstrate that UCTR-ST achieves above 90% of the supervised model performance on different tasks and domains, reducing the dependence on manual annotation. Additionally, our approach can serve as a data augmentation technique, significantly boosting the performance of supervised models in low-resourced domains.}
}


@article{DBLP:journals/tkde/ChenJYLZH24,
	author = {Zi Chen and
                  Xinyu Ji and
                  Long Yuan and
                  Xuemin Lin and
                  Wenjie Zhang and
                  Shan Huang},
	title = {Parallel Contraction Hierarchies Construction on Road Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9011--9024},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3437243},
	doi = {10.1109/TKDE.2024.3437243},
	timestamp = {Mon, 03 Feb 2025 07:43:04 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenJYLZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shortest path query on road networks is a fundamental problem to support many location-based services and wide variant applications. Contraction Hierarchies(CH) is widely adopted to accelerate the shortest path query by leveraging shortcuts among vertices. However, the state-of-the-art CH construction method named \\mathsf{VCHCons} suffers from inefficiencies due to their strong reliance on pre-determined vertex order. This leads to the generation of a large number of invalid shortcuts and the limit of parallel processing capability. Motivated by it, in this paper, an innovative CH construction algorithm called \\mathsf{ECHCons} is devised following an edge-centric paradigm, which addresses the issue of invalid shortcut production by introducing a novel edge-ordering strategy. Furthermore, it optimizes shortcut calculation within a dynamically constructed optimal subgraph, which is significantly smaller than the original network, thus shrinking the traversal space during index construction. To further enhance efficiency and overcome the limitations in parallelism inherent to \\mathsf{VCHCons}, our approach leverages batch contraction of edges and introduces a well-defined lower bound technique to unlock more efficient parallel computation resources. Our approach provides both theoretical guarantee and practical advancement in CH construction. Extensive and comprehensive experiments are conducted on real road networks. The experimental results demonstrate the effectiveness and efficiency of our proposed approach.}
}


@article{DBLP:journals/tkde/GuoSWLZ24,
	author = {Pengcheng Guo and
                  Yonghong Song and
                  Boyu Wang and
                  Jiaohao Liu and
                  Qi Zhang},
	title = {{PLBR:} {A} Semi-Supervised Document Key Information Extraction via
                  Pseudo-Labeling Bias Rectification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9025--9036},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3443928},
	doi = {10.1109/TKDE.2024.3443928},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GuoSWLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Document key information extraction (DKIE) methods often require a large number of labeled samples, imposing substantial annotation costs in practical scenarios. Fortunately, pseudo-labeling based semi-supervised learning (PSSL) algorithms provide an effective paradigm to alleviate the reliance on labeled data by leveraging unlabeled data. However, the main challenges for PSSL in DKIE tasks: 1) context dependency of DKIE results in incorrect pseudo-labels. 2) high intra-class variance and low inter-class variation on DKIE. To this end, this paper proposes a similarity matrix Pseudo-Label Bias Rectification (PLBR) semi-supervised method for DKIE tasks, which improves the quality of pseudo-labels on DKIE benchmarks with rare labels. More specifically, the Similarity Matrix Bias Rectification (SMBR) module is proposed to improve the quality of pseudo-labels, which utilizes the contextual information of DKIE data through the analysis of similarity between labeled and unlabeled data. Moreover, a dual branch adaptive alignment (DBAA) mechanism is designed to adaptively align intra-class variance and alleviate inter-class variation on DKIE benchmarks, which is composed of two adaptive alignment ways. One is the intra-class alignment branch, which is designed to adaptively align intra-class variance. The other one is the inter-class alignment branch, which is developed to adaptively alleviate inter-class variance changes on the representation level. Extensive experiment results on two benchmarks demonstrate that PLBR achieves state-of-the-art performance and its performance surpasses the previous SOTA by 2.11\\% \\sim 2.53\\%\n, 2.09\\% \\sim 2.49\\%\nF1-score on FUNSD and CORD with rare labeled samples, respectively. Code will be open to the public.}
}


@article{DBLP:journals/tkde/LinWGHJL24,
	author = {Yan Lin and
                  Huaiyu Wan and
                  Shengnan Guo and
                  Jilin Hu and
                  Christian S. Jensen and
                  Youfang Lin},
	title = {Pre-Training General Trajectory Embeddings With Maximum Multi-View
                  Entropy Coding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9037--9050},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3347513},
	doi = {10.1109/TKDE.2023.3347513},
	timestamp = {Mon, 03 Mar 2025 22:25:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LinWGHJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatio-temporal trajectories provide valuable information about movement and travel behavior, enabling various downstream tasks that in turn power real-world applications. Learning trajectory embeddings can improve task performance but may incur high computational costs and face limited training data availability. Pre-training learns generic embeddings by means of specially constructed pretext tasks that enable learning from unlabeled data. Existing pre-training methods face (i) difficulties in learning general embeddings due to biases towards certain downstream tasks incurred by the pretext tasks, (ii) limitations in capturing both travel semantics and spatio-temporal correlations, and (iii) the complexity of long, irregularly sampled trajectories. To tackle these challenges, we propose Maximum Multi-view Trajectory Entropy Coding (MMTEC) for learning general and comprehensive trajectory embeddings. We introduce a pretext task that reduces biases in pre-trained trajectory embeddings, yielding embeddings that are useful for a wide variety of downstream tasks. We also propose an attention-based discrete encoder and a NeuralCDE-based continuous encoder that extract and represent travel behavior and continuous spatio-temporal correlations from trajectories in embeddings, respectively. Extensive experiments on two real-world datasets and three downstream tasks offer insight into the design properties of our proposal and indicate that it is capable of outperforming existing trajectory embedding methods.}
}


@article{DBLP:journals/tkde/ChenCJZDHYYLY24,
	author = {Qian Chen and
                  Yiqiang Chen and
                  Xinlong Jiang and
                  Teng Zhang and
                  Weiwei Dai and
                  Wuliang Huang and
                  Bingjie Yan and
                  Zhen Yan and
                  Wang Lu and
                  Bo Ye},
	title = {PrivFusion: Privacy-Preserving Model Fusion via Decentralized Federated
                  Graph Matching},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9051--9064},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3430819},
	doi = {10.1109/TKDE.2024.3430819},
	timestamp = {Tue, 11 Feb 2025 20:30:29 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenCJZDHYYLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Model fusion is becoming a crucial component in the context of model-as-a-service scenarios, enabling the delivery of high-quality model services to local users. However, this approach introduces privacy risks and imposes certain limitations on its applications. Ensuring secure model exchange and knowledge fusion among users becomes a significant challenge in this setting. To tackle this issue, we propose PrivFusion, a novel architecture that preserves privacy while facilitating model fusion under the constraints of local differential privacy. PrivFusion leverages a graph-based structure, enabling the fusion of models from multiple parties without additional training. By employing randomized mechanisms, PrivFusion ensures privacy guarantees throughout the fusion process. To enhance model privacy, our approach incorporates a hybrid local differentially private mechanism and decentralized federated graph matching, effectively protecting both activation values and weights. Additionally, we introduce a perturbation filter adapter to alleviate the impact of randomized noise, thereby recovering the utility of the fused model. Through extensive experiments conducted on diverse image datasets and real-world healthcare applications, we provide empirical evidence showcasing the effectiveness of PrivFusion in maintaining model performance while preserving privacy. Our contributions offer valuable insights and practical solutions for secure and collaborative data analysis within the domain of privacy-preserving model fusion.}
}


@article{DBLP:journals/tkde/GuoYLLLCW24,
	author = {Xianjie Guo and
                  Kui Yu and
                  Lin Liu and
                  Jiuyong Li and
                  Jiye Liang and
                  Fuyuan Cao and
                  Xindong Wu},
	title = {Progressive Skeleton Learning for Effective Local-to-Global Causal
                  Structure Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9065--9079},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3461832},
	doi = {10.1109/TKDE.2024.3461832},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GuoYLLLCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Causal structure learning (CSL) from observational data is a crucial objective in various machine learning applications. Recent advances in CSL have focused on local-to-global learning, which offers improved efficiency and accuracy. The local-to-global CSL algorithms first learn the local skeleton of each variable in a dataset, then construct the global skeleton by combining these local skeletons, and finally orient edges to infer causality. However, data quality issues such as noise and small samples often result in the presence of problematic asymmetric edges during global skeleton construction, hindering the creation of a high-quality global skeleton. To address this challenge, we propose a novel local-to-global CSL algorithm with a progressive enhancement strategy and make the following novel contributions: 1) To construct an accurate global skeleton, we design a novel strategy to iteratively correct asymmetric edges and progressively improve the accuracy of the global skeleton. 2) Based on the learned accurate global skeleton, we design an integrated global skeleton orientation strategy to infer the correct directions of edges for obtaining an accurate and reliable causal structure. Extensive experiments demonstrate that our method achieves better performance than the existing CSL methods.}
}


@article{DBLP:journals/tkde/WenF24,
	author = {Zhihao Wen and
                  Yuan Fang},
	title = {Prompt Tuning on Graph-Augmented Low-Resource Text Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9080--9095},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3440068},
	doi = {10.1109/TKDE.2024.3440068},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WenF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Text classification is a fundamental problem in information retrieval with many real-world applications, such as predicting the topics of online articles and the categories of e-commerce product descriptions. However, low-resource text classification, with no or few labeled samples, presents a serious concern for supervised learning. Meanwhile, many text data are inherently grounded on a network structure, such as a hyperlink/citation network for online articles, and a user-item purchase network for e-commerce products. These graph structures capture rich semantic relationships, which can potentially augment low-resource text classification. In this paper, we propose a novel model called Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource text classification in a two-pronged approach. During pre-training, we propose three graph interaction-based contrastive strategies to jointly pre-train a graph-text model; during downstream classification, we explore handcrafted discrete prompts and continuous prompt tuning for the jointly pre-trained model to achieve zero- and few-shot classification, respectively. Moreover, we explore the possibility of employing continuous prompt tuning for zero-shot inference. Specifically, we aim to generalize continuous prompts to unseen classes while leveraging a set of base classes. To this end, we extend G2P2 into G2P2\n∗\n, hinging on a new architecture of conditional prompt tuning. Extensive experiments on four real-world datasets demonstrate the strength of G2P2 in zero- and few-shot low-resource text classification tasks, and illustrate the advantage of G2P2\n∗\nin dealing with unseen classes.}
}


@article{DBLP:journals/tkde/LiuDLX24,
	author = {Xiao{-}Qian Liu and
                  Xue{-}Ying Ding and
                  Xin Luo and
                  Xin{-}Shun Xu},
	title = {ProtoUDA: Prototype-Based Unsupervised Adaptation for Cross-Domain
                  Text Recognition},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9096--9108},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3344761},
	doi = {10.1109/TKDE.2023.3344761},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuDLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Text recognition reads from real scene text or handwritten text, facilitating many real-world applications such as driverless cars, visual Q&A, and image-based machine translation. Although impressive results have been achieved in single-domain text recognition, it still suffers from great challenges in cross-domain due to the domain gaps among the synthetic text, the real scene text, and the handwritten text. Existing standard unsupervised domain adaptation (UDA) methods struggle to solve the text recognition task since they view a domain or a text image (containing a character sequence) as a whole, ignoring the subunits that make up the sequence. In the paper, we present a Prototyped-based Unsupervised Domain Adaptation method for text recognition (ProtoUDA), where the class prototypes are computed from the source domain, target domain, and the mixed (source-target) domain, respectively. Technically, ProtoUDA initially extracts pseudo-labeled character features under word-level supervised information. Further, based on these character features, we propose two parallel and complementary modules to perform class-level and instance-level alignment, which explicitly transfer the knowledge learned in the source domain to the target domain. Among them, class-level alignment is to close the distance between the similar source prototypes and target prototypes. The instance-level alignment is based on contrastive learning, making the character instances of the mixed domain close to the corresponding class mixed prototype while staying away from other class mixed prototypes. To our knowledge, we are the first to adopt contrastive learning in UDA-based text recognition tasks. Extensive experiments on several benchmark datasets show the superiority of our method over state-of-the-art methods.}
}


@article{DBLP:journals/tkde/LuoZWCPZLXY24,
	author = {Xuexiong Luo and
                  Sheng Zhang and
                  Jia Wu and
                  Hongyang Chen and
                  Hao Peng and
                  Chuan Zhou and
                  Zhao Li and
                  Shan Xue and
                  Jian Yang},
	title = {ReiPool: Reinforced Pooling Graph Neural Networks for Graph-Level
                  Representation Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9109--9122},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3466508},
	doi = {10.1109/TKDE.2024.3466508},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LuoZWCPZLXY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph pooling technique as the essential component of graph neural networks has gotten increasing attention recently and it aims to learn graph-level representations for the whole graph. Besides, graph pooling is important in graph classification and graph generation tasks. However, current graph pooling methods mainly coarsen a sequence of small-sized graphs to capture hierarchical structures, potentially resulting in the deterioration of the global structure of the original graph and influencing the quality of graph representations. Furthermore, these methods artificially select the number of graph pooling layers for different graph datasets rather than considering each graph individually. In reality, the structure and size differences among graphs necessitate a specific number of graph pooling layers for each graph. In this work, we propose reinforced pooling graph neural networks via adaptive hybrid graph coarsening networks. Specifically, we design a hybrid graph coarsening strategy to coarsen redundant structures of the original graph while retaining the global structure. In addition, we introduce multi-agent reinforcement learning to adaptively perform the graph coarsening process to extract the most representative coarsened graph for each graph, enhancing the quality of graph-level representations. Finally, we design graph-level contrast to improve the preservation of global information in graph-level representations. Extensive experiments with rich baselines on six benchmark datasets show the effectiveness of ReiPool1.}
}


@article{DBLP:journals/tkde/LiuGHZZY24,
	author = {Yunhui Liu and
                  Xinyi Gao and
                  Tieke He and
                  Tao Zheng and
                  Jianhua Zhao and
                  Hongzhi Yin},
	title = {Reliable Node Similarity Matrix Guided Contrastive Graph Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9123--9135},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3435887},
	doi = {10.1109/TKDE.2024.3435887},
	timestamp = {Tue, 11 Feb 2025 20:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuGHZZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph clustering, which involves the partitioning of nodes within a graph into disjoint clusters, holds significant importance for numerous subsequent applications. Recently, contrastive learning, known for utilizing supervisory information, has demonstrated encouraging results in deep graph clustering. This methodology facilitates the learning of favorable node representations for clustering by attracting positively correlated node pairs and distancing negatively correlated pairs within the representation space. Nevertheless, a significant limitation of existing methods is their inadequacy in thoroughly exploring node-wise similarity. For instance, some hypothesize that the node similarity matrix within the representation space is identical, ignoring the inherent semantic relationships among nodes. Given the fundamental role of instance similarity in clustering, our research investigates contrastive graph clustering from the perspective of the node similarity matrix. We argue that an ideal node similarity matrix within the representation space should accurately reflect the inherent semantic relationships among nodes, ensuring the preservation of semantic similarities in the learned representations. In response to this, we introduce a new framework, Reliable Node Similarity Matrix Guided Contrastive Graph Clustering (NS4GC), which estimates an approximately ideal node similarity matrix within the representation space to guide representation learning. Our method introduces node-neighbor alignment and semantic-aware sparsification, ensuring the node similarity matrix is both accurate and efficiently sparse. Comprehensive experiments conducted on 8 real-world datasets affirm the efficacy of learning the node similarity matrix and the superior performance of NS4GC.}
}


@article{DBLP:journals/tkde/ZhangXCCZXY24,
	author = {Xiao Zhang and
                  Shuqing Xu and
                  Huashan Chen and
                  Zekai Chen and
                  Fuzhen Zhuang and
                  Hui Xiong and
                  Dongxiao Yu},
	title = {Rethinking Robust Multivariate Time Series Anomaly Detection: {A}
                  Hierarchical Spatio-Temporal Variational Perspective},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9136--9149},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3466291},
	doi = {10.1109/TKDE.2024.3466291},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangXCCZXY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The robust multivariate time series anomaly detection can facilitate intelligent decisions and timely maintenance in various kinds of monitor systems. However, the robustness is highly restricted by the stochasticity in multivariate time series, which is summarized as temporal stochasticity and spatial stochasticity specifically. In this paper, we explicitly model the temporal stochasticity variables and the latent graph relationship variables into a unified graphical framework, which can achieve better robustness to dynamicity from both the spatial and temporal perspective. First, within the spatial encoder, every connection exists or not is modeled as a binary stochastic variable, and the graph structure can be learnt automatically. Then, the temporal encoder would embed the highly structured time series into latent stochastic variables to capture both complex temporal dependencies and neighbors information. Moreover, we design a history-future combined anomaly score mechanism with both reconstruction decoder and forecasting decoder to improve the anomaly detection performance. By weighting the historical anomaly factor, the future anomaly factor, and the prediction error of current timestamp, the anomaly detection at current timestamp could be more sensitive to anomaly detection. Finally, extensive experiments on three publicly available anomaly detection datasets demonstrate our proposed method can achieve the best performance in terms of recall and F1 compared with state-of-the-arts baselines.}
}


@article{DBLP:journals/tkde/SunQLPPH24,
	author = {Yuan Sun and
                  Yang Qin and
                  Yongxiang Li and
                  Dezhong Peng and
                  Xi Peng and
                  Peng Hu},
	title = {Robust Multi-View Clustering With Noisy Correspondence},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9150--9162},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3423307},
	doi = {10.1109/TKDE.2024.3423307},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunQLPPH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep multi-view clustering leverages deep neural networks to achieve promising performance, but almost all existing methods implicitly assume that all views are aligned correctly. This assumption is unrealistic in many real-world scenarios, where noise, occlusion, or sensor differences can inevitably cause misaligned data. Based on this observation, we reveal and study a practical but understudied problem in multi-view clustering (MVC), i.e., noisy correspondence (NC). Considering this problem, we argue that the main challenge is to prevent the model from overfiting NC. To this end, we propose a novel Robust Multi-view Clustering with Noisy Correspondence (RMCNC) method, which alleviates the influence of the misaligned pairs from multi-view data. To be specific, we first compute a united probability with all positive pairs to learn cross-view alignment consistency, thereby alleviating the adverse impact of the individual false positives. To further mitigate the overfitting problem, we propose a noise-tolerance multi-view contrastive loss that avoids overemphasizing noisy data. Moreover, RMCNC is a unified framework, which can deal with both partially view-aligned and NC problems in multi-view clustering. To the best of our knowledge, it could be the first study on NC in multi-view clustering. The experimental results on eight benchmark datasets indicate our RMCNC achieves competitive performance and robustness.}
}


@article{DBLP:journals/tkde/KimHCK24,
	author = {Seonghak Kim and
                  Gyeongdo Ham and
                  Yucheol Cho and
                  Daeshik Kim},
	title = {Robustness-Reinforced Knowledge Distillation With Correlation Distance
                  and Network Pruning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9163--9175},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3438074},
	doi = {10.1109/TKDE.2024.3438074},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/KimHCK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The improvement in the performance of efficient and lightweight models (i.e., the student model) is achieved through knowledge distillation (KD), which involves transferring knowledge from more complex models (i.e., the teacher model). However, most existing KD techniques rely on Kullback-Leibler (KL) divergence, which has certain limitations. First, if the teacher distribution has high entropy, the KL divergence's mode-averaging nature hinders the transfer of sufficient target information. Second, when the teacher distribution has low entropy, the KL divergence tends to excessively focus on specific modes, which fails to convey an abundant amount of valuable knowledge to the student. Consequently, when dealing with datasets that contain numerous confounding or challenging samples, student models may struggle to acquire sufficient knowledge, resulting in subpar performance. Furthermore, in previous KD approaches, we observed that data augmentation, a technique aimed at enhancing a model's generalization, can have an adverse impact. Therefore, we propose a Robustness-Reinforced Knowledge Distillation (R2KD) that leverages correlation distance and network pruning. This approach enables KD to effectively incorporate data augmentation for performance improvement. Extensive experiments on various datasets, including CIFAR-100, FGVR, TinyImagenet, and ImageNet, demonstrate our method's superiority over current state-of-the-art methods.}
}


@article{DBLP:journals/tkde/LinGN24,
	author = {Qi Lin and
                  Binbin Gu and
                  Faisal Nawab},
	title = {RollStore: Hybrid Onchain-Offchain Data Indexing for Blockchain Applications},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9176--9191},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3436514},
	doi = {10.1109/TKDE.2024.3436514},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LinGN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The interest in building blockchain Decentralized Applications (DApps) has been growing over the past few years. DApps are implemented as smart contracts which are programs that are maintained by a blockchain network. Building DApps, however, faces many challenges—most notably the performance and monetary overhead of writing to blockchain smart contracts. To overcome this challenge, many DApp developers have explored utilizing off-chain resources—nodes outside of the blockchain network—to offload part of the processing and storage. In this paper, we propose RollStore, a data indexing solution for hybrid onchain-offchain DApps. RollStore provides efficiency in terms of reduced cost and latency, as well as security in terms of tolerating Byzantine (i.e., malicious) off-chain nodes. RollStore achieves this by: (1) a three-stage commitment strategy where each stage represents a point in a performance-security trade-off—i.e., the first stage is fast but less secure while the last stage is slower but more secure. (2) utilizing zero-knowledge (zk) proofs to enable the on-chain smart contract to verify off-chain operations with a small cost. (3) Combining Log-Structured Merge (LSM) trees and Merkle Mountain Range (MMR) trees to efficiently enable both access and verification of indexed data. We experimentally evaluate the cost and performance benefits of RollStore while comparing with BlockchainDB and BigChainDB.}
}


@article{DBLP:journals/tkde/WangZLWZHJW24,
	author = {Ziming Wang and
                  Kai Zhang and
                  Yangming Lv and
                  Yinglong Wang and
                  Zhigang Zhao and
                  Zhenying He and
                  Yinan Jing and
                  X. Sean Wang},
	title = {{RTOD:} Efficient Outlier Detection With Ray Tracing Cores},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9192--9204},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3453901},
	doi = {10.1109/TKDE.2024.3453901},
	timestamp = {Mon, 23 Dec 2024 08:37:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangZLWZHJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Outlier detection in data streams is a critical component in numerous applications, such as network intrusion detection, financial fraud detection, and public health. To detect abnormal behaviors in real-time, these applications generally have stringent requirements for the performance of outlier detection. This paper proposes RTOD, a high-performance outlier detection approach that utilizes RT cores in modern GPUs for acceleration. RTOD transforms distance-based outlier detection in data streams into an efficient ray tracing job. By creating spheres centered at points within a window and casting rays from each point, RTOD identifies the outlier points according to the number of intersections between rays and spheres. Besides, we propose two optimization techniques, namely Grid Filtering and Ray-BVH Inversion, to further accelerate the detection efficiency of RT cores. Experimental results show that RTOD achieves up to 9.9× speedups over existing start-of-the-art outlier detection algorithms.}
}


@article{DBLP:journals/tkde/ParkLLK24,
	author = {Jeongha Park and
                  Bo{-}Young Lim and
                  Kisung Lee and
                  Hyuk{-}Yoon Kwon},
	title = {SaaN 2L-GRL: Two-Level Graph Representation Learning Empowered With
                  Subgraph-as-a-Node},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9205--9219},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3421933},
	doi = {10.1109/TKDE.2024.3421933},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ParkLLK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this study, we propose a novel graph representation learning (GRL) model, called Two-Level GRL with Subgraph-as-a-Node (SaaN 2L-GRL in short), that partitions input graphs into smaller subgraphs for effective and scalable GRL in two levels: 1) local GRL and 2) global GRL. To realize the two-level GRL in an efficient manner, we propose an abstracted graph, called Subgraph-as-a-Node Graph (SaaN in short), to effectively maintain the high-level graph topology while significantly reducing the size of the graph. By applying the SaaN graph to both local and global GRL, SaaN 2L-GRL can effectively preserve the overall structure of the entire graph while precisely representing the nodes within each subgraph. Through time complexity analysis, we confirm that SaaN 2L-GRL significantly reduces the learning time of existing GRL models by using the SaaN graph for global GRL, instead of using the original graph, and processing local GRL on subgraphs in parallel. Our extensive experiments show that SaaN 2L-GRL outperforms existing GRL models in both accuracy and efficiency. In addition, we show the effectiveness of SaaN 2L-GRL using diverse kinds of graph partitioning methods, including five community detection algorithms and representative edge- and vertex-cut algorithms.}
}


@article{DBLP:journals/tkde/HuangLXYLXL24,
	author = {Qing Huang and
                  Dianshu Liao and
                  Zhenchang Xing and
                  Zhiqiang Yuan and
                  Qinghua Lu and
                  Xiwei Xu and
                  Jiaxing Lu},
	title = {{SE} Factual Knowledge in Frozen Giant Code Model: {A} Study on {FQN}
                  and Its Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9220--9234},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3436883},
	doi = {10.1109/TKDE.2024.3436883},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuangLXYLXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Giant pre-trained code models (PCMs) start coming into the developers’ daily practices. Understanding the type and amount of software knowledge in PCMs is essential for integrating PCMs into software engineering (SE) tasks and unlocking their potential. In this work, we conduct the first systematic study on the SE factual knowledge in the state-of-the-art PCM CoPilot, focusing on APIs’ Fully Qualified Names (FQNs), the fundamental knowledge for effective code analysis, search and reuse. Driven by FQNs’ data distribution properties, we design a novel lightweight in-context learning on Copilot for FQN inference, which does not require code compilation as traditional methods or gradient update by recent FQN prompt-tuning. We systematically experiment with five in-context learning design factors to identify the best configuration for practical use. With this best configuration, we investigate the impact of example prompts and FQN data properties on CoPilot's FQN inference capability. Our results confirm that CoPilot stores diverse FQN knowledge and can be applied for FQN inference due to its high accuracy and non-reliance on code analysis. Additionally, our extended study shows that the in-context learning method can be generalized to retrieve other SE factual knowledge embedded in giant PCMs. Furthermore, we find that the advanced general model GPT-4 also stores substantial SE knowledge. Comparing FQN inference between CoPilot and GPT-4, we observe that as model capabilities improve, the same prompts yield better results. Based on our experience interacting with Copilot, we discuss various opportunities to improve human-CoPilot interaction in the FQN inference task.}
}


@article{DBLP:journals/tkde/ChaiXLZZ24,
	author = {Shuwen Chai and
                  Yutang Xiao and
                  Feng Liu and
                  Jian Zhu and
                  Yuan Zhou},
	title = {Securing Multi-Source Domain Adaptation With Global and Domain-Wise
                  Privacy Demands},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9235--9248},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3459890},
	doi = {10.1109/TKDE.2024.3459890},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChaiXLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Making available a large size of training data for deep learning models and preserving data privacy are two ever-growing concerns in the machine learning community. Multi-source domain adaptation (MDA) leverages the data information from different domains and aggregates them to improve the performance in the target task, while the privacy leakage risk of publishing models under malicious attacker for membership or attribute inference is even more complicated than the one faced by single-source domain adaptation. In this paper, we tackle the problem of effectively protecting data privacy while training and aggregating multi-source information, where each source domain enjoys an independent privacy budget. Specifically, we develop a differentially private MDA (DPMDA) algorithm to provide domain-wise privacy protection with adaptive weighting scheme based on task similarity and task-specific privacy budget. We evaluate our algorithm on three benchmark tasks and show that DPMDA can effectively leverage different private budgets from source domains and consistently outperforms the existing private baselines with a reasonable gap with non-private state-of-the-art.}
}


@article{DBLP:journals/tkde/XieXXZLWHT24,
	author = {Guangqiang Xie and
                  Haoran Xu and
                  Jiyuan Xu and
                  Shupeng Zhao and
                  Yang Li and
                  Chang{-}Dong Wang and
                  Xianbiao Hu and
                  Yonghong Tian},
	title = {Sequential Trajectory Data Publishing With Adaptive Grid-Based Weighted
                  Differential Privacy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9249--9262},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3449433},
	doi = {10.1109/TKDE.2024.3449433},
	timestamp = {Mon, 03 Mar 2025 22:25:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XieXXZLWHT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of wireless communication and localization technologies, the easier collection of trajectory data can bring potential data-driven value. Recently, there has been an increasing interest in how to publish trajectory dataset without revealing personal information. However, since the large-scale and real-world sequential trajectory dataset presents a heterogeneous regional distribution, the existing study ignores the relationship between privacy budget allocation and spatial characteristics, resulting in unreasonable continuity and mapping distortion, and thus lowering the utility of the synthetic dataset. To address this problem, we propose a probability distribution model named Adaptive grid-based Weighted Differential Privacy (AWDP). First, trajectories are adaptively discretized into the multi-resolution grid structures to make trajectories more uniformly distributed and less disturbed by the noise. Second, we allocate different weighted budgets for different grids according to density-based regional characteristics. Third, a spatio-temporal continuity maintenance method is designed to solve unrealistic direction- and density-based continuity deviations of synthetic trajectories. An application system is developed for demonstration purposes which is available online at http://qgailab.com/awdp/. The extensive experiments on three datasets demonstrate that AWDP performs significantly better than the state-of-the-art model in preserving the density distribution of the original trajectories with differential privacy guarantee and high utility.}
}


@article{DBLP:journals/tkde/SunZLXPR24,
	author = {Qiheng Sun and
                  Jiayao Zhang and
                  Jinfei Liu and
                  Li Xiong and
                  Jian Pei and
                  Kui Ren},
	title = {Shapley Value Approximation Based on Complementary Contribution},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9263--9281},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3438213},
	doi = {10.1109/TKDE.2024.3438213},
	timestamp = {Fri, 28 Feb 2025 17:23:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunZLXPR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shapley value provides a unique way to fairly assess each player's contribution in a coalition and has enjoyed many applications. However, the exact computation of Shapley value is #P-hard due to the combinatoric nature of Shapley value. Many existing applications of Shapley value are based on Monte-Carlo approximation, which requires a large number of samples and the assessment of utility on many coalitions to reach high-quality approximation, and thus is still far from being efficient. Can we achieve an efficient approximation of Shapley value by smartly obtaining samples? In this paper, we treat the sampling approach to Shapley value approximation as a stratified sampling problem. Our main technical contributions are a novel stratification design and a sampling method based on Neyman allocation. Moreover, computing the Shapley value in a dynamic setting, where new players may join the game and others may leave it poses an additional challenge due to the considerable cost of recomputing from scratch. To tackle this issue, we propose to capture changes in Shapley value, making our approaches applicable to scenarios with dynamic players. Experimental results on several real data sets and synthetic data sets demonstrate the effectiveness and efficiency of our approaches.}
}


@article{DBLP:journals/tkde/WangQWXBF24,
	author = {Yizhou Wang and
                  Can Qin and
                  Rongzhe Wei and
                  Yi Xu and
                  Yue Bai and
                  Yun Fu},
	title = {SLA{\textdollar}{\^{}}\{\{{\textbackslash}text\{2\}\}\}{\textdollar}2P:
                  Self-Supervised Anomaly Detection With Adversarial Perturbation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9282--9293},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3448473},
	doi = {10.1109/TKDE.2024.3448473},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangQWXBF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection is a foundational yet difficult problem in machine learning. In this work, we propose a new and effective framework, dubbed as SLA2P, for unsupervised anomaly detection. Following the extraction of delegate embeddings from raw data, we implement random projections on the features and consider features transformed by disparate projections as being associated with separate pseudo-classes. We then train a neural network for classification on these transformed features to conduct self-supervised learning. Subsequently, we introduce adversarial disturbances to the modified attributes, and we develop anomaly scores built on the classifier's predictive uncertainties concerning these disrupted features. Our approach is motivated by the fact that as anomalies are relatively rare and decentralized, 1) the training of the pseudo-label classifier concentrates more on acquiring the semantic knowledge of regular data instead of anomalous data; 2) the altered attributes of the normal data exhibit greater resilience to disturbances compared to those of the anomalous data. Therefore, the disrupted modified attributes of anomalies can not be well classified and correspondingly tend to attain lesser anomaly scores. The results of experiments on various benchmark datasets for images, text, and inherently tabular data demonstrate that SLA2P achieves state-of-the-art performance consistently.}
}


@article{DBLP:journals/tkde/YangYCLC24,
	author = {Kaixiang Yang and
                  Zhiwen Yu and
                  Wuxing Chen and
                  Zefeng Liang and
                  C. L. Philip Chen},
	title = {Solving the Imbalanced Problem by Metric Learning and Oversampling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9294--9307},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3419834},
	doi = {10.1109/TKDE.2024.3419834},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangYCLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Imbalanced data poses a substantial challenge to conventional classification methods, which often disproportionately favor samples from the majority class. To mitigate this issue, various oversampling techniques have been deployed, but opportunities for optimizing data distributions remain underexplored. By exploiting the ability of metric learning to refine the sample distribution, we propose a novel approach, Imbalance Large Margin Nearest Neighbor (ILMNN). Initially, ILMNN is applied to establish a latent feature space, pulling intra-class samples closer and distancing inter-class samples, thereby amplifying the efficacy of oversampling techniques. Subsequently, we allocate varying weights to samples contingent upon their local distribution and relative class frequency, thereby equalizing contributions from minority and majority class samples. Lastly, we employ Kullback-Leibler (KL) divergence as a safeguard to maintain distributional similarity to the original dataset, mitigating severe intra-class imbalances. Comparative experiments on various class-imbalanced datasets verify that our ILMNN approach yields superior results.}
}


@article{DBLP:journals/tkde/GongWGLLZWZL24,
	author = {Letian Gong and
                  Huaiyu Wan and
                  Shengnan Guo and
                  Xiucheng Li and
                  Yan Lin and
                  Erwen Zheng and
                  Tianyi Wang and
                  Zeyu Zhou and
                  Youfang Lin},
	title = {Spatial-Temporal Cross-View Contrastive Pre-Training for Check-in
                  Sequence Representation Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9308--9321},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3434565},
	doi = {10.1109/TKDE.2024.3434565},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GongWGLLZWZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid growth of location-based services (LBS) has yielded massive amounts of data on human mobility. Effectively extracting meaningful representations for user-generated check-in sequences is pivotal for facilitating various downstream services. However, the user-generated check-in data are simultaneously influenced by the surrounding objective circumstances and the user's subjective intention. Specifically, the temporal uncertainty and spatial diversity exhibited in check-in data make it difficult to capture the macroscopic spatial-temporal patterns of users and to understand the semantics of user mobility activities. Furthermore, the distinct characteristics of the temporal and spatial information in check-in sequences call for an effective fusion method to incorporate these two types of information. In this paper, we propose a novel Spatial-Temporal Cross-view Contrastive Representation (STCCR) framework for check-in sequence representation learning. Specifically, STCCR addresses the above challenges by employing self-supervision from “spatial topic” and “temporal intention” views, facilitating effective fusion of spatial and temporal information at the semantic level. Besides, STCCR leverages contrastive clustering to uncover users’ shared spatial topics from diverse mobility activities, while employing angular momentum contrast to mitigate the impact of temporal uncertainty and noise. We extensively evaluate STCCR on three real-world datasets and demonstrate its superior performance across three downstream tasks.}
}


@article{DBLP:journals/tkde/QiLHFYQ24,
	author = {Jiaxing Qi and
                  Zhongzhi Luan and
                  Shaohan Huang and
                  Carol J. Fung and
                  Hailong Yang and
                  Depei Qian},
	title = {SpikeLog: Log-Based Anomaly Detection via Potential-Assisted Spiking
                  Neuron Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9322--9335},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3347695},
	doi = {10.1109/TKDE.2023.3347695},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/QiLHFYQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing volume and complexity of log data generated by modern systems have made it challenging to analyze and extract useful insights manually. To address this problem, many machine learning methods have been proposed for log-based anomaly detection. However, most of these methods lack interpretability, and their underlying premises do not always reflect real scenarios. In this paper, we consider a more reasonable premise scenario where a large number of logs are unlabeled, while only a small number of anomalous logs are labeled. Moreover, a small proportion of anomaly contamination may be present. To handle this practical scenario, we propose a novel hybrid potential-assisted framework (SpikeLog) using the membrane potential of spiking neurons. SpikeLog adopts a weakly supervised approach to train an anomaly score model, which effectively utilizes a limited number of labeled anomalies alongside abundant unlabeled logs while ensuring computational efficiency without compromising accuracy. Extensive experiments have demonstrated that SpikeLog outperforms baseline methods in terms of performance, robustness, interpretability, and energy consumption.}
}


@article{DBLP:journals/tkde/JooK24,
	author = {Yongwan Joo and
                  Soon Seok Kim},
	title = {{SUHDSA:} Secure, Useful, and High-Performance Data Stream Anonymization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9336--9347},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3476684},
	doi = {10.1109/TKDE.2024.3476684},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/JooK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study addresses privacy concerns in real-time streaming data, including personal biometric signals and private information from sources such as real-time crime reporting, online sales transactions, and hospital patient-monitoring devices. Anonymization is crucial because it hides sensitive personal data. Achieving anonymity in real-time streaming data involves satisfying the unique demands of real-time scenarios, which is distinct from traditional methods. Specifically, security and minimal information loss must be maintained within a specified timeframe (referred to as the average delay time). The most recent solution in this context is the utility-based approach to data stream anonymization (UBDSA) algorithm developed by Sopaoglu and Abul. This study aims to enhance the performance of UBDSA by introducing a secure, useful, and high-performance data stream anonymization (SUHDSA) algorithm. SUHDSA outperforms UBDSA in terms of runtime and information loss while still ensuring privacy protection and an average delay time. The experimental results, using the same dataset and cluster size as in a previous UBDSA study, demonstrate significant performance improvements with the proposed algorithm. It achieves a minimum runtime of 24.05 s and a maximum runtime of 29.88 s, with information loss rates ranging from 14% to 77%. These results surpass the performance of the previous UBDSA algorithm.}
}


@article{DBLP:journals/tkde/YuanYWLL24,
	author = {Aihong Yuan and
                  Mengbo You and
                  Yuhan Wang and
                  Xun Li and
                  Xuelong Li},
	title = {Symmetrical Self-Representation and Data-Grouping Strategy for Unsupervised
                  Feature Selection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9348--9360},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3437364},
	doi = {10.1109/TKDE.2024.3437364},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YuanYWLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsupervised feature selection (UFS) is an important technology for dimensionality reduction and has gained great interest in a wide range of fields. Recently, most popular methods are spectral-based which frequently use adaptive graph constraints to promote performance. However, no literature has considered the grouping characteristic of the data features, which is the most basic and important characteristic for arbitrary data. In this paper, based on the spectral analysis method, we first simulate the data feature grouping characteristic. Then, the similarity between data is adaptively reconstructed through the similarity between groups, which can explore the more fine-grained relationship between data than the previous adaptive graph methods. In order to achieve the aforementioned goal, the local similarity matrix and the global similarity matrix are defined, and the weighted KL entropy is used to constrain the relationship between the global similarity matrix and the local similarity matrices. Furthermore, the symmetrical self-representation structure is used to improve the performance of the reconstruction error term in the conventional spectral-based methods. After the model is constructed, a simple but efficient algorithm is proposed to solve the full model. Extensive experiments on 8 benchmark dataset with different types to show the effectiveness of the proposed method.}
}


@article{DBLP:journals/tkde/NiCCY24,
	author = {Wangze Ni and
                  Peng Cheng and
                  Lei Chen and
                  Shiyu Yang},
	title = {Task Assignment Framework for Online Car-Hailing Systems With Electric
                  Vehicles},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9361--9373},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3434567},
	doi = {10.1109/TKDE.2024.3434567},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NiCCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, transportation-as-a-service (TaaS) becomes an increasing trend, and online taxi platforms start to apply electric vehicles to serve passengers. Since the recharging time of an electric vehicle is long and non-negligible, it is necessary to smartly arrange the recharging schedules of electric vehicles in working schedules. In order to maximize the number of served taxi-calling tasks, online taxi platforms assign electric vehicles whose remaining electric power is enough to serve the dynamically arriving taxi-calling tasks and schedule suitable idle vehicles to recharging piles to recharge. We formally define the power-aware electric vehicle assignment (PAEVA) problem to serve as many taxi-calling tasks as possible under the constraints of remaining electric power and deadline. We prove that the PAEVA problem is NP-hard. To solve PAEVA, we design a novel strategy to help arrange the schedules of electric vehicles. Specifically, the strategy requires that, in a time slot and an area gird, the ratio of the number of electric vehicles whose remaining electric power is higher than a threshold \\alpha to the number of predicted taxi-calling tasks should be higher than a threshold \\beta. We propose two approximation approaches with theoretical guarantees to adaptively determine the values of the two thresholds of the strategy. We evaluate our solutions’ effectiveness and efficiency by comprehensive experiments on real datasets.}
}


@article{DBLP:journals/tkde/ChenLWLHY24,
	author = {Tingxuan Chen and
                  Jun Long and
                  Zidong Wang and
                  Shuai Luo and
                  Jincai Huang and
                  Liu Yang},
	title = {{THCN:} {A} Hawkes Process Based Temporal Causal Convolutional Network
                  for Extrapolation Reasoning in Temporal Knowledge Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9374--9387},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3474051},
	doi = {10.1109/TKDE.2024.3474051},
	timestamp = {Mon, 03 Mar 2025 22:25:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenLWLHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal Knowledge Graphs (TKGs) serve as indispensable tools for dynamic facts storage and reasoning. However, predicting future facts in TKGs presents a formidable challenge due to the unknowable nature of future facts. Existing temporal reasoning models depend on fact recurrence and periodicity, leading to information degradation over prolonged temporal evolution. In particular, the occurrence of one fact may influence the likelihood of another. To this end, we propose THCN, a novel Temporal Causal Convolutional Network based on Hawkes processes, designed for temporal reasoning under the extrapolation setting. Specifically, THCN harnesses a temporal causal convolutional network with dilated factors to capture historical dependencies among facts spanning diverse time intervals. Then, we construct a conditional intensity function based on Hawkes processes for fitting the likelihood of fact occurrence. Importantly, THCN pioneers a dual-level dynamic modeling mechanism, enabling the simultaneous capture of the collective features of nodes and the individual characteristics of facts. Extensive experiments on six real-world TKG datasets demonstrate our method significantly outperforms the state-of-the-art across all four evaluation metrics, indicating that THCN is more applicable for extrapolation reasoning in TKGs.}
}


@article{DBLP:journals/tkde/WangZLQFT24,
	author = {Ziyu Wang and
                  Yinghai Zhou and
                  Hao Liu and
                  Jing Qiu and
                  Binxing Fang and
                  Zhihong Tian},
	title = {ThreatInsight: Innovating Early Threat Detection Through Threat-Intelligence-Driven
                  Analysis and Attribution},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9388--9402},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3474792},
	doi = {10.1109/TKDE.2024.3474792},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangZLQFT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The complexity and ongoing evolution of Advanced Persistent Threats (APTs) compromise the efficacy of conventional cybersecurity measures. Firewalls, intrusion detection systems, and antivirus software, which are dependent on static rules and predefined signatures, are increasingly ineffective against these sophisticated threats. Moreover, the use of system audit logs for threat hunting involves a retrospective review of cybersecurity incidents to reconstruct attack paths for attribution, which affects the timeliness and effectiveness of threat detection and response. Even when the attacker is identified, this method does not prevent cyber attacks. To address these challenges, we introduce ThreatInsight, a novel early-stage threat detection solution that minimizes reliance on system audit logs. ThreatInsight detects potential threats by analyzing IPs captured from HoneyPoints. These IPs are processed through threat data mining and threat feature modeling. By employing fact-based and semantic reasoning techniques based on the APT Threat Intelligence Knowledge Graph (APT-TI-KG), ThreatInsight identifies and attributes attackers. The system generates analysis reports detailing the threat knowledge concerning IPs and attributed attackers, equipping analysts with actionable insights and defense strategies. The system architecture includes modules for HoneyPoint IP extraction, Threat Intelligence (TI) data analysis, attacker attribution, and analysis report generation. ThreatInsight facilitates real-time analysis and the identification of potential threats at early stages, thereby enhancing the early detection capabilities of cybersecurity defense systems and improving overall threat detection and proactive defense effectiveness.}
}


@article{DBLP:journals/tkde/YiBHWPZ24,
	author = {Peiyu Yi and
                  Zhifeng Bao and
                  Feihu Huang and
                  Jince Wang and
                  Jian Peng and
                  Linghao Zhang},
	title = {Towards Effective Long-Term Wind Power Forecasting: {A} Deep Conditional
                  Generative Spatio-Temporal Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9403--9417},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3435859},
	doi = {10.1109/TKDE.2024.3435859},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YiBHWPZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurately forecasting long-term future wind power is critical to achieve safe power grid integration. This problem is quite challenging due to wind power's high volatility and randomness. In this paper, we propose a novel time series forecasting method, namely Deep Conditional Generative Spatio-Temporal model (DCGST), and its high accuracy is achieved by tackling two critical issues simultaneously: a proper handling of the non-stationarity of multiple wind power time series, and a fine-grained modeling of their complicated yet dynamic spatio-temporal dependencies. Specifically, we first formally define the Spatio-Temporal Concept Drift (STCD) problem of wind power, and then we propose a novel deep conditional generative model to learn probabilistic distributions of future wind power values under STCD. Three different tailored neural networks are designed for distributions parameterization, including a graph-based prior network, an attention-based recognition network, and a stochastic seq2seq-based generation network. They are able to encode the dynamic spatio-temporal dependencies of multiple wind power time series and infer one-to-many mappings for future wind power generation. Compared to existing methods, DCGST can learn better spatio-temporal representations of wind power data and learn better uncertainties of data distribution to generate future values. Comprehensive experiments on real-world datasets including the largest public turbine-level wind power dataset verify the effectiveness, efficiency, generality and scalability of our method.}
}


@article{DBLP:journals/tkde/ChenFZMHK24,
	author = {Yankai Chen and
                  Yixiang Fang and
                  Yifei Zhang and
                  Chenhao Ma and
                  Yang Hong and
                  Irwin King},
	title = {Towards Effective Top-N Hamming Search via Bipartite Graph Contrastive
                  Hashing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9418--9432},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3425891},
	doi = {10.1109/TKDE.2024.3425891},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenFZMHK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Searching on bipartite graphs serves as a fundamental task for various real-world applications, such as recommendation systems, database retrieval, and document querying. Conventional approaches rely on similarity matching in continuous euclidean space of vectorized node embeddings. To handle intensive similarity computation efficiently, hashing techniques for graph-structured data have emerged as a prominent research direction. However, despite the retrieval efficiency in Hamming space, previous studies have encountered catastrophic performance decay. To address this challenge, we investigate the problem of hashing with Graph Convolutional Network for effective Top-N search. Our findings indicate the learning effectiveness of incorporating hashing techniques within the exploration of bipartite graph reception fields, as opposed to simply treating hashing as post-processing to output embeddings. To further enhance the model performance, we advance upon these findings and propose Bipartite Graph Contrastive Hashing (BGCH+). BGCH+ introduces a novel dual augmentation approach to both intermediate information and hash code outputs in the latent feature spaces, thereby producing more expressive and robust hash codes within a dual self-supervised learning paradigm. Comprehensive empirical analyses on six real-world benchmarks validate the effectiveness of our dual feature contrastive learning in boosting the performance of BGCH+ compared to existing approaches.}
}


@article{DBLP:journals/tkde/LianGSLLC24,
	author = {Defu Lian and
                  Zhenguo Gao and
                  Xia Song and
                  Yucheng Li and
                  Qi Liu and
                  Enhong Chen},
	title = {Training Recommenders Over Large Item Corpus With Importance Sampling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9433--9447},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3344657},
	doi = {10.1109/TKDE.2023.3344657},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LianGSLLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By predicting a personalized ranking on a set of items, item recommendation helps users determine the information they need. While optimizing a ranking-focused loss is more in line with the objectives of item recommendation, previous studies have indicated that current sampling-based ranking methods don't always surpass non-sampling ones. This is because it is either inefficient to sample a pool of representative negatives for better generalization or challenging to gauge their contributions to ranking-focused losses accurately. To this end, we propose a novel weighted ranking loss, which weights each negative with the softmax probability based on model's predictive score. Our theoretical analysis suggests that optimizing this loss boosts the normalized discounted cumulative gain. Furthermore, it appears that this loss acts as an approximate analytic solution for adversarial training of personalized ranking. To improve optimization efficiency, we approximate the weighted ranking loss with self-normalized importance sampling and show that the loss has good generalization properties. To improve generalization, we further develop efficient cluster-based negative samplers based on clustering over item vectors, to decrease approximation error caused by the divergence between the proposal and the target distribution. Comprehensive evaluations on real-world datasets show that our methods remarkably outperform leading item recommendation algorithms.}
}


@article{DBLP:journals/tkde/LiSDS24,
	author = {Ming Li and
                  Yan Song and
                  Derui Ding and
                  Ran Sun},
	title = {Triple Factorization-Based {SNLF} Representation With Improved Momentum-Incorporated
                  {AGD:} {A} Knowledge Transfer Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9448--9463},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3450469},
	doi = {10.1109/TKDE.2024.3450469},
	timestamp = {Sun, 19 Jan 2025 13:53:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiSDS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Symmetric, high-dimensional and sparse (SHiDS) networks usually contain rich knowledge regarding various patterns. To adequately extract useful information from SHiDS networks, a novel biased triple factorization-based (TF) symmetric and non-negative latent factor (SNLF) model is put forward by utilizing the transfer learning (TL) method, namely biased TL-incorporated TF-SNLF (BT^{2}-SNLF) model. The proposed BT^{2}-SNLF model mainly includes the following four ideas: 1) the implicit knowledge of the auxiliary matrix in the ternary rating domain is transferred to the target matrix in the numerical rating domain, facilitating the feature extraction; 2) two linear bias vectors are considered into the objective function to discover the knowledge describing the individual entity-oriented effect; 3) an improved momentum-incorporated additive gradient descent algorithm is developed to speed up the model convergence as well as guarantee the non-negativity of target SHiDS networks; and 4) a rigorous proof is provided to show that, under the assumption that the objective function is L-smooth and \\mu-convex, when t\\geq t_{0}, the algorithm begins to descend and it can find an \\epsilon-solution within O(ln((1+\\frac{\\mu L}{L(1+\\mu )+8\\mu })/\\epsilon )). Experimental results on six datasets from real applications demonstrate the effectiveness of our proposed T^{2}-SNLF and BT^{2}-SNLF models.}
}


@article{DBLP:journals/tkde/ZhangZYTDHT24,
	author = {Liwen Zhang and
                  Lianzhen Zhong and
                  Fan Yang and
                  Linglong Tang and
                  Di Dong and
                  Hui Hui and
                  Jie Tian},
	title = {TripleSurv: Triplet Time-Adaptive Coordinate Learning Approach for
                  Survival Analysis},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9464--9475},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3450910},
	doi = {10.1109/TKDE.2024.3450910},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZYTDHT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A core challenge in survival analysis is to model the distribution of time-to-event data, where the event of interest may be a death, failure, or occurrence of a specific event. Previous studies have showed that ranking and maximum likelihood estimation loss functions are widely-used learning approaches for survival analysis. However, ranking loss only focus on the ranking of survival time and does not consider potential effect of samples’ exact survival time values. Furthermore, the maximum likelihood estimation is unbounded and easily subject to outliers (e.g., censored data), which may cause poor performance of modeling. To handle the complexities of learning process and exploit valuable survival time values, we propose a time-adaptive coordinate loss function, TripleSurv, to achieve adaptive adjustments by introducing the differences in the survival time between sample pairs into the ranking, which can encourage the model to quantitatively rank relative risk of pairs, ultimately enhancing the accuracy of predictions. Most importantly, the TripleSurv is proficient in quantifying the relative risk between samples by ranking ordering of pairs, and consider the time interval as a trade-off to calibrate the robustness of model over sample distribution. Our TripleSurv is evaluated on three real-world survival datasets and a public synthetic dataset. The results show that our method outperforms the state-of-the-art methods and exhibits good model performance and robustness on modeling various sophisticated data distributions with different censor rates.}
}


@article{DBLP:journals/tkde/YuLLSXZ24,
	author = {Hang Yu and
                  Jinpeng Li and
                  Jie Lu and
                  Yiliao Song and
                  Shaorong Xie and
                  Guangquan Zhang},
	title = {Type-LDD: {A} Type-Driven Lite Concept Drift Detector for Data Streams},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9476--9489},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3344602},
	doi = {10.1109/TKDE.2023.3344602},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YuLLSXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Concept drift is a phenomenon that the distribution of data streams changes with time. When this happens, model predictions become less accurate. Hence, concept drift needs to be detected and adapted. Existing drift detection methods are good at determining when drift has occurred, but few retrieve information about how the drift came to be present in the stream, i.e., what type of drift has occurred. Hence, discussing the impact of the type of drift on adaptation is a difficult thing. To fill this gap, we propose a pre-trained framework for training a drift detector called a type-driven lite concept drift detector (Type-LDD) that retrieves information about both when and how a drift has occurred. In our proposed pre-trained framework, the Type-LDD including a drift-type identifier and a drift-point locator was based on a synthetic dataset containing a range of drift types. When repurposing the pre-trained model for detecting new data streams, a knowledge distillation module fine-tunes the proposed Type-LDD to speed up inference and keep detection accuracy. The proposed Type-LDD is validated on both synthetic data and real-world data, and demonstrated that accurately identifying the type of drift that has occurred can improve adaptation accuracy.}
}


@article{DBLP:journals/tkde/LiuZLWWW24,
	author = {Guofan Liu and
                  Jinghao Zhang and
                  Qiang Liu and
                  Junfei Wu and
                  Shu Wu and
                  Liang Wang},
	title = {Uni-Modal Event-Agnostic Knowledge Distillation for Multimodal Fake
                  News Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9490--9503},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3477977},
	doi = {10.1109/TKDE.2024.3477977},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuZLWWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid expansion of multimodal content in online social media, automatic detection of multimodal fake news has received much attention. Multimodal joint training commonly used in existing methods is expected to benefit from thoroughly leveraging cross-modal features, yet these methods still suffer from insufficient learning of uni-modal features. Due to the heterogeneity of multimodal networks, optimizing a single objective will inevitably make the models prone to rely on specific modality while leaving other modalities under-optimized. On the other hand, simply expecting each modality to play a significant role in identifying all the rumors is also not appropriate as the multimodal fake news often involves tampering in only one modality. Therefore, how to find the genuine tampering on the per-sample basis becomes the key point to unlock the full power of each modality in a good collaborative manner. To address these issues, we propose a Uni-modal Event-agnostic Knowledge Distillation framework (UEKD), which aims to transfer knowledge contained in the fine-grained prediction from uni-modal teachers to the multimodal student model through modality-specific distillation. Specifically, we find that the uni-modal teachers simply trained on the whole training set are easy to memorize the event-specific noise information to make a correct but biased prediction, failing to reflect the genuine degree of tampering in each modality. To tackle this problem, we propose to train and validate the teacher models on different domains in training dataset through a cross-validation manner, as the predictions from the out-of-domain teachers can be regarded as event-agnostic knowledge without spurious connections with event-specific information. Finally, to balance the convergence speeds across modalities, we dynamically monitor the involvement of each modality during training, through which we could identify the more under-optimized modalities and re-weight the distillation loss accordingly. Our method could be served as a plug-and-play module for existing multimodal fake news detection backbones. Extensive experiments on three public datasets and four state-of-the-art fake news detection backbones show that our proposed method can improve the performance by a large margin.}
}


@article{DBLP:journals/tkde/ZhouLKG24,
	author = {Jian Zhou and
                  Jiasheng Li and
                  Li Kuang and
                  Ning Gui},
	title = {Unsupervised Graph Representation Learning Beyond Aggregated View},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9504--9516},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3418576},
	doi = {10.1109/TKDE.2024.3418576},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhouLKG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsupervised graph representation learning aims to condense graph information into dense vector embeddings to support various downstream tasks. To achieve this goal, existing UGRL approaches mainly adopt the message-passing mechanism to simultaneously incorporate graph topology and node attribute with an aggregated view. However, recent research points out that this direct aggregation may lead to issues such as over-smoothing and/or topology distortion, as topology and node attribute of totally different semantics. To address this issue, this paper proposes a novel Graph Dual-view AutoEncoder framework (GDAE) which introduces the node-wise view for an individual node beyond the traditional aggregated view for aggregation of connected nodes. Specifically, the node-wise view captures the unique characteristics of individual node through a decoupling design, i.e., topology encoding by multi-steps random walk while preserving node-wise individual attribute. Meanwhile, the aggregated view aims to better capture the collective commonality among long-range nodes through an enhanced strategy, i.e., topology masking then attribute aggregation. Extensive experiments on 5 synthetic and 11 real-world benchmark datasets demonstrate that GDAE achieves the best results with up to 49.5% and 21.4% relative improvement in node degree prediction and cut-vertex detection tasks and remains top in node classification and link prediction tasks.}
}


@article{DBLP:journals/tkde/CuiLLWL24,
	author = {Lixiao Cui and
                  Yijing Luo and
                  Yusen Li and
                  Gang Wang and
                  Xiaoguang Liu},
	title = {When Learned Indexes Meet Persistent Memory: The Analysis and the
                  Optimization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9517--9531},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2023.3342825},
	doi = {10.1109/TKDE.2023.3342825},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CuiLLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging persistent memory (PM) is increasingly being leveraged to construct high-performance and persistent indexes. By exploiting data distribution, recent learned indexes open up a new index design paradigm. Some prior studies try to refit the learned index according to the features of PM. However, they neglect to analyze the performance of existing learned index schemes on PM. In this paper, we provide a comprehensive analysis of learned indexes on PM and propose two optimization methods to improve the performance. In particular, we evaluate ALEX, PGM-index, and XIndex after converting them to persistent indexes. With appropriate modifications, some design choices of volatile learned index still show favorable performance on PM under workloads with simple data distribution. But they perform poorly when the data distribution becomes complex. According to the experiment results, we summarize some instructive insights and optimize persistent learned indexes for complex data distributions with two methods: 1) a cost-based insertion pattern selection to minimize PM writes and 2) recoverable internal nodes selective persistence to decrease the overhead of internal lookups. Our evaluations demonstrate the performance of optimized ALEX is 2.09x/1.53x of the original ALEX in insert/search. Meanwhile, it also outperforms the specific-designed persistent learned index.}
}


@article{DBLP:journals/tkde/WuSZZYC24,
	author = {Sai Wu and
                  Meng Shi and
                  Dongxiang Zhang and
                  Junbo Zhao and
                  Gongsheng Yuan and
                  Gang Chen},
	title = {When Quantum Computing Meets Database: {A} Hybrid Sampling Framework
                  for Approximate Query Processing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9532--9546},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3480278},
	doi = {10.1109/TKDE.2024.3480278},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WuSZZYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum computing represents a next-generation technology in data processing, promising to transcend the limitations of traditional computation. In this paper, we undertake an early exploration of the potential integration of quantum computing with database query optimization. We introduce a pioneering hybrid classical-quantum algorithm for sampling-based approximate query processing (AQP). The core concept of the algorithm revolves around identifying rare groups, which often follow a long-tail distribution, and applying distinct sampling methodologies to normal and rare groups. By leveraging the quantum capabilities of the diffusion gate and QRAM, the algorithm defines a novel quantum sampling approach that iteratively amplifies the signals of these infrequent groups. The algorithm operates without the need for preprocessing or prior knowledge of workloads or data. It utilizes the power of quadratic acceleration to achieve well-balanced sampling across various data categories. Experimental results demonstrate that in the context of AQP, the new sampling scheme provides higher accuracy at the same sampling cost. Additionally, the benefits of quantum computing become more pronounced as query selectivity increases.}
}


@article{DBLP:journals/tkde/CheCHSC24,
	author = {Wenkui Che and
                  Zhiwen Chen and
                  Daokun Hu and
                  Jianhua Sun and
                  Hao Chen},
	title = {ZBTree: {A} Fast and Scalable B{\textdollar}{\^{}}+{\textdollar}+-Tree
                  for Persistent Memory},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {36},
	number = {12},
	pages = {9547--9563},
	year = {2024},
	url = {https://doi.org/10.1109/TKDE.2024.3421232},
	doi = {10.1109/TKDE.2024.3421232},
	timestamp = {Sun, 22 Dec 2024 15:49:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/CheCHSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present the design and implementation of ZBTree, a hotness-aware B^+-Tree for persistent memory (PMem). ZBTree leverages the PMem+DRAM architecture, which is featured with a volatile operation layer to accelerate data access and an order-preserving persistent layer to achieve fast recovery and low-overhead consistency and persistence guarantees. The operation layer contains inner nodes for indexing and compacted leaf nodes (DLeaves) that hold metadata. Based on leaf node compaction, we present a data lodging method, which supports to load hot data into fast DRAM dynamically, avoiding PMem accesses for subsequent reads of hot data and achieving improved read performance without incurring extra DRAM usage. In addition, we present a lightweight node splitting mechanism with constant persistence overhead that does not vary with node size. Our extensive evaluations show that ZBTree achieves higher throughput by a factor of 1.4x-6.3x compared to state-of-the-art tree indexes under a wide range of workloads. Meanwhile, ZBTree achieves comparable or faster recovery speed compared to existing designs.}
}
