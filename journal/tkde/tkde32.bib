@article{DBLP:journals/tkde/GiasemidisKAN20,
	author = {Georgios Giasemidis and
                  Nikolaos Kaplis and
                  Ioannis Agrafiotis and
                  Jason R. C. Nurse},
	title = {A Semi-Supervised Approach to Message Stance Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {1--11},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2880192},
	doi = {10.1109/TKDE.2018.2880192},
	timestamp = {Thu, 19 Dec 2019 09:25:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GiasemidisKAN20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social media communications are becoming increasingly prevalent; some useful, some false, whether unwittingly or maliciously. An increasing number of rumours daily flood the social networks. Determining their veracity in an autonomous way is a very active and challenging field of research, with a variety of methods proposed. However, most of the models rely on determining the constituent messages' stance towards the rumour, a feature known as the “wisdom of the crowd.” Although several supervised machine-learning approaches have been proposed to tackle the message stance classification problem, these have numerous shortcomings. In this paper, we argue that semi-supervised learning is more effective than supervised models and use two graphbased methods to demonstrate it. This is not only in terms of classification accuracy, but equally important, in terms of speed and scalability. We use the Label Propagation and Label Spreading algorithms and run experiments on a dataset of 72 rumours and hundreds of thousands messages collected from Twitter. We compare our results on two available datasets to the state-of-the-art to demonstrate our algorithms' performance regarding accuracy, speed, and scalability for real-time applications.}
}


@article{DBLP:journals/tkde/GhanbarpourN20,
	author = {Asieh Ghanbarpour and
                  Hassan Naderi},
	title = {An Attribute-Specific Ranking Method Based on Language Models for
                  Keyword Search over Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {12--25},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2879863},
	doi = {10.1109/TKDE.2018.2879863},
	timestamp = {Thu, 19 Dec 2019 09:25:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GhanbarpourN20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many real-world networks such as Facebook, LinkedIn, and Wikipedia exhibit rich connectivity patterns along with worthwhile content nodes often labeled with meaningful attributes. Keyword search is an effective method to retrieve information from such useful networks. The aim of keyword search is to find a set of answers (subgraphs) covering all or part of the queried keywords. A challenge in keyword search systems is to rank answers according to their relevance to the query. This relevance lies in the textual content and structural compactness of the answers. In this paper, an attribute-specific ranking method is proposed based on language models to rank candidate answers according to their semantic information up to the attribute level. This method scores answers using a model enriched with attribute-specific preferences and integrating both the structure and content of answers. The proposed model is directly estimated on the sub-graphs (answers) and is defined such that it can preserve the local importance of keywords in nodes. Extensive experiments conducted on a standard evaluation framework with three real-world datasets illustrate the superior effectiveness of the proposed ranking method to that of the state-of-the-art methods.}
}


@article{DBLP:journals/tkde/ZhengZJHSLZ20,
	author = {Bolong Zheng and
                  Kai Zheng and
                  Christian S. Jensen and
                  Nguyen Quoc Viet Hung and
                  Han Su and
                  Guohui Li and
                  Xiaofang Zhou},
	title = {Answering Why-Not Group Spatial Keyword Queries},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {26--39},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2879819},
	doi = {10.1109/TKDE.2018.2879819},
	timestamp = {Wed, 07 Dec 2022 23:01:52 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengZJHSLZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of geo-textual objects on the web, extensive efforts have been devoted to improving the efficiency of top-k spatial keyword queries in different settings. However, comparatively much less work has been reported on enhancing the quality and usability of such queries. In this context, we propose means of enhancing the usability of a top-k group spatial keyword query, where a group of users aim to find k objects that contain given query keywords and are nearest to the users. Specifically, when users receive the result of such a query, they may find that one or more objects that they expect to be in the result are in fact missing, and they may wonder why. To address this situation, we develop a so-called why-notquery that is able to minimally modifythe original query into a query that returns the expected, but missing, objects, in addition to other objects. Specifically, we formalize the why-not query in relation to the top-k group spatial keyword query, called the Why-not Group Spatial Keyword Query (WGSK) that is able to provide a group of users with a more satisfactory query result. We propose a three-phase framework for efficiently computing the WGSK. The first phase substantially reduces the search space for the subsequent phases by retrieving a set of objects that may affect the ranking of the user-expected objects. The second phase provides an incremental sampling algorithm that generates candidate weightings of more promising queries. The third phase determines the penalty of each refined query and returns the querywith minimal penalty, i.e., the minimally modified query. Extensive experiments with real and synthetic data offer evidence that the proposed solution excels over baselines with respect to both effectiveness and efficiency.}
}


@article{DBLP:journals/tkde/TeoCL20,
	author = {Sin G. Teo and
                  Jianneng Cao and
                  Vincent C. S. Lee},
	title = {{DAG:} {A} General Model for Privacy-Preserving Data Mining},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {40--53},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2880743},
	doi = {10.1109/TKDE.2018.2880743},
	timestamp = {Thu, 19 Dec 2019 09:25:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/TeoCL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Secure multi-party computation (SMC) allows parties to jointly compute a function over their inputs, while keeping every input confidential. It has been extensively applied in tasks with privacy requirements, such as privacy-preserving data mining (PPDM), to learn task output and at the same time protect input data privacy. However, existing SMC-based solutions are ad-hoc - they are proposed for specific applications, and thus cannot be applied to other applications directly. To address this issue, we propose a privacy model DAG (Directed Acyclic Graph) that consists of a set of fundamental secure operators (e.g., +, -, ×, /, and power). Our model is general - its operators, if pipelined together, can implement various functions, even complicated ones like Naı̈ve Bayes classifier. It is also extendable - new secure operators can be defined to expand the functions that the model supports. For case study, we have applied our DAG model to two data mining tasks: kernel regression and Naı̈ve Bayes. Experimental results show that DAG generates outputs that are almost the same as those by non-private setting, where multiple parties simply disclose their data. The experimental results also show that our DAG model runs in acceptable time, e.g., in kernel regression, when training data size is 683,093, one prediction in non-private setting takes 5.93 sec, and that by our DAG model takes 12.38 sec.}
}


@article{DBLP:journals/tkde/OsiaTSKHR20,
	author = {Seyed Ali Osia and
                  Ali Taheri and
                  Ali Shahin Shamsabadi and
                  Kleomenis Katevas and
                  Hamed Haddadi and
                  Hamid R. Rabiee},
	title = {Deep Private-Feature Extraction},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {54--66},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2878698},
	doi = {10.1109/TKDE.2018.2878698},
	timestamp = {Thu, 19 Dec 2019 09:25:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/OsiaTSKHR20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present and evaluate Deep Private-Feature Extractor (DPFE) , a deep model which is trained and evaluated based on information theoretic constraints. Using the selective exchange of information between a user's device and a service provider, DPFE enables the user to prevent certain sensitive information from being shared with a service provider, while allowing them to extract approved information using their model. We introduce and utilize the log-rank privacy, a novel measure to assess the effectiveness of DPFE in removing sensitive information and compare different models based on their accuracy-privacy trade-off. We then implement and evaluate the performance of DPFE on smartphones to understand its complexity, resource demands, and efficiency trade-offs. Our results on benchmark image datasets demonstrate that under moderate resource utilization, DPFE can achieve high accuracy for primary tasks while preserving the privacy of sensitive information.}
}


@article{DBLP:journals/tkde/SutediSA20,
	author = {Sutedi Sutedi and
                  Noor Akhmad Setiawan and
                  Teguh Bharata Adji},
	title = {Enhanced Graph Transforming {V2} Algorithm for Non-Simple Graph in
                  Big Data Pre-Processing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {67--77},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2880971},
	doi = {10.1109/TKDE.2018.2880971},
	timestamp = {Mon, 28 Aug 2023 21:37:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SutediSA20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incapability of relational database in handling large-scale data triggers the development of NoSQL database that becomes part of a big data ecosystem. NoSQL database has different characteristics compared to the relational database. However, NoSQL database requires data from the relational database as one of the structured data sources. Therefore, data pre-processing is required to ensure proper data migration from a relational database to NoSQL database. This data pre-processing is normally called data transformation. One of the simple and understandable transformation algorithms is graph transforming algorithm. However, the algorithm has a problem in solving a non-simple graph (multigraph). This research proposes an algorithm to overcome several multigraph problems. The experimental work confirms that the algorithm proposed in this research is able to transform data from a relational database to NoSQL schema that has a minimum number of redundant attributes while the data completeness is still maintained.}
}


@article{DBLP:journals/tkde/ZhangLZQZ20,
	author = {Fan Zhang and
                  Conggai Li and
                  Ying Zhang and
                  Lu Qin and
                  Wenjie Zhang},
	title = {Finding Critical Users in Social Communities: The Collapsed Core and
                  Truss Problems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {78--91},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2880976},
	doi = {10.1109/TKDE.2018.2880976},
	timestamp = {Tue, 04 Jan 2022 17:01:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangLZQZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In social networks, the leave of critical users may significantly break network engagement, i.e., lead a large number of other users to drop out. A popular model to measure social network engagement is k-core, the maximal subgraph in which every vertex has at least k neighbors. To identify critical users, we propose the collapsed k-core problem: given a graph G, a positive integer k and a budget b, we aim to find b vertices in G such that the deletion of the b vertices leads to the smallest k-core. We prove the problem is NP-hard and in approximate. An efficient algorithm is proposed, which significantly reduces the number of candidate vertices. We also study the user leave towards the model of k-truss which further considers tie strength by conducting additional computation w.r.t. k-core. We prove the corresponding collapsed k-truss problem is also NP-hard and in approximate. An efficient algorithm is proposed to solve the problem. The advantages and disadvantages of the two proposed models are experimentally compared. Comprehensive experiments on nine real-life social networks demonstrate the effectiveness and efficiency of our proposed methods.}
}


@article{DBLP:journals/tkde/DongZSZY20,
	author = {Kaixing Dong and
                  Bowen Zhang and
                  Yanyan Shen and
                  Yanmin Zhu and
                  Jiadi Yu},
	title = {{GAT:} {A} Unified GPU-Accelerated Framework for Processing Batch
                  Trajectory Queries},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {92--107},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2879862},
	doi = {10.1109/TKDE.2018.2879862},
	timestamp = {Fri, 31 Jan 2020 07:56:57 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DongZSZY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing amount of trajectory data facilitates a wide spectrum of practical applications in which large numbers of trajectory range and similarity queries are issued continuously. This calls for high-throughput trajectory query processing. Traditional in-memory databases lack considerations of the unique features of trajectories, while specialized trajectory query processing systems are typically designed for only one type of trajectory queries. This paper introduces GAT, a unified GPU-accelerated framework to process batch trajectory queries with the objective of high throughput. GAT follows the filtering-and-verification paradigm where we develop a novel index GTIDX for effectively filtering invalid trajectories on the CPU, and exploit the massive parallelism of the GPU for verification. To optimize the performance of GAT, we first greedily partition batch queries to reduce the amortized query processing latency. We then apply the Morton-based encoding method to coalesce data access requests from the GPU cores, and maintain a hash table to avoid redundant data transfer between CPU and GPU. To achieve load balance, we group size-varying cells into balanced blocks with similar numbers of trajectory points. Extensive experiments have been conducted over real-life trajectory datasets. The results show that GAT is efficient, scalable, and achieves high throughput with acceptable indexing cost.}
}


@article{DBLP:journals/tkde/YagoubiAMP20,
	author = {Djamel Edine Yagoubi and
                  Reza Akbarinia and
                  Florent Masseglia and
                  Themis Palpanas},
	title = {Massively Distributed Time Series Indexing and Querying},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {108--120},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2880215},
	doi = {10.1109/TKDE.2018.2880215},
	timestamp = {Thu, 19 Dec 2019 09:25:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YagoubiAMP20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indexing is crucial for many data mining tasks that rely on efficient and effective similarity query processing. Consequently, indexing large volumes of time series, along with high performance similarity query processing, have became topics of high interest. For many applications across diverse domains though, the amount of data to be processed might be intractable for a single machine, making existing centralized indexing solutions inefficient. We propose a parallel indexing solution that gracefully scales to billions of time series, and a parallel query processing strategy that, given a batch of queries, efficiently exploits the index. Our experiments, on both synthetic and real world data, illustrate that our index creation algorithm works on four billion time series in less than five hours, while the state of the art centralized algorithms do not scale and have their limit on 1 billion time series, where they need more than five days. Also, our distributed querying algorithm is able to efficiently process millions of queries over collections of billions of time series, thanks to an effective load balancing mechanism.}
}


@article{DBLP:journals/tkde/HosseiniKAAFZR20,
	author = {Seyed Abbas Hosseini and
                  Ali Khodadadi and
                  Keivan Alizadeh and
                  Ali Arabzadeh and
                  Mehrdad Farajtabar and
                  Hongyuan Zha and
                  Hamid R. Rabiee},
	title = {Recurrent Poisson Factorization for Temporal Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {121--134},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2879796},
	doi = {10.1109/TKDE.2018.2879796},
	timestamp = {Thu, 19 Dec 2019 09:25:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HosseiniKAAFZR20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Poisson Factorization (PF) is the gold standard framework for recommendation systems with implicit feedback whose variants show state-of-the-art performance on real-world recommendation tasks. However, they do not explicitly take into account the temporal behavior of users which is essential to recommend the right item to the right user at the right time. In this paper, we introduce Recurrent Poisson Factorization (RPF) framework that generalizes the classical PF methods by utilizing a Poisson process for modeling the implicit feedback. RPF treats time as a natural constituent of the model, and takes important factors for recommendation into consideration to provide a rich family of time-sensitive factorization models. They include Hierarchical RPFthat captures the consumption heterogeneity among users and items, Dynamic RPF that handles dynamic user preferences and item specifications, Social RPF that models the social-aspect of product adoption, Item-Item RPFthat considers the inter-item correlations, and eXtended Item-Item RPF that utilizes items' metadata to better infer the correlation among engagement patterns of users with items. We also develop an efficient variational algorithm for approximate inference that scales up to massive datasets. We demonstrate RPF's superior performance over many state-of-the-art methods on synthetic dataset, and wide variety of large scale real-world datasets.}
}


@article{DBLP:journals/tkde/ZhangCJZT20,
	author = {Chen Jason Zhang and
                  Lei Chen and
                  H. V. Jagadish and
                  Mengchen Zhang and
                  Yongxin Tong},
	title = {Reducing Uncertainty of Schema Matching via Crowdsourcing with Accuracy
                  Rates},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {135--151},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2881185},
	doi = {10.1109/TKDE.2018.2881185},
	timestamp = {Wed, 20 Mar 2024 15:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangCJZT20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Schema matching is a central challenge for data integration systems. Inspired by the popularity and the success of crowdsourcing platforms, we explore the use of crowdsourcing to reduce the uncertainty of schema matching. Since crowdsourcing platforms are most effective for simple questions, we assume that each Correspondence Correctness Question (CCQ) asks the crowd to decide whether a given correspondence should exist in the correct matching. Furthermore, members of a crowd may sometimes return incorrect answers with different probabilities. Accuracy rates of individual crowd workers can be attributes of CCQs as well as evaluations of individual workers. We prove that uncertainty reduction equals to entropy of answers minus entropy of crowds and show how to obtain lower and upper bounds for it. We propose frameworks and efficient algorithms to dynamically manage the CCQs to maximize the uncertainty reduction within a limited budget of questions. We develop two novel approaches, namely “Single CCQ” and “Multiple CCQ”, which adaptively select, publish, and manage questions. We verify the value of our solutions with simulation and real implementation.}
}


@article{DBLP:journals/tkde/HeldensLS20,
	author = {Stijn Heldens and
                  Nelly Litvak and
                  Maarten van Steen},
	title = {Scalable Detection of Crowd Motion Patterns},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {152--164},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2879079},
	doi = {10.1109/TKDE.2018.2879079},
	timestamp = {Thu, 19 Dec 2019 09:25:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HeldensLS20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Studying the movements of crowds is important for understanding and predicting the behavior of large groups of people. When analyzing crowds, one is often interested in the long-term macro-level motions of the crowd as a whole, as opposed to the micro-level short-term movements of individuals. A high-level representation of these motions is thus desirable. In this work, we present a scalable method for detection of crowd motion patterns , i.e., spatial areas describing the dominant motions within crowds. For measuring crowd movements, we propose a fast, scalable, and low-cost method based on proximity graphs. For analyzing crowd movements, we utilize a three-stage pipeline: (1) represents the behavior of each person at each moment in time as a low-dimensional data point, (2) cluster these data points based on spatial relations, and (3) concatenate these clusters based on temporal relations. Experiments on synthetic datasets reveals our method can handle various scenarios including curved lanes and diverging flows. Evaluation on real-world datasets shows our method is able to extract useful motion patterns which could not be properly detected by existing methods. Overall, we see our work as an initial step towards rich pattern recognition.}
}


@article{DBLP:journals/tkde/ChenYNM20,
	author = {Xiaojun Chen and
                  Guowen Yuan and
                  Feiping Nie and
                  Zhong Ming},
	title = {Semi-Supervised Feature Selection via Sparse Rescaled Linear Square
                  Regression},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {165--176},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2879797},
	doi = {10.1109/TKDE.2018.2879797},
	timestamp = {Wed, 10 Jun 2020 15:40:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenYNM20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid increase of the data size, it has increasing demands for selecting features by exploiting both labeled and unlabeled data. In this paper, we propose a novel semi-supervised embedded feature selection method. The new method extends the least square regression model by rescaling the regression coefficients in the least square regression with a set of scale factors, which is used for evaluating the importance of features. An iterative algorithm is proposed to optimize the new model. It has been proved that solving the new model is equivalent to solving a sparse model with a flexible and adaptable ℓ 2;p norm regularization. Moreover, the optimal solution of scale factors provides a theoretical explanation for why we can use {||w 1 || 2 , . . .,||w d || 2 } to evaluate the importance of features. Experimental results on eight benchmark data sets show the superior performance of the proposed method.}
}


@article{DBLP:journals/tkde/OkolicaPMG20,
	author = {James S. Okolica and
                  Gilbert L. Peterson and
                  Robert F. Mills and
                  Michael R. Grimaila},
	title = {Sequence Pattern Mining with Variables},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {177--187},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2881675},
	doi = {10.1109/TKDE.2018.2881675},
	timestamp = {Mon, 26 Oct 2020 08:28:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/OkolicaPMG20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequence pattern mining (SPM) seeks to find multiple items that commonly occur together in a specific order. One common assumption is that the relevant differences between items are captured through creating distinct items. In some domains, this leads to an exponential increase in the number of items. This paper presents a new SPM, Sequence Mining of Temporal Clusters (SMTC), that allows item differentiation through attribute variables for domains with large numbers of items. It also provides a new technique for addressing interleaving, a phenomena that occurs when two sequences occur simultaneously resulting in their items alternating. By first clustering items temporally and only focusing on sequences after the temporal clusters are established, it sidesteps the traditional interleaving issues. SMTC is evaluated on a digital forensics dataset, a domain with a large number of items and frequent interleaving. Its results are compared with Discontinuous Varied Order Sequence Mining (DVSM) with variables added (DVSM-V). By adding variables, both algorithms reduce the data by 96 percent, and identify 100 percent of the events while keeping the false positive rate below 0.03 percent. SMTC mines the data in 20 percent of the time it takes DVSM-V and provides a lower false positive rate even at higher similarity thresholds.}
}


@article{DBLP:journals/tkde/ShangZLCTZTJ20,
	author = {Fanhua Shang and
                  Kaiwen Zhou and
                  Hongying Liu and
                  James Cheng and
                  Ivor W. Tsang and
                  Lijun Zhang and
                  Dacheng Tao and
                  Licheng Jiao},
	title = {{VR-SGD:} {A} Simple Stochastic Variance Reduction Method for Machine
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {1},
	pages = {188--202},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2878765},
	doi = {10.1109/TKDE.2018.2878765},
	timestamp = {Sat, 30 May 2020 19:46:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShangZLCTZTJ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a simple variant of the original SVRG, called variance reduced stochastic gradient descent (VR-SGD). Unlike the choices of snapshot and starting points in SVRG and its proximal variant, Prox-SVRG, the two vectors of VR-SGD are set to the average and last iterate of the previous epoch, respectively. The settings allow us to use much larger learning rates, and also make our convergence analysis more challenging. We also design two different update rules for smooth and non-smooth objective functions, respectively, which means that VR-SGD can tackle non-smooth and/or non-strongly convex problems directly without any reduction techniques. Moreover, we analyze the convergence properties of VR-SGD for strongly convex problems, which show that VR-SGD attains linear convergence. Different from most algorithms that have no convergence guarantees for non-strongly convex problems, we also provide the convergence guarantees of VR-SGD for this case, and empirically verify that VR-SGD with varying learning rates achieves similar performance to its momentum accelerated variant that has the optimal convergence rate \\mathcal {O}(1/T^2)\n. Finally, we apply VR-SGD to solve various machine learning problems, such as convex and non-convex empirical risk minimization, and leading eigenvalue computation. Experimental results show that VR-SGD converges significantly faster than SVRG and Prox-SVRG, and usually outperforms state-of-the-art accelerated methods, e.g., Katyusha.}
}


@article{DBLP:journals/tkde/YangSHSXQGJ20,
	author = {Zhou Yang and
                  Heli Sun and
                  Jianbin Huang and
                  Zhongbin Sun and
                  Hui Xiong and
                  Shaojie Qiao and
                  Ziyu Guan and
                  Xiaolin Jia},
	title = {An Efficient Destination Prediction Approach Based on Future Trajectory
                  Prediction and Transition Matrix Optimization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {203--217},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2883938},
	doi = {10.1109/TKDE.2018.2883938},
	timestamp = {Fri, 10 Mar 2023 16:01:56 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YangSHSXQGJ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Destination prediction is an essential task in various mobile applications and up to now many methods have been proposed. However, existing methods usually suffer from the problems of heavy computational burden, data sparsity, and low coverage. Therefore, a novel approach named DestPD is proposed to tackle the aforementioned problems. Differing from an earlier approach that only considers the starting and current location of a partial trip, DestPD first determines the most likely future location and then predicts the destination. It comprises two phases, the offline training and the online prediction. During the offline training, transition probabilities between two locations are obtained via Markov transition matrix multiplication. In order to improve the efficiency of matrix multiplication, we propose two data constructs, Efficient Transition Probability (ETP) and Transition Probabilities with Detours (TPD). They are capable of pinpointing the minimum amount of needed computation. During the online prediction, we design Obligatory Update Point (OUP) and Transition Affected Area (TAA) to accelerate the frequent update of ETP and TPD for recomputing the transition probabilities. Moreover, a new future trajectory prediction approach is devised. It captures the most recent movement based on a query trajectory. It consists of two components: similarity finding through Best Path Notation (BPN) and best node selection. Our novel BPN similarity finding scheme keeps track of the nodes that induces inefficiency and then finds similarity fast based on these nodes. It is particularly suitable for trajectories with overlapping segments. Finally, the destination is predicted by combining transition probabilities and the most probable future location through Bayesian reasoning. The DestPD method is proved to achieve one order of cut in both time and space complexity. Furthermore, the experimental results on real-world and synthetic datasets have shown that DestPD consistently surpasses the state-of-the-art methods in terms of both efficiency (approximately over 100 times faster) and accuracy.}
}


@article{DBLP:journals/tkde/HuGLWDM20,
	author = {Weiming Hu and
                  Jun Gao and
                  Bing Li and
                  Ou Wu and
                  Junping Du and
                  Stephen J. Maybank},
	title = {Anomaly Detection Using Local Kernel Density Estimation and Context-Based
                  Regression},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {218--233},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2882404},
	doi = {10.1109/TKDE.2018.2882404},
	timestamp = {Thu, 06 Feb 2020 18:12:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuGLWDM20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Current local density-based anomaly detection methods are limited in that the local density estimation and the neighborhood density estimation are not accurate enough for complex and large databases, and the detection performance depends on the size parameter of the neighborhood. In this paper, we propose a new kernel function to estimate samples’ local densities and propose a weighted neighborhood density estimation to increase the robustness to changes in the neighborhood size. We further propose a local kernel regression estimator and a hierarchical strategy for combining information from the multiple scale neighborhoods to refine anomaly factors of samples. We apply our general anomaly detection method to image saliency detection by regarding salient pixels in objects as anomalies to the background regions. Local density estimation in the visual feature space and kernel-based saliency score propagation in the image enable the assignment of similar saliency values to homogenous object regions. Experimental results on several benchmark datasets demonstrate that our anomaly detection methods overall outperform several state-of-art anomaly detection methods. The effectiveness of our image saliency detection method is validated by comparison with several state-of-art saliency detection methods.}
}


@article{DBLP:journals/tkde/SaadallahMSKJG20,
	author = {Amal Saadallah and
                  Lu{\'{\i}}s Moreira{-}Matias and
                  Ricardo Teixeira Sousa and
                  Jihed Khiari and
                  Erik Jenelius and
                  Jo{\~{a}}o Gama},
	title = {{BRIGHT} - Drift-Aware Demand Predictions for Taxi Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {234--245},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2883616},
	doi = {10.1109/TKDE.2018.2883616},
	timestamp = {Wed, 22 Nov 2023 15:44:46 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SaadallahMSKJG20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive data broadcast by GPS-equipped vehicles provide unprecedented opportunities. One of the main tasks in order to optimize our transportation networks is to build data-driven real-time decision support systems. However, the dynamic environments where the networks operate disallow the traditional assumptions required to put in practice many off-the-shelf supervised learning algorithms, such as finite training sets or stationary distributions. In this paper, we propose BRIGHT: a drift-aware supervised learning framework to predict demand quantities. BRIGHT aims to provide accurate predictions for short-term horizons through a creative ensemble of time series analysis methods that handles distinct types of concept drift. By selecting neighborhoods dynamically, BRIGHT reduces the likelihood of overfitting. By ensuring diversity among the base learners, BRIGHT ensures a high reduction of variance while keeping bias stable. Experiments were conducted using three large-scale heterogeneous real-world transportation networks in Porto (Portugal), Shanghai (China), and Stockholm (Sweden), as well as with controlled experiments using synthetic data where multiple distinct drifts were artificially induced. The obtained results illustrate the advantages of BRIGHT in relation to state-of-the-art methods for this task.}
}


@article{DBLP:journals/tkde/YeoHKKL20,
	author = {Jinyoung Yeo and
                  Seung{-}won Hwang and
                  Sungchul Kim and
                  Eunyee Koh and
                  Nedim Lipka},
	title = {Conversion Prediction from Clickstream: Modeling Market Prediction
                  and Customer Predictability},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {246--259},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2884467},
	doi = {10.1109/TKDE.2018.2884467},
	timestamp = {Mon, 10 Feb 2020 12:05:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YeoHKKL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As 98 percent of shoppers do not make a purchase on the first visit, we study the problem of predicting whether they would come back for a purchase later (i.e., conversion prediction). This problem is important for strategizing “retargeting”, for example, by sending coupons for customers who are likely to convert. For this goal, we study the following two problems, prediction of market and predictability of customer. First, prediction of market aims at identifying a conversion rate for a given product and its customer behavior modeling, which is an important analytics metric for retargeting process. Compared to existing approaches using either of customer or product-level conversion pattern, we propose a joint modeling of both patterns based on the well-studied buying decision process. Second, we can observe customer-specific behaviors after showing retargeting ads, to predict whether this specific customer follows the market model (high predictability) or not (low predictability). For the former, we apply the market model, and for the latter, we propose a new customer-specific prediction based on dynamic ad behavior features. To evaluate the effectiveness of our methods, we perform extensive experiments on the simulated dataset generated based on a set of real-world web logs and retargeting campaign logs. The evaluation results show that conversion predictions and predictability by our approach are consistently more accurate and robust than those by existing baselines in dynamic market environment.}
}


@article{DBLP:journals/tkde/SonKKYKH20,
	author = {Yongseok Son and
                  Moonsub Kim and
                  Sunggon Kim and
                  Heon Young Yeom and
                  Nam Sung Kim and
                  Hyuck Han},
	title = {Design and Implementation of SSD-Assisted Backup and Recovery for
                  Database Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {260--274},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2884466},
	doi = {10.1109/TKDE.2018.2884466},
	timestamp = {Thu, 06 Feb 2020 18:12:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SonKKYKH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As flash-based solid-state drive (SSD) becomes more prevalent because of the rapid fall in price and the significant increase in capacity, customers expect better data services than traditional disk-based systems. However, the order of magnitude performance provided and new characteristics of flash require a rethinking of data services. For example, backup and recovery is an important service in a database system since it protects data against unexpected hardware and software failures. To provide backup and recovery, backup/recovery tools or backup/recovery methods by operating systems can be used. However, the tools perform time-consuming jobs, and the methods may negatively affect run-time performance during normal operation even though high-performance SSDs are used. To handle these issues, we propose an SSD-assisted backup/recovery scheme for database systems. Our scheme is to utilize the characteristics (e.g., out-of-place update) of flash-based SSD for backup/recovery operations. To this end, we exploit the resources (e.g., flash translation layer and DRAM cache with supercapacitors) inside SSD, and we call our SSD with new backup/ recovery functionality BR-SSD. We design and implement the functionality in the Samsung enterprise-class SSD (i.e., SM843Tn) for more realistic systems. Furthermore, we exploit and integrate BR-SSDs into database systems (i.e., MySQL) in replication and redundant array of independent disks (RAID) environments, as well as a database system in a single BR-SSD. The experimental result demonstrates that our scheme provides fast backup and recovery but does not negatively affect the run-time performance during normal operation.}
}


@article{DBLP:journals/tkde/SongSZCW20,
	author = {Shaoxu Song and
                  Yu Sun and
                  Aoqian Zhang and
                  Lei Chen and
                  Jianmin Wang},
	title = {Enriching Data Imputation under Similarity Rule Constraints},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {275--287},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2883103},
	doi = {10.1109/TKDE.2018.2883103},
	timestamp = {Thu, 07 Dec 2023 07:48:57 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SongSZCW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incomplete information often occurs along with many database applications, e.g., in data integration, data cleaning, or data exchange. The idea of data imputation is often to fill the missing data with the values of its neighbors who share the same/similar information. Such neighbors could either be identified certainly by editing rules or extensively by similarity relationships. Owing to data sparsity, the number of neighbors identified by editing rules w.r.t. value equality is rather limited, especially in the presence of data values with variances. To enrich the imputation candidates, a natural idea is to extensively consider the neighbors with similarity relationship. However, the candidates suggested by these (heterogenous) similarity neighbors may conflict with each other. In this paper, we propose to utilize the similarity rules with tolerance to small variations (instead of the aforesaid editing rules with strict equality constraints) to rule out the invalid candidates provided by similarity neighbors. To enrich the data imputation, i.e., imputing the missing values more, we study the problem of maximizing the missing data imputation. Our major contributions include (1) the NP-hardness analysis on solving as well as approximating the problem, (2) exact algorithms for tackling the problem, and (3) efficient approximation with performance guarantees. Experiments on real and synthetic data sets demonstrate the superiority of our proposal in filling accuracy. We also demonstrate that the record matching application is indeed improved, after applying the proposed imputation.}
}


@article{DBLP:journals/tkde/YanTTYSZ20,
	author = {Yan Yan and
                  Mingkui Tan and
                  Ivor W. Tsang and
                  Yi Yang and
                  Qinfeng Shi and
                  Chengqi Zhang},
	title = {Fast and Low Memory Cost Matrix Factorization: Algorithm, Analysis,
                  and Case Study},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {288--301},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2882197},
	doi = {10.1109/TKDE.2018.2882197},
	timestamp = {Tue, 21 Mar 2023 21:10:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YanTTYSZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Matrix factorization has been widely applied to various applications. With the fast development of storage and internet technologies, we have been witnessing a rapid increase of data. In this paper, we propose new algorithms for matrix factorization with the emphasis on efficiency. In addition, most existing methods of matrix factorization only consider a general smooth least square loss. Differently, many real-world applications have distinctive characteristics. As a result, different losses should be used accordingly. Therefore, it is beneficial to design new matrix factorization algorithms that are able to deal with both smooth and non-smooth losses. To this end, one needs to analyze the characteristics of target data and use the most appropriate loss based on the analysis. We particularly study two representative cases of low-rank matrix recovery, i.e., collaborative filtering for recommendation and high dynamic range imaging. To solve these two problems, we respectively propose a stage-wise matrix factorization algorithm by exploiting manifold optimization techniques. From our theoretical analysis, they are both are provably guaranteed to converge to a stationary point. Extensive experiments on recommender systems and high dynamic range imaging demonstrate the satisfactory performance and efficiency of our proposed method on large-scale real data.}
}


@article{DBLP:journals/tkde/MoreoES20,
	author = {Alejandro Moreo and
                  Andrea Esuli and
                  Fabrizio Sebastiani},
	title = {Learning to Weight for Text Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {302--316},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2883446},
	doi = {10.1109/TKDE.2018.2883446},
	timestamp = {Sun, 04 Aug 2024 19:47:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MoreoES20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In information retrieval (IR) and related tasks, term weighting approaches typically consider the frequency of the term in the document and in the collection in order to compute a score reflecting the importance of the term for the document. In tasks characterized by the presence of training data (such as text classification) it seems logical that the term weighting function should take into account the distribution (as estimated from training data) of the term across the classes of interest. Although “supervised term weighting” approaches that use this intuition have been described before, they have failed to show consistent improvements. In this article, we analyze the possible reasons for this failure, and call consolidated assumptions into question. Following this criticism, we propose a novel supervised term weighting approach that, instead of relying on any predefined formula, learns a term weighting function optimized on the training set of interest; we dub this approach Learning to Weight (LTW). The experiments that we run on several well-known benchmarks, and using different learning methods, show that our method outperforms previous term weighting approaches in text classification.}
}


@article{DBLP:journals/tkde/CuiWLZW20,
	author = {Qiang Cui and
                  Shu Wu and
                  Qiang Liu and
                  Wen Zhong and
                  Liang Wang},
	title = {{MV-RNN:} {A} Multi-View Recurrent Neural Network for Sequential Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {317--331},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2881260},
	doi = {10.1109/TKDE.2018.2881260},
	timestamp = {Sat, 05 Sep 2020 17:44:55 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CuiWLZW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation is a fundamental task for network applications, and it usually suffers from the item cold start problem due to the insufficiency of user feedbacks. There are currently three kinds of popular approaches which are respectively based on matrix factorization (MF) of collaborative filtering, Markov chain (MC), and recurrent neural network (RNN). Although widely used, they have some limitations. MF based methods could not capture dynamic user's interest. The strong Markov assumption greatly limits the performance of MC based methods. RNN based methods are still in the early stage of incorporating additional information. Based on these basic models, many methods with additional information only validate incorporating one modality in a separate way. In this work, to make the sequential recommendation and deal with the item cold start problem, we propose a M ulti- V iew R recurrent N eural N etwork ( MV-RNN ) model. Given the latent feature, MV-RNN can alleviate the item cold start problem by incorporating visual and textual information. First, At the input of MV-RNN, three different combinations of multi-view features are studied, like concatenation, fusion by addition and fusion by reconstructing the original multi-modal data. MV-RNN applies the recurrent structure to dynamically capture the user's interest. Second, we design a separate structure and a united structure on the hidden state of MV-RNN to explore a more effective way to handle multi-view features. Experiments on two real-world datasets show that MV-RNN can effectively generate the personalized ranking list, tackle the missing modalities problem, and significantly alleviate the item cold start problem.}
}


@article{DBLP:journals/tkde/JinLCL20,
	author = {Hai Jin and
                  Changfu Lin and
                  Hanhua Chen and
                  Jiangchuan Liu},
	title = {QuickPoint: Efficiently Identifying Densest Sub-Graphs in Online Social
                  Networks for Event Stream Dissemination},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {332--346},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2881435},
	doi = {10.1109/TKDE.2018.2881435},
	timestamp = {Thu, 06 Feb 2020 18:12:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/JinLCL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient event stream dissemination is a challenging problem in large-scale Online Social Network (OSN) systems due to the costly inter-server communications caused by the per-user view data storage. To solve the problem, previous schemes mainly explore the structures of social graphs to reduce the inter-server traffic. Based on the observation of high cluster coefficients in OSNs, a state-of-the-art social piggyback scheme can save redundant messages by exploiting an intrinsic hub-structure in an OSN graph for message piggybacking. Essentially, finding the best hub-structure for piggybacking is equivalent to finding a variation of the densest sub-graph. The existing scheme computes the best hub-structure by iteratively removing the node with the minimum weighted degree. Such a scheme incurs a worst computation cost of O(n 2 ), making it not scalable to large-scale OSN graphs. Using alternative hubstructure instead of the best hub-structure can speed up the piggyback assignment. However, they greatly sacrifice the communication efficiency of the assignment schedule. Different from the existing designs, in this work, we propose a QuickPoint algorithm, which removes a fraction of nodes in each iteration in finding the best hub-structure. We mathematically prove that QuickPoint converges in O(log α n)(α > 1) iterations in finding the best hub-structure for efficient piggyback. We implement QuickPoint in parallel atop Pregel, a vertex-centric distributed graph processing platform. Comprehensive experiments using large-scale data from Twitter and Flickr show that our scheme is 38.8× more efficient compared to existing schemes.}
}


@article{DBLP:journals/tkde/HouCCNL20,
	author = {Boyi Hou and
                  Qun Chen and
                  Zhaoqiang Chen and
                  Youcef Nafa and
                  Zhanhuai Li},
	title = {r-HUMO: {A} Risk-Aware Human-Machine Cooperation Framework for Entity
                  Resolution with Quality Guarantees},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {347--359},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2883532},
	doi = {10.1109/TKDE.2018.2883532},
	timestamp = {Tue, 23 Feb 2021 12:00:08 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HouCCNL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Even though many approaches have been proposed for entity resolution (ER), it remains very challenging to enforce quality guarantees. To this end, we propose a risk-aware HUman-Machine cOoperation framework for ER, denoted by r-HUMO. Built on the existing HUMO framework, r-HUMO similarly enforces both precision and recall guarantees by partitioning an ER workload between the human and the machine. However, r-HUMO is the first solution that optimizes the process of human workload selection from a risk perspective. It iteratively selects human workload by real-time risk analysis based on the human-labeled results as well as the prespecified machine metric. In this paper, we first introduce the r-HUMO framework and then present the risk model to prioritize the instances for manual inspection. Finally, we empirically evaluate r-HUMO's performance on real data. Our extensive experiments show that r-HUMO is effective in enforcing quality guarantees, and compared with the state-of-the-art alternatives, it can achieve desired quality control with reduced human cost.}
}


@article{DBLP:journals/tkde/TsalouchidouBMB20,
	author = {Ioanna Tsalouchidou and
                  Francesco Bonchi and
                  Gianmarco De Francisci Morales and
                  Ricardo Baeza{-}Yates},
	title = {Scalable Dynamic Graph Summarization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {360--373},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2884471},
	doi = {10.1109/TKDE.2018.2884471},
	timestamp = {Mon, 28 Aug 2023 21:37:39 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TsalouchidouBMB20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale dynamic interaction graphs can be challenging to process and store, due to their size and the continuous change of communication patterns between nodes. In this work, we address the problem of summarizing large-scale dynamic graphs, while maintaining the evolution of their structure and interactions. Our approach is based on grouping the nodes of the graph in supernodes according to their connectivity and communication patterns. The resulting summary graph preserves the information about the evolution of the graph within a time window. We propose two online algorithms for summarizing this type of graphs. Our baseline algorithm kC based on clustering is fast but rather memory expensive. The second method we propose, named /LC, reduces the memory requirements by introducing an intermediate step that keeps statistics of the clustering of the previous rounds. Our algorithms are distributed by design, and we implement them over the Apache Spark framework, so as to address the problem of scalability for large-scale graphs and massive streams. We apply our methods to several dynamic graphs, and show that we can efficiently use the summary graphs to answer temporal and probabilistic graph queries.}
}


@article{DBLP:journals/tkde/SunRMWLXWZ20,
	author = {Xu Sun and
                  Xuancheng Ren and
                  Shuming Ma and
                  Bingzhen Wei and
                  Wei Li and
                  Jingjing Xu and
                  Houfeng Wang and
                  Yi Zhang},
	title = {Training Simplification and Model Simplification for Deep Learning
                  : {A} Minimal Effort Back Propagation Method},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {374--387},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2883613},
	doi = {10.1109/TKDE.2018.2883613},
	timestamp = {Tue, 12 Nov 2024 16:30:42 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunRMWLXWZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-\nk\nelements (in terms of magnitude) are kept. As a result, only\nk\nrows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost. Based on the sparsified gradients, we further simplify the model by eliminating the rows or columns that are seldom updated, which will reduce the computational cost both in the training and decoding, and potentially accelerate decoding in real-world applications. Surprisingly, experimental results demonstrate that most of the time we only need to update fewer than 5 percent of the weights at each back propagation pass. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x, without any loss on accuracy or even with improved accuracy.}
}


@article{DBLP:journals/tkde/YeoPLLH20,
	author = {Jinyoung Yeo and
                  Haeju Park and
                  Sanghoon Lee and
                  Eric Wonhee Lee and
                  Seung{-}won Hwang},
	title = {{XINA:} Explainable Instance Alignment Using Dominance Relationship},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {388--401},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2881956},
	doi = {10.1109/TKDE.2018.2881956},
	timestamp = {Tue, 18 Feb 2020 15:26:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YeoPLLH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past few years, knowledge bases (KBs) like DBPedia, Freebase, and YAGO have accumulated a massive amount of knowledge from web data. Despite their seemingly large size, however, individual KBs often lack comprehensive information on any given domain. For example, over 70 percent of people on Freebase lack information on place of birth. For this reason, the complementary nature across different KBs motivates their integration through a process of aligning instances. Meanwhile, since application-level machine systems, such as medical diagnosis, have heavily relied on KBs, it is necessary to provide users with trustworthy reasons why the alignment decisions are made. To address this problem, we propose a new paradigm, explainable instance alignment (XINA), which provides user-understandable explanations for alignment decisions. Specifically, given an alignment candidate, XINA replaces existing scalar representation of an aggregated score, by decision and explanation-vector spaces for machine decision and user understanding, respectively. To validate XINA, we perform extensive experiments on real-world KBs and show that XINA achieves comparable performance with state-of-the-arts, even with far less human effort.}
}


@article{DBLP:journals/tkde/Vilenchik20,
	author = {Dan Vilenchik},
	title = {Simple Statistics Are Sometime Too Simple: {A} Case Study in Social
                  Media Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {2},
	pages = {402--408},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2899355},
	doi = {10.1109/TKDE.2019.2899355},
	timestamp = {Thu, 06 Feb 2020 18:12:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/Vilenchik20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work we ask to which extent are simple statistics useful to make sense of social media data. By simple statistics we mean counting and bookkeeping type features such as the number of likes given to a user's post, a user's number of friends, etc. We find that relying solely on simple statistics is not always a good approach. Specifically, we develop a statistical framework that we term semantic shattering which allows to detect semantic inconsistencies in the data that may occur due to relying solely on simple statistics. We apply our framework to simple-statistics data collected from six online social media platforms and arrive at a surprising counter-intuitive finding in three of them, Twitter, Instagram and YouTube. We find that overall, the activity of the user is not correlated with the feedback that the user receives on that activity. A hint to understand this phenomenon may be found in the fact that the activity-feedback shattering did not occur in LinkedIn, Steam and Flickr. A possible explanation for this separation is the amount of effort required to produce content. The lesser the effort the lesser the correlation between activity and feedback. The amount of effort may be a proxy to the level of commitment that the users feel towards each other in the network, and indeed sociologists claim that commitment explains consistent human behavior, or lack thereof. However, the amount of effort or the level of commitment are by no means a simple statistic.}
}


@article{DBLP:journals/tkde/ZhangWWX20,
	author = {Yong Zhang and
                  Jiacheng Wu and
                  Jin Wang and
                  Chunxiao Xing},
	title = {A Transformation-Based Framework for {KNN} Set Similarity Search},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {409--423},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2886189},
	doi = {10.1109/TKDE.2018.2886189},
	timestamp = {Thu, 14 Oct 2021 08:57:04 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangWWX20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Set similarity search is a fundamental operation in a variety of applications. While many previous studies focus on threshold based set similarity search and join, few efforts have been paid for KNN set similarity search. In this paper, we propose a transformation based framework to solve the problem of KNN set similarity search, which given a collection of set records and a query set, returns\nk\nresults with the largest similarity to the query. We devise an effective transformation mechanism to transform sets with various lengths to fixed length vectors which can map similar sets closer to each other. Then, we index such vectors with a tiny tree structure. Next, we propose efficient search algorithms and pruning strategies to perform exact KNN set similarity search. We also design an estimation technique by leveraging the data distribution to support approximate KNN search, which can speed up the search while retaining high recall. Experimental results on real world datasets show that our framework significantly outperforms state-of-the-art methods in both memory and disk based settings.}
}


@article{DBLP:journals/tkde/ByunWK20,
	author = {Jaewook Byun and
                  Sungpil Woo and
                  Daeyoung Kim},
	title = {ChronoGraph: Enabling Temporal Graph Traversals for Efficient Information
                  Diffusion Analysis over Time},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {424--437},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2891565},
	doi = {10.1109/TKDE.2019.2891565},
	timestamp = {Mon, 28 Aug 2023 21:37:39 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ByunWK20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {ChronoGraph is a novel system enabling temporal graph traversals. Compared to snapshot-oriented systems, this traversal-oriented system is suitable for analyzing information diffusion over time without violating a time constraint on temporal paths. The cornerstone of ChronoGraph aims at bridging the chasm between point-based semantics and period-based semantics and the gap between temporal graph traversals and static graph traversals. Therefore, our graph model and traversal language provide the temporal syntax for both semantics, and we present a method converting point-based semantics to period-based ones. Also, ChronoGraph exploits the temporal support and parallelism to handle the temporal degree, which explosively increases compared to static graphs. We demonstrate how three traversal recipes can be implemented on top of our system: temporal breadth-first search (tBFS), temporal depth-first search (tDFS), and temporal single source shortest path (tSSSP). According to our evaluation, our temporal support and parallelism enhance temporal graph traversals in terms of convenience and efficiency. Also, ChronoGraph outperforms existing property graph databases in terms of temporal graph traversals. We prototype ChronoGraph by extending Tinkerpop, a de facto standard for property graphs. Therefore, we expect that our system would be readily accessible to existing property graph users.}
}


@article{DBLP:journals/tkde/RossiZA20,
	author = {Ryan A. Rossi and
                  Rong Zhou and
                  Nesreen K. Ahmed},
	title = {Deep Inductive Graph Representation Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {438--452},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2878247},
	doi = {10.1109/TKDE.2018.2878247},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/RossiZA20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a general inductive graph representation learning framework called\nDeepGL\nfor learning deep node and edge features that generalize across-networks. In particular,\nDeepGL\nbegins by deriving a set of base features from the graph (e.g., graphlet features) and automatically learns a multi-layered hierarchical graph representation where each successive layer leverages the output from the previous layer to learn features of a higher-order. Contrary to previous work,\nDeepGL\nlearns relational functions (each representing a feature) that naturally generalize across-networks and are therefore useful for graph-based transfer learning tasks. Moreover,\nDeepGL\nnaturally supports attributed graphs, learns interpretable inductive graph representations, and is space-efficient (by learning sparse feature vectors). In addition,\nDeepGL\nis expressive, flexible with many interchangeable components, efficient with a time complexity of\nO(|E|)\n, and scalable for large networks via an efficient parallel implementation. Compared with recent methods,\nDeepGL\nis (1) effective for across-network transfer learning tasks and large (attributed) graphs, (2) space-efficient requiring up to 6x less memory, (3) fast with up to 106x speedup in runtime performance, and (4) accurate with an average improvement in AUC of 20 percent or more on many learning tasks and across a wide variety of networks.}
}


@article{DBLP:journals/tkde/QinZCHZ20,
	author = {Dong Qin and
                  Xiangmin Zhou and
                  Lei Chen and
                  Guangyan Huang and
                  Yanchun Zhang},
	title = {Dynamic Connection-Based Social Group Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {453--467},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2879658},
	doi = {10.1109/TKDE.2018.2879658},
	timestamp = {Tue, 03 Mar 2020 09:37:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/QinZCHZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Group recommendation has become highly demanded when users communicate in the forms of group activities in online sharing communities. These group activities include student group study, family TV program watching, friends travel decision, etc. Existing group recommendation techniques mainly focus on the small user groups. However, online sharing communities have enabled group activities among thousands of users. Accordingly, recommendation over large groups has become urgent. In this paper, we propose a new framework to accomplish this goal by exploring the group interests and the connections between group users. We first divide a big group into different interest subgroups, each of which contains users closely connected with each other and sharing the similar interests. Then, for each interest subgroup, our framework exploits the connections between group users to collect a comparably compact potential candidate set of media-user pairs, on which the collaborative filtering is performed to generate an interest subgroup-based recommendation list. After that, a novel aggregation function is proposed to integrate the recommended media lists of all interest subgroups as the final group recommendation results. Extensive experiments have been conducted on two real social media datasets to demonstrate the effectiveness and efficiency of our proposed approach.}
}


@article{DBLP:journals/tkde/ZhangZSQ20,
	author = {Junbo Zhang and
                  Yu Zheng and
                  Junkai Sun and
                  Dekang Qi},
	title = {Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {468--478},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2891537},
	doi = {10.1109/TKDE.2019.2891537},
	timestamp = {Sun, 06 Oct 2024 21:41:30 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZSQ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting flows (e.g., the traffic of vehicles, crowds, and bikes), consisting of the in-out traffic at a node and transitions between different nodes, in a spatio-temporal network plays an important role in transportation systems. However, this is a very challenging problem, affected by multiple complex factors, such as the spatial correlation between different locations, temporal correlation among different time intervals, and external factors (like events and weather). In addition, the flow at a node (called node flow) and transitions between nodes (edge flow) mutually influence each other. To address these issues, we propose a multitask deep-learning framework that simultaneously predicts the node flow and edge flow throughout a spatio-temporal network. Based on fully convolutional networks, our approach designs two sophisticated models for predicting node flow and edge flow, respectively. These two models are connected by coupling their latent representations of middle layers, and trained together. The external factor is also integrated into the framework through a gating fusion mechanism. In the edge flow prediction model, we employ an embedding component to deal with the sparse transitions between nodes. We evaluate our method based on the taxicab data in Beijing and New York City. Experimental results show the advantages of our method beyond 11 baselines, such as ConvLSTM, CNN, and Markov Random Field.}
}


@article{DBLP:journals/tkde/KimSBKS20,
	author = {Sungwook Kim and
                  Hyejin Shin and
                  Chung Hun Baek and
                  Soohyung Kim and
                  Junbum Shin},
	title = {Learning New Words from Keystroke Data with Local Differential Privacy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {479--491},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2885749},
	doi = {10.1109/TKDE.2018.2885749},
	timestamp = {Tue, 03 Mar 2020 09:37:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/KimSBKS20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Keystroke data collected from smart devices includes various sensitive information about users. Collecting and analyzing such data raise serious privacy concerns. Google and Apple have recently applied local differential privacy (LDP) to address privacy issue on learning new words from users’ keystroke data. However, these solutions require multiple LDP reports for a single word, which result in inefficient use of privacy budget and high computational cost. In this paper, we develop a novel algorithm for learning new words under LDP. Unlike the existing solutions, the proposed method generates only one LDP report for a single word. This enables the proposed method to use full privacy budget for generating a report and brings the benefit that the proposed method provides better utility at the same privacy degree than the existing methods. In our algorithm, each user appends a hash value to new word and sends only one LDP report of an\nn\n-gram selected randomly from the string packed by each new word and its hash value. The server then decodes frequent\nn\n-grams at each position of the string and discovers the candidate words by exploring graph-theoretic links between\nn\n-grams and checking integrity of candidates with hash values. Frequencies of frequent new words discovered are estimated from distribution estimates of\nn\n-grams by robust regression. We theoretically show that our algorithm can recover popular new words even though the server does not know the domain of the raw data. In addition, we theoretically and empirically demonstrate that our algorithm achieves higher accuracy compared to the existing solutions.}
}


@article{DBLP:journals/tkde/DuY000020,
	author = {Xingzhong Du and
                  Hongzhi Yin and
                  Ling Chen and
                  Yang Wang and
                  Yi Yang and
                  Xiaofang Zhou},
	title = {Personalized Video Recommendation Using Rich Contents from Videos},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {492--505},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2885520},
	doi = {10.1109/TKDE.2018.2885520},
	timestamp = {Wed, 27 Oct 2021 12:33:55 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DuY000020.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video recommendation has become an essential way of helping people explore the massive videos and discover the ones that may be of interest to them. In the existing video recommender systems, the models make the recommendations based on the user-video interactions and single specific content features. When the specific content features are unavailable, the performance of the existing models will seriously deteriorate. Inspired by the fact that rich contents (e.g., text, audio, motion, and so on) exist in videos, in this paper, we explore how to use these rich contents to overcome the limitations caused by the unavailability of the specific ones. Specifically, we propose a novel general framework that incorporates arbitrary single content feature with user-video interactions, named as collaborative embedding regression (CER) model, to make effective video recommendation in both in-matrix and out-of-matrix scenarios. Our extensive experiments on two real-world large-scale datasets show that CER beats the existing recommender models with any single content feature and is more time efficient. In addition, we propose a priority-based late fusion (PRI) method to gain the benefit brought by the integrating the multiple content features. The corresponding experiment shows that PRI brings real performance improvement to the baseline and outperforms the existing fusion methods.}
}


@article{DBLP:journals/tkde/ZhangHTC20,
	author = {Chenzi Zhang and
                  Shuguang Hu and
                  Zhihao Gavin Tang and
                  T.{-}H. Hubert Chan},
	title = {Re-Revisiting Learning on Hypergraphs: Confidence Interval, Subgradient
                  Method, and Extension to Multiclass},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {506--518},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2880448},
	doi = {10.1109/TKDE.2018.2880448},
	timestamp = {Tue, 03 Mar 2020 09:37:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangHTC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We revisit semi-supervised learning on hypergraphs. Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable. We exploit the non-uniqueness of the optimal solutions, and consider confidence intervals which give the exact ranges that unlabeled vertices take in any optimal solution. Moreover, we give a much simpler approach for solving the convex program based on the subgradient method. Our experiments on real-world datasets confirm that our confidence interval approach on hypergraphs outperforms existing methods, and our subgradient method gives faster running times when the number of vertices is much larger than the number of edges. Our experiments also support that using directed hypergraphs to capture causal relationships can improve the prediction accuracy. Furthermore, our model can be readily extended to capture multiclass learning.}
}


@article{DBLP:journals/tkde/JinYWYZ20,
	author = {Peiquan Jin and
                  Chengcheng Yang and
                  Xiaoliang Wang and
                  Lihua Yue and
                  Dezhi Zhang},
	title = {SAL-Hashing: {A} Self-Adaptive Linear Hashing Index for SSDs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {519--532},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2884714},
	doi = {10.1109/TKDE.2018.2884714},
	timestamp = {Tue, 03 Mar 2020 09:37:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/JinYWYZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Flash memory based solid state drives (SSDs) have emerged as a new alternative to replace magnetic disks due to their high performance and low power consumption. However, random writes on SSDs are much slower than SSD reads. Therefore, traditional index structures, which are designed based on the symmetrical I/O property of magnetic disks, cannot completely exert the high performance of SSDs. In this paper, we propose an SSD-optimized linear hashing index called Self-Adaptive Linear Hashing ( SAL-hashing ) to reduce small random-writes to SSDs that are caused by index operations. The contributions of our work are manifold. First, we propose to organize the buckets of a linear hashing index into groups and sets to facilitate coarse-grained writes and adaptivity to access patterns. A group consisting of a fixed number of buckets is proposed to transform small random writes to buckets into coarse-grained writes and in turn improve write performance of the index. A set consists of a number of groups, and we propose to employ different split strategies for each set. With this mechanism, SAL-hashing is able to adapt to the changes of access patterns. Second, we attach a log region to each set, and amortize the cost of reads and writes by committing updates to the log region in batch. Third, in order to reduce search cost, each log region is equipped with Bloom filters to index update logs. We devise a cost-based online algorithm to adaptively merge the log region with the corresponding set when the set becomes search-intensive. Fourth, we propose a new technique called virtual split to optimize the search performance of SAL-hashing . Finally, we propose a new scheme for the management of the log buffer. We conduct extensive experiments on real SSDs. The results suggest that our proposal is self-adaptive according to the change of access patterns, and outperforms several competitors under various workloads.}
}


@article{DBLP:journals/tkde/FangWWHZ20,
	author = {Zhuhe Fang and
                  Chuliang Weng and
                  Li Wang and
                  Huiqi Hu and
                  Aoying Zhou},
	title = {Scheduling Resources to Multiple Pipelines of One Query in a Main
                  Memory Database Cluster},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {533--546},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2884724},
	doi = {10.1109/TKDE.2018.2884724},
	timestamp = {Tue, 03 Mar 2020 09:37:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/FangWWHZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To fully utilize the resources of a main memory database cluster, we additionally take the independent parallelism into account to parallelize multiple pipelines of one query. However, scheduling resources to multiple pipelines is an intractable problem. Traditional static approaches to this problem may lead to a serious waste of resources and suboptimal execution order of pipelines, because it is hard to predict the actual data distribution and fluctuating workloads at compile time. In response, we propose a dynamic scheduling algorithm, List with Filling and Preemption (LFPS), based on two novel techniques. (1) Adaptive filling improves resource utilization by issuing more extra pipelines to adaptively fill idle resource “holes” during execution. (2) Rank-based preemption strictly guarantees scheduling the pipelines on the critical path first at run time. Interestingly, the latter facilitates the former filling idle “holes” with best efforts to finish multiple pipelines as soon as possible. We implement LFPS in our prototype database system. Under the workloads of TPC-H, experiments show our work improves the finish time of parallelizable pipelines from one query up to 2.5X than a static approach and 2.1X than a serialized execution.}
}


@article{DBLP:journals/tkde/WuCZCLM20,
	author = {Bo Wu and
                  Wen{-}Huang Cheng and
                  Yongdong Zhang and
                  Juan Cao and
                  Jintao Li and
                  Tao Mei},
	title = {Unlocking Author Power: On the Exploitation of Auxiliary Author-Retweeter
                  Relations for Predicting Key Retweeters},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {547--559},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2889664},
	doi = {10.1109/TKDE.2018.2889664},
	timestamp = {Tue, 28 May 2024 11:16:25 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuCZCLM20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Retweeting is a powerful driving force in information propagation on microblogging sites. However, identifying the most effective retweeters of a message (called the ”key retweeter prediction” problem) has become a significant research topic. Conventional approaches have addressed this topic from two main aspects: by analyzing either the personal attributes of microblogging users or the structures of user graph networks. However, according to sociological findings, author-retweeter dependencies also play a crucial role in influencing message propagation. In this paper, we propose a novel model to solve the key retweeter prediction problem by incorporating the auxiliary relations between a tweet author and potential retweeters. Without loss of generality, we formulate the relations from four relational factors: status relation, temporal relation, locational relation, and interactive relation. In addition, we propose a novel method, called “Relation-based Learning to Rank (RL2R),” to determine the key retweeters for a given tweet by ranking the potential retweeters in terms of their spreadability. The experimental results show that our method outperforms the state-of-the-art algorithms at top-k retweeter prediction, achieving a significant relative average improvement of 19.7-29.4 percent. These findings provide new insights for understanding user behaviors on social media for key retweeter prediction purposes.}
}


@article{DBLP:journals/tkde/HanYZS20,
	author = {Lei Han and
                  Jian Yang and
                  Weiliang Zhao and
                  Quan Z. Sheng},
	title = {User Interface Derivation for Business Processes},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {560--573},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2891655},
	doi = {10.1109/TKDE.2019.2891655},
	timestamp = {Tue, 03 Mar 2020 09:37:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HanYZS20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User Interfaces (UI) are the bridge to connect Business Processes (BPs) and end users. The implementation of UIs normally needs a lot of manual efforts of developers. Aiming to resolve this issue, this work proposes a UI derivation method with a role-enriched BP (REBP) model as its foundation. This process model has the capability to present the details of task control flow and data operations in tasks. A set of control flow patterns and data operation patterns is identified. For each participant role, tasks of a process are abstracted and aggregated, then data relationships are extracted according to the identified control flow patterns and data operation patterns. A set of mandatory and recommended rules has been developed for deriving the UI logic from a BP. The solution for the UI derivation has been provided and implemented in the prototype. This proposed UI derivation method can provide help for the analysis, design, and maintenance of UI components of BPs.}
}


@article{DBLP:journals/tkde/YanLZZ20,
	author = {Surong Yan and
                  Kwei{-}Jay Lin and
                  Xiaolin Zheng and
                  Wenyu Zhang},
	title = {Using Latent Knowledge to Improve Real-Time Activity Recognition for
                  Smart IoT},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {574--587},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2891659},
	doi = {10.1109/TKDE.2019.2891659},
	timestamp = {Fri, 12 Feb 2021 09:44:45 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/YanLZZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time/online activity recognition (AR) is an important technology in smart Internet of Things (IoT) systems where users are assisted by smart devices in their daily activities. How to generate appropriate feature representation from sensor event streaming is a challenging issue for accurate and efficient real-time AR. Previous AR models that rely on explicit domain knowledge are not appropriate for online recognition of complex human activities. We propose to use unsupervised learning to learn about the latent knowledge and embed the activity probability distribution prediction as high-level features to boost real-time AR performance. The proposed approach first learns the latent knowledge from explicit-activity window sequences using unsupervised learning, and derives the probability distribution prediction of activity classes for a given sliding window. Our approach then feeds the prediction with other basic features of the sliding window into a classifier to produce the final class result on each event-count sliding window. Experiments on five smart home datasets show that the proposed method achieves a higher accuracy by at least 20 percent improvement on F1_score than previous traditional algorithms, while maintaining a lower time cost than deep learning based methods. An analysis on the feature importance shows that the addition of probability distribution prediction about activity classes leads to a promising direction for real-time AR.}
}


@article{DBLP:journals/tkde/TrotzekKF20,
	author = {Marcel Trotzek and
                  Sven Koitka and
                  Christoph M. Friedrich},
	title = {Utilizing Neural Networks and Linguistic Metadata for Early Detection
                  of Depression Indications in Text Sequences},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {588--601},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2885515},
	doi = {10.1109/TKDE.2018.2885515},
	timestamp = {Mon, 28 Aug 2023 21:37:41 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TrotzekKF20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Depression is ranked as the largest contributor to global disability and is also a major reason for suicide. Still, many individuals suffering from forms of depression are not treated for various reasons. Previous studies have shown that depression also has an effect on language usage and that many depressed individuals use social media platforms or the internet in general to get information or discuss their problems. This paper addresses the early detection of depression using machine learning models based on messages on a social platform. In particular, a convolutional neural network based on different word embeddings is evaluated and compared to a classification based on user-level linguistic metadata. An ensemble of both approaches is shown to achieve state-of-the-art results in a current early detection task. Furthermore, the currently popular ERDE score as metric for early detection systems is examined in detail and its drawbacks in the context of shared tasks are illustrated. A slightly modified metric is proposed and compared to the original score. Finally, a new word embedding was trained on a large corpus of the same domain as the described task and is evaluated as well.}
}


@article{DBLP:journals/tkde/LiuZPBQ20,
	author = {Xianying Liu and
                  Qiang Zhu and
                  Sakti Pramanik and
                  C. Titus Brown and
                  Gang Qian},
	title = {VA-Store: {A} Virtual Approximate Store Approach to Supporting Repetitive
                  Big Data in Genome Sequence Analyses},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {3},
	pages = {602--616},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2018.2885952},
	doi = {10.1109/TKDE.2018.2885952},
	timestamp = {Tue, 03 Mar 2020 09:37:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiuZPBQ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, we have witnessed an increasing demand to process big data in numerous applications. It is observed that there often exist substantial amounts of repetitive data in different portions of a big data repository/dataset for applications such as genome sequence analyses. In this paper, we present a novel method, called the VA-Store, to reduce the large space requirement for repetitive data in prevailing genome sequence analysis tasks using k-mers (i.e., subsequences of length k) with multiple k values. The VA-Store maintains a physical store for one portion of the input dataset (i.e., k 0 -mers) and supports multiple virtual stores for other portions of the dataset (i.e., k-mers with k ≠ k 0 ). Utilizing important relationships among repetitive data, the VA-Store transforms a given query on a virtual store into one or more queries on the physical store for execution. Both precise and approximate transformations are considered. Accuracy estimation models for approximate solutions are derived. Query optimization strategies are suggested to improve query performance. Our experiments using real and synthetic datasets demonstrate that the VA-Store is quite promising in providing effective storage and efficient query processing for solving a kernel database problem on repetitive big data for genome sequence analysis applications.}
}


@article{DBLP:journals/tkde/MaSLW20,
	author = {Lerong Ma and
                  Dandan Song and
                  Lejian Liao and
                  Jingang Wang},
	title = {A Hybrid Discriminative Mixture Model for Cumulative Citation Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {617--630},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893328},
	doi = {10.1109/TKDE.2019.2893328},
	timestamp = {Thu, 19 Mar 2020 10:22:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MaSLW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper explores Cumulative Citation Recommendation (CCR) for Knowledge Base Acceleration (KBA). The CCR task aims to detect potential citations of a set of target entities with priorities from a volume of temporally-ordered stream corpus. Previous approaches for CCR that build an individual relevance model for each entity fail to deal with unseen entities without annotation. A compromised solution is to build a global entity-unspecific model for all entities without respect to the relationship information among entities, which cannot guarantee achieving a satisfactory result for each entity. Moreover, most previous methods can not adequately exploit prior knowledge embedded in entities or documents due to considering all kinds of features indifferently. In this paper, we propose a novel entity and document class-dependent discriminative mixture model by introducing one intermediate layer to model the correlation between entity-document pairs and hybrid latent entity-document classes. The model can better adjust to different types of entities and documents, and achieve better performance when dealing with a broad range of entity and document classes. An extensive set of experiments has been conducted on two offical datasets, and the experimental results demonstrate that the proposed model can achieve the state-of-the-art performance.}
}


@article{DBLP:journals/tkde/ZhuLHWGLC20,
	author = {Yu Zhu and
                  Jinghao Lin and
                  Shibi He and
                  Beidou Wang and
                  Ziyu Guan and
                  Haifeng Liu and
                  Deng Cai},
	title = {Addressing the Item Cold-Start Problem by Attribute-Driven Active
                  Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {631--644},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2891530},
	doi = {10.1109/TKDE.2019.2891530},
	timestamp = {Tue, 05 Sep 2023 17:29:06 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuLHWGLC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recommender systems, cold-start issues are situations where no previous events, e.g., ratings, are known for certain users or items. In this paper, we focus on the item cold-start problem. Both content information (e.g., item attributes) and initial user ratings are valuable for seizing users' preferences on a new item. However, previous methods for the item cold-start problem either (1) incorporate content information into collaborative filtering to perform hybrid recommendation, or (2) actively select users to rate the new item without considering content information and then do collaborative filtering. In this paper, we propose a novel recommendation scheme for the item cold-start problem by leveraging both active learning and items' attribute information. Specifically, we design useful user selection criteria based on items' attributes and users' rating history, and combine the criteria in an optimization framework for selecting users. By exploiting the feedback ratings, users' previous ratings and items' attributes, we then generate accurate rating predictions for the other unselected users. Experimental results on two real-world datasets show the superiority of our proposed method over traditional methods.}
}


@article{DBLP:journals/tkde/MaHWLH20,
	author = {Shuai Ma and
                  Renjun Hu and
                  Luoshu Wang and
                  Xuelian Lin and
                  Jinpeng Huai},
	title = {An Efficient Approach to Finding Dense Temporal Subgraphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {645--658},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2891604},
	doi = {10.1109/TKDE.2019.2891604},
	timestamp = {Thu, 19 Mar 2020 10:22:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MaHWLH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dense subgraph discovery has proven useful in various applications of temporal networks. We focus on a special class of temporal networks whose nodes and edges are kept fixed, but edge weights regularly vary with timestamps. However, finding dense subgraphs in temporal networks is non-trivial, and its state of the art solution uses a filter-and-verification framework that is not scalable on large temporal networks. In this study, we propose a highly efficient approach to finding dense subgraphs in large temporal networks with\nT\ntimestamps. (1) We first develop a statistics-driven approach that employs hidden statistics to identifying\nk\ntime intervals, instead of\nT(T+1)/2\nones (\nk\nis typically much smaller than\nT\n), which strikes a balance between quality and efficiency. (2) After proving that the problem has no constant factor approximation algorithms, we design better heuristic algorithms to attack the problem, by connecting finding dense subgraphs with a variant of the Prize Collecting Steiner Tree problem. (3) Finally, we have conducted an extensive experimental study to verify that our approach is both effective and efficient.}
}


@article{DBLP:journals/tkde/ZhangWSZP20,
	author = {Huaqing Zhang and
                  Jian Wang and
                  Zhanquan Sun and
                  Jacek M. Zurada and
                  Nikhil R. Pal},
	title = {Feature Selection for Neural Networks Using Group Lasso Regularization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {659--673},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893266},
	doi = {10.1109/TKDE.2019.2893266},
	timestamp = {Tue, 18 May 2021 16:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangWSZP20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose an embedded/integrated feature selection method based on neural networks with Group Lasso penalty. Group Lasso regularization is considered to produce sparsity on the inputs to the network, i.e., for selection of useful features. Lasso based feature selection using a multi-layer perceptron usually requires an additional set of weights, while our Group Lasso formulation does not require that. However, Group Lasso penalty is non-differentiable at the origin. This may lead to oscillations in numerical simulations and make it difficult to analyze theoretically. To address this issue, four smoothing Group Lasso penalties are introduced. A rigorous proof for the convergence of the proposed algorithm is presented under suitable assumptions. To verify the effectiveness, a three-step algorithmic architecture is adopted in implementation. Experimental results on several datasets validate the theoretical results and demonstrate the competitive performance of the proposed method.}
}


@article{DBLP:journals/tkde/DuMZL20,
	author = {Yulu Du and
                  Xiangwu Meng and
                  Yujie Zhang and
                  Pengtao Lv},
	title = {{GERF:} {A} Group Event Recommendation Framework Based on Learning-to-Rank},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {674--687},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893361},
	doi = {10.1109/TKDE.2019.2893361},
	timestamp = {Thu, 19 Mar 2020 10:22:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/DuMZL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Event recommendation is an essential means to enable people to find attractive upcoming social events, such as party, exhibition, and concert. While growing line of research has focused on suggesting events to individuals, making event recommendation for a group of users has not been well studied. In this paper, we aim to recommend upcoming events for a group of users. We formalize group recommendation as a ranking problem and propose a group event recommendation framework GERF based on learning-to-rank technique. Specifically, we first analyze different contextual influences on user's event attendance, and extract preference of user to event considering each contextual influence. Then, the preference scores of the users in a group are taken as the features for learningto-rank to model the preference of the group. Moreover, a fast pairwise learning-to-rank algorithm, Bayesian group ranking, is proposed to learn ranking model for each group. Our framework is easily to incorporate additional contextual influences, and can be applied to other group recommendation scenarios. Extensive experiments have been conducted to evaluate the performance of GERF on two real-world datasets and demonstrate the appealing performance of our method on both accuracy and time efficiency.}
}


@article{DBLP:journals/tkde/GaoXHLWL20,
	author = {Yang Gao and
                  Yue Xu and
                  Heyan Huang and
                  Qian Liu and
                  Linjing Wei and
                  Luyang Liu},
	title = {Jointly Learning Topics in Sentence Embedding for Document Summarization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {688--699},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2892430},
	doi = {10.1109/TKDE.2019.2892430},
	timestamp = {Thu, 15 Feb 2024 11:39:27 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GaoXHLWL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Summarization systems for various applications, such as opinion mining, online news services, and answering questions, have attracted increasing attention in recent years. These tasks are complicated, and a classic representation using bag-of-words does not adequately meet the comprehensive needs of applications that rely on sentence extraction. In this paper, we focus on representing sentences as continuous vectors as a basis for measuring relevance between user needs and candidate sentences in source documents. Embedding models based on distributed vector representations are often used in the summarization community because, through cosine similarity, they simplify sentence relevance when comparing two sentences or a sentence/query and a document. However, the vector-based embedding models do not typically account for the salience of a sentence, and this is a very necessary part of document summarization. To incorporate sentence salience, we developed a model, called CCTSenEmb, that learns latent discriminative Gaussian topics in the embedding space and extended the new framework by seamlessly incorporating both topic and sentence embedding into one summarization system. To facilitate the semantic coherence between sentences in the framework of prediction-based tasks for sentence embedding, the CCTSenEmb further considers the associations between neighboring sentences. As a result, this novel sentence embedding framework combines sentence representations, word-based content, and topic assignments to predict the representation of the next sentence. A series of experiments with the DUC datasets validate CCTSenEmb's efficacy in document summarization in a query-focused extraction-based setting and an unsupervised ILP-based setting.}
}


@article{DBLP:journals/tkde/ZhengC20,
	author = {Libin Zheng and
                  Lei Chen},
	title = {Multi-Campaign Oriented Spatial Crowdsourcing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {700--713},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893293},
	doi = {10.1109/TKDE.2019.2893293},
	timestamp = {Mon, 28 Aug 2023 21:37:39 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhengC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, spatial crowdsourcing has been drawing increasing attention with its great potential in collecting geographical knowledge. The system throughput (number of assigned tasks) and workers' travel distance are two of many important factors in spatial crowdsourcing, and the improvement to one of them usually means the sacrifice of the other. However, most existing works resolve the trade-off between these two factors by simply targeting tasks within a bounding circle of each worker. In this paper, we compromise between the throughput and the distance by formulating these two factors as score terms in the objective function. This flexible formulation has the advantages of abandoning distant tasks and minimizing workers' travel distance for reachable tasks. Aside from that, we study the multi-campaign scenario of spatial crowdsourcing, which is not uncommon in practical applications while not yet discussed in existing works. The worker diversity of the campaigns is considered to be another goal and formulated as another score term in the objective function. Subsequently, the problem of multi-campaign oriented spatial crowdsourcing is to maximize the objective function comprised by the aforementioned score terms. We prove that the problem is NP-hard, thus, we propose several approximation solutions. Extensive experiments have been conducted to confirm the effectiveness and the efficiency of the devised solutions.}
}


@article{DBLP:journals/tkde/LiuLHLXP20,
	author = {Zheli Liu and
                  Bo Li and
                  Yanyu Huang and
                  Jin Li and
                  Yang Xiang and
                  Witold Pedrycz},
	title = {NewMCOS: Towards a Practical Multi-Cloud Oblivious Storage Scheme},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {714--727},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2891581},
	doi = {10.1109/TKDE.2019.2891581},
	timestamp = {Fri, 31 Jul 2020 08:40:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLHLXP20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Encryption alone is not enough to protect data privacy, because access pattern leaks some sensitive information. Oblivious RAM (ORAM), the solution to this problem, is still far from practical deployment for heavy storage and communication/computation overhead. To reduce them, an insightful idea was proposed to utilize non-colluding clouds to shift client computation and client-cloud communication to the clouds. The proposed multi-cloud ORAM achieved O(1) client-cloud bandwidth cost and removed most of client computation. In this paper, we exploit “disconnected ORAMoperation” and design “two-layerencryption” to further reduce these overheads. Experiments show that our proposed scheme, NewMCOS, significantly reduces evict cache size from GB/MB to KB level with about 2-3 times lower response time and 20 percent savings in bandwidth for clouds, compared to other schemes. Theoretically speaking, we reduce evict cache size from O(√N) to O(ZK), where N is the number of real data blocks, K is the number of clouds (2 <; K <; <; √N ), and Z is the number of real blocks uploaded from the client for eviction. By employing “lazy eviction operation”, the write frequency is reduced by O(Z), the shuffling bandwidth cost is reduced by Ω(Z log Z). Meanwhile, NewMCOS is proved to be secure.}
}


@article{DBLP:journals/tkde/HuangCLTL20,
	author = {Qinghua Huang and
                  Yongdong Chen and
                  Longzhong Liu and
                  Dacheng Tao and
                  Xuelong Li},
	title = {On Combining Biclustering Mining and AdaBoost for Breast Tumor Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {728--738},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2891622},
	doi = {10.1109/TKDE.2019.2891622},
	timestamp = {Thu, 23 Jun 2022 20:04:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuangCLTL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Breast cancer is now considered as one of the leading causes of deaths among women all over the world. Aiming to assist clinicians in improving the accuracy of diagnostic decisions, computer-aided diagnosis (CAD) system is of increasing interest in breast cancer detection and analysis nowadays. In this paper, a novel computer-aided diagnosis scheme with human-in-the-loop is proposed to help clinicians identify the benign and malignant breast tumors in ultrasound. In this framework, feature acquisition is performed by a user-participated feature scoring scheme that is based on Breast Imaging Reporting and Data System (BI-RADS) lexicon and experience of doctors. Biclustering mining is then used as a useful tool to discover the column consistency patterns on the training data. The patterns frequently appearing in the tumors with the same label can be regarded as a potential diagnostic rule. Subsequently, the diagnostic rules are utilized to construct component classifiers of the Adaboost algorithm via a novel rules combination strategy which resolves the problem of classification in different feature spaces (PC-DFS). Finally, the AdaBoost learning is performed to discover effective combinations and integrate them into a strong classifier. The proposed approach has been validated using a large ultrasounic dataset of 1,062 breast tumor instances (including 418 benign cases and 644 malignant cases) and its performance was compared with several conventional approaches. The experimental results show that the proposed method yielded the best prediction performance, indicating a good potential in clinical applications.}
}


@article{DBLP:journals/tkde/Chen20,
	author = {Min Chen},
	title = {Reducing Web Page Complexity to Facilitate Effective User Navigation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {739--753},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893242},
	doi = {10.1109/TKDE.2019.2893242},
	timestamp = {Wed, 24 Feb 2021 10:59:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/Chen20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a website evolves to align with users' changing information needs and interests, its structure can outgrow the original design, accumulating links and pages in unanticipated places. This increases complexity to both web pages and the navigation structure, which could cause difficulty in locating relevant links and information. Though the increasing complexity of website and its impact on users' psychological perception have been anecdotally well-recognized, the need to address it with a formal and rigorous method is understudied in the literature. This paper is one of the first studies to examine how to streamline website structures to enhance navigation. We use a widely used metric in the literature - a page's outdegree (the number of links in a page) - as the measurement complexity, because it not only serves as a good proximity for page complexity but also has a significant implication on website structure. We propose a method based on mathematical programming (MP) model that can significantly reduce users' cognitive load by effectively eliminating appropriate links from pages with high complexity. We have performed extensive experiments on both a real dataset and very large synthetic datasets with statistical similarities to the real dataset. The results indicate that our method not only significantly reduces web page and structure complexity with very small impact to user navigation, but also can be effectively solved and scales up remarkably well, suggesting it is useful for website maintenance on a progressive basis. In addition, we conduct a study to evaluate the performance of streamlined website structures using the real dataset and the results confirm the validity of our method.}
}


@article{DBLP:journals/tkde/LierdeCC20,
	author = {Hadrien Van Lierde and
                  Tommy W. S. Chow and
                  Guanrong Chen},
	title = {Scalable Spectral Clustering for Overlapping Community Detection in
                  Large-Scale Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {754--767},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2892096},
	doi = {10.1109/TKDE.2019.2892096},
	timestamp = {Thu, 19 Mar 2020 10:22:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LierdeCC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While the majority of methods for community detection produce disjoint communities of nodes, most real-world networks naturally involve overlapping communities. In this paper, a scalable method for the detection of overlapping communities in large networks is proposed. The method is based on an extension of the notion of normalized cut to cope with overlapping communities. A spectral clustering algorithm is formulated to solve the related cut minimization problem. When available, the algorithm may take into account prior information about the likelihood for each node to belong to several communities. This information can either be extracted from the available metadata or from node centrality measures. We also introduce a hierarchical version of the algorithm to automatically detect the number of communities. In addition, a new benchmark model extending the stochastic blockmodel for graphs with overlapping communities is formulated. Our experiments show that the proposed spectral method outperforms the state-of-the-art algorithms in terms of computational complexity and accuracy on our benchmark graph model and on five real-world networks, including a lexical network and large-scale social networks. The scalability of the proposed algorithm is also demonstrated on large synthetic graphs with millions of nodes and edges.}
}


@article{DBLP:journals/tkde/RafieiD20,
	author = {Davood Rafiei and
                  Fan Deng},
	title = {Similarity Join and Similarity Self-Join Size Estimation in a Streaming
                  Environment},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {768--781},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893175},
	doi = {10.1109/TKDE.2019.2893175},
	timestamp = {Fri, 15 Mar 2024 12:30:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/RafieiD20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of similarity self-join and similarity join size estimation in a streaming setting where the goal is to estimate, in one scan of the input and with sublinear space in the input size, the number of record pairs that have a similarity within a given threshold. The problem has many applications in data cleaning and query plan generation, where the cost of a similarity join may be estimated before actually doing the join. On unary input where two records either match or don't match, the problem becomes join and self-join size estimation for which one-pass algorithms are readily available. Our work addresses the problem for d-ary input, for d ≥ 1, where the degree of similarity can vary from 1 to d. We show that our proposed algorithm gives an accurate estimate and scales well with the input size. We provide error bounds and time and space costs, and conduct an extensive experimental evaluation of our algorithm, comparing its estimation accuracy to a few competitors, including some multi-pass algorithms. Our results show that given the same space, the proposed algorithm has an order of magnitude less error for a large range of similarity thresholds.}
}


@article{DBLP:journals/tkde/XiaoMLZLZZ20,
	author = {Mingjun Xiao and
                  Kai Ma and
                  An Liu and
                  Hui Zhao and
                  Zhixu Li and
                  Kai Zheng and
                  Xiaofang Zhou},
	title = {{SRA:} Secure Reverse Auction for Task Assignment in Spatial Crowdsourcing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {782--796},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893240},
	doi = {10.1109/TKDE.2019.2893240},
	timestamp = {Mon, 28 Aug 2023 21:37:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XiaoMLZLZZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study a new type of spatial crowdsourcing, namely competitive detour tasking, where workers can make detours from their original travel paths to perform multiple tasks, and each worker is allowed to compete for preferred tasks by strategically claiming his/her detour costs. The objective is to make suitable task assignment by maximizing the social welfare of crowdsourcing systems and protecting workers' private sensitive information. We first model the task assignment problem as a reverse auction process. We formalize the winning bid selection of reverse auction as an n-to-one weighted bipartite graph matching problem with multiple 0-1 knapsack constraints. Since this problem is NP-hard, we design an approximation algorithm to select winning bids and determine corresponding payments. Based on this, a Secure Reverse Auction (SRA) protocol is proposed for this novel spatial crowdsourcing. We analyze the approximation performance of the proposed protocol and prove that it has some desired properties, including truthfulness, individual rationality, computational efficiency, and security. To the best of our knowledge, this is the first theoretically provable secure auction protocol for spatial crowdsourcing systems. In addition, we also conduct extensive simulations on a real trace to verify the performance of the proposed protocol.}
}


@article{DBLP:journals/tkde/LiCW20,
	author = {Xuelong Li and
                  Mulin Chen and
                  Qi Wang},
	title = {Adaptive Consistency Propagation Method for Graph Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {797--802},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2936195},
	doi = {10.1109/TKDE.2019.2936195},
	timestamp = {Mon, 28 Aug 2023 21:37:39 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiCW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph clustering plays an important role in data mining. Based on an input data graph, data points are partitioned into clusters. However, most existing methods keep the data graph fixed during the clustering procedure, so they are limited to exploit the implied data manifold and highly dependent on the initial graph construction. Inspired by the recent development on manifold learning, this paper proposes an Adaptive Consistency Propagation (ACP) method for graph clustering. In order to utilize the features captured from different perspectives, we further put forward the Multi-view version of the ACP model (MACP). The main contributions are threefold: (1) the manifold structure of input data is sufficiently exploited by propagating the topological connectivities between data points from near to far; (2) the optimal graph for clustering is learned by taking graph learning as a part of the optimization procedure; and (3) the negotiation among the heterogeneous features is captured by the multi-view clustering model. Extensive experiments on real-world datasets validate the effectiveness of the proposed methods on both single-and multi-view clustering, and show their superior performance over the state-of-the-arts.}
}


@article{DBLP:journals/tkde/ZhangSBG20,
	author = {Bohan Zhang and
                  Scott Sanner and
                  Mohamed Reda Bouadjenek and
                  Shagun Gupta},
	title = {Bayesian Networks for Data Integration in the Absence of Foreign Keys},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {803--808},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2940019},
	doi = {10.1109/TKDE.2019.2940019},
	timestamp = {Thu, 19 Mar 2020 10:22:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangSBG20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the era of open data, a single data source rarely contains all of the attributes we need for inference in specific applications. For example, a marketing department may aim to integrate retailer-specific purchase data with separate demographic data for purposes of targeted advertising – a capability not possible with either dataset alone. In this work, we address two key desiderata of an automated framework for probabilistic data integration over multiple data sources: (1) we require that each relational data source share at least one attribute with another relational data source, but we do not require these attributes to be foreign keys (e.g., attributes such as gender, age, and postal code are not foreign keys because they do not uniquely identify individuals in a data source) and (2) we require inference to be probabilistic to reflect inherent uncertainty in population-level predictions given the absence of foreign keys. While some frameworks such as Probabilistic Relational Models (PRMs) address point (2), they do not address point (1) since they rely on foreign keys to link tables. To achieve both desiderata simultaneously, we develop an automated framework to construct Bayesian networks for data integration capable of answering any probabilistic query spanning the attributes of multiple relational data sources. We demonstrate that our framework is able to closely approximate the inference of a global Bayesian network over a single relation that has been projected onto multiple local relations and further investigate properties of local relations such as the number of shared attributes and their cardinality to understand how these properties affect the quality of inference.}
}


@article{DBLP:journals/tkde/LiCW20a,
	author = {Xuelong Li and
                  Mulin Chen and
                  Qi Wang},
	title = {Discrimination-Aware Projected Matrix Factorization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {809--814},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2936855},
	doi = {10.1109/TKDE.2019.2936855},
	timestamp = {Mon, 14 Feb 2022 16:41:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LiCW20a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-negative Matrix Factorization (NMF) has been one of the most popular clustering techniques in machine leaning, and involves various real-world applications. Most existing works perform matrix factorization on high-dimensional data directly. However, the intrinsic data structure is always hidden within the low-dimensional subspace. And, the redundant features within the input space may affect the final result adversely. In this paper, a new unsupervised matrix factorization method, Discrimination-aware Projected Matrix Factorization (DPMF), is proposed for data clustering. The main contributions are threefold: (1) The linear discriminant analysis is jointly incorporated into the unsupervised matrix factorization framework, so the clustering can be accomplished in the discriminant subspace. (2) The manifold regularization is introduced to perceive the geometric information, and the ℓ 2,1 -norm is utilized to improve the robustness. (3) An efficient optimization algorithm is designed to solve the proposed problem with proved convergence. Experimental results on one toy dataset and eight real-world benchmarks show the effectiveness of the proposed method.}
}


@article{DBLP:journals/tkde/GhafooriEBKL20,
	author = {Zahra Ghafoori and
                  Sarah M. Erfani and
                  James C. Bezdek and
                  Shanika Karunasekera and
                  Christopher Leckie},
	title = {{LN-SNE:} Log-Normal Distributed Stochastic Neighbor Embedding for
                  Anomaly Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {815--820},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2934450},
	doi = {10.1109/TKDE.2019.2934450},
	timestamp = {Thu, 19 Mar 2020 10:22:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/GhafooriEBKL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a new unsupervised dimensionality reduction technique, called LN-SNE, for anomaly detection. LN-SNE generates a parametric embedding by means of Restricted Boltzmann Machines and uses a heavy-tail distribution to project data to a lower dimensional space such that dissimilarities between normal data and anomalies are preserved or strengthened. We compare LN-SNE to several benchmark dimensionality reduction methods on real datasets. The results suggest that LN-SNE for anomaly detection is less sensitive to the dimension of the latent space than the other methods and outperforms them in terms of accuracy. We empirically show that our technique scales near-linearly with respect to the number of dimensions and data size.}
}


@article{DBLP:journals/tkde/MajdaraN20,
	author = {Aref Majdara and
                  Saeid Nooshabadi},
	title = {Nonparametric Density Estimation Using Copula Transform, Bayesian
                  Sequential Partitioning, and Diffusion-Based Kernel Estimator},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {4},
	pages = {821--826},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2930052},
	doi = {10.1109/TKDE.2019.2930052},
	timestamp = {Thu, 19 Mar 2020 10:22:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MajdaraN20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-parametric density estimation methods are more flexible than parametric methods, due to the fact that they do not assume any specific shape or structure for the data. Most non-parametric methods, like Kernel estimation, require tuning of parameters to achieve good data smoothing, a non-trivial task, even in low dimensions. In higher dimensions, sparsity of data in local neighborhoods becomes a challenge even for non-parametric methods. In this paper, we use the copula transform and two efficient non-parametric methods to develop a new method for improved non-parametric density estimation in multivariate domain. After separation of marginal and joint densities using copula transform, a diffusion-based kernel estimator is employed to estimate the marginals. Next, Bayesian sequential partitioning (BSP) is used in the joint density estimation.}
}


@article{DBLP:journals/tkde/WanN20,
	author = {Shanshan Wan and
                  Zhendong Niu},
	title = {A Hybrid E-Learning Recommendation Approach Based on Learners' Influence
                  Propagation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {827--840},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2895033},
	doi = {10.1109/TKDE.2019.2895033},
	timestamp = {Fri, 22 May 2020 21:54:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WanN20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In e-learning recommender systems, interpersonal information between learners is very scarce, which makes it difficult to apply collaborative filtering (CF) techniques to achieve recommendations. In this study, we propose a hybrid filtering recommendation approach (SI - IFL) combining learner influence model (LIM), self-organization based (SOB) recommendation strategy, and sequential pattern mining (SPM) together for recommending learning objects (LOs) to learners. The method works as follows: (i) LIM is applied to acquire the interpersonal information by computing the influence that a learner exerts on others. LIM consists of learner similarity, knowledge credibility, and learner aggregation, meanwhile, LIM is independent of ratings. Furthermore, to address the uncertainty and fuzzy natures of learners, intuitionistic fuzzy logic (IFL) is applied to optimize the LIM. (ii) A SOB recommendation strategy is applied to recommend the optimal learner cliques for active learners by simulating the influence propagation among learners. Influence propagation means that a learner can move towards active learners, and such behaviors can stimulate the moving behaviors of his/her neighbors. This SOB recommendation approach achieves a stable structure based on distributed and bottom-up behaviors of individuals. (iii) SPM is applied to decide the final learning objects (LOs) and navigational paths based on the recommended learner cliques. The experimental results demonstrate that SI - IFL can provide personalized and diversified recommendations, and it shows promising efficiency and adaptability in e-learning scenarios.}
}


@article{DBLP:journals/tkde/YangXRC20,
	author = {Chi Yang and
                  Xianghua Xu and
                  Kotagiri Ramamohanarao and
                  Jinjun Chen},
	title = {A Scalable Multi-Data Sources Based Recursive Approximation Approach
                  for Fast Error Recovery in Big Sensing Data on Cloud},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {841--854},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2895612},
	doi = {10.1109/TKDE.2019.2895612},
	timestamp = {Mon, 28 Aug 2023 21:37:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YangXRC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Big sensing data is commonly encountered from various surveillance or sensing systems. Sampling and transferring errors are commonly encountered during each stage of sensing data processing. How to recover from these errors with accuracy and efficiency is quite challenging because of high sensing data volume and unrepeatable wireless communication environment. While Cloud provides a promising platform for processing big sensing data, however scalable and accurate error recovery solutions are still need. In this paper, we propose a novel approach to achieve fast error recovery in a scalable manner on cloud. This approach is based on the prediction of a recovery replacement data by making multiple data sources based approximation. The approximation process will use coverage information carried by data units to limit the algorithm in a small cluster of sensing data instead of a whole data spectrum. Specifically, in each sensing data cluster, a Euclidean distance based approximation is proposed to calculate a time series prediction. With the calculated time series, a detected error can be recovered with a predicted data value. Through the experiment with real world meteorological data sets on cloud, we demonstrate that the proposed error recovery approach can achieve high accuracy in data approximation to replace the original data error. At the same time, with MapReduce based implementation for scalability, the experimental results also show significant efficiency on time saving.}
}


@article{DBLP:journals/tkde/TangDHYTC20,
	author = {Jinhui Tang and
                  Xiaoyu Du and
                  Xiangnan He and
                  Fajie Yuan and
                  Qi Tian and
                  Tat{-}Seng Chua},
	title = {Adversarial Training Towards Robust Multimedia Recommender System},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {855--867},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893638},
	doi = {10.1109/TKDE.2019.2893638},
	timestamp = {Mon, 09 Sep 2024 19:07:22 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TangDHYTC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the prevalence of multimedia content on the Web, developing recommender solutions that can effectively leverage the rich signal in multimedia data is in urgent need. Owing to the success of deep neural networks in representation learning, recent advances on multimedia recommendation has largely focused on exploring deep learning methods to improve the recommendation accuracy. To date, however, there has been little effort to investigate the robustness of multimedia representation and its impact on the performance of multimedia recommendation. In this paper, we shed light on the robustness of multimedia recommender system. Using the state-of-the-art recommendation framework and deep image features, we demonstrate that the overall system is not robust, such that a small (but purposeful) perturbation on the input image will severely decrease the recommendation accuracy. This implies the possible weakness of multimedia recommender system in predicting user preference, and more importantly, the potential of improvement by enhancing its robustness. To this end, we propose a novel solution named Adversarial Multimedia Recommendation (AMR), which can lead to a more robust multimedia recommender model by using adversarial learning. The idea is to train the model to defend an adversary, which adds perturbations to the target image with the purpose of decreasing the model's accuracy. We conduct experiments on two representative multimedia recommendation tasks, namely, image recommendation and visually-aware product recommendation. Extensive results verify the positive effect of adversarial learning and demonstrate the effectiveness of our AMR method. Source codes are available in https://github.com/duxy-me/AMR.}
}


@article{DBLP:journals/tkde/LiWYYHZ20,
	author = {Yanchao Li and
                  Yongli Wang and
                  Dong{-}Jun Yu and
                  Ning Ye and
                  Peng Hu and
                  Ruxin Zhao},
	title = {{ASCENT:} Active Supervision for Semi-Supervised Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {868--882},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2897307},
	doi = {10.1109/TKDE.2019.2897307},
	timestamp = {Thu, 05 May 2022 16:44:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiWYYHZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Active learning algorithms attempt to overcome the labeling bottleneck by asking queries from large collection of unlabeled examples. Existing batch mode active learning algorithms suffer from three limitations: (1) The methods that are based on similarity function or optimizing certain diversity measurement, in which may lead to suboptimal performance and produce the selected set with redundant examples. (2) The models with assumption on data are hard in finding images that are both informative and representative. (3) The problem of noise labels has been an obstacle for algorithms. In this paper, we propose a novel active learning method that makes embeddings of labeled examples to those of unlabeled ones and back via deep neural networks. The active scheme makes correct association cycles that end up at the same class from that the association was started, which considers both the informativeness and representativeness of examples, as well as being robust to the noise labels. We apply our active learning method to semi-supervised classification and clustering. The submodular function is designed to reduce the redundancy of the selected examples. Specifically, we incorporate our batch mode active scheme into the classification approaches, in which the generalization ability is improved. For semi-supervised clustering, we try to use our active scheme for constraints to make fast convergence and perform better than unsupervised clustering. Finally, we apply our active learning method to data filtering. To validate the effectiveness of the proposed algorithms, extensive experiments are conducted on diversity benchmark datasets for different tasks, i.e., classification, clustering, and data filtering, and the experimental results demonstrate consistent and substantial improvements over the state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/DattaND20,
	author = {Shounak Datta and
                  Sayak Nag and
                  Swagatam Das},
	title = {Boosting with Lexicographic Programming: Addressing Class Imbalance
                  without Cost Tuning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {883--897},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2894148},
	doi = {10.1109/TKDE.2019.2894148},
	timestamp = {Fri, 22 May 2020 21:54:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DattaND20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A large amount of research effort has been dedicated to adapting boosting for imbalanced classification. However, boosting methods are yet to be satisfactorily immune to class imbalance, especially for multi-class problems. This is because most of the existing solutions for handling class imbalance rely on expensive cost set tuning for determining the proper level of compensation. We show that the assignment of weights to the component classifiers of a boosted ensemble can be thought of as a game of Tug of War between the classes in the margin space. We then demonstrate how this insight can be used to attain a good compromise between the rare and abundant classes without having to resort to cost set tuning, which has long been the norm for imbalanced classification. The solution is based on a lexicographic linear programming framework which requires two stages. Initially, class-specific component weight combinations are found so as to minimize a hinge loss individually for each of the classes. Subsequently, the final component weights are assigned so that the maximum deviation from the class-specific minimum loss values (obtained in the previous stage) is minimized. Hence, the proposal is not only restricted to two-class situations, but is also readily applicable to multi-class problems. Additionally, we also derive the dual formulation corresponding to the proposed framework. Experiments conducted on artificial and real-world imbalanced datasets as well as on challenging applications such as hyperspectral image classification and ImageNet classification establish the efficacy of the proposal.}
}


@article{DBLP:journals/tkde/FiondaG20,
	author = {Valeria Fionda and
                  Antonella Guzzo},
	title = {Control-Flow Modeling with Declare: Behavioral Properties, Computational
                  Complexity, and Tools},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {898--911},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2897309},
	doi = {10.1109/TKDE.2019.2897309},
	timestamp = {Fri, 22 May 2020 21:54:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FiondaG20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Declarative approaches to control-flow modeling use logic-based languages to formalize a number of constraints that valid traces must satisfy. The most noticeable example is the Declare framework based on linear temporal logic. Despite the interest that Declare has been attracting, the current knowledge about its formal properties was rather limited. The goal of this paper is to fill this gap by: (i) analyzing the behavioral properties of Declare by comparing it with the modeling capabilities of traditional procedural design approaches, in particular, block-structured processes; (ii) analyzing Declare from the computational point of view. As for the former point, we identify both the block-structured processes constructs that can be simulated in Declare and the features of Declare that can be encoded in block-structured processes. As for the latter point, we show that checking whether a given set of Declare patterns admits a satisfying trace is an\nNP\n-hard problem. In particular, we identify some Declare specifications whose satisfying traces are all of exponential length and some useful Declare fragments where a satisfying trace whose length is polynomially bounded is guaranteed to exist. The paper also discusses the declare2sat prototype system and the results of a thorough experimental validation.}
}


@article{DBLP:journals/tkde/LinWLG20,
	author = {Yiming Lin and
                  Hongzhi Wang and
                  Jianzhong Li and
                  Hong Gao},
	title = {Efficient Entity Resolution on Heterogeneous Records},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {912--926},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2898191},
	doi = {10.1109/TKDE.2019.2898191},
	timestamp = {Thu, 17 Oct 2024 14:48:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LinWLG20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity resolution (ER) is the problem of identifying and merging records that refer to the same real-world entity. In many scenarios, raw records are stored under heterogeneous environment. Specifically, the schemas of records may differ from each other. To leverage such records better, most existing work assume that schema matching and data exchange have been done to convert records under different schemas to those under a predefined schema. However, we observe that schema matching would lose information in some cases, which could be useful or even crucial to ER. To leverage sufficient information from heterogeneous sources, in this paper, we address several challenges of ER on heterogeneous records and show that none of existing similarity metrics or their transformations could be applied to find similar records under heterogeneous settings. Motivated by this, we design the similarity function and propose a novel framework to iteratively find records which refer to the same entity. Regarding efficiency, we build an index to generate candidates and accelerate similarity computation. Evaluations on real-world datasets show the effectiveness and efficiency of our methods.}
}


@article{DBLP:journals/tkde/AaLR20,
	author = {Han van der Aa and
                  Henrik Leopold and
                  Hajo A. Reijers},
	title = {Efficient Process Conformance Checking on the Basis of Uncertain Event-to-Activity
                  Mappings},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {927--940},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2897557},
	doi = {10.1109/TKDE.2019.2897557},
	timestamp = {Thu, 14 Oct 2021 08:57:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/AaLR20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conformance checking enables organizations to automatically identify compliance violations based on the analysis of observed event data. A crucial requirement for conformance-checking techniques is that observed events can be mapped to normative process models used to specify allowed behavior. Without a mapping, it is not possible to determine if an observed event trace conforms to the specification or not. A considerable problem in this regard is that establishing a mapping between events and process model activities is an inherently uncertain task. Since the use of a particular mapping directly influences the conformance of an event trace to a specification, this uncertainty represents a major issue for conformance checking. To overcome this issue, we introduce a probabilistic conformance-checking technique that can deal with uncertain mappings. Our technique avoids the need to select a single mapping by taking the entire spectrum of possible mappings into account. A quantitative evaluation demonstrates that our technique can be applied on a considerable number of real-world processes where existing conformance-checking techniques fail.}
}


@article{DBLP:journals/tkde/EbisuI20,
	author = {Takuma Ebisu and
                  Ryutaro Ichise},
	title = {Generalized Translation-Based Embedding of Knowledge Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {941--951},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893920},
	doi = {10.1109/TKDE.2019.2893920},
	timestamp = {Mon, 28 Aug 2023 21:37:43 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/EbisuI20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graphs are useful for many AI tasks but often have missing facts. To populate the graphs, knowledge graph embedding models have been developed. TransE is one of such models and the first translation-based method. TransE is well known because the principle of TransE can effectively capture the rules of a knowledge graph although it seems very simple. However, TransE has problems with its regularization and an unchangeable ratio of negative sampling. In this paper, we generalize TransE to solve these problems by proposing knowledge graph embedding on a Lie group (KGLG) and the Weighted Negative Part (WNP) method for the objective function of translation-based models. KGLG is the novel translation-based method which embeds entities and relations of a knowledge graph on any Lie group. It allows us not to employ regularization during training of the model if we choose a compact lie group for the embedding space. The WNP method is for changing the ratio of negative sampling, which enhances translation-based models. Our approach outperforms other state-of-the-art approaches such as TransE, DistMult, and ComplEx on a standard link prediction task. We show that TorusE, KGLG on a torus, is scalable to large-size knowledge graphs and faster than the original TransE.}
}


@article{DBLP:journals/tkde/ZhangZLTYW20,
	author = {Zhao Zhang and
                  Yan Zhang and
                  Guangcan Liu and
                  Jinhui Tang and
                  Shuicheng Yan and
                  Meng Wang},
	title = {Joint Label Prediction Based Semi-Supervised Adaptive Concept Factorization
                  for Robust Data Representation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {952--970},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893956},
	doi = {10.1109/TKDE.2019.2893956},
	timestamp = {Tue, 08 Mar 2022 11:23:02 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZLTYW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Constrained Concept Factorization (CCF) yields the enhanced representation ability over CF by incorporating label information as additional constraints, but it cannot classify and group unlabeled data appropriately. Minimizing the difference between the original data and its reconstruction directly can enable CCF to model a small noisy perturbation, but is not robust to gross sparse errors. Besides, CCF cannot preserve the manifold structures in new representation space explicitly, especially in an adaptive manner. In this paper, we propose a joint label prediction based Robust Semi-Supervised Adaptive Concept Factorization (RS 2 ACF) framework. To obtain robust representation, RS 2 ACF relaxes the factorization to make it simultaneously stable to small entrywise noise and robust to sparse errors. To enrich prior knowledge to enhance the discrimination, RS 2 ACF clearly uses class information of labeled data and more importantly propagates it to unlabeled data by jointly learning an explicit label indicator for unlabeled data. By the label indicator, RS 2 ACF can ensure the unlabeled data of the same predicted label to be mapped into the same class in feature space. Besides, RS 2 ACF incorporates the joint neighborhood reconstruction error over the new representations and predicted labels of both labeled and unlabeled data, so the manifold structures can be preserved explicitly and adaptively in the representation space and label space at the same time. Owing to the adaptive manner, the tricky process of determining the neighborhood size or kernel width can be avoided. Extensive results on public databases verify that our RS 2 ACF can deliver state-of-the-art data representation, compared with other related methods.}
}


@article{DBLP:journals/tkde/SunTDQLYZLYFQL20,
	author = {Yibo Sun and
                  Duyu Tang and
                  Nan Duan and
                  Tao Qin and
                  Shujie Liu and
                  Zhao Yan and
                  Ming Zhou and
                  Yuanhua Lv and
                  Wenpeng Yin and
                  Xiaocheng Feng and
                  Bing Qin and
                  Ting Liu},
	title = {Joint Learning of Question Answering and Question Generation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {971--982},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2897773},
	doi = {10.1109/TKDE.2019.2897773},
	timestamp = {Mon, 29 Jan 2024 17:56:07 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SunTDQLYZLYFQL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Question answering (QA) and question generation (QG) are closely related tasks that could improve each other; however, the connection of these two tasks is not well explored in the literature. In this paper, we present two training algorithms for learning better QA and QG models through leveraging one another. The first algorithm extends Generative Adversarial Network (GAN), which selectively incorporates artificially generated instances as additional QA training data. The second algorithm is an extension of dual learning, which incorporates the probabilistic correlation of QA and QG as additional regularization in training objectives. To test the scalability of our algorithms, we conduct experiments on both document based and table based question answering tasks. Results show that both algorithms improve a QA model in terms of accuracy and QG model in terms of BLEU score. Moreover, we find that the performance of a QG model could be easily improved by a QA model via policy gradient, however, directly applying GAN that regards all the generated questions as negative instances could not improve the accuracy of the QA model. Our algorithm that selectively assigns labels to generated questions would bring a performance boost.}
}


@article{DBLP:journals/tkde/AbeywickramaCK20,
	author = {Tenindra Abeywickrama and
                  Muhammad Aamir Cheema and
                  Arijit Khan},
	title = {{K-SPIN:} Efficiently Processing Spatial Keyword Queries on Road Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {983--997},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2894140},
	doi = {10.1109/TKDE.2019.2894140},
	timestamp = {Fri, 16 Jul 2021 15:33:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/AbeywickramaCK20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A significant proportion of all search volume consists of local searches. As a result, search engines must be capable of finding relevant results combining both spatial proximity and textual relevance with high query throughput. We observe that existing techniques answering these spatial keyword queries use keyword aggregated indexing, which has several disadvantages on road networks. We propose K-SPIN, a versatile framework that instead uses keyword separated indexing to delay and avoid expensive operations. At first glance, this strategy appears to have impractical pre-processing costs. However, by exploiting several useful observations, we make the indexing cost not only viable but also light-weight. For example, we propose a novel p-Approximate Network Voronoi Diagram (NVD) with one order of magnitude less space cost than exact NVDs. By carefully exploiting features of the K-SPIN framework, our query algorithms are up to two orders of magnitude more efficient than the state-of-the-art as shown in our experimental investigation on various queries, parameter settings, and real road network and keyword datasets.}
}


@article{DBLP:journals/tkde/HuWMCLB20,
	author = {Qin Hu and
                  Shengling Wang and
                  Peizi Ma and
                  Xiuzhen Cheng and
                  Weifeng Lv and
                  Rongfang Bie},
	title = {Quality Control in Crowdsourcing Using Sequential Zero-Determinant
                  Strategies},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {998--1009},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2896926},
	doi = {10.1109/TKDE.2019.2896926},
	timestamp = {Mon, 28 Aug 2023 21:37:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuWMCLB20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quality control in crowdsourcing is challenging due to the heterogeneous nature of the workers. The state-of-the-art solutions attempt to address the issue from the technical perspective, which may be costly because they function as an additional procedure in crowdsourcing. In this paper, an economics based idea is adopted to embed quality control into the crowdsourcing process, where the requestor can take advantage of the market power to stimulate the workers for submitting high-quality jobs. Specifically, we employ two sequential games to model the interactions between the requestor and the workers, with one considering binary strategies while the other taking continuous strategies. Accordingly, two incentive algorithms for improving the job quality are proposed to tackle the sequential crowdsourcing dilemma problem. Both algorithms are based on a sequential zero-determinant (ZD) strategy modified from the classical ZD strategy. Such a revision not only provides a theoretical basis for designing our incentive algorithms, but also enlarges the application space of the classical ZD strategy itself. Our incentive algorithms have the following desired features: 1) they do not depend on any specific crowdsourcing scenario; 2) they leverage economics theory to train the workers to behave nicely for better job quality instead of filtering out the unprofessional workers; 3) no extra costs are incurred in a long run of crowdsourcing; and 4) fairness is realized as even the requestor (the ZD player), who dominates the game, cannot increase her utility by arbitrarily penalizing any innocent worker.}
}


@article{DBLP:journals/tkde/DabiriLHR20,
	author = {Sina Dabiri and
                  Chang{-}Tien Lu and
                  Kevin P. Heaslip and
                  Chandan K. Reddy},
	title = {Semi-Supervised Deep Learning Approach for Transportation Mode Identification
                  Using {GPS} Trajectory Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {1010--1023},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2896985},
	doi = {10.1109/TKDE.2019.2896985},
	timestamp = {Mon, 01 May 2023 13:02:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DabiriLHR20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Identification of travelers’ transportation modes is a fundamental step for various problems that arise in the domain of transportation such as travel demand analysis, transport planning, and traffic management. In this paper, we aim to identify travelers’ transportation modes purely based on their GPS trajectories. First, a segmentation process is developed to partition a user's trip into GPS segments with only one transportation mode. A majority of studies have proposed mode inference models based on hand-crafted features, which might be vulnerable to traffic and environmental conditions. Furthermore, the classification task in almost all models have been performed in a supervised fashion while a large amount of unlabeled GPS trajectories has remained unused. Accordingly, we propose a deep SE mi-Supervised C onvolutional A utoencoder ( SECA ) architecture that can not only automatically extract relevant features from GPS segments but also exploit useful information in unlabeled data. The SECA integrates a convolutional-deconvolutional autoencoder and a convolutional neural network into a unified framework to concurrently perform supervised and unsupervised learning. The two components are simultaneously trained using both labeled and unlabeled GPS segments, which have already been converted into an efficient representation for the convolutional operation. An optimum schedule for varying the balancing parameters between reconstruction and classification errors are also implemented. The performance of the proposed SECA model, trip segmentation, the method for converting a raw trajectory into a new representation, the hyperparameter schedule, and the model configuration are evaluated by comparing to several baselines and alternatives for various amounts of labeled and unlabeled data. Our experimental results demonstrate the superiority of the proposed model over the state-of-the-art semi-supervised and supervised methods with respect to metrics such as accuracy and F-measure.}
}


@article{DBLP:journals/tkde/CenZWQMDYT20,
	author = {Yukuo Cen and
                  Jing Zhang and
                  Gaofei Wang and
                  Yujie Qian and
                  Chuizheng Meng and
                  Zonghong Dai and
                  Hongxia Yang and
                  Jie Tang},
	title = {Trust Relationship Prediction in Alibaba E-Commerce Platform},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {5},
	pages = {1024--1035},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2893939},
	doi = {10.1109/TKDE.2019.2893939},
	timestamp = {Fri, 22 May 2020 21:54:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CenZWQMDYT20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces how to infer trust relationships from billion-scale networked data to benefit Alibaba E-Commerce business. To effectively leverage the network correlations between labeled and unlabeled relationships to predict trust relationships, we formalize trust into multiple types and propose a graphical model to incorporate type-based dyadic and triadic correlations, namely eTrust. We also present a fast learning algorithm in order to handle billion-scale networks. Systematically, we evaluate the proposed methods on four different genres of datasets with labeled trust relationships: Alibaba, Epinions, Ciao, and Advogato. Experimental results show that the proposed methods achieve significantly better performance than several comparison methods (+1.7-32.3% by accuracy;\np<<0.01\n, with\nt\n-test). Most importantly, when handling the real large networked data with over 1,200,000,000 edges (Ali-large), our method achieves 2,000× speedup to infer trust relationships, comparing with the traditional graph learning algorithms. Finally, we have applied the inferred trust relationships to Alibaba E-commerce platform: Taobao, and achieved 2.75 percent improvement on gross merchandise volume (GMV).}
}


@article{DBLP:journals/tkde/ChenZZJMH20,
	author = {Xiaoshuang Chen and
                  Yin Zheng and
                  Peilin Zhao and
                  Zhuxi Jiang and
                  Wenye Ma and
                  Junzhou Huang},
	title = {A Generalized Locally Linear Factorization Machine with Supervised
                  Variational Encoding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1036--1049},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2903403},
	doi = {10.1109/TKDE.2019.2903403},
	timestamp = {Mon, 05 Feb 2024 20:21:14 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenZZJMH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Factorization Machines (FMs) learn weights for feature interactions, and achieve great success in many data mining tasks. Recently, Locally Linear Factorization Machines (LLFMs) have been proposed to capture the underlying structures of data for better performance. However, one obvious drawback of LLFM is that the local coding is only operated in the original feature space, which limits the model to be applied to high-dimensional and sparse data. In this work, we present a generalized LLFM (GLLFM) which overcomes this limitation by modeling the local coding procedure in a latent space. Moreover, a novel Supervised Variational Encoding (SVE) technique is proposed such that the distance can effectively describe the similarity between data points. Specifically, the proposed GLLFM-SVE trains several local FMs in the original space to model the higher order feature interactions effectively, where each FM associates to an anchor point in the latent space induced by SVE. The prediction for a data point is computed by a weighted sum of several local FMs, where the weights are determined by local coding coordinates with anchor points. Actually, GLLFM-SVE is quite flexible and other Neural Network (NN) based FMs can be easily embedded into this framework. Experimental results show that GLLFM-SVE significantly improves the performance of LLFM. By using NN-based FMs as local predictors, our model outperforms all the state-of-the-art methods on large-scale real-world benchmarks with similar number of parameters and comparable training time.}
}


@article{DBLP:journals/tkde/AliannejadiRC20,
	author = {Mohammad Aliannejadi and
                  Dimitrios Rafailidis and
                  Fabio Crestani},
	title = {A Joint Two-Phase Time-Sensitive Regularized Collaborative Ranking
                  Model for Point of Interest Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1050--1063},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2903463},
	doi = {10.1109/TKDE.2019.2903463},
	timestamp = {Fri, 22 May 2020 21:54:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/AliannejadiRC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The popularity of location-based social networks (LBSNs) has led to a tremendous amount of user check-in data. Recommending points of interest (POIs) plays a key role in satisfying users needs in LBSNs. While recent work has explored the idea of adopting collaborative ranking (CR) for recommendation, there have been few attempts to incorporate temporal information for POI recommendation using CR. In this article, we propose a two-phase CR algorithm that incorporates the geographical influence of POIs and is regularized based on the variance of POIs popularity and users activities over time. The time-sensitive regularizer penalizes user and POIs that have been more time-sensitive in the past, helping the model to account for their long-term behavioral patterns while learning from user-POI interactions. Moreover, in the first phase, it attempts to rank visited POIs higher than the unvisited ones, and at the same time, apply the geographical influence. In the second phase, our algorithm tries to rank users favorite POIs higher on the recommendation list. Both phases employ a collaborative learning strategy that enables the model to capture complex latent associations from two different perspectives. Experiments on real-world datasets show that our proposed time-sensitive collaborative ranking model beats state-of-the-art POI recommendation methods.}
}


@article{DBLP:journals/tkde/RuscheinskiWU20,
	author = {Andreas Ruscheinski and
                  Tom Warnke and
                  Adelinde M. Uhrmacher},
	title = {Artifact-Based Workflows for Supporting Simulation Studies},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1064--1078},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2899840},
	doi = {10.1109/TKDE.2019.2899840},
	timestamp = {Fri, 22 May 2020 21:54:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/RuscheinskiWU20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Valid models are central for credible simulation studies. If those models do not exist, they need to be developed. In fact, entire simulation studies are often aimed at developing valid models. Thereby, successive model refinement and execution of diverse simulation experiments are closely intertwined. Whereas software-based support for individual simulation experiments exists, the intricate interdependencies and the diversity of tasks that govern simulation studies have prevented a more comprehensive support. To achieve the required flexibility while adhering to the constraints that apply between individual tasks, we adopt a declarative, artifact-based workflow approach. Therefore, central products of these simulation studies are identified and specified as artifacts: the conceptual model (with a focus on formally defined requirements), the simulation model, and the experiment. Each artifact is characterized by stages the artifact moves through to reach certain milestones and which are guarded by conditions. Thereby, the relations and constraints between artifacts become explicit. This is instrumental to check and ensure the consistency between conceptual model and simulation model, to automatically execute simulation experiments to probe the specified requirements, and to develop plans to provide goal-directed guidance to the user. We demonstrate the approach by using it to repeat an existing simulation study.}
}


@article{DBLP:journals/tkde/LiZ20,
	author = {Yexin Li and
                  Yu Zheng},
	title = {Citywide Bike Usage Prediction in a Bike-Sharing System},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1079--1091},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2898831},
	doi = {10.1109/TKDE.2019.2898831},
	timestamp = {Fri, 22 May 2020 21:54:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To operate a bike-sharing system efficiently, system operators need to accurately predict how many bikes are to be rented and returned throughout the city. In this paper, we propose a Hierarchical Consistency Prediction (HCP) model to predict the citywide bike usage in the next period. First, an Adaptive Transition Constraint (AdaTC) clustering algorithm is proposed to cluster stations into groups, making the rent and transition at each cluster more regular than those at each single station. Second, a Similarity-based efficient Gaussian Process Regressor (SGPR) is proposed to respectively predict how many bikes are to be rented at different-scale locations, i.e., at each station, each cluster, and in the entire city. Besides largely improving the training and online prediction efficiency, our regressor considers external impacted factors, addresses the data unbalance issue, and better captures the non-linearity in spatio-temporal data. Third, we design a General Least Square (GLS) formulation to collectively improve those obtained predictions via a mutual reinforcement way. GLS makes the final predictions for rent more reasonable. Considering the causality between rent and return, a Transition based Inference (TINF) method is designed to infer the citywide bike return demand based on the predicted rent demands. Experiments on real-world data are conducted to confirm the effectiveness of our model.}
}


@article{DBLP:journals/tkde/ZhaoGHXSWH20,
	author = {Wei Zhao and
                  Ziyu Guan and
                  Yuhui Huang and
                  Tingting Xi and
                  Huan Sun and
                  Zhiheng Wang and
                  Xiaofei He},
	title = {Discerning Influence Patterns with Beta-Poisson Factorization in Microblogging
                  Environments},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1092--1103},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2897932},
	doi = {10.1109/TKDE.2019.2897932},
	timestamp = {Wed, 31 May 2023 19:02:38 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoGHXSWH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social influence analysis in microblogging services has attracted much attention in recent years. However, most previous studies were focused on measuring users' (topical) influence. Little effort has been made to discern and quantify how a user is influenced. Specifically, the fact that user i retweets a tweet from author j could be either because i is influenced by j (i.e., j is a topical authority), or simply because he is “influenced” by the content (interested in the content). To mine such influence patterns, we propose a novel Bayesian factorization model, dubbed Influence Beta-Poisson Factorization (IBPF). IBPF jointly factorizes the retweet data and tweet content to quantify latent topical factors of user preference, author influence and content influence. It generates every retweet record according to the sum of two causing terms: one representing author influence, and the other one derived from content influence. To control the impact of the two terms, for each user IBPF generates a probability for each latent topic by Beta distribution, indicating how strongly the user cares about the topical authority of the author. We develop an efficient variational inference algorithm for IBPF. We demonstrate the efficacy of IBPF on two public microblogging datasets.}
}


@article{DBLP:journals/tkde/FernandesCY20,
	author = {Everlandio R. Q. Fernandes and
                  Andr{\'{e}} C. P. L. F. de Carvalho and
                  Xin Yao},
	title = {Ensemble of Classifiers Based on Multiobjective Genetic Sampling for
                  Imbalanced Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1104--1115},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2898861},
	doi = {10.1109/TKDE.2019.2898861},
	timestamp = {Fri, 22 May 2020 21:54:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FernandesCY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Imbalanced datasets may negatively impact the predictive performance of most classical classification algorithms. This problem, commonly found in real-world, is known in machine learning domain as imbalanced learning. Most techniques proposed to deal with imbalanced learning have been proposed and applied only to binary classification. When applied to multiclass tasks, their efficiency usually decreases and negative side effects may appear. This paper addresses these limitations by presenting a novel adaptive approach, E-MOSAIC (Ensemble of Classifiers based on MultiObjective Genetic Sampling for Imbalanced Classification). E-MOSAIC evolves a selection of samples extracted from training dataset, which are treated as individuals of a MOEA. The multiobjective process looks for the best combinations of instances capable of producing classifiers with high predictive accuracy in all classes. E-MOSAIC also incorporates two mechanisms to promote the diversity of these classifiers, which are combined into an ensemble specifically designed for imbalanced learning. Experiments using twenty imbalanced multi-class datasets were carried out. In these experiments, the predictive performance of E-MOSAIC is compared with state-of-the-art methods, including methods based on presampling, active-learning, cost-sensitive, and boosting. According to the experimental results, the proposed method obtained the best predictive performance for the multiclass accuracy measures mAUC and G-mean.}
}


@article{DBLP:journals/tkde/WangYL20,
	author = {Hao Wang and
                  Yan Yang and
                  Bing Liu},
	title = {{GMC:} Graph-Based Multi-View Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1116--1129},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2903810},
	doi = {10.1109/TKDE.2019.2903810},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangYL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view graph-based clustering aims to provide clustering solutions to multi-view data. However, most existing methods do not give sufficient consideration to weights of different views and require an additional clustering step to produce the final clusters. They also usually optimize their objectives based on fixed graph similarity matrices of all views. In this paper, we propose a general Graph-based Multi-view Clustering (GMC) to tackle these problems. GMC takes the data graph matrices of all views and fuses them to generate a unified graph matrix. The unified graph matrix in turn improves the data graph matrix of each view, and also gives the final clusters directly. The key novelty of GMC is its learning method, which can help the learning of each view graph matrix and the learning of the unified graph matrix in a mutual reinforcement manner. A novel multi-view fusion technique can automatically weight each data graph matrix to derive the unified graph matrix. A rank constraint without introducing a tuning parameter is also imposed on the graph Laplacian matrix of the unified matrix, which helps partition the data points naturally into the required number of clusters. An alternating iterative optimization algorithm is presented to optimize the objective function. Experimental results using both toy data and real-world data demonstrate that the proposed method outperforms state-of-the-art baselines markedly.}
}


@article{DBLP:journals/tkde/SmedtDW20,
	author = {Johannes De Smedt and
                  Galina Deeva and
                  Jochen De Weerdt},
	title = {Mining Behavioral Sequence Constraints for Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1130--1142},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2897311},
	doi = {10.1109/TKDE.2019.2897311},
	timestamp = {Fri, 22 May 2020 21:54:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SmedtDW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequence classification deals with the task of finding discriminative and concise sequential patterns. To this purpose, many techniques have been proposed, which mainly resort to the use of partial orders to capture the underlying sequences in a database according to the labels. Partial orders, however, pose many limitations, especially on expressiveness, i.e., the aptitude towards capturing certain behavior, and on conciseness, i.e., doing so in a compact and informative way. These limitations can be addressed by using a better representation. In this paper, we present the interesting Behavioral Constraint Miner (iBCM), a sequence classification technique that discovers patterns using behavioral constraint templates. The templates comprise a variety of constraints and can express patterns ranging from simple occurrence, to looping and position-based behavior over a sequence. Furthermore, iBCM also captures negative constraints, i.e., absence of particular behavior. The constraints can be discovered by using simple string operations in an efficient way. Finally, deriving the constraints with a window-based approach allows to pinpoint where the constraints hold in a string, and to detect whether patterns are subject to concept drift. Through empirical evaluation, it is shown that iBCM is better capable of classifying sequences more accurately and concisely in a scalable manner.}
}


@article{DBLP:journals/tkde/ZhangZ20,
	author = {Teng Zhang and
                  Zhi{-}Hua Zhou},
	title = {Optimal Margin Distribution Machine},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1143--1156},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2897662},
	doi = {10.1109/TKDE.2019.2897662},
	timestamp = {Sun, 06 Sep 2020 22:24:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Support Vector Machine (SVM) has always been one of the most successful learning algorithms, with the central idea of maximizing the minimum margin , i.e., the smallest distance from the instances to the classification boundary. However, recent theoretical results disclosed that maximizing the minimum margin does not necessarily lead to better generalization performance, and instead, the margin distribution has been proven to be more crucial. Based on this idea, we propose the Optimal margin Distribution Machine (ODM), which can achieve a better generalization performance by optimizing the margin distribution explicitly. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The proposed method is a general learning approach which can be applied in any place where SVMs are used, and its superiority is verified both theoretically and empirically in this paper.}
}


@article{DBLP:journals/tkde/LeeK20,
	author = {Sangmin Lee and
                  Seoung Bum Kim},
	title = {Parallel Simulated Annealing with a Greedy Algorithm for Bayesian
                  Network Structure Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1157--1166},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2899096},
	doi = {10.1109/TKDE.2019.2899096},
	timestamp = {Mon, 12 Sep 2022 19:38:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LeeK20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a hybrid algorithm called parallel simulated annealing with a greedy algorithm (PSAGA) to learn Bayesian network structures. This work focuses on simulated annealing and its parallelization with memoization to accelerate the search process. At each step of the local search, a hybrid search method combining simulated annealing with a greedy algorithm was adopted. The proposed PSAGA aims to achieve both the efficiency of parallel search and the effectiveness of a more exhaustive search. The Bayesian Dirichlet equivalence metric was used to determine an optimal structure for PSAGA. The proposed PSAGA was evaluated on seven well-known Bayesian network benchmarks generated at random. We first conducted experiments to evaluate the computational time performance of the proposed parallel search. We then compared PSAGA with existing variants of simulated annealing-based algorithms to evaluate the quality of the learned structure. Overall, the experimental results demonstrate that the proposed PSAGA shows better performance than the alternatives in terms of computational time and accuracy.}
}


@article{DBLP:journals/tkde/NieSL20,
	author = {Feiping Nie and
                  Shaojun Shi and
                  Xuelong Li},
	title = {Semi-Supervised Learning with Auto-Weighting Feature and Adaptive
                  Graph},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1167--1178},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2901853},
	doi = {10.1109/TKDE.2019.2901853},
	timestamp = {Mon, 28 Aug 2023 21:37:41 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NieSL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional graph-based Semi-Supervised Learning (SSL) methods usually contain two separate steps. First, constructing an affinity matrix. Second, inferring the unknown labels. While such a two-step method has been successful, it cannot take full advantage of the correlation between affinity matrix and label information. In order to address the above problem, we propose a novel graph-based SSL method. It can learn the affinity matrix and infer the unknown labels simultaneously. Moreover, feature selection with auto-weighting is introduced to extract the effective and robust features. Further, the proposed method learns the data similarity matrix by assigning the adaptive neighbors for each data point based on the local distance. We solve the unified problem via an alternative minimization algorithm. Extensive experimental results on synthetic data and benchmark data show that the proposed method consistently outperforms the state-of-the-art approaches.}
}


@article{DBLP:journals/tkde/LoghinCCDFLNOST20,
	author = {Dumitrel Loghin and
                  Shaofeng Cai and
                  Gang Chen and
                  Tien Tuan Anh Dinh and
                  Feiyi Fan and
                  Qian Lin and
                  Janice Ng and
                  Beng Chin Ooi and
                  Xutao Sun and
                  Quang{-}Trung Ta and
                  Wei Wang and
                  Xiaokui Xiao and
                  Yang Yang and
                  Meihui Zhang and
                  Zhonghua Zhang},
	title = {The Disruptions of 5G on Data-Driven Technologies and Applications},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1179--1198},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2020.2967670},
	doi = {10.1109/TKDE.2020.2967670},
	timestamp = {Wed, 02 Aug 2023 15:25:33 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LoghinCCDFLNOST20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With 5G on the verge of being adopted as the next mobile network, there is a need to analyze its impact on the landscape of computing and data management. In this paper, we analyze the impact of 5G on both traditional and emerging technologies and project our view on future research challenges and opportunities. With a predicted increase of 10-100× in bandwidth and 5-10x decrease in latency, 5G is expected to be the main enabler for smart cities, smart IoT and efficient healthcare, where machine learning is conducted at the edge. In this context, we investigate how 5G can help the development of federated learning. Network slicing, another key feature of 5G, allows running multiple isolated networks on the same physical infrastructure. However, security remains the main concern in the context of virtualization, multi-tenancy and high device density. Formal verification of 5G networks can be applied to detect security issues in massive virtualized environments. In summary, 5G will make the world even more densely and closely connected. What we have experienced in 4G connectivity will pale in comparison to the vast amounts of possibilities engendered by 5G.}
}


@article{DBLP:journals/tkde/YaoZSLZZS20,
	author = {Yazhou Yao and
                  Jian Zhang and
                  Fumin Shen and
                  Li Liu and
                  Fan Zhu and
                  Dongxiang Zhang and
                  Heng Tao Shen},
	title = {Towards Automatic Construction of Diverse, High-Quality Image Datasets},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1199--1211},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2903036},
	doi = {10.1109/TKDE.2019.2903036},
	timestamp = {Fri, 22 May 2020 21:54:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YaoZSLZZS20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The availability of labeled image datasets has been shown critical for high-level image understanding, which continuously drives the progress of feature designing and models developing. However, constructing labeled image datasets is laborious and monotonous. To eliminate manual annotation, in this work, we propose a novel image dataset construction framework by employing multiple textual queries. We aim at collecting diverse and accurate images for given queries from the Web. Specifically, we formulate noisy textual queries removing and noisy images filtering as a multi-view and multi-instance learning problem separately. Our proposed approach not only improves the accuracy but also enhances the diversity of the selected images. To verify the effectiveness of our proposed approach, we construct an image dataset with 100 categories. The experiments show significant performance gains by using the generated data of our approach on several tasks, such as image classification, cross-dataset generalization, and object detection. The proposed method also consistently outperforms existing weakly supervised and web-supervised approaches.}
}


@article{DBLP:journals/tkde/HuangWWLK20,
	author = {Dong Huang and
                  Chang{-}Dong Wang and
                  Jian{-}Sheng Wu and
                  Jian{-}Huang Lai and
                  Chee{-}Keong Kwoh},
	title = {Ultra-Scalable Spectral Clustering and Ensemble Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1212--1226},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2903410},
	doi = {10.1109/TKDE.2019.2903410},
	timestamp = {Thu, 27 Jul 2023 08:18:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuangWWLK20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper focuses on scalability and robustness of spectral clustering for extremely large-scale datasets with limited resources. Two novel algorithms are proposed, namely, ultra-scalable spectral clustering (U-SPEC) and ultra-scalable ensemble clustering (U-SENC). In U-SPEC, a hybrid representative selection strategy and a fast approximation method for K\n-nearest representatives are proposed for the construction of a sparse affinity sub-matrix. By interpreting the sparse sub-matrix as a bipartite graph, the transfer cut is then utilized to efficiently partition the graph and obtain the clustering result. In U-SENC, multiple U-SPEC clusterers are further integrated into an ensemble clustering framework to enhance the robustness of U-SPEC while maintaining high efficiency. Based on the ensemble generation via multiple U-SEPC's, a new bipartite graph is constructed between objects and base clusters and then efficiently partitioned to achieve the consensus clustering result. It is noteworthy that both U-SPEC and U-SENC have nearly linear time and space complexity, and are capable of robustly and efficiently partitioning 10-million-level nonlinearly-separable datasets on a PC with 64 GB memory. Experiments on various large-scale datasets have demonstrated the scalability and robustness of our algorithms. The MATLAB code and experimental data are available at https://www.researchgate.net/publication/330760669 .}
}


@article{DBLP:journals/tkde/ChenFZZW20,
	author = {Hongmei Chen and
                  Yixiang Fang and
                  Ying Zhang and
                  Wenjie Zhang and
                  Lizhen Wang},
	title = {{ESPM:} Efficient Spatial Pattern Matching},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {6},
	pages = {1227--1233},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2947505},
	doi = {10.1109/TKDE.2019.2947505},
	timestamp = {Fri, 15 Mar 2024 17:12:01 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenFZZW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With recent advances in information technologies such as global position system and mobile internet, a huge volume of spatio-textual objects have been generated from location-based services, which enable a wide range of spatial keyword queries. Recently, researchers have proposed a novel query, called Spatial Pattern Matching (SPM), which uses a pattern to capture the user's intention. It has been demonstrated to be fundamental and useful for many real applications. Despite its usefulness, the SPM problem is computationally intractable. Existing algorithms suffer from the low efficiency issue, especially on large scale datasets. To enhance the performance of SPM, in this paper we propose a novel Efficient Spatial Pattern Matching (ESPM) algorithm, which exploits the inverted linear quadtree index and computes matched node pairs and object pairs level by level in a top-down manner. In particular, it focuses on pruning unpromising nodes and node pairs at the high levels, resulting in a large number of unpromising objects and object pairs to be pruned before accessing them from disk. We experimentally evaluate the performance of ESPM on real large datasets. Our results show that ESPM is over one order of magnitude faster than the state-of-the-art algorithm, and also uses much less I/O cost.}
}


@article{DBLP:journals/tkde/ChatterjeeMB20,
	author = {Sujoy Chatterjee and
                  Anirban Mukhopadhyay and
                  Malay Bhattacharyya},
	title = {A Review of Judgment Analysis Algorithms for Crowdsourced Opinions},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1234--1248},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904064},
	doi = {10.1109/TKDE.2019.2904064},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChatterjeeMB20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The crowd-powered systems have been shown to be highly successful in the current decade to manage collective contribution of online workers for solving different complex tasks. It can also be used for soliciting opinions from a large set of people working in a distributed manner. Unfortunately, the online community of crowd workers might involve non-experts as opinion providers. As a result, such approaches may give rise to noise making it hard to predict the appropriate (gold) judgment. Judgment analysis is in general a way of learning about human decision from multiple opinions. A spectrum of algorithms has been proposed in the last few decades to address this problem. They are broadly made up of supervised or unsupervised types. However, they have been readdressed in recent years having focus on different strategies for obtaining the gold judgment from crowdsourced opinions, viz., estimating the accuracy of opinions, difficulties of the problem, spammer identification, handling noise, etc. Besides this, investigation of various types of crowdsourced opinions to solve complex real-life problems provide new insights in this domain. In this survey, we provide a comprehensive overview of the judgment analysis problem and some of its novel variants, addressed with different approaches, where the opinions are crowdsourced.}
}


@article{DBLP:journals/tkde/ChenWLLZY20,
	author = {Yong Chen and
                  Junjie Wu and
                  Jianying Lin and
                  Rui Liu and
                  Hui Zhang and
                  Zhiwen Ye},
	title = {Affinity Regularized Non-Negative Matrix Factorization for Lifelong
                  Topic Modeling},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1249--1262},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904687},
	doi = {10.1109/TKDE.2019.2904687},
	timestamp = {Tue, 08 Dec 2020 00:31:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChenWLLZY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Lifelong topic model (LTM), an emerging paradigm for never-ending topic learning, aims to yield higher-quality topics as time passes through knowledge accumulated from the past yet learned for the future. In this paper, we propose a novel lifelong topic model based on non-negative matrix factorization (NMF), called Affinity Regularized NMF for LTM (NMF-LTM), which to our best knowledge is distinctive from the popular LDA-based LTMs. NMF-LTM achieves lifelong learning by introducing word-word graph Laplacian as semantic affinity regularization. Other priors such as sparsity, diversity, and between-class affinity are incorporated as well for better performance, and a theoretical guarantee is provided for the algorithmic convergence to a local minimum. Extensive experiments on various public corpora demonstrate the effectiveness of NMF-LTM, particularly its human-like behaviors in two carefully designed learning tasks and the ability in topic modeling of big data. A further exploration of semantic relatedness in knowledge graphs and a case study on a large-scale real-world corpus exhibit the strength of NMF-LTM in discovering high-quality topics in an efficient and robust way.}
}


@article{DBLP:journals/tkde/NguyenCVD20,
	author = {Hung T. Nguyen and
                  Alberto Cano and
                  Tam Vu and
                  Thang N. Dinh},
	title = {Blocking Self-Avoiding Walks Stops Cyber-Epidemics: {A} Scalable GPU-Based
                  Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1263--1275},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904969},
	doi = {10.1109/TKDE.2019.2904969},
	timestamp = {Mon, 16 Aug 2021 11:45:32 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NguyenCVD20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyber-epidemics, the widespread of fake news or propaganda through social media, can cause devastating economic and political consequences. A common countermeasure against cyber-epidemics is to disable a small subset of suspected social connections or accounts to effectively contain the epidemics. An example is the recent shutdown of 125,000 ISIS-related Twitter accounts. Despite many proposed methods to identify such a subset, none are scalable enough to provide high-quality solutions in nowadays’ billion-size networks. To this end, we investigate the Spread Interdiction problems that seek the most effective links (or nodes) for removal under the well-known Linear Threshold model. We propose novel CPU-GPU methods that scale to networks with billions of edges , yet possess rigorous theoretical guarantee on the solution quality. At the core of our methods is an\nO(1)\nO(1)-space out-of-core algorithm to generate a new type of random walks, called Hitting Self-avoiding Walks (\nHSAW\ns). Such a low memory requirement enables handling of big networks and, more importantly, hiding latency via scheduling of millions of threads on GPUs. Comprehensive experiments on real-world networks show that our algorithms provide much higher quality solutions and are several orders of magnitude faster than the state-of-the art. Comparing to the (single-core) CPU counterpart, our GPU implementations achieve significant speedup factors up to 177x on a single GPU and 338x on a GPU pair.}
}


@article{DBLP:journals/tkde/LiuLGSNH20,
	author = {Bin Liu and
                  Ying Li and
                  Soumya Ghosh and
                  Zhaonan Sun and
                  Kenney Ng and
                  Jianying Hu},
	title = {Complication Risk Profiling in Diabetes Care: {A} Bayesian Multi-Task
                  and Feature Relationship Learning Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1276--1289},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904060},
	doi = {10.1109/TKDE.2019.2904060},
	timestamp = {Fri, 23 Oct 2020 15:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLGSNH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Diabetes mellitus, commonly known as diabetes, is a chronic disease that often results in multiple complications. Risk prediction of diabetes complications is critical for healthcare professionals to design personalized treatment plans for patients in diabetes care for improved outcomes. In this paper, focusing on Type 2 diabetes mellitus (T2DM), we study the risk of developing complications after the initial T2DM diagnosis from longitudinal patient records. We propose a novel multi-task learning approach to simultaneously model multiple complications where each task corresponds to the risk modeling of one complication. Specifically, the proposed method strategically captures the relationships (1) between the risks of multiple T2DM complications, (2) between different risk factors, and (3) between the risk factor selection patterns, which assumes similar complications have similar contributing risk factors. The method uses coefficient shrinkage to identify an informative subset of risk factors from high-dimensional data, and uses a hierarchical Bayesian framework to allow domain knowledge to be incorporated as priors. The proposed method is favorable for healthcare applications because in addition to improved prediction performance, relationships among the different risks and among risk factors are also identified. Extensive experimental results on a large electronic medical claims database show that the proposed method outperforms state-of-the-art models by a significant margin. Furthermore, we show that the risk associations learned and the risk factors identified lead to meaningful clinical insights.}
}


@article{DBLP:journals/tkde/DuMZ20,
	author = {Yulu Du and
                  Xiangwu Meng and
                  Yujie Zhang},
	title = {{CVTM:} {A} Content-Venue-Aware Topic Model for Group Event Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1290--1303},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904066},
	doi = {10.1109/TKDE.2019.2904066},
	timestamp = {Tue, 30 Jun 2020 11:40:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DuMZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Event recommendation is essential to help people find attractive events to attend, but it intrinsically faces cold-start problem. The previous studies exploit multiple contextual factors to overcome the cold-start problem in event recommendation. However, they do not consider the correlation among different contextual factors. Moreover, suggesting events for a group of users also has not been well studied. In this paper, we first discover the correlation between organizer and textual content, i.e., the events held by the same organizer tend to have more similar content. Based on this observation, we present a content-venue-aware topic model (CVTM) to capture group interests on an event from two perspectives: content and venue. The correlation between organizer and content is modeled in CVTM to alleviate the sparsity of textual content, and then we can further extract group interests on content of an event more accurately. Finally, a group event recommendation method using CVTM is proposed. We conduct comprehensive experiments to evaluate the recommendation performance of our model on two real-world datasets. The results demonstrate that the proposed model outperforms the state-of-the-art methods that suggest upcoming events for groups. Besides, CVTM can learn semantically coherent latent topics which are useful to explain recommendations.}
}


@article{DBLP:journals/tkde/FangYD20,
	author = {Lei Fang and
                  Juan Ye and
                  Simon Dobson},
	title = {Discovery and Recognition of Emerging Human Activities Using a Hierarchical
                  Mixture of Directional Statistical Models},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1304--1316},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2905207},
	doi = {10.1109/TKDE.2019.2905207},
	timestamp = {Thu, 14 Oct 2021 08:57:06 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FangYD20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human activity recognition plays a significant role in enabling pervasive applications as it abstracts low-level noisy sensor data into high-level human activities, which applications can respond to. With more and more activity-aware applications deployed in real-world environments, a research challenge emerges—discovering and learning new activities that have not been pre-defined or observed in the training phase. This paper tackles this challenge by proposing a hierarchical mixture of directional statistical models. The model supports incrementally, continuously updating the activity model over time with the reduced annotation effort and without the need for storing historical sensor data. We have validated this solution on four publicly available, third-party smart home datasets, and have demonstrated up to 91.5 percent accuracies of detecting and recognising new activities.}
}


@article{DBLP:journals/tkde/YuLPXCQ20,
	author = {Wenhui Yu and
                  Jinfei Liu and
                  Jian Pei and
                  Li Xiong and
                  Xu Chen and
                  Zheng Qin},
	title = {Efficient Contour Computation of Group-Based Skyline},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1317--1332},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2905239},
	doi = {10.1109/TKDE.2019.2905239},
	timestamp = {Wed, 27 Apr 2022 16:59:06 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YuLPXCQ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Skyline, aiming at finding a Pareto optimal subset of points in a multi-dimensional dataset, has gained great interest due to its extensive use for multi-criteria analysis and decision making. The skyline consists of all points that are not dominated by any other points. It is a candidate set of the optimal solution, which depends on a specific evaluation criterion for optimum. However, conventional skyline queries, which return individual points, are inadequate in group querying case since optimal combinations are required. To address this gap, we study the skyline computation in the group level and propose efficient methods to find the Group-based skyline (G-skyline). For computing the front l skyline layers, we lay out an efficient approach that does the search concurrently on each dimension and investigates each point in the subspace. After that, we present a novel structure to construct the G-skyline with a queue of combinations of the first-layer points. We further demonstrate that the G-skyline is a complete candidate set of top-l solutions, which is the main superiority over previous group-based skyline definitions. However, as G-skyline is complete, it contains a large number of groups which can make it impractical. To represent the “contour” of the G-skyline, we define the Representative G-skyline (RG-skyline). Then, we propose a Group-based clustering (G-clustering) algorithm to find out RG-skyline groups. Experimental results show that our algorithms are several orders of magnitude faster than the previous work.}
}


@article{DBLP:journals/tkde/ZhangPFP20,
	author = {Jianpeng Zhang and
                  Yulong Pei and
                  George Fletcher and
                  Mykola Pechenizkiy},
	title = {Evaluation of the Sample Clustering Process on Graphs},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1333--1347},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904682},
	doi = {10.1109/TKDE.2019.2904682},
	timestamp = {Mon, 15 Nov 2021 16:15:59 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangPFP20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An increasing number of networks are becoming large-scale and continuously growing in nature, such that clustering on them in their entirety could be intractable. A feasible way to overcome this problem is to sample a representative subgraph and exploit its clustering structure (namely, sample clustering process ). However, there are two issues that we should address in current studies. One underlying question is how to evaluate the clustering quality of the entire sample clustering process . Another non-trivial issue is that multiple ground-truths exist in networks, thus evaluating the clustering results in such scenario is also a challenging task. In this paper, first we utilize the set-matching methodology to quantitatively evaluate how differently the clusters of the sampled counterpart correspond to the ground-truth(s) in the original graph, and propose several new quality metrics to capture the differences of clustering structure in various aspects. Second, we put forward an evaluation framework for the general problems of evaluating the clustering quality on graph samples. Extensive experiments on various synthetic and real-world graphs demonstrate that our new quality metrics are more accurate and insightful for the sample clustering evaluation than conventional metrics (e.g., NMI). Thus the evaluation framework is effective and practical to assess the clustering quality of the sample clustering process on massive graphs.}
}


@article{DBLP:journals/tkde/BuLZCLS20,
	author = {Zhan Bu and
                  Hui{-}Jia Li and
                  Chengcui Zhang and
                  Jie Cao and
                  Aihua Li and
                  Yong Shi},
	title = {Graph K-means Based on Leader Identification, Dynamic Game, and Opinion
                  Dynamics},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1348--1361},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2903712},
	doi = {10.1109/TKDE.2019.2903712},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BuLZCLS20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosion of social media networks, many modern applications are concerning about people's connections, which leads to the so-called social computing . An elusive question is to study how opinion communities form and evolve in real-world networks with great individual diversity and complex human connections. In this scenario, the classic K-means technique and its extended versions could not be directly applied, as they largely ignore the relationship among interactive objects. On the other side, traditional community detection approaches in statistical physics would be neither adequate nor fair: they only consider the network topological structure but ignore the heterogeneous-objects’ attributive information. To this end, we attempt to model a realistic social media network as a discrete-time dynamical system, where the opinion matrix and the community structure could mutually affect each other. In this paper, community detection in social media networks is naturally formulated as a multi-objective optimization problem (MOOP), i.e., finding a set of densely connected components with similar opinion vectors. We propose a novel and powerful graph K-means framework, which is composed of three coupled phases in each discrete-time period. Specifically, the first phase uses a fast heuristic approach to identify those opinion leaders who have relatively high local reputation; the second phase adopts a novel dynamic game model to find the locally Pareto-optimal community structure; and the final phase employs a robust opinion dynamics model to simulate the evolution of the opinion matrix. We conduct a series of comprehensive experiments on real-world benchmark networks to validate the performance of GK-means through comparisons with the state-of-the-art graph clustering technologies.}
}


@article{DBLP:journals/tkde/SongHTBY20,
	author = {Jie Song and
                  HongYan He and
                  Richard Thomas and
                  Yubin Bao and
                  Ge Yu},
	title = {Haery: {A} Hadoop Based Query System on Accumulative and High-Dimensional
                  Data Model for Big Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1362--1377},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904056},
	doi = {10.1109/TKDE.2019.2904056},
	timestamp = {Fri, 28 Aug 2020 15:30:51 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/SongHTBY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Column-oriented stores, known for their scalability and flexibility, are a common NoSQL database implementation and are increasingly used in big data management. In column-oriented stores, a “full-scan” query strategy is inefficient and the search space can be reduced if data is well partitioned or indexed; however, there is no pre-defined schema for building and maintaining partitions and indexes at lower cost. We leverage an accumulative and high-dimensional data model, a sophisticated linearization algorithm, and an efficient query algorithm, to solve the challenge of how a pre-defined and well-partitioned data model can be applied to flexible and time-varied key-value data. We adapt a high-dimensional array as the data model to partition the key-value data without additional storage and massive calculation; improve the Z-order linearization algorithm, which map multidimensional data to one dimension while preserving locality of the data points, for flexibility; efficiently build an expansion mechanism for the data model to support time-varied data. The result is Haery, a column-oriented store, based on a distributed file system and computing framework. In experiments, Haery is compared with Hive, HBase, Cassandra, MongoDB, PostgresXL, and HyperDex in terms of query performance. With results indicating Haery on average performs 4.57x, 4.23x, 3.55x, 1.79x, 1.82x, and 120.6x faster, respectively.}
}


@article{DBLP:journals/tkde/ChenTY20,
	author = {Zhen Chen and
                  Hanghang Tong and
                  Lei Ying},
	title = {Inferring Full Diffusion History from Partial Timestamps},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1378--1392},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2905210},
	doi = {10.1109/TKDE.2019.2905210},
	timestamp = {Mon, 02 Oct 2023 15:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChenTY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Understanding diffusion processes in networks has emerged as an important research topic because of its wide range of applications. Analysis of diffusion traces can help us answer important questions such as the source(s) of diffusion and the role of each node during the diffusion process. However, in large-scale networks, due to the cost and privacy concerns, it is almost impossible to monitor the entire network and collect the complete diffusion trace. In this paper, we tackle the problem of reconstructing the diffusion history from a partial observation. We formulate the diffusion history reconstruction problem as a maximum a posteriori (MAP) problem and prove the problem is NP-hard. Then, we propose a step-by-step reconstruction algorithm, which can always produce a diffusion history that is consistent with the partial observation. Our experimental results based on synthetic and real networks show that the algorithm significantly outperforms some existing methods.}
}


@article{DBLP:journals/tkde/LiuGTSYH20,
	author = {Wenhe Liu and
                  Dong Gong and
                  Mingkui Tan and
                  Qinfeng (Javen) Shi and
                  Yi Yang and
                  Alexander G. Hauptmann},
	title = {Learning Distilled Graph for Large-Scale Social Network Data Clustering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1393--1404},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904068},
	doi = {10.1109/TKDE.2019.2904068},
	timestamp = {Mon, 28 Aug 2023 21:37:41 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuGTSYH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectral analysis is critical in social network analysis. As a vital step of the spectral analysis, the graph construction in many existing works utilizes content data only. Unfortunately, the content data often consists of noisy, sparse, and redundant features, which makes the resulting graph unstable and unreliable. In practice, besides the content data, social network data also contain link information, which provides additional information for graph construction. Some of previous works utilize the link data. However, the link data is often incomplete, which makes the resulting graph incomplete. To address these issues, we propose a novel Distilled Graph Clustering (DGC) method. It pursuits a distilled graph based on both the content data and the link data. The proposed algorithm alternates between two steps: in the feature selection step, it finds the most representative feature subset w.r.t. an intermediate graph initialized with link data; in graph distillation step, the proposed method updates and refines the graph based on only the selected features. The final resulting graph, which is referred to as the distilled graph, is then utilized for spectral clustering on the large-scale social network data. Extensive experiments demonstrate the superiority of the proposed method.}
}


@article{DBLP:journals/tkde/ZhangGHCL20,
	author = {Kaiqi Zhang and
                  Hong Gao and
                  Xixian Han and
                  Zhipeng Cai and
                  Jianzhong Li},
	title = {Modeling and Computing Probabilistic Skyline on Incomplete Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1405--1418},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904967},
	doi = {10.1109/TKDE.2019.2904967},
	timestamp = {Wed, 02 Aug 2023 15:25:33 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangGHCL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The skyline query is important in the database community. In recent years, the researches on incomplete data have been increasingly considered, especially for the skyline query. However, the existing skyline definition on incomplete data cannot provide users with valuable references. In this paper, we propose a novel skyline definition utilizing probabilistic model on incomplete data where each point has a probability to be in the skyline. In particular, it returns K points with the highest skyline probabilities. In addition, we propose incomplete models and estimate probability density functions of missing values on independent, correlated, and anti-correlated distributions, respectively. Meanwhile, it is a big challenge to compute probabilistic skyline on incomplete data. We propose three efficient algorithms SPISkyline, SPCSkyline, and SPASkyline for probabilistic skyline computation on incomplete data complying with independent, correlated, and anti-correlated distributions, respectively. They employ pruning strategy, optimization of the process of probability computation, and sorting technique to improve the efficiency of probabilistic skyline computation on incomplete data. Our experimental results demonstrate that our proposed concept of probabilistic skyline is an effective method to tackle skyline query on incomplete data and our algorithms are tens of times faster than the naive algorithm on both synthetic and real datasets.}
}


@article{DBLP:journals/tkde/XuHNL20,
	author = {Jinglin Xu and
                  Junwei Han and
                  Feiping Nie and
                  Xuelong Li},
	title = {Multi-View Scaling Support Vector Machines for Classification and
                  Feature Selection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1419--1430},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904256},
	doi = {10.1109/TKDE.2019.2904256},
	timestamp = {Mon, 14 Feb 2022 16:41:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XuHNL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive growth of data, the multi-view data is widely used in many fields, such as data mining, machine learning, computer vision, and so on. Because such data always has a complex structure, i.e., many categories, many perspectives of description and high dimension, how to formulate an accurate and reliable framework for the multi-view classification is a very challenging task. In this paper, we propose a novel multi-view classification method by using multiple multi-class Support Vector Machines (SVMs) with a novel collaborative strategy. Here, each multi-class SVM embeds the scaling factor to renewedly adjust the weight allocation of all features, which is beneficial to highlight more important and discriminative features. Furthermore, we adopt the decision function values to integrate multiple multi-class learners and introduce the confidence score across multiple classes to determine the final classification result. In addition, through a series of the mathematical deduction, we bridge the proposed model with the solvable problem and solve it through an alternating iteration optimization method. We evaluate the proposed method on several image and face datasets, and the experimental results demonstrate that our proposed method performs better than other state-of-the-art learning algorithms.}
}


@article{DBLP:journals/tkde/ZhuLLX20,
	author = {Haoyang Zhu and
                  Xiaoyong Li and
                  Qiang Liu and
                  Zichen Xu},
	title = {Top-k Dominating Queries on Skyline Groups},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {7},
	pages = {1431--1444},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2904065},
	doi = {10.1109/TKDE.2019.2904065},
	timestamp = {Tue, 06 Sep 2022 16:39:57 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuLLX20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The top-k dominating (TKD) query on skyline groups returns k skyline groups that dominate the maximum number of points in a given data set. The TKD query combines the advantages of skyline groups and top-k dominating queries, thus has been frequently used in decision making, recommendation systems, and quantitative economics. Traditional skylines are inadequate to answer queries from both individual and groups of points. The group size could be too large to be processed in a reasonable time as a single operator (i.e., the skyline group operator). In this paper, we address the performance problem of grouping for TKD queries in skyline database. We formulate the problem of grouping, define the group operator in skyline, and propose several efficient algorithms to find top-k skyline groups. Thus, we provide a systematic study of TKD queries on skyline groups and validate our algorithms with extensive empirical results on synthetic and realworld data.}
}


@article{DBLP:journals/tkde/PohlBH20,
	author = {Daniela Pohl and
                  Abdelhamid Bouchachia and
                  Hermann Hellwagner},
	title = {Active Online Learning for Social Media Analysis to Support Crisis
                  Management},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1445--1458},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2906173},
	doi = {10.1109/TKDE.2019.2906173},
	timestamp = {Thu, 06 Aug 2020 21:46:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PohlBH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {People use social media (SM) to describe and discuss different situations they are involved in, like crises. It is therefore worthwhile to exploit SM contents to support crisis management, in particular by revealing useful and unknown information about the crises in real-time. Hence, we propose a novel active online multiple-prototype classifier, called AOMPC. It identifies relevant data related to a crisis. AOMPC is an online learning algorithm that operates on data streams and which is equipped with active learning mechanisms to actively query the label of ambiguous unlabeled data. The number of queries is controlled by a fixed budget strategy. Typically, AOMPC accommodates partly labeled data streams. AOMPC was evaluated using two types of data: (1) synthetic data and (2) SM data from Twitter related to two crises, Colorado Floods and Australia Bushfires. To provide a thorough evaluation, a whole set of known metrics was used to study the quality of the results. Moreover, a sensitivity analysis was conducted to show the effect of AOMPC's parameters on the accuracy of the results. A comparative study of AOMPC against other available online learning algorithms was performed. The experiments showed very good behavior of AOMPC for dealing with evolving, partly-labeled data streams.}
}


@article{DBLP:journals/tkde/ZhuQYC20,
	author = {Yuanyuan Zhu and
                  Lu Qin and
                  Jeffrey Xu Yu and
                  Hong Cheng},
	title = {Answering Top-{\textdollar}k{\textdollar} k Graph Similarity Queries
                  in Graph Databases},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1459--1474},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2906608},
	doi = {10.1109/TKDE.2019.2906608},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhuQYC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Searching similar graphs in graph databases for a query graph has attracted extensive attention recently. Existing works on graph similarity queries are threshold based approaches which return graphs with distances to the query smaller than a given threshold. However, in many applications the number of answer graphs for the same threshold can vary significantly for different queries. In this paper, we study the problem of finding top-k most similar graphs for a query under the distance measure based on maximum common subgraph (MCS). Since computing MCS is NP-hard, we devise a novel framework to prune unqualified graphs based on the lower bounds of graph distance, and accordingly derive four lower bounds with different tightness and computational cost for pruning. To further reduce the number of MCS computations, we also propose an improved framework based on both lower and upper bounds, and derive three new upper bounds. To support efficient pruning, we design three indexes with different tradeoffs between pruning power and construction cost. To accelerate the index construction, we explore bound relaxation techniques, based on which approximate indexes can be efficiently built. We conducted extensive performance studies on real-life graph datasets to validate the effectiveness and efficiency of our approaches.}
}


@article{DBLP:journals/tkde/LiZSWLZL20,
	author = {Wen Li and
                  Ying Zhang and
                  Yifang Sun and
                  Wei Wang and
                  Mingjie Li and
                  Wenjie Zhang and
                  Xuemin Lin},
	title = {Approximate Nearest Neighbor Search on High Dimensional Data - Experiments,
                  Analyses, and Improvement},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1475--1488},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2909204},
	doi = {10.1109/TKDE.2019.2909204},
	timestamp = {Thu, 05 Aug 2021 09:02:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiZSWLZL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nearest neighbor search is a fundamental and essential operation in applications from many domains, such as databases, machine learning, multimedia, and computer vision. Because exact searching results are not efficient for a high-dimensional space, a lot of efforts have turned to approximate nearest neighbor search. Although many algorithms have been continuously proposed in the literature each year, there is no comprehensive evaluation and analysis of their performance. In this paper, we conduct a comprehensive experimental evaluation of many state-of-the-art methods for approximate nearest neighbor search. Our study (1) is cross-disciplinary (i.e., including 19 algorithms in different domains, and from practitioners) and (2) has evaluated a diverse range of settings, including 20 datasets, several evaluation metrics, and different query workloads. The experimental results are carefully reported and analyzed to understand the performance results. Furthermore, we propose a new method that achieves both high query efficiency and high recall empirically on majority of the datasets under a wide range of settings.}
}


@article{DBLP:journals/tkde/RammelaereG20,
	author = {Joeri Rammelaere and
                  Floris Geerts},
	title = {Cleaning Data with Forbidden Itemsets},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1489--1501},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2905548},
	doi = {10.1109/TKDE.2019.2905548},
	timestamp = {Tue, 29 Dec 2020 18:17:25 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/RammelaereG20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Methods for cleaning dirty data typically employ additional information about the data such as user-provided constraints specifying when data is dirty, e.g., domain restrictions, illegal value combinations, or logical rules. However, real-world scenarios usually only have dirty data available, without known constraints. In such settings, constraints are automatically discovered on dirty data and discovered constraints are used to detect and repair errors. Typical repairing processes stop there. Yet, when constraint discovery algorithms are re-run on the repaired data (assumed to be clean), new constraints and thus errors are often found. The repairing process then introduces new constraint violations. We present a different type of repairing method, which prevents introducing new constraint violations, according to a discovery algorithm. Summarily, our repairs guarantee that all errors identified by constraints discovered on the dirty data are fixed; and the constraint discovery process cannot identify new constraint violations. We do this for a new kind of constraints, called forbidden itemsets (FBIs), capturing unlikely value co-occurrences. We show that FBIs detect errors with high precision. Evaluation on real-world data shows that our repair method obtains high-quality repairs without introducing new FBIs. Optional user interaction is readily integrated, with users deciding how much effort to invest.}
}


@article{DBLP:journals/tkde/LinRCRMR20,
	author = {Yujie Lin and
                  Pengjie Ren and
                  Zhumin Chen and
                  Zhaochun Ren and
                  Jun Ma and
                  Maarten de Rijke},
	title = {Explainable Outfit Recommendation with Joint Outfit Matching and Comment
                  Generation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1502--1516},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2906190},
	doi = {10.1109/TKDE.2019.2906190},
	timestamp = {Thu, 23 Jun 2022 20:04:49 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LinRCRMR20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most previous work on outfit recommendation focuses on designing visual features to enhance recommendations. Existing work neglects user comments of fashion items, which have been proven to be effective in generating explanations along with better recommendation results. We propose a novel neural network framework, neural outfit recommendation (NOR), that simultaneously provides outfit recommendations and generates abstractive comments. Neural outfit recommendation (NOR) consists of two parts: outfit matching and comment generation. For outfit matching, we propose a convolutional neural network with a mutual attention mechanism to extract visual features. The visual features are then decoded into a rating score for the matching prediction. For abstractive comment generation, we propose a gated recurrent neural network with a cross-modality attention mechanism to transform visual features into a concise sentence. The two parts are jointly trained based on a multi-task learning framework in an end-to-end back-propagation paradigm. Extensive experiments conducted on an existing dataset and a collected real-world dataset show NOR achieves significant improvements over state-of-the-art baselines for outfit recommendation. Meanwhile, our generated comments achieve impressive ROUGE and BLEU scores in comparison to human-written comments. The generated comments can be regarded as explanations for the recommendation results. We release the dataset and code to facilitate future research.}
}


@article{DBLP:journals/tkde/LiuLZJSWH20,
	author = {Ye{-}Zheng Liu and
                  Zhe Li and
                  Chong Zhou and
                  Yuanchun Jiang and
                  Jianshan Sun and
                  Meng Wang and
                  Xiangnan He},
	title = {Generative Adversarial Active Learning for Unsupervised Outlier Detection},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1517--1528},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2905606},
	doi = {10.1109/TKDE.2019.2905606},
	timestamp = {Wed, 26 Aug 2020 11:04:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLZJSWH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Outlier detection is an important topic in machine learning and has been used in a wide range of applications. In this paper, we approach outlier detection as a binary-classification issue by sampling potential outliers from a uniform reference distribution. However, due to the sparsity of data in high-dimensional space, a limited number of potential outliers may fail to provide sufficient information to assist the classifier in describing a boundary that can separate outliers from normal data effectively. To address this, we propose a novel Single-Objective Generative Adversarial Active Learning (SO-GAAL) method for outlier detection, which can directly generate informative potential outliers based on the mini-max game between a generator and a discriminator. Moreover, to prevent the generator from falling into the mode collapsing problem, the stop node of training should be determined when SO-GAAL is able to provide sufficient information. But without any prior information, it is extremely difficult for SO-GAAL. Therefore, we expand the network structure of SO-GAAL from a single generator to multiple generators with different objectives (MO-GAAL), which can generate a reasonable reference distribution for the whole dataset. We empirically compare the proposed approach with several state-of-the-art outlier detection methods on both synthetic and real-world datasets. The results show that MO-GAAL outperforms its competitors in the majority of cases, especially for datasets with various cluster types or high irrelevant variable ratio. The experiment codes are available at: https://github.com/leibinghe/GAAL-based-outlier-detection.}
}


@article{DBLP:journals/tkde/HeBRLLHZ20,
	author = {Tianfu He and
                  Jie Bao and
                  Sijie Ruan and
                  Ruiyuan Li and
                  Yanhua Li and
                  Hui He and
                  Yu Zheng},
	title = {Interactive Bike Lane Planning Using Sharing Bikes' Trajectories},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1529--1542},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2907091},
	doi = {10.1109/TKDE.2019.2907091},
	timestamp = {Fri, 28 Aug 2020 09:06:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HeBRLLHZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cycling as a green transportation mode has been promoted by many governments all over the world. As a result, constructing effective bike lanes has become a crucial task to promote the cycling life style, as well-planned bike lanes can reduce traffic congestions and safety risks. Unfortunately, existing trajectory mining approaches for bike lane planning do not consider one or more key realistic government constraints: 1) budget limitations, 2) construction convenience, and 3) bike lane utilization. In this paper, we propose a data-driven approach to develop bike lane construction plans based on the large-scale real world bike trajectory data collected from Mobike, a station-less bike sharing system. We enforce these constraints to formulate our problem and introduce a flexible objective function to tune the benefit between coverage of users and the length of their trajectories. We prove the NP-hardness of the problem and propose greedy-based heuristics to address it. To improve the efficiency of the bike lane planning system for the urban planner, we propose a novel trajectory indexing structure and deploy the system based on a parallel computing framework (Storm) to improve the system's efficiency. Finally, extensive experiments and case studies are provided to demonstrate the system efficiency and effectiveness.}
}


@article{DBLP:journals/tkde/LiuH20,
	author = {Chien{-}Liang Liu and
                  Po{-}Yen Hsieh},
	title = {Model-Based Synthetic Sampling for Imbalanced Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1543--1556},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2905559},
	doi = {10.1109/TKDE.2019.2905559},
	timestamp = {Thu, 06 Aug 2020 21:46:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Imbalanced data is characterized by the severe difference in observation frequency between classes and has received a lot of attention in data mining research. The prediction performances usually deteriorate as classifiers learn from imbalanced data, as most classifiers assume the class distribution is balanced or the costs for different types of classification errors are equal. Although several methods have been devised to deal with imbalance problems, it is still difficult to generalize those methods to achieve stable improvement in most cases. In this study, we propose a novel framework called model-based synthetic sampling (MBS) to cope with imbalance problems, in which we integrate modeling and sampling techniques to generate synthetic data. The key idea behind the proposed method is to use regression models to capture the relationship between features and to consider data diversity in the process of data generation. We conduct experiments on 13 datasets and compare the proposed method with 10 methods. The experimental results indicate that the proposed method is not only comparative but also stable. We also provide detailed investigations and visualizations of the proposed method to empirically demonstrate why it could generate good data samples.}
}


@article{DBLP:journals/tkde/ChengTSCWZ20,
	author = {Xiang Cheng and
                  Peng Tang and
                  Sen Su and
                  Rui Chen and
                  Zequn Wu and
                  Binyuan Zhu},
	title = {Multi-Party High-Dimensional Data Publishing Under Differential Privacy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1557--1571},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2906610},
	doi = {10.1109/TKDE.2019.2906610},
	timestamp = {Wed, 27 Apr 2022 19:47:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ChengTSCWZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the problem of publishing high-dimensional data in a distributed multi-party environment under differential privacy. In particular, with the assistance of a semi-trusted curator, the parties (i.e., local data owners) collectively generate a synthetic integrated dataset while satisfying\nε\n-differential privacy. To solve this problem, we present a differentially private sequential update of Bayesian network (DP-SUBN) approach. In DP-SUBN, the parties and the curator collaboratively identify the Bayesian network\nN\nthat best fits the integrated dataset in a sequential manner, from which a synthetic dataset can then be generated. The fundamental advantage of adopting the sequential update manner is that the parties can treat the intermediate results provided by previous parties as their prior knowledge to direct how to learn\nN\n. The core of DP-SUBN is the construction of the search frontier, which can be seen as a priori knowledge to guide the parties to update\nN\n. By exploiting the correlations of attribute pairs, we propose exact and heuristic methods to construct the search frontier. In particular, to privately quantify the correlations of attribute pairs without introducing too much noise, we first put forward a non-overlapping covering design (NOCD) method, and then devise a dynamic programming method for determining the optimal parameters used in NOCD. Through privacy analysis, we show that DP-SUBN satisfies\nε\n-differential privacy. Extensive experiments on real datasets demonstrate that DP-SUBN offers desirable data utility with low communication cost.}
}


@article{DBLP:journals/tkde/YiBRLNB20,
	author = {Xun Yi and
                  Elisa Bertino and
                  Fang{-}Yu Rao and
                  Kwok{-}Yan Lam and
                  Surya Nepal and
                  Athman Bouguettaya},
	title = {Privacy-Preserving User Profile Matching in Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1572--1585},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2912748},
	doi = {10.1109/TKDE.2019.2912748},
	timestamp = {Sat, 05 Sep 2020 17:44:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/YiBRLNB20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider a scenario where a user queries a user profile database, maintained by a social networking service provider, to identify users whose profiles match the profile specified by the querying user. A typical example of this application is online dating. Most recently, an online dating website, Ashley Madison, was hacked, which resulted in a disclosure of a large number of dating user profiles. This data breach has urged researchers to explore practical privacy protection for user profiles in a social network. In this paper, we propose a privacy-preserving solution for profile matching in social networks by using multiple servers. Our solution is built on homomorphic encryption and allows a user to find out matching users with the help of multiple servers without revealing to anyone the query and the queried user profiles in clear. Our solution achieves user profile privacy and user query privacy as long as at least one of the multiple servers is honest. Our experiments demonstrate that our solution is practical.}
}


@article{DBLP:journals/tkde/WongY20,
	author = {Tzu{-}Tsung Wong and
                  Po{-}Yang Yeh},
	title = {Reliable Accuracy Estimates from k-Fold Cross Validation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1586--1594},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2912815},
	doi = {10.1109/TKDE.2019.2912815},
	timestamp = {Thu, 06 Aug 2020 21:46:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WongY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is popular to evaluate the performance of classification algorithms by k-fold cross validation. A reliable accuracy estimate will have a relatively small variance, and several studies therefore suggested to repeatedly perform k-fold cross validation. Most of them did not consider the correlation among the replications of k-fold cross validation, and hence the variance could be underestimated. The purpose of this study is to explore whether k-fold cross validation should be repeatedly performed for obtaining reliable accuracy estimates. The dependency relationships between the predictions of the same instance in two replications of k-fold cross validation are first analyzed for k-nearest neighbors with k = 1. Then, statistical methods are proposed to test the strength of the dependency level between the accuracy estimates resulting from two replications of k-fold cross validation. The experimental results on 20 data sets show that the accuracy estimates obtained from various replications of k-fold cross validation are generally highly correlated, and the correlation will be higher as the number of folds increases. The k-fold cross validation with a large number of folds and a small number of replications should be adopted for performance evaluation of classification algorithms.}
}


@article{DBLP:journals/tkde/LuoCKXZH20,
	author = {Siqiang Luo and
                  Reynold Cheng and
                  Ben Kao and
                  Xiaokui Xiao and
                  Shuigeng Zhou and
                  Jiafeng Hu},
	title = {{ROAM:} {A} Fundamental Routing Query on Road Networks with Efficiency},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1595--1609},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2906188},
	doi = {10.1109/TKDE.2019.2906188},
	timestamp = {Thu, 06 Aug 2020 21:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuoCKXZH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Novel road-network applications often recommend a moving object (e.g., a vehicle) about interesting services or tasks on its way to a destination. A taxi-sharing system, for instance, suggests a new passenger to a taxi while it is serving another one. The traveling cost is then shared among these passengers. A fundamental query is: given two nodes s and t, and an area A on road network graph , is there a “good” route (e.g., short enough path) P from s to t that crosses A in G? In a taxi-sharing system, s and t can be a taxi's current and destined locations, and A contains all the places to which a person waiting for a taxi is willing to walk. Answering this Route and Area Matching (ROAM) Query allows the application involved to recommend appropriate services to users efficiently. In this paper, we examine efficient ROAM query algorithms. Particularly, we develop solutions for finding a ρ-route, which is an s-t path that passes A, with a length of at most (1 + ρ) times the shortest distance between s and t. The existence of a ρ-route implies that a service or task located at A can be found for a given moving object m, and that m only deviates slightly from its current route. We present comprehensive studies on index-free and index-based algorithms for answering ROAM queries. Comprehensive experiments show that our algorithm runs up to 30 times faster than baseline algorithms.}
}


@article{DBLP:journals/tkde/XuLZ20,
	author = {Miao Xu and
                  Yufeng Li and
                  Zhi{-}Hua Zhou},
	title = {Robust Multi-Label Learning with {PRO} Loss},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1610--1624},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2908898},
	doi = {10.1109/TKDE.2019.2908898},
	timestamp = {Mon, 13 May 2024 08:02:23 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuLZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-label learning methods assign multiple labels to one object. In practice, in addition to differentiating relevant labels from irrelevant ones, it is often desired to rank relevant labels for an object, whereas the ranking of irrelevant labels is not important. Thus, we require an algorithm to do classification and ranking of relevant labels simultaneously. Such a requirement, however, cannot be met because most existing methods were designed to optimize existing criteria, yet there is no criterion which encodes the aforementioned requirement. In this paper, we present a new criterion, PRO LOSS, concerning the prediction of all labels as well as the ranking of only relevant labels. We then propose ProSVM which optimizes PRO LOSS efficiently using alternating direction method of multipliers. We further improve its efficiency with an upper approximation that reduces the number of constraints from O(T 2 ) to O(T), where T is the number of labels. We then notice that in real applications, it is difficult to get full supervised information for multi-label data. To make the proposed algorithm more robust to supervised information, we adapt ProSVM to deal with the multi-label learning with partial labels problem. Experiments show that our proposal is not only superior on PRO LOSS, but also highly competitive on existing evaluation criteria.}
}


@article{DBLP:journals/tkde/KafezaKMPV20,
	author = {Eleanna Kafeza and
                  Andreas Kanavos and
                  Christos Makris and
                  Georgios Pispirigos and
                  Pantelis Vikatos},
	title = {{T-PCCE:} Twitter Personality based Communicative Communities Extraction
                  System for Big Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1625--1638},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2906197},
	doi = {10.1109/TKDE.2019.2906197},
	timestamp = {Mon, 01 Jul 2024 13:19:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KafezaKMPV20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The identification of social media communities has recently been of major concern, since users participating in such communities can contribute to viral marketing campaigns. In this work, we focus on users’ communication considering personality as a key characteristic for identifying communicative networks i.e., networks with high information flows. We describe the Twitter Personality based Communicative Communities Extraction (T-PCCE) system that identifies the most communicative communities in a Twitter network graph considering users’ personality. We then expand existing approaches in users’ personality extraction by aggregating data that represent several aspects of user behavior using machine learning techniques. We use an existing modularity based community detection algorithm and we extend it by inserting a post-processing step that eliminates graph edges based on users’ personality. The effectiveness of our approach is demonstrated by sampling the Twitter graph and comparing the communication strength of the extracted communities with and without considering the personality factor. We define several metrics to count the strength of communication within each community. Our algorithmic framework and the subsequent implementation employ the cloud infrastructure and use the MapReduce Programming Environment. Our results show that the T-PCCE system creates the most communicative communities.}
}


@article{DBLP:journals/tkde/LiLMR20,
	author = {Hui Li and
                  Ye Liu and
                  Nikos Mamoulis and
                  David S. Rosenblum},
	title = {Translation-Based Sequential Recommendation for Complex Users on Sparse
                  Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1639--1651},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2906180},
	doi = {10.1109/TKDE.2019.2906180},
	timestamp = {Wed, 02 Sep 2020 18:40:34 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiLMR20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation is one of the main tasks in recommender systems, where the next action (e.g., purchase, visit, and click) of the user is predicted based on his/her past sequence of actions. Translating Embeddings is a knowledge graph completion approach which was recently adapted to a translation-based sequential recommendation (TransRec) method. We observe a flaw of TransRec when handling complex translations, which hinders it from generating accurate suggestions. In view of this, we propose a translation-based recommender for complex users (CTransRec), which utilizes category-specific projection and temporal dynamic relaxation. Using our proposed Margin-based Pairwise Bayesian Personalized Ranking and Time-Aware Negative Sampling, CTransRec outperforms state-of-the-art methods for sequential recommendation on extremely sparse data. The superiority of CTransRec, which is confirmed by our extensive experiments on both public data and real data obtained from the industry, comes from not only the additional information used in training but also the fact that CTransRec makes good use of this additional information to model the complex translations.}
}


@article{DBLP:journals/tkde/ZhaoJLGCZX20,
	author = {Hongke Zhao and
                  Binbin Jin and
                  Qi Liu and
                  Yong Ge and
                  Enhong Chen and
                  Xi Zhang and
                  Tong Xu},
	title = {Voice of Charity: Prospecting the Donation Recurrence {\&} Donor
                  Retention in Crowdfunding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {8},
	pages = {1652--1665},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2906199},
	doi = {10.1109/TKDE.2019.2906199},
	timestamp = {Sun, 02 Oct 2022 15:51:33 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoJLGCZX20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online donation-based crowdfunding has brought new life to charity by soliciting small monetary contributions from crowd donors to help others in trouble or with dreams. However, a crucial issue for crowdfunding platforms as well as traditional charities is the problem of high donor attrition, i.e., many donors donate only once or very few times within a rather short lifecycle and then leave. Thus, it is an urgent task to analyze the factors of and then further predict the donors behaviors. Especially, we focus on two types of behavioral events, e.g., donation recurrence (whether one donor will make donations at some time slices in the future) and donor retention (whether she will remain on the crowdfunding platform until a future time). However, this problem has not been well explored due to many domain and technical challenges, such as the heterogeneous influence, the relevance of the two types of events, and the censoring phenomenon of retention records. In this paper, we present a focused study on donation recurrence and donor retention with the help of large-scale behavioral data collected from crowdfunding. Specifically, we propose a Joint Deep Survival model, i.e., JDS, which can integrate heterogeneous features, e.g., donor motives, projects recently donated to, social contacts, to jointly model the donation recurrence and donor retention since these two types of behavioral events are highly relevant. In addition, we model the censoring phenomenon and dependence relations of different behaviors from the survival analysis view by designing multiple innovative constraints and incorporating them into the objective functions. Finally, we conduct extensive analysis and validation experiments with large-scale data collected from Kiva.org. The experimental results clearly demonstrate the effectiveness of our proposed models for analyzing and predicting the donation recurrence and donor retention in crowdfunding.}
}


@article{DBLP:journals/tkde/KangPLB20,
	author = {Bo Kang and
                  Kai Puolam{\"{a}}ki and
                  Jefrey Lijffijt and
                  Tijl De Bie},
	title = {A Constrained Randomization Approach to Interactive Visual Data Exploration
                  with Subjective Feedback},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1666--1679},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2907082},
	doi = {10.1109/TKDE.2019.2907082},
	timestamp = {Thu, 27 Jul 2023 08:18:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KangPLB20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data visualization and iterative/interactive data mining are growing rapidly in attention, both in research as well as in industry. However, while there are a plethora of advanced data mining methods and lots of works in the field of visualization, integrated methods that combine advanced visualization and/or interaction with data mining techniques in a principled way are rare. We present a framework based on constrained randomization which lets users explore high-dimensional data via `subjectively informative' two-dimensional data visualizations. The user is presented with `interesting' projections, allowing users to express their observations using visual interactions that update a background model representing the user's belief state. This background model is then considered by a projection-finding algorithm employing data randomization to compute a new `interesting' projection. By providing users with information that contrasts with the background model, we maximize the chance that the user encounters striking new information present in the data. This process can be iterated until the user runs out of time or until the difference between the randomized and the real data is insignificant. We present two case studies, one controlled study on synthetic data and another on census data, using the proof-of-concept tool SIDE that demonstrates the presented framework.}
}


@article{DBLP:journals/tkde/GuoLZZLXY20,
	author = {Xifeng Guo and
                  Xinwang Liu and
                  En Zhu and
                  Xinzhong Zhu and
                  Miaomiao Li and
                  Xin Xu and
                  Jianping Yin},
	title = {Adaptive Self-Paced Deep Clustering with Data Augmentation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1680--1693},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911833},
	doi = {10.1109/TKDE.2019.2911833},
	timestamp = {Wed, 28 Aug 2024 08:59:04 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GuoLZZLXY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep clustering gains superior performance than conventional clustering by jointly performing feature learning and cluster assignment. Although numerous deep clustering algorithms have emerged in various applications, most of them fail to learn robust cluster-oriented features which in turn hurts the final clustering performance. To solve this problem, we propose a two-stage deep clustering algorithm by incorporating data augmentation and self-paced learning. Specifically, in the first stage, we learn robust features by training an autoencoder with examples that are augmented by random shifting and rotating the given clean examples. Then, in the second stage, we encourage the learned features to be cluster-oriented by alternatively finetuning the encoder with the augmented examples and updating the cluster assignments of the clean examples. During finetuning the encoder, the target of each augmented example in the loss function is the center of the cluster to which the clean example is assigned. The targets may be computed incorrectly, and the examples with incorrect targets could mislead the encoder network. To stabilize the network training, we select most confident examples in each iteration by utilizing the adaptive self-paced learning. Extensive experiments validate that our algorithm outperforms the state of the arts on four image datasets.}
}


@article{DBLP:journals/tkde/RyuJP20,
	author = {Hyeong{-}Cheol Ryu and
                  Sungwon Jung and
                  Sakti Pramanik},
	title = {An Effective Clustering Method over CF{\textdollar}{\^{}}+{\textdollar}+
                  Tree Using Multiple Range Queries},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1694--1706},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911520},
	doi = {10.1109/TKDE.2019.2911520},
	timestamp = {Wed, 26 Aug 2020 11:04:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/RyuJP20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many existing clustering methods usually compute clusters from the reduced data sets obtained by summarizing the original very large data sets. BIRCH is a popular summary-based clustering method that first builds a CF tree, and then performs a global clustering using the leaf entries of the tree. However, to the best of our knowledge, no prior studies have proposed a global clustering method that uses the structure of a CF tree. Therefore, we propose a novel global clustering method ERC (effective multiple range queries-based clustering), which takes advantage of the structure of a CF tree. We further propose a CF + tree, which optimizes the node split scheme used in the CF tree. As a result, the CF + -ERC (CF + tree-based ERC) method effectively computes clusters over large data sets. Furthermore, it does not require a predefined number of clusters to compute the clusters. We present in-depth theoretical and experimental analyses of our method. Experimental results on very large synthetic data sets demonstrate that the proposed approach is effective in terms of cluster quality and robustness and is significantly faster than existing clustering methods. In addition, we apply our clustering method to real data sets and achieve promising results.}
}


@article{DBLP:journals/tkde/KojicM20,
	author = {Nemanja Kojic and
                  Dragan Milicev},
	title = {Equilibrium of Redundancy in Relational Model for Optimized Data Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1707--1721},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911580},
	doi = {10.1109/TKDE.2019.2911580},
	timestamp = {Wed, 26 Aug 2020 11:04:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KojicM20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conceptual and relational data models of online transaction processing (OLTP) applications are usually created and maintained following the principle of normalization, which implies avoidance of redundancy. Data retrieval from a disk-based normalized relational database often requires complex and inefficient queries that may cause noticeable performance issues when executed on larger volumes of data. Computer professionals sometimes intentionally trade off the strict normal form to optimize data retrieval queries through error-prone manual tuning and denormalization. We propose a fully automatic optimization approach, based on data redundancy, that relies on a formal cost-benefit model. We prove that finding the optimal level of data redundancy, for given workload statistics, is an NP-Complete optimization problem. A detailed reduction of the problem to binary linear programming is presented in the paper. The proposed optimization approach was evaluated using the TPCE benchmark for OLTP systems. The evaluation has shown that the proposed optimization approach is highly scalable, and that it can be efficiently applied to real-life relational data models.}
}


@article{DBLP:journals/tkde/GhasemianHC20,
	author = {Amir Ghasemian and
                  Homa Hosseinmardi and
                  Aaron Clauset},
	title = {Evaluating Overfit and Underfit in Models of Network Community Structure},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1722--1735},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911585},
	doi = {10.1109/TKDE.2019.2911585},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/GhasemianHC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A common graph mining task is community detection, which seeks an unsupervised decomposition of a network into groups based on statistical regularities in network connectivity. Although many such algorithms exist, community detection's No Free Lunch theorem implies that no algorithm can be optimal across all inputs. However, little is known in practice about how different algorithms over or underfit to real networks, or how to reliably assess such behavior across algorithms. Here, we present a broad investigation of over and underfitting across 16 state-of-the-art community detection algorithms applied to a novel benchmark corpus of 572 structurally diverse real-world networks. We find that (i) algorithms vary widely in the number and composition of communities they find, given the same input; (ii) algorithms can be clustered into distinct high-level groups based on similarities of their outputs on real-world networks; (iii) algorithmic differences induce wide variation in accuracy on link-based learning tasks; and, (iv) no algorithm is always the best at such tasks across all inputs. Finally, we quantify each algorithm's overall tendency to over or underfit to network data using a theoretically principled diagnostic, and discuss the implications for future advances in community detection.}
}


@article{DBLP:journals/tkde/LiuCZ20,
	author = {Xin Liu and
                  Hui{-}Min Cheng and
                  Zhong{-}Yuan Zhang},
	title = {Evaluation of Community Detection Methods},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1736--1746},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911943},
	doi = {10.1109/TKDE.2019.2911943},
	timestamp = {Wed, 26 Aug 2020 11:04:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuCZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community structures are critical towards understanding not only the network topology but also how the network functions. However, how to evaluate the quality of detected community structures is still challenging and remains unsolved. The most widely used metric, normalized mutual information (NMI), was proven to have finite size effect, and its improved form relative normalized mutual information (rNMI) has reverse finite size effect. Corrected normalized mutual information (cNMI) was thus proposed and has neither finite size effect nor reverse finite size effect. However, in this paper, we show that cNMI violates the so-called proportionality assumption. In addition, NMI-type metrics have the problem of ignoring importance of small communities. Finally, they cannot be used to evaluate a single community of interest. In this paper, we map the computed community labels to the ground-truth ones through integer linear programming, and then use kappa index and F-score to evaluate the detected community structures. Experimental results demonstrate the advantages of our method.}
}


@article{DBLP:journals/tkde/TangLZXLXWW20,
	author = {Chang Tang and
                  Xinwang Liu and
                  Xinzhong Zhu and
                  Jian Xiong and
                  Miaomiao Li and
                  Jingyuan Xia and
                  Xiangke Wang and
                  Lizhe Wang},
	title = {Feature Selective Projection with Low-Rank Embedding and Dual Laplacian
                  Regularization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1747--1760},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911946},
	doi = {10.1109/TKDE.2019.2911946},
	timestamp = {Wed, 28 Aug 2024 08:59:04 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TangLZXLXWW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature extraction and feature selection have been regarded as two independent dimensionality reduction methods in most of the existing literature. In this paper, we propose to integrate both approaches into a unified framework and design an unsupervised linear feature selective projection (FSP) for feature extraction with low-rank embedding and dual Laplacian regularization, with the aim to exploit the intrinsic relationship among data and suppress the impact of noise. Specifically, a projection matrix with an\nl\n2,1\n-norm regularization is introduced to project original high dimensional data points into a new subspace with lower dimension, where the\nl\n2,1\n-norm regularization can endow the projection with good interpretability. We deploy a coefficient matrix with low rank constraint to reconstruct the data points and the\nl\n2,1\n-norm is imposed to regularize the data reconstruction errors in the low-dimensional subspace and make FSP robust to noise. Furthermore, a dual graph Laplacian regularization term is imposed on the low dimensional data and data reconstruction matrix for preserving the local manifold geometrical structure of data. Finally, an alternatively iterative algorithm is carefully designed for solving the proposed optimization model. Theoretical convergence and computational complexity analysis of the algorithm are also provided. Comprehensive experiments on various benchmark datasets have been carried out to evaluate the performance of the proposed FSP. As indicated, our algorithm significantly outperforms other state-of-the-art methods for feature extraction.}
}


@article{DBLP:journals/tkde/HuangYLLGC20,
	author = {Hao Huang and
                  Qian Yan and
                  Wei Lu and
                  Huaizhong Lin and
                  Yunjun Gao and
                  Lei Chen},
	title = {{LERI:} Local Exploration for Rare-Category Identification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1761--1772},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911941},
	doi = {10.1109/TKDE.2019.2911941},
	timestamp = {Mon, 04 Jan 2021 16:15:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HuangYLLGC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To identify the data examples of rare categories that form small compact clusters in large data sets, existing approaches mostly require enough labeled data examples as a training set to learn a classifier, assuming that the rare-category clusters are spherical or nearly spherical. Nonetheless, a large enough training set is usually difficult to obtain in practice, and rare categories in many real-world applications often form small compact clusters with arbitrary shapes. In this paper, we investigate how to identify all data examples of a rare category with an arbitrary shape based on only one seed (i.e., a labeled rare-category data example). Instead of finding a compact and spherical local region around the seed, we locally explore the data set from the seed by continuously searching and visiting the k-nearest neighbors of each newly visited data example. The local exploration connects the data examples in the objective rare category by the relationship of k-nearest neighbors, and meanwhile, suspected external data examples are filtered out if they are not close enough to any visited data example. Experimental results on both synthetic and real-world data sets are conducted, and the results verify the effectiveness and efficiency of our approach.}
}


@article{DBLP:journals/tkde/BartalR20,
	author = {Alon Bartal and
                  Gilad Ravid},
	title = {Member Behavior in Dynamic Online Communities: Role Affiliation Frequency
                  Model},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1773--1784},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911067},
	doi = {10.1109/TKDE.2019.2911067},
	timestamp = {Wed, 26 Aug 2020 11:04:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BartalR20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {People's social life has become more embedded in dynamic online communities. Each online community can be viewed as a temporal online social network (OSN). The interaction level among OSN members leads to the emergence of dynamic social roles, which change and evolve over time, creating a sequence of temporal roles. These role sequences show diversity in the role-affiliation frequency of members. That diversity enables modeling the dynamic behaviors of individuals. This paper proposes a temporal role-affiliation frequency model (RAFM) which detects the time evolving roles of each member and analyzes her/his role-affiliation frequency to infer her/his latent behavior. Applying the RAFM to real interaction data, collected in four online communities, revealed the identity of influential members. In addition, members with similar temporal behavioral patterns were found to have similar latent behavior patterns. These patterns are manifested via similar role transitions in different OSNs whose temporal interaction rhythms were compatible. These two research findings contribute to OSN research and knowledge via improved understanding of member behavior online based on role-affiliation frequency and role transitions. Thus, member latent behavior can be inferred, and influential members can be identified.}
}


@article{DBLP:journals/tkde/FernandoDSF20,
	author = {Tharindu Fernando and
                  Simon Denman and
                  Sridha Sridharan and
                  Clinton Fookes},
	title = {Memory Augmented Deep Generative Models for Forecasting the Next Shot
                  Location in Tennis},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1785--1797},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911507},
	doi = {10.1109/TKDE.2019.2911507},
	timestamp = {Wed, 26 Aug 2020 11:04:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FernandoDSF20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a novel framework for predicting shot location and type in tennis. Inspired by recent neuroscience discoveries, we incorporate neural memory modules to model the episodic and semantic memory components of a tennis player. We propose a Semi-Supervised Generative Adversarial Network architecture that couples these memory models with the automatic feature learning power of deep neural networks, and demonstrate methodologies for learning player level behavioral patterns with the proposed framework. We evaluate the effectiveness of the proposed model on tennis tracking data from the 2012 Australian Tennis Open and exhibit applications of the proposed method in discovering how players adapt their style depending on the match context.}
}


@article{DBLP:journals/tkde/AgrawalSBCADLK20,
	author = {Saurabh Agrawal and
                  Michael S. Steinbach and
                  Daniel Boley and
                  Snigdhansu Chatterjee and
                  Gowtham Atluri and
                  Anh The Dang and
                  Stefan Liess and
                  Vipin Kumar},
	title = {Mining Novel Multivariate Relationships in Time Series Data Using
                  Correlation Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1798--1811},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911681},
	doi = {10.1109/TKDE.2019.2911681},
	timestamp = {Mon, 05 Feb 2024 20:21:14 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/AgrawalSBCADLK20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many domains, there is significant interest in capturing novel relationships between time series that represent activities recorded at different nodes of a highly complex system. In this paper, we introduce multipoles, a novel class of linear relationships between more than two time series. A multipole is a set of time series that have strong linear dependence among themselves, with the requirement that each time series makes a significant contribution to the linear dependence. We demonstrate that most interesting multipoles can be identified as cliques of negative correlations in a correlation network. Such cliques are typically rare in a real-world correlation network, which allows us to find almost all multipoles efficiently using a clique-enumeration approach. Using our proposed framework, we demonstrate the utility of multipoles in discovering new physical phenomena in two scientific domains: climate science and neuroscience. In particular, we discovered several multipole relationships that are reproducible in multiple other independent datasets and lead to novel domain insights.}
}


@article{DBLP:journals/tkde/PibiriV20,
	author = {Giulio Ermanno Pibiri and
                  Rossano Venturini},
	title = {On Optimally Partitioning Variable-Byte Codes},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1812--1823},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911288},
	doi = {10.1109/TKDE.2019.2911288},
	timestamp = {Wed, 26 Aug 2020 11:04:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PibiriV20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ubiquitous Variable-Byte encoding is one of the fastest compressed representation for integer sequences. However, its compression ratio is usually not competitive with other more sophisticated encoders, especially when the integers to be compressed are small which is the typical case for inverted indexes. This paper shows that the compression ratio of Variable-Byte can be improved by 2× by adopting a partitioned representation of the inverted lists. This makes Variable-Byte surprisingly competitive in space with the best bit-aligned encoders, hence disproving the folklore belief that Variable-Byte is space-inefficient for inverted index compression. Despite the significant space savings, we show that our optimization almost comes for free, given that: we introduce an optimal partitioning algorithm that does not affect indexing time because of its linear-time complexity; we show that the query processing speed of Variable-Byte is preserved, with an extensive experimental analysis and comparison with several other state-of-the-art encoders.}
}


@article{DBLP:journals/tkde/LiuLCL20,
	author = {Li Liu and
                  Xin Li and
                  William K. Cheung and
                  Lejian Liao},
	title = {Structural Representation Learning for User Alignment Across Social
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1824--1837},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911516},
	doi = {10.1109/TKDE.2019.2911516},
	timestamp = {Thu, 10 Aug 2023 08:30:12 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiuLCL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aligning users across different social networks has become increasingly studied as an important task to social network analysis. In this paper, we propose a novel representation learning method that mainly exploits social structures for the network alignment. In particular, the proposed network embedding framework models the follower-ship and followee-ship of each user explicitly as input and output context vectors, while preserving the proximity of users with “similar” followers and followees in the embedded space. We incorporate both known and predicted user anchors across the networks as constraints to facilitate the transfer of context information to achieve accurate user alignment. Both network embedding and user alignment are inferred under a unified optimization framework with negative sampling adopted to ensure scalability. Also, variants of the proposed framework, including the incorporation of higher-order structural features, are also explored for further boosting the alignment accuracy. Extensive experiments on large-scale social and academia network datasets demonstrate the efficacy of our proposed model compared with state-of-the-art methods.}
}


@article{DBLP:journals/tkde/KhanLHS20,
	author = {Imran Khan and
                  Zongwei Luo and
                  Joshua Zhexue Huang and
                  Waseem Shahzad},
	title = {Variable Weighting in Fuzzy k-Means Clustering to Determine the Number
                  of Clusters},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {9},
	pages = {1838--1853},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2911582},
	doi = {10.1109/TKDE.2019.2911582},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/KhanLHS20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the most significant problems in cluster analysis is to determine the number of clusters in unlabeled data, which is the input for most clustering algorithms. Some methods have been developed to address this problem. However, little attention has been paid on algorithms that are insensitive to the initialization of cluster centers and utilize variable weights to recover the number of clusters. To fill this gap, we extend the standard fuzzy k-means clustering algorithm. It can automatically determine the number of clusters by iteratively calculating the weights of all variables and the membership value of each object in all clusters. Two new steps are added to the fuzzy k-means clustering process. One of them is to introduce a penalty term to make the clustering process insensitive to the initial cluster centers. The other one is to utilize a formula for iterative updating of variable weights in each cluster based on the current partition of data. Experimental results on real-world and synthetic datasets have shown that the proposed algorithm effectively determined the correct number of clusters while initializing the different number of cluster centroids. We also tested the proposed algorithm on gene data to determine a subset of important genes.}
}


@article{DBLP:journals/tkde/WuCHFXW20,
	author = {Le Wu and
                  Lei Chen and
                  Richang Hong and
                  Yanjie Fu and
                  Xing Xie and
                  Meng Wang},
	title = {A Hierarchical Attention Model for Social Contextual Image Recommendation},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1854--1867},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913394},
	doi = {10.1109/TKDE.2019.2913394},
	timestamp = {Thu, 14 Oct 2021 08:57:05 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WuCHFXW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Image based social networks are among the most popular social networking services in recent years. With a tremendous amount of images uploaded everyday, understanding users' preferences on user-generated images and making recommendations have become an urgent need. In fact, many hybrid models have been proposed to fuse various kinds of side information (e.g., image visual representation, social network) and user-item historical behavior for enhancing recommendation performance. However, due to the unique characteristics of the user generated images in social image platforms, the previous studies failed to capture the complex aspects that influence users' preferences in a unified framework. Moreover, most of these hybrid models relied on predefined weights in combining different kinds of information, which usually resulted in sub-optimal recommendation performance. To this end, in this paper, we develop a hierarchical attention model for social contextual image recommendation. In addition to basic latent user interest modeling in the popular matrix factorization based recommendation, we identify three key aspects (i.e., upload history, social influence, and owner admiration) that affect each user's latent preferences, where each aspect summarizes a contextual factor from the complex relationships between users and images. After that, we design a hierarchical attention network that naturally mirrors the hierarchical relationship (elements in each aspects level, and the aspect level) of users' latent interests with the identified key aspects. Specifically, by taking embeddings from state-of-the-art deep learning models that are tailored for each kind of data, the hierarchical attention network could learn to attend differently to more or less content. Finally, extensive experimental results on real-world datasets clearly show the superiority of our proposed model.}
}


@article{DBLP:journals/tkde/WangQ20,
	author = {Huan Wang and
                  Chunming Qiao},
	title = {A Nodes' Evolution Diversity Inspired Method to Detect Anomalies in
                  Dynamic Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1868--1880},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2912574},
	doi = {10.1109/TKDE.2019.2912574},
	timestamp = {Tue, 06 Oct 2020 17:42:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangQ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently dynamic social networks witnessed a massive surge in popularity, especially in the area of anomaly detection. Although the text-based methods have achieved impressive detection performances, their applications are limited to the social text provided by users. This research focuses on graph-based methods and proposes a universal method for generalized social networks. Different from the existing graph-based methods that summarize a number of structural features, the proposed nodes' evolution diversity inspired method (NEDM) detects anomalies in dynamic social networks from the perspective of diverse evolution mechanisms. More specifically, NEDM applies link prediction algorithms at the micro-level to fit evolution mechanisms followed by the behaviors of nodes, and designs indices to evaluate their fitting degrees in edge removal and generation processes. In addition, the behavior of a node is represented as a quantum superposition state where such behavior follows different evolution mechanisms with uncertain probabilities. We propose a quantum mechanism based particle swarm optimization algorithm (QMPSO) in NEDM. QMPSO determines the optimal observation states of the behaviors of different nodes, and maximally reflects the evolutional fluctuations in the evolution processes of social networks. As a result, NEDM can quantify the evolutional fluctuations in different periods, and detect anomalies in dynamic social networks. Comparing with art-of-the-state methods and real social data in extensive experiments on disparate real-world social networks, we verify the outstanding performance of NEDM in terms of both accuracy and universality.}
}


@article{DBLP:journals/tkde/TuarobKWPSHH20,
	author = {Suppawong Tuarob and
                  Sung Woo Kang and
                  Poom Wettayakorn and
                  Chanatip Pornprasit and
                  Tanakitti Sachati and
                  Saeed{-}Ul Hassan and
                  Peter Haddawy},
	title = {Automatic Classification of Algorithm Citation Functions in Scientific
                  Literature},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1881--1896},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913376},
	doi = {10.1109/TKDE.2019.2913376},
	timestamp = {Sun, 06 Oct 2024 21:41:29 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/TuarobKWPSHH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computer sciences and related disciplines evolve around developing, evaluating, and applying algorithms. Typically, an algorithm is not developed from scratch, but uses and builds upon existing ones, which often are proposed and published in scholarly articles. The ability to capture this evolution relationship among these algorithms in scientific literature would not only allow us to understand how a particular algorithm is composed, but also shed light on large-scale analysis of algorithmic evolution through different temporal spans and thematic scales. We propose to capture such evolution relationship between two algorithms by investigating the knowledge represented in citation contexts, where authors explain how cited algorithms are used in their works. A set of heterogeneous ensemble machine-learning methods is proposed, where the combination of two base classifiers trained with heterogeneous feature types is used to automatically identify the algorithm usage relationship. The proposed heterogeneous ensemble methods achieve the best average F1 of 0.749 and 0.905 for fine-grained and binary algorithm citation function classification, respectively. The success of this study will allow us to generate a large-scale algorithm citation network from a collection of scholarly documents representing multiple time spans, venues, and fields of study. Such a network will be used as an instrument not only to answer critical questions in algorithm search, such as identifying the most influential and generalizable algorithms, but also to study the evolution of algorithmic development and trends over time.}
}


@article{DBLP:journals/tkde/LuoXLK20,
	author = {Siqiang Luo and
                  Xiaokui Xiao and
                  Wenqing Lin and
                  Ben Kao},
	title = {{BATON:} Batch One-Hop Personalized PageRanks with Efficiency and
                  Accuracy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1897--1908},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2912606},
	doi = {10.1109/TKDE.2019.2912606},
	timestamp = {Tue, 06 Oct 2020 17:42:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LuoXLK20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized PageRank (PPR) is a classic measure of the relevance among different nodes in a graph, and has been applied in numerous systems, such as Twitter's Who-To-Follow and Pinterest's Related Pins. Existing work on PPR has mainly focused on three general types of queries, namely, single-pair PPR, single-source PPR, and all-pair PPR. However, we observe that there are applications that rely on a new query type (referred to as batch one-hop PPR), which takes as input a set S of source nodes and, for each nodes E ϵ S and each of s's neighbor v, asks for the PPR value of v with respect to s. None of the existing PPR algorithms is able to efficiently process batch one-hop queries, due to the inherent differences between batch one-hop PPR and the three general query types. To address the limitations of existing algorithms, this paper presents Baton, an algorithm for batch one-hop PPR that offers both strong theoretical guarantees and practical efficiency. Baton leverages the characteristics of one-hop PPR to avoid unnecessary computation, and it incorporates advanced mechanisms to improve the cost-effectiveness of PPR derivations. Extensive experiments on benchmark datasets show that Baton is up to three orders of magnitude faster than the state of the art, while offering the same accuracy.}
}


@article{DBLP:journals/tkde/HaoMHWLG20,
	author = {Yanbin Hao and
                  Tingting Mu and
                  Richang Hong and
                  Meng Wang and
                  Xueliang Liu and
                  John Yannis Goulermas},
	title = {Cross-Domain Sentiment Encoding through Stochastic Word Embedding},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1909--1922},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913379},
	doi = {10.1109/TKDE.2019.2913379},
	timestamp = {Tue, 06 Oct 2020 17:42:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HaoMHWLG20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sentiment analysis is an important topic concerning identification of feelings, attitudes, emotions and opinions from text. To automate such analysis, a large amount of example text needs to be manually annotated for model training. This is laborious and expensive, but the cross-domain technique is a key solution to reducing the cost by reusing annotated reviews across domains. However, its success largely relies on the learning of a robust common representation space across domains. In the recent years, significant effort has been invested to improve the cross-domain representation learning by designing increasingly more complex and elaborate model inputs and architectures. We support that it is not necessary to increase design complexity as this inevitably consumes more time in model training. Instead, we propose to explore the word polarity and occurrence information through a simple mapping and encode such information more accurately whilst managing lower computational costs. The proposed approach is unique and takes advantage of the stochastic embedding technique to tackle cross-domain sentiment alignment. Its effectiveness is benchmarked with over ten data tasks constructed from two review corpora and it is compared against ten classical and state-of-the-art methods.}
}


@article{DBLP:journals/tkde/ZhaoCY20,
	author = {Liang Zhao and
                  Feng Chen and
                  Yanfang Ye},
	title = {Efficient Learning with Exponentially-Many Conjunctive Precursors
                  for Interpretable Spatial Event Forecasting},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1923--1935},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2912187},
	doi = {10.1109/TKDE.2019.2912187},
	timestamp = {Tue, 18 Oct 2022 14:44:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoCY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Forecasting spatial societal events in social media is significant and challenging. Most existing methods consider the frequencies of keywords or n-grams to be features, but have not explored the exponentially large space of the conjunctions of those features, such as keyword co-occurrence in messages, which can serve as crucial precursor rules. Due to the inherent exponential complexity of ensemble rule learning, existing work typically adopts greedy/heuristic strategies. This means that they cannot guarantee the solution's optimality, which would require a considerably more sophisticated model for spatial event forecasting, while still suffering from major challenges: 1) Exponentially-dimensional feature learning with distant supervision, 2) Numerical values of conjunctive features, and 3) Spatially heterogeneous conjunction patterns. To concurrently address all these challenges with a theoretical guarantee, we propose a novel spatial event forecasting model which learns numerical conjunctive features efficiently. Specifically, to consider their magnitude, traditional Boolean rules are innovatively generalized to deal with numerical conjunctive features with amenable computational properties. To handle the geographical similarity and heterogeneity in numerical conjunctive feature learning, we propose a new model that implements through a new bi-space hierarchical sparsity regularization for locations and features. Moreover, we propose a new algorithm to optimize the model parameters and prove that it enjoys theoretical guarantees for both the error bounds and time efficiency. Extensive experiments on multiple datasets demonstrate the effectiveness and efficiency of the proposed method.}
}


@article{DBLP:journals/tkde/LinLWZWL20,
	author = {Hao Lin and
                  Guannan Liu and
                  Junjie Wu and
                  Yuan Zuo and
                  Xin Wan and
                  Hong Li},
	title = {Fraud Detection in Dynamic Interaction Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1936--1950},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2912817},
	doi = {10.1109/TKDE.2019.2912817},
	timestamp = {Wed, 10 Mar 2021 08:11:03 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/LinLWZWL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fraud detection from massive user behaviors is often regarded as trying to find a needle in a haystack. In this paper, we suggest abnormal behavioral patterns can be better revealed if both sequential and interaction behaviors of users can be modeled simultaneously, which however has rarely been addressed in prior work. Along this line, we propose a COllective Sequence and INteraction (COSIN) model, in which the behavioral sequences and interactions between source and target users in a dynamic interaction network are modeled uniformly in a probabilistic graphical model. More specifically, the sequential schema is modeled with a hierarchical Hidden Markov Model, and meanwhile it is shifted to the interaction schema to generate the interaction counts through Poisson factorization. A hybrid Gibbs-Variational algorithm is then proposed for efficient parameter estimation of the COSIN model. We conduct extensive experiments on both synthetic and real-world telecom datasets in different scales, and the results show that the proposed model outperforms some competitive baseline methods and is scalable. A case is further presented to show the precious explainability of the model.}
}


@article{DBLP:journals/tkde/NieJCZZY20,
	author = {Xiushan Nie and
                  Weizhen Jing and
                  Chaoran Cui and
                  Chen Jason Zhang and
                  Lei Zhu and
                  Yilong Yin},
	title = {Joint Multi-View Hashing for Large-Scale Near-Duplicate Video Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1951--1965},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913383},
	doi = {10.1109/TKDE.2019.2913383},
	timestamp = {Tue, 06 Oct 2020 17:42:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/NieJCZZY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view hashing can well support large-scale near-duplicate video retrieval, due to its desirable advantages of mutual reinforcement of multiple features, low storage cost, and fast retrieval speed. However, there are still two limitations that impede its performance. First, existing methods only consider local structures in multiple features. They ignore the global structure that is important for near-duplicate video retrieval, and cannot fully exploit the dependence and complementarity of multiple features. Second, existing works always learn hashing functions bit by bit, which unfortunately increases the time complexity of hash function learning. In this paper, we propose a supervised hashing scheme, termed as joint multi-view hashing (JMVH), to address the aforementioned problems. It jointly preserves the global and local structures of multiple features while learning hashing functions efficiently. Specially, JMVH considers features of video as items, based on which an underlying Hamming space is learned by simultaneously preserving their local and global structures. In addition, a simple but efficient multi-bit hash function learning based on generalized eigenvalue decomposition is devised to learn multiple hash functions within a single step. It can significantly reduce the time complexity of conventional hash function learning processes that sequentially learn multiple hash functions bit by bit. The proposed JMVH is evaluated on two public databases: CC_WEB_VIDEO and UQ_VIDEO. Experimental results demonstrate that the proposed JMVH achieves more than a 5 percent improvement compared to several state-of-the-art methods which indicates the superior performance of JMVH.}
}


@article{DBLP:journals/tkde/XieZHPJY20,
	author = {Xike Xie and
                  Kai Zou and
                  Xingjun Hao and
                  Torben Bach Pedersen and
                  Peiquan Jin and
                  Wei Yang},
	title = {{OLAP} over Probabilistic Data Cubes {II:} Parallel Materialization
                  and Extended Aggregates},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1966--1981},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913420},
	doi = {10.1109/TKDE.2019.2913420},
	timestamp = {Tue, 06 Oct 2020 17:42:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XieZHPJY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {On-Line Analytical Processing (OLAP) enables powerful analytics by quickly computing aggregate values of numerical measures over multiple hierarchical dimensions for massive datasets. However, many types of source data, e.g., from GPS, sensors, and other measurement devices, are intrinsically inaccurate (imprecise and/or uncertain) and thus OLAP cannot be readily applied. In this paper, we address the resulting data veracityproblem in OLAP by proposing the concept of probabilistic data cubes. Such a cube is comprised of a set of probabilistic cuboids which summarize the aggregated values in the form of probability mass functions (pmfs in short) and thus offer insights into the underlying data quality and enable confidence-aware query evaluation and analysis. However, the probabilistic nature of data poses computational challenges, since a probabilistic database can have exponential number of possible worlds under the possible world semantics. Even worse, it is hard to share computations among different cuboids, as aggregation functions that are distributive for traditional data cubes, e.g., SUM, become holistic in probabilistic settings. In this paper, we propose a complete set of techniques for probabilistic data cubes, from cuboid aggregation, over cube materialization, to query evaluation. We study two types of aggregation: convolution and sketch-based, which take polynomial time complexities for aggregation and jointly enable efficient query processing. Also, our proposal is versatile in terms of: 1) its capability of supporting common aggregation functions, i.e., SUM, COUNT, MAX, and AVG; 2) its adaptivity to different materialization strategies, e.g., full versus partial materialization, with support of our devised cost models and parallelization framework; 3) its coverage of common OLAP operations, i.e., probabilistic slicing and dicing queries. Extensive experiments over real and synthetic datasets show that our techniques are effective and scalable.}
}


@article{DBLP:journals/tkde/ArzamasovaBGSS20,
	author = {Natalia Arzamasova and
                  Klemens B{\"{o}}hm and
                  Bertrand Goldman and
                  Christian Saaler and
                  Martin Sch{\"{a}}ler},
	title = {On the Usefulness of SQL-Query-Similarity Measures to Find User Interests},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {1982--1999},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913381},
	doi = {10.1109/TKDE.2019.2913381},
	timestamp = {Tue, 06 Oct 2020 17:42:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ArzamasovaBGSS20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the sciences and elsewhere, the use of relational databases has become ubiquitous. An important challenge is finding hot spots of user interests. In principle, one can discover user interests by clustering the queries in the query log. Such a clustering requires a notion of query similarity. This, in turn, raises the question of what features of SQL queries are meaningful. We have studied the query representations proposed in the literature and corresponding similarity functions and have identified shortcomings of all of them. To overcome these limitations, we propose new similarity functions for SQL queries. They rely on the so-called access area of a query and, more specifically, on the overlap and the closeness of the access areas. We have carried out experiments systematically to compare the various similarity functions described in this article. The first series of experiments measures the quality of clustering and compares it to a ground truth. In the second series, we focus on the query log from the well-known SkyServer database. Here, a domain expert has interpreted various clusters by hand. We conclude that clusters obtained with our new measures of similarity seem to be good indicators of user interests.}
}


@article{DBLP:journals/tkde/CullyD20,
	author = {Antoine Cully and
                  Yiannis Demiris},
	title = {Online Knowledge Level Tracking with Data-Driven Student Models and
                  Collaborative Filtering},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {2000--2013},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2912367},
	doi = {10.1109/TKDE.2019.2912367},
	timestamp = {Tue, 06 Oct 2020 17:42:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/CullyD20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent Tutoring Systems are promising tools for delivering optimal and personalized learning experiences to students. A key component for their personalization is the student model, which infers the knowledge level of the students to balance the difficulty of the exercises. While important advances have been achieved, several challenges remain. In particular, the models should be able to track in real-time the evolution of the students' knowledge levels. These evolutions are likely to follow different profiles for each student, while measuring the exact knowledge level remains difficult given the limited and noisy information provided by the interactions. This paper introduces a novel model that addresses these challenges with three contributions:. 1) the model relies on Gaussian Processes to track online the evolution of the student's knowledge level over time, 2) it uses collaborative filtering to rapidly provide long-term predictions by leveraging the information from previous users, and 3) it automatically generates abstract representations of knowledge components via automatic relevance determination of covariance matrices. The model is evaluated on three datasets, including real users. The results demonstrate that the model converges to accurate predictions in average four times faster than the compared methods.}
}


@article{DBLP:journals/tkde/WangNWHL20,
	author = {Rong Wang and
                  Feiping Nie and
                  Zhen Wang and
                  Haojie Hu and
                  Xuelong Li},
	title = {Parameter-Free Weighted Multi-View Projected Clustering with Structured
                  Graph Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {2014--2025},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913377},
	doi = {10.1109/TKDE.2019.2913377},
	timestamp = {Mon, 28 Aug 2023 21:37:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangNWHL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many real-world applications, we are often confronted with high dimensional data which are represented by various heterogeneous views. How to cluster this kind of data is still a challenging problem due to the curse of dimensionality and effectively integration of different views. To address this problem, we propose two parameter-free weighted multi-view projected clustering methods which perform structured graph learning and dimensionality reduction simultaneously. We can use the obtained structured graph directly to extract the clustering indicators, without performing other discretization procedures as previous graph-based clustering methods have to do. Moreover, two parameter-free strategies are adopted to learn an optimal weight for each view automatically, without introducing a regularization parameter as previous methods do. Extensive experiments on several public datasets demonstrate that the proposed methods outperform other state-of-the-art approaches and can be used more practically.}
}


@article{DBLP:journals/tkde/WangNY20,
	author = {Lei Wang and
                  Jianwei Niu and
                  Shui Yu},
	title = {SentiDiff: Combining Textual Information and Sentiment Diffusion Patterns
                  for Twitter Sentiment Analysis},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {2026--2039},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913641},
	doi = {10.1109/TKDE.2019.2913641},
	timestamp = {Fri, 30 Dec 2022 14:18:08 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangNY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Twitter sentiment analysis has become a hot research topic in recent years. Most of existing solutions to Twitter sentiment analysis basically only consider textual information of Twitter messages, and struggle to perform well when facing short and ambiguous Twitter messages. Recent studies show that sentiment diffusion patterns on Twitter have close relationships with sentiment polarities of Twitter messages. Therefore, in this paper, we focus on how to fuse textual information of Twitter messages and sentiment diffusion patterns to obtain better performance of sentiment analysis on Twitter data. To this end, we first analyze sentiment diffusion by investigating a phenomenon called sentiment reversal, and find some interesting properties of sentiment reversals. Then, we consider the inter-relationships between textual information of Twitter messages and sentiment diffusion patterns, and propose an iterative algorithm called SentiDiff to predict sentiment polarities expressed in Twitter messages. To the best of our knowledge, this work is the first to utilize sentiment diffusion patterns to help improve Twitter sentiment analysis. Extensive experiments on real-world dataset demonstrate that compared with state-of-the-art textual information based sentiment analysis algorithms, our proposed algorithm yields PR-AUC improvements between 5.09 and 8.38 percent on Twitter sentiment classification tasks.}
}


@article{DBLP:journals/tkde/Omidvar-Tehrani20,
	author = {Behrooz Omidvar{-}Tehrani and
                  Sihem Amer{-}Yahia},
	title = {User Group Analytics Survey and Research Opportunities},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {10},
	pages = {2040--2059},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913651},
	doi = {10.1109/TKDE.2019.2913651},
	timestamp = {Tue, 06 Oct 2020 17:42:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/Omidvar-Tehrani20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User data can be acquired from various domains and is characterized by a combination of demographics such as age and occupation, and user actions such as rating a movie or recording one's blood pressure. User data is appealing to analysts in their role as data scientists who seek to conduct large-scale population studies, and gain insights on various population segments. It is also appealing to users in their role as information consumers who use the social Web for routine tasks such as finding a book club or choosing a physical activity. User data analytics usually relies on identifying group-level behaviors such as “Asian women who publish regularly in databases”. Group analytics addresses peculiarities of user data such as noise and sparsity to enable insights. In this survey, we discuss different approaches for each component of user group analytics, i.e., discovery, exploration, and visualization. We focus on related work which arises from combining those components. We also discuss challenges and future directions of having an all-in-one system, where all those components are combined. This survey has been presented in the form of two tutorials [1] , [2].}
}


@article{DBLP:journals/tkde/ShanM0C0Z20,
	author = {Caihua Shan and
                  Nikos Mamoulis and
                  Guoliang Li and
                  Reynold Cheng and
                  Zhipeng Huang and
                  Yudian Zheng},
	title = {A Crowdsourcing Framework for Collecting Tabular Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2060--2074},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2914903},
	doi = {10.1109/TKDE.2019.2914903},
	timestamp = {Mon, 28 Aug 2023 21:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ShanM0C0Z20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In crowdsourcing, human workers are employed to tackle problems that are traditionally difficult for computers (e.g., data cleaning, missing value filling, and sentiment analysis). In this paper, we study the effective use of crowdsourcing in filling missing values in a given relation (e.g., a table containing different attributes of celebrity stars, such as nationality and age). A task given to a worker typically consists of questions about the missing attribute values (e.g., What is the age of Jet Li?). Although this problem has been studied before, existing work often treats related attributes independently, leading to suboptimal performance. In this paper, we present T-Crowd, which is a crowdsourcing system that considers attribute relationships. Particularly, T-Crowd integrates each worker's answers on different attributes to effectively learn his/her trustworthiness and the true data values. The attribute relationship information is used to guide task allocation to workers. Our solution seamlessly supports categorical and continuous attributes. Our extensive experiments on real and synthetic datasets show that T-Crowd outperforms state-of-the-art methods, improving the quality of truth inference and reducing the monetary cost of crowdsourcing.}
}


@article{DBLP:journals/tkde/Valverde-Albacete20,
	author = {Francisco J. Valverde{-}Albacete and
                  Carmen Pel{\'{a}}ez{-}Moreno},
	title = {A Framework for Supervised Classification Performance Analysis with
                  Information-Theoretic Methods},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2075--2087},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2915643},
	doi = {10.1109/TKDE.2019.2915643},
	timestamp = {Tue, 20 Oct 2020 18:26:34 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/Valverde-Albacete20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce a framework for the evaluation of multiclass classifiers by exploring their confusion matrices. Instead of using error-counting measures of performance, we concentrate in quantifying the information transfer from true to estimated labels using information-theoretic measures. First, the Entropy Triangle allows us to visualize the balance of mutual information, variation of information, and the deviation from uniformity in the true and estimated label distributions. Next, the Entropy-Modified Accuracy allows us to rank classifiers by performance while the Normalized Information Transfer rate allows us to evaluate classifiers by the amount of information accrued during learning. Finally, if the question rises to elucidate which errors are systematically committed by the classifier, we use a generalization of Formal Concept Analysis to elicit such knowledge. All such techniques can be applied either to artificially or biologically embodied classifiers—e.g., human performance on perceptual tasks. We instantiate the framework in a number of examples to provide guidelines for the use of these tools in the case of assessing single classifiers or populations of them—whether induced with the same technique or not—either on single tasks or in a set of them. These include well-known UCI tasks and the more complex KDD cup 99 competition on Intrusion Detection.}
}


@article{DBLP:journals/tkde/MeoLMP20,
	author = {Pasquale De Meo and
                  Mark Levene and
                  Fabrizio Messina and
                  Alessandro Provetti},
	title = {A General Centrality Framework-Based on Node Navigability},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2088--2100},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2947035},
	doi = {10.1109/TKDE.2019.2947035},
	timestamp = {Tue, 20 Oct 2020 18:26:34 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/MeoLMP20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Centrality metrics are a popular tool in Network Science to identify important nodes within a graph. We introduce the Potential Gain as a centrality measure that unifies many walk-based centrality metrics in graphs and captures the notion of node navigability, interpreted as the property of being reachable from anywhere else (in the graph) through short walks. Two instances of the Potential Gain (called the Geometric and the Exponential Potential Gain) are presented and we describe scalable algorithms for computing them on large graphs. We also give a proof of the relationship between the new measures and established centralities. The geometric potential gain of a node can thus be characterized as the product of its Degree centrality by its Katz centrality scores. At the same time, the exponential potential gain of a node is proved to be the product of Degree centrality by its Communicability index. These formal results connect potential gain to both the “popularity” and “similarity” properties that are captured by the above centralities.}
}


@article{DBLP:journals/tkde/DingYYNZ20,
	author = {Shuai Ding and
                  Zijie Yue and
                  Shanlin Yang and
                  Feng Niu and
                  Youtao Zhang},
	title = {A Novel Trust Model Based Overlapping Community Detection Algorithm
                  for Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2101--2114},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2914201},
	doi = {10.1109/TKDE.2019.2914201},
	timestamp = {Wed, 21 Oct 2020 19:29:38 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/DingYYNZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the fast advances in Internet technologies, social networks have become a major platform for social interaction, lifestyle demonstration, and message dissemination. Effective community detection in social networks helps to assess public sentiment, identify community leaders, and produce personalized recommendation. While different community detection approaches have been proposed in the literature, the trust model based detection schemes model user interactions as trust transfer, which helps to capture the implicit relation in the network. Unfortunately, trust model based detection schemes face a cold start problem, i.e., they cannot accurately model newly joined users as these users have few interactions for a duration after joining the network. In this paper, we propose TLCDA, a novel trust model based community detection algorithm. By enhancing the traditional trust computation with inter-node relation strength and similarity in social networks, TLCDA detects communities through coarse-grained K-Mediods clustering. Our evaluation on real social networks shows that the communities detected by TLCDA exhibit superior preference cohesion while satisfying the topology cohesion.}
}


@article{DBLP:journals/tkde/HuCYX20,
	author = {Lun Hu and
                  Keith C. C. Chan and
                  Xiaohui Yuan and
                  Shengwu Xiong},
	title = {A Variational Bayesian Framework for Cluster Analysis in a Complex
                  Network},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2115--2128},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2914200},
	doi = {10.1109/TKDE.2019.2914200},
	timestamp = {Tue, 20 Oct 2020 18:26:34 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/HuCYX20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A complex network is a network with non-trivial topological structures. It contains not just topological information but also attribute information available in the rich content of nodes. Concerning the task of cluster analysis in a complex network, model-based algorithms are preferred over distance-based ones, as they avoid designing specific distance measures. However, their models are only applicable to complex networks where the attribute information is composed of attributes in binary form. To overcome this disadvantage, we introduce a three-layer node-attribute-value hierarchical structure to describe the attribute information in a flexible and interpretable manner. Then, a new Bayesian model is proposed to simulate the generative process of a complex network. In this model, the attribute information is generated by following the hierarchical structure while the links between pairwise nodes are generated by a stochastic blockmodel. To solve the corresponding inference problem, we develop a variational Bayesian algorithm called TARA, which allows us to identify functionally meaningful clusters through an iterative procedure. Our extensive experiment results show that TARA can be an effective algorithm for cluster analysis in a complex network. Moreover, the parallelized version of TARA makes it possible to perform efficiently at its tasks when applied to large complex networks.}
}


@article{DBLP:journals/tkde/PramanikHKPM20,
	author = {Soumajit Pramanik and
                  Rajarshi Haldar and
                  Anand Kumar and
                  Sayan Pathak and
                  Bivas Mitra},
	title = {Deep Learning Driven Venue Recommender for Event-Based Social Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2129--2143},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2915523},
	doi = {10.1109/TKDE.2019.2915523},
	timestamp = {Tue, 20 Oct 2020 18:26:34 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/PramanikHKPM20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Event-based online social platforms, such as Meetup and Plancast, have experienced increased popularity and rapid growth in recent years. In EBSN setup, selecting suitable venues for hosting events, which can attract a great turnout, is a key challenge. In this paper, we present a deep learning based venue recommendation system\nDeepVenue\nwhich provides context driven venue recommendations for the Meetup event-hosts to host their events. The crux of the proposed model relies on the notion of similarity between multiple Meetup entities such as events, venues, groups, etc. We develop deep learning techniques to compute a compact descriptor for each entity, such that two entities (say, venues) can be compared numerically. Notably, to mitigate the scarcity of venue related information in Meetup, we leverage on the cross domain knowledge transfer from popular LBSN service Yelp to extract rich venue related content. For hosting an event, the proposed\nDeepVenue\nmodel computes a success score for each candidate venue and ranks those venues according to the scores and finally recommend the top k venues. Our rigorous evaluation on the Meetup data collected for the city of Chicago shows that\nDeepVenue\nsignificantly outperforms the baselines algorithms. Precisely, for 84 percent of events, the correct hosting venue appears in the top 5 of the\nDeepVenue\nrecommended list.}
}


@article{DBLP:journals/tkde/JinWZJHF020,
	author = {Di Jin and
                  Kunzeng Wang and
                  Ge Zhang and
                  Pengfei Jiao and
                  Dongxiao He and
                  Fran{\c{c}}oise Fogelman{-}Souli{\'{e}} and
                  Xin Huang},
	title = {Detecting Communities with Multiplex Semantics by Distinguishing Background,
                  General, and Specialized Topics},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2144--2158},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2937298},
	doi = {10.1109/TKDE.2019.2937298},
	timestamp = {Fri, 05 Aug 2022 07:36:32 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JinWZJHF020.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding semantic communities using network topology and contents together is a hot topic in community detection. Existing methods often use word attributes in an indiscriminate way to help finding communities. Through analysis we find that, words in networked contents often embody a hierarchical semantic structure. Some words reflect a background topic of the whole network with all communities, some imply the high-level general topic covering several topic-related communities, and some imply the high-resolution specialized topic to describe each community. Ignoring such semantic structures often leads to defects in depicting networked contents where deep semantics are not fully utilized. To solve this problem, we propose a new Bayesian probabilistic model. By distinguishing words from either a background topic or some two-level topics (i.e., general and specialized topics), this model not only better utilizes the networked contents to help finding communities, but also provides a clearer multiplex semantic community interpretation. We then give an efficient variational algorithm for model inference. The superiority of this new approach is demonstrated by comparing with ten state-of-the-art methods on nine real networks and an artificial benchmark. A case study is further provided to show its strong ability in deep semantic interpretation of communities.}
}


@article{DBLP:journals/tkde/LiH020,
	author = {Lusi Li and
                  Haibo He and
                  Jie Li},
	title = {Entropy-based Sampling Approaches for Multi-Class Imbalanced Problems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2159--2170},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913859},
	doi = {10.1109/TKDE.2019.2913859},
	timestamp = {Tue, 20 Oct 2020 18:26:34 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/LiH020.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In data mining, large differences between multi-class distributions regarded as class imbalance issues have been known to hinder the classification performance. Unfortunately, existing sampling methods have shown their deficiencies such as causing the problems of over-generation and over-lapping by oversampling techniques, or the excessive loss of significant information by undersampling techniques. This paper presents three proposed sampling approaches for imbalanced learning: the first one is the entropy-based oversampling (EOS) approach; the second one is the entropy-based undersampling (EUS) approach; the third one is the entropy-based hybrid sampling (EHS) approach combined by both oversampling and undersampling approaches. These three approaches are based on a new class imbalance metric, termed entropy-based imbalance degree (EID), considering the differences of information contents between classes instead of traditional imbalance-ratio. Specifically, to balance a data set after evaluating the information influence degree of each instance, EOS generates new instances around difficult-to-learn instances and only remains the informative ones. EUS removes easy-to-learn instances. While EHS can do both simultaneously. Finally, we use all the generated and remaining instances to train several classifiers. Extensive experiments over synthetic and real-world data sets demonstrate the effectiveness of our approaches.}
}


@article{DBLP:journals/tkde/Zheng0L0CZ20,
	author = {Chaoqun Zheng and
                  Lei Zhu and
                  Xu Lu and
                  Jingjing Li and
                  Zhiyong Cheng and
                  Hanwang Zhang},
	title = {Fast Discrete Collaborative Multi-Modal Hashing for Large-Scale Multimedia
                  Retrieval},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2171--2184},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2913388},
	doi = {10.1109/TKDE.2019.2913388},
	timestamp = {Sat, 30 Sep 2023 10:29:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/Zheng0L0CZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many achievements have been made on learning to hash for uni-modal and cross-modal retrieval. However, it is still an unsolved problem that how to directly and efficiently learn discriminative discrete hash codes for the multimedia retrieval, where both query and database samples are represented with heterogeneous multi-modal features. With this motivation, we propose a Fast Discrete Collaborative Multi-modal Hashing (FDCMH) method in this paper. We first propose an efficient collaborative multi-modal mapping that first transforms heterogeneous multi-modal features into the unified factors to exploit the complementarity of multi-modal features and preserve the semantic correlations in multiple modalities with linear computation and space complexity. Such shared factors also bridge the heterogeneous modality gap and remove the inter-modality redundancy. Further, we develop an asymmetric hashing learning module to simultaneously correlate the learned hash codes with low-level data distribution and high-level semantics. In particular, this design could avoid the challenging symmetric semantic matrix factorization and O(n2) memory cost (n is the number of training samples). It can support both computation and memory efficient discrete hash optimization. Experiments on several public multimedia retrieval datasets demonstrate the superiority of the proposed approach compared with state-of-the-art hashing techniques, in terms of both model learning efficiency and retrieval accuracy.}
}


@article{DBLP:journals/tkde/RawassizadehKP20,
	author = {Reza Rawassizadeh and
                  Hamidreza Keshavarz and
                  Michael J. Pazzani},
	title = {Ghost Imputation: Accurately Reconstructing Missing Data of the Off
                  Period},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2185--2197},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2914653},
	doi = {10.1109/TKDE.2019.2914653},
	timestamp = {Tue, 20 Oct 2020 18:26:34 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/RawassizadehKP20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Noise and missing data are intrinsic characteristics of real-world data, leading to uncertainty that negatively affects the quality of knowledge extracted from the data. The burden imposed by missing data is often severe in sensors that collect data from the physical world, where large gaps of missing data may occur when the system is temporarily off or disconnected. How can we reconstruct missing data for these periods? We introduce an accurate and efficient algorithm for missing data reconstruction (imputation), that is specifically designed to recover off-period segments of missing data. This algorithm, Ghost, searches the sequential dataset to find data segments that have a prior and posterior segment that matches those of the missing data. If there is a similar segment that also satisfies the constraint - such as location or time of day - then it is substituted for the missing data. A baseline approach results in quadratic computational complexity, therefore we introduce a caching approach that reduces the search space and improves the computational complexity to linear in the common case. Experimental evaluations on five real-world datasets show that our algorithm significantly outperforms four state-of-the-art algorithms with an average of 18 percent higher F-score.}
}


@article{DBLP:journals/tkde/WeiWL020,
	author = {Victor Junqiu Wei and
                  Raymond Chi{-}Wing Wong and
                  Cheng Long and
                  Pan Hui},
	title = {On Nearby-Fit Spatial Keyword Queries},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2198--2212},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2915295},
	doi = {10.1109/TKDE.2019.2915295},
	timestamp = {Tue, 22 Oct 2024 20:38:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WeiWL020.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geo-textual data is ubiquitous nowadays, where each object has a location and is associated with some keywords. Many types of queries based on geo-textual data, termed as spatial keyword queries , have been proposed, and are to find optimal object(s) in terms of both its (their) location(s) and keywords. In this paper, we propose a new type of query called nearby-fit spatial keyword query (NSKQ), where an optimal object is defined based not only on the location and the keywords of the object itself, but also on those of the objects nearby . For example, in an application of finding a hotel, not only the location of a hotel but also the objects near the hotel (e.g., shopping malls, restaurants, and bus stops nearby) might need to be taken into consideration. The query is proved to be NP-hard, and in order to perform the query efficiently, we developed two approximate algorithms with small constant approximation factors equal to 1.155 and 1.79. We conducted extensive experiments based on both real and synthetic datasets, which verified our algorithms.}
}


@article{DBLP:journals/tkde/Ji0WL20,
	author = {Shenggong Ji and
                  Yu Zheng and
                  Wenjun Wang and
                  Tianrui Li},
	title = {Real-Time Ambulance Redeployment: {A} Data-Driven Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2213--2226},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2914206},
	doi = {10.1109/TKDE.2019.2914206},
	timestamp = {Thu, 15 Jul 2021 13:46:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/Ji0WL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emergency Medical Services (EMS) are of great importance to saving people's lives from emergent accidents and diseases by efficiently picking up patients using ambulances. The transporting capability of an EMS system (e.g., defined as the average pickup time of patients) significantly depends on the real-time redeployment strategy of ambulances. That is, which station should an ambulance be redeployed to, after it becomes available (after it transports a patient to a hospital or after it finishes the in-site treatment for a patient)? However, it is a challenging task concerning with the multiple data D1-D5 as detailed in Introduction. To this end, in this paper, we propose a data-driven real-time ambulance redeployment approach that redeploys an ambulance to a proper station after it becomes available, so as to optimize the transporting capability of an EMS system, considering the aforementioned multiple data D1-D5. Specifically, the proposed approach is comprised of two stages to well consider the D1-D5. First, we propose a method (a safety time-based urgency index) to incorporate D1, D2, and D3 into each ambulance station's urgency degree (D*). Second, we propose an optimal matching algorithm to combine D*, D4, and D5 into the redeployment of the current available ambulance. Experimental results using data collected in real world demonstrate the significant advantages of our approach over many baselines. Comparing with baselines, our approach can save ~4 minutes (~35 percent) of the average pickup time for each patient, improve the ratio of patients picked up within 10 minutes from 0.684 and 0.803 (~17 percent), and largely enhance the survival rate of patients (~12 percent for patients in category A1 and ~17 percent for patients in A2).}
}


@article{DBLP:journals/tkde/00010LZ0020,
	author = {Kai Zheng and
                  Yan Zhao and
                  Defu Lian and
                  Bolong Zheng and
                  Guanfeng Liu and
                  Xiaofang Zhou},
	title = {Reference-Based Framework for Spatio-Temporal Trajectory Compression
                  and Query Processing},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2227--2240},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2914449},
	doi = {10.1109/TKDE.2019.2914449},
	timestamp = {Fri, 09 Apr 2021 18:22:30 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/00010LZ0020.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The pervasiveness of GPS-enabled devices and wireless communication technologies results in massive trajectory data, incurring expensive cost for storage, transmission, and query processing. To relieve this problem, in this paper we propose a novel framework for compressing trajectory data, REST (Reference-based Spatio-temporal trajectory compression), by which a raw trajectory is represented by concatenation of a series of historical (sub-)trajectories (called reference trajectories) that form the compressed trajectory within a given spatio-temporal deviation threshold. In order to construct a reference trajectory set that can most benefit the subsequent compression, we propose three kinds of techniques to select reference trajectories wisely from a large dataset such that the resulting reference set is more compact yet covering most footprints of trajectories in the area of interest. To address the computational issue caused by the large number of combinations of reference trajectories that may exist for resembling a given trajectory, we propose efficient greedy algorithms that run in the blink of an eye and dynamic programming algorithms that can achieve the optimal compression ratio. Compared to existing work on trajectory compression, our framework has few assumptions about data such as moving within a road network or moving with constant direction and speed, and better compression performance with fairly small spatio-temporal loss. In addition, by indexing the reference trajectories directly with an in-memory R-tree and building connections to the raw trajectories with inverted index, we develop an extremely efficient algorithm that can answer spatio-temporal range queries over trajectories in their compressed form. Extensive experiments on a real taxi trajectory dataset demonstrate the superiority of our framework over existing representative approaches in terms of both compression ratio and efficiency.}
}


@article{DBLP:journals/tkde/BurySS20,
	author = {Marc Bury and
                  Chris Schwiegelshohn and
                  Mara Sorella},
	title = {Similarity Search for Dynamic Data Streams},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2241--2253},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2916858},
	doi = {10.1109/TKDE.2019.2916858},
	timestamp = {Tue, 20 Oct 2020 18:26:34 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/BurySS20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nearest neighbor searching systems are an integral part of many online applications, including but not limited to pattern recognition, plagiarism detection, and recommender systems. With increasingly larger data sets, scalability has become an important issue. Many of the most space and running time efficient algorithms are based on locality-sensitive hashing. Here, we view the data set as an n by lUl matrix where each row corresponds to one of n users and the columns correspond to items drawn from a universe U. The de-facto standard approach to quickly answer nearest neighbor queries on such a data set is usually a form of min-hashing. Not only is min-hashing very fast, but it is also space efficient and can be implemented in many computational models aimed at dealing with large data sets such as MapReduce and streaming. However, a significant drawback is that minhashing and related methods are only able to handle insertions to user profiles and tend to perform poorly when items may be removed. We initiate the study of scalable locality-sensitive hashing (LSH) for fully dynamic data-streams. Specifically, using the Jaccard index as similarity measure, we design (1) a collaborative filtering mechanism maintainable in dynamic data streams and (2) a sketching algorithm for similarity estimation. Our algorithms have little overhead in terms of running time compared to previous LSH approaches for the insertion only case, and drastically outperform previous algorithms in case of deletions.}
}


@article{DBLP:journals/tkde/FengGCB020,
	author = {Kaiyu Feng and
                  Tao Guo and
                  Gao Cong and
                  Sourav S. Bhowmick and
                  Shuai Ma},
	title = {{SURGE:} Continuous Detection of Bursty Regions Over a Stream of Spatial
                  Objects},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2254--2268},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2915654},
	doi = {10.1109/TKDE.2019.2915654},
	timestamp = {Mon, 21 Oct 2024 15:07:23 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/FengGCB020.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of mobile devices and location-based services, continuous generation of massive volume of streaming spatial objects (i.e., geo-tagged data) opens up new opportunities to address real-world problems by analyzing them. In this paper, we present a novel continuous bursty region detection (SURGE) problem that aims to continuously detect a burstyregion of a given size in a specified geographical area from a stream of spatial objects. Specifically, a bursty region shows maximum spike in the number of spatial objects in a given time window. The SURGE problem is useful in addressing several real-world challenges such as surge pricing problem in online transportation and disease outbreak detection. To solve the problem, we propose an exact solution and two approximate solutions, and the approximation ratio is 1-α/4 in terms of the burst score, where α is a parameter to control the burst score. We further extend these solutions to support detection of top-k bursty regions. Extensive experiments with real-world data are conducted to demonstrate the efficiency and effectiveness of our solutions.}
}


@article{DBLP:journals/tkde/WangWWGX20,
	author = {Jingyuan Wang and
                  Junjie Wu and
                  Ze Wang and
                  Fei Gao and
                  Zhang Xiong},
	title = {Understanding Urban Dynamics via Context-Aware Tensor Factorization
                  with Neighboring Regularization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {11},
	pages = {2269--2283},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2915231},
	doi = {10.1109/TKDE.2019.2915231},
	timestamp = {Wed, 14 Aug 2024 08:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangWWGX20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the world-wide emergence of mega-metropolises with incredibly huge populations. Understanding residents mobility patterns, or urban dynamics, thus becomes crucial for building modern smart cities. In this paper, we propose a Neighbor-Regularized and context-aware Non-negative Tensor Factorization model (NR-cNTF) to discover interpretable urban dynamics from urban heterogeneous data. Different from many existing studies concerned with prediction tasks via tensor completion, NR-cNTF focuses on gaining urban managerial insights from spatial, temporal, and spatio-temporal patterns. This is enabled by high-quality Tucker factorizations regularized by both POI-based urban contexts and geographically neighboring relations. NR-cNTF is also capable of unveiling long-term evolutions of urban dynamics via a pipeline initialization approach. We apply NR-cNTF to a real-life data set containing rich taxi GPS trajectories and POI records of Beijing. The results indicate: 1) NR-cNTF accurately captures four kinds of city rhythms and seventeen spatial communities; 2) the rapid development of Beijing, epitomized by the CBD area, indeed intensifies the job-housing imbalance; 3) the southern areas with recent government investments have shown more healthy development tendency. Finally, NR-cNTF is compared with some baselines on traffic prediction, which further justifies the importance of urban contexts awareness and neighboring regulations.}
}


@article{DBLP:journals/tkde/ChungKPTW20,
	author = {Yeounoh Chung and
                  Tim Kraska and
                  Neoklis Polyzotis and
                  Ki Hyun Tae and
                  Steven Euijong Whang},
	title = {Automated Data Slicing for Model Validation: {A} Big Data - {AI} Integration
                  Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2284--2296},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2916074},
	doi = {10.1109/TKDE.2019.2916074},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ChungKPTW20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As machine learning systems become democratized, it becomes increasingly important to help users easily debug their models. However, current data tools are still primitive when it comes to helping users trace model performance problems all the way to the data. We focus on the particular problem of slicing data to identify subsets of the validation data where the model performs poorly. This is an important problem in model validation because the overall model performance can fail to reflect that of the smaller subsets, and slicing allows users to analyze the model performance on a more granular-level. Unlike general techniques (e.g., clustering) that can find arbitrary slices, our goal is to find interpretable slices (which are easier to take action compared to arbitrary subsets) that are problematic and large. We propose SliceFinder, which is an interactive framework for identifying such slices using statistical techniques. Applications include diagnosing model fairness and fraud detection, where identifying slices that are interpretable to humans is crucial. This research is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.}
}


@article{DBLP:journals/tkde/HanHHXTXH20,
	author = {Kai Han and
                  Yuntian He and
                  Keke Huang and
                  Xiaokui Xiao and
                  Shaojie Tang and
                  Jingxin Xu and
                  Liusheng Huang},
	title = {Best Bang for the Buck: Cost-Effective Seed Selection for Online Social
                  Networks},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2297--2309},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2922271},
	doi = {10.1109/TKDE.2019.2922271},
	timestamp = {Tue, 02 Jan 2024 17:19:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/HanHHXTXH20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the min-cost seed selection problem in online social networks for viral marketing, where the goal is to select a set of seed nodes with the minimum total cost such that the expected number of influenced nodes in the network exceeds a predefined threshold. We propose several algorithms that outperform the previous studies both on the theoretical approximation ratio and on the experimental performance. In the case where the nodes have heterogeneous costs, our algorithms are the first bi-criteria approximation algorithms with polynomial running time and provable approximation ratio. In the case where the users have uniform costs, our algorithms achieve logarithmic approximation ratio and provable time complexity which is smaller than that of the existing algorithms in orders of magnitude. We conduct extensive experiments using real social networks. The experimental results show that, our algorithms significantly outperform the existing algorithms both on the total cost and on the running time, and also scale well to billion-scale networks.}
}


@article{DBLP:journals/tkde/WangPZ20,
	author = {Shangfei Wang and
                  Guozhu Peng and
                  Zhuangqiang Zheng},
	title = {Capturing Joint Label Distribution for Multi-Label Classification
                  Through Adversarial Learning},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2310--2321},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2922603},
	doi = {10.1109/TKDE.2019.2922603},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/WangPZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Label correlations are important for multi-label learning. Although current multi-label learning approaches can exploit first-order, second-order, and high-order label dependencies, they fail to exploit complete label correlations, which are included in the joint label distribution of the ground truth labels. However, directly modeling the complex and unknown joint label distribution is very challenging, if not impossible. In this paper, we propose an adversarial learning framework to enforce similarity between joint distribution of the ground truth multi-labels and the predicted multiple labels. Specifically, the proposed multi-label learning method includes a multi-label classifier and a label discriminator. The classifier minimizes error between predicted labels and corresponding ground truth labels and gives the discriminator room for error. The object of the discriminator is to distinguish the predicted labels from the ground truth labels. The classifier and discriminator are trained simultaneously through an alternate process. By adversarial learning, the joint label distribution of the predicted multi-labels converges to the joint distribution inherent in the ground truth multi-labels, and thus boosts the performance of multi-label learning as demonstrated in the experiments on 11 benchmark databases.}
}


@article{DBLP:journals/tkde/ZhangWLLX20,
	author = {Peng Zhang and
                  Suge Wang and
                  Deyu Li and
                  Xiaoli Li and
                  Zhikang Xu},
	title = {Combine Topic Modeling with Semantic Embedding: Embedding Enhanced
                  Topic Model},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2322--2335},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2922179},
	doi = {10.1109/TKDE.2019.2922179},
	timestamp = {Mon, 03 Jan 2022 21:57:03 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangWLLX20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Topic model and word embedding reflect two perspectives of text semantics. Topic model maps documents into topic distribution space by utilizing word collocation patterns within and across documents, while word embedding represents words within a continuous embedding space by exploiting the local word collocation patterns in context windows. Clearly, these two types of patterns are complementary. In this paper, we propose a novel integration framework to combine the two representation methods, where topic information can be transmitted into corresponding semantic embedding structure. Based on this framework, we construct a Embedding Enhanced Topic Model (EETM), which can improve topic modeling and generate topic embeddings by leveraging the word embedding. Extensive experimental results show that EETM can learn high-quality document representations for common text analysis tasks across multiple data sets, indicating it is very effective for merging topic models with word embeddings.}
}


@article{DBLP:journals/tkde/ZhaoZLSLZ20,
	author = {Yan Zhao and
                  Kai Zheng and
                  Yang Li and
                  Han Su and
                  Jiajun Liu and
                  Xiaofang Zhou},
	title = {Destination-Aware Task Assignment in Spatial Crowdsourcing: {A} Worker
                  Decomposition Approach},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2336--2350},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2922604},
	doi = {10.1109/TKDE.2019.2922604},
	timestamp = {Mon, 28 Aug 2023 21:37:39 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/ZhaoZLSLZ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of GPS-enabled smart devices and increased availability of wireless network, spatial crowdsourcing (SC) has been recently proposed as a framework to automatically request workers (i.e., smart device carriers) to perform location-sensitive tasks (e.g., taking scenic photos, reporting events). In this paper, we study a destination-aware task assignment problem that concerns the optimal strategy of assigning each task to proper worker such that the total number of completed tasks can be maximized whilst all workers can reach their destinations before deadlines after performing assigned tasks. Finding the global optimal assignment turns out to be an intractable problem since it does not imply optimal assignment for individual worker. Observing that the task assignment dependency only exists amongst subsets of workers, we utilize tree-decomposition technique to separate workers into independent clusters and develop an efficient depth-first search algorithm with progressive bounds to prune non-promising assignments. In order to make our proposed framework applicable to more scenarios, we further optimize the original framework by proposing strategies to reduce the overall travel cost and allow each task to be assigned to multiple workers. Extensive empirical studies verify that the proposed technique and optimization strategies perform effectively and settle the problem nicely.}
}


@article{DBLP:journals/tkde/ThangavelV20,
	author = {M. Thangavel and
                  P. Varalakshmi},
	title = {Enabling Ternary Hash Tree Based Integrity Verification for Secure
                  Cloud Data Storage},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2351--2362},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2922357},
	doi = {10.1109/TKDE.2019.2922357},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ThangavelV20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud Computing enables the remote users to access data, services, and applications in on-demand from the shared pool of configurable computing resources, without the consideration of storage, hardware, and software management. On the other hand, it is not easy for cloud users to identify whether Cloud Service Provider's (CSP) tag along with the data security legal expectations. So, cloud users could not rely on CSP's in terms of trust. So, it is significant to build a secure and efficient data auditing framework for increasing and maintaining cloud users trust with CSP. Researchers suggested introducing Third Party Auditor (TPA) on behalf of cloud user for verifying the outsourced data integrity, which may reduce the computation overhead of cloud users. In this work, we proposed a novel integrity verification framework for securing cloud storage based on Ternary Hash Tree (THT) and Replica based Ternary Hash Tree (R-THT), which will be used by TPA to perform data auditing. Differing from existing work, the proposed framework performs Block-level, File-level and Replica-level auditing with tree block ordering, storage block ordering for verifying the data integrity and ensuring data availability in the cloud. We further extend our framework to support error localization with data correctness, dynamic updates with block update, insert, and delete operations in the cloud. The structure of THT and R-THT will reduce the computation cost and provide efficiency in data updates compared to the existing schemes. The security analysis of the proposed public auditing framework indicates the achievement of desired properties and performance has been evaluated with the detailed experiment set. The results show that the proposed secure cloud auditing framework is highly secure and efficient in storage, communication, and computation costs.}
}


@article{DBLP:journals/tkde/SongYITCC20,
	author = {Ikkyun Song and
                  Yicheng Yang and
                  Jongho Im and
                  Tong Tong and
                  Halil Ceylan and
                  In Ho Cho},
	title = {Impacts of Fractional Hot-Deck Imputation on Learning and Prediction
                  of Engineering Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2363--2373},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2922638},
	doi = {10.1109/TKDE.2019.2922638},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SongYITCC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In broad engineering fields, missing data is a common issue which often causes undesired bias and sparseness impeding rigorous data analyses. To tackle this problem, many imputation theories have been proposed and widely used. However, prior methods often require distributional assumptions and prior knowledge regarding data which may cause some difficulty for engineering research. Essentially, the fractional hot-deck imputation (FHDI) is an assumption-free imputation method, holding broad applicability in the engineering domains. FHDIs internal parameters and impact on statistical and machine learning methods, however, have been rarely understood. Thus, this study investigates the behavior and impacts of FHDI on prediction methods including generalized additive model, support vector machine, extremely randomized trees, and artificial neural network, for which four practical datasets (appliance energy, air quality, phenotypes, and weather) are used. Results show that FHDI performs better for improving the prediction accuracy compared to a simple naive method which cures missing data using the mean value of attributes, and FHDI has an asymptotically positive effect on prediction accuracy with decreasing response rates. Regarding an optimal setting, 30 to 35 is recommended for the FHDIs internal categorization number while 5 is recommended for the FHDI donors, which is aligned with Rubins recommendation.}
}


@article{DBLP:journals/tkde/WangCCZY20,
	author = {Senzhang Wang and
                  Hao Chen and
                  Jiannong Cao and
                  Jiawei Zhang and
                  Philip S. Yu},
	title = {Locally Balanced Inductive Matrix Completion for Demand-Supply Inference
                  in Stationless Bike-Sharing Systems},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2374--2388},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2922636},
	doi = {10.1109/TKDE.2019.2922636},
	timestamp = {Thu, 23 Jun 2022 13:36:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/WangCCZY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stationless bike-sharing systems such as Mobike are currently becoming extremely popular in China as well as some other big cities in the world. Compared to traditional bicycle-sharing systems, stationless bike-sharing systems do not need bike stations. Users can rent and return bikes at arbitrary locations through an App installed on their smart phones. Such a convenient and flexible bike-sharing mode greatly solves the last mile issue of the commuters, and better meets their real bike usage demand. However, it also poses new challenges for operators to manage the system. The first primary challenge is how to accurately estimate the real bike usage demand in different areas of a city and in different time intervals, which is crucial for the system planning and operation. This paper for the first time proposes a data driven approach for bike usage demand inference in stationless bike-sharing systems. The idea is that we first estimate the demands in some regions and time intervals from a small number of observed bike check-out/in data directly, and then use them as seeds to infer the region-level bike usage demands of an entire city. Specifically, we formulate this problem as a matrix completion task by modeling the bike usage demand as a matrix whose two dimensions are time intervals of a day and regions of a city, respectively. With the observation that POI distribution of a region is an important indicator to bike demand, we propose to utilize inductive matrix factorization by considering POIs as side information. As the bike usage data are highly correlated in both spatial and temporal dimensions, we also incorporate the spatial-temporal correlations as well as the balanced bike usage constraint into a joint optimization framework. We evaluate the proposed model on a large Mobike trip dataset collected from Beijing, and the experimental results show its superior performance by comparison with various baseline methods.}
}


@article{DBLP:journals/tkde/NieTWL20,
	author = {Feiping Nie and
                  Lai Tian and
                  Rong Wang and
                  Xuelong Li},
	title = {Multiview Semi-Supervised Learning Model for Image Classification},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2389--2400},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2920985},
	doi = {10.1109/TKDE.2019.2920985},
	timestamp = {Mon, 14 Feb 2022 16:41:11 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NieTWL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semi-supervised learning models for multiview data are important in image classification tasks, since heterogeneous features are easy to obtain and semi-supervised schemes are economical and effective. To model the view importance, conventional graph-based multiview learning models learn a linear combination of views while assuming a priori weights distribution. In this paper, we present a novel structural regularized semi-supervised model for multiview data, termed Adaptive MUltiview SEmi-supervised model (AMUSE). Our new model learns weights from a priori graph structure, which is more reasonable than weight regularization. Theoretical analysis reveals the significant difference between AMUSE and the prior arts. An efficient optimization algorithm is provided to solve the new model. Experimental results on six real-world data sets demonstrate the effectiveness of the structural regularized weights learning scheme.}
}


@article{DBLP:journals/tkde/XieS20,
	author = {Xijiong Xie and
                  Shiliang Sun},
	title = {Multi-View Support Vector Machines with the Consensus and Complementarity
                  Information},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2401--2413},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2933511},
	doi = {10.1109/TKDE.2019.2933511},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XieS20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view learning (MVL) is an active direction in machine learning that aims at exploiting the consensus and complementarity information among multiple distinct feature sets to boost the generalization performance of the counterpart algorithm. So far, two classical SVM-based MVL methods are SVM-2K and multi-view twin support vector machine (MvTSVM). They are designed only for two-view classification and cannot tackle the general multi-view classification problem. They also cannot effectively leverage the complementarity information among different feature views. In this paper, we propose two novel multi-view support vector machines with the consensus and complementarity information for MVL that not only can deal with the two-view classification problem but also the general multi-view classification problem by jointly learning multiple different views in a non-pairwise way. The disagreement among different views is regarded as a constraint or a regularization term in the objective function which plays an important role in exploring the consensus information. Combination weights for the reconstruction of each view in regularization terms are learned to explore complementarity information among different views. Finally, an efficient iteration algorithm with the classical convex quadratic programming is developed for optimization. Experimental results validate the effectiveness of our proposed methods.}
}


@article{DBLP:journals/tkde/XuCWY20,
	author = {Linchuan Xu and
                  Jiannong Cao and
                  Xiaokai Wei and
                  Philip S. Yu},
	title = {Network Embedding via Coupled Kernelized Multi-Dimensional Array Factorization},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2414--2425},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2931833},
	doi = {10.1109/TKDE.2019.2931833},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/XuCWY20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network embedding has been widely employed in networked data mining applications as it can learn low-dimensional and dense node representations from the high-dimensional and sparse network structure. While most existing network embedding methods only model the proximity between two nodes regardless of the order of the proximity, this paper proposes to explicitly model multi-node proximities which can be widely observed in practice, e.g., multiple researchers coauthor a paper, and multiple genes co-express a protein. Explicitly modeling multi-node proximities is important because some two-node interactions may not come into existence without a third node. By proving that LINE(1st), a recent network embedding method, is equivalent to kernelized matrix factorization, this paper proposes coupled kernelized multi-dimensional array factorization (Cetera) which jointly factorizes multiple multi-dimensional arrays by enforcing a consensus representation for each node. In this way, node representations can be more comprehensive and effective, which is demonstrated on three real-world networks through link prediction and multi-label classification.}
}


@article{DBLP:journals/tkde/ZhangCSLRL20,
	author = {Xiaoqian Zhang and
                  Beijia Chen and
                  Huaijiang Sun and
                  Zhigui Liu and
                  Zhenwen Ren and
                  Yanmeng Li},
	title = {Robust Low-Rank Kernel Subspace Clustering based on the Schatten p-norm
                  and Correntropy},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2426--2437},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2922637},
	doi = {10.1109/TKDE.2019.2922637},
	timestamp = {Tue, 29 Dec 2020 18:17:25 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/ZhangCSLRL20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Subspace clustering plays an important role in the tasks such as data processing and pattern recognition. Since the high-dimensional data may contain complex noise, as well as non-linear structure, learning low-dimensional subspace structures is a challenging task. However, the existing methods to deal with both problems relax the original problem convexly. The results of solving by these methods deviate from the solution of the original problem. In this paper, to overcome this deficiency, we propose a robust low-rank kernel subspace clustering model, which coalesces the non-convex Schatten p-norm (0 <; p ≤ 1) regularizer with “kernel trick” and correntropy. Our “kernel trick” extends linear subspace clustering to non-linear counterparts, the Schatten p-norm regularizer can approximate the rank of the data in feature space effectively, and the correntropy is a robust measure to large corruptions. Furthermore, an efficient iterative algorithm (HQ-ADMM) is designed to solve the formulated problem, which coalesces the half-quadratic technique and Alternating Direction Method of Multipliers. This algorithm can ensure the closed form solutions at each iteration, which improves the computation speed of the algorithm. Extensive experiments on face/object clustering and motion segmentation clearly attest the ascendancy of the proposed method over several state-of-the-art methods.}
}


@article{DBLP:journals/tkde/NajafiSJ20,
	author = {Mohammadreza Najafi and
                  Mohammad Sadoghi and
                  Hans{-}Arno Jacobsen},
	title = {Scalable Multiway Stream Joins in Hardware},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2438--2452},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2916860},
	doi = {10.1109/TKDE.2019.2916860},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/NajafiSJ20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient real-time analytics are an integral part of an increasing number of data management applications, such as computational targeted advertising, algorithmic trading, and Internet of Things. In this paper, we focus primarily on accelerating stream joins, which are arguably one of the most commonly used and resource-intensive operators in stream processing. We propose a scalable circular pipeline design (\nCircular-MJ\n) in hardware to orchestrate a multiway join while minimizing data flow disruption. In this circular design, each new tuple (given its origin stream) starts its processing from a specific join core and passes through all respective join cores in a pipeline sequence to produce the final results. We also present a novel two-stage pipeline stream join (\nStashed-MJ\n) that uses a best-effort buffering technique (referred to as stash) to maintain intermediate results. If an overwrite is detected in the stash, our design automatically resorts to recomputing intermediate results. Finally, we present a parallelized version of our multiway stream join by integrating our proposed pipelines into a parallel unidirectional flow-based architecture (\nParallel-MJ\n). Our experimental results demonstrate a linear throughput scaling with respect to the numbers of streams and processing cores.}
}


@article{DBLP:journals/tkde/SongJC20,
	author = {Wei Song and
                  Hans{-}Arno Jacobsen and
                  Fangfei Chen},
	title = {Scientific Workflow Protocol Discovery from Public Event Logs in Clouds},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2453--2466},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2922183},
	doi = {10.1109/TKDE.2019.2922183},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/SongJC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advancement of cloud computing, many challenging scientific problems can be solved using scientific workflow technology which integrates geo-distributed instruments, applications, and big data effectively and efficiently. For workflow collaboration, the workflow protocols of all participants are needed. However, workflow protocols are not always available and are often outdated as the workflow evolve frequently. To address this problem, we propose a novel workflow discovery approach which can extract up-to-date scientific workflow protocols from public event logs in clouds, without the need to access the full-fledged event logs involving private events. Our approach leverages transitive precedence relations between events to achieve this. We implement our approach as a ProM plug-in, and evaluate it through extensive experiments on event logs of real-world scientific workflows. The experimental results demonstrate that our approach requires a weaker completeness notion of event logs than the state-of-the-art do, and our approach derives the same workflow protocol from the public event log as that discovered from the original event log, and thus the private events can be protected.}
}


@article{DBLP:journals/tkde/XuYWGXG20,
	author = {Yang Xu and
                  Bin Yao and
                  Zhi{-}Jie Wang and
                  Xiaofeng Gao and
                  Jiong Xie and
                  Minyi Guo},
	title = {Skia: Scalable and Efficient In-Memory Analytics for Big Spatial-Textual
                  Data},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2467--2480},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2915828},
	doi = {10.1109/TKDE.2019.2915828},
	timestamp = {Mon, 13 Jun 2022 14:32:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/XuYWGXG20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, spatial-keyword queries have attracted much attention with the fast development of location-based services. However, current spatial-keyword techniques are disk-based, which cannot fulfill the requirements of high throughput and low response time. With the surging data size, people tend to process data in distributed in-memory environments to achieve low latency. In this paper, we present the distributed solution, i.e., Skia (Spatial-Keyword In-memory Analytics), to provide a scalable backend for spatial-textual analytics. Skia introduces a two-level index framework for big spatial-textual data including: (1) efficient and scalable global index, which prunes the candidate partitions a lot while achieving small space budget; and (2) four novel local indexes, that further support low latency services for exact and approximate spatial-keyword queries. Skia can support common spatial-keyword queries via traditional SQL programming interfaces. The experiments conducted on large-scale real datasets have demonstrated the promising performance of the proposed indexes and our distributed solution.}
}


@article{DBLP:journals/tkde/MiorandiRSC20,
	author = {Daniele Miorandi and
                  Alessandra Rizzardi and
                  Sabrina Sicari and
                  Alberto Coen{-}Porisini},
	title = {Sticky Policies: {A} Survey},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2481--2499},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2019.2936353},
	doi = {10.1109/TKDE.2019.2936353},
	timestamp = {Thu, 31 Dec 2020 01:34:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkde/MiorandiRSC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the digital age, where the Internet connects things across the globe and individuals are constantly online, data security and privacy are becoming key drivers (and barriers) of change for adoption of innovative solutions. Traditional approaches, whereby communication links are secured by means of encryption, and access control is run in a static way by a centralized authority, are showing their limits when applied to massive-scale, interconnected and distributed systems. Regulations, while still fragmented, are moving to adapt to changes in technology and society, with the aim to protect confidential information by governments, businesses, and individual citizens. In this landscape, proper mechanisms should be defined to allow a strict control over the data life-cycle and to guarantee the privacy and the application of specific regulations on personal information's disclosure, usage and access. Sticky policies represent one approach to improve owners' control over their data. In such an approach, machine-readable policies are attached to data. They are called 'sticky' in that they travel together with data, as data travels across multiple administrative domains. In this article we survey the state-of-the-art in sticky policies, discussing limitations, open issues, applications and research challenges, with a specific focus on their applicability to Internet of Things, cloud computing, and Content Centric Networking.}
}


@article{DBLP:journals/tkde/JiangC20,
	author = {Shan Jiang and
                  Hsinchun Chen},
	title = {Corrections to "NATERGM: {A} Model for Examining the Role of Nodal
                  Attributes in Dynamic Social Media Networks"},
	journal = {{IEEE} Trans. Knowl. Data Eng.},
	volume = {32},
	number = {12},
	pages = {2500},
	year = {2020},
	url = {https://doi.org/10.1109/TKDE.2020.3005579},
	doi = {10.1109/TKDE.2020.3005579},
	timestamp = {Thu, 27 Apr 2023 09:09:05 +0200},
	biburl = {https://dblp.org/rec/journals/tkde/JiangC20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Presents corrections to affiliation information in the above named paper.}
}
