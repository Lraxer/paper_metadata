@article{DBLP:journals/toit/BoldiG21,
	author = {Paolo Boldi and
                  Georgios Gousios},
	title = {Fine-Grained Network Analysis for Modern Software Ecosystems},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {1:1--1:14},
	year = {2021},
	url = {https://doi.org/10.1145/3418209},
	doi = {10.1145/3418209},
	timestamp = {Tue, 27 Apr 2021 15:11:04 +0200},
	biburl = {https://dblp.org/rec/journals/toit/BoldiG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern software development is increasingly dependent on components, libraries, and frameworks coming from third-party vendors or open-source suppliers and made available through a number of platforms (or forges). This way of writing software puts an emphasis on reuse and on composition, commoditizing the services that modern applications require. On the other hand, bugs and vulnerabilities in a single library living in one such ecosystem can affect, directly or by transitivity, a huge number of other libraries and applications. Currently, only product-level information on library dependencies is used to contain this kind of danger, but this knowledge often reveals itself too imprecise to lead to effective (and possibly automated) handling policies. We will discuss how fine-grained function-level dependencies can greatly improve reliability and reduce the impact of vulnerabilities on the whole software ecosystem.}
}


@article{DBLP:journals/toit/HuangMLH21,
	author = {Liwei Huang and
                  Yutao Ma and
                  Yanbo Liu and
                  Keqing He},
	title = {{DAN-SNR:} {A} Deep Attentive Network for Social-aware Next Point-of-interest
                  Recommendation},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {2:1--2:27},
	year = {2021},
	url = {https://doi.org/10.1145/3430504},
	doi = {10.1145/3430504},
	timestamp = {Wed, 19 Jun 2024 17:14:15 +0200},
	biburl = {https://dblp.org/rec/journals/toit/HuangMLH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next (or successive) point-of-interest (POI) recommendation, which aims to predict where users are likely to go next, has recently emerged as a new research focus of POI recommendation. Most of the previous studies on next POI recommendation attempted to incorporate the spatiotemporal information and sequential patterns of user check-ins into recommendation models to predict the target user's next move. However, few of the next POI recommendation approaches utilized the social influence of each user's friends. In this study, we discuss a new topic of next POI recommendation and present a deep attentive network for social-aware next POI recommendation called DAN-SNR. In particular, the DAN-SNR makes use of the self-attention mechanism instead of the architecture of recurrent neural networks to model sequential influence and social influence in a unified manner. Moreover, we design and implement two parallel channels to capture short-term user preference and long-term user preference as well as social influence, respectively. By leveraging multi-head self-attention, the DAN-SNR can model long-range dependencies between any two historical check-ins efficiently and weigh their contributions to the next destination adaptively. We also carried out a comprehensive evaluation using large-scale real-world datasets collected from two popular location-based social networks, namely, Gowalla and Brightkite. Experimental results indicate that the DAN-SNR outperforms seven competitive baseline approaches regarding recommendation performance and is highly efficient among six neural-network-based methods, four of which utilize the attention mechanism.}
}


@article{DBLP:journals/toit/GanLFCY21,
	author = {Wensheng Gan and
                  Jerry Chun{-}Wei Lin and
                  Philippe Fournier{-}Viger and
                  Han{-}Chieh Chao and
                  Philip S. Yu},
	title = {Beyond Frequency: Utility Mining with Varied Item-specific Minimum
                  Utility},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {3:1--3:32},
	year = {2021},
	url = {https://doi.org/10.1145/3425498},
	doi = {10.1145/3425498},
	timestamp = {Sat, 08 Jan 2022 02:24:26 +0100},
	biburl = {https://dblp.org/rec/journals/toit/GanLFCY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consumer behavior plays a very important role in economics and targeted marketing. However, understanding economic consumer behavior is quite challenging, such as finding credible and reliable information on product profitability. Different from frequent pattern mining, utility-oriented mining integrates utility theory and data mining. Utility mining is a useful tool for understanding economic consumer behavior. Traditional algorithms for mining high-utility patterns (HUPs) applies a single/uniform minimum utility threshold (minutil) to obtain the set of HUPs, but in some real-life circumstances, some specific products may bring lower utilities compared with others, but their profit may offer some vital information. If minutil is set high, the patterns with low minutil are missed; if minutil is set low, the number of patterns becomes unmanageable. In this article, an efficient one-phase utility-oriented pattern mining algorithm, called HIMU, is proposed for mining HUPs with varied item-specific minimum utility. A novel tree structure called a multiple item utility set-enumeration tree (MIU-tree) and the global sorted and the conditional downward closure properties are introduced in HIMU. In addition, we extended the compact utility-list structure to keep the necessary information, and thus this one-phase HIMU model greatly reduces the computational costs and memory requirements. Moreover, two pruning strategies are then extended to enhance the performance. We conducted extensive experiments in several synthetic and real-world datasets; the results indicate that the designed one-phase HIMU algorithm can address the “rare item problem” and has better performance than the state-of-the-art algorithms in terms of runtime, memory usage, and scalability. Furthermore, the enhanced algorithms outperform the non-optimized HIMU approach.}
}


@article{DBLP:journals/toit/MohammedFRKFA21,
	author = {Sabah Mohammed and
                  Jinan Fiaidhi and
                  Carlos Ramos and
                  Tai{-}Hoon Kim and
                  Wai{-}Chi Fang and
                  Tarek F. Abdelzaher},
	title = {Blockchain in eCommerce: {A} Special Issue of the {ACM} Transactions
                  on Internet of ThingsBlockchain in eCommerce: {A} Special Issue of
                  the {ACM} Transactions on Internet of Things},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {11--55},
	year = {2021},
	url = {https://doi.org/10.1145/3445788},
	doi = {10.1145/3445788},
	timestamp = {Mon, 26 Jun 2023 20:52:11 +0200},
	biburl = {https://dblp.org/rec/journals/toit/MohammedFRKFA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As blockchain technology is becoming a driving force in the global economy, it is also gaining critical acclaim in the e-commerce industry. Both the blockchain and e-commerce are inseparable as they involve transactions. Blockchain protect transactions and e-commerce activities rely on them. Blockchain technology enables a decentralized marketplace to support important business activities like secure payments, managing the supply chain and reducing the fraud to mention few. In this special issue editorial we are introducing 11 research articles in this hot area of research that were selected by our reviewers from over than 250 submissions. As blockchain technology is becoming a driving force in the global economy, it is also gaining critical acclaim in the e-commerce industry. Both the blockchain and e-commerce are inseparable as they involve transactions. Blockchain protect transactions and e-commerce activities rely on them. Blockchain technology enables a decentralized marketplace to support important business activities like secure payments, managing the supply chain and reducing the fraud to mention few. In this special issue editorial we are introducing 11 research articles in this hot area of research that were selected by our reviewers from over than 250 submissions.}
}


@article{DBLP:journals/toit/RenQLWK21,
	author = {Yongjun Ren and
                  Jian Qi and
                  Yepeng Liu and
                  Jin Wang and
                  Gwang{-}Jun Kim},
	title = {Integrity Verification Mechanism of Sensor Data Based on Bilinear
                  Map Accumulator},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {5:1--5:19},
	year = {2021},
	url = {https://doi.org/10.1145/3380749},
	doi = {10.1145/3380749},
	timestamp = {Sat, 08 Jan 2022 02:24:24 +0100},
	biburl = {https://dblp.org/rec/journals/toit/RenQLWK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive growth in the number of IoT devices, ensuring the integrity of the massive data generated by these devices has become an important issue. Due to the limitation of hardware, most past data integrity verification schemes randomly select partial data blocks and then perform integrity validation on those blocks instead of examining the entire dataset. This will result in that unsampled data blocks cannot be detected even if they are tampered with. To solve this problem, we propose a new and effective integrity auditing mechanism of sensor data based on a bilinear map accumulator. Using the proposed approach will examine all the data blocks in the dataset, not just some of the data blocks, thus, eliminating the possibility of any cloud manipulation. Compared with other schemes, our proposed solution has been proved to be highly secure for all necessary security requirements, including tag forgery, data deletion, replacement, replay, and data leakage attacks. The solution reduces the computational and storage costs of cloud storage providers and verifiers, and also supports dynamic operations for data owners to insert, delete, and update data by using a tag index table (TIT). Compared with existing schemes based on RSA accumulator, our scheme has the advantages of fast verification and witness generation and no need to map data blocks to prime numbers. The new solution supports all the characteristics of a data integrity verification scheme.}
}


@article{DBLP:journals/toit/GaoHD21,
	author = {Honghao Gao and
                  Wanqiu Huang and
                  Yucong Duan},
	title = {The Cloud-edge-based Dynamic Reconfiguration to Service Workflow for
                  Mobile Ecommerce Environments: {A} QoS Prediction Perspective},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {6:1--6:23},
	year = {2021},
	url = {https://doi.org/10.1145/3391198},
	doi = {10.1145/3391198},
	timestamp = {Tue, 27 Apr 2021 15:11:03 +0200},
	biburl = {https://dblp.org/rec/journals/toit/GaoHD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of mobile service composition meets the current needs for real-time eCommerce. However, the requirements for eCommerce, such as safety and timeliness, are becoming increasingly strict. Thus, the cloud-edge hybrid computing model has been introduced to accelerate information processing, especially in a mobile scenario. However, the mobile environment is characterized by limited resource storage and users who frequently move, and these characteristics strongly affect the reliability of service composition running in this environment. Consequently, applications are likely to fail if inappropriate services are invoked. To ensure that the composite service can operate normally, traditional dynamic reconfiguration methods tend to focus on cloud services scheduling. Unfortunately, most of these approaches cannot support timely responses to dynamic changes. In this article, the cloud-edge based dynamic reconfiguration to service workflow for mobile eCommerce environments is proposed. First, the service quality concept is extended. Specifically, the value and cost attributes of a service are considered. The value attribute is used to assess the stability of the service for some time to come, and the cost attribute is the cost of a service invocation. Second, a long short-term memory (LSTM) neural network is used to predict the stability of services, which is related to the calculation of the value attribute. Then, in view of the limited available equipment resources, a method for calculating the cost of calling a service is introduced. Third, candidate services are selected by considering both service stability and the cost of service invocation, thus yielding a dynamic reconfiguration scheme that is more suitable for the cloud-edge environment. Finally, a series of comparative experiments were carried out, and the experimental results prove that the method proposed in this article offers higher stability, less energy consumption, and more accurate service prediction.}
}


@article{DBLP:journals/toit/XuZYWQD21,
	author = {Xiaolong Xu and
                  Dawei Zhu and
                  Xiaoxian Yang and
                  Shuo Wang and
                  Lianyong Qi and
                  Wanchun Dou},
	title = {Concurrent Practical Byzantine Fault Tolerance for Integration of
                  Blockchain and Supply Chain},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {7:1--7:17},
	year = {2021},
	url = {https://doi.org/10.1145/3395331},
	doi = {10.1145/3395331},
	timestamp = {Thu, 23 Jun 2022 20:04:05 +0200},
	biburl = {https://dblp.org/rec/journals/toit/XuZYWQD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, the integration of the supply chain and blockchain is promising, as blockchain successfully eliminates the bullwhip effect in the supply chain. Generally, concurrent Practical Byzantine Fault Tolerance (PBFT) consensus method, named C-PBFT, is powerful to deal with the consensus inefficiencies, caused by the fast node expansion in the supply chain. However, due to the tremendous complicated transactions in the supply chain, it remains challenging to select the credible primary peers in the concurrent clusters. To address this challenge, the peers in the supply chain are classified into several clusters by analyzing the historic transactions in the ledger. Then, the primary peer for each cluster is identified by reputation assessment. Finally, the performance of C-PBFT is evaluated by conducting experiments in Fabric.}
}


@article{DBLP:journals/toit/KimK21,
	author = {Junho Kim and
                  Mucheol Kim},
	title = {Intelligent Mediator-based Enhanced Smart Contract for Privacy Protection},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {8:1--8:16},
	year = {2021},
	url = {https://doi.org/10.1145/3404892},
	doi = {10.1145/3404892},
	timestamp = {Tue, 27 Apr 2021 15:11:04 +0200},
	biburl = {https://dblp.org/rec/journals/toit/KimK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of ICT technology, users are actively participated in content creation and sharing. Service providers have provided more diverse and extensive information services. With the recent evolution to a customized service, the demand for the use of personal information is increasing. However, it affects the efficiency and convenience of service provision, as social problems and security threats such as personal information leakage, human trafficking, and misuse increase. In this article, we proposed an intelligent mediator-based enhanced smart contract for privacy protection. The proposed approach performs the tasks, which are mediation transactions, authorization, and transaction record management, with blockchain-based personal information management. Then it is possible to prevent misuse of personal information and to support rational decision-making on the transparency of information and the use of personal information by autonomously performing personal information management.}
}


@article{DBLP:journals/toit/DengCZGY21,
	author = {Shuiguang Deng and
                  Guanjie Cheng and
                  Hailiang Zhao and
                  Honghao Gao and
                  Jianwei Yin},
	title = {Incentive-Driven Computation Offloading in Blockchain-Enabled E-Commerce},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {9:1--9:19},
	year = {2021},
	url = {https://doi.org/10.1145/3397160},
	doi = {10.1145/3397160},
	timestamp = {Thu, 14 Oct 2021 09:44:23 +0200},
	biburl = {https://dblp.org/rec/journals/toit/DengCZGY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain is regarded as one of the most promising technologies to upgrade e-commerce. This article analyzes the challenges that current e-commerce is facing and introduces a new scenario of e-commerce enabled by blockchain. A framework is proposed for mining tasks in this scenario offloaded onto edge servers based on mobile edge computing. Then, the offloading issue is modeled as a multi-constrained optimization problem, and evolutionary algorithms are utilized and re-designed as solvers. The experimental results validate the efficiency of the framework and algorithms and also show that the lower bound of computation resources exists to obtain the maximum overall revenue.}
}


@article{DBLP:journals/toit/ChenPLLXZ21,
	author = {Liang Chen and
                  Jiaying Peng and
                  Yang Liu and
                  Jintang Li and
                  Fenfang Xie and
                  Zibin Zheng},
	title = {Phishing Scams Detection in Ethereum Transaction Network},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {10:1--10:16},
	year = {2021},
	url = {https://doi.org/10.1145/3398071},
	doi = {10.1145/3398071},
	timestamp = {Sun, 05 Mar 2023 01:38:20 +0100},
	biburl = {https://dblp.org/rec/journals/toit/ChenPLLXZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain has attracted an increasing amount of researches, and there are lots of refreshing implementations in different fields. Cryptocurrency as its representative implementation, suffers the economic loss due to phishing scams. In our work, accounts and transactions are treated as nodes and edges, thus detection of phishing accounts can be modeled as a node classification problem. Correspondingly, we propose a detecting method based on Graph Convolutional Network and autoencoder to precisely distinguish phishing accounts. Experiments on different large-scale real-world datasets from Ethereum show that our proposed model consistently performs promising results compared with related methods.}
}


@article{DBLP:journals/toit/ChengLJLW21,
	author = {Lichen Cheng and
                  Jiqiang Liu and
                  Yi Jin and
                  Yidong Li and
                  Wei Wang},
	title = {Account Guarantee Scheme: Making Anonymous Accounts Supervised in
                  Blockchain},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {11:1--11:19},
	year = {2021},
	url = {https://doi.org/10.1145/3406092},
	doi = {10.1145/3406092},
	timestamp = {Sat, 08 Jan 2022 02:24:25 +0100},
	biburl = {https://dblp.org/rec/journals/toit/ChengLJLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In blockchain networks, reaching effective supervision while maintaining anonymity to the public has been an ongoing challenge. In existing solutions, certification authorities need to record all pairs of identities and pseudonyms, which is demanding and costly. This article proposed an account guarantee scheme to realize feasible supervision for existing anonymous blockchain networks with lower storage costs. Users are able to guarantee anonymous accounts with account guarantee key pairs generated from certificated polynomial functions, which inherently maintains one-to-n mapping certifications. Single or limited account guarantee key pairs do not leak privacy. Victims are able to request TCs to screen a cheater or disclose a cheater with enough fraud transactions by themselves. Detailed security and privacy analysis showed that the account guarantee scheme preserves user privacy and realizes feasible supervision. Experimental results demonstrated that the account guarantee scheme is efficient and practical.}
}


@article{DBLP:journals/toit/GuanWFLWW21,
	author = {Zhitao Guan and
                  Naiyu Wang and
                  Xunfeng Fan and
                  Xueyan Liu and
                  Longfei Wu and
                  Shaohua Wan},
	title = {Achieving Secure Search over Encrypted Data for e-Commerce: {A} Blockchain
                  Approach},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {12:1--12:17},
	year = {2021},
	url = {https://doi.org/10.1145/3408309},
	doi = {10.1145/3408309},
	timestamp = {Mon, 28 Aug 2023 21:41:18 +0200},
	biburl = {https://dblp.org/rec/journals/toit/GuanWFLWW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advances of Internet technology has resulted in the rapid and pervasive development of e-commerce, which has not only changed the production and operation mode of many enterprises, but also affected the economic development mode of the whole society. This trend has incurred a strong need to store and process large amounts of sensitive data. The traditional data storage and search solutions cannot meet such requirements. To tackle this problem, in this article, we proposed Consortium Blockchain-based Distributed Secure Search (CBDSS) Scheme over encrypted data in e-Commerce environment. By integrating the blockchain and searchable encryption model, sensitive data can be effectively protected. The consortium blockchain can ensure that only authorized nodes can join the system. To fairly assign nodes for the search tasks, we developed an endorsement strategy in which two agent roles are set up to divide and match the search tasks with the virtual resources according to the load capacity of each node. The security analysis and experiments are conducted to evaluate the performance of our proposed scheme. The evaluation results have proved the reliability and security of our scheme over existing methods.}
}


@article{DBLP:journals/toit/SongCZLFT21,
	author = {Qun Song and
                  Yuhao Chen and
                  Yan Zhong and
                  Kun Lan and
                  Simon Fong and
                  Rui Tang},
	title = {A Supply-chain System Framework Based on Internet of Things Using
                  Blockchain Technology},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {13:1--13:24},
	year = {2021},
	url = {https://doi.org/10.1145/3409798},
	doi = {10.1145/3409798},
	timestamp = {Fri, 24 Nov 2023 12:28:19 +0100},
	biburl = {https://dblp.org/rec/journals/toit/SongCZLFT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Numerous supply-chain combines with internet of things (IoT) applications have been proposed, and many methods and algorithms enhance the convenience of supply chains. However, new businesses still find it challenging to enter a supply chain, because unauthorised IoT devices of different companies illegally access resources. As security is paramount in a supply chain, IoT management has become very difficult. Public resources allocation and waste management also pose a problem. To solve the above problems, we proposed a new IoT management framework that embraces blockchain technology to help companies to form a supply chain effectively. This framework consists of an access control system, a backup peer mechanism and an internal data isolation and transmission approach. The access control system has a registrar module and an inspection module. The registrar module is mainly responsible for information registration with a registration policy, which has to be followed by all the companies in the supply chain. Besides, it provides a revocation and updating function. The inspection module focuses on judging misbehaviour and monitors the actions of the subjects; when any misoperation occurs, the system will correspondingly penalise violators. So that all related actions and information are verified and stored into blockchain, the IoT access control and safety of IoT admission are enhanced. Furthermore, in a blockchain system, if one single peer in the network breaks down, then the whole system may stop, because consensus cannot be reached. The data of the broken peer may be lost if it does not commit yet. The backup peer mechanism allows the primary peer and the backup peer to connect to an inspecting server for acquiring real-time data. The internal data isolation and transmission modules transmit and stores private data without creating a new subchannel. The proposed method is taken full account of the stability of the network and the fault tolerance to guarantee the robust of the system. To obtain unbiases results, experiments are conducted in two different blockchain environment. The results show our proposed method are promising IoT blockchain system for the supply chain.}
}


@article{DBLP:journals/toit/JeongKILS21,
	author = {Junho Jeong and
                  Donghyo Kim and
                  Sun{-}Young Ihm and
                  Yangsun Lee and
                  Yunsik Son},
	title = {Multilateral Personal Portfolio Authentication System Based on Hyperledger
                  Fabric},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {14:1--14:17},
	year = {2021},
	url = {https://doi.org/10.1145/3423554},
	doi = {10.1145/3423554},
	timestamp = {Sat, 30 Sep 2023 10:29:29 +0200},
	biburl = {https://dblp.org/rec/journals/toit/JeongKILS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Korean education-related evaluation agencies utilize a centralized system that directly manages learner data. This leaves the intellectual property of the organizations and the personal information of the students vulnerable to leakage should the central server be attacked. In this study, the researchers propose a multilateral personal portfolio authentication system that guarantees the reliability, integrity, and transparency of the data such as learner's schooling history. The system uses the features of blockchain in a distributed network wherein a learner submits schooling data to a peer in the network. The data is then verified through an agreement among the peers and recorded in a chronologically encrypted ledger. The proposed system is implemented based on Hyperledger Fabric and the analysis thereof conducted by evaluating its processing speed, capacity, and security. The analysis indicates that the system is able to successfully intercept the transactions of unvalidated users, thereby preventing the recording of incorrect data by an unauthorized user. Furthermore, it provides a record of previously made changes to a learner's profile, thus improving the integrity and reliability of the data. This system provides a platform to share learner information safely and promptly among schools, certification authorities, and higher learning institutions.}
}


@article{DBLP:journals/toit/SunXZSG21,
	author = {You Sun and
                  Rui Xue and
                  Rui Zhang and
                  Qianqian Su and
                  Sheng Gao},
	title = {RTChain: {A} Reputation System with Transaction and Consensus Incentives
                  for E-commerce Blockchain},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {15:1--15:24},
	year = {2021},
	url = {https://doi.org/10.1145/3430502},
	doi = {10.1145/3430502},
	timestamp = {Mon, 26 Jun 2023 20:52:11 +0200},
	biburl = {https://dblp.org/rec/journals/toit/SunXZSG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain technology, whose most successful application is Bitcoin, enables non-repudiation and non-tamperable online transactions without the participation of a trusted central party. As a global ledger, the blockchain achieves the consistency of replica stored on each node through a consensus mechanism. A well-designed consensus mechanism, on one hand, needs to be efficient to meet the high frequency of online transactions. For example, the existing electronic payment systems can handle over 50,000 transactions per second (TPS), while Bitcoin can only handle an average of about 3TPS. On the other hand, it needs to have good security and high fault tolerance; that is, in the case when some nodes are captured by adversaries, the network can still operate normally. In this article, we establish a reputation system, called RTChain, to be integrated into the e-commerce blockchain to achieve a distributed consensus and transaction incentives. The proposed scheme has the following advantages. First, an incentive mechanism is used to influence the consensus behavior of nodes and the transaction behavior of users, which in turn influence the reputation scores of both nodes and users. That is, when a node correctly processes a transaction, it will receive the corresponding reputation value as a reward, and the reputation value will be reduced as punishment not only when the node is dishonest and violates the consensus agreement but also the transaction is not completed as required. Just like electronic transactions in the real world, the higher the reputation of the user, the more likely it is to be selected as the transaction partner. A user with a low reputation will be gradually eliminated in our system because it is difficult to complete the transaction. Second, RTChain uses a verifiable random function to generate the leader in each round, which guarantees fairness for all participants and, unlike PoW, does not consume a large amount of computing resources. Then our consensus mechanism selects the nodes with high reputation scores to reduce the number of nodes participating in the consensus, thus improving the consensus efficiency, so that RTChain’s throughput can reach 4,000TPS. Third, we built a reputation chain to implement the distributed storage and management of reputation. Finally, our consensus mechanism is secure against existing attacks, such as flash attacks, selfish mining attacks, eclipse attacks, and double spending attacks, and allows nodes that participate in the consensus to fail, as long as the reputation of the failure node does not exceed one-third of the total reputation. We build a prototype of RTChain, and the experimental results show that RTChain is promising and deployable for e-commerce blockchains.}
}


@article{DBLP:journals/toit/YanPFY21,
	author = {Zheng Yan and
                  Li Peng and
                  Wei Feng and
                  Laurence T. Yang},
	title = {Social-Chain: Decentralized Trust Evaluation Based on Blockchain in
                  Pervasive Social Networking},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {17:1--17:28},
	year = {2021},
	url = {https://doi.org/10.1145/3419102},
	doi = {10.1145/3419102},
	timestamp = {Sat, 08 Jan 2022 02:24:23 +0100},
	biburl = {https://dblp.org/rec/journals/toit/YanPFY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pervasive Social Networking (PSN) supports online and instant social activities with the support of heterogeneous networks. Since reciprocal activities among both familiar/unfamiliar strangers and acquaintances are quite common in PSN, it is essential to offer trust information to PSN users. Past work normally evaluates trust based on a centralized party, which is not feasible due to the dynamic changes of PSN topology and its specific characteristics. The literature still lacks a decentralized trust evaluation scheme in PSN. In this article, we propose a novel blockchain-based decentralized system for trust evaluation in PSN, called Social-Chain. Considering mobile devices normally lack computing resources to process cryptographic puzzle calculation, we design a lightweight consensus mechanism based on Proof-of-Trust (PoT), which remarkably improves system effectivity compared with other blockchain systems. Serious security analysis and experimental results further illustrate the security and efficiency of Social-Chain for being feasibly applied into PSN.}
}


@article{DBLP:journals/toit/SinghTJSEM21,
	author = {Amit Kumar Singh and
                  Sriti Thakur and
                  Alireza Jolfaei and
                  Gautam Srivastava and
                  Mohamed Elhoseny and
                  Anand Mohan},
	title = {Joint Encryption and Compression-Based Watermarking Technique for
                  Security of Digital Documents},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {18:1--18:20},
	year = {2021},
	url = {https://doi.org/10.1145/3414474},
	doi = {10.1145/3414474},
	timestamp = {Mon, 28 Aug 2023 21:41:18 +0200},
	biburl = {https://dblp.org/rec/journals/toit/SinghTJSEM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, due to the increase in popularity of the Internet, the problem of digital data security over the Internet is increasing at a phenomenal rate. Watermarking is used for various notable applications to secure digital data from unauthorized individuals. To achieve this, in this article, we propose a joint encryption then-compression based watermarking technique for digital document security. This technique offers a tool for confidentiality, copyright protection, and strong compression performance of the system. The proposed method involves three major steps as follows: (1) embedding of multiple watermarks through non-sub-sampled contourlet transform, redundant discrete wavelet transform, and singular value decomposition; (2) encryption and compression via SHA-256 and Lempel Ziv Welch (LZW), respectively; and (3) extraction/recovery of multiple watermarks from the possibly distorted cover image. The performance estimations are carried out on various images at different attacks, and the efficiency of the system is determined in terms of peak signal-to-noise ratio (PSNR) and normalized correlation (NC), structural similarity index measure (SSIM), number of changing pixel rate (NPCR), unified averaged changed intensity (UACI), and compression ratio (CR). Furthermore, the comparative analysis of the proposed system with similar schemes indicates its superiority to them.}
}


@article{DBLP:journals/toit/PengCVKH21,
	author = {Cong Peng and
                  Jianhua Chen and
                  Pandi Vijayakumar and
                  Neeraj Kumar and
                  Debiao He},
	title = {Efficient Distributed Decryption Scheme for IoT Gateway-based Applications},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {19:1--19:23},
	year = {2021},
	url = {https://doi.org/10.1145/3414475},
	doi = {10.1145/3414475},
	timestamp = {Thu, 06 Jul 2023 22:32:33 +0200},
	biburl = {https://dblp.org/rec/journals/toit/PengCVKH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the evolvement of the Internet of things (IoT), privacy and security have become the primary indicators for users to deploy IoT applications. In the gateway-based IoT architecture, gateways aggregate data collected by perception-layer devices and upload message packets to platforms, while platforms automatically push different categories of data to different applications. However, security in processes of data transmission via gateways, storage in platforms, access by applications is the major challenge for user privacy protection. To tackle this challenge, this article presents a secure IoT scheme based on a fine-grained multi-receive signcryption scheme to realize end-to-end secure transmission and data access control. To enhance the security of online application decryption keys, we design a distributed threshold decryption scheme based on secret-sharing. Moreover, from the provable security perspective, we demonstrate that the scheme can achieve the expected IND-CCA security and EUF-CMA security. After the performance analysis, evaluation results show that the computational performance is efficient and linearly subject to the number of messages and the number of receivers.}
}


@article{DBLP:journals/toit/ChichaBNCHOBA21,
	author = {Elie Chicha and
                  Bechara al Bouna and
                  Mohamed Nassar and
                  Richard Chbeir and
                  Ramzi A. Haraty and
                  Mourad Oussalah and
                  Djamal Benslimane and
                  Mansour Naser Alraja},
	title = {A User-Centric Mechanism for Sequentially Releasing Graph Datasets
                  under Blowfish Privacy},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {20:1--20:25},
	year = {2021},
	url = {https://doi.org/10.1145/3431501},
	doi = {10.1145/3431501},
	timestamp = {Thu, 14 Oct 2021 09:44:23 +0200},
	biburl = {https://dblp.org/rec/journals/toit/ChichaBNCHOBA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we present a privacy-preserving technique for user-centric multi-release graphs. Our technique consists of sequentially releasing anonymized versions of these graphs under Blowfish Privacy. To do so, we introduce a graph model that is augmented with a time dimension and sampled at discrete time steps. We show that the direct application of state-of-the-art privacy-preserving Differential Private techniques is weak against background knowledge attacker models. We present different scenarios where randomizing separate releases independently is vulnerable to correlation attacks. Our method is inspired by Differential Privacy (DP) and its extension Blowfish Privacy (BP). To validate it, we show its effectiveness as well as its utility by experimental simulations.}
}


@article{DBLP:journals/toit/CanE21,
	author = {Yekta Said Can and
                  Cem Ersoy},
	title = {Privacy-preserving Federated Deep Learning for Wearable IoT-based
                  Biomedical Monitoring},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {21:1--21:17},
	year = {2021},
	url = {https://doi.org/10.1145/3428152},
	doi = {10.1145/3428152},
	timestamp = {Sat, 09 Apr 2022 12:33:23 +0200},
	biburl = {https://dblp.org/rec/journals/toit/CanE21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IoT devices generate massive amounts of biomedical data with increased digitalization and development of the state-of-the-art automated clinical data collection systems. When combined with advanced machine learning algorithms, the big data could be useful to improve the health systems for decision-making, diagnosis, and treatment. Mental healthcare is also attracting attention, since most medical problems can be associated with mental states. Affective computing is among the emerging biomedical informatics fields for automatically monitoring a person’s mental state in ambulatory environments by using physiological and physical signals. However, although affective computing applications are promising to improve our daily lives, before analyzing physiological signals, privacy issues and concerns need to be dealt with. Federated learning is a promising candidate for developing high-performance models while preserving the privacy of individuals. It is a privacy protection solution that stores model parameters instead of the data itself and abides by the data protection laws such as EU General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). We applied federated learning to heart activity data collected with smart bands for stress-level monitoring in different events. We achieved encouraging results for using federated learning in IoT-based wearable biomedical monitoring systems by preserving the privacy of the data.}
}


@article{DBLP:journals/toit/HouranyHFMPB21,
	author = {Edy Hourany and
                  Bachir Habib and
                  Camille Fountaine and
                  Abdallah Makhoul and
                  Beno{\^{\i}}t Piranda and
                  Julien Bourgeois},
	title = {{PROLISEAN:} {A} New Security Protocol for Programmable Matter},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {22:1--22:29},
	year = {2021},
	url = {https://doi.org/10.1145/3432250},
	doi = {10.1145/3432250},
	timestamp = {Fri, 21 Jan 2022 22:02:34 +0100},
	biburl = {https://dblp.org/rec/journals/toit/HouranyHFMPB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The vision for programmable matter is to create a material that can be reprogrammed to have different shapes and to change its physical properties on demand. They are autonomous systems composed of a huge number of independent connected elements called particles. The connections to one another form the overall shape of the system. These particles are capable of interacting with each other and take decisions based on their environment. Beyond sensing, processing, and communication capabilities, programmable matter includes actuation and motion capabilities. It could be deployed in different domains and will constitute an intelligent component of the IoT. A lot of applications can derive from this technology, such as medical or industrial applications. However, just like any other technology, security is a huge concern. Given its distributed architecture and its processing limitations, programmable matter cannot handle the traditional security protocols and encryption algorithms. This article proposes a new security protocol optimized and dedicated for IoT programmable matter. This protocol is based on lightweight cryptography and uses the same encryption protocol as a hashing function while keeping the distributed architecture in mind. The analysis and simulation results show the efficiency of the proposed method and that a supercomputer will need about 5.93 × 1025 years to decrypt the message.}
}


@article{DBLP:journals/toit/LoukilGBBB21,
	author = {Faiza Loukil and
                  Chirine Ghedira Guegan and
                  Khouloud Boukadi and
                  A{\"{\i}}cha{-}Nabila Benharkat and
                  Elhadj Benkhelifa},
	title = {Data Privacy Based on IoT Device Behavior Control Using Blockchain},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {23:1--23:20},
	year = {2021},
	url = {https://doi.org/10.1145/3434776},
	doi = {10.1145/3434776},
	timestamp = {Tue, 07 May 2024 20:27:47 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LoukilGBBB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) is expected to improve the individuals’ quality of life. However, ensuring security and privacy in the IoT context is a non-trivial task due to the low capability of these connected devices. Generally, the IoT device management is based on a centralized entity that validates communication and connection rights. Therefore, this centralized entity can be considered as a single point of failure. Yet, in the case of distributed approaches, it is difficult to delegate the right validation to IoT devices themselves in untrustworthy IoT environments. Fortunately, the blockchain may provide decentralization of overcoming the trust problem while designing a privacy-preserving system. To this end, we propose a novel privacy-preserving IoT device management framework based on the blockchain technology. In the proposed system, the IoT devices are controlled by several smart contracts that validate the connection rights according to the privacy permission settings predefined by the data owners and the stored record array of detected misbehavior of each IoT device. In fact, smart contracts can immediately detect the devices that have vulnerabilities and have been hacked or pose a threat to the IoT network. Therefore, the data owner’s privacy is preserved by enforcing the control over the own devices. For validation purposes, we deploy the proposed solution on a private Ethereum blockchain and give the performance evaluation.}
}


@article{DBLP:journals/toit/HuLLS21,
	author = {Kaixi Hu and
                  Lin Li and
                  Jianquan Liu and
                  Daniel Sun},
	title = {DuroNet: {A} Dual-robust Enhanced Spatial-temporal Learning Network
                  for Urban Crime Prediction},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {24:1--24:24},
	year = {2021},
	url = {https://doi.org/10.1145/3432249},
	doi = {10.1145/3432249},
	timestamp = {Sat, 08 Jan 2022 02:24:24 +0100},
	biburl = {https://dblp.org/rec/journals/toit/HuLLS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Urban crime is an ongoing problem in metropolitan development and attracts general concern from the international community. As an effective means of defending urban safety, crime prediction plays a crucial role in patrol force allocation and public safety. However, urban crime data is a macro result of crime patterns overlapped by various irrelevant factors that cause inhomogeneous noises—local outliers and irregular waves. These noises might obstruct the learning process of crime prediction models and result in a deviation of performance. To tackle the problem, we propose a novel paradigm of <underline>Du</underline>al-<underline>ro</underline>bust Enhanced Spatial-temporal Learning <underline>Net</underline>work\xa0(DuroNet), an encoder-decoder architecture that possesses an adaptive robustness for reducing the effect of outliers and waves. The robustness is mainly reflected on two aspects. One is a locality enhanced module that employs local temporal context information to smooth the deviation of outliers and dynamic spatial information to assist in understanding normal points. The other is a self-attention-based pattern representation module to weaken the effect of irregular waves by learning attentive weights. Finally, extensive experiments are conducted on two real-world crime datasets before and after adding Gaussian noises. The results demonstrate the superior performance of our DuroNet over the state-of-the-art methods.}
}


@article{DBLP:journals/toit/MehtaGBPN21,
	author = {Vikram Mehta and
                  Daniel Gooch and
                  Arosha K. Bandara and
                  Blaine A. Price and
                  Bashar Nuseibeh},
	title = {Privacy Care: {A} Tangible Interaction Framework for Privacy Management},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {25:1--25:32},
	year = {2021},
	url = {https://doi.org/10.1145/3430506},
	doi = {10.1145/3430506},
	timestamp = {Wed, 19 May 2021 08:30:50 +0200},
	biburl = {https://dblp.org/rec/journals/toit/MehtaGBPN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of ubiquitous computing (UbiComp) environments has increased the risk of undesired access to individuals’ physical space or their information, anytime and anywhere, raising potentially serious privacy concerns. Individuals lack awareness and control of the vulnerabilities in everyday contexts and need support and care in regulating disclosures to their physical and digital selves. Existing GUI-based solutions, however, often feel physically interruptive, socially disruptive, time-consuming and cumbersome. To address such challenges, we investigate the user interaction experience and discuss the need for more tangible and embodied interactions for effective and seamless natural privacy management in everyday UbiComp settings. We propose the Privacy Care interaction framework, which is rooted in the literature of privacy management and tangible computing. Keeping users at the center, Awareness and Control are established as the core parts of our framework. This is supported with three interrelated interaction tenets: Direct, Ready-to-Hand, and Contextual. Direct refers to intuitiveness through metaphor usage. Ready-to-Hand supports granularity, non-intrusiveness, and ad hoc management, through periphery-to-center style attention transitions. Contextual supports customization through modularity and configurability. Together, they aim to provide experience of an embodied privacy care with varied interactions that are calming and yet actively empowering. The framework provides designers of such care with a basis to refer to, to generate effective tangible tools for privacy management in everyday settings. Through five semi-structured focus groups, we explore the privacy challenges faced by a sample set of 15 older adults (aged 60+) across their cyber-physical-social spaces. The results show conformity to our framework, demonstrating the relevance of the facets of the framework to the design of privacy management tools in everyday UbiComp contexts.}
}


@article{DBLP:journals/toit/SharmaDB21,
	author = {Tanusree Sharma and
                  Hunter A. Dyer and
                  Masooda N. Bashir},
	title = {Enabling User-centered Privacy Controls for Mobile Applications: {COVID-19}
                  Perspective},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {1},
	pages = {26:1--26:24},
	year = {2021},
	url = {https://doi.org/10.1145/3434777},
	doi = {10.1145/3434777},
	timestamp = {Tue, 07 May 2024 20:27:47 +0200},
	biburl = {https://dblp.org/rec/journals/toit/SharmaDB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile apps have transformed many aspects of clinical practice and are becoming a commonplace in healthcare settings. The recent COVID-19 pandemic has provided the opportunity for such apps to play an important role in reducing the spread of the virus. Several types of COVID-19 apps have enabled healthcare professionals and governments to communicate with the public regarding the pandemic spread, coronavirus awareness, and self-quarantine measures. While these apps provide immense benefits for the containment of the spread, privacy and security of these digital tracing apps are at the center of public debate. To address this gap, we conducted an online survey of a midwestern region in the United State to assess people’s attitudes toward such apps and to examine their privacy and security concerns and preferences. Survey results from 1,550 participants indicate that privacy/security protections and trust play a vital role in people’s adoption of such apps. Furthermore, results reflect users’ preferences wanting to have control over their personal information and transparency on how their data is handled. In addition, personal data protection priorities selected by the participants were surprising and yet revealing of the disconnect between technologists and users. In this article, we present our detailed survey results as well as design guidelines for app developers to develop innovative human-centered technologies that are not only functional but also respectful of social norms and protections of civil liberties. Our study examines users’ preferences for COVID-19 apps and integrates important factors of trust, willingness, and preferences in the context of app development. Through our research findings, we suggest mechanisms for designing inclusive apps’ privacy and security measures that can be put into practice for healthcare-related apps, so that timely adoption is made possible.}
}


@article{DBLP:journals/toit/LiuWMZMLH21,
	author = {Xuanzhe Liu and
                  Shangguang Wang and
                  Yun Ma and
                  Ying Zhang and
                  Qiaozhu Mei and
                  Yunxin Liu and
                  Gang Huang},
	title = {Operating Systems for Resource-adaptive Intelligent Software: Challenges
                  and Opportunities},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {27:1--27:19},
	year = {2021},
	url = {https://doi.org/10.1145/3425866},
	doi = {10.1145/3425866},
	timestamp = {Sat, 09 Apr 2022 12:33:23 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LiuWMZMLH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The past decades witnessed the fast and wide deployment of Internet. The Internet has bred the ubiquitous computing environment that is spanning the cloud, edge, mobile devices, and IoT. Software running over such a ubiquitous computing environment environment is eating the world. A recently emerging trend of Internet-based software systems is “resource adaptive,” i.e., software systems should be robust and intelligent enough to the changes of heterogeneous resources, both physical and logical, provided by their running environment. To keep pace of such a trend, we argue that some considerations should be taken into account for the future operating system design and implementation. From the structural perspective, rather than the “monolithic OS” that manages the aggregated resources on the single machine, the OS should be dynamically composed over the distributed resources and flexibly adapt to the resource and environment changes. Meanwhile, the OS should leverage advanced machine/deep learning techniques to derive configurations and policies and automatically learn to tune itself and schedule resources. This article envisions our recent thinking of the new OS abstraction, namely, ServiceOS, for future resource-adaptive intelligent software systems. The idea of ServiceOS is inspired by the delivery model of “Software-as-a-Service” that is supported by the Service-Oriented Architecture (SOA). The key principle of ServiceOS is based on resource disaggregation, resource provisioning as a service, and learning-based resource scheduling and allocation. The major goal of this article is not providing an immediately deployable OS. Instead, we aim to summarize the challenges and potentially promising opportunities and try to provide some practical implications for researchers and practitioners.}
}


@article{DBLP:journals/toit/LvS21,
	author = {Zhihan Lv and
                  Amit Kumar Singh},
	title = {Big Data Analysis of Internet of Things System},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {28:1--28:15},
	year = {2021},
	url = {https://doi.org/10.1145/3389250},
	doi = {10.1145/3389250},
	timestamp = {Mon, 20 Sep 2021 09:50:45 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LvS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The study aims at exploring the Internet of things (IoT) system from the perspective of data and further improving the performance of the IoT system. The IoT data energy collection and information transmission system model is constructed by combining IoT and wireless relay cooperative transmission technology. Moreover, the energy efficiency, outage probability (OP), and accuracy of the model are evaluated by simulation experiments. The results show that, in the energy efficiency analysis, with the increase of power split factor ρ, the information transmission ability of the system increases. Whereas, the energy collection ability decreases, so the energy efficiency is reduced. Thus, choosing a more suitable power split factor for the energy efficiency of IoT is important. By analyzing OP and bit error rate (BER), as the values of m (Nakagami, the fading index of the fading distribution) and multi-hop paths increase, the OP and BER are reduced while the system performance is increased. Therefore, this article uses wireless relay cooperative transmission technology to integrate big data analysis into the IoT system. Finally, by adding multi-hop path and other methods to reduce the OP and BER of system, the system performance is improved. It provides experimental basis for the development of IoT systems.}
}


@article{DBLP:journals/toit/TanveerSM21,
	author = {Muhammad Tanveer and
                  S. Sharma and
                  Khan Muhammad},
	title = {Large-Scale Least Squares Twin SVMs},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {29:1--29:19},
	year = {2021},
	url = {https://doi.org/10.1145/3398379},
	doi = {10.1145/3398379},
	timestamp = {Sat, 30 Sep 2023 10:29:30 +0200},
	biburl = {https://dblp.org/rec/journals/toit/TanveerSM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the last decade, twin support vector machine (TWSVM) classifiers have achieved considerable emphasis on pattern classification tasks. However, the TWSVM formulation still suffers from the following two shortcomings: (1) TWSVM deals with the inverse matrix calculation in the Wolfe-dual problems, which is intractable for large-scale datasets with numerous features and samples, and (2) TWSVM minimizes the empirical risk instead of the structural risk in its formulation. With the advent of huge amounts of data today, these disadvantages render TWSVM an ineffective choice for pattern classification tasks. In this article, we propose an efficient large-scale least squares twin support vector machine (LS-LSTSVM) for pattern classification that rectifies all the aforementioned shortcomings. The proposed LS-LSTSVM introduces different Lagrangian functions to eliminate the need for calculating inverse matrices. The proposed LS-LSTSVM also does not employ kernel-generated surfaces for the non-linear case, and thus uses the kernel trick directly. This ensures that the proposed LS-LSTSVM model is superior to the original TWSVM and LSTSVM. Lastly, the structural risk is minimized in LS-LSTSVM. This exhibits the essence of statistical learning theory, and consequently, classification accuracy on datasets can be improved due to this change. The proposed LS-LSTSVM is solved using the sequential minimal optimization (SMO) technique, making it more suitable for large-scale problems. We further proved the convergence of the proposed LS-LSTSVM. Exhaustive experiments on several real-world benchmarks and NDC-based large-scale datasets demonstrate that the proposed LS-LSTSVM is feasible for large datasets and, in most cases, performed better than existing algorithms.}
}


@article{DBLP:journals/toit/SavaglioF21,
	author = {Claudio Savaglio and
                  Giancarlo Fortino},
	title = {A Simulation-driven Methodology for IoT Data Mining Based on Edge
                  Computing},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {30:1--30:22},
	year = {2021},
	url = {https://doi.org/10.1145/3402444},
	doi = {10.1145/3402444},
	timestamp = {Mon, 03 Jan 2022 22:13:07 +0100},
	biburl = {https://dblp.org/rec/journals/toit/SavaglioF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the ever-increasing diffusion of smart devices and Internet of Things (IoT) applications, a completely new set of challenges have been added to the Data Mining domain. Edge Mining and Cloud Mining refer to Data Mining tasks aimed at IoT scenarios and performed according to, respectively, Cloud or Edge computing principles. Given the orthogonality and interdependence among the Data Mining task goals (e.g., accuracy, support, precision), the requirements of IoT applications (mainly bandwidth, energy saving, responsiveness, privacy preserving, and security) and the features of Edge/Cloud deployments (de-centralization, reliability, and ease of management), we propose EdgeMiningSim, a simulation-driven methodology inspired by software engineering principles for enabling IoT Data Mining. Such a methodology drives the domain experts in disclosing actionable knowledge, namely descriptive or predictive models for taking effective actions in the constrained and dynamic IoT scenario. A Smart Monitoring application is instantiated as a case study, aiming to exemplify the EdgeMiningSim approach and to show its benefits in effectively facing all those multifaceted aspects that simultaneously impact on IoT Data Mining.}
}


@article{DBLP:journals/toit/KaurGKK21,
	author = {Kuljeet Kaur and
                  Sahil Garg and
                  Georges Kaddoum and
                  Neeraj Kumar},
	title = {Energy and SLA-driven MapReduce Job Scheduling Framework for Cloud-based
                  Cyber-Physical Systems},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {31:1--31:24},
	year = {2021},
	url = {https://doi.org/10.1145/3409772},
	doi = {10.1145/3409772},
	timestamp = {Sun, 25 Jul 2021 11:43:28 +0200},
	biburl = {https://dblp.org/rec/journals/toit/KaurGKK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Energy consumption minimization of cloud data centers (DCs) has attracted much attention from the research community in the recent years; particularly due to the increasing dependence of emerging Cyber-Physical Systems on them. An effective way to improve the energy efficiency of DCs is by using efficient job scheduling strategies. However, the most challenging issue in selection of efficient job scheduling strategy is to ensure service-level agreement (SLA) bindings of the scheduled tasks. Hence, an energy-aware and SLA-driven job scheduling framework based on MapReduce is presented in this article. The primary aim of the proposed framework is to explore task-to-slot/container mapping problem as a special case of energy-aware scheduling in deadline-constrained scenario. Thus, this problem can be viewed as a complex multi-objective problem comprised of different constraints. To address this problem efficiently, it is segregated into three major subproblems (SPs), namely, deadline segregation, map and reduce phase energy-aware scheduling. These SPs are individually formulated using Integer Linear Programming. To solve these SPs effectively, heuristics based on Greedy strategy along with classical Hungarian algorithm for serial and serial-parallel systems are used. Moreover, the proposed scheme also explores the potential of splitting Map/Reduce phase(s) into multiple stages to achieve higher energy reductions. This is achieved by leveraging the concepts of classical Greedy approach and priority queues. The proposed scheme has been validated using real-time data traces acquired from OpenCloud. Moreover, the performance of the proposed scheme is compared with the existing schemes using different evaluation metrics, namely, number of stages, total energy consumption, total makespan, and SLA violated. The results obtained prove the efficacy of the proposed scheme in comparison to the other schemes under different workload scenarios.}
}


@article{DBLP:journals/toit/TsaiF21,
	author = {Chun{-}Wei Tsai and
                  Zhi{-}Yan Fang},
	title = {An Effective Hyperparameter Optimization Algorithm for {DNN} to Predict
                  Passengers at a Metro Station},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {32:1--32:24},
	year = {2021},
	url = {https://doi.org/10.1145/3410156},
	doi = {10.1145/3410156},
	timestamp = {Mon, 20 Sep 2021 09:50:45 +0200},
	biburl = {https://dblp.org/rec/journals/toit/TsaiF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the public transportation systems, metro is certainly an indispensable part in urban areas of a metropolis today. Several successful results have shown that deep learning technologies might provide an effective way to predict the number of passengers at a metro station. However, most information systems based on deep learning technologies are usually designed and tuned manually by using domain knowledge and trial-and-error; thus, how to find out a set of suitable hyperparameters for a deep neural network (DNN) has become a critical research issue. To deal with the problem of hyperparameter setting for a DNN in solving the prediction of passengers at a metro station, a novel metaheuristic algorithm called search economics for hyperparameter optimization is presented to improve the accuracy rate of such a prediction system. The basic idea of the proposed algorithm is to divide the solution space into a set of subspaces first and then assign a different number of search agents to each subspace based on the “potential of each subspace.” The potential is estimated based on the objective values of the searched solutions, the objective values of the probe solutions, and the computation time invested in each subspace. The proposed method is compared with Bayesian, random forest, support vector regression, DNN, and DNN with different hyperparameter search algorithms, namely, grid search, simulated annealing, and particle swarm optimization. The simulation results using the data provided by the government of Taipei city, Taiwan, indicate that the proposed method outperforms all the other forecasting methods compared in this article in terms of the mean absolute percentage error.}
}


@article{DBLP:journals/toit/NiLLC21,
	author = {Pin Ni and
                  Yuming Li and
                  Gangmin Li and
                  Victor I. Chang},
	title = {A Hybrid Siamese Neural Network for Natural Language Inference in
                  Cyber-Physical Systems},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {33:1--33:25},
	year = {2021},
	url = {https://doi.org/10.1145/3418208},
	doi = {10.1145/3418208},
	timestamp = {Mon, 26 Jun 2023 20:52:11 +0200},
	biburl = {https://dblp.org/rec/journals/toit/NiLLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyber-Physical Systems (CPS), as a multi-dimensional complex system that connects the physical world and the cyber world, has a strong demand for processing large amounts of heterogeneous data. These tasks also include Natural Language Inference (NLI) tasks based on text from different sources. However, the current research on natural language processing in CPS does not involve exploration in this field. Therefore, this study proposes a Siamese Network structure that combines Stacked Residual Long Short-Term Memory (bidirectional) with the Attention mechanism and Capsule Network for the NLI module in CPS, which is used to infer the relationship between text/language data from different sources. This model is mainly used to implement NLI tasks and conduct a detailed evaluation in three main NLI benchmarks as the basic semantic understanding module in CPS. Comparative experiments prove that the proposed method achieves competitive performance, has a certain generalization ability, and can balance the performance and the number of trained parameters.}
}


@article{DBLP:journals/toit/ZhangTLYY21,
	author = {Chen Zhang and
                  Zhuo Tang and
                  Kenli Li and
                  Jianzhong Yang and
                  Li Yang},
	title = {A Polishing Robot Force Control System Based on Time Series Data in
                  Industrial Internet of Things},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {34:1--34:22},
	year = {2021},
	url = {https://doi.org/10.1145/3419469},
	doi = {10.1145/3419469},
	timestamp = {Thu, 14 Dec 2023 07:56:42 +0100},
	biburl = {https://dblp.org/rec/journals/toit/ZhangTLYY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Installing a six-dimensional force/torque sensor on an industrial arm for force feedback is a common robotic force control strategy. However, because of the high price of force/torque sensors and the closedness of an industrial robot control system, this method is not convenient for industrial mass production applications. Various types of data generated by industrial robots during the polishing process can be saved, transmitted, and applied, benefiting from the growth of the industrial internet of things (IIoT). Therefore, we propose a constant force control system that combines an industrial robot control system and industrial robot offline programming software for a polishing robot based on IIoT time series data. The system mainly consists of four parts, which can achieve constant force polishing of industrial robots in mass production. (1) Data collection module. Install a six-dimensional force/torque sensor at a manipulator and collect the robot data (current series data, etc.) and sensor data (force/torque series data). (2) Data analysis module. Establish a relationship model based on variant long short-term memory which we propose between current time series data of the polishing manipulator and data of the force sensor. (3) Data prediction module. A large number of sensorless polishing robots of the same type can utilize that model to predict force time series. (4) Trajectory optimization module. The polishing trajectories can be adjusted according to the prediction sequences. The experiments verified that the relational model we proposed has an accurate prediction, small error, and a manipulator taking advantage of this method has a better polishing effect.}
}


@article{DBLP:journals/toit/YinXLCGL21,
	author = {Yuyu Yin and
                  Haoran Xu and
                  Tingting Liang and
                  Manman Chen and
                  Honghao Gao and
                  Antonella Longo},
	title = {Leveraging Data Augmentation for Service QoS Prediction in Cyber-physical
                  Systems},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {35:1--35:25},
	year = {2021},
	url = {https://doi.org/10.1145/3425795},
	doi = {10.1145/3425795},
	timestamp = {Wed, 07 Dec 2022 23:03:34 +0100},
	biburl = {https://dblp.org/rec/journals/toit/YinXLCGL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the fast-developing domain of cyber-physical systems (CPS), constructing the CPS with high-quality services becomes an imperative task. As one of the effective solutions for information overload in CPS construction, quality of service (QoS)-aware service recommendation has drawn much attention in academia and industry. However, the lack of most QoS values limits the recommendation performance and it is time-consuming for users to get the QoS values by invoking all the services. Therefore, a powerful prediction model is required to predict the unobserved QoS values. Considering the fact that most existing QoS prediction models are unable to effectively address the data-sparsity problem, a novel two-stage framework called AgQ is proposed for QoS prediction. Specifically, a data augmentation strategy is designed in the first stage to enlarge the training set by drawing additional virtual instances. In the second stage, a prediction model is applied that considers both virtual and factual instances during the training procedure. We conduct extensive experiments on the WSDream dataset to demonstrate the effectiveness of the our QoS prediction framework and verify that the data augmentation strategy can indeed alleviate the data-sparsity problem. In terms of mean absolute error, taking the Multilayer Perceptron model as an example, the maximum improvement achieves 5% under 5% sparsity.}
}


@article{DBLP:journals/toit/GarrigaATTH21,
	author = {Martin Garriga and
                  Koen Aarns and
                  Christos Tsigkanos and
                  Damian A. Tamburri and
                  Willem{-}Jan van den Heuvel},
	title = {DataOps for Cyber-Physical Systems Governance: The Airport Passenger
                  Flow Case},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {36:1--36:25},
	year = {2021},
	url = {https://doi.org/10.1145/3432247},
	doi = {10.1145/3432247},
	timestamp = {Fri, 13 Aug 2021 14:56:50 +0200},
	biburl = {https://dblp.org/rec/journals/toit/GarrigaATTH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advancements in information technology have ushered a new wave of systems integrating Internet technology with sensing, wireless communication, and computational resources over existing infrastructures. As a result, myriad complex, non-traditional Cyber-Physical Systems (CPS) have emerged, characterized by interaction among people, physical facilities, and embedded sensors and computers, all generating vast amounts of complex data. Such a case is encountered within a contemporary airport hall setting: passengers roaming, information systems governing various functions, and data being generated and processed by cameras, phones, sensors, and other Internet of Things technology. This setting has considerable potential of contributing to goals entertained by the CPS operators, such as airlines, airport operators/owners, technicians, users, and more. We model the airport setting as an instance of such a complex, data-intensive CPS where multiple actors and data sources interact, and generalize a methodology to support it and other similar systems. Furthermore, this article instantiates the methodology and pipeline for predictive analytics for passenger flow, as a characteristic manifestation of such systems requiring a tailored approach. Our methodology also draws from DataOps principles, using multi-modal and real-life data to predict the underlying distribution of the passenger flow on a flight-level basis (improving existing day-level predictions), anticipating when and how the passengers enter the airport and move through the check-in and baggage drop-off process. This allows to plan airport resources more efficiently while improving customer experience by avoiding passenger clumping at check-in and security. We demonstrate results obtained over a case from a major international airport in the Netherlands, improving up to 60% upon predictions of daily passenger flow currently in place.}
}


@article{DBLP:journals/toit/WeissVTK21,
	author = {Iris Wei{\ss} and
                  Birgit Vogel{-}Heuser and
                  Emanuel Trunzer and
                  Simon Kruppa},
	title = {Product Quality Monitoring in Hydraulic Presses Using a Minimal Sample
                  of Sensor and Actuator Data},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {37:1--37:23},
	year = {2021},
	url = {https://doi.org/10.1145/3436238},
	doi = {10.1145/3436238},
	timestamp = {Wed, 14 Jul 2021 15:56:24 +0200},
	biburl = {https://dblp.org/rec/journals/toit/WeissVTK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning and artificial intelligence provide methods and algorithms to take advantage of sensor and actuator data in automated production systems. Product quality monitoring is one of the promising applications available for data-driven modeling, particularly in cases where the quality parameters cannot be measured with reasonable effort. This is the case for defects such as cracks in workpieces of hydraulic metal powder presses. However, the variety of shapes produced at a powder press requires training of individual models based on a minimal sample size of unlabeled data to adapt to changing settings. Therefore, this article proposes an unsupervised product quality monitoring approach based on dynamic time warping and non-linear regression to detect anomalies in unlabeled sensor and actuator data. A preprocessing step that isolates only the relevant intervals of the process is further introduced, facilitating efficient product quality monitoring. The evaluation on an industrial dataset with 37 samples, generated in test runs, shows a true-positive rate for detected product quality defects of 100% while preserving an acceptable accuracy. Moreover, the approach achieves the output within less than 10 seconds, assuring that the result is available before the next workpiece is processed. In this way, efficient product quality management is possible, reducing time- and cost-intensive quality inspections.}
}


@article{DBLP:journals/toit/CuiXWCPDQ21,
	author = {Laizhong Cui and
                  Zhe Xiao and
                  Jiahao Wang and
                  Fei Chen and
                  Yi Pan and
                  Hua Dai and
                  Jing Qin},
	title = {Improving Vaccine Safety Using Blockchain},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {38:1--38:24},
	year = {2021},
	url = {https://doi.org/10.1145/3388446},
	doi = {10.1145/3388446},
	timestamp = {Sat, 09 Apr 2022 12:33:23 +0200},
	biburl = {https://dblp.org/rec/journals/toit/CuiXWCPDQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, vaccine incidents occurred around the world, which endangers people’s lives. In the technical respect, these incidents are partially due to the fact that existing vaccine management systems are distributively managed by different entities in the vaccine supply chain. This architecture makes it relatively easy to modify or even delete the vaccine circulation data maliciously, which makes tracing problematic vaccine hard and identifying the responsibility for a vaccine accident hard. To solve these issues, this article presents a blockchain-based solution to protect the whole process of vaccine circulation. We first propose a model to supervise the vaccine circulation process by incorporating existing regulatory practices. Then, we propose a blockchain-based tracing system to implement this model. The proposed system takes the blockchain as a global, unique, and verifiable database to store all the circulation data. Through data insertions and queries on the global and unique database, the proposed system achieves the protection of vaccine circulation. We also implement a proof-of-concept prototype of the proposed system. Experimental results confirm that the proposed system is beneficial.}
}


@article{DBLP:journals/toit/NguyenT21,
	author = {Truc D. T. Nguyen and
                  My T. Thai},
	title = {A Blockchain-based Iterative Double Auction Protocol Using Multiparty
                  State Channels},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {39:1--39:22},
	year = {2021},
	url = {https://doi.org/10.1145/3389249},
	doi = {10.1145/3389249},
	timestamp = {Mon, 05 Feb 2024 20:21:53 +0100},
	biburl = {https://dblp.org/rec/journals/toit/NguyenT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although the iterative double auction has been widely used in many different applications, one of the major problems in its current implementations is that they rely on a trusted third party to handle the auction process. This imposes the risk of single point of failures, monopoly, and bribery. In this article, we aim to tackle this problem by proposing a novel decentralized and trustless framework for iterative double auction based on blockchain. Our design adopts the smart contract and state channel technologies to enable a double auction process among parties that do not need to trust each other, while minimizing the blockchain transactions. In specific, we propose an extension to the original concept of state channels that can support multiparty computation. Then, we provide a formal development of the proposed framework and prove the security of our design against adversaries. Finally, we develop a proof-of-concept implementation of our framework using Elixir and Solidity, on which we conduct various experiments to demonstrate its feasibility and practicality.}
}


@article{DBLP:journals/toit/LinHZHL21,
	author = {Chao Lin and
                  Debiao He and
                  Sherali Zeadally and
                  Xinyi Huang and
                  Zhe Liu},
	title = {Blockchain-based Data Sharing System for Sensing-as-a-Service in Smart
                  Cities},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {40:1--40:21},
	year = {2021},
	url = {https://doi.org/10.1145/3397202},
	doi = {10.1145/3397202},
	timestamp = {Thu, 24 Feb 2022 11:50:55 +0100},
	biburl = {https://dblp.org/rec/journals/toit/LinHZHL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The sensing-as-a-service (SaaS) model has been explored to address the challenge of intractability of managing a large number of sensors faced by future smart cities. However, how to effectively share sensor data without compromising confidentiality, privacy protection, and fair trading without third parties is one of critical issues that must be solved in the SaaS in smart cities. While blockchain shows promise in solving these issues, the existing blockchain-based data sharing (BBDS) systems are difficult to apply to SaaS in smart cities because of many unresolved issues such as requiring a customized blockchain, huge storage, communication and computation costs, and dependence on a third party to achieve fair trading. We propose a BBDS system model with its security requirements before we present a concrete construction by combining -protocol, Paillier encryption scheme, and any secure symmetrical encryption and signature schemes. To demonstrate the utility of our proposed BBDS system, we present a security analysis and compare our system with other solutions. We implement the prototype in Remix to analyze the gas cost, and we conduct experiments to evaluate the communication and computation costs of the BBDS system using symmetric encryption (advanced encryption standard (AES)) and a signature scheme (elliptic curve digital signature algorithm (ECDSA)).}
}


@article{DBLP:journals/toit/FengYZGM21,
	author = {Jun Feng and
                  Laurence T. Yang and
                  Yuxiang Zhu and
                  Nicholaus J. Gati and
                  Yijun Mo},
	title = {Blockchain-enabled Tensor-based Conditional Deep Convolutional {GAN}
                  for Cyber-physical-Social Systems},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {41:1--41:17},
	year = {2021},
	url = {https://doi.org/10.1145/3404890},
	doi = {10.1145/3404890},
	timestamp = {Sun, 12 Nov 2023 02:19:58 +0100},
	biburl = {https://dblp.org/rec/journals/toit/FengYZGM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning techniques have shown significant success in cyber-physical-social systems (CPSS). As an instance of deep learning models, generative adversarial nets (GAN) model enables powerful and flexible image augmentation, image generation, and classification, thus can be applied to real-world CPSS settings. GAN model training needs a large collection of cyber-physical-social data originating from various CPSS devices. Numerous prevailing GAN models depend on a tacit assumption that several cyber-physical-social data providers present a reliable source to collect training data, which is seldom the case in real CPSS. The existing GAN models also fail to consider multi-dimensional latent structure. In our work, we put forward a novel blockchain-enabled tensor-based conditional deep convolutional GAN (TCDC-GAN) model for cyber-physical-social systems. The blockchain is employed to develop a decentralized and reliable cyber-physical-social data-sharing platform between numerous cyber-physical-social data providers, such that the training data and the model are documented on a ledger that is distributed. Furthermore, a tensor-based generator and a tensor-based discriminator are well designed by employing the tensor model. The results of extensive simulation experiments show the efficacy of the proposed TCDC-GAN model. Compared with the state-of-the-art models, our model gains superior estimation performance.}
}


@article{DBLP:journals/toit/LuoSZCWZC21,
	author = {Ye Luo and
                  Zehai Su and
                  Wei Zheng and
                  Zhaobin Chen and
                  Fuqin Wang and
                  Zhemin Zhang and
                  Jinjun Chen},
	title = {A Novel Memory-hard Password Hashing Scheme for Blockchain-based Cyber-physical
                  Systems},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {42:1--42:21},
	year = {2021},
	url = {https://doi.org/10.1145/3408310},
	doi = {10.1145/3408310},
	timestamp = {Mon, 28 Aug 2023 21:41:17 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LuoSZCWZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There has been an increasing interest of integrating blockchain into cyber-physical systems (CPS). The design of password hashing schemes (PHSs) is in the core of blockchain security. However, no existing PHS seems to meet both the requirements of sufficient security and small code size for blockchain-based CPSs. In this article, a novel memory-hard PHS based on the classic PBKDF2 is proposed. Evaluation results show that the proposed scheme is promising for blockchain-based CPS, as it manages to provide enhanced security in comparison to PBKDF2 with limited increase in code size.}
}


@article{DBLP:journals/toit/WangMGWDW21,
	author = {Hao Wang and
                  Shenglan Ma and
                  Chaonian Guo and
                  Yulei Wu and
                  Hong{-}Ning Dai and
                  Di Wu},
	title = {Blockchain-Based Power Energy Trading Management},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {43:1--43:16},
	year = {2021},
	url = {https://doi.org/10.1145/3409771},
	doi = {10.1145/3409771},
	timestamp = {Sun, 30 Apr 2023 18:47:28 +0200},
	biburl = {https://dblp.org/rec/journals/toit/WangMGWDW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed peer-to-peer power energy markets are emerging quickly. Due to central governance and lack of effective information aggregation mechanisms, energy trading cannot be efficiently scheduled and tracked. We devise a new distributed energy transaction system over the energy Industrial Internet of Things based on predictive analytics, blockchain, and smart contract technologies. We propose a solution for scheduling distributed energy sources based on the Minimum Cut Maximum Flow theory. Blockchain is used to record transactions and reach consensus. Payment clearing for the actual power consumption is executed via smart contracts. Experimental results on real data show that our solution is practical and achieves a lower total cost for power energy consumption.}
}


@article{DBLP:journals/toit/KrolSATRP21,
	author = {Michal Kr{\'{o}}l and
                  Alberto Sonnino and
                  Mustafa Al{-}Bassam and
                  Argyrios G. Tasiopoulos and
                  Etienne Rivi{\`{e}}re and
                  Ioannis Psaras},
	title = {Proof-of-Prestige: {A} Useful Work Reward System for Unverifiable
                  Tasks},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {44:1--44:27},
	year = {2021},
	url = {https://doi.org/10.1145/3419483},
	doi = {10.1145/3419483},
	timestamp = {Wed, 14 Jul 2021 15:56:24 +0200},
	biburl = {https://dblp.org/rec/journals/toit/KrolSATRP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As cryptographic tokens and altcoins are increasingly being built to serve as utility tokens, the notion of useful work consensus protocols is becoming ever more important. With useful work consensus protocols, users get rewards after they have carried out some specific tasks useful for the network. While in some cases the proof of some utility or service can be provided, the majority of tasks are impossible to verify reliably. To deal with such cases, we design “Proof-of-Prestige” (PoP)—a reward system that can run directly on Proof-of-Stake (PoS) blockchains or as a smart contract on top of Proof-of-Work (PoW) blockchains. PoP introduces “prestige,” which is a volatile resource that, in contrast to coins, regenerates over time. Prestige can be gained by performing useful work, spent when benefiting from services, and directly translates to users minting power. Our scheme allows us to reliably reward decentralized workers while keeping the system free for the end-users. PoP is resistant against Sybil and collusion attacks and can be used with a vast range of unverifiable tasks. We build a simulator to assess the cryptoeconomic behavior of the system and deploy a full prototype of a content dissemination platform rewarding its participants. We implement the blockchain component on both Ethereum (PoW) and Cosmos (PoS), provide a mobile application, and connect it with our scheme with a negligible memory footprint. Finally, we adapt a fair exchange protocol allowing us to atomically exchange files for rewards also in scenarios where not all the parties have Internet connectivity. Our evaluation shows that even for large Ethereum traces, PoP introduces sub-millisecond computational overhead for miners in Cosmos and less than 0.013$ smart contract invocation cost for users in Ethereum.}
}


@article{DBLP:journals/toit/CaoWWGFDYL21,
	author = {Bin Cao and
                  Jiawei Wu and
                  Sichao Wang and
                  Honghao Gao and
                  Jing Fan and
                  Shuiguang Deng and
                  Jianwei Yin and
                  Xuan Liu},
	title = {Unsupervised Derivation of Keyword Summary for Short Texts},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {45:1--45:23},
	year = {2021},
	url = {https://doi.org/10.1145/3397162},
	doi = {10.1145/3397162},
	timestamp = {Mon, 26 Jul 2021 16:09:31 +0200},
	biburl = {https://dblp.org/rec/journals/toit/CaoWWGFDYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automatically summarizing a group of short texts that mainly share one topic is a fundamental task in many applications, e.g., summarizing the main symptoms for a disease based on a group of medical texts that are usually short, i.e., tens of words. Conventional unsupervised short text summarization techniques tend to find the most representative short text document. However, they may cause privacy issues, e.g., personal information in the medical texts may be exposed. Moreover, compared with the complete short text where some unimportant words may exist, a summary consisting of only a few keywords is more preferable by the user due to its clear and concise form. Due to the above reasons, in this article, we aim to solve the problem of unsupervised derivation of keyword summary for short texts. Existing keyword extraction methods such as Latent Dirichlet Allocation cannot be applied to solve this problem, since (1)\xa0the ordering relations among the extracted keywords are ignored, which causes troubles for people to capture the main idea of the event, and (2)\xa0short texts contain limited context, which makes it hard to find the optimal words for semantic coverage. Hence, we propose a simple but yet effective method named Frequent Closed Wordsets Ranking (FCWRank) to derive the keyword summary from a short text cluster. FCWRank is an unsupervised method that builds on the idea of frequent closed itemset mining in transaction database. FCWRank first mines all frequent closed wordsets from a cluster of short texts and then selects the most important wordset based on an importance model where the similarity between closed wordsets and the relation between the closed wordset and the short text document are considered simultaneously. To make the keywords within the wordset more understandable, FCWRank further unfolds the semantics behind them by sorting them. Experiments on real-world short text collections show that FCWRank outperforms the state-of-the-art baselines in terms of Recall-Oriented Understudy for Gisting Evaluation-Longest common subsequence F1, precision and recall scores.}
}


@article{DBLP:journals/toit/BhartiBS21,
	author = {Vandana Bharti and
                  Bhaskar Biswas and
                  Kaushal Kumar Shukla},
	title = {A Novel Multiobjective {GDWCN-PSO} Algorithm and Its Application to
                  Medical Data Security},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {46:1--46:28},
	year = {2021},
	url = {https://doi.org/10.1145/3397679},
	doi = {10.1145/3397679},
	timestamp = {Mon, 28 Aug 2023 21:41:18 +0200},
	biburl = {https://dblp.org/rec/journals/toit/BhartiBS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nature-inspired optimization is one of the most prevalent research domains with a confounding history that fascinates the research communities. Particle Swarm Optimization is one of the well-known optimizers that belongs to the family of nature-inspired algorithms. It often suffers from premature convergence leading to a local optimum. To address this, several methods were presented using different network topologies of the particles, but either lacked accuracy or were slow. To solve these problems, an improved version of the Directed Weighted Complex Network Particle Swarm Optimization using the Genetic Algorithm\xa0(GDWCN-PSO) is presented. This method uses the concept of the Genetic Algorithm\xa0after each update to enhance convergence and diversity. Since most of the real-world applications and complex optimization problems involve more than one objective function so to suit this problem, a multiobjective version of GDWCN-PSO is also proposed and validated on standard benchmarks. To demonstrate its applicability in real-world applications, GDWCN-PSO is applied to solve the optimal key-based medical image encryption. It is one of the most challenging problems in health IoTs for protecting sensitive and confidential patient data as well as addressing the major concern of integrity and security of data in today’s advanced digital world.}
}


@article{DBLP:journals/toit/HaoCWGW21,
	author = {Shijie Hao and
                  Tao Chen and
                  Yang Wang and
                  Yanrong Guo and
                  Meng Wang},
	title = {Adaptive Multi-Task Dual-Structured Learning with Its Application
                  on Alzheimer's Disease Study},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {47:1--47:16},
	year = {2021},
	url = {https://doi.org/10.1145/3398728},
	doi = {10.1145/3398728},
	timestamp = {Wed, 08 Mar 2023 09:59:30 +0100},
	biburl = {https://dblp.org/rec/journals/toit/HaoCWGW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-task learning has been widely applied to Alzheimer’s Disease (AD) studies due to its capability of simultaneously rating the disease severity (classification) and predicting corresponding clinical scores (regression). In this article, we propose a novel technique of Adaptive Multi-task Dual-Structured Learning, named AMDSL, by mutually exploring the dual manifold structure for the label and regression score of the disease data under joint classification and regression tasks, while learning an adaptive shared similarity measure and corresponding feature mapping among these two tasks. We encode both the reconstructed label representation and regression score adaptive to the ideal similarity measure on disease data to achieve the ideal performance on these two joint tasks. The alternating algorithm is proposed to optimize the above objective. We theoretically prove the convergence of the optimization algorithm. The superiority of AMDSL is experimentally validated under joint classification and regression as per various evaluation metrics against the most authoritative Alzheimer’s disease data.}
}


@article{DBLP:journals/toit/ZhangJQBQ21,
	author = {Yuanpeng Zhang and
                  Yizhang Jiang and
                  Lianyong Qi and
                  Md. Zakirul Alam Bhuiyan and
                  Pengjiang Qian},
	title = {Epilepsy Diagnosis Using Multi-view {\&} Multi-medoid Entropy-based
                  Clustering with Privacy Protection},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {48:1--48:21},
	year = {2021},
	url = {https://doi.org/10.1145/3404893},
	doi = {10.1145/3404893},
	timestamp = {Thu, 29 Jul 2021 13:41:55 +0200},
	biburl = {https://dblp.org/rec/journals/toit/ZhangJQBQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Using unsupervised learning methods for clinical diagnosis is very meaningful. In this study, we propose an unsupervised multi-view & multi-medoid variant-entropy-based fuzzy clustering (M2VEFC) method for epilepsy EEG signals detecting. Comparing with existing related studies, M2VEFC has four main merits and contributions: (1) Features in original EEG data are represented from different perspectives that can provide more pattern information for epilepsy signals detecting. (2) During multi-view modeling, multi-medoids are used to capture the structure of clusters in each view. Furthermore, we assume that the medoids in a cluster observed from different views should keep invariant, which is taken as one of the collaborative learning mechanisms in this study. (3) A variant entropy is designed as another collaborative learning mechanism in which view weight learning is controlled by a user-free parameter. The parameter is derived from the distribution of samples in each view such that the learned weights have more discrimination. (4) M2VEFC does not need original data as its input—it only needs a similarity matrix and feature statistical information. Therefore, the original data are not exposed to users and hence the privacy is protected. We use several different kinds of feature extraction techniques to extract several groups of features as multi-view data from original EEG data to test the proposed method M2VEFC. Experimental results indicate M2VEFC achieves a promising performance that is better than benchmarking models.}
}


@article{DBLP:journals/toit/WuSLT21,
	author = {Jimmy Ming{-}Tai Wu and
                  Gautam Srivastava and
                  Jerry Chun{-}Wei Lin and
                  Qian Teng},
	title = {A Multi-Threshold Ant Colony System-based Sanitization Model in Shared
                  Medical Environments},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {49:1--49:26},
	year = {2021},
	url = {https://doi.org/10.1145/3408296},
	doi = {10.1145/3408296},
	timestamp = {Thu, 05 Jan 2023 14:06:40 +0100},
	biburl = {https://dblp.org/rec/journals/toit/WuSLT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {During the past several years, revealing some useful knowledge or protecting individual’s private information in an identifiable health dataset (i.e., within an Electronic Health Record) has become a tradeoff issue. Especially in this era of a global pandemic, security and privacy are often overlooked in lieu of usability. Privacy preserving data mining (PPDM) is definitely going to be have an important role to resolve this problem. Nevertheless, the scenario of mining information in an identifiable health dataset holds high complexity compared to traditional PPDM problems. Leaking individual private information in an identifiable health dataset has becomes a serious legal issue. In this article, the proposed Ant Colony System to Data Mining algorithm takes the multi-threshold constraint to secure and sanitize patents’ records in different lengths, which is applicable in a real medical situation. The experimental results show the proposed algorithm not only has the ability to hide all sensitive information but also to keep useful knowledge for mining usage in the sanitized database.}
}


@article{DBLP:journals/toit/PfitznerSA21,
	author = {Bjarne Pfitzner and
                  Nico Steckhan and
                  Bert Arnrich},
	title = {Federated Learning in a Medical Context: {A} Systematic Literature
                  Review},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {50:1--50:31},
	year = {2021},
	url = {https://doi.org/10.1145/3412357},
	doi = {10.1145/3412357},
	timestamp = {Thu, 29 Jul 2021 13:41:55 +0200},
	biburl = {https://dblp.org/rec/journals/toit/PfitznerSA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data privacy is a very important issue. Especially in fields like medicine, it is paramount to abide by the existing privacy regulations to preserve patients’ anonymity. However, data is required for research and training machine learning models that could help gain insight into complex correlations or personalised treatments that may otherwise stay undiscovered. Those models generally scale with the amount of data available, but the current situation often prohibits building large databases across sites. So it would be beneficial to be able to combine similar or related data from different sites all over the world while still preserving data privacy. Federated learning has been proposed as a solution for this, because it relies on the sharing of machine learning models, instead of the raw data itself. That means private data never leaves the site or device it was collected on. Federated learning is an emerging research area, and many domains have been identified for the application of those methods. This systematic literature review provides an extensive look at the concept of and research into federated learning and its applicability for confidential healthcare datasets.}
}


@article{DBLP:journals/toit/ChaayaCAAPBB21,
	author = {Karam Bou Chaaya and
                  Richard Chbeir and
                  Mansour Naser Alraja and
                  Philippe Arnould and
                  Charith Perera and
                  Mahmoud Barhamgi and
                  Djamal Benslimane},
	title = {{\(\delta\)}-\emph{Risk}: Toward Context-aware Multi-objective Privacy
                  Management in Connected Environments},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {51:1--51:31},
	year = {2021},
	url = {https://doi.org/10.1145/3418499},
	doi = {10.1145/3418499},
	timestamp = {Sun, 25 Jul 2021 11:43:28 +0200},
	biburl = {https://dblp.org/rec/journals/toit/ChaayaCAAPBB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In today’s highly connected cyber-physical environments, users are becoming more and more concerned about their privacy and ask for more involvement in the control of their data. However, achieving effective involvement of users requires improving their privacy decision-making. This can be achieved by: (i) raising their awareness regarding the direct and indirect privacy risks they accept to take when sharing data with consumers; (ii) helping them in optimizing their privacy protection decisions to meet their privacy requirements while maximizing data utility. In this article, we address the second goal by proposing a user-centric multi-objective approach for context-aware privacy management in connected environments, denoted δ-Risk. Our approach features a new privacy risk quantification model to dynamically calculate and select the best protection strategies for the user based on her preferences and contexts. Computed strategies are optimal in that they seek to closely satisfy user requirements and preferences while maximizing data utility and minimizing the cost of protection. We implemented our proposed approach and evaluated its performance and effectiveness in various scenarios. The results show that δ-Risk delivers scalability and low-complexity in time and space. Besides, it handles privacy reasoning in real-time, making it able to support the user in various contexts, including ephemeral ones. It also provides the user with at least one best strategy per context.}
}


@article{DBLP:journals/toit/KumarSLSS21,
	author = {Abhinav Kumar and
                  Sanjay Kumar Singh and
                  K. Lakshmanan and
                  Sonal Saxena and
                  Sameer Shrivastava},
	title = {A Novel Cloud-Assisted Secure Deep Feature Classification Framework
                  for Cancer Histopathology Images},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {2},
	pages = {52:1--52:22},
	year = {2021},
	url = {https://doi.org/10.1145/3424221},
	doi = {10.1145/3424221},
	timestamp = {Sat, 09 Apr 2022 12:33:23 +0200},
	biburl = {https://dblp.org/rec/journals/toit/KumarSLSS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advancements in the Internet of Things (IoT) and cloud services have enabled the availability of smart e-healthcare services in a distant and distributed environment. However, this has also raised major privacy and efficiency concerns that need to be addressed. While sharing clinical data across the cloud that often consists of sensitive patient-related information, privacy is a major challenge. Adequate protection of patients’ privacy helps to increase public trust in medical research. Additionally, DL-based models are complex, and in a cloud-based approach, efficient data processing in such models is complicated. To address these challenges, we propose an efficient and secure cancer diagnostic framework for histopathological image classification by utilizing both differential privacy and secure multi-party computation. For efficient computation, instead of performing the whole operation on the cloud, we decouple the layers into two modules: one for feature extraction using the VGGNet module at the user side and the remaining layers for private prediction over the cloud. The efficacy of the framework is validated on two datasets composed of histopathological images of the canine mammary tumor and human breast cancer. The application of differential privacy preserving to the proposed model makes the model secure and capable of preserving the privacy of sensitive data from any adversary, without significantly compromising the model accuracy. Extensive experiments show that the proposed model efficiently achieves the trade-off between privacy and model performance.}
}


@article{DBLP:journals/toit/Al-KhafajiyOBAM21,
	author = {Mohammed Al{-}Khafajiy and
                  Safa Otoum and
                  Thar Baker and
                  Muhammad Asim and
                  Zakaria Maamar and
                  Moayad Aloqaily and
                  Mark Taylor and
                  Martin Randles},
	title = {Intelligent Control and Security of Fog Resources in Healthcare Systems
                  via a Cognitive Fog Model},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {54:1--54:23},
	year = {2021},
	url = {https://doi.org/10.1145/3382770},
	doi = {10.1145/3382770},
	timestamp = {Sat, 30 Sep 2023 10:29:29 +0200},
	biburl = {https://dblp.org/rec/journals/toit/Al-KhafajiyOBAM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There have been significant advances in the field of Internet of Things (IoT) recently, which have not always considered security or data security concerns: A high degree of security is required when considering the sharing of medical data over networks. In most IoT-based systems, especially those within smart-homes and smart-cities, there is a bridging point (fog computing) between a sensor network and the Internet which often just performs basic functions such as translating between the protocols used in the Internet and sensor networks, as well as small amounts of data processing. The fog nodes can have useful knowledge and potential for constructive security and control over both the sensor network and the data transmitted over the Internet. Smart healthcare services utilise such networks of IoT systems. It is therefore vital that medical data emanating from IoT systems is highly secure, to prevent fraudulent use, whilst maintaining quality of service providing assured, verified and complete data. In this article, we examine the development of a Cognitive Fog (CF) model, for secure, smart healthcare services, that is able to make decisions such as opting-in and opting-out from running processes and invoking new processes when required, and providing security for the operational processes within the fog system. Overall, the proposed ensemble security model performed better in terms of Accuracy Rate, Detection Rate, and a lower False Positive Rate (standard intrusion detection measurements) than three base classifiers (K-NN, DBSCAN, and DT) using a standard security dataset (NSL-KDD).}
}


@article{DBLP:journals/toit/LvP21,
	author = {Zhihan Lv and
                  Francesco Piccialli},
	title = {The Security of Medical Data on Internet Based on Differential Privacy
                  Technology},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {55:1--55:18},
	year = {2021},
	url = {https://doi.org/10.1145/3382769},
	doi = {10.1145/3382769},
	timestamp = {Mon, 28 Aug 2023 21:41:18 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LvP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The study aims at discussing the security of medical data in the Internet era. By using k-anonymity (K-A) and differential privacy (DP), an algorithm model combining K-A and DP was proposed, which was simulated through the experiments. In the Magic and EIA datasets, the algorithm constructed was compared with K-A and the L-diversity model to verify the performance of the model. The model constructed based on DP had the lowest privacy-leakage risks, which increased with the number of identifiers in the Magic and EIA datasets, and the information disclosure was the least. In addition, in its usability analysis, it was found that its value was the most obviously improved and its operation efficiency was the highest. The K-A-DP algorithm can effectively reduce the risk of privacy leakage and information loss, and has achieved excellent results. Despite the deficiencies in the process of the experiment, the study still provides a reference for solving the problem of medical data security.}
}


@article{DBLP:journals/toit/WangZBJX21,
	author = {Tao Wang and
                  Zhigao Zheng and
                  Ali Kashif Bashir and
                  Alireza Jolfaei and
                  Yanyan Xu},
	title = {FinPrivacy: {A} Privacy-preserving Mechanism for Fingerprint Identification},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {56:1--56:15},
	year = {2021},
	url = {https://doi.org/10.1145/3387130},
	doi = {10.1145/3387130},
	timestamp = {Tue, 23 Aug 2022 09:22:01 +0200},
	biburl = {https://dblp.org/rec/journals/toit/WangZBJX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fingerprint provides an extremely convenient way of identification for a wide range of real-life applications owing to its universality, uniqueness, collectability, and invariance. However, digitized fingerprints may reveal the privacy of individuals. Differential privacy is a promising privacy-preserving solution that is enforced by injecting random noise into preserved objects, such that an adversary with arbitrary background knowledge cannot infer private input from the noisy results. This study proposes FinPrivacy, a privacy-preserving mechanism for fingerprint identification. This mechanism utilizes the low-rank matrix approximation to reduce the dimensionality of fingerprint and the exponential mechanism to carefully determine the value of the optimal rank. Thereafter, FinPrivacy injects Laplace noise to the singular values of the approximated singular matrix, thereby trading off between privacy and utility. Analytic proofs and results of the comparative experiments demonstrate that FinPrivacy can simultaneously enforce ɛ-differential privacy and maintain an efficient fingerprint recognition.}
}


@article{DBLP:journals/toit/YueDZZCTJZ21,
	author = {Zijie Yue and
                  Shuai Ding and
                  Lei Zhao and
                  Youtao Zhang and
                  Zehong Cao and
                  Mohammad Tanveer and
                  Alireza Jolfaei and
                  Xi Zheng},
	title = {Privacy-preserving Time-series Medical Images Analysis Using a Hybrid
                  Deep Learning Framework},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {57:1--57:21},
	year = {2021},
	url = {https://doi.org/10.1145/3383779},
	doi = {10.1145/3383779},
	timestamp = {Thu, 23 Jun 2022 20:04:05 +0200},
	biburl = {https://dblp.org/rec/journals/toit/YueDZZCTJZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-series medical images are an important type of medical data that contain rich temporal and spatial information. As a state-of-the-art, computer-aided diagnosis (CAD) algorithms are usually used on these image sequences to improve analysis accuracy. However, such CAD algorithms are often required to upload medical images to honest-but-curious servers, which introduces severe privacy concerns. To preserve privacy, the existing CAD algorithms support analysis on each encrypted image but not on the whole encrypted image sequences, which leads to the loss of important temporal information among frames. To meet this challenge, a convolutional-LSTM network, named HE-CLSTM, is proposed for analyzing time-series medical images encrypted by a fully homomorphic encryption mechanism. Specifically, several convolutional blocks are constructed to extract discriminative spatial features, and LSTM-based sequence analysis layers (HE-LSTM) are leveraged to encode temporal information from the encrypted image sequences. Moreover, a weighted unit and a sequence voting layer are designed to incorporate both spatial and temporal features with different weights to improve performance while reducing the missed diagnosis rate. The experimental results on two challenging benchmarks (a Cervigram dataset and the BreaKHis public dataset) provide strong evidence that our framework can encode visual representations and sequential dynamics from encrypted medical image sequences; our method achieved AUCs above 0.94 both on the Cervigram and BreaKHis datasets, constituting a significant margin of statistical improvement compared with several competing methods.}
}


@article{DBLP:journals/toit/XiaWMW21,
	author = {Kaijian Xia and
                  Xiang Wu and
                  Yaqing Mao and
                  Huanhuan Wang},
	title = {Secure {DNA} Motif-Finding Method Based on Sampling Candidate Pruning},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {58:1--58:19},
	year = {2021},
	url = {https://doi.org/10.1145/3382078},
	doi = {10.1145/3382078},
	timestamp = {Mon, 20 Sep 2021 08:52:09 +0200},
	biburl = {https://dblp.org/rec/journals/toit/XiaWMW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continuous exploration of genetic research, gradually exposed privacy issues become the bottleneck that limits its development. DNA motif finding is an important study to understand the regulation of gene expression; however, the existing methods generally ignore the potential sensitive information that may be exposed in the process. In this work, we utilize the -differential privacy model to provide provable privacy guarantees which is independent of attackers’ background knowledge. Our method makes use of sample databases to prune the generated candidate motifs to lower the magnitude of added noise. Furthermore, to improve the utility of mining results, a strategy of threshold modification is designed to reduce the propagation and random sampling errors in the mining process. Extensive experiments on actual DNA databases confirm that our approach can privately find DNA motifs with high utility and efficiency.}
}


@article{DBLP:journals/toit/RichhariyaT21,
	author = {Bharat Richhariya and
                  Mohammad Tanveer},
	title = {An Efficient Angle-based Universum Least Squares Twin Support Vector
                  Machine for Classification},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {59:1--59:24},
	year = {2021},
	url = {https://doi.org/10.1145/3387131},
	doi = {10.1145/3387131},
	timestamp = {Mon, 20 Sep 2021 08:52:09 +0200},
	biburl = {https://dblp.org/rec/journals/toit/RichhariyaT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Universum-based support vector machine incorporates prior information about the distribution of data in training of the classifier. This leads to better generalization performance but with increased computation cost. Various twin hyperplane-based models are proposed to reduce the computation cost of universum-based algorithms. In this work, we present an efficient angle-based universum least squares twin support vector machine (AULSTSVM) for classification. This is a novel approach of incorporating universum in the formulation of least squares-based twin SVM model. First, the proposed AULSTSVM constructs a universum hyperplane, which is proximal to universum data points. Then, the classifying hyperplane is constructed by minimizing the angle with the universum hyperplane. This gives prior information about data distribution to the classifier. In addition to the quadratic loss, we introduce linear loss in the optimization problem of the proposed AULSTSVM, which leads to lesser computation cost of the model. Numerical experiments are performed on several benchmark synthetic, real-world, and large-scale datasets. The results show that proposed AULSTSVM performs better than existing algorithms w.r.t. generalization performance as well as computation time. Moreover, an application to Alzheimer’s disease is presented, where AULSTSVM obtains accuracy of 95% for classification of healthy and Alzheimers subjects. The results imply that the proposed AULSTSVM is a better alternative for classification of large-scale datasets and biomedical applications.}
}


@article{DBLP:journals/toit/KimKK21,
	author = {Tae{-}Yeun Kim and
                  Sung{-}Hwan Kim and
                  Hoon Ko},
	title = {Design and Implementation of BCI-based Intelligent Upper Limb Rehabilitation
                  Robot System},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {60:1--60:17},
	year = {2021},
	url = {https://doi.org/10.1145/3392115},
	doi = {10.1145/3392115},
	timestamp = {Tue, 05 Oct 2021 10:02:54 +0200},
	biburl = {https://dblp.org/rec/journals/toit/KimKK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The present study aimed to use the proposed system to measure and analyze brain waves of users to allow intelligent upper limb rehabilitation and to optimize the system using a genetic algorithm. The study used EPOC Neuroheadset for Emotiv with EEG electrodes attached as a non-invasive method for measuring brain waves. The brain waves were measured according to the EEG 10-20 standard electrode layout, which allows measurement of signals from each spot where electrodes are attached based on EEG characteristics. The measured data were added in a database. In the intelligent neuro-fuzzy model, wave transform was used for extracting brain wave characteristics according to user intentions and to eliminate noise from the signals in an effort to increase reliability. Moreover, to construct the option rules of the neuro-fuzzy system, FCM technique and optimal cluster evaluation method were used. Furthermore, the asymmetric Gaussian membership function was used to improve performance, whereas SD and WF divided into left and right sides were used to express the chromosomes. Optimal EEG electrode locations were found, and comparative analysis was performed on the differences based on membership function, number of clusters, and number of learning generations, learning algorithm, and wavelet settings. The performance evaluation results showed that the optimal EEG electrode locations were F7, F8, FC5, and FC6, whereas the accuracy of learning and test data of user-intention recognition was found to be 94.2% and 92.3%, respectively, which suggests that the proposed system can be used to recognize user intention for specific behavior. The system proposed in the present study can allow continued rehabilitation exercise in everyday living according to user intentions, which is expected to help improve the user's willingness to participate in rehabilitation and his or her quality of life.}
}


@article{DBLP:journals/toit/PengHCKK21,
	author = {Cong Peng and
                  Debiao He and
                  Jianhua Chen and
                  Neeraj Kumar and
                  Muhammad Khurram Khan},
	title = {{EPRT:} An Efficient Privacy-Preserving Medical Service Recommendation
                  and Trust Discovery Scheme for eHealth System},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {61:1--61:24},
	year = {2021},
	url = {https://doi.org/10.1145/3397678},
	doi = {10.1145/3397678},
	timestamp = {Sat, 30 Sep 2023 10:29:30 +0200},
	biburl = {https://dblp.org/rec/journals/toit/PengHCKK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the essential applications of health information technology, the eHealth system plays a significant role in enabling various internet medicine service scenes, most of which primarily rely on service recommendation or an evaluation mechanism. To avoid privacy leakage, some privacy-preserving mechanisms must be adopted to protect raters’ privacy and make evaluation trust reliable. To tackle this challenge, this article proposes an efficient service recommendation and evaluation scheme, called EPRT, which is based on a similarity calculation and trust discovery method. This scheme uses homomorphic encryption technology to encrypt the sensitive data and combines the threshold mechanism and double-trap mechanism to realize the secure computing on the encrypted data, so as to ensure that the plaintexts of the final calculation results (e.g., recommendation value and evaluation truth) are only obtained by the authorized subject. In addition, a detailed security analysis shows that the proposed EPRT scheme can achieve the expected security. In addition, performance comparison results are carried out, demonstrating its effectiveness and accuracy.}
}


@article{DBLP:journals/toit/MaitiKJB21,
	author = {Somanka Maiti and
                  Ashish Kumar and
                  Smriti Jain and
                  Gaurav Bhatnagar},
	title = {A Novel Image Inpainting Framework Using Regression},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {62:1--62:16},
	year = {2021},
	url = {https://doi.org/10.1145/3402177},
	doi = {10.1145/3402177},
	timestamp = {Tue, 05 Oct 2021 10:02:54 +0200},
	biburl = {https://dblp.org/rec/journals/toit/MaitiKJB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, a blockwise regression-based image inpainting framework is proposed. The core idea is to fill the unknown region in two stages: Extrapolate the edges to the unknown region and then fill the unknown pixels values in each sub-region demarcated by the extended edges. Canny edge detection and linear edge extension are used to respectively identify and extend edges to the unknown region followed by regression within each sub-region to predict the unknown pixel values. Two different regression models based on K-nearest neighbours and support vectors machine are used to predict the unknown pixel values. The proposed framework has the advantage of inpainting without requiring prior training on any image dataset. The extensive experiments on different images with contrasting distortions demonstrate the robustness of the proposed framework and a detailed comparative analysis shows that the proposed technique outperforms existing state-of-the-art image inpainting methods. Finally, the proposed techniques are applied to MRI images suffering from susceptibility artifacts to illustrate the practical usage of the proposed work.}
}


@article{DBLP:journals/toit/LiPXPZGL21,
	author = {Haolun Li and
                  Chi{-}Man Pun and
                  Feng Xu and
                  Longsheng Pan and
                  Rui Zong and
                  Hao Gao and
                  Huimin Lu},
	title = {A Hybrid Feature Selection Algorithm Based on a Discrete Artificial
                  Bee Colony for Parkinson's Diagnosis},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {63:1--63:22},
	year = {2021},
	url = {https://doi.org/10.1145/3397161},
	doi = {10.1145/3397161},
	timestamp = {Tue, 05 Mar 2024 15:16:46 +0100},
	biburl = {https://dblp.org/rec/journals/toit/LiPXPZGL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parkinson's disease is a neurodegenerative disease that affects millions of people around the world and cannot be cured fundamentally. Automatic identification of early Parkinson's disease on feature data sets is one of the most challenging medical tasks today. Many features in these datasets are useless or suffering from problems like noise, which affect the learning process and increase the computational burden. To ensure the optimal classification performance, this article proposes a hybrid feature selection algorithm based on an improved discrete artificial bee colony algorithm to improve the efficiency of feature selection. The algorithm combines the advantages of filters and wrappers to eliminate most of the uncorrelated or noisy features and determine the optimal subset of features. In the filter, three different variable ranking methods are employed to pre-rank the candidate features, then the population of artificial bee colony is initialized based on the significance degree of the re-rank features. In the wrapper part, the artificial bee colony algorithm evaluates individuals (feature subsets) based on the classification accuracy of the classifier to achieve the optimal feature subset. In addition, for the first time, we introduce a strategy that can automatically select the best classifier in the search framework more quickly. By comparing with several publicly available datasets, the proposed method achieves better performance than other state-of-the-art algorithms and can extract fewer effective features.}
}


@article{DBLP:journals/toit/MehrabiMJ21,
	author = {Mohamad Ali Mehrabi and
                  Naila Mukhtar and
                  Alireza Jolfaei},
	title = {Power Side-Channel Analysis of {RNS} {GLV} {ECC} Using Machine and
                  Deep Learning Algorithms},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {65:1--65:20},
	year = {2021},
	url = {https://doi.org/10.1145/3423555},
	doi = {10.1145/3423555},
	timestamp = {Mon, 03 Jan 2022 22:13:08 +0100},
	biburl = {https://dblp.org/rec/journals/toit/MehrabiMJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many Internet of Things applications in smart cities use elliptic-curve cryptosystems due to their efficiency compared to other well-known public-key cryptosystems such as RSA. One of the important components of an elliptic-curve-based cryptosystem is the elliptic-curve point multiplication which has been shown to be vulnerable to various types of side-channel attacks. Recently, substantial progress has been made in applying deep learning to side-channel attacks. Conceptually, the idea is to monitor a core while it is running encryption for information leakage of a certain kind, for example, power consumption. The knowledge of the underlying encryption algorithm can be used to train a model to recognise the key used for encryption. The model is then applied to traces gathered from the crypto core in order to recover the encryption key. In this article, we propose an RNS GLV elliptic curve cryptography core which is immune to machine learning and deep learning based side-channel attacks. The experimental analysis confirms the proposed crypto core does not leak any information about the private key and therefore it is suitable for hardware implementations.}
}


@article{DBLP:journals/toit/GargCMNCG21,
	author = {Prateek Garg and
                  Anirudh Srinivasan Chakravarthy and
                  Murari Mandal and
                  Pratik Narang and
                  Vinay Chamola and
                  Mohsen Guizani},
	title = {ISDNet: AI-enabled Instance Segmentation of Aerial Scenes for Smart
                  Cities},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {66:1--66:18},
	year = {2021},
	url = {https://doi.org/10.1145/3418205},
	doi = {10.1145/3418205},
	timestamp = {Tue, 05 Oct 2021 10:02:54 +0200},
	biburl = {https://dblp.org/rec/journals/toit/GargCMNCG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aerial scenes captured by UAVs have immense potential in IoT applications related to urban surveillance, road and building segmentation, land cover classification, and so on, which are necessary for the evolution of smart cities. The advancements in deep learning have greatly enhanced visual understanding, but the domain of aerial vision remains largely unexplored. Aerial images pose many unique challenges for performing proper scene parsing such as high-resolution data, small-scaled objects, a large number of objects in the camera view, dense clustering of objects, background clutter, and so on, which greatly hinder the performance of the existing deep learning methods. In this work, we propose ISDNet (Instance Segmentation and Detection Network), a novel network to perform instance segmentation and object detection on visual data captured by UAVs. This work enables aerial image analytics for various needs in a smart city. In particular, we use dilated convolutions to generate improved spatial context, leading to better discrimination between foreground and background features. The proposed network efficiently reuses the segment-mask features by propagating them from early stages using residual connections. Furthermore, ISDNet makes use of effective anchors to accommodate varying object scales and sizes. The proposed method obtains state-of-the-art results in the aerial context.}
}


@article{DBLP:journals/toit/GaoZLYCY21,
	author = {Guangwei Gao and
                  Dong Zhu and
                  Huimin Lu and
                  Yi Yu and
                  Heyou Chang and
                  Dong Yue},
	title = {Robust Facial Image Super-Resolution by Kernel Locality-Constrained
                  Coupled-Layer Regression},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {67:1--67:15},
	year = {2021},
	url = {https://doi.org/10.1145/3418462},
	doi = {10.1145/3418462},
	timestamp = {Wed, 15 Nov 2023 17:34:52 +0100},
	biburl = {https://dblp.org/rec/journals/toit/GaoZLYCY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Super-resolution methods for facial image via representation learning scheme have become very effective methods due to their efficiency. The key problem for the super-resolution of facial image is to reveal the latent relationship between the low-resolution (LR) and the corresponding high-resolution (HR) training patch pairs. To simultaneously utilize the contextual information of the target position and the manifold structure of the primitive HR space, in this work, we design a robust context-patch facial image super-resolution scheme via a kernel locality-constrained coupled-layer regression (KLC2LR) scheme to obtain the desired HR version from the acquired LR image. Here, KLC2LR proposes to acquire contextual surrounding patches to represent the target patch and adds an HR layer constraint to compensate the detail information. Additionally, KLC2LR desires to acquire more high-frequency information by searching for nearest neighbors in the HR sample space. We also utilize kernel function to map features in original low-dimensional space into a high-dimensional one to obtain potential nonlinear characteristics. Our compared experiments in the noisy and noiseless cases have verified that our suggested methodology performs better than many existing predominant facial image super-resolution methods.}
}


@article{DBLP:journals/toit/PiccialliGPCC21,
	author = {Francesco Piccialli and
                  Fabio Giampaolo and
                  Edoardo Prezioso and
                  Danilo Crisci and
                  Salvatore Cuomo},
	title = {Predictive Analytics for Smart Parking: {A} Deep Learning Approach
                  in Forecasting of IoT Data},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {68:1--68:21},
	year = {2021},
	url = {https://doi.org/10.1145/3412842},
	doi = {10.1145/3412842},
	timestamp = {Sun, 02 Oct 2022 15:51:54 +0200},
	biburl = {https://dblp.org/rec/journals/toit/PiccialliGPCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, a sustainable and smart city focuses on energy efficiency and the reduction of polluting emissions through smart mobility projects and initiatives to “sensitize” infrastructure. Smart parking is one of the building blocks of intelligent mobility, innovative mobility that aims to be flexible, integrated, and sustainable and consequently integrated into a Smart City. By using the Internet of Things (IoT) sensors located in the parking areas or the underground car parks in combination with a mobile application, which indicates to citizens the free places in the different areas of the city and guides them toward the chosen parking, it is possible to reduce air pollution and fluidifying noise traffic. In this article, we present and discuss an innovative Deep Learning-based ensemble technique in forecasting the parking space occupancy to reduce the search time for parking and to optimize the flow of cars in particularly congested areas, with an overall positive impact on traffic in urban centres. A genetic algorithm has also been used to optimize predictors parameters. The main goal is to design an intelligent IoT-based service that can predict, in the next few hours, the parking spaces occupancy of a street. The proposed approach has been assessed on a real IoT dataset composed by over than 15M of collected sensor records. Obtained results demonstrate that our method outperforms both single predictors and the widely used strategy of the mean providing inherently robust predictions.}
}


@article{DBLP:journals/toit/HuangLGLAC21,
	author = {Feiran Huang and
                  Chaozhuo Li and
                  Boyu Gao and
                  Yun Liu and
                  Sattam Alotaibi and
                  Hao Chen},
	title = {Deep Attentive Multimodal Network Representation Learning for Social
                  Media Images},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {69:1--69:17},
	year = {2021},
	url = {https://doi.org/10.1145/3417295},
	doi = {10.1145/3417295},
	timestamp = {Mon, 13 May 2024 14:58:47 +0200},
	biburl = {https://dblp.org/rec/journals/toit/HuangLGLAC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The analysis for social networks, such as the socially connected Internet of Things, has shown a deep influence of intelligent information processing technology on industrial systems for Smart Cities. The goal of social media representation learning is to learn dense, low-dimensional, and continuous representations for multimodal data within social networks, facilitating many real-world applications. Since social media images are usually accompanied by rich metadata (e.g., textual descriptions, tags, groups, and submitted users), simply modeling the image is not effective to learn the comprehensive information from social media images. In this work, we treat the image and its textual description as multimodal content, and transform other metainformation into the links between contents (such as two images marked by the same tag or submitted by the same user). Based on the multimodal content and social links, we propose a Deep Attentive Multimodal Graph Embedding model named DAMGE for more effective social image representation learning. We introduce both small- and large-scale datasets to conduct extensive experiments, of which the results confirm the superiority of the proposal on the tasks of social image classification and link prediction.}
}


@article{DBLP:journals/toit/ChiuXG21,
	author = {David K. Y. Chiu and
                  Tao Xu and
                  Iker Gondra},
	title = {Random Graph-based Multiple Instance Learning for Structured IoT Smart
                  City Applications},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {70:1--70:17},
	year = {2021},
	url = {https://doi.org/10.1145/3448611},
	doi = {10.1145/3448611},
	timestamp = {Tue, 05 Oct 2021 10:02:55 +0200},
	biburl = {https://dblp.org/rec/journals/toit/ChiuXG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Because of the complex activities involved in IoT networks of a smart city, an important question arises: What are the core activities of the networks as a whole and its basic information flow structure? Identifying and discovering core activities and information flow is a crucial step that can facilitate the analysis. This is the question we are addressing—that is, to identify the core services as a common core substructure despite the probabilistic nature and the diversity of its activities. If this common substructure can be discovered, a systemic analysis and planning can then be performed and key policies related to the community can be developed. Here, a local IoT network can be represented as an attributed graph. From an ensemble of attributed graphs, identifying the common subgraph pattern is then critical in understanding the complexity. We introduce this as the common random subgraph (CRSG) modeling problem, aiming at identifying a subgraph pattern that is the structural “core” that conveys the probabilistically distributed graph characteristics. Given an ensemble of network samples represented as attributed graphs, the method generates a CRSG model that encompasses both structural and statistical characteristics from the related samples while excluding unrelated networks. In generating a CRSG model, our method using a multiple instance learning algorithm transforms an attributed graph (composed of structural elements as edges and their two endpoints) into a “bag” of instances in a vector space. Common structural components across positively labeled graphs are then identified as the common instance patterns among instances across different bags. The structure of the CRSG arises through the combining of common patterns. The probability distribution of the CRSG can then be estimated based on the connections and distributions from the common elements. Experimental results demonstrate that CRSG models are highly expressive in describing typical network characteristics.}
}


@article{DBLP:journals/toit/WanZSW21,
	author = {Liangtian Wan and
                  Mingyue Zhang and
                  Lu Sun and
                  Xianpeng Wang},
	title = {Machine Learning Empowered IoT for Intelligent Vehicle Location in
                  Smart Cities},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {71:1--71:25},
	year = {2021},
	url = {https://doi.org/10.1145/3448612},
	doi = {10.1145/3448612},
	timestamp = {Fri, 14 Apr 2023 16:22:19 +0200},
	biburl = {https://dblp.org/rec/journals/toit/WanZSW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent Transportation System (ITS) can boost the development of smart cities, and artificial intelligence and edge computing are key technologies that support the implementation of ITS. Vehicle localization is critical for ITS since the safety driving and location-aware serves highly depend on the accurate location information. In this article, we construct a vehicle localization system architecture composed of multiple Internet of Things (IoT) with arbitrary array configuration and a large amount of vehicles in smart cities. In order to deal with the coexisting of circular and non-circular signals transmitted by vehicles, we proposed several vehicle number estimation methods for non-circular signals. Based on the machine learning technique, we extend the vehicle number estimation method into mixed signals in more complex scenario of smart cities. Then the DOA estimation method for non-circular signals based on IoT is proposed, and then the performance of this method is analyzed as well. Simulation outcomes verify the excellent performance of the proposed vehicle number estimation methods and the DOA estimation method in smart cities, and the vehicle positions can be achieved with high estimation accuracy.}
}


@article{DBLP:journals/toit/AleZKG21,
	author = {Laha Ale and
                  Ning Zhang and
                  Scott A. King and
                  Jose Guardiola},
	title = {Spatio-temporal Bayesian Learning for Mobile Edge Computing Resource
                  Planning in Smart Cities},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {72:1--72:21},
	year = {2021},
	url = {https://doi.org/10.1145/3448613},
	doi = {10.1145/3448613},
	timestamp = {Thu, 23 Sep 2021 11:47:00 +0200},
	biburl = {https://dblp.org/rec/journals/toit/AleZKG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A smart city improves operational efficiency and comfort of living by harnessing techniques such as the Internet of Things (IoT) to collect and process data for decision-making. To better support smart cities, data collected by IoT should be stored and processed appropriately. However, IoT devices are often task-specialized and resource-constrained, and thus, they heavily rely on online resources in terms of computing and storage to accomplish various tasks. Moreover, these cloud-based solutions often centralize the resources and are far away from the end IoTs and cannot respond to users in time due to network congestion when massive numbers of tasks offload through the core network. Therefore, by decentralizing resources spatially close to IoT devices, mobile edge computing (MEC) can reduce latency and improve service quality for a smart city, where service requests can be fulfilled in proximity. As the service demands exhibit spatial-temporal features, deploying MEC servers at optimal locations and allocating MEC resources play an essential role in efficiently meeting service requirements in a smart city. In this regard, it is essential to learn the distribution of resource demands in time and space. In this work, we first propose a spatio-temporal Bayesian hierarchical learning approach to learn and predict the distribution of MEC resource demand over space and time to facilitate MEC deployment and resource management. Second, the proposed model is trained and tested on real-world data, and the results demonstrate that the proposed method can achieve very high accuracy. Third, we demonstrate an application of the proposed method by simulating task offloading. Finally, the simulated results show that resources allocated based upon our models’ predictions are exploited more efficiently than the resources are equally divided into all servers in unobserved areas.}
}


@article{DBLP:journals/toit/IwendiRJKS21,
	author = {Celestine Iwendi and
                  Saif Ur Rehman and
                  Abdul Rehman Javed and
                  Suleman Khan and
                  Gautam Srivastava},
	title = {Sustainable Security for the Internet of Things Using Artificial Intelligence
                  Architectures},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {73:1--73:22},
	year = {2021},
	url = {https://doi.org/10.1145/3448614},
	doi = {10.1145/3448614},
	timestamp = {Fri, 24 Mar 2023 19:51:45 +0100},
	biburl = {https://dblp.org/rec/journals/toit/IwendiRJKS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this digital age, human dependency on technology in various fields has been increasing tremendously. Torrential amounts of different electronic products are being manufactured daily for everyday use. With this advancement in the world of Internet technology, cybersecurity of software and hardware systems are now prerequisites for major business’ operations. Every technology on the market has multiple vulnerabilities that are exploited by hackers and cyber-criminals daily to manipulate data sometimes for malicious purposes. In any system, the Intrusion Detection System (IDS) is a fundamental component for ensuring the security of devices from digital attacks. Recognition of new developing digital threats is getting harder for existing IDS. Furthermore, advanced frameworks are required for IDS to function both efficiently and effectively. The commonly observed cyber-attacks in the business domain include minor attacks used for stealing private data. This article presents a deep learning methodology for detecting cyber-attacks on the Internet of Things using a Long Short Term Networks classifier. Our extensive experimental testing show an Accuracy of 99.09%, F1-score of 99.46%, and Recall of 99.51%, respectively. A detailed metric representing our results in tabular form was used to compare how our model was better than other state-of-the-art models in detecting cyber-attacks with proficiency.}
}


@article{DBLP:journals/toit/HuMLHZ21,
	author = {He{-}Xuan Hu and
                  Wen{-}Jie Mao and
                  Zhen{-}Zhou Lin and
                  Qiang Hu and
                  Ye Zhang},
	title = {Multimodal Brain Tumor Segmentation Based on an Intelligent {UNET-LSTM}
                  Algorithm in Smart Hospitals},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {74:1--74:14},
	year = {2021},
	url = {https://doi.org/10.1145/3450519},
	doi = {10.1145/3450519},
	timestamp = {Fri, 16 Feb 2024 14:06:11 +0100},
	biburl = {https://dblp.org/rec/journals/toit/HuMLHZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart hospitals are important components of smart cities. An intelligent medical system for brain tumor segmentation is required to construct smart hospitals. To achieve intelligent brain tumor segmentation, morphological variety and serious category imbalance must be managed effectively. Conventional deep neural networks have difficulty in predicting high-accuracy segmentation images due to these issues. To solve these problems, we propose using multimodal brain tumor images combined with the UNET and LSTM models to construct a new network structure with a mixed loss function to solve sample imbalance and describe an intelligent segmentation process to identify brain tumors. To verify the practicability of this algorithm, we used the open source Brain Tumor Segmentation Challenge dataset to train and verify the proposed network. We obtained DSCs of 0.91, 0.82, and 0.80; sensitivities of 0.93, 0.85, and 0.82; and specificities of 0.99, 0.99, and 0.98 in three tumor regions, including the whole tumor (WT), tumor core (TC), and enhanced tumor (ET). We also compared the results of the proposed network with those of other brain tumor segmentation methods, and the results showed that the proposed algorithm could segment different tumor lesions more accurately, highlighting its potential application value in the clinical diagnosis of brain tumors.}
}


@article{DBLP:journals/toit/LiMSZWBYT21,
	author = {Qianmu Li and
                  Shunmei Meng and
                  Xiaonan Sang and
                  Hanrui Zhang and
                  Shoujin Wang and
                  Ali Kashif Bashir and
                  Keping Yu and
                  Usman Tariq},
	title = {Dynamic Scheduling Algorithm in Cyber Mimic Defense Architecture of
                  Volunteer Computing},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {75:1--75:33},
	year = {2021},
	url = {https://doi.org/10.1145/3408291},
	doi = {10.1145/3408291},
	timestamp = {Wed, 07 Dec 2022 23:03:34 +0100},
	biburl = {https://dblp.org/rec/journals/toit/LiMSZWBYT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Volunteer computing uses computers volunteered by the general public to do distributed scientific computing. Volunteer computing is being used in high-energy physics, molecular biology, medicine, astrophysics, climate study, and other areas. These projects have attained unprecedented computing power. However, with the development of information technology, the traditional defense system cannot deal with the unknown security problems of volunteer computing. At the same time, Cyber Mimic Defense (CMD) can defend the unknown attack behavior through its three characteristics: dynamic, heterogeneous, and redundant. As an important part of the CMD, the dynamic scheduling algorithm realizes the dynamic change of the service centralized executor, which can enusre the security and reliability of CMD of volunteer computing. Aiming at the problems of passive scheduling and large scheduling granularity existing in the existing scheduling algorithms, this article first proposes a scheduling algorithm based on time threshold and task threshold and realizes the dynamic randomness of mimic defense from two different dimensions; finally, combining time threshold and random threshold, a dynamic scheduling algorithm based on multi-level queue is proposed. The experiment shows that the dynamic scheduling algorithm based on multi-level queue can take both security and reliability into account, has better dynamic heterogeneous redundancy characteristics, and can effectively prevent the transformation rule of heterogeneous executors from being mastered by attackers.}
}


@article{DBLP:journals/toit/ChenYGLC21,
	author = {Wu Chen and
                  Yong Yu and
                  Keke Gai and
                  Jiamou Liu and
                  Kim{-}Kwang Raymond Choo},
	title = {Time-Efficient Ensemble Learning with Sample Exchange for Edge Computing},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {76:1--76:17},
	year = {2021},
	url = {https://doi.org/10.1145/3409265},
	doi = {10.1145/3409265},
	timestamp = {Tue, 05 Oct 2021 10:02:54 +0200},
	biburl = {https://dblp.org/rec/journals/toit/ChenYGLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In existing ensemble learning algorithms (e.g., random forest), each base learner’s model needs the entire dataset for sampling and training. However, this may not be practical in many real-world applications, and it incurs additional computational costs. To achieve better efficiency, we propose a decentralized framework: Multi-Agent Ensemble. The framework leverages edge computing to facilitate ensemble learning techniques by focusing on the balancing of access restrictions (small sub-dataset) and accuracy enhancement. Specifically, network edge nodes (learners) are utilized to model classifications and predictions in our framework. Data is then distributed to multiple base learners who exchange data via an interaction mechanism to achieve improved prediction. The proposed approach relies on a training model rather than conventional centralized learning. Findings from the experimental evaluations using 20 real-world datasets suggest that Multi-Agent Ensemble outperforms other ensemble approaches in terms of accuracy even though the base learners require fewer samples (i.e., significant reduction in computation costs).}
}


@article{DBLP:journals/toit/MasudSGKAAA21,
	author = {Mehedi Masud and
                  Parminder Singh and
                  Gurjot Singh Gaba and
                  Avinash Kaur and
                  Roobaea Alroobaea and
                  Mubarak Alrashoud and
                  Salman Ali AlQahtani},
	title = {{CROWD:} Crow Search and Deep Learning based Feature Extractor for
                  Classification of Parkinson's Disease},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {77:1--77:18},
	year = {2021},
	url = {https://doi.org/10.1145/3418500},
	doi = {10.1145/3418500},
	timestamp = {Tue, 07 May 2024 20:27:47 +0200},
	biburl = {https://dblp.org/rec/journals/toit/MasudSGKAAA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge Artificial Intelligence (AI) is the latest trend for next-generation computing for data analytics, particularly in predictive edge analytics for high-risk diseases like Parkinson’s Disease (PD). Deep learning learning techniques facilitate edge AI applications for enhanced, real-time handling of data. Dopamine is the cause of Parkinson’s that happens due to the interference of brain cells that produce the substance to regulate the communication of brain cells. The brain cells responsible for generating the dopamine perform adaptation, control, and movement with fluency. Parkinson’s motor symptoms appear on the loss of 60% to 80% of cells, due to the non-production of appropriate dopamine. Recent research found a close connection between the speech impairment and PD. Many researchers have developed a classification algorithm to identify the PD from speech signals. In this article, Adaptive Crow Search Algorithm\xa0(ACSA) and Deep Learning (DL)–based optimal feature selection method are introduced. The proposed model is the combination of CROW Search and Deep learning (CROWD) stack sparse autoencoder neural network. Parkinson’s dataset is taken for the experiment from the Irvine dataset repository at the University of California (UCI). In the first phase, dataset cleaning is performed to handle the missing values in the dataset. After that, the proposed ACSA algorithm is employed to find the scrunched feature vector. Furthermore, stack spare autoencoder with seven hidden layers is employed to generate the compressed feature vector. The performance of the proposed CROWD autoencoder model is compared with three feature selection approaches for six supervised classification techniques. The experiment result demonstrates that the performance of the proposed CROWD autoencoder feature selection model has outperformed the benchmarked feature selection techniques: (i) Maximum Relevance (mRMR) (ii) Recursive Feature Elimination (RFE), and (iii) Correlation-based Feature Selection (CFS), to classify Parkinson’s disease. This research has significance in the healthcare sector for the enhancement of classification accuracy up to 0.96%.}
}


@article{DBLP:journals/toit/ChaudhryIYKAZ21,
	author = {Shehzad Ashraf Chaudhry and
                  Azeem Irshad and
                  Khalid Yahya and
                  Neeraj Kumar and
                  Mamoun Alazab and
                  Yousaf Bin Zikria},
	title = {Rotating behind Privacy: An Improved Lightweight Authentication Scheme
                  for Cloud-based IoT Environment},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {78:1--78:19},
	year = {2021},
	url = {https://doi.org/10.1145/3425707},
	doi = {10.1145/3425707},
	timestamp = {Mon, 26 Jun 2023 20:52:11 +0200},
	biburl = {https://dblp.org/rec/journals/toit/ChaudhryIYKAZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advancements in the internet of things (IoT) require specialized security protocols to provide unbreakable security along with computation and communication efficiencies. Moreover, user privacy and anonymity has emerged as an integral part, along with other security requirements. Unfortunately, many recent authentication schemes to secure IoT-based systems were either proved as vulnerable to different attacks or prey of inefficiencies. Some of these schemes suffer from a faulty design that happened mainly owing to undue emphasis on privacy and anonymity alongside performance efficiency. This article aims to show the design faults by analyzing a very recent hash functions-based authentication scheme for cloud-based IoT systems with misunderstood privacy cum efficiency tradeoff owing to an unadorned design flaw, which is also present in many other such schemes. Precisely, it is proved in this article that the scheme of Wazid et\xa0al. cannot provide mutual authentication and key agreement between a user and a sensor node when there exists more than one registered user. We then proposed an improved scheme and proved its security through formal and informal methods. The proposed scheme completes the authentication cycle with a minor increase in computation cost but provides all security goals along with privacy.}
}


@article{DBLP:journals/toit/CasconeCNNP21,
	author = {Lucia Cascone and
                  Aniello Castiglione and
                  Michele Nappi and
                  Fabio Narducci and
                  Ignazio Passero},
	title = {Waiting for Tactile: Robotic and Virtual Experiences in the Fog},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {79:1--79:19},
	year = {2021},
	url = {https://doi.org/10.1145/3421507},
	doi = {10.1145/3421507},
	timestamp = {Sat, 09 Apr 2022 12:33:23 +0200},
	biburl = {https://dblp.org/rec/journals/toit/CasconeCNNP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social robots adopt an emotional touch to interact with users inducing and transmitting humanlike emotions. Natural interaction with humans needs to be in real time and well grounded on the full availability of information on the environment. These robots base their way of communicating on direct interaction (touch, listening, view), supported by a range of sensors on the surrounding environment that provide a radially central and partial knowledge on it. Over the past few years, social robots have been demonstrated to implement different features, going from biometric applications to the fusion of machine learning environmental information collected on the edge. This article aims at describing the experiences performed and still ongoing and characterizes a simulation environment developed for the social robot Pepper that aims to foresee the new scenarios and benefits that tactile connectivity will enable.}
}


@article{DBLP:journals/toit/TanSYAJ21,
	author = {Liang Tan and
                  Na Shi and
                  Keping Yu and
                  Moayad Aloqaily and
                  Yaser Jararweh},
	title = {A Blockchain-empowered Access Control Framework for Smart Devices
                  in Green Internet of Things},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {3},
	pages = {80:1--80:20},
	year = {2021},
	url = {https://doi.org/10.1145/3433542},
	doi = {10.1145/3433542},
	timestamp = {Thu, 21 Apr 2022 09:15:17 +0200},
	biburl = {https://dblp.org/rec/journals/toit/TanSYAJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Green Internet of things (GIoT) generally refers to a new generation of Internet of things design concept. It can save energy and reduce emissions, reduce environmental pollution, waste of resources, and harm to human body and environment, in which green smart device (GSD) is a basic unit of GIoT for saving energy. With the access of a large number of heterogeneous bottom-layer GSDs in GIoT, user access and control of GSDs have become more and more complicated. Since there is no unified GSD management system, users need to operate different GIoT applications and access different GIoT cloud platforms when accessing and controlling these heterogeneous GSDs. This fragmented GSD management model not only increases the complexity of user access and control for heterogeneous GSDs, but also reduces the scalability of GSDs applications. To address this issue, this article presents a blockchain-empowered general GSD access control framework, which provides users with a unified GSD management platform. First, based on the World Wide Web Consortium (W3C) decentralized identifiers (DIDs) standard, users and GSD are issued visual identity (VID). Then, we extended the GSD-DIDs protocol to authenticate devices and users. Finally, based on the characteristics of decentralization and non-tampering of blockchain, a unified access control system for GSD was designed, including the registration, granting, and revoking of access rights. We implement and test on the Raspberry Pi device and the FISCO-BCOS alliance chain. The experimental results prove that the framework provides a unified and feasible way for users to achieve decentralized, lightweight, and fine-grained access control of GSDs. The solution reduces the complexity of accessing and controlling GSDs, enhances the scalability of GSD applications, as well as guarantees the credibility and immutability of permission data and identity data during access.}
}


@article{DBLP:journals/toit/OtoumKM21,
	author = {Safa Otoum and
                  Burak Kantarci and
                  Hussein T. Mouftah},
	title = {A Comparative Study of AI-Based Intrusion Detection Techniques in
                  Critical Infrastructures},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {81:1--81:22},
	year = {2021},
	url = {https://doi.org/10.1145/3406093},
	doi = {10.1145/3406093},
	timestamp = {Sat, 09 Apr 2022 12:33:23 +0200},
	biburl = {https://dblp.org/rec/journals/toit/OtoumKM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Volunteer computing uses Internet-connected devices (laptops, PCs, smart devices, etc.), in which their owners volunteer them as storage and computing power resources, has become an essential mechanism for resource management in numerous applications. The growth of the volume and variety of data traffic on the Internet leads to concerns on the robustness of cyberphysical systems especially for critical infrastructures. Therefore, the implementation of an efficient Intrusion Detection System for gathering such sensory data has gained vital importance. In this article, we present a comparative study of Artificial Intelligence (AI)-driven intrusion detection systems for wirelessly connected sensors that track crucial applications. Specifically, we present an in-depth analysis of the use of machine learning, deep learning and reinforcement learning solutions to recognise intrusive behavior in the collected traffic. We evaluate the proposed mechanisms by using KDD’99 as real attack dataset in our simulations. Results present the performance metrics for three different IDSs, namely the Adaptively Supervised and Clustered Hybrid IDS (ASCH-IDS), Restricted Boltzmann Machine-based Clustered IDS (RBC-IDS), and Q-learning based IDS (Q-IDS), to detect malicious behaviors. We also present the performance of different reinforcement learning techniques such as State-Action-Reward-State-Action Learning (SARSA) and the Temporal Difference learning (TD). Through simulations, we show that Q-IDS performs with  detection rate while SARSA-IDS and TD-IDS perform at the order of .}
}


@article{DBLP:journals/toit/AlizadehTSAJ21,
	author = {Mojtaba Alizadeh and
                  Mohammad Hesam Tadayon and
                  Kouichi Sakurai and
                  Hiroaki Anada and
                  Alireza Jolfaei},
	title = {A Secure Ticket-Based Authentication Mechanism for Proxy Mobile IPv6
                  Networks in Volunteer Computing},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {82:1--82:16},
	year = {2021},
	url = {https://doi.org/10.1145/3407189},
	doi = {10.1145/3407189},
	timestamp = {Thu, 23 Jun 2022 20:04:05 +0200},
	biburl = {https://dblp.org/rec/journals/toit/AlizadehTSAJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Technology advances—such as improving processing power, battery life, and communication functionalities—contribute to making mobile devices an attractive research area. In 2008, in order to manage mobility, the Internet Engineering Task Force (IETF) developed Proxy Mobile IPv6, which is a network-based mobility management protocol to support seamless connectivity of mobile devices. This protocol can play a key role in volunteer computing paradigms as a user can seamlessly access computing resources. The procedure of user authentication is not defined in this standard; thus, many studies have been carried out to propose suitable authentication schemes. However, in the current authentication methods, with reduced latency and packet loss, some security and privacy considerations are neglected. In this study, we propose a secure and anonymous ticket-based authentication (SATA) method to protect mobile nodes against existing security and privacy issues. The proposed method reduces the overhead of handover authentication procedures using the ticket-based concept. We evaluated security and privacy strengths of the proposed method using security theorems and BAN logic.}
}


@article{DBLP:journals/toit/LvCS21,
	author = {Zhihan Lv and
                  Dongliang Chen and
                  Amit Kumar Singh},
	title = {Big Data Processing on Volunteer Computing},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {83:1--83:20},
	year = {2021},
	url = {https://doi.org/10.1145/3409801},
	doi = {10.1145/3409801},
	timestamp = {Mon, 28 Aug 2023 21:41:18 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LvCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In order to calculate the node big data contained in complex networks and realize the efficient calculation of complex networks, based on voluntary computing, taking ICE middleware as the communication medium, the loose coupling distributed framework DCBV based on voluntary computing is proposed. Then, the Master, Worker, and MiddleWare layers in the framework, and the development structure of a DCBV framework are designed. The task allocation and recovery strategy, message passing and communication mode, and fault tolerance processing are discussed. Finally, to calculate and verify parameters such as the average shortest path of the framework and shorten calculation time, an improved accurate shortest path algorithm, the N-SPFA algorithm, is proposed. Under different datasets, the node calculation and performance of the N-SPFA algorithm are explored. The algorithm is compared with four approximate shortest-path algorithms: Combined Link and Attribute (CLA), Lexicographic Breadth First Search (LBFS), Approximate algorithm of shortest path length based on center distance of area division (CDZ), and Hub Vertex of area and Core Expressway (HEA-CE). The results show that when the number of CPU threads is 4, the computation time of the DCBV framework is the shortest (514.63 ms). As the number of CPU cores increases, the overall computation time of the framework decreases gradually. For every 2 additional CPU cores, the number of tasks increases by 1. When the number of Worker nodes is 8 and the number of nodes is 1, the computation time of the framework is the shortest (210,979 ms), and the IO statistics data increase with the increase of Worker nodes. When the datasets are Undirected01 and Undirected02, the computation time of the N-SPFA algorithm is the shortest, which is 4520 ms and 7324 ms, respectively. However, the calculation time in the ca-condmat_undirected dataset is 175,292 ms, and the performance is slightly worse. Overall, however, the performance of the N-SPFA and SPFA algorithms is good. Therefore, the two algorithms are combined. For networks with less complexity, the computational scale coefficient of the SPFA algorithm can be set to 0.06, and for general networks, 0.2. When compared with other algorithms in different datasets, the pretreatment time, average query time, and overall query time of N-SPFA algorithm are the shortest, being 49.67 ms, 5.12 ms, and 94,720 ms, respectively. The accuracy (1.0087) and error rate (0.024) are also the best. In conclusion, voluntary computing can be applied to the processing of big data, which has a good reference significance for the distributed analysis of large-scale complex networks.}
}


@article{DBLP:journals/toit/YuanLZYJ21,
	author = {Bin Yuan and
                  Chen Lin and
                  Deqing Zou and
                  Laurence Tianruo Yang and
                  Hai Jin},
	title = {Detecting Malicious Switches for a Secure Software-defined Tactile
                  Internet},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {84:1--84:23},
	year = {2021},
	url = {https://doi.org/10.1145/3415146},
	doi = {10.1145/3415146},
	timestamp = {Mon, 28 Aug 2023 21:41:18 +0200},
	biburl = {https://dblp.org/rec/journals/toit/YuanLZYJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of the Internet of Things has led to demand for high-speed data transformation. Serving this purpose is the Tactile Internet, which facilitates data transfer in extra-low latency. In particular, a Tactile Internet based on software-defined networking (SDN) has been broadly deployed because of the proven benefits of SDN in flexible and programmable network management. However, the vulnerabilities of SDN also threaten the security of the Tactile Internet. Specifically, an SDN controller relies on the network status (provided by the underlying switches) to make network decisions, e.g., calculating a routing path to deliver data in the Tactile Internet. Hence, the attackers can compromise the switches to jeopardize the SDN and further attack Tactile Internet systems. For example, an attacker can compromise switches to launch distributed denial-of-service attacks to overwhelm the SDN controller, which will disrupt all the applications in the Tactile Internet. In pursuit of a more secure Tactile Internet, the problem of abnormal SDN switches in the Tactile Internet is analyzed in this article, including the cause of abnormal switches and their influences on different network layers. Then we propose an approach that leverages the messages sent by all switches to identify abnormal switches, which adopts a linear structure to store historical messages at a relatively low cost. By mapping each flow message to the flow establishment model, our method can effectively identify malicious SDN switches in the Tactile Internet and thus enhance its security.}
}


@article{DBLP:journals/toit/MasudHAACIMRG21,
	author = {Mehedi Masud and
                  M. Shamim Hossain and
                  Hesham Alhumyani and
                  Sultan S. Alshamrani and
                  Omar Cheikhrouhou and
                  Saleh Ibrahim and
                  Ghulam Muhammad and
                  Amr Ezz El{-}Din Rashed and
                  B. B. Gupta},
	title = {Pre-Trained Convolutional Neural Networks for Breast Cancer Detection
                  Using Ultrasound Images},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {85:1--85:17},
	year = {2021},
	url = {https://doi.org/10.1145/3418355},
	doi = {10.1145/3418355},
	timestamp = {Thu, 23 Jun 2022 20:04:05 +0200},
	biburl = {https://dblp.org/rec/journals/toit/MasudHAACIMRG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Volunteer computing based data processing is a new trend in healthcare applications. Researchers are now leveraging volunteer computing power to train deep learning networks consisting of billions of parameters. Breast cancer is the second most common cause of death in women among cancers. The early detection of cancer may diminish the death risk of patients. Since the diagnosis of breast cancer manually takes lengthy time and there is a scarcity of detection systems, development of an automatic diagnosis system is needed for early detection of cancer. Machine learning models are now widely used for cancer detection and prediction research for improving the successive therapy of patients. Considering this need, this study implements pre-trained convolutional neural network based models for detecting breast cancer using ultrasound images. In particular, we tuned the pre-trained models for extracting key features from ultrasound images and included a classifier on the top layer. We measured accuracy of seven popular state-of-the-art pre-trained models using different optimizers and hyper-parameters through fivefold cross validation. Moreover, we consider Grad-CAM and occlusion mapping techniques to examine how well the models extract key features from the ultrasound images to detect cancers. We observe that after fine tuning, DenseNet201 and ResNet50 show 100% accuracy with Adam and RMSprop optimizers. VGG16 shows 100% accuracy using the Stochastic Gradient Descent optimizer. We also develop a custom convolutional neural network model with a smaller number of layers compared to large layers in the pre-trained models. The model also shows 100% accuracy using the Adam optimizer in classifying healthy and breast cancer patients. It is our belief that the model will assist healthcare experts with improved and faster patient screening and pave a way to further breast cancer research.}
}


@article{DBLP:journals/toit/HoseinyAST21,
	author = {Farooq Hoseiny and
                  Sadoon Azizi and
                  Mohammad Shojafar and
                  Rahim Tafazolli},
	title = {Joint QoS-aware and Cost-efficient Task Scheduling for Fog-cloud Resources
                  in a Volunteer Computing System},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {86:1--86:21},
	year = {2021},
	url = {https://doi.org/10.1145/3418501},
	doi = {10.1145/3418501},
	timestamp = {Tue, 19 Oct 2021 10:43:29 +0200},
	biburl = {https://dblp.org/rec/journals/toit/HoseinyAST21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Volunteer computing is an Internet-based distributed computing in which volunteers share their extra available resources to manage large-scale tasks. However, computing devices in a Volunteer Computing System (VCS) are highly dynamic and heterogeneous in terms of their processing power, monetary cost, and data transferring latency. To ensure both of the high Quality of Service (QoS) and low cost for different requests, all of the available computing resources must be used efficiently. Task scheduling is an NP-hard problem that is considered as one of the main critical challenges in a heterogeneous VCS. Due to this, in this article, we design two task scheduling algorithms for VCSs, named Min-CCV and Min-V. The main goal of the proposed algorithms is jointly minimizing the computation, communication, and delay violation cost for the Internet of Things (IoT) requests. Our extensive simulation results show that proposed algorithms are able to allocate tasks to volunteer fog/cloud resources more efficiently than the state-of-the-art. Specifically, our algorithms improve the deadline satisfaction task rates around 99.5% and decrease the total cost between 15 to 53% in comparison with the genetic-based algorithm.}
}


@article{DBLP:journals/toit/RidhawiAJ21,
	author = {Ismaeel Al Ridhawi and
                  Moayad Aloqaily and
                  Yaser Jararweh},
	title = {An Incentive-based Mechanism for Volunteer Computing Using Blockchain},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {87:1--87:22},
	year = {2021},
	url = {https://doi.org/10.1145/3419104},
	doi = {10.1145/3419104},
	timestamp = {Sat, 25 Dec 2021 15:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/toit/RidhawiAJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of fast communication media both at the core and at the edge has resulted in unprecedented numbers of sophisticated and intelligent wireless IoT devices. Tactile Internet has enabled the interaction between humans and machines within their environment to achieve revolutionized solutions both on the move and in real-time. Many applications such as intelligent autonomous self-driving, smart agriculture and industrial solutions, and self-learning multimedia content filtering and sharing have become attainable through cooperative, distributed, and decentralized systems, namely, volunteer computing. This article introduces a blockchain-enabled resource sharing and service composition solution through volunteer computing. Device resource, computing, and intelligence capabilities are advertised in the environment to be made discoverable and available for sharing with the aid of blockchain technology. Incentives in the form of on-demand service availability are given to resource and service providers to ensure fair and balanced cooperative resource usage. Blockchains are formed whenever a service request is initiated with the aid of fog and mobile edge computing (MEC) devices to ensure secure communication and service delivery for the participants. Using both volunteer computing techniques and tactile internet architectures, we devise a fast and reliable service provisioning framework that relies on a reinforcement learning technique. Simulation results show that the proposed solution can achieve high reward distribution, increased number of blockchain formations, reduced delays, and balanced resource usage among participants, under the premise of high IoT device availability.}
}


@article{DBLP:journals/toit/WuSM21,
	author = {Di Wu and
                  Wei Shi and
                  Xiangyu Ma},
	title = {A Novel Real-time Anti-spam Framework},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {88:1--88:27},
	year = {2021},
	url = {https://doi.org/10.1145/3423153},
	doi = {10.1145/3423153},
	timestamp = {Mon, 26 Jun 2023 20:52:11 +0200},
	biburl = {https://dblp.org/rec/journals/toit/WuSM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the most pervasive current modes of communication, email needs to be fast and reliable. However, spammers and attackers use it as a primary channel to conduct illegal activities. Although many approaches have been developed and evaluated for spam detection, they do not provide sufficient accuracy. This deficiency results in significant economic losses for organizations. In this article, we first propose a framework for creating novel spam filters using Keras to combine a Convolutional Neural Network (CNN) with Long Short-Term Memory (LSTM) classification models. We then use this framework to introduce a specific solution applicable to realistic scenarios involving dynamic incoming email data in real-time. This solution takes the form of a real-time content-based spam classifier. We evaluate its performance concerning accuracy, precision, recall, false-positive, and false-negative rates. Our experimental results show that our approach can significantly outperform existing solutions for real-time spam detection.}
}


@article{DBLP:journals/toit/WuTSPL21,
	author = {Jimmy Ming{-}Tai Wu and
                  Qian Teng and
                  Gautam Srivastava and
                  Matin Pirouz and
                  Jerry Chun{-}Wei Lin},
	title = {The Efficient Mining of Skyline Patterns from a Volunteer Computing
                  Network},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {89:1--89:20},
	year = {2021},
	url = {https://doi.org/10.1145/3423557},
	doi = {10.1145/3423557},
	timestamp = {Mon, 28 Aug 2023 21:41:17 +0200},
	biburl = {https://dblp.org/rec/journals/toit/WuTSPL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the ever-growing world, the concepts of High-utility Itemset Mining (HUIM) as well as Frequent Itemset Mining (FIM) are fundamental works in knowledge discovery. Several algorithms have been designed successfully. However, these algorithms only used one factor to estimate an itemset. In the past, skyline pattern mining by considering both aspects of frequency and utility has been extensively discussed. In most cases, however, people tend to focus on purchase quantities of itemsets rather than frequencies. In this article, we propose a new knowledge called skyline quantity-utility pattern (SQUP) to provide better estimations in the decision-making process by considering quantity and utility together. Two algorithms, respectively, called SQU-Miner and SKYQUP are presented to efficiently mine the set of SQUPs. Moreover, the usage of volunteer computing is proposed to show the potential in real supermarket applications. Two new efficient utility-max structures are also mentioned for the reduction of the candidate itemsets, respectively, utilized in SQU-Miner and SKYQUP. These two new utility-max structures are used to store the upper-bound of utility for itemsets under the quantity constraint instead of frequency constraint, and the second proposed utility-max structure moreover applies a recursive updated process to further obtain strict upper-bound of utility. Our in-depth experimental results prove that SKYQUP has stronger performance when a comparison is made to SQU-Miner in terms of memory usage, runtime, and the number of candidates.}
}


@article{DBLP:journals/toit/ChenHCWH21,
	author = {Yan{-}Chun Chen and
                  Ren{-}Hung Hwang and
                  Mu{-}Yen Chen and
                  Chih{-}Chin Wen and
                  Chih{-}Ping Hsu},
	title = {Screw Slot Quality Inspection System Based on Tactile Network},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {90:1--90:17},
	year = {2021},
	url = {https://doi.org/10.1145/3423556},
	doi = {10.1145/3423556},
	timestamp = {Wed, 07 Dec 2022 23:03:34 +0100},
	biburl = {https://dblp.org/rec/journals/toit/ChenHCWH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The popularity of 5G networks has made smart manufacturing not limited to high-tech industries such as semiconductors due to its high speed, ultra-high reliability, and low latency. With the advance of system on chip (SoC) design and manufacturing, 5G is also suitable for data transmission in harsh manufacturing environments such as high temperatures, dust, and extreme vibration. The defect of the screw head is caused by the wear and deformation of the die forming the head after mass production. Therefore, the screw quality inspection system based on the tactile network in this article monitors the production quality of the screw; the system will send a warning signal through the router to remind the technician to solve the production problem when the machine produces a defective product. Sensors are embedded into the traditional screw heading machine, and sensing data are transmitted through a gateway to the voluntary computing node for screw slot quality inspection. The anomaly detection data set collected by the screw heading machine has a ratio of anomaly to normal data of 0.006; thus, we propose a time-series deep AutoEncoder architecture for anomaly detection of screw slots. Our experimental results show that the proposed solution outperforms existing works in terms of efficiency and that the specificity and accuracy can reach 97% through the framework proposed in this article.}
}


@article{DBLP:journals/toit/LinZJLLL21,
	author = {Zhiyang Lin and
                  Jihua Zhu and
                  Zutao Jiang and
                  Yujie Li and
                  Yaochen Li and
                  Zhongyu Li},
	title = {Merging Grid Maps in Diverse Resolutions by the Context-based Descriptor},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {91:1--91:21},
	year = {2021},
	url = {https://doi.org/10.1145/3403948},
	doi = {10.1145/3403948},
	timestamp = {Fri, 28 Jun 2024 17:20:42 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LinZJLLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Building an accurate map is essential for autonomous robot navigation in the environment without GPS. Compared with single-robot, the multiple-robot system has much better performance in terms of accuracy, efficiency and robustness for the simultaneous localization and mapping (SLAM). As a critical component of multiple-robot SLAM, the problem of map merging still remains a challenge. To this end, this article casts it into point set registration problem and proposes an effective map merging method based on the context-based descriptors and correspondence expansion. It first extracts interest points from grid maps by the Harris corner detector. By exploiting neighborhood information of interest points, it automatically calculates the maximum response radius as scale information to compute the context-based descriptor, which includes eigenvalues and normals computed from local structures of each interest point. Then, it effectively establishes origin matches with low precision by applying the nearest neighbor search on the context-based descriptor. Further, it designs a scale-based corresponding expansion strategy to expand each origin match into a set of feature matches, where one similarity transformation between two grid maps can be estimated by the Random Sample Consensus algorithm. Subsequently, a measure function formulated from the trimmed mean square error is utilized to confirm the best similarity transformation and accomplish the coarse map merging. Finally, it utilizes the scaling trimmed iterative closest point algorithm to refine initial similarity transformation so as to achieve accurate merging. As the proposed method considers scale information in the context-based descriptor, it is able to merge grid maps in diverse resolutions. Experimental results on real robot datasets demonstrate its superior performance over other related methods on accuracy and robustness.}
}


@article{DBLP:journals/toit/LvQW21,
	author = {Zhihan Lv and
                  Liang Qiao and
                  Qingjun Wang},
	title = {Cognitive Robotics on 5G Networks},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {92:1--92:18},
	year = {2021},
	url = {https://doi.org/10.1145/3414842},
	doi = {10.1145/3414842},
	timestamp = {Sat, 25 Dec 2021 15:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/toit/LvQW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emotional cognitive ability is a key technical indicator to measure the friendliness of interaction. Therefore, this research aims to explore robots with human emotion cognitively. By discussing the prospects of 5G technology and cognitive robots, the main direction of the study is cognitive robots. For the emotional cognitive robots, the analysis logic similar to humans is difficult to imitate; the information processing levels of robots are divided into three levels in this study: cognitive algorithm, feature extraction, and information collection by comparing human information processing levels. In addition, a multi-scale rectangular direction gradient histogram is used for facial expression recognition, and robust principal component analysis algorithm is used for facial expression recognition. In the pictures where humans intuitively feel smiles in sad emotions, the proportion of emotions obtained by the method in this study are as follows: calmness accounted for 0%, sadness accounted for 15.78%, fear accounted for 0%, happiness accounted for 76.53%, disgust accounted for 7.69%, anger accounted for 0%, and astonishment accounted for 0%. In the recognition of micro-expressions, humans intuitively feel negative emotions such as surprise and fear, and the proportion of emotions obtained by the method adopted in this study are as follows: calmness accounted for 32.34%, sadness accounted for 34.07%, fear accounted for 6.79%, happiness accounted for 0%, disgust accounted for 0%, anger accounted for 13.91%, and astonishment accounted for 15.89%. Therefore, the algorithm explored in this study can realize accuracy in cognition of emotions. From the preceding research results, it can be seen that the research method in this study can intuitively reflect the proportion of human expressions, and the recognition methods based on facial expressions and micro-expressions have good recognition effects, which is in line with human intuitive experience.}
}


@article{DBLP:journals/toit/JinDZHLMSL21,
	author = {Xin Jin and
                  Yuwei Duan and
                  Ying Zhang and
                  Yating Huang and
                  Mengdong Li and
                  Ming Mao and
                  Amit Kumar Singh and
                  Yujie Li},
	title = {Fast Search of Lightweight Block Cipher Primitives via Swarm-like
                  Metaheuristics for Cyber Security},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {93:1--93:15},
	year = {2021},
	url = {https://doi.org/10.1145/3417296},
	doi = {10.1145/3417296},
	timestamp = {Fri, 12 Jan 2024 21:08:39 +0100},
	biburl = {https://dblp.org/rec/journals/toit/JinDZHLMSL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the construction and improvement of 5G infrastructure, more devices choose to access the Internet to achieve some functions. People are paying more attention to information security in the use of network devices. This makes lightweight block ciphers become a hotspot. A lightweight block cipher with superior performance can ensure the security of information while reducing the consumption of device resources. Traditional optimization tools, such as brute force or random search, are often used to solve the design of Symmetric-Key primitives. The metaheuristic algorithm was first used to solve the design of Symmetric-Key primitives of SKINNY. The genetic algorithm and the simulated annealing algorithm are used to increase the number of active S-boxes in SKINNY, thus improving the security of SKINNY. Based on this, to improve search efficiency and optimize search results, we design a novel metaheuristic algorithm, named particle swarm-like normal optimization algorithm (PSNO) to design the Symmetric-Key primitives of SKINNY. With our algorithm, one or better algorithm components can be obtained more quickly. The results in the experiments show that our search results are better than those of the genetic algorithm and the simulated annealing algorithm. The search efficiency is significantly improved. The algorithm we proposed can be generalized to the design of Symmetric-Key primitives of other lightweight block ciphers with clear evaluation indicators, where the corresponding indicators can be used as the objective functions.}
}


@article{DBLP:journals/toit/LanWHDSCL21,
	author = {Rushi Lan and
                  Jing Wang and
                  Wenming Huang and
                  Zhenrong Deng and
                  Xiyan Sun and
                  Zhuo Chen and
                  Xiaonan Luo},
	title = {Chinese Emotional Dialogue Response Generation via Reinforcement Learning},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {94:1--94:17},
	year = {2021},
	url = {https://doi.org/10.1145/3446390},
	doi = {10.1145/3446390},
	timestamp = {Tue, 19 Oct 2021 10:43:29 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LanWHDSCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In an open-domain dialogue system, recognition and expression of emotions are the key factors for success. Most of the existing research related to Chinese dialogue systems aims at improving the quality of content but ignores the expression of human emotions. In this article, we propose a Chinese emotional dialogue response generation algorithm based on reinforcement learning that can generate responses not only according to content but also according to emotion. In the proposed method, a multi-emotion classification model is first used to add emotion labels to the corpus of post-response pairs. Then, with the help of reinforcement learning, the reward function is constructed based on two aspects, namely, emotion and content. Among the generated candidates, the system selects the one with long-term success as the best reply. At the same time, to avoid safe responses and diversify dialogue, a diversity beam search algorithm is applied in the decoding process. The comparative experiments demonstrate that the proposed model achieves satisfactory results according to both automatic and human evaluations.}
}


@article{DBLP:journals/toit/DengCDGW21,
	author = {Song Deng and
                  Fulin Chen and
                  Xia Dong and
                  Guangwei Gao and
                  Xindong Wu},
	title = {Short-term Load Forecasting by Using Improved {GEP} and Abnormal Load
                  Recognition},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {95:1--95:28},
	year = {2021},
	url = {https://doi.org/10.1145/3447513},
	doi = {10.1145/3447513},
	timestamp = {Tue, 19 Oct 2021 10:43:29 +0200},
	biburl = {https://dblp.org/rec/journals/toit/DengCDGW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Load forecasting in short term is very important to economic dispatch and safety assessment of power system. Although existing load forecasting in short-term algorithms have reached required forecast accuracy, most of the forecasting models are black boxes and cannot be constructed to display mathematical models. At the same time, because of the abnormal load caused by the failure of the load data collection device, time synchronization, and malicious tampering, the accuracy of the existing load forecasting models is greatly reduced. To address these problems, this article proposes a Short-Term Load Forecasting algorithm by using Improved Gene Expression Programming and Abnormal Load Recognition (STLF-IGEP_ALR). First, the Recognition algorithm of Abnormal Load based on Probability Distribution and Cross Validation is proposed. By analyzing the probability distribution of rows and columns in load data, and using the probability distribution of rows and columns for cross-validation, misjudgment of normal load in abnormal load data can be better solved. Second, by designing strategies for adaptive generation of population parameters, individual evolution of populations and dynamic adjustment of genetic operation probability, an Improved Gene Expression Programming based on Evolutionary Parameter Optimization is proposed. Finally, the experimental results on two real load datasets and one open load dataset show that compared with the existing abnormal data detection algorithms, the algorithm proposed in this article have higher advantages in missing detection rate, false detection rate and precision rate, and STLF-IGEP_ALR is superior to other short-term load forecasting algorithms in terms of the convergence speed, MAE, MAPE, RSME, and R2.}
}


@article{DBLP:journals/toit/PradhanHRSL21,
	author = {Buddhadeb Pradhan and
                  Nirmal Baran Hui and
                  Diptendu Sinha Roy and
                  Gautam Srivastava and
                  Jerry Chun{-}Wei Lin},
	title = {Game-Theoretic Strategic Coordination and Navigation of Multiple Wheeled
                  Robots},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {96:1--96:15},
	year = {2021},
	url = {https://doi.org/10.1145/3450521},
	doi = {10.1145/3450521},
	timestamp = {Thu, 05 Jan 2023 14:06:40 +0100},
	biburl = {https://dblp.org/rec/journals/toit/PradhanHRSL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multiple robots negotiating in a dynamic workspace may lead to collisions. To avoid such issues, multi-robot navigation and coordination becomes necessary but is computationally very challenging, particularly when there are many robots. This article addresses the problem of multi-robot navigation where individual robots require coordination. Although a few such attempts for modeling multi-robot coordination and navigation have been studied, this work proposes a game-theoretic coordination strategy, also referred to as strategic coordination. We make use of a genetic algorithm tuned fuzzy logic–based motion planner. The proposed strategic coordination strategy has been pitted against a basic potential field-based motion planner, also referred to as the heuristic method, for performance comparison. Results are compared through computer simulation with 8 to 17 robots at different rounds. From the obtained results, it was observed that the proposed coordination scheme’s efficacy is strong for a larger number of robots. In addition, the proposed strategic coordination scheme with the genetic-fuzzy-based motion planner was found to outperform other combinations as far as the quality of solutions and time to reach the goal positions. The computational complexity of different methods has also been compared and presented.}
}


@article{DBLP:journals/toit/ChenXHMZT21,
	author = {Min Chen and
                  Wenjing Xiao and
                  Long Hu and
                  Yujun Ma and
                  Yin Zhang and
                  Guangming Tao},
	title = {Cognitive Wearable Robotics for Autism Perception Enhancement},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {97:1--97:16},
	year = {2021},
	url = {https://doi.org/10.1145/3450630},
	doi = {10.1145/3450630},
	timestamp = {Mon, 28 Aug 2023 21:41:17 +0200},
	biburl = {https://dblp.org/rec/journals/toit/ChenXHMZT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autism spectrum disorder (ASD) is a serious hazard to the physical and mental health of children, which limits the social activities of patients throughout their lives and places a heavy burden on families and society. The developments of communication techniques and artificial intelligence (AI) have provided new potential methods for the treatment of autism. The existing treatment systems based on AI for children with ASD focus on detecting health status and developing social skills. However, the contradiction between the terminal interaction capability and availability cannot meet the needs for real application scenarios. At the same time, the lack of diverse data cannot provide individualized care for autistic children. To explore this robot-based approach, a novel AI-based first-view-robot architecture is proposed in this article. By providing care from the first-person perspective, the proposed wearable robot overcomes the difficulty of the absence of cognitive ability in the third-view of traditional robotics and improves the social interaction ability of children with ASD. The first-view-robot architecture meets the requirements of dynamic, individualized, and highly immersed interaction services for autistic children. First, the multi-modal and multi-scene data collection processes of standard, static, and dynamic datasets are introduced in detail. Then, to comprehensively evaluate the learning ability of children with ASD through mental states and external performances, a learning assessment model with emotion correction is proposed. Besides, a wearable robot-assisted environment perception and expression enhancement mechanism for children with ASD is realized by reinforcement learning, which can be adapted to interactive environments with optimal action policies. An interactive testbed for children with ASD treatments is demonstrated and experimental cases for test subjects are presented. Last, three open issues are discussed from data processing, robot designing, and service responding perspectives.}
}


@article{DBLP:journals/toit/LuYWWJH21,
	author = {Wenpeng Lu and
                  Rui Yu and
                  Shoujin Wang and
                  Can Wang and
                  Ping Jian and
                  Heyan Huang},
	title = {Sentence Semantic Matching Based on 3D {CNN} for Human-Robot Language
                  Interaction},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {98:1--98:24},
	year = {2021},
	url = {https://doi.org/10.1145/3450520},
	doi = {10.1145/3450520},
	timestamp = {Tue, 21 Mar 2023 21:15:18 +0100},
	biburl = {https://dblp.org/rec/journals/toit/LuYWWJH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The development of cognitive robotics brings an attractive scenario where humans and robots cooperate to accomplish specific tasks. To facilitate this scenario, cognitive robots are expected to have the ability to interact with humans with natural language, which depends on natural language understanding (NLU) technologies. As one core task in NLU, sentence semantic matching (SSM) has widely existed in various interaction scenarios. Recently, deep learning–based methods for SSM have become predominant due to their outstanding performance. However, each sentence consists of a sequence of words, and it is usually viewed as one-dimensional (1D) text, leading to the existing available neural models being restricted into 1D sequential networks. A few researches attempt to explore the potential of 2D or 3D neural models in text representation. However, it is hard for their works to capture the complex features in texts, and thus the achieved performance improvement is quite limited. To tackle this challenge, we devise a novel 3D CNN-based SSM (3DSSM) method for human–robot language interaction. Specifically, first, a specific architecture called feature cube network is designed to transform a 1D sentence into a multi-dimensional representation named as semantic feature cube. Then, a 3D CNN module is employed to learn a semantic representation for the semantic feature cube by capturing both the local features embedded in word representations and the sequential information among successive words in a sentence. Given a pair of sentences, their representations are concatenated together to feed into another 3D CNN to capture the interactive features between them to generate the final matching representation. Finally, the semantic matching degree is judged with the sigmoid function by taking the learned matching representation as the input. Extensive experiments on two real-world datasets demonstrate that 3DSSM is able to achieve comparable or even better performance over the state-of-the-art competing methods.}
}


@article{DBLP:journals/toit/LvQSW21,
	author = {Zhihan Lv and
                  Liang Qiao and
                  Amit Kumar Singh and
                  Qingjun Wang},
	title = {AI-empowered IoT Security for Smart Cities},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {99:1--99:21},
	year = {2021},
	url = {https://doi.org/10.1145/3406115},
	doi = {10.1145/3406115},
	timestamp = {Sat, 09 Apr 2022 12:33:23 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LvQSW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart cities fully utilize the new generation of Internet of Things (IoT) technology in the process of urban informatization to optimize the urban management and service. However, in the IoT system, while information exchange and communication, wireless sensor network devices may not be able to resist all forms of attacks, which may lead to security issues such as user data disclosure. Aiming at the information security risks in smart city, the typical technologies in IoT is analyzed from the perspective of IoT perception layer and provides corresponding security solutions for the existing security threats. Regarding the communication security, the emerging wireless technology, long range (LoRa), is discussed, and the performance of wireless communication protocol is analyzed through simulation experiments, to verify that the IoT technology based on LoRa communication technology can improve the security of the system in the construction of smart city. The results show that REBEB, a new backoff algorithm, is similar to the binary exponential backoff algorithm in terms of throughput performance. REBEB focuses more on fairness, which is up to 0.985, and to a certain extent, its security is significantly improved. The fairness of REBEB algorithm is more than 0.4 in different nodes and competing windows, and the fairness of the system is better when the number of nodes is small. To sum up, the IoT system based on LoRa communication can effectively improve the security performance of the system in the construction of smart city and avoid the security threats in the IoT signal transmission.}
}


@article{DBLP:journals/toit/SekaranPA21,
	author = {Ramesh Sekaran and
                  Rizwan Patan and
                  Fadi Al{-}Turjman},
	title = {A Novel Approach for Efficient Packet Transmission in Volunteered
                  Computing {MANET}},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {100:1--100:15},
	year = {2021},
	url = {https://doi.org/10.1145/3418203},
	doi = {10.1145/3418203},
	timestamp = {Tue, 07 May 2024 20:27:47 +0200},
	biburl = {https://dblp.org/rec/journals/toit/SekaranPA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A mobile ad hoc network (MANET) is summarized as a combination device that can move, synchronize and converse without any preceding management. Enhancing the lifetime energy is based on the status of the concerned channel. The node is accomplished of control the control messages. Due to unplanned methods of energy conservation, the node lifespan and quality of packet flow is defaced in the existing solution. It results in a network-to-node-energy trade-off, ensuing in a failure of the post-network. This failure results in reduced time-to-live and higher overhead. This paper discusses an effective buffer management mechanism, in addition to proposing a novel performance modeling in Volunteered Computing MANET and tactile internet Next, the best execution the nodes can accomplish under fractional data is completely portrayed for utilities for a general purpose. To associate the space between network efficiency and energy conservation based on the minimal overhead, this article proposes a switch state promoting mutual Optimized MAC protocol for conservation of a node's energy and the optimal use of available nodes before their energy drain. Simulation results are provided as proof of the proposed solution. The simulation results are compared with the existing system with performance measures of delay, throughput, energy consumption, and availability of the node.}
}


@article{DBLP:journals/toit/ShahidSAKDKM21,
	author = {Huniya Shahid and
                  Munam Ali Shah and
                  Ahmad Almogren and
                  Hasan Ali Khattak and
                  Ikram Ud Din and
                  Neeraj Kumar and
                  Carsten Maple},
	title = {Machine Learning-based Mist Computing Enabled Internet of Battlefield
                  Things},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {101:1--101:26},
	year = {2021},
	url = {https://doi.org/10.1145/3418204},
	doi = {10.1145/3418204},
	timestamp = {Tue, 21 Mar 2023 21:15:18 +0100},
	biburl = {https://dblp.org/rec/journals/toit/ShahidSAKDKM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement in information and communication technology has revolutionized military departments and their operations. This advancement also gave birth to the idea of the Internet of Battlefield Things (IoBT). The IoBT refers to the fusion of the Internet of Things (IoT) with military operations on the battlefield. Various IoBT-based frameworks have been developed for the military. Nonetheless, many of these frameworks fail to maintain a high Quality of Service (QoS) due to the demanding and critical nature of IoBT. This study makes the use of mist computing while leveraging machine learning. Mist computing places computational capabilities on the edge itself (mist nodes), e.g., on end devices, wearables, sensors, and micro-controllers. This way, mist computing not only decreases latency but also saves power consumption and bandwidth as well by eliminating the need to communicate all data acquired, produced, or sensed. A mist-based version of the IoTNetWar framework is also proposed in this study. The mist-based IoTNetWar framework is a four-layer structure that aims at decreasing latency while maintaining QoS. Additionally, to further minimize delays, mist nodes utilize machine learning. Specifically, they use the delay-based K nearest neighbour algorithm for device-to-device communication purposes. The primary research objective of this work is to develop a system that is not only energy, time, and bandwidth-efficient, but it also helps military organizations with time-critical and resources-critical scenarios to monitor troops. By doing so, the system improves the overall decision-making process in a military campaign or battle. The proposed work is evaluated with the help of simulations in the EdgeCloudSim. The obtained results indicate that the proposed framework can achieve decreased network latency of 0.01 s and failure rate of 0.25% on average while maintaining high QoS in comparison to existing solutions.}
}


@article{DBLP:journals/toit/KuangZLG21,
	author = {Li Kuang and
                  Jianbo Zheng and
                  Kemu Li and
                  Honghao Gao},
	title = {Intelligent Traffic Signal Control Based on Reinforcement Learning
                  with State Reduction for Smart Cities},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {102:1--102:24},
	year = {2021},
	url = {https://doi.org/10.1145/3418682},
	doi = {10.1145/3418682},
	timestamp = {Sun, 02 Oct 2022 15:51:54 +0200},
	biburl = {https://dblp.org/rec/journals/toit/KuangZLG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient signal control at isolated intersections is vital for relieving congestion, accidents, and environmental pollution caused by increasing numbers of vehicles. However, most of the existing studies not only ignore the constraint of the limited computing resources available at isolated intersections but also the matching degree between the signal timing and the traffic demand, leading to high complexity and reduced learning efficiency. In this article, we propose a traffic signal control method based on reinforcement learning with state reduction. First, a reinforcement learning model is established based on historical traffic flow data, and we propose a dual-objective reward function that can reduce vehicle delay and improve the matching degree between signal time allocation and traffic demand, allowing the agent to learn the optimal signal timing strategy quickly. Second, the state and action spaces of the model are preliminarily reduced by selecting a proper control phase combination; then, the state space is further reduced by eliminating rare or nonexistent states based on the historical traffic flow. Finally, a simplified Q-table is generated and used to optimize the complexity of the control algorithm. The results of simulation experiments show that our proposed control algorithm effectively improves the capacity of isolated intersections while reducing the time and space costs of the signal control algorithm.}
}


@article{DBLP:journals/toit/MarquesEAMHCDC21,
	author = {Rafael Salema Marques and
                  Gregory Epiphaniou and
                  Haider M. Al{-}Khateeb and
                  Carsten Maple and
                  Mohammad Hammoudeh and
                  Paulo Andr{\'{e}} Lima de Castro and
                  Ali Dehghantanha and
                  Kim{-}Kwang Raymond Choo},
	title = {A Flow-based Multi-agent Data Exfiltration Detection Architecture
                  for Ultra-low Latency Networks},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {103:1--103:30},
	year = {2021},
	url = {https://doi.org/10.1145/3419103},
	doi = {10.1145/3419103},
	timestamp = {Sun, 02 Oct 2022 15:51:54 +0200},
	biburl = {https://dblp.org/rec/journals/toit/MarquesEAMHCDC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern network infrastructures host converged applications that demand rapid elasticity of services, increased security, and ultra-fast reaction times. The Tactile Internet promises to facilitate the delivery of these services while enabling new economies of scale for high fidelity of machine-to-machine and human-to-machine interactions. Unavoidably, critical mission systems served by the Tactile Internet manifest high demands not only for high speed and reliable communications but equally, the ability to rapidly identify and mitigate threats and vulnerabilities. This article proposes a novel Multi-Agent Data Exfiltration Detector Architecture (MADEX), inspired by the mechanisms and features present in the human immune system. MADEX seeks to identify data exfiltration activities performed by evasive and stealthy malware that hides malicious traffic from an infected host in low-latency networks. Our approach uses cross-network traffic information collected by agents to effectively identify unknown illicit connections by an operating system subverted. MADEX does not require prior knowledge of the characteristics or behavior of the malicious code or a dedicated access to a knowledge repository. We tested the performance of MADEX in terms of its capacity to handle real-time data and the sensitivity of our algorithm’s classification when exposed to malicious traffic. Experimental evaluation results show that MADEX achieved 99.97% sensitivity, 98.78% accuracy, and an error rate of 1.21% when compared to its best rivals. We created a second version of MADEX, called MADEX level 2, that further improves its overall performance with a slight increase in computational complexity. We argue for the suitability of MADEX level 1 in non-critical environments, while MADEX level 2 can be used to avoid data exfiltration in critical mission systems. To the best of our knowledge, this is the first article in the literature that addresses the detection of rootkits real-time in an agnostic way using an artificial immune system approach while it satisfies strict latency requirements.}
}


@article{DBLP:journals/toit/LvQVK21,
	author = {Zhihan Lv and
                  Liang Qiao and
                  Sahil Verma and
                  Kavita},
	title = {AI-enabled IoT-Edge Data Analytics for Connected Living},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {104:1--104:20},
	year = {2021},
	url = {https://doi.org/10.1145/3421510},
	doi = {10.1145/3421510},
	timestamp = {Tue, 07 May 2024 20:27:47 +0200},
	biburl = {https://dblp.org/rec/journals/toit/LvQVK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As deep learning, virtual reality, and other technologies become mature, real-time data processing applications running on intelligent terminals are emerging endlessly; meanwhile, edge computing has developed rapidly and has become a popular research direction in the field of distributed computing. Edge computing network is a network computing environment composed of multi-edge computing nodes and data centers. First, the edge computing framework and key technologies are analyzed to improve the performance of real-time data processing applications. In the system scenario where the collaborative deployment tasks of multi-edge nodes and data centers are considered, the stream processing task deployment process is formally described, and an efficient multi-edge node-computing center collaborative task deployment algorithm is proposed, which solves the problem of copy-free task deployment in the task deployment problem. Furthermore, a heterogeneous edge collaborative storage mechanism with tight coupling of computing and data is proposed, which solves the contradiction between the limited computing and storage capabilities of data and intelligent terminals, thereby improving the performance of data processing applications. Here, a Feasible Solution (FS) algorithm is designed to solve the problem of placing copy-free data processing tasks in the system. The FS algorithm has excellent results once considering the overall coordination. Under light load, the V value is reduced by 73% compared to the Only Data Center-available (ODC) algorithm and 41% compared to the Hash algorithm. Under heavy load, the V value is reduced by 66% compared to the ODC algorithm and 35% compared to the Hash algorithm. The algorithm has achieved good results after considering the overall coordination and cooperation and can more effectively use the bandwidth of edge nodes to transmit and process data stream, so that more tasks can be deployed in edge computing nodes, thereby saving time for data transmission to the data centers. The end-to-end collaborative real-time data processing task scheduling mechanism proposed here can effectively avoid the disadvantages of long waiting times and unable to obtain the required data, which significantly improves the success rate of the task and thus ensures the performance of real-time data processing.}
}


@article{DBLP:journals/toit/GomathyJASRVR21,
	author = {V. Gomathy and
                  K. Janarthanan and
                  Fadi Al{-}Turjman and
                  Ramachandran Sitharthan and
                  M. Rajesh and
                  Krishnasamy Vengatesan and
                  T. Priya Reshma},
	title = {Investigating the Spread of Coronavirus Disease via Edge-AI and Air
                  Pollution Correlation},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {105:1--105:10},
	year = {2021},
	url = {https://doi.org/10.1145/3424222},
	doi = {10.1145/3424222},
	timestamp = {Sat, 06 Jul 2024 18:17:15 +0200},
	biburl = {https://dblp.org/rec/journals/toit/GomathyJASRVR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Coronavirus Disease 19 (COVID-19) is a highly infectious viral disease affecting millions of people worldwide in 2020. Several studies have shown that COVID-19 results in a severe acute respiratory syndrome and may lead to death. In past research, a greater number of respiratory diseases has been caused by exposure to air pollution for long periods of time. This article investigates the spread of COVID-19 as a result of air pollution by applying linear regression in machine learning method based edge computing. The analysis in this investigation have been based on the death rates caused by COVID-19 as well as the region of death rates based on hazardous air pollution using data retrieved from the Copernicus Sentinel-5P satellite. The results obtained in the investigation prove that the mortality rate due to the spread of COVID-19 is 77% higher in areas with polluted air. This investigation also proves that COVID-19 severely affected 68% of the individuals who had been exposed to polluted air.}
}


@article{DBLP:journals/toit/WangCZGKW21,
	author = {Wei Wang and
                  Junyang Chen and
                  Yushu Zhang and
                  Zhiguo Gong and
                  Neeraj Kumar and
                  Wei Wei},
	title = {A Multi-graph Convolutional Network Framework for Tourist Flow Prediction},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {21},
	number = {4},
	pages = {106:1--106:13},
	year = {2021},
	url = {https://doi.org/10.1145/3424220},
	doi = {10.1145/3424220},
	timestamp = {Sat, 30 Sep 2023 10:29:30 +0200},
	biburl = {https://dblp.org/rec/journals/toit/WangCZGKW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advancement of Cyber Physic Systems and Social Internet of Things, the tourism industry is facing challenges and opportunities. We can now able to collect, store, and analyze large amounts of travel data. With the help of data science and artificial intelligence, smart tourism enables tourists with great autonomy and convenience for an intelligent trip. It is of great significance to make full use of these massive data to provide better services for smart tourism. However, due to the skewed and imbalanced visiting for point of interest located at different places, it is of great significance to predict the tourist flow of each place, which can help the service providers for designing a better schedule visiting strategy in advance. Against this background, this article proposes a multi-graph convolutional network framework, named AMOUNT, for tourist flow prediction. To capture the diverse relationships among POIs, AMOUNT first constructs three subgraphs, including the geographical graph, interaction graph, and the co-relation graph. Then, a multi-graph convolution network is utilized to predict the future tourist flow. Experimental results on two real-world datasets indicate that the proposed AMOUNT model outperforms all other baseline tourist flow prediction approaches.}
}
