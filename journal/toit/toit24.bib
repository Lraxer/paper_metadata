@article{DBLP:journals/toit/SongCLB24,
	author = {Zhiyi Song and
                  Dipankar Chaki and
                  Abdallah Lakhdari and
                  Athman Bouguettaya},
	title = {Positional Encoding-based Resident Identification in Multi-resident
                  Smart Homes},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {1},
	pages = {1:1--1:27},
	year = {2024},
	url = {https://doi.org/10.1145/3631353},
	doi = {10.1145/3631353},
	timestamp = {Mon, 01 Apr 2024 11:15:45 +0200},
	biburl = {https://dblp.org/rec/journals/toit/SongCLB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a novel resident identification framework to identify residents in a multi-occupant smart environment. The proposed framework employs a feature extraction model based on the concepts of positional encoding. The feature extraction model considers the locations of homes as a graph. We design a novel algorithm to build such graphs from layout maps of smart environments. The Node2Vec algorithm is used to transform the graph into high-dimensional node embeddings. A Long Short-Term Memory model is introduced to predict the identities of residents using temporal sequences of sensor events with the node embeddings. Extensive experiments show that our proposed scheme effectively identifies residents in a multi-occupant environment. Evaluation results on two real-world datasets demonstrate that our proposed approach achieves 94.5% and 87.9% accuracy, respectively.}
}


@article{DBLP:journals/toit/PanSZ24,
	author = {Bofeng Pan and
                  Natalia Stakhanova and
                  Zhongwen Zhu},
	title = {EtherShield: Time-interval Analysis for Detection of Malicious Behavior
                  on Ethereum},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {1},
	pages = {2:1--2:30},
	year = {2024},
	url = {https://doi.org/10.1145/3633514},
	doi = {10.1145/3633514},
	timestamp = {Sun, 19 Jan 2025 14:17:13 +0100},
	biburl = {https://dblp.org/rec/journals/toit/PanSZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advances in blockchain technology have attracted significant attention across the world. The practical blockchain applications emerging in various domains, ranging from finance, healthcare, and entertainment, have quickly become attractive targets for adversaries. The novelty of the technology coupled with the high degree of anonymity it provides made malicious activities even less visible in the blockchain environment. This made their robust detection challenging. This article presents EtherShield, a novel approach for identifying malicious activity on the Ethereum blockchain. By combining temporal transaction information and contract code characteristics, EtherShield can detect various types of threats and provide insight into the behavior of contracts. The time-interval-based analysis used by EtherShield enables expedited detection, achieving comparable accuracy to other approaches with significantly less data. Our validation analysis, which involved over 15,000 Ethereum accounts, demonstrated that EtherShield can significantly expedite the detection of malicious activity while maintaining high accuracy levels (86.52% accuracy with 1 hour of transaction history data and 91.33% accuracy with 1 year of transaction history data).}
}


@article{DBLP:journals/toit/DongWF24,
	author = {Chao Dong and
                  Fang Wang and
                  Dan Feng},
	title = {DxHash: {A} Memory-saving Consistent Hashing Algorithm},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {1},
	pages = {3:1--3:22},
	year = {2024},
	url = {https://doi.org/10.1145/3631708},
	doi = {10.1145/3631708},
	timestamp = {Sun, 19 Jan 2025 14:17:15 +0100},
	biburl = {https://dblp.org/rec/journals/toit/DongWF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consistent Hashing (CH) algorithms are widely adopted in networks and distributed systems for their ability to achieve load balancing and minimize disruptions. However, the rise of the Internet of Things (IoT) has introduced new challenges for existing CH algorithms, characterized by high memory usage and update overhead. This article presents DxHash, a novel CH algorithm based on repeated pseudo-random number generation. DxHash offers significant benefits, including a remarkably low memory footprint, high lookup throughput, and minimal update overhead. Additionally, we introduce a weighted variant of DxHash, enabling adaptable weight adjustments to handle heterogeneous load distribution. Through extensive evaluation, we demonstrate that DxHash outperforms AnchorHash, a state-of-the-art CH algorithm, in terms of the reduction of up to 98.4% in memory footprint and comparable performance in lookup and update.}
}


@article{DBLP:journals/toit/WuDKYXW24,
	author = {Jiashu Wu and
                  Hao Dai and
                  Kenneth B. Kent and
                  Jerome Yen and
                  Cheng{-}Zhong Xu and
                  Yang Wang},
	title = {Open Set Dandelion Network for IoT Intrusion Detection},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {1},
	pages = {4:1--4:26},
	year = {2024},
	url = {https://doi.org/10.1145/3639822},
	doi = {10.1145/3639822},
	timestamp = {Mon, 01 Apr 2024 11:15:45 +0200},
	biburl = {https://dblp.org/rec/journals/toit/WuDKYXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Internet of Things devices become widely used in the real-world, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this article, we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactness. The dandelion-based target membership mechanism then forms the target dandelion. Then, the dandelion angular separation mechanism achieves better inter-category separability, and the dandelion embedding alignment mechanism further aligns both dandelions in a finer manner. To promote intra-category compactness, the discriminating sampled dandelion mechanism is used. Assisted by the intrusion classifier trained using both known and generated unknown intrusion knowledge, a semantic dandelion correction mechanism emphasises easily-confused categories and guides better inter-category separability. Holistically, these mechanisms form the OSDN model that effectively performs intrusion knowledge transfer to benefit IoT intrusion detection. Comprehensive experiments on several intrusion datasets verify the effectiveness of the OSDN model, outperforming three state-of-the-art baseline methods by 16.9%. The contribution of each OSDN constituting component, the stability and the efficiency of the OSDN model are also verified.}
}


@article{DBLP:journals/toit/YangZLXZ24,
	author = {Xuezheng Yang and
                  Zhiwen Zeng and
                  Anfeng Liu and
                  Neal N. Xiong and
                  Shaobo Zhang},
	title = {{ADTO:} {A} Trust Active Detecting-based Task Offloading Scheme in
                  Edge Computing for Internet of Things},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {1},
	pages = {5:1--5:25},
	year = {2024},
	url = {https://doi.org/10.1145/3640013},
	doi = {10.1145/3640013},
	timestamp = {Sun, 19 Jan 2025 14:17:12 +0100},
	biburl = {https://dblp.org/rec/journals/toit/YangZLXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In edge computing, Internet of Things devices with weak computing power offload tasks to nearby edge servers for execution, so the task completion time can be reduced and delay-sensitive tasks can be facilitated. However, if the task is offloaded to malicious edge servers, then the system will suffer losses. Therefore, it is significant to identify the trusted edge servers and offload tasks to trusted edge servers, which can improve the performance of edge computing. However, it is still challenging. In this article, a trust Active Detecting-based Task Offloading (ADTO) scheme is proposed to maximize revenue in edge computing. The main innovation points of our work are as follows: (a) The ADTO scheme innovatively proposes a method to actively get trust by trust detection. This method offloads microtasks to edge servers whose trust needs to be identified, and then quickly identifies the trust of edge servers according to the completion of tasks by edge servers. Based on the identification of the trust, tasks can be offloaded to trusted edge servers, to improve the success rate of tasks. (b) Although the trust of edge servers can be identified by our detection, it needs to pay a price. Therefore, to maximize system revenue, searching the most suitable number of trusted edge servers for various conditions is transformed into an optimization problem. Finally, theoretical and experimental analysis shows the effectiveness of the proposed strategy, which can effectively identify the trusted and untrusted edge servers. The task offloading strategy based on trust detection proposed in this article greatly improves the success rate of tasks, compared with the strategy without trust detection, the task success rate is increased by 40.27%, and there is a significant increase in revenue, which fully demonstrates the effectiveness of the strategy.}
}


@article{DBLP:journals/toit/ChenL24,
	author = {Yi{-}Cheng Chen and
                  Wang{-}Chien Lee},
	title = {A Novel Cross-Domain Recommendation with Evolution Learning},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {1},
	pages = {6:1--6:23},
	year = {2024},
	url = {https://doi.org/10.1145/3639567},
	doi = {10.1145/3639567},
	timestamp = {Mon, 01 Apr 2024 11:15:45 +0200},
	biburl = {https://dblp.org/rec/journals/toit/ChenL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this “info-plosion” era, recommendation systems (or recommenders) play a significant role in finding interesting items in the surge of online digital activities and e-commerce. Several techniques have been widely applied for recommendation systems, but the cold-start and sparsity problems remain a major challenge. The cold-start\xa0problem occurs when generating recommendations for new users and items without sufficient information. Sparsity refers to the problem of having a large amount of users and items but with few transactions or interactions. In this article, a novel cross-domain recommendation model, Cross-Domain Evolution Learning Recommendation (abbreviated as CD-ELR), is developed to communicate the information from different domains in order to tackle the cold-start and sparsity issues by integrating matrix factorization and recurrent neural network. We introduce an evolutionary concept to describe the preference variation of users over time. Furthermore, several optimization methods are developed for combining the domain features for precision recommendation. Experimental results show that CD-ELR outperforms existing state-of-the-art recommendation baselines. Finally, we conduct experiments on several real-world datasets to demonstrate the practicability of the proposed CD-ELR.}
}


@article{DBLP:journals/toit/SachanK24,
	author = {Anuj Sachan and
                  Neetesh Kumar},
	title = {SDN-enabled Quantized {LQR} for Smart Traffic Light Controller to
                  Optimize Congestion},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {1},
	pages = {7:1--7:25},
	year = {2024},
	url = {https://doi.org/10.1145/3641104},
	doi = {10.1145/3641104},
	timestamp = {Sun, 19 Jan 2025 14:17:15 +0100},
	biburl = {https://dblp.org/rec/journals/toit/SachanK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing intersection management systems, in urban cities, lack in meeting the current requirements of self-configuration, lightweight computing, and software-defined control, which are necessarily required for congested road-lane networks. To satisfy these requirements, this work proposes effective, scalable, multi-input and multi-output, and congestion prevention-enabled intersection management system utilizing a software-defined control interface that not only regularly monitors the traffic to prevent congestion for minimizing queue length and waiting time but also offers a computationally efficient solution in real-time. For effective intersection management, a modified linear-quadratic regulator, i.e., Quantized Linear Quadratic Regulator (QLQR), is designed along with Software-defined Networking (SDN)-enabled control interface to maximize throughput and vehicles speed and minimize queue length and waiting time at the intersection. Experimental results prove that the proposed SDN-QLQR improves the comparative performance in the interval of 24.94%–49.07%, 35.78%–68.86%, 36.67%–59.08%, and 29.94%–57.87% for various performance metrics, i.e., average queue length, average waiting time, throughput, and average speed, respectively.}
}


@article{DBLP:journals/toit/LiBCGXY24,
	author = {Wen Li and
                  Lingfeng Bao and
                  Jiachi Chen and
                  John C. Grundy and
                  Xin Xia and
                  Xiaohu Yang},
	title = {Market Manipulation of Cryptocurrencies: Evidence from Social Media
                  and Transaction Data},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {2},
	pages = {8:1--8:26},
	year = {2024},
	url = {https://doi.org/10.1145/3643812},
	doi = {10.1145/3643812},
	timestamp = {Sun, 19 Jan 2025 14:17:15 +0100},
	biburl = {https://dblp.org/rec/journals/toit/LiBCGXY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The cryptocurrency market cap has experienced a great increase in recent years. However, large price fluctuations demonstrate the need for governance structures and identify whether there are market manipulations. In this article, we conduct three analyses—social media data analysis, blockchain data analysis, and price bubble analysis—to investigate whether market manipulation exists on Bitcoin, Ethereum, and Dogecoin platforms. Social media data analysis aims to find the reasons for price fluctuations. Blockchain data analysis is used to find detailed behavior of the manipulators. Price bubble analysis is used to investigate the relation between price fluctuation and manipulators’ behavior. By using the three analyses, we show that market manipulation exists on Bitcoin, Ethereum, and Dogecoin. However, market manipulation of Bitcoin is limited, and for most of Bitcoin’s price fluctuations, we found other explanations. The price for Ethereum is the most sensitive to technical updates. Technical companies/teams usually hype some new concepts (e.g., ICO, DeFi), which causes a price spike. The price of Dogecoin has a high correlation with Elon Musk’s X (formerly known as Twitter) activity, showing that influential individuals have the ability to manipulate its prices. In addition, the poor monetary liquidity of Dogecoin allows some users to manipulate its price.}
}


@article{DBLP:journals/toit/CharmetMTT24,
	author = {Fabien Charmet and
                  Tomohiro Morikawa and
                  Akira Tanaka and
                  Takeshi Takahashi},
	title = {{VORTEX} : Visual phishing detectiOns aRe Through EXplanations},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {2},
	pages = {9:1--9:24},
	year = {2024},
	url = {https://doi.org/10.1145/3654665},
	doi = {10.1145/3654665},
	timestamp = {Tue, 18 Jun 2024 09:26:04 +0200},
	biburl = {https://dblp.org/rec/journals/toit/CharmetMTT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Phishing attacks reached a record high in 2022, as reported by the Anti-Phishing Work Group, following an upward trend accelerated during the pandemic. Attackers employ increasingly sophisticated tools in their attempts to deceive unaware users into divulging confidential information. Recently, the research community has turned to the utilization of screenshots of legitimate and malicious websites to identify the brands that attackers aim to impersonate. In the field of Computer Vision, convolutional neural networks (CNNs) have been employed to analyze the visual rendering of websites, addressing the problem of phishing detection. However, along with the development of these new models, arose the need to understand their inner workings and the rationale behind each prediction. Answering the question, “How is this website attempting to steal the identity of a well-known brand?” becomes crucial when protecting end-users from such threats. In cybersecurity, the application of explainable AI (XAI) is an emerging approach that aims to answer such questions. In this article, we propose VORTEX, a phishing website detection solution equipped with the capability to explain how a screenshot attempts to impersonate a specific brand. We conduct an extensive analysis of XAI methods for the phishing detection problem and demonstrate that VORTEX provides meaningful explanations regarding the detection results. Additionally, we evaluate the robustness of our model against Adversarial Example attacks. We adapt these attacks to the VORTEX architecture and evaluate their efficacy across multiple models and datasets. Our results show that VORTEX achieves superior accuracy compared to previous models, and learns semantically meaningful patterns to provide actionable explanations about phishing websites. Finally, VORTEX demonstrates an acceptable level of robustness against adversarial example attacks.}
}


@article{DBLP:journals/toit/ChenLFX24,
	author = {Yanming Chen and
                  Tong Luo and
                  Weiwei Fang and
                  Neal Naixue Xiong},
	title = {EdgeCI: Distributed Workload Assignment and Model Partitioning for
                  {CNN} Inference on Edge Clusters},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {2},
	pages = {10:1--10:24},
	year = {2024},
	url = {https://doi.org/10.1145/3656041},
	doi = {10.1145/3656041},
	timestamp = {Sun, 19 Jan 2025 14:17:12 +0100},
	biburl = {https://dblp.org/rec/journals/toit/ChenLFX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning technology has grown significantly in new application scenarios such as smart cities and driverless vehicles, but its deployment needs to consume a lot of resources. It is usually difficult to execute inference task solely on resource-constrained Intelligent Internet-of-Things (IoT) devices to meet strictly service delay requirements. CNN-based inference task is usually offloaded to the edge server or cloud. However, it may lead to unstable performance and privacy leaks. To address the above challenges, this article aims to design a low latency distributed inference framework, EdgeCI, which assigns inference tasks to locally idle, connected, and resource-constrained IoT device cluster networks. EdgeCI exploits two key optimization knobs, including: (1) Auction-based Workload Assignment Scheme (AWAS), which achieves the workload balance by assigning each workload partition to the more matching IoT device; (2) Fused-Layer parallelization strategy based on non-recursive Dynamic Programming (DPFL), which is aimed at further minimizing the inference time. We have implemented EdgeCI based on PyTorch and evaluated its performance with VGG-16 and ResNet-34 image recognition models. The experimental results prove that our proposed AWAS and DPFL outperform the typical state-of-the-art solutions. When they are well combined, EdgeCI can improve inference speed by 34.72% to 43.52%. EdgeCI outperforms the state-of-the art approaches on our edge cluster.}
}


@article{DBLP:journals/toit/WangGWCC24,
	author = {Zichen Wang and
                  Xiangshan Gao and
                  Cong Wang and
                  Peng Cheng and
                  Jiming Chen},
	title = {Efficient Vertical Federated Unlearning via Fast Retraining},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {2},
	pages = {11:1--11:22},
	year = {2024},
	url = {https://doi.org/10.1145/3657290},
	doi = {10.1145/3657290},
	timestamp = {Tue, 18 Jun 2024 09:26:04 +0200},
	biburl = {https://dblp.org/rec/journals/toit/WangGWCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vertical federated learning (VFL) revolutionizes privacy-preserved collaboration for small businesses that have distinct but complementary feature sets. However, as the scope of VFL expands, the constant entering and leaving of participants and the subsequent exercise of the “right to be forgotten” pose a great challenge in practice. The question of how to efficiently erase one’s contribution from the shared model remains largely unexplored in the context of VFL. In this article, we introduce a vertical federated unlearning framework, which integrates model checkpointing techniques with a hybrid, first-order optimization technique. The core concept is to reduce backpropagation time and improve convergence/generalization by combining the advantages of the existing optimizers. We provide in-depth theoretical analysis and time complexity to illustrate the effectiveness of the proposed design. We conduct extensive experiments on six public datasets and demonstrate that our method could achieve up to 6.3× speedup compared to the baseline, with negligible influence on the original learning task.}
}


@article{DBLP:journals/toit/ChhetriTSJPN24,
	author = {Mohan Baruwal Chhetri and
                  Shahroz Tariq and
                  Ronal Singh and
                  Fatemeh Jalalvand and
                  C{\'{e}}cile Paris and
                  Surya Nepal},
	title = {Towards Human-AI Teaming to Mitigate Alert Fatigue in Security Operations
                  Centres},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {3},
	pages = {1--22},
	year = {2024},
	url = {https://doi.org/10.1145/3670009},
	doi = {10.1145/3670009},
	timestamp = {Sun, 19 Jan 2025 14:17:15 +0100},
	biburl = {https://dblp.org/rec/journals/toit/ChhetriTSJPN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Security Operations Centres (SOCs) play a pivotal role in defending organisations against evolving cyber threats. They function as central hubs for detecting, analysing, and responding promptly to cyber incidents with the primary objective of ensuring the confidentiality, integrity, and availability of digital assets. However, they struggle against the growing problem of alert fatigue, where the sheer volume of alerts overwhelms SOC analysts and raises the risk of overlooking critical threats. In recent times, there has been a growing call for human-AI teaming, wherein humans and AI collaborate with each other, leveraging their complementary strengths and compensating for their weaknesses. The rapid advances in AI and the growing integration of AI-enabled tools and technologies within SOCs give rise to a compelling argument for the implementation of human-AI teaming within the SOC environment. Therefore, in this article, we present our vision for human-AI teaming to address the problem of alert fatigue in the SOC. We propose the 𝒜2 𝒞 Framework, which enables flexible and dynamic decision making by allowing seamless transitions between automated, augmented, and collaborative modes of operation. Our framework allows AI-powered automation for routine alerts, AI-driven augmentation for expedited expert decision making, and collaborative exploration for tackling complex, novel threats. By implementing and operationalising 𝒜2 𝒞, SOCs can significantly reduce alert fatigue while empowering analysts to efficiently and effectively respond to security incidents.}
}


@article{DBLP:journals/toit/ValiAS24,
	author = {Ali Akbar Vali and
                  Sadoon Azizi and
                  Mohammad Shojafar},
	title = {{RESP:} {A} Recursive Clustering Approach for Edge Server Placement
                  in Mobile Edge Computing},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {3},
	pages = {1--25},
	year = {2024},
	url = {https://doi.org/10.1145/3666091},
	doi = {10.1145/3666091},
	timestamp = {Sun, 22 Dec 2024 15:49:29 +0100},
	biburl = {https://dblp.org/rec/journals/toit/ValiAS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid advancement of the Internet of Things and 5G networks in smart cities, the inevitable generation of massive amounts of data, commonly known as big data, has introduced increased latency within the traditional cloud computing paradigm. In response to this challenge, Mobile Edge Computing (MEC) has emerged as a viable solution, offloading a portion of mobile device workloads to nearby edge servers equipped with ample computational resources. Despite significant research in MEC systems, optimizing the placement of edge servers in smart cities to enhance network performance has received little attention. In this article, we propose RESP, a novel Recursive clustering technique for Edge Server Placement in MEC environments. RESP operates based on the median of each cluster determined by the number of base transceiver stations, strategically placing edge servers to achieve workload balance and minimize network traffic between them. Our proposed clustering approach substantially improves load balancing compared to existing methods and demonstrates superior performance in handling traffic dynamics. Through experimental evaluation with real-world data from Shanghai Telecom’s base station dataset, our approach outperforms several representative techniques in terms of workload balancing and network traffic optimization. By addressing the ESP problem and introducing an advanced recursive clustering technique, this work makes a substantial contribution to optimizing mobile edge computing networks in smart cities. The proposed algorithm outperforms alternative methodologies, demonstrating a 10% average improvement in optimizing network traffic. Moreover, it achieves a 53% more suitable result in terms of computational load.}
}


@article{DBLP:journals/toit/SaryazdiABL24,
	author = {Sepehr Saryazdi and
                  Balsam Alkouz and
                  Athman Bouguettaya and
                  Abdallah Lakhdari},
	title = {Using Reinforcement Learning and Error Models for Drone Precise Landing},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {3},
	pages = {1--30},
	year = {2024},
	url = {https://doi.org/10.1145/3670997},
	doi = {10.1145/3670997},
	timestamp = {Sun, 19 Jan 2025 14:17:12 +0100},
	biburl = {https://dblp.org/rec/journals/toit/SaryazdiABL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a novel framework for achieving precision landing in drone services. The proposed framework consists of two distinct decoupled modules, each designed to address a specific aspect of landing accuracy. The first module is concerned with intrinsic errors, where new error models are introduced. This includes a spherical error model that takes into account the orientation of the drone. Additionally, we propose a live position correction algorithm that employs the error models to correct for intrinsic errors in real time. The second module focuses on external wind forces and presents an aerodynamics model with wind generation to simulate the drone’s physical environment. We utilize reinforcement learning to train the drone in simulation with the goal of landing precisely under dynamic wind conditions. Experimental results, conducted through simulations and validated in the physical world, demonstrate that our proposed framework significantly increases landing accuracy while maintaining a low onboard computational cost.}
}


@article{DBLP:journals/toit/AguruB24,
	author = {Aswani Devi Aguru and
                  Erukala Suresh Babu},
	title = {OTI-IoT: {A} Blockchain-based Operational Threat Intelligence Framework
                  for Multi-vector DDoS Attacks},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {3},
	pages = {1--31},
	year = {2024},
	url = {https://doi.org/10.1145/3664287},
	doi = {10.1145/3664287},
	timestamp = {Sun, 19 Jan 2025 14:17:11 +0100},
	biburl = {https://dblp.org/rec/journals/toit/AguruB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) refers to a complex network comprising interconnected devices that transmit their data via the Internet. Due to their open environment, limited computation power, and absence of built-in security, IoT environments are susceptible to various cyberattacks. Denial of service (DDoS) attacks are among the most destructive types of threats. The Multi-vector DDoS attack is a contemporary and formidable form of DDoS wherein the attacker employs a collection of compromised IoT devices as zombies to initiate numerous DDoS attacks against a target server. A Blockchain-based Operational Threat Intelligence framework, OTI-IoT, is proposed in this article to counter multi-vector DDoS attacks in IoT networks. A “Prevent-then-Detect” methodology was utilized to deploy the OTI-IoT framework in two distinct stages. During Phase 1, the consortium Blockchain network validators employ the IPS module, composed of a smart contract for attack prevention and access control, and Proof of Voting consensus, to thwart attacks. Validators are outfitted with deep learning-based IDS instances to detect multi-vector DDoS attacks during Phase 2. Alert messages are generated by the IDS module’s alert generation and propagation smart contract in response to identifying malicious IoT sources. The feedback loop from the IDS module to the IPS module prevents incoming traffic from malicious sources. The proposed OTI framework capabilities are realized as an outcome of combining and storing the outcomes of the IDS and IPS modules on the consortium Blockchain. Each validator maintains a shared ledger containing information regarding threat sources to ensure robust security, transparency, and integrity. The operational execution of OTI-IoT occurs on an individual Ethereum Blockchain. The empirical findings indicate that our proposed framework is most suitable for real-time applications due to its ability to lower attack detection time, decreased block validation time, and higher attack prevention rate.}
}


@article{DBLP:journals/toit/GanWXL24,
	author = {Rundong Gan and
                  Le Wang and
                  Liang Xue and
                  Xiaodong Lin},
	title = {Exposing Stealthy Wash Trading on Automated Market Maker Exchanges},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {4},
	pages = {1--30},
	year = {2024},
	url = {https://doi.org/10.1145/3689631},
	doi = {10.1145/3689631},
	timestamp = {Sun, 19 Jan 2025 14:17:15 +0100},
	biburl = {https://dblp.org/rec/journals/toit/GanWXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decentralized Finance (DeFi), a pivotal component of the emerging Web3 landscape, is gaining popularity but remains vulnerable to market manipulations, such as wash trading. Wash trading is an illegal practice, where traders buy and sell assets to themselves within cryptocurrency exchanges to artificially inflate trading volumes and distort market perceptions. However, current research primarily focuses on traditional exchanges based on the Order-book mechanism (similar to stock markets), while ignoring the Automated Market Maker (AMM) exchanges, which dominate over 75% of the market and represent a significant innovation within the DeFi. This study utilizes entity recognition technology to detect wash trading on AMM exchanges within Ethereum-like systems, based on the understanding that colluding addresses (perceived as the same entity) must use ETH for transaction fees and exhibit direct or indirect ETH transfer links. We identify wash trading when addresses with transfer connections almost simultaneously buy and sell assets while their total asset holdings remain nearly constant. This comprehensive blockchain network analysis, compared to focusing solely on transactions within exchanges, unveils covert wash trading activities. Our detection method achieves a 95.9% recall and a 96.7% true negative rate in identifying pools affected by wash trading, demonstrating its superiority over existing methods. Furthermore, we apply our method to 98,945 pools from Uniswap V2 & V3 (the most popular AMM exchanges on Ethereum) and identify 1,070,626 abnormal transactions, totaling $27.51 billion in trading volume. Analysis of these transactions uncovers insights into wash traders’ behaviors, including the utilization of multiple addresses and the dual roles of certain addresses as wash traders and liquidity providers. These insights are crucial for developing more effective strategies to combat fraudulent activities in the DeFi ecosystem and enhance financial scrutiny.}
}


@article{DBLP:journals/toit/ZhangLT24,
	author = {Wenjun Zhang and
                  Xiaoli Liu and
                  Sasu Tarkoma},
	title = {FedGK: Communication-Efficient Federated Learning through Group-Guided
                  Knowledge Distillation},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {4},
	pages = {1--21},
	year = {2024},
	url = {https://doi.org/10.1145/3674973},
	doi = {10.1145/3674973},
	timestamp = {Sun, 22 Dec 2024 15:49:29 +0100},
	biburl = {https://dblp.org/rec/journals/toit/ZhangLT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) empowers a cohort of participating devices to contribute collaboratively to a global neural network model, ensuring that their training data remains private and stored locally. Despite its advantages in computational efficiency and privacy preservation, FL grapples with the challenge of non-IID (not independent and identically distributed) data from diverse clients, leading to discrepancies between local and global models and potential performance degradation. In this article, we propose FedGK, an innovative communication-efficient Group-Guided FL framework designed for heterogeneous data distributions. FedGK employs a localized-guided framework that enables the client to effectively assimilate key knowledge from teachers and peers while minimizing extraneous peer information in FL scenarios. We conduct an in-depth analysis of the dynamic similarities among clients over successive communication rounds and develop a novel clustering approach that accurately groups clients with diverse heterogeneities. We implement FedGK on public datasets with an innovative data transformation pattern called “cluster-shift non-IID”, which mirrors the more prevalent data distributions in real-world settings and could be grouped into clusters with similar data distributions. Extensive experimental results on public datasets demonstrate that the proposed approach FedGK improves accuracy by up to 32.89% and saves up to 53.33% communication cost over state-of-the-art baselines.}
}


@article{DBLP:journals/toit/BarrigaBTC24,
	author = {Arturo Barriga and
                  Jos{\'{e}} A. Barriga and
                  Miguel {\'{A}}ngel P{\'{e}}rez Toledano and
                  Pedro J. Clemente},
	title = {Model-Driven Development Towards Distributed Intelligent Systems},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {4},
	pages = {1--28},
	year = {2024},
	url = {https://doi.org/10.1145/3687472},
	doi = {10.1145/3687472},
	timestamp = {Mon, 03 Mar 2025 22:25:55 +0100},
	biburl = {https://dblp.org/rec/journals/toit/BarrigaBTC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A Distributed Intelligent System (DIS) encompasses a set of intelligent subsystems and components that collaborate to perform tasks and solve problems. Given the advancements of paradigms such as the Internet of Things, along with the advancements of technologies such as Machine Learning and Digital Twins, DISs are on the rise. These systems are increasingly integrating components that perform intelligent functions, and these intelligent functions are increasingly heterogeneous and varied. Moreover, there is no standardized framework to help researchers and practitioners adequately address DISs. As a result, the complexity, interoperability issues, and development time and costs of these systems are growing. However, Model-Driven Development (MDD) can help to address these challenges by providing a Domain-Specific Language (DSL) for developing DISs. In this work, a DSL for the design, validation, generation, and deployment of DISs is proposed. Firstly, the proposed DSL captures in a metamodel the key and high-level abstract concepts of the distinct DISs documented in the literature. Then, it allows modeling of DISs conforming to this metamodel. Subsequently, the DSL enables formal validation of the modeled systems. Lastly, it allows the generation and deployment of all DISs into production. Therefore, the work undertaken in this communication provides a methodological, formal, and standardized approach to defining and developing DISs from a high level of abstraction. This work allows users to address DISs by facilitating agility, minimizing manual tasks, and reducing the number of defects introduced in their development. To illustrate the applicability of the proposed DSL, a real case study of an agricultural digital twin is presented.}
}


@article{DBLP:journals/toit/WangZZ24,
	author = {Jian Wang and
                  Delei Zhao and
                  Guosheng Zhao},
	title = {Malicious Participants and Fake Task Detection Incorporating Gaussian
                  Bias},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {4},
	pages = {1--19},
	year = {2024},
	url = {https://doi.org/10.1145/3696419},
	doi = {10.1145/3696419},
	timestamp = {Sun, 22 Dec 2024 15:49:29 +0100},
	biburl = {https://dblp.org/rec/journals/toit/WangZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing (MCS) is a combination of crowdsourcing ideas and mobile sensing devices, designed to enable rational allocation of resources at scale. However, the MCS platform is highly vulnerable to injection attacks from malicious participants and fake tasks that interfere with platform service capabilities and sensing activities. To this end, the participant and task submission process is modeled as a multivariate time series, and a detection model for malicious participants and fake tasks (MP-FTD) with a Gaussian prior on the attentional mechanism and a two-stage adversarial training process is proposed. The attention mechanism was corrected using Gaussian bias, and then the corrected attention mechanism was used to obtain the correlation discrepancies between the data. Using the adversarial training method of Generative Adversarial Networks (GAN), the output of the correlation discrepancy reconstruction phase is transformed into a focus score, to amplify the reconstruction error in the output of the focus score reconstruction phase, and to improve the differentiation between the injected data and normal data of malicious attackers. The detection of these malicious attackers will effectively improve the robustness of the sensing platform. Experiments on six real-world datasets showed that the average F1-score reached 93.44%, outperforming the current baseline method, and resulting in an average 12.07% improvement in participant assignment accuracy and an average 12.25% improvement in task assignment accuracy in task assignment experiments.}
}


@article{DBLP:journals/toit/LiuWZ24,
	author = {Jia Liu and
                  Jian Wang and
                  Guosheng Zhao},
	title = {Conscious Task Recommendation via Cognitive Reasoning Computing in
                  Mobile Crowd Sensing},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {4},
	pages = {1--25},
	year = {2024},
	url = {https://doi.org/10.1145/3694786},
	doi = {10.1145/3694786},
	timestamp = {Sun, 22 Dec 2024 15:49:29 +0100},
	biburl = {https://dblp.org/rec/journals/toit/LiuWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowd Sensing is a human-based data collection model, and the approach taken to recommend data collection tasks to users in order to maximize task acceptance rates is an important part of this research. Existing task recommendation methods are based only on intuitive data for unconscious analysis and decision-making, and lack the embodiment of cognitive intelligence. To address the above problem, a conscious task recommendation based on cognitive reasoning computing in Mobile Crowd Sensing has been proposed, using knowledge from cognitive science to simulate the human thinking process in order to achieve warm learning and conscious recommendation of sensing tasks. First, the task attributes are segmented into positive and negative attributes using a Kernel Density Estimation method based on bandwidth self-selection. Then, the user's attribute preferences are diagnosed by the Cognitive Diagnostic Method to obtain the user's preference vector. Finally, get the overall preference trend of users based on the Drift Diffusion Model, and make decisions according to whether the current task drift direction is consistent with the user preference trend. Simulation experiments were conducted using the Taobao dataset, MTurk dataset, and synthetic dataset, it was ultimately proven that conscious task recommendation combined with user cognitive ability effectively reduced RMSE and improved task acceptance rate. RMSE was 10.5%∼70.8% lower than other methods, and the task acceptance rate was basically over 80%, with most of the results being over 90%.}
}


@article{DBLP:journals/toit/JasimS24,
	author = {Mohammed Jasim and
                  Nazli Siasi},
	title = {Local Load Migration in High-Capacity Fog Computing},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {4},
	pages = {1--31},
	year = {2024},
	url = {https://doi.org/10.1145/3690386},
	doi = {10.1145/3690386},
	timestamp = {Sun, 22 Dec 2024 15:49:29 +0100},
	biburl = {https://dblp.org/rec/journals/toit/JasimS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fog computing brings storage and computational capabilities closer to the data source, which reduces latency and enhances efficiency in processing data. However, these capabilities are resource-constrained at the fog nodes as compared to the cloud core. This limitation can result in high computational loads that yield in saturation and congestion at the fog nodes when processing large traffic volumes. Hence, this article introduces a novel load migration scheme for delay-sensitive and computation-intensive requests in fog computing without relaying to the cloud core, thus avoiding prolonged link delays. First, a grouped service function chain (SFC) provisioning framework is embedded on a heterogeneous architecture composed of super fog (SF) and ordinary fog (OF) nodes, where all the virtual network functions (VNFs) in the SFC are mapped on a single SF node to reduce resource use. Here, the SF nodes are always in active mode to receive incoming traffic, whereas the OF nodes are maintained in idle mode to save power and computing resources. When the SF node approaches a saturation threshold alarm, it diffuses its load (hosted VNFs) gradually to neighboring OF nodes in the vicinity, thus avoiding saturation at the SF node and providing service continuity. The framework is implemented on a resource-constrained network to achieve realistic operating conditions. Overall, the proposed work achieves high admission and resource utilization rates, reduced delays, and power consumption, as compared to key network architectures and provisioning schemes that separately map delay-sensitive and computation-intensive on hierarchical fog layers, which incur increased delays, network traffic, and power usage.}
}


@article{DBLP:journals/toit/FanSWMW24,
	author = {Shuming Fan and
                  Hongjian Shi and
                  Chenpei Wang and
                  Ruhui Ma and
                  Xiaoming Wang},
	title = {{RFL-LSU:} {A} Robust Federated Learning Approach with Localized Stepwise
                  Updates},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {4},
	pages = {1--26},
	year = {2024},
	url = {https://doi.org/10.1145/3690822},
	doi = {10.1145/3690822},
	timestamp = {Sun, 22 Dec 2024 15:49:29 +0100},
	biburl = {https://dblp.org/rec/journals/toit/FanSWMW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed intelligence enables the widespread deployment of AI technology, greatly promoting the development of AI. Federated learning is a widely used distributed intelligence technology that allows iterative optimization of global model while protecting user data privacy. Currently, federated learning faces some security threats, as its open architecture provides attackers with opportunities to disrupt the learning process by submitting malicious updates or inserting backdoors. In this article, we propose a robust federated learning method to defend against potential malicious attacks. Specifically, we enhance the algorithm’s performance and stability by implementing localized stepwise updates on the client side and element-wise anomaly detection on the server side. We conducted experiments in a more realistic non-i.i.d. scenario and compared the results with other typical federated learning methods. Our results demonstrate that our approach exhibits strong robustness under non-i.i.d. data distributions, outperforming other methods in terms of test accuracy and resilience to attacks.}
}

@article{DBLP:journals/toit/MudvariVOTT24,
  author       = {Akrit Mudvari and
                  Antero Vainio and
                  Iason Ofeidis and
                  Sasu Tarkoma and
                  Leandros Tassiulas},
  title        = {Adaptive Compression-Aware Split Learning and Inference for Enhanced
                  Network Efficiency},
  journal      = {{ACM} Trans. Internet Techn.},
  volume       = {24},
  number       = {4},
  pages        = {1--26},
  year         = {2024},
  url          = {https://doi.org/10.1145/3687471},
  doi          = {10.1145/3687471},
  timestamp    = {Sun, 19 Jan 2025 14:17:13 +0100},
  biburl       = {https://dblp.org/rec/journals/toit/MudvariVOTT24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract     = {The growing number of AI-driven applications in mobile devices has led to solutions that integrate deep learning models with the available edge-cloud resources. Due to multiple benefits such as reduction in on-device energy consumption, improved latency, improved network usage, and certain privacy improvements, split learning, where deep learning models are split away from the mobile device and computed in a distributed manner, has become an extensively explored topic. Incorporating compression-aware methods (where learning adapts to compression level of the communicated data) has made split learning even more advantageous. This method could even offer a viable alternative to traditional methods, such as federated learning techniques. In this work, we develop an adaptive compression-aware split learning method (“deprune”) to improve and train deep learning models so that they are much more network-efficient, which would make them ideal to deploy in weaker devices with the help of edge-cloud resources. This method is also extended (“prune”) to very quickly train deep learning models through a transfer learning approach, which tradesoff little accuracy for much more network-efficient inference abilities. We show that the “deprune” method can reduce network usage by 4× when compared with a split-learning approach (that does not use our method) without loss of accuracy, while also improving accuracy over compression-aware split-learning by up to 4 percent. Lastly, we show that the “prune” method can reduce the training time for certain models by up to 6× without affecting the accuracy when compared against a compression-aware split-learning approach.}
}


@article{DBLP:journals/toit/MayerBGFS24,
	author = {Simon Mayer and
                  Arne Broering and
                  Kimberly Garc{\'{\i}}a and
                  Konstantinos Fysarakis and
                  Beatriz Soret},
	title = {Introduction to the Special Issue on Distributed Intelligence on the
                  Internet},
	journal = {{ACM} Trans. Internet Techn.},
	volume = {24},
	number = {4},
	pages = {1},
	year = {2024},
	url = {https://doi.org/10.1145/3700769},
	doi = {10.1145/3700769},
	timestamp = {Sun, 22 Dec 2024 15:49:29 +0100},
	biburl = {https://dblp.org/rec/journals/toit/MayerBGFS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/toit/AgarwalMP24,
  author       = {Vidushi Agarwal and
                  Shruti Mishra and
                  Sujata Pal},
  title        = {Towards a Sustainable Blockchain: {A} Peer-to-Peer Federated Learning
                  based Approach},
  journal      = {{ACM} Trans. Internet Techn.},
  volume       = {24},
  number       = {4},
  pages        = {1--26},
  year         = {2024},
  url          = {https://doi.org/10.1145/3680544},
  doi          = {10.1145/3680544},
  timestamp    = {Sun, 19 Jan 2025 14:17:11 +0100},
  biburl       = {https://dblp.org/rec/journals/toit/AgarwalMP24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract     = {In the rapidly evolving digital world, blockchain technology is becoming the foundation for numerous applications, ranging from financial services to supply chain management. As the usage of blockchain is becoming more prevalent, the energy-intensive nature of this technology has raised concerns about its long-term sustainability and environmental footprint. To address this challenge, we explore the potential of Peer-to-Peer Federated Learning (P2P-FL), a distributed machine learning approach that allows multiple nodes to collaborate without sharing raw data. We present a novel integration of P2P-FL with blockchain technology, aimed at enhancing the sustainability and efficiency of blockchain networks. The basic idea of our approach is the use of distributed learning mechanisms to find the optimal performance parameters of blockchain without relying on centralized control. These parameters are then used by a load-balancing mechanism that prioritizes energy efficiency to distribute loads on different blockchains. Furthermore, we formulate a non-cooperative game theory model to align the individual node strategies with the collective objective of energy optimization, ensuring a balance between self-interest and overall network performance. Our work is exemplified through a case study in the renewable energy sector, demonstrating the application of our model in creating an efficient marketplace for energy trading. The experimentation and results indicate a significant improvement in the execution times and energy consumption of blockchain networks. Therefore, the overall sustainability of the network is enhanced, making our framework practical and applicable in real-world scenarios.}
}